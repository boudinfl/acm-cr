@inproceedings{10.1145/3132847.3137173,
author = {Rastogi, Rajeev},
title = {Machine Learning @ Amazon},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3137173},
doi = {10.1145/3132847.3137173},
abstract = {In this talk, I will first provide an overview of key problem areas where we are applying Machine Learning (ML) techniques within Amazon such as product demand forecasting, product search, and information extraction from reviews, and associated technical challenges. I will then talk about three specific applications where we use a variety of methods to learn semantically rich representations of data: question answering where we use deep learning techniques, product size recommendations where we use probabilistic models, and fake reviews detection where we use tensor factorization algorithms.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1},
numpages = {1},
keywords = {question answering, deep learning, recommendations, bayesian inference},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3137174,
author = {Mihalcea, Rada},
title = {Deception Detection: When Computers Become Better than Humans},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3137174},
doi = {10.1145/3132847.3137174},
abstract = {Whether we like it or not, deception happens every day and everywhere: thousands of trials taking place daily around the world; little white lies: "I'm busy that day!" even if your calendar is blank; news "with a twist" (a.k.a. fake news) meant to attract the readers attraction, and get some advertisement clicks on the side; portrayed identities, on dating sites and elsewhere. Can a computer automatically detect deception in written accounts or in video recordings? In this talk, I will describe our work in building linguistic and multimodal algorithms for deception detection, targeting deceptive statements, trial videos, fake news, identity deceptions, and also going after deception in multiple cultures. I will also show how these algorithms can provide insights into what makes a good lie - and thus teach us how we can spot a liar. As it turns out, computers can be trained to identify lies in many different contexts, and they can do it much better than humans do! },
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {3},
numpages = {1},
keywords = {Keynote Talk},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3137175,
author = {Yang, Qiang},
title = {When Deep Learning Meets Transfer Learning},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3137175},
doi = {10.1145/3132847.3137175},
abstract = {Deep learning has achieved great success as evidenced by many practical applications and contests. However, deep learning developed so far also has some inherent limitations. In particular, deep learning is not yet very adaptable to different related domains and cannot handle small data. In this talk, I will give an overview of how transfer learning can help alleviate these problems. In particular, I will present some recent progress on integrating deep learning and transfer learning together and show some interesting applications in sentiment analysis, image processing and urban computing.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {5},
numpages = {1},
keywords = {machine learning, transfer learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3137176,
author = {Krishnan, K. Ananth},
title = {A Hyper-Connected World},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3137176},
doi = {10.1145/3132847.3137176},
abstract = {As the world gets hyper-connected, cities are evolving into complex ecosystems, technically and behaviourally. Machines and humans interact continually, generating streams of data and behavior patterns. To be a true smart city in a hyper-connected world, cities today have to use technology like a modern enterprise: build a digital spine; become intelligent and leverage automation. However, this technology core should be people centric. In a multiple stakeholder ecosystem, city administrators, industries and citizens, will look at the city from a different perspective and expect different experiences. Finally citizen experience will be the determinant of success of a smart city. While articulating this vision, Ananth will highlight how differently businesses must orient themselves in this environment.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {7},
numpages = {1},
keywords = {hyper-connected, smart cities, enterprise digital spine.},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133030,
author = {Li, Chao and Yang, Yang and Cao, Jiewei and Huang, Zi},
title = {Jointly Modeling Static Visual Appearance and Temporal Pattern for Unsupervised Video Hashing},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133030},
doi = {10.1145/3132847.3133030},
abstract = {Recently, hashing has been evidenced as an efficient and effective method to facilitate large-scale video retrieval. Most of existing hashing methods are based on visual features, which are expected to capture the appearance of videos. The intrinsic temporal pattern embedded in videos has also shown its discriminative power for similarity search, and is explored and utilised in some recent studies. However, how to leverage the strengths in both aspects remains unknown.In this paper, we propose to jointly model static visual appearance and temporal pattern for video hash code generation, as both of them are believed to be carrying important information for learning an effective hash function. A novel unsupervised video hashing framework is designed correspondingly, where its hash function is comprised of two encoders including the temporal encoder and the appearance encoder. The two encoders are learned by self-supervision and designed to be able to reconstruct the temporal pattern of videos and visual appearance of frames respectively. Last but not least, for jointly learning of the two encoders, we impose three learning criteria including minimal binarization loss, balanced hash codes and independent hash codes. From the extensive experiments conducted on two large-scale video datasets (i.e. FCVID and ActivityNet), we have confirmed the superior performance of our method comparing to the state-of-the-art video hashing methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {9–17},
numpages = {9},
keywords = {lstm, learning to hash, deep learning, visual content retrieval, video hashing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132982,
author = {Kim, Hyunsoo and Jeon, Youngbae and Yoon, Ji Won},
title = {Construction of a National Scale ENF Map Using Online Multimedia Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132982},
doi = {10.1145/3132847.3132982},
abstract = {The frequency of power distribution networks in a power grid is called electrical network frequency (ENF). Because it provides the spatio-temporal changes of the power grid in a particular location, ENF is used in many application domains including the prediction of grid instability and blackouts, detection of system breakup, and even digital forensics. In order to build high performing applications and systems, it is necessary to capture a large-scale nationwide or worldwide ENF map. Consequently, many studies have been conducted on the distribution of specialized physical devices that capture the ENF signals. However, this approach is not practical because it requires significant effort from design to setup, moreover, it has a limitation in its efficiency to monitor and stably retain the collection equipment distributed throughout the world. Furthermore, this approach requires a significant budget.In this paper, we proposed a novel approach to constructing the worldwide ENF map by analyzing streaming data obtained by online multimedia services, such as "Youtube", "Earthcam", and "Ustream" instead of expensive specialized hardware. However, extracting accurate ENF from the streaming data is not a straightforward process because multimedia has its own noise and uncertainty. By applying several signal processing techniques, we can reduce noise and uncertainty, and improve the quality of the restored ENF.For the evaluation of this process, we compared the performance between the ENF signals restored by our proposed approach and collected by the frequency disturbance recorder (FDR) from FNET/GridEye. The experimental results show that our proposed approach outperforms in stable acquisition and management of the ENF signals compared to the conventional approach.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {19–28},
numpages = {10},
keywords = {power grid, electrical network frequency, frequency domain, multimedia data},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132920,
author = {Zhao, Wei and Xu, Wei and Yang, Min and Ye, Jianbo and Zhao, Zhou and Feng, Yabing and Qiao, Yu},
title = {Dual Learning for Cross-Domain Image Captioning},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132920},
doi = {10.1145/3132847.3132920},
abstract = {Recent AI research has witnessed increasing interests in automatically generating image descriptions in text, which is coined as theimage captioning problem. Significant progresses have been made in domains where plenty of labeled training data (i.e. image-text pairs) are readily available or collected. However, obtaining rich annotated data is a time-consuming and expensive process, creating a substantial barrier for applying image captioning methods to a new domain. In this paper, we propose a cross-domain image captioning approach that uses a novel dual learning mechanism to overcome this barrier. First, we model the alignment between the neural representations of images and that of natural languages in the source domain where one can access sufficient labeled data. Second, we adjust the pre-trained model based on examining limited data (or unpaired data) in the target domain. In particular, we introduce a dual learning mechanism with a policy gradient method that generates highly rewarded captions. The mechanism simultaneously optimizes two coupled objectives: generating image descriptions in text and generating plausible images from text descriptions, with the hope that by explicitly exploiting their coupled relation, one can safeguard the performance of image captioning in the target domain. To verify the effectiveness of our model, we use MSCOCO dataset as the source domain and two other datasets (Oxford-102 and Flickr30k) as the target domains. The experimental results show that our model consistently outperforms previous methods for cross-domain image captioning.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {29–38},
numpages = {10},
keywords = {dual learning, image captioning, image synthesis, reinforcement learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132872,
author = {Wu, Sai and Zhang, Mengdan and Chen, Gang and Chen, Ke},
title = {A New Approach to Compute CNNs for Extremely Large Images},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132872},
doi = {10.1145/3132847.3132872},
abstract = {CNN (Convolution Neural Network) is widely used in visual analysis and achieves exceptionally high performances in image classification, face detection, object recognition, image recoloring, and other learning jobs. Using deep learning frameworks, such as Torch and Tensorflow, CNN can be efficiently computed by leveraging the power of GPU. However, one drawback of GPU is its limited memory which prohibits us from handling large images. Passing a 4K resolution image to the VGG network will result in an exception of out-of-memory for Titan-X GPU. In this paper, we propose a new approach that adopts the BSP (bulk synchronization parallel) model to compute CNNs for images of any size. Before fed to a specific CNN layer, the image is split into smaller pieces which go through the neural network separately. Then, a specific padding and normalization technique is adopted to merge sub-images back into one image. Our approach can be easily extended to support distributed multi-GPUs. In this paper, we use neural style network as our example to illustrate the effectiveness of our approach. We show that using one Titan-X GPU, we can transfer the style of an image with 10,000\texttimes{}10,000 pixels within 1 minute.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {39–48},
numpages = {10},
keywords = {multi-gpus, style transfer, convolution neural network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133015,
author = {Li, Dan and Kanoulas, Evangelos},
title = {Active Sampling for Large-Scale Information Retrieval Evaluation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133015},
doi = {10.1145/3132847.3133015},
abstract = {Evaluation is crucial in Information Retrieval. The development of models, tools and methods has significantly benefited from the availability of reusable test collections formed through a standardized and thoroughly tested methodology, known as the Cranfield paradigm. Constructing these collections requires obtaining relevance judgments for a pool of documents, retrieved by systems participating in an evaluation task; thus involves immense human labor. To alleviate this effort different methods for constructing collections have been proposed in the literature, falling under two broad categories: (a) sampling, and (b) active selection of documents. The former devises a smart sampling strategy by choosing only a subset of documents to be assessed and inferring evaluation measure on the basis of the obtained sample; the sampling distribution is being fixed at the beginning of the process. The latter recognizes that systems contributing documents to be judged vary in quality, and actively selects documents from good systems. The quality of systems is measured every time a new document is being judged. In this paper we seek to solve the problem of large-scale retrieval evaluation combining the two approaches. We devise an active sampling method that avoids the bias of the active selection methods towards good systems, and at the same time reduces the variance of the current sampling approaches by placing a distribution over systems, which varies as judgments become available. We validate the proposed method using TREC data and demonstrate the advantages of this new method compared to past approaches.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {49–58},
numpages = {10},
keywords = {horvitz-thompson estimator, cranfield, evaluation, sampling with varying probabilities},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132870,
author = {Mandayam Comar, Prakash and Sengamedu, Srinivasan H.},
title = {Intent Based Relevance Estimation from Click Logs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132870},
doi = {10.1145/3132847.3132870},
abstract = {Estimating the relevance of documents based on the user feedback is an essential component of search, retrieval and ranking problems. User click modeling in search has focused primarily on factoring out the position bias. It is easy to see that the query type (generic queries vs specific queries) and user intent (purchase vs exploration) also introduce a bias in the click signal. In other words, the results not matching with the user intent will not be clicked. In this paper, we outline a technique to model the interplay of query, user intent and position bias with respect to the relevance of the retrieved search results. In particular, we define two intents namely purchase and explore, and estimate the relevance of the documents with respect to these two intents. We also relate them to the relevance estimates from considering only the position bias. We empirically demonstrate the effectiveness of the proposed approach by comparing its performance against the well-known CoEC measure and the recently proposed factor model approach for relevance estimation.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {59–66},
numpages = {8},
keywords = {relevance, ranking, user intent, search, click logs},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133000,
author = {Baruah, Gaurav and McCreadie, Richard and Lin, Jimmy},
title = {A Comparison of Nuggets and Clusters for Evaluating Timeline Summaries},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133000},
doi = {10.1145/3132847.3133000},
abstract = {There is growing interest in systems that generate timeline summaries by filtering high-volume streams of documents to retain only those that are relevant to a particular event or topic. Continued advances in algorithms and techniques for this task depend on standardized and reproducible evaluation methodologies for comparing systems. However, timeline summary evaluation is still in its infancy, with competing methodologies currently being explored in international evaluation forums such as TREC. One area of active exploration is how to explicitly represent the units of information that should appear in a "good" summary. Currently, there are two main approaches, one based on identifying nuggets in an external "ground truth", and the other based on clustering system outputs. In this paper, by building test collections that have both nugget and cluster annotations, we are able to compare these two approaches. Specifically, we address questions related to evaluation effort, differences in the final evaluation products, and correlations between scores and rankings generated by both approaches. We summarize advantages and disadvantages of nuggets and clusters to offer recommendations for future system evaluations.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {67–76},
numpages = {10},
keywords = {meta-evaluation, real-time summarization, temporal summarization, summary evaluation, trec},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132895,
author = {Oosterhuis, Harrie and de Rijke, Maarten},
title = {Sensitive and Scalable Online Evaluation with Theoretical Guarantees},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132895},
doi = {10.1145/3132847.3132895},
abstract = {Multileaved comparison methods generalize interleaved comparison methods to provide a scalable approach for comparing ranking systems based on regular user interactions. Such methods enable the increasingly rapid research and development of search engines. However, existing multileaved comparison methods that provide reliable outcomes do so by degrading the user experience during evaluation. Conversely, current multileaved comparison methods that maintain the user experience cannot guarantee correctness. Our contribution is two-fold. First, we propose a theoretical framework for systematically comparing multileaved comparison methods using the notions of considerateness, which concerns maintaining the user experience, and fidelity, which concerns reliable correct outcomes. Second, we introduce a novel multileaved comparison method, Pairwise Preference Multileaving (PPM), that performs comparisons based on document-pair preferences, and prove that it is considerate and has fidelity. We show empirically that, compared to previous multileaved comparison methods, PPM is more sensitive to user preferences and scalable with the number of rankers being compared.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {77–86},
numpages = {10},
keywords = {multileaving, theoretical guarantees, information retrieval, ranker evaluation, online evaluation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132897,
author = {Thonet, Thibaut and Cabanac, Guillaume and Boughanem, Mohand and Pinel-Sauvagnat, Karen},
title = {Users Are Known by the Company They Keep: Topic Models for Viewpoint Discovery in Social Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132897},
doi = {10.1145/3132847.3132897},
abstract = {Social media platforms such as weblogs and social networking sites provide Internet users with an unprecedented means to express their opinions and debate on a wide range of issues. Concurrently with their growing importance in public communication, social media platforms may foster echo chambers and filter bubbles: homophily and content personalization lead users to be increasingly exposed to conforming opinions. There is therefore a need for unbiased systems able to identify and provide access to varied viewpoints. To address this task, we propose in this paper a novel unsupervised topic model, the Social Network Viewpoint Discovery Model (SNVDM). Given a specific issue (e.g., U.S. policy) as well as the text and social interactions from the users discussing this issue on a social networking site, SNVDM jointly identifies the issue's topics, the users' viewpoints, and the discourse pertaining to the different topics and viewpoints. In order to overcome the potential sparsity of the social network (i.e., some users interact with only a few other users), we propose an extension to SNVDM based on the Generalized P\'{o}lya Urn sampling scheme (SNVDM-GPU) to leverage "acquaintances of acquaintances" relationships. We benchmark the different proposed models against three baselines, namely TAM, SN-LDA, and VODUM, on a viewpoint clustering task using two real-world datasets. We thereby provide evidence that our model SNVDM and its extension SNVDM-GPU significantly outperform state-of-the-art baselines, and we show that utilizing social interactions greatly improves viewpoint clustering performance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {87–96},
numpages = {10},
keywords = {topic modeling, viewpoint discovery, social networks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133037,
author = {Cheng, Jiajun and Zhao, Shenglin and Zhang, Jiani and King, Irwin and Zhang, Xin and Wang, Hui},
title = {Aspect-Level Sentiment Classification with HEAT (HiErarchical ATtention) Network},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133037},
doi = {10.1145/3132847.3133037},
abstract = {Aspect-level sentiment classification is a fine-grained sentiment analysis task, which aims to predict the sentiment of a text in different aspects. One key point of this task is to allocate the appropriate sentiment words for the given aspect.Recent work exploits attention neural networks to allocate sentiment words and achieves the state-of-the-art performance. However, the prior work only attends to the sentiment information and ignores the aspect-related information in the text, which may cause mismatching between the sentiment words and the aspects when an unrelated sentiment word is semantically meaningful for the given aspect. To solve this problem, we propose a HiErarchical ATtention (HEAT) network for aspect-level sentiment classification. The HEAT network contains a hierarchical attention module, consisting of aspect attention and sentiment attention. The aspect attention extracts the aspect-related information to guide the sentiment attention to better allocate aspect-specific sentiment words of the text. Moreover, the HEAT network supports to extract the aspect terms together with aspect-level sentiment classification by introducing the Bernoulli attention mechanism. To verify the proposed method, we conduct experiments on restaurant and laptop review data sets from SemEval at both the sentence level and the review level. The experimental results show that our model better allocates appropriate sentiment expressions for a given aspect benefiting from the guidance of aspect terms. Moreover, our method achieves better performance on aspect-level sentiment classification than state-of-the-art models.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {97–106},
numpages = {10},
keywords = {hierarchical attention network, aspect, sentiment classification},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132936,
author = {Tay, Yi and Tuan, Luu Anh and Hui, Siu Cheung},
title = {Dyadic Memory Networks for Aspect-Based Sentiment Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132936},
doi = {10.1145/3132847.3132936},
abstract = {This paper proposes Dyadic Memory Networks (DyMemNN), a novel extension of end-to-end memory networks (memNN) for aspect-based sentiment analysis (ABSA). Originally designed for question answering tasks, memNN operates via a memory selection operation in which relevant memory pieces are adaptively selected based on the input query. In the problem of ABSA, this is analogous to aspects and documents in which the relationship between each word in the document is compared with the aspect vector. In the standard memory networks, simple dot products or feed forward neural networks are used to model the relationship between aspect and words which lacks representation learning capability. As such, our dyadic memory networks ameliorates this weakness by enabling rich dyadic interactions between aspect and word embeddings by integrating either parameterized neural tensor compositions or holographic compositions into the memory selection operation. To this end, we propose two variations of our dyadic memory networks, namely the Tensor DyMemNN and Holo DyMemNN. Overall, our two models are end-to-end neural architectures that enable rich dyadic interaction between aspect and document which intuitively leads to better performance. Via extensive experiments, we show that our proposed models achieve the state-of-the-art performance and outperform many neural architectures across six benchmark datasets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {107–116},
numpages = {10},
keywords = {aspect-based sentiment analysis, neural networks, aspect, deep learning, aspect-level sentiment analysis, sentiment analysis},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132915,
author = {Chen, Qiang and Li, Chenliang and Li, Wenjie},
title = {Modeling Language Discrepancy for Cross-Lingual Sentiment Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132915},
doi = {10.1145/3132847.3132915},
abstract = {Language discrepancy is inherent and be part of human languages. Thereby, the same sentiment would be expressed in different patterns across different languages. Unfortunately, the language discrepancy is overlooked by existing works of cross-lingual sentiment analysis. How to accommodate the inherent language discrepancy in sentiment for better cross-lingual sentiment analysis is still an open question. In this paper, we aim to model the language discrepancy in sentiment expressions as intrinsic bilingual polarity correlations (IBPCs) for better cross-lingual sentiment analysis. Specifically, given a document of source language and its translated counterpart, we firstly devise a sentiment representation learning phase to extract monolingual sentiment representation for each document in this pair separately. Then, the two sentiment representations are transferred to be the points in a shared latent space, named hybrid sentiment space. The language discrepancy is then modeled as a fixed transfer vector under each particular polarity between the source and target languages in this hybrid sentiment space. Two relation-based bilingual sentiment transfer models (i.e., RBST-s, RBST-hp) are proposed to learn the fixped transfer vectors. The sentiment of a target-language document is then determined based on the transfer vector between it and its translated counterpart in the hybrid sentiment space. Extensive experiments over a real-world benchmark dataset demonstrate the superiority of the proposed models against several state-of-the-art alternatives.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {117–126},
numpages = {10},
keywords = {cross-lingual sentiment analysis, bilingual sentiment transfer model, language discrepancy},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132909,
author = {Ma, Guixiang and He, Lifang and Lu, Chun-Ta and Shao, Weixiang and Yu, Philip S. and Leow, Alex D. and Ragin, Ann B.},
title = {Multi-View Clustering with Graph Embedding for Connectome Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132909},
doi = {10.1145/3132847.3132909},
abstract = {Multi-view clustering has become a widely studied problem in the area of unsupervised learning. It aims to integrate multiple views by taking advantages of the consensus and complimentary information from multiple views. Most of the existing works in multi-view clustering utilize the vector-based representation for features in each view. However, in many real-world applications, instances are represented by graphs, where those vector-based models cannot fully capture the structure of the graphs from each view. To solve this problem, in this paper we propose a Multi-view Clustering framework on graph instances with Graph Embedding (MCGE). Specifically, we model the multi-view graph data as tensors and apply tensor factorization to learn the multi-view graph embeddings, thereby capturing the local structure of graphs. We build an iterative framework by incorporating multi-view graph embedding into the multi-view clustering task on graph instances, jointly performing multi-view clustering and multi-view graph embedding simultaneously. The multi-view clustering results are used for refining the multi-view graph embedding, and the updated multi-view graph embedding results further improve the multi-view clustering. Extensive experiments on two real brain network datasets (i.e., HIV and Bipolar) demonstrate the superior performance of the proposed MCGE approach in multi-view connectome analysis for clinical investigation and application.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {127–136},
numpages = {10},
keywords = {connectome analysis, graph embedding, multi-view clustering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132905,
author = {Wang, Suhang and Aggarwal, Charu and Tang, Jiliang and Liu, Huan},
title = {Attributed Signed Network Embedding},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132905},
doi = {10.1145/3132847.3132905},
abstract = {The major task of network embedding is to learn low-dimensional vector representations of social-network nodes. It facilitates many analytical tasks such as link prediction and node clustering and thus has attracted increasing attention. The majority of existing embedding algorithms are designed for unsigned social networks. However, many social media networks have both positive and negative links, for which unsigned algorithms have little utility. Recent findings in signed network analysis suggest that negative links have distinct properties and added value over positive links. This brings about both challenges and opportunities for signed network embedding. In addition, user attributes, which encode properties and interests of users, provide complementary information to network structures and have the potential to improve signed network embedding. Therefore, in this paper, we study the novel problem of signed social network embedding with attributes. We propose a novel framework SNEA, which exploits the network structure and user attributes simultaneously for network representation learning. Experimental results on link prediction and node clustering with real-world datasets demonstrate the effectiveness of SNEA.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {137–146},
numpages = {10},
keywords = {network embedding, node attributes, signed social networks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132900,
author = {Lyu, Tianshu and Zhang, Yuan and Zhang, Yan},
title = {Enhancing the Network Embedding Quality with Structural Similarity},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132900},
doi = {10.1145/3132847.3132900},
abstract = {Neural network techniques are widely used in network embedding, boosting the result of node classification, link prediction, visualization and other tasks in both aspects of efficiency and quality. All the state of art algorithms put effort on the neighborhood information and try to make full use of it. However, it is hard to recognize core periphery structures simply based on neighborhood. In this paper, we first discuss the influence brought by random-walk based sampling strategies to the embedding results. Theoretical and experimental evidences show that random-walk based sampling strategies fail to fully capture structural equivalence. We present a new method, SNS, that performs network embeddings using structural information (namely graphlets) to enhance its quality. SNS effectively utilizes both neighbor information and local-subgraphs similarity to learn node embeddings. This is the first framework that combines these two aspects as far as we know, positively merging two important areas in graph mining and machine learning. Moreover, we investigate what kinds of local-subgraph features matter the most on the node classification task, which enables us to further improve the embedding quality. Experiments show that our algorithm outperforms other unsupervised and semi-supervised neural network embedding algorithms on several real-world datasets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {147–156},
numpages = {10},
keywords = {network embedding, latent representation, graphlet},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132885,
author = {Hu, Jiafeng and Cheng, Reynold and Huang, Zhipeng and Fang, Yixang and Luo, Siqiang},
title = {On Embedding Uncertain Graphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132885},
doi = {10.1145/3132847.3132885},
abstract = {Graph data are prevalent in communication networks, social media, and biological networks. These data, which are often noisy or inexact, can be represented by uncertain graphs, whose edges are associated with probabilities to indicate the chances that they exist. Recently, researchers have studied various algorithms (e.g., clustering, classification, and k-NN) for uncertain graphs. These solutions face two problems: (1) high dimensionality: uncertain graphs are often highly complex, which can affect the mining quality; and (2) low reusability, where an existing mining algorithm has to be redesigned to deal with uncertain graphs. To tackle these problems, we propose a solution called URGE, or UnceRtain Graph Embedding. Given an uncertain graph G, URGE generates G's embedding, or a set of low-dimensional vectors, which carry the proximity information of nodes in G. This embedding enables the dimensionality of G to be reduced, without destroying node proximity information. Due to its simplicity, existing mining solutions can be used on the embedding. We investigate two low- and high-order node proximity measures in the embedding generation process, and develop novel algorithms to enable fast evaluation.  To our best knowledge, there is no prior study on the use of embedding for uncertain graphs. We have further performed extensive experiments for clustering, classification, and k-NN on several uncertain graph datasets. Our results show that URGE attains better effectiveness than current uncertain data mining algorithms, as well as state-of-the-art embedding solutions. The embedding and mining performance is also highly efficient in our experiments.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {157–166},
numpages = {10},
keywords = {uncertain data mining, uncertain graph, graph embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132868,
author = {Bhamidipati, Narayan and Kant, Ravi and Mishra, Shaunak},
title = {A Large Scale Prediction Engine for App Install Clicks and Conversions},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132868},
doi = {10.1145/3132847.3132868},
abstract = {Predicting the probability of users clicking on app install ads and installing those apps comes with its own specific challenges. In this paper, we describe (a) how we built a scalable machine learning pipeline from scratch to predict the probability of users clicking and installing apps in response to ad impressions, (b) the novel features we developed to improve our model performance, (c) the training and scoring pipelines that were put into production, (d) our A/B testing process along with the metrics used to determine significant improvements, and (e) the results of our experiments. Our algorithmic improvements resulted in a 3X improvement in satisfaction for app install advertisers on our ad platform. In addition, we dive into how sequential model training, deep learning, and transfer learning resulted in a further 7% lift in conversion rate and 11% lift in revenue. Finally, we share the scientific, data-related, and product-related challenges that we encountered -- we expect others across the industry would greatly benefit from these considerations and our experiences when they kick-start similar efforts.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {167–175},
numpages = {9},
keywords = {cross feature, sequential training, neural network, app install, deep learning, transfer learning, scoring latency, feature engineering, advertiser satisfaction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133009,
author = {Su, Yu and Awadallah, Ahmed Hassan and Khabsa, Madian and Pantel, Patrick and Gamon, Michael and Encarnacion, Mark},
title = {Building Natural Language Interfaces to Web APIs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133009},
doi = {10.1145/3132847.3133009},
abstract = {As the Web evolves towards a service-oriented architecture, application program interfaces (APIs) are becoming an increasingly important way to provide access to data, services, and devices. We study the problem of natural language interface to APIs (NL2APIs), with a focus on web APIs for web services. Such NL2APIs have many potential benefits, for example, facilitating the integration of web services into virtual assistants.We propose the first end-to-end framework to build an NL2API for a given web API. A key challenge is to collect training data, i.e., NL command-API call pairs, from which an NL2API can learn the semantic mapping from ambiguous, informal NL commands to formal API calls. We propose a novel approach to collect training data for NL2API via crowdsourcing, where crowd workers are employed to generate diversified NL commands. We optimize the crowdsourcing process to further reduce the cost. More specifically, we propose a novel hierarchical probabilistic model for the crowdsourcing process, which guides us to allocate budget to those API calls that have a high value for training NL2APIs. We apply our framework to real-world APIs, and show that it can collect high-quality training data at a low cost, and build NL2APIs with good performance from scratch. We also show that our modeling of the crowdsourcing process can improve its effectiveness, such that the training data collected via our approach leads to better performance of NL2APIs than a strong baseline.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {177–186},
numpages = {10},
keywords = {web api, hierarchical probabilistic model, natural language interface, crowdsourcing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132887,
author = {El-Roby, Ahmed and Aboulnaga, Ashraf},
title = {UFeed: Refining Web Data Integration Based on User Feedback},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132887},
doi = {10.1145/3132847.3132887},
abstract = {One of the main challenges in large-scale data integration for relational schemas is creating an accurate mediated schema, and generating accurate semantic mappings between heterogeneous data sources and this mediated schema. Some applications can start with a moderately accurate mediated schema and mappings and refine them over time, which is referred to as the pay-as-you-go approach to data integration. Creating the mediated schema and mappings automatically to bootstrap the pay-as-you-go approach has been extensively studied. However, refining the mediated schema and mappings is still an open challenge because the data sources are usually heterogeneous and use diverse and sometimes ambiguous vocabularies. In this paper, we introduce UFeed, a system that refines relational mediated schemas and mappings based on user feedback over query answers. UFeed translates user actions into refinement operations that are applied to the mediated schema and mappings to improve their quality. We experimentally verify that UFeed improves the quality of query answers over real heterogeneous data sources extracted from the web.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {187–196},
numpages = {10},
keywords = {schema mapping, data integration, schema matching, holistic schema matching, user feedback, probabilistic schema matching},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132875,
author = {Velloso, Roberto Panerai and Dorneles, Carina F.},
title = {Extracting Records from the Web Using a Signal Processing Approach},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132875},
doi = {10.1145/3132847.3132875},
abstract = {Extracting records from web pages enables a number of important applications and has immense value due to the amount and diversity of available information that can be extracted. This problem, although vastly studied, remains open because it is not a trivial one. Due to the scale of data, a feasible approach must be both automatic and efficient (and of course effective). We present here a novel approach, fully automatic and computationally efficient, using signal processing techniques to detect regularities and patterns in the structure of web pages. Our approach segments the web page, detects the data regions within it, identifies the records boundaries and aligns the records. Results show high f-score and linearithmic time complexity behaviour.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {197–206},
numpages = {10},
keywords = {information retrieval, record alignment, record extraction, structure detection, web mining},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133003,
author = {Kansal, Akshay and Spezzano, Francesca},
title = {A Scalable Graph-Coarsening Based Index for Dynamic Graph Databases},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133003},
doi = {10.1145/3132847.3133003},
abstract = {A graph database D is a collection of graphs. To speed up subgraph query answering on graph databases, indexes are commonly used. State-of-the-art graph database indexes do not adapt or scale well to dynamic graph database use; they are static, and their ability to prune possible search responses to meet user needs worsens over time as databases change and grow. Users can re-mine indexes to gain some improvement, but it is time consuming. Users must also tune numerous parameters on an ongoing basis to optimize performance and can inadvertently worsen the query response time if they do not choose parameters wisely. Recently, a one-pass algorithm has been developed to enhance the performance of frequent subgraphs based indexes by using the algorithm to update them regularly. However, there are some drawbacks, most notably the need to make updates as the query workload changes.In this paper, we propose a new index based on graph-coarsening to speed up subgraph query answering time in dynamic graph databases. Our index is parameter-free, query-independent, scalable,small enough to store in the main memory, and is simpler and less costly to maintain for database updates. Experimental results show that our index outperforms hybrid-indexes (i.e. indexes updated with one-pass) for query answering time in the case of social network databases, and is comparable with these indexes for frequent and infrequent queries on chemical databases. Our index can be updated up to 60 times faster in comparison to one-pass on dynamic graph databases. Moreover, our index is independent of the query workload for index update and is up to 15 times faster after hybrid-indexes are attuned to query workload.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {207–216},
numpages = {10},
keywords = {subgraph query processing, indexing, graph coarsening, dynamic graph databases},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132977,
author = {Zheng, Weiguo and Cheng, Hong and Zou, Lei and Yu, Jeffrey Xu and Zhao, Kangfei},
title = {Natural Language Question/Answering: Let Users Talk With The Knowledge Graph},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132977},
doi = {10.1145/3132847.3132977},
abstract = {The ever-increasing knowledge graphs impose an urgent demand of providing effective and easy-to-use query techniques for end users. Structured query languages, such as SPARQL, offer a powerful expression ability to query RDF datasets. However, they are difficult to use. Keywords are simple but have a very limited expression ability. Natural language question (NLQ) is promising on querying knowledge graphs. A huge challenge is how to understand the question clearly so as to translate the unstructured question into a structured query. In this paper, we present a data + oracle approach to answer NLQs over knowledge graphs. We let users verify the ambiguities during the query understanding. To reduce the interaction cost, we formalize an interaction problem and design an efficient strategy to solve the problem. We also propose a query prefetch technique by exploiting the latency in the interactions with users. Extensive experiments over the QALD dataset demonstrate that our proposed approach is effective as it outperforms state-of-the-art methods in terms of both precision and recall.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {217–226},
numpages = {10},
keywords = {interactive query, knowledge graph, natural language question and answering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132957,
author = {Han, Shuo and Zou, Lei and Yu, Jeffery Xu and Zhao, Dongyan},
title = {Keyword Search on RDF Graphs - A Query Graph Assembly Approach},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132957},
doi = {10.1145/3132847.3132957},
abstract = {Keyword search provides ordinary users an easy-to-use interface for querying RDF data. Given the input keywords, in this paper, we study how to assemble a query graph that is to represent user's query intention accurately and efficiently. Based on the input keywords, we first obtain the elementary query graph building blocks, such as entity/class vertices and predicate edges. Then, we formally define the query graph assembly (QGA) problem. Unfortunately, we prove theoretically that QGA is a NP-complete problem. In order to solve that, we design some heuristic lower bounds and propose a bipartite graph matching-based best-first search algorithm. The algorithm's time complexity is O(k2l ... l3l), where l is the number of the keywords and k is a tunable parameter, i.e., the maximum number of candidate entity/class vertices and predicate edges allowed to match each keyword. Although QGA is intractable, both l and k are small in practice. Furthermore, the algorithm's time complexity does not depend on the RDF graph size, which guarantees the good scalability of our system in large RDF graphs. Experiments on DBpedia and Freebase confirm the superiority of our system on both effectiveness and efficiency.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {227–236},
numpages = {10},
keywords = {rdf, graph data management, keyword search},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133006,
author = {Wang, Hongjian and Li, Zhenhui},
title = {Region Representation Learning via Mobility Flow},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133006},
doi = {10.1145/3132847.3133006},
abstract = {Increasing amount of urban data are being accumulated and released to public; this enables us to study the urban dynamics and address urban issues such as crime, traffic, and quality of living. In this paper, we are interested in learning vector representations for regions using the large-scale taxi flow data. These representations could help us better measure the relationship strengths between regions, and the relationships can be used to better model the region properties. Different from existing studies, we propose to consider both temporal dynamics and multi-hop transitions in learning the region representations. We propose to jointly learn the representations from a flow graph and a spatial graph. Such a combined graph could simulate individual movements and also addresses the data sparsity issue. We demonstrate the effectiveness of our method using three different real datasets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {237–246},
numpages = {10},
keywords = {graph embedding, spatial-temporal data, mobility flow},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132943,
author = {Fan, Yixing and Guo, Jiafeng and Lan, Yanyan and Xu, Jun and Pang, Liang and Cheng, Xueqi},
title = {Learning Visual Features from Snapshots for Web Search},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132943},
doi = {10.1145/3132847.3132943},
abstract = {When applying learning to rank algorithms to Web search, a large number of features are usually designed to capture the relevance signals. Most of these features are computed based on the extracted textual elements, link analysis, and user logs. However, Web pages are not solely linked texts, but have structured layout organizing a large variety of elements in different styles. Such layout itself can convey useful visual information, indicating the relevance of a Web page. For example, the query-independent layout (i.e., raw page layout) can help identify the page quality, while the query-dependent layout (i.e., page rendered with matched query words) can further tell rich structural information (e.g., size, position and proximity) of the matching signals. However, such visual information of layout has been seldom utilized in Web search in the past. In this work, we propose to learn rich visual features automatically from the layout of Web pages (i.e., Web page snapshots) for relevance ranking. Both query-independent and query-dependent snapshots are considered as the new inputs. We then propose a novel visual perception model inspired by human's visual search behaviors on page viewing to extract the visual features. This model can be learned end-to-end together with traditional human-crafted features. We also show that such visual features can be efficiently acquired in the online setting with an extended inverted indexing scheme. Experiments on benchmark collections demonstrate that learning visual features from Web page snapshots can significantly improve the performance of relevance ranking in ad-hoc Web retrieval tasks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {247–256},
numpages = {10},
keywords = {web search, snapshot, visual feature},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132914,
author = {Pang, Liang and Lan, Yanyan and Guo, Jiafeng and Xu, Jun and Xu, Jingfang and Cheng, Xueqi},
title = {DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132914},
doi = {10.1145/3132847.3132914},
abstract = {This paper concerns a deep learning approach to relevance ranking in information retrieval (IR). Existing deep IR models such as DSSM and CDSSM directly apply neural networks to generate ranking scores, without explicit understandings of the relevance. According to the human judgement process, a relevance label is generated by the following three steps: 1) relevant locations are detected; 2) local relevances are determined; 3) local relevances are aggregated to output the relevance label. In this paper we propose a new deep learning architecture, namely DeepRank, to simulate the above human judgment process. Firstly, a detection strategy is designed to extract the relevant contexts. Then, a measure network is applied to determine the local relevances by utilizing a convolutional neural network (CNN) or two-dimensional gated recurrent units (2D-GRU). Finally, an aggregation network with sequential integration and term gating mechanism is used to produce a global relevance score. DeepRank well captures important IR characteristics, including exact/semantic matching signals, proximity heuristics, query term importance, and diverse relevance requirement. Experiments on both benchmark LETOR dataset and a large scale clickthrough data show that DeepRank can significantly outperform learning to ranking methods, and existing deep learning methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {257–266},
numpages = {10},
keywords = {information retrieval, deep learning, text matching, ranking},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133040,
author = {Biega, Asia J. and Ghazimatin, Azin and Ferhatosmanoglu, Hakan and Gummadi, Krishna P. and Weikum, Gerhard},
title = {Learning to Un-Rank: Quantifying Search Exposure for Users in Online Communities},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133040},
doi = {10.1145/3132847.3133040},
abstract = {Search engines in online communities such as Twitter or Facebook not only return matching posts, but also provide links to the profiles of the authors. Thus, when a user appears in the top-k results for a sensitive keyword query, she becomes widely exposed in a sensitive context. The effects of such exposure can result in a serious privacy violation, ranging from embarrassment all the way to becoming a victim of organizational discrimination.In this paper, we propose the first model for quantifying search exposure on the service provider side, casting it into a reverse k-nearest-neighbor problem. Moreover, since a single user can be exposed by a large number of queries, we also devise a learning-to-rank method for identifying the most critical queries and thus making the warnings user-friendly. We develop efficient algorithms, and present experiments with a large number of user profiles from Twitter that demonstrate the practical viability and effectiveness of our framework.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {267–276},
numpages = {10},
keywords = {privacy, information retrieval, social search, ranking exposure, search exposure},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132896,
author = {Oosterhuis, Harrie and de Rijke, Maarten},
title = {Balancing Speed and Quality in Online Learning to Rank for Information Retrieval},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132896},
doi = {10.1145/3132847.3132896},
abstract = {In Online Learning to Rank (OLTR) the aim is to find an optimal ranking model by interacting with users. When learning from user behavior, systems must interact with users while simultaneously learning from those interactions. Unlike other Learning to Rank (LTR) settings, existing research in this field has been limited to linear models. This is due to the speed-quality tradeoff that arises when selecting models: complex models are more expressive and can find the best rankings but need more user interactions to do so, a requirement that risks frustrating users during training. Conversely, simpler models can be optimized on fewer interactions and thus provide a better user experience, but they will converge towards suboptimal rankings. This tradeoff creates a deadlock, since novel models will not be able to improve either the user experience or the final convergence point, without sacrificing the other. Our contribution is twofold. First, we introduce a fast OLTR model called Sim-MGD that addresses the speed aspect of the speed-quality tradeoff. Sim-MGD ranks documents based on similarities with reference documents. It converges rapidly and, hence, gives a better user experience but it does not converge towards the optimal rankings. Second, we contribute Cascading Multileave Gradient De- scent (C-MGD) for OLTR that directly addresses the speed-quality tradeoff by using a cascade that enables combinations of the best of two worlds: fast learning and high quality final convergence. C-MGD can provide the better user experience of Sim-MGD while maintaining the same convergence as the state-of-the-art MGD model. This opens the door for future work to design new models for OLTR without having to deal with the speed-quality tradeoff.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {277–286},
numpages = {10},
keywords = {user behaviour, information retrieval, learning to rank, online learning to rank, reinforcement learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132910,
author = {Liu, Chang and Zhang, Yinan and Liu, Lei and Cui, Lizhen and Yuan, Dong and Miao, Chunyan},
title = {Crowd-Enabled Pareto-Optimal Objects Finding Employing Multi-Pairwise-Comparison Questions},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132910},
doi = {10.1145/3132847.3132910},
abstract = {Today, Pareto-optimal objects finding has been applied in various fields, such as group decision making and opinion collection. Many of the existing solutions to this problem require explicit attributes for objects. However, these attributes cannot be obtained sometimes. To address this issue, we propose an algorithm, which uses preference relations given by crowdsourcing, to find Pareto-optimal objects with shorter latency and lower monetary costs. It employs two multi-pairwise-comparison question models: BEST-form and BETTER-form questions. Multiple BEST (or BETTER) questions can be sent to crowds concurrently. Extensive experimental results show that the number of questions reduces greatly. In addition, the numerical results show that the latency is significantly shortened at a reasonable monetary cost, compared with the existing methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {287–295},
numpages = {9},
keywords = {crowdsourcing, pareto-optimal objects finding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132894,
author = {Zhao, Yan and Li, Yang and Wang, Yu and Su, Han and Zheng, Kai},
title = {Destination-Aware Task Assignment in Spatial Crowdsourcing},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132894},
doi = {10.1145/3132847.3132894},
abstract = {With the proliferation of GPS-enabled smart devices and increased availability of wireless network, spatial crowdsourcing (SC) has been recently proposed as a framework to automatically request workers (i.e., smart device carriers) to perform location-sensitive tasks (e.g., taking scenic photos, reporting events). In this paper we study a destination-aware task assignment problem that concerns the optimal strategy of assigning each task to proper worker such that the total number of completed tasks can be maximized whilst all workers can reach their destinations before deadlines after performing assigned tasks. Finding the global optimal assignment turns out to be an intractable problem since it does not imply optimal assignment for individual worker. Observing that the task assignment dependency only exists amongst subsets of workers, we utilize tree-decomposition technique to separate workers into independent clusters and develop an efficient depth-first search algorithm with progressive bounds to prune non-promising assignments. Our empirical studies demonstrate that our proposed technique is quite effective and settle the problem nicely.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {297–306},
numpages = {10},
keywords = {spatial crowdsourcing, user mobility, spatial task assignment},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132891,
author = {Weng, Xueping and Li, Guoliang and Hu, Huiqi and Feng, Jianhua},
title = {Crowdsourced Selection on Multi-Attribute Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132891},
doi = {10.1145/3132847.3132891},
abstract = {Crowdsourced selection asks the crowd to select entities that satisfy a query condition, e.g., selecting the photos of people wearing sunglasses from a given set of photos. Existing studies focus on a single query predicate and in this paper we study the crowdsourced selection problem on multi-attribute data, e.g., selecting the female photos with dark eyes and wearing sunglasses. A straightforward method asks the crowd to answer every entity by checking every predicate in the query. Obviously, this method involves huge monetary cost. Instead, we can select an optimized predicate order and ask the crowd to answer the entities following the order. Since if an entity does not satisfy a predicate, we can prune this entity without needing to ask other predicates and thus this method can reduce the cost. There are two challenges in finding the optimized predicate order. The first is how to detect the predicate order and the second is to capture correlation among different predicates. To address this problem, we propose predicate order based framework to reduce monetary cost. Firstly, we define an expectation tree to store selectivities on predicates and estimate the best predicate order. In each iteration, we estimate the best predicate order from the expectation tree, and then choose a predicate as a question to ask the crowd. After getting the result of the current predicate, we choose next predicate to ask until we get the result. We will update the expectation tree using the answer obtained from the crowd and continue to the next iteration. We also study the problem of answering multiple queries simultaneously, and reduce its cost using the correlation between queries. Finally, we propose a confidence based method to improve the quality. The experiment result shows that our predicate order based algorithm is effective and can reduce cost significantly compared with baseline approaches.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {307–316},
numpages = {10},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132876,
author = {Yalavarthi, Vijaya Krishna and Ke, Xiangyu and Khan, Arijit},
title = {Select Your Questions Wisely: For Entity Resolution With Crowd Errors},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132876},
doi = {10.1145/3132847.3132876},
abstract = {Crowdsourcing is becoming increasingly important in entity resolution tasks due to their inherent complexity such as clustering of images and natural language processing. Humans can provide more insightful information for these difficult problems compared to machine-based automatic techniques. Nevertheless, human workers can make mistakes due to lack of domain expertise or seriousness, ambiguity, or even due to malicious intents. The bulk of literature usually deals with human errors via majority voting or by assigning a universal error rate over crowd workers. However, such approaches are incomplete, and often inconsistent, because the expertise of crowd workers are diverse with possible biases, thereby making it largely inappropriate to assume a universal error rate for all workers over all crowdsourcing tasks. We mitigate the above challenges by considering an uncertain graph model, where the edge probability between two records A and B denotes the ratio of crowd workers who voted YES on the question if A and B are same entity. To reflect independence across different crowdsourcing tasks, we apply the notion of possible worlds, and develop parameter-free algorithms for both next crowdsourcing and entity resolution tasks. In particular, for next crowdsourcing, we identify the record pair that maximally increases the reliability of the current clustering. Since reliability takes into account the connected-ness inside and across all clusters, this metric is more effective in deciding next questions, in comparison with state-of-the-art works, which consider local features, such as individual edges, paths, or nodes to select next crowdsourcing questions. Based on detailed empirical analysis over real-world datasets, we find that our proposed solution, PERC (probabilistic entity resolution with imperfect crowd) improves the quality by 15% and reduces the overall cost by 50% for the crowdsourcing-based entity resolution.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {317–326},
numpages = {10},
keywords = {uncertain graphs, human error, reliability, crowdsourcing, entity resolution},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132979,
author = {Van Gysel, Christophe and Mitra, Bhaskar and Venanzi, Matteo and Rosemarin, Roy and Kukla, Grzegorz and Grudzien, Piotr and Cancedda, Nicola},
title = {Reply With: Proactive Recommendation of Email Attachments},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132979},
doi = {10.1145/3132847.3132979},
abstract = {Email responses often contain items---such as a file or a hyperlink to an external document---that are attached to or included inline in the body of the message. Analysis of an enterprise email corpus reveals that 35% of the time when users include these items as part of their response, the attachable item is already present in their inbox or sent folder. A modern email client can proactively retrieve relevant attachable items from the user's past emails based on the context of the current conversation, and recommend them for inclusion, to reduce the time and effort involved in composing the response. In this paper, we propose a weakly supervised learning framework for recommending attachable items to the user. As email search systems are commonly available, we constrain the recommendation task to formulating effective search queries from the context of the conversations. The query is submitted to an existing IR system to retrieve relevant items for attachment. We also present a novel strategy for generating labels from an email corpus---without the need for manual annotations---that can be used to train and evaluate the query formulation model. In addition, we describe a deep convolutional neural network that demonstrates satisfactory performance on this query formulation task when evaluated on the publicly available Avocado dataset and a proprietary dataset of internal emails obtained through an employee participation program.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {327–336},
numpages = {10},
keywords = {email overload, email attachment recommendation, proactive retrieval, neural networks, query formulation, query extraction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132911,
author = {Xiao, Lin and Min, Zhang and Yongfeng, Zhang and Yiqun, Liu and Shaoping, Ma},
title = {Learning and Transferring Social and Item Visibilities for Personalized Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132911},
doi = {10.1145/3132847.3132911},
abstract = {User feedback in the form of movie-watching history, item ratings, or product consumption is very helpful in training recommender systems. However, relatively few interactions between items and users can be observed. Instances of missing user--item entries are caused by the user not seeing the item (although the actual preference to the item could still be positive) or the user seeing the item but not liking it. Separating these two cases enables missing interactions to be modeled with finer granularity, and thus reflects user preferences more accurately. However, most previous studies on the modeling of missing instances have not fully considered the case where the user has not seen the item. Social connections are known to be helpful for modeling users' potential preferences more extensively, although a similar visibility problem exists in accurately identifying social relationships. That is, when two users are unaware of each other's existence, they have no opportunity to connect. In this paper, we propose a novel user preference model for recommender systems that considers the visibility of both items and social relationships. Furthermore, the two kinds of information are coordinated in a unified model inspired by the idea of transfer learning. Extensive experiments have been conducted on three real-world datasets in comparison with five state-of-the-art approaches. The encouraging performance of the proposed system verifies the effectiveness of social knowledge transfer and the modeling of both item and social visibilities.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {337–346},
numpages = {10},
keywords = {recommender system, implicit feedback, social network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132889,
author = {Wang, Hongwei and Wang, Jia and Zhao, Miao and Cao, Jiannong and Guo, Minyi},
title = {Joint Topic-Semantic-Aware Social Recommendation for Online Voting},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132889},
doi = {10.1145/3132847.3132889},
abstract = {Online voting is an emerging feature in social networks, in which users can express their attitudes toward various issues and show their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors in a comprehensive manner when doing voting recommendation. First, due to the fact that existing text mining methods such as topic model and semantic model cannot well process the content of votings that is typically short and ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to learn word and document representation by jointly considering their topics and semantics. Then we propose our Joint Topic-Semantic-aware social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social similarity during matrix factorization. To evaluate the performance of TEWE representation and JTS-MF model, we conduct extensive experiments on real online voting dataset. The results prove the efficacy of our approach against several state-of-the-art baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {347–356},
numpages = {10},
keywords = {recommender systems, matrix factorization, topic-enhanced word embedding, online voting},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132880,
author = {Wang, Xin and Hoi, Steven C.H. and Liu, Chenghao and Ester, Martin},
title = {Interactive Social Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132880},
doi = {10.1145/3132847.3132880},
abstract = {Social recommendation has been an active research topic over the last decade, based on the assumption that social information from friendship networks is beneficial for improving recommendation accuracy, especially when dealing with cold-start users who lack sufficient past behavior information for accurate recommendation. However, it is nontrivial to use such information, since some of a person's friends may share similar preferences in certain aspects, but others may be totally irrelevant for recommendations. Thus one challenge is to explore and exploit the extend to which a user trusts his/her friends when utilizing social information to improve recommendations. On the other hand, most existing social recommendation models are non-interactive in that their algorithmic strategies are based on batch learning methodology, which learns to train the model in an offline manner from a collection of training data which are accumulated from users? historical interactions with the recommender systems. In the real world, new users may leave the systems for the reason of being recommended with boring items before enough data is collected for training a good model, which results in an inefficient customer retention. To tackle these challenges, we propose a novel method for interactive social recommendation, which not only simultaneously explores user preferences and exploits the effectiveness of personalization in an interactive way, but also adaptively learns different weights for different friends. In addition, we also give analyses on the complexity and regret of the proposed model. Extensive experiments on three real-world datasets illustrate the improvement of our proposed method against the state-of-the-art algorithms.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {357–366},
numpages = {10},
keywords = {recommender systems, user behavior modeling, exploration-exploitation, personalization, social recommendation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132975,
author = {Yang, Dejian and Wang, Senzhang and Li, Chaozhuo and Zhang, Xiaoming and Li, Zhoujun},
title = {From Properties to Links: Deep Network Embedding on Incomplete Graphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132975},
doi = {10.1145/3132847.3132975},
abstract = {As an effective way of learning node representations in networks, network embedding has attracted increasing research interests recently. Most existing approaches use shallow models and only work on static networks by extracting local or global topology information of each node as the algorithm input. It is challenging for such approaches to learn a desirable node representation on incomplete graphs with a large number of missing links or on dynamic graphs with new nodes joining in. It is even challenging for them to deeply fuse other types of data such as node properties into the learning process to help better represent the nodes with insufficient links. In this paper, we for the first time study the problem of network embedding on incomplete networks. We propose a Multi-View Correlation-learning based Deep Network Embedding method named MVC-DNE to incorporate both the network structure and the node properties for more effectively and efficiently perform network embedding on incomplete networks. Specifically, we consider the topology structure of the network and the node properties as two correlated views. The insight is that the learned representation vector of a node should reflect its characteristics in both views. Under a multi-view correlation learning based deep autoencoder framework, the structure view and property view embeddings are integrated and mutually reinforced through both self-view and cross-view learning. As MVC-DNE can learn a representation mapping function, it can directly generate the representation vectors for the new nodes without retraining the model. Thus it is especially more efficient than previous methods. Empirically, we evaluate MVC-DNE over three real network datasets on two data mining applications, and the results demonstrate that MVC-DNE significantly outperforms state-of-the-art methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {367–376},
numpages = {10},
keywords = {network embedding, incomplete graph, deep learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132925,
author = {Cavallari, Sandro and Zheng, Vincent W. and Cai, Hongyun and Chang, Kevin Chen-Chuan and Cambria, Erik},
title = {Learning Community Embedding with Community Detection and Node Embedding on Graphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132925},
doi = {10.1145/3132847.3132925},
abstract = {In this paper, we study an important yet largely under-explored setting of graph embedding, i.e., embedding communities instead of each individual nodes. We find that community embedding is not only useful for community-level applications such as graph visualization, but also beneficial to both community detection and node classification. To learn such embedding, our insight hinges upon a closed loop among community embedding, community detection and node embedding. On the one hand, node embedding can help improve community detection, which outputs good communities for fitting better community embedding. On the other hand, community embedding can be used to optimize the node embedding by introducing a community-aware high-order proximity. Guided by this insight, we propose a novel community embedding framework that jointly solves the three tasks together. We evaluate such a framework on multiple real-world datasets, and show that it improves graph visualization and outperforms state-of-the-art baselines in various application tasks, e.g., community detection and node classification.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {377–386},
numpages = {10},
keywords = {graph embedding, community embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132919,
author = {Li, Jundong and Dani, Harsh and Hu, Xia and Tang, Jiliang and Chang, Yi and Liu, Huan},
title = {Attributed Network Embedding for Learning in a Dynamic Environment},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132919},
doi = {10.1145/3132847.3132919},
abstract = {Network embedding leverages the node proximity manifested to learn a low-dimensional node vector representation for each node in the network. The learned embeddings could advance various learning tasks such as node classification, network clustering, and link prediction. Most, if not all, of the existing works, are overwhelmingly performed in the context of plain and static networks. Nonetheless, in reality, network structure often evolves over time with addition/deletion of links and nodes. Also, a vast majority of real-world networks are associated with a rich set of node attributes, and their attribute values are also naturally changing, with the emerging of new content patterns and the fading of old content patterns. These changing characteristics motivate us to seek an effective embedding representation to capture network and attribute evolving patterns, which is of fundamental importance for learning in a dynamic environment. To our best knowledge, we are the first to tackle this problem with the following two challenges: (1) the inherently correlated network and node attributes could be noisy and incomplete, it necessitates a robust consensus representation to capture their individual properties and correlations; (2) the embedding learning needs to be performed in an online fashion to adapt to the changes accordingly. In this paper, we tackle this problem by proposing a novel dynamic attributed network embedding framework - DANE. In particular, DANE first provides an offline method for a consensus embedding and then leverages matrix perturbation theory to maintain the freshness of the end embedding results in an online manner. We perform extensive experiments on both synthetic and real attributed networks to corroborate the effectiveness and efficiency of the proposed framework.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {387–396},
numpages = {10},
keywords = {network embedding, dynamic networks, attributed networks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132918,
author = {Zhang, Yao and Xiong, Yun and Kong, Xiangnan and Zhu, Yangyong},
title = {Learning Node Embeddings in Interaction Graphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132918},
doi = {10.1145/3132847.3132918},
abstract = {Node embedding techniques have gained prominence since they produce continuous and low-dimensional features, which are effective for various tasks. Most existing approaches learn node embeddings by exploring the structure of networks and are mainly focused on static non-attributed graphs. However, many real-world applications, such as stock markets and public review websites, involve bipartite graphs with dynamic and attributed edges, called attributed interaction graphs. Different from conventional graph data, attributed interaction graphs involve two kinds of entities (e.g. investors/stocks and users/businesses) and edges of temporal interactions with attributes (e.g. transactions and reviews). In this paper, we study the problem of node embedding in attributed interaction graphs. Learning embeddings in interaction graphs is highly challenging due to the dynamics and heterogeneous attributes of edges. Different from conventional static graphs, in attributed interaction graphs, each edge can have totally different meanings when the interaction is at different times or associated with different attributes. We propose a deep node embedding method called IGE (Interaction Graph Embedding). IGE is composed of three neural networks: an encoding network is proposed to transform attributes into a fixed-length vector to deal with the heterogeneity of attributes; then encoded attribute vectors interact with nodes multiplicatively in two coupled prediction networks that investigate the temporal dependency by treating incident edges of a node as the analogy of a sentence in word embedding methods. The encoding network can be specifically designed for different datasets as long as it is differentiable, in which case it can be trained together with prediction networks by back-propagation. We evaluate our proposed method and various comparing methods on four real-world datasets. The experimental results prove the effectiveness of the learned embeddings by IGE on both node clustering and classification tasks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {397–406},
numpages = {10},
keywords = {graph mining, attributed interaction graph, representation learning, node embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133012,
author = {Rahman, Md Farhadur and Asudeh, Abolfazl and Koudas, Nick and Das, Gautam},
title = {Efficient Computation of Subspace Skyline over Categorical Domains},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133012},
doi = {10.1145/3132847.3133012},
abstract = {Platforms such as AirBnB, Zillow, Yelp, and related sites have transformed the way we search for accommodation, restaurants, etc. The underlying datasets in such applications have numerous attributes that are mostly Boolean or Categorical. Discovering the skyline of such datasets over a subset of attributes would identify entries that stand out while enabling numerous applications. There are only a few algorithms designed to compute the skyline over categorical attributes, yet are applicable only when the number of attributes is small. In this paper, we place the problem of skyline discovery over categorical attributes into perspective and design efficient algorithms for two cases. (i) In the absence of indices, we propose two algorithms, ST-S and ST-P, that exploit the categorical characteristics of the datasets, organizing tuples in a tree data structure, supporting efficient dominance tests over the candidate set. (ii) We then consider the existence of widely used precomputed sorted lists. After discussing several approaches, and studying their limitations, we propose TA-SKY, a novel threshold style algorithm that utilizes sorted lists. Moreover, we further optimize TA-SKY and explore its progressive nature, making it suitable for applications with strict interactive requirements. In addition to the extensive theoretical analysis of the proposed algorithms, we conduct a comprehensive experimental evaluation of the combination of real (including the entire AirBnB data collection) and synthetic datasets to study the practicality of the proposed algorithms. The results showcase the superior performance of our techniques, outperforming applicable approaches by orders of magnitude.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {407–416},
numpages = {10},
keywords = {subspace skyline computation, tree, categorical domains, sorted list},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132950,
author = {Yu, Wenhui and Qin, Zheng and Liu, Jinfei and Xiong, Li and Chen, Xu and Zhang, Huidi},
title = {Fast Algorithms for Pareto Optimal Group-Based Skyline},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132950},
doi = {10.1145/3132847.3132950},
abstract = {Skyline, aiming at finding a Pareto optimal subset of points in a multi-dimensional dataset, has gained great interest due to its extensive use for multi-criteria analysis and decision making. Skyline consists of all points that are not dominated by, or not worse than other points. It is a candidate set of optimal solution, which depends on a specific evaluation criterion for optimum. However, conventional skyline queries, which return individual points, are inadequate in group querying case since optimal combinations are required. To address this gap, we study the skyline computation in group case and propose fast methods to find the group-based skyline (G-skyline), which contains Pareto optimal groups. For computing the front k skyline layers, we lay out an efficient approach that does the search concurrently on each dimension and investigates each point in subspace. After that, we present a novel structure to construct the G-skyline with a queue of combinations of the first-layer points. Experimental results show that our algorithms are several orders of magnitude faster than the previous work.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {417–426},
numpages = {10},
keywords = {group skyline, concurrent search, subspace skyline, multiple skyline layers, combination queue},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132930,
author = {Zhang, Kaiqi and Gao, Hong and Han, Xixian and Cai, Zhipeng and Li, Jianzhong},
title = {Probabilistic Skyline on Incomplete Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132930},
doi = {10.1145/3132847.3132930},
abstract = {The skyline query is important in database community. In recent years, the researches on incomplete data have been increasingly considered, especially for the skyline query. However, the existing skyline definition on incomplete data cannot provide users with valuable references. In this paper, we propose a novel skyline definition utilizing probabilistic model on incomplete data where each point has a probability to be in the skyline. In particular, it returnsK points with the highest skyline probabilities. Meanwhile, it is a big challenge to compute probabilistic skyline on incomplete data. We propose an efficient algorithm PISkyline, which utilizes two pruning strategies to reduce the number of points and adopts two optimizations to accelerate probability computation for each point. Nevertheless, PISkyline is susceptible to the order of input data and there is still a great deal of room for optimization. We develop a point-level sorting technique by adjusting the order of accessing points to further improve the efficiency of PISkyline. Our experimental results demonstrate that our algorithms are tens of times faster than the naive algorithm on both synthetic and real datasets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {427–436},
numpages = {10},
keywords = {probabilistic skyline, incomplete data, query processing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132927,
author = {Zhang, Haoyu and Zhang, Qin},
title = {Communication-Efficient Distributed Skyline Computation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132927},
doi = {10.1145/3132847.3132927},
abstract = {In this paper we study skyline queries in the distributed computational model, where we have s remote sites and a central coordinator; each site holds a piece of data, and the coordinator wants to compute the skyline of the union of the s datasets. The computation is in terms of rounds, and the goal is to minimize both the total communication cost and the round cost.  We first give an algorithm with a small communication cost but potentially a large round cost; we show information-theoretically that the communication cost is optimal even if we allow an infinite number of communication rounds. We next give algorithms with smooth communication-round tradeoffs. We also show a strong lower bound for the communication cost if we can only use one round of communication. Finally, we demonstrate the superiority of our algorithms over existing ones by an extensive set of experiments on both synthetic and real world datasets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {437–446},
numpages = {10},
keywords = {communication-efficient algorithms, skyline computation, distributed computation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132863,
author = {Kenthapadi, Krishnaram and Ambler, Stuart and Zhang, Liang and Agarwal, Deepak},
title = {Bringing Salary Transparency to the World: Computing Robust Compensation Insights via LinkedIn Salary},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132863},
doi = {10.1145/3132847.3132863},
abstract = {The recently launched LinkedIn Salary product has been designed with the goal of providing compensation insights to the world's professionals and thereby helping them optimize their earning potential. We describe the overall design and architecture of the statistical modeling system underlying this product. We focus on the unique data mining challenges while designing and implementing the system, and describe the modeling components such as Bayesian hierarchical smoothing that help to compute and present robust compensation insights to users. We report on extensive evaluation with nearly one year of de-identified compensation data collected from over one million LinkedIn users, thereby demonstrating the efficacy of the statistical models. We also highlight the lessons learned through the deployment of our system at LinkedIn.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {447–455},
numpages = {9},
keywords = {outlier detection, privacy-preserving statistical modeling, bayesian hierarchical smoothing, robust compensation insights, linkedin salary},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133016,
author = {Proskurnia, Julia and Mavlyutov, Ruslan and Castillo, Carlos and Aberer, Karl and Cudr\'{e}-Mauroux, Philippe},
title = {Efficient Document Filtering Using Vector Space Topic Expansion and Pattern-Mining: The Case of Event Detection in Microposts},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133016},
doi = {10.1145/3132847.3133016},
abstract = {Automatically extracting information from social media is challenging given that social content is often noisy, ambiguous, and inconsistent. However, as many stories break on social channels first before being picked up by mainstream media, developing methods to better handle social content is of utmost importance. In this paper, we propose a robust and effective approach to automatically identify microposts related to a specific topic defined by a small sample of reference documents. Our framework extracts clusters of semantically similar microposts that overlap with the reference documents, by extracting combinations of key features that define those clusters through frequent pattern mining. This allows us to construct compact and interpretable representations of the topic, dramatically decreasing the computational burden compared to classical clustering and k-NN-based machine learning techniques and producing highly-competitive results even with small training sets (less than 1'000 training objects). Our method is efficient and scales gracefully with large sets of incoming microposts. We experimentally validate our approach on a large corpus of over 60M microposts, showing that it significantly outperforms state-of-the-art techniques.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {457–466},
numpages = {10},
keywords = {microposts, frequent patterns mining, semantic attributes, event detection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132997,
author = {Ma, Changsha and Yan, Zhisheng and Chen, Chang Wen},
title = {LARM: A Lifetime Aware Regression Model for Predicting YouTube Video Popularity},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132997},
doi = {10.1145/3132847.3132997},
abstract = {Online content popularity prediction provides substantial value to a broad range of applications in the end-to-end social media systems, from network resource allocation to targeted advertising. While using historical popularity can predict the near-term popularity with a reasonable accuracy, the bursty nature of online content popularity evolution makes it difficult to capture the correlation between historical data and future data in the long term. Although various existing efforts have been made toward long-term prediction, they need to accumulate a long enough historical data before the prediction and their model assumptions cannot be applied to the complex YouTube networks with inherent unpredictability.In this paper, we aim to achieve fast prediction of long-term video popularity in the complex YouTube networks. We propose LARM, a lifetime aware regression model, representing the first work that leverages content lifetime to compensate the insufficiency of historical data without assumptions of network structure. The proposed LARM is empowered by a lifetime metric that is both predictable via early-accessible features and adaptable to different observation intervals, as well as a set of specialized regression models to handle different classes of videos with different lifetime. We validate LARM on two YouTube data sets with hourly and daily observation intervals. Experimental results indicate that LARM outperforms several non-trivial baselines from the literature by up to 20% and 18% of prediction error reduction in the two data sets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {467–476},
numpages = {10},
keywords = {youtube, social media, popularity prediction, time series analysis, regression model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132923,
author = {Kim, Minkyoung and McFarland, Daniel A. and Leskovec, Jure},
title = {Modeling Affinity Based Popularity Dynamics},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132923},
doi = {10.1145/3132847.3132923},
abstract = {Information items draw collective attention across a heterogeneous social system, leading to great disparities of popularity. Unveiling underlying diffusion processes is very challenging, since a social system consists of time-evolving subgroups interacting and exerting disproportionate influences on an individual item's popularity. In this study, we propose the Affinity Poisson Process model (APP) which models popularity dynamics, by incorporating (1) affinities between subgroups, (2) heterogeneous preferential attachment, and (3) subgroup-level time decay. As a case study, we apply our proposed model to scholarly publications in computer science. Our model outperforms the state of the art approach in predicting citation volumes of individual papers. More importantly, the proposed model enables us to uncover popularity dynamics driven by intra- and inter-subgroup interactions, which has been neglected in prior work. We expect that our model can afford interpretable insights on the attention economy in terms of affinity and aging effect.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {477–486},
numpages = {10},
keywords = {affinity, popularity dynamics, interdisciplinary citations, poisson process, diffusion process},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132874,
author = {Lu, Ying and Josse, Gregor and Emrich, Tobias and Demiryurek, Ugur and Renz, Matthias and Shahabi, Cyrus and Schubert, Matthias},
title = {Scenic Routes Now: Efficiently Solving the Time-Dependent Arc Orienteering Problem},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132874},
doi = {10.1145/3132847.3132874},
abstract = {Due to the availability of large transportation (e.g., road network sensor data) and transportation-related (e.g., pollution, crime) data as well as the ubiquity of car navigation systems, recent route planning techniques need to optimize for multiple criteria (e.g., travel time or distance, utility/value such as safety or attractiveness). In this paper, we introduce a novel problem called Twofold Time-Dependent Arc Orienteering Problem (2TD-AOP), which seeks to find a path from a source to a destination maximizing an accumulated value (e.g., attractiveness of the path) while not exceeding a cost budget (e.g., total travel time). 2TD-AOP has many applications in spatial crowdsourcing, real-time delivery, and online navigation systems (e.g., safest path, most scenic path). Although 2TD-AOP can be framed as a variant of AOP, existing AOP approaches cannot solve 2TD-AOP accurately as they assume that travel-times and values of network edges are constant. However, in real-world the travel-times and values are time-dependent, where the actual travel time and utility of an edge depend on the arrival time to the edge. We first discuss the practicality of this novel problem by demonstrating the benefits of considering time-dependency, empirically. Subsequently, we show that optimal solutions are infeasible (NP-hard) and solutions to the static problem are often invalid (i.e., exceed the cost budget). Therefore, we propose an efficient approximate solution with spatial pruning techniques, optimized for fast response systems. Experiments on a large-scale, fine-grained, real-world road network demonstrate that our approach always produces valid paths, is orders of magnitude faster than any optimal solution with acceptable accumulated value.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {487–496},
numpages = {10},
keywords = {arc orienteering problem, time dependent, scenic path, road network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133024,
author = {Zhao, Xiangyu and Tang, Jiliang},
title = {Modeling Temporal-Spatial Correlations for Crime Prediction},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133024},
doi = {10.1145/3132847.3133024},
abstract = {Crime prediction plays a crucial role in improving public security and reducing the financial loss of crimes. The vast majority of traditional algorithms performed the prediction by leveraging demographic data, which could fail to capture the dynamics of crimes in urban. In the era of big data, we have witnessed advanced ways to collect and integrate fine-grained urban, mobile, and public service data that contains various crime-related sources and rich temporal-spatial information. Such information provides better understandings about the dynamics of crimes and has potentials to advance crime prediction. In this paper, we exploit temporal-spatial correlations in urban data for crime prediction. In particular, we validate the existence of temporal-spatial correlations in crime and develop a principled approach to model these correlations into the coherent framework TCP for crime prediction. The experimental results on real-world data demonstrate the effectiveness of the proposed framework. Further experiments have been conducted to understand the importance of temporal-spatial correlations in crime prediction.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {497–506},
numpages = {10},
keywords = {crime prediction, crime prevention, temporal-spatial correlation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132996,
author = {Zhang, Xuchao and Zhao, Liang and Boedihardjo, Arnold P. and Lu, Chang-Tien and Ramakrishnan, Naren},
title = {Spatiotemporal Event Forecasting from Incomplete Hyper-Local Price Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132996},
doi = {10.1145/3132847.3132996},
abstract = {Hyper-local pricing data, e.g., about foods and commodities, exhibit subtle spatiotemporal variations that can be useful as crucial precursors of future events. Three major challenges in modeling such pricing data include: i) temporal dependencies underlying features; ii) spatiotemporal missing values; and iii) constraints underlying economic phenomena. These challenges hinder traditional event forecasting models from being applied effectively. This paper proposes a novel spatiotemporal event forecasting model that concurrently addresses the above challenges. Specifically, given continuous price data, a new soft time-lagged model is designed to select temporally dependent features. To handle missing values, we propose a data tensor completion method based on price domain knowledge. The parameters of the new model are optimized using a novel algorithm based on the Alternative Direction Methods of Multipliers (ADMM). Extensive experimental evaluations on multiple datasets demonstrate the effectiveness of our proposed approach.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {507–516},
numpages = {10},
keywords = {optimization, hyper-local price, spatiotemporal data mining, event forecasting},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132898,
author = {Chen, Wei and Yin, Hongzhi and Wang, Weiqing and Zhao, Lei and Hua, Wen and Zhou, Xiaofang},
title = {Exploiting Spatio-Temporal User Behaviors for User Linkage},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132898},
doi = {10.1145/3132847.3132898},
abstract = {Cross-device and cross-domain user linkage have been attracting a lot of attention recently. An important branch of the study is to achieve user linkage with spatio-temporal data generated by the ubiquitous GPS-enabled devices. The main task in this problem is twofold, i.e., how to extract the representative features of a user; how to measure the similarities between users with the extracted features. To tackle the problem, we propose a novel model STUL (Spatio-Temporal User Linkage) that consists of the following two components. 1) Extract users - spatial features with a density based clustering method, and extract the users - temporal features with the Gaussian Mixture Model. To link user pairs more precisely, we assign different weights to the extracted features, by lightening the common features and highlighting the discriminative features. 2) Propose novel approaches to measure the similarities between users based on the extracted features, and return the pair-wise users with similarity scores higher than a predefined threshold. We have conducted extensive experiments on three real-world datasets, and the results demonstrate the superiority of our proposed STUL over the state-of-the-art methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {517–526},
numpages = {10},
keywords = {cross-domain, user linkage, spatio-temporal behaviors},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133032,
author = {Jiang, Jiepu and Allan, James},
title = {Similarity-Based Distant Supervision for Definition Retrieval},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133032},
doi = {10.1145/3132847.3133032},
abstract = {Recognizing definition sentences from free text corpora often requires hand-crafted patterns or explicitly labeled training instances. We present a distant supervision approach addressing this challenge without using explicitly labeled data. We use plausibly good but imperfect definition sentences from Wikipedia as references to annotate sentences in a target corpus based on text similarity measures such as ROUGE. Experimental results show our approach is highly effective, generating noisy but large, useful, and localized training instances. Definition sentence retrieval models trained using the synthesized training examples are more effective than those learned from manual judgments of a few thousand sentences. We also examine different text similarity measures for annotation, including both unsupervised and supervised ones. We show that our method can significantly benefit from supervised text similarity measures learned from either external training data (from the SemEval Semantic Text Similarity task) or local ones (a few hundred judged sentences on the target corpus). Our method offers a cheap, effective, and flexible solution to this task and can benefit a broad range of applications such as web search engines and QA systems.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {527–536},
numpages = {10},
keywords = {definitional question answering, semantic textual similarity, definition sentence retrieval, distant supervision},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132861,
author = {Khurana, Prerna and Agarwal, Puneet and Shroff, Gautam and Vig, Lovekesh and Srinivasan, Ashwin},
title = {Hybrid BiLSTM-Siamese Network for FAQ Assistance},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132861},
doi = {10.1145/3132847.3132861},
abstract = {We describe an automated assistant for answering frequently asked questions; our system has been deployed, and is currently answering HR-related queries in two different areas (leave management and health insurance) to a large number of users. The needs of a large global corporate lead us to model a frequently asked question (FAQ) to be an equivalence class of actually asked questions, for which there is a common answer (certified as being consistent with the organization's policy). When a new question is posed to our system, it finds the class of question, and responds with the answer for the class. At this point, the system is either correct (gives correct answer); or incorrect (gives wrong answer); or incomplete (says "I don't know''). We employ a hybrid deep-learning architecture in which a BiLSTM-based classifier is combined with second BiLSTM-based Siamese network in an iterative manner: Questions for which the classifier makes an error during training are used to generate a set of misclassified question-question pairs. These, along with correct pairs, are used to train the Siamese network to drive apart the (hidden) representations of the misclassified pairs. We present experimental results from our deployment showing that our iteratively trained hybrid network: (a) results in better performance than using just a classifier network, or just a Siamese network; (b) performs better than state-of-the art sentence classifiers in the two areas in which it has been deployed, in terms of both accuracy as well as precision-recall tradeoff; and (c) also performs well on a benchmark public dataset. We also observe that using question-question pairs in our hybrid network, results in marginally better performance than using question-to-answer pairs. Finally, estimates of precision and recall from the deployment of our automated assistant suggest that we can expect the burden on our HR department to drop from answering about 6000 queries a day to about 1000.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {537–545},
numpages = {9},
keywords = {bilstm, deep learning, chatbot, siamese network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133011,
author = {Saha, Tanay Kumar and Joty, Shafiq and Hassan, Naeemul and Hasan, Mohammad Al},
title = {Regularized and Retrofitted Models for Learning Sentence Representation with Context},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133011},
doi = {10.1145/3132847.3133011},
abstract = {Vector representation of sentences is important for many text processing tasks that involve classifying, clustering, or ranking sentences. For solving these tasks, bag-of-word based representation has been used for a long time. In recent years, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform traditional bag-of-words representations. However, most existing methods belonging to the neural models consider only the content of a sentence, and disregard its relations with other sentences in the context. In this paper, we first characterize two types of contexts depending on their scope and utility. We then propose two approaches to incorporate contextual information into content-based models. We evaluate our sentence representation models in a setup, where context is available to infer sentence vectors. Experimental results demonstrate that our proposed models outshine existing models on three fundamental tasks, such as, classifying, clustering, and ranking sentences.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {547–556},
numpages = {10},
keywords = {retrofitting, distributed representation of sentences, feature learning, sen2vec, ranking, clustering, classification, discourse},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132893,
author = {Rao, Jinfeng and Ture, Ferhan and He, Hua and Jojic, Oliver and Lin, Jimmy},
title = {Talking to Your TV: Context-Aware Voice Search with Hierarchical Recurrent Neural Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132893},
doi = {10.1145/3132847.3132893},
abstract = {We tackle the novel problem of navigational voice queries posed against an entertainment system, where viewers interact with a voice-enabled remote controller to specify the TV program to watch. This is a difficult problem for several reasons: such queries are short, even shorter than comparable voice queries in other domains, which offers fewer opportunities for deciphering user intent. Furthermore, ambiguity is exacerbated by underlying speech recognition errors. We address these challenges by integrating word- and character-level query representations and by modeling voice search sessions to capture the contextual dependencies in query sequences. Both are accomplished with a probabilistic framework in which recurrent and feedforward neural network modules are organized in a hierarchical manner. From a raw dataset of 32M voice queries from 2.5M viewers on the Comcast Xfinity X1 entertainment system, we extracted data to train and test our models. We demonstrate the benefits of our hybrid representation and context-aware model, which significantly outperforms competitive baselines that use learning to rank as well as neural networks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {557–566},
numpages = {10},
keywords = {lstm, navigational voice queries, voice search sessions, context modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132960,
author = {Kozawa, Yusuke and Amagasa, Toshiyuki and Kitagawa, Hiroyuki},
title = {GPU-Accelerated Graph Clustering via Parallel Label Propagation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132960},
doi = {10.1145/3132847.3132960},
abstract = {Graph clustering has recently attracted much attention as a technique to extract community structures from various kinds of graph data. Since available graph data becomes increasingly large, the acceleration of graph clustering is an important issue for handling large-scale graphs. To this end, this paper proposes a fast graph clustering method using GPUs. The proposed method is based on parallelization of label propagation, one of the fastest graph clustering algorithms. Our method has the following three characteristics: (1) efficient parallelization: the algorithm of label propagation is transformed into a sequence of data-parallel primitives; (2) load balance: the method takes into account load balancing by adopting the primitives that make the load among threads and blocks well balanced; and (3) out-of-core processing: we also develop algorithms to efficiently deal with large-scale datasets that do not fit into GPU memory. Moreover, this GPU out-of-core algorithm is extended to simultaneously exploit both CPUs and GPUs for further performance gain. Extensive experiments with real-world and synthetic datasets show that our proposed method outperforms an existing parallel CPU implementation by a factor of up to 14.3 without sacrificing accuracy.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {567–576},
numpages = {10},
keywords = {label propagation, gpu, graph clustering, community detection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132955,
author = {Fani, Hossein and Bagheri, Ebrahim and Du, Weichang},
title = {Temporally Like-Minded User Community Identification through Neural Embeddings},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132955},
doi = {10.1145/3132847.3132955},
abstract = {We propose a neural embedding approach to identify temporally like-minded user communities, i.e., those communities of users who have similar temporal alignment in their topics of interest. Like-minded user communities in social networks are usually identified by either considering explicit structural connections between users (link analysis), users' topics of interest expressed in their posted contents (content analysis), or in tandem. In such communities, however, the users' rich temporal behavior towards topics of interest is overlooked. Only few recent research efforts consider the time dimension and define like-minded user communities as groups of users who share not only similar topical interests but also similar temporal behavior. Temporal like-minded user communities find application in areas such as recommender systems where relevant items are recommended to the users at the right time. In this paper, we tackle the problem of identifying temporally like-minded user communities by leveraging unsupervised feature learning (embeddings). Specifically, we learn a mapping from the user space to a low-dimensional vector space of features that incorporate both topics of interest and their temporal nature. We demonstrate the efficacy of our proposed approach on a Twitter dataset in the context of three applications: news recommendation, user prediction and community selection, where our work is able to outperform the state-of-the-art on important information retrieval metrics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {577–586},
numpages = {10},
keywords = {community detection, neural embedding, social network analysis},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132904,
author = {Chen, Zheng and Yu, Xinli and Song, Bo and Gao, Jianliang and Hu, Xiaohua and Yang, Wei-Shih},
title = {Community-Based Network Alignment for Large Attributed Network},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132904},
doi = {10.1145/3132847.3132904},
abstract = {Network alignment is becoming an active topic in network data analysis. Despite extensive research, we realize that efficient use of topological and attribute information for large attributed network alignment has not been sufficiently addressed in previous studies. In this paper, based on Stochastic Block Model (SBM) and Dirichlet-multinomial, we propose "divide-and-conquer" models CAlign that jointly consider network alignment, community discovery and community alignment in one framework for large networks with node attributes, in an effort to reduce both the computation time and memory usage while achieving better or competitive performance. It is provable that the algorithms derived from our model have sub-quadratic time complexity and linear space complexity on a network with small densification power, which is true for most real-world networks. Experiments show CAlign is superior to two recent state-of-art models in terms of accuracy, time and memory on large networks, and CAlign is capable of handling millions of nodes on a modern desktop machine.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {587–596},
numpages = {10},
keywords = {Community Discovery, Large Network, Dirichilet-Mutinomial, Attributed Network Alignment, Stochastic Block Model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132902,
author = {Sun, Bing-Jie and Shen, Huawei and Gao, Jinhua and Ouyang, Wentao and Cheng, Xueqi},
title = {A Non-Negative Symmetric Encoder-Decoder Approach for Community Detection},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132902},
doi = {10.1145/3132847.3132902},
abstract = {Community detection or graph clustering is crucial to understanding the structure of complex networks and extracting relevant knowledge from networked data. Latent factor model, e.g., non-negative matrix factorization and mixed membership block model, is one of the most successful methods for community detection. Latent factor models for community detection aim to find a distributed and generally low-dimensional representation, or coding, that captures the structural regularity of network and reflects the community membership of nodes. Existing latent factor models are mainly based on reconstructing a network from the representation of its nodes, namely network decoder, while constraining the representation to have certain desirable properties. These methods, however, lack an encoder that transforms nodes into their representation. Consequently, they fail to give a clear explanation about the meaning of a community and suffer from undesired computational problems. In this paper, we propose a non-negative symmetric encoder-decoder approach for community detection. By explicitly integrating a decoder and an encoder into a unified loss function, the proposed approach achieves better performance over state-of-the-art latent factor models for community detection task. Moreover, different from existing methods that explicitly impose the sparsity constraint on the representation of nodes, the proposed approach implicitly achieves the sparsity of node representation through its symmetric and non-negative properties, making the optimization much easier than competing methods based on sparse matrix factorization.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {597–606},
numpages = {10},
keywords = {encoder-decoder, community detection, latent factor model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133028,
author = {Cristo, Marco and Hanada, Ra\'{\i}za and Carvalho, Andr\'{e} and Lores, Fernando Anglada and Pimentel, Maria da Gra\c{c}a C.},
title = {Fast Word Recognition for Noise Channel-Based Models in Scenarios with Noise Specific Domain Knowledge},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133028},
doi = {10.1145/3132847.3133028},
abstract = {Word recognition is a challenging task faced by many applications, specially in very noisy scenarios. This problem is usually seen as the transmission of a word through a noisy-channel, such that it is necessary to determine which known word of a lexicon is the received string. To be feasible, just a reduced set of candidate words are selected. They are usually chosen if they can be transformed into the input string by applying up to k character edit operations. To rank the candidates, the most effective estimates use domain knowledge about noise sources and error distributions, extracted from real use data. In scenarios with much noise, however, such estimates, and the index strategies normally required, do not scale well as they grow exponentially with k and the lexicon size. In this work, we propose very efficient methods for word recognition in very noisy scenarios which support effective edit-based distance algorithms in a Mor-Fraenkel index, searchable using a minimum perfect hashing. The method allows the early processing of most promising candidates, such that fast pruned searches present negligible loss in word ranking quality. We also propose a linear heuristic for estimating edit-based distances which take advantage of information already provided by the index. Our methods achieve precision similar to a state-of-the-art approach, being about ten times faster.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {607–616},
numpages = {10},
keywords = {noise channel model, eye-based typing, approximate searching, word recognition},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133027,
author = {Yuan, Quan and Shang, Jingbo and Cao, Xin and Zhang, Chao and Geng, Xinhe and Han, Jiawei},
title = {Detecting Multiple Periods and Periodic Patterns in Event Time Sequences},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133027},
doi = {10.1145/3132847.3133027},
abstract = {Periodicity is prevalent in physical world, and many events involve more than one periods, eg individual's mobility, tide pattern, and massive transportation utilization. Knowing the true periods of events can benefit a number of applications, such as traffic prediction, time-aware recommendation and advertisement, and anomaly detection. However, detecting multiple periods is a very challenging task due to not only the interwoven periodic patterns but also the low quality of event tracking records. In this paper, we study the problem of discovering all true periods and the corresponded occurring patterns of an event from a noisy and incomplete observation sequence. We devise a novel scoring function, by maximizing which we can identify the true periodic patterns involved in the sequence. We prove that, however, optimizing the objective function is an NP-hard problem. To address this challenge, we develop a heuristic algorithm named Timeslot Coverage Model (TiCom), for identifying the periods and periodic patterns approximately. The results of extensive experiments on both synthetic and real-life datasets show that our model outperforms the state-of-the-art baselines significantly in various tasks, including period detection, periodic pattern identification, and anomaly detection.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {617–626},
numpages = {10},
keywords = {periodicity detection, anomaly detection, sequence mining, np hard},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132981,
author = {Ghosh, Abhirup and Lucas, Christopher and Sarkar, Rik},
title = {Finding Periodic Discrete Events in Noisy Streams},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132981},
doi = {10.1145/3132847.3132981},
abstract = {Periodic phenomena are ubiquitous, but detecting and predicting periodic events can be difficult in noisy environments. We describe a model of periodic events that covers both idealized and realistic scenarios characterized by multiple kinds of noise. The model incorporates false-positive events and the possibility that the underlying period and phase of the events change over time. We then describe a particle filter that can efficiently and accurately estimate the parameters of the process generating periodic events intermingled with independent noise events. The system has a small memory footprint, and, unlike alternative methods, its computational complexity is constant in the number of events that have been observed. As a result, it can be applied in low-resource settings that require real-time performance over long periods of time. In experiments on real and simulated data we find that it outperforms existing methods in accuracy and can track changes in periodicity and other characteristics in dynamic event streams.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {627–636},
numpages = {10},
keywords = {temporal sequence mining, periodicity, particle filter},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132980,
author = {Sch\"{a}fer, Patrick and Leser, Ulf},
title = {Fast and Accurate Time Series Classification with WEASEL},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132980},
doi = {10.1145/3132847.3132980},
abstract = {Time series (TS) occur in many scientific and commercial applications, ranging from earth surveillance to industry automation to the smart grids. An important type of TS analysis is classification, which can, for instance, improve energy load forecasting in smart grids by detecting the types of electronic devices based on their energy consumption profiles recorded by automatic sensors. Such sensor-driven applications are very often characterized by (a) very long TS and (b) very large TS datasets needing classification. However, current methods to time series classification (TSC) cannot cope with such data volumes at acceptable accuracy; they are either scalable but offer only inferior classification quality, or they achieve state-of-the-art classification quality but cannot scale to large data volumes. In this paper, we present WEASEL (Word ExtrAction for time SEries cLassification), a novel TSC method which is both fast and accurate. Like other state-of-the-art TSC methods, WEASEL transforms time series into feature vectors, using a sliding-window approach, which are then analyzed through a machine learning classifier. The novelty of WEASEL lies in its specific method for deriving features, resulting in a much smaller yet much more discriminative feature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more accurate than the best current non-ensemble algorithms at orders-of-magnitude lower classification and training times, and it is almost as accurate as ensemble classifiers, whose computational complexity makes them inapplicable even for mid-size datasets. The outstanding robustness of WEASEL is also confirmed by experiments on two real smart grid datasets, where it out-of-the-box achieves almost the same accuracy as highly tuned, domain-specific methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {637–646},
numpages = {10},
keywords = {classification, feature selection, bag-of-patterns, time series, word co-occurrences},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132921,
author = {Bast, Hannah and Buchhold, Bj\"{o}rn},
title = {QLever: A Query Engine for Efficient SPARQL+Text Search},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132921},
doi = {10.1145/3132847.3132921},
abstract = {We present QLever, a query engine for efficient combined search on a knowledge base and a text corpus, in which named entities from the knowledge base have been identified (that is, recognized and disambiguated). The query language is SPARQL extended by two QLever-specific predicates ql:contains-entity and ql:contains-word, which can express the occurrence of an entity or word (the object of the predicate) in a text record (the subject of the predicate). We evaluate QLever on two large datasets, including FACC (the ClueWeb12 corpus linked to Freebase). We compare against three state-of-the-art query engines for knowledge bases with varying support for text search: RDF-3X, Virtuoso, Broccoli. Query times are competitive and often faster on the pure SPARQL queries, and several orders of magnitude faster on the SPARQL+Text queries. Index size is larger for pure SPARQL queries, but smaller for SPARQL+Text queries.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {647–656},
numpages = {10},
keywords = {efficiency, sparql+text, indexing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132916,
author = {Cheng, Xuntao and He, Bingsheng and Du, Xiaoli and Lau, Chiew Tong},
title = {A Study of Main-Memory Hash Joins on Many-Core Processor: A Case with Intel Knights Landing Architecture},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132916},
doi = {10.1145/3132847.3132916},
abstract = {Advanced processor architectures have been driving new designs, implementations and optimizations of main-memory hash join algorithms recently. The newly released Intel Xeon Phi many-core processor of the Knights Landing architecture (KNL) embraces interesting hardware features such as many low-frequency out-of-order cores connected on a 2D mesh, and high-bandwidth multi-channel memory (MCDRAM). In this paper, we experimentally revisit the state-of-the-art main-memory hash join algorithms to study how the new hardware features of KNL affect the algorithmic design and tuning as well as to identify the opportunities for further performance improvement on KNL. Our experiments show that, although many existing optimizations are still valid on KNL with proper tuning, even the state-of-the-art algorithms have severely underutilized the memory bandwidth and other hardware resources.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {657–666},
numpages = {10},
keywords = {many-core processor, database operators, hash join algorithms},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132901,
author = {Liu, Yingfan and Cheng, Hong and Cui, Jiangtao},
title = {PQBF: I/O-Efficient Approximate Nearest Neighbor Search by Product Quantization},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132901},
doi = {10.1145/3132847.3132901},
abstract = {Approximate nearest neighbor (ANN) search in high-dimensional space plays an essential role in many multimedia applications. Recently, product quantization (PQ) based methods for ANN search have attracted enormous attention in the community of computer vision, due to its good balance between accuracy and space requirement. PQ based methods embed a high-dimensional vector into a short binary code (called PQ code), and the squared Euclidean distance is estimated by asymmetric quantizer distance (AQD) with pretty high precision. Thus, ANN search in the original space can be converted to similarity search on AQD using the PQ approach. All existing PQ methods are in-memory solutions, which may not handle massive data if they cannot fit entirely in memory. In this paper, we propose an I/O-efficient PQ based solution for ANN search. We design an index called PQB+-forest to support efficient similarity search on AQD. PQB+-forest first creates a number of partitions of the PQ codes by a coarse quantizer and then builds a B+-tree, called PQB+-tree, for each partition. The search process is greatly expedited by focusing on a few selected partitions that are closest to the query, as well as by the pruning power of PQB+-trees. According to the experiments conducted on two large-scale data sets containing up to 1 billion vectors, our method outperforms its competitors, including the state-of-the-art PQ method and the state-of-the-art LSH methods for ANN search.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {667–676},
numpages = {10},
keywords = {product quantization, b+-tree, approximate nearest neighbor search},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132888,
author = {Moffat, Alistair and Petri, Matthias},
title = {ANS-Based Index Compression},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132888},
doi = {10.1145/3132847.3132888},
abstract = {Techniques for effectively representing the postings lists associated with inverted indexes have been studied for many years. Here we combine the recently developed "asymmetric numeral systems" (ANS) approach to entropy coding and a range of previous index compression methods, including VByte, Simple, and Packed. The ANS mechanism allows each of them to provide markedly improved compression effectiveness, at the cost of slower decoding rates. Using the 426 GB GOV2 collection, we show that the combination of blocking and ANS-based entropy-coding against a set of 16 magnitude-based probability models yields compression effectiveness superior to most previous mechanisms, while still providing reasonable decoding speed.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {677–686},
numpages = {10},
keywords = {inverted index, index compression, postings list, entropy coder, asymmetric numeral systems},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132935,
author = {Cao, Bin and Hou, Chenyu and Fan, Jing},
title = {Covering the Optimal Time Window Over Temporal Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132935},
doi = {10.1145/3132847.3132935},
abstract = {In this paper, we propose a new problem: covering the optimal time window over temporal data. Given a duration constraint d and a set of users where each user has multiple time intervals, the goal is to find all time windows which (1) are greater than or equal to the duration d, and (2) can be covered by the intervals from as many as possible users. This problem can be applied to real scenarios where people need to determine the best time for maximizing the number of people to be involved in an activity, e.g., the meeting organization and the online live video broadcasting. As far as we know, there is no existing algorithm that can solve the problem directly. In this paper, we propose two algorithms to solve the problem, the first one is considered as a baseline algorithm called sliding time window (STW), where we utilize the start and end points of all users - intervals to construct time windows satisfying duration d. And then we calculate the number of users whose intervals can cover the current time window. The second method, named TLI, is designed based on the the data structures from the Timeline Index in SAP HANA. In TLI algorithm, we conduct three consecutive phases to achieve the purpose of efficiency improvement, namely construction of Timeline Index, calculation of valid user set and calculation of time windows. Within the third phase, we prune the number of time windows by keeping track of the number of users in current optimal time window, which can help shrink the search space. Through extensive experimental evaluations, we find TLI algorithm outperforms STW two orders of magnitude in terms of querying time.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {687–696},
numpages = {10},
keywords = {time interval, temporal data, optimal time window covering, timeline index},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133038,
author = {Chekol, Melisachew Wudage},
title = {Scaling Probabilistic Temporal Query Evaluation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133038},
doi = {10.1145/3132847.3133038},
abstract = {Open information extraction has driven automatic construction of (temporal) knowledge graphs (e.g. YAGO) that maintain probabilistic (temporal) facts and inference rules. One of the most important tasks in these knowledge graphs is query evaluation. This task is well known to be #P-hard. One of the bottlenecks of probabilistic (temporal) query evaluation is finding efficient ways of grounding the query and inference rules, to generate a factor graph that can be used for approximate query evaluation or to retrieve lineages of queries for exact evaluation. In this work, we propose the PRATiQUE (PRobAbilistic Temporal QUery Evaluation) framework for scalable temporal query evaluation. It harnesses the structure of temporal inference rules for efficient in-database grounding, i.e., it uses partitions to store structurally equivalent rules. Besides,PRATiQUE leverages a state-of-the-art Gibbs sampler to compute marginal probabilities of query answers. We report on an extensive experimental evaluation, which confirms the efficiency of our proposal.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {697–706},
numpages = {10},
keywords = {temporal knowledge graphs, query evaluation, probabilistic},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132854,
author = {Dong, Boxiang and Chen, Zhengzhang and Wang, Hui (Wendy) and Tang, Lu-An and Zhang, Kai and Lin, Ying and Li, Zhichun and Chen, Haifeng},
title = {Efficient Discovery of Abnormal Event Sequences in Enterprise Security Systems},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132854},
doi = {10.1145/3132847.3132854},
abstract = {Intrusion detection system (IDS) is an important part of enterprise security system architecture. In particular, anomaly-based IDS has been widely applied to detect single abnormal process events that deviate from the majority. However, intrusion activity usually consists of a series of low-level heterogeneous events. The gap between low-level process events and high-level intrusion activities makes it particularly challenging to identify process events that are truly involved in a real malicious activity, and especially considering the massive 'noisy' events filling the event sequences. Hence, the existing work that focus on detecting single events can hardly achieve high detection accuracy. In this work, we formulate a novel problem in intrusion detection - suspicious event sequence discovery, and propose GID, an efficient graph-based intrusion detection technique that can identify abnormal event sequences from massive heterogeneous process traces with high accuracy. We fully implement GID and deploy it into a real-world enterprise security system, and it greatly helps detect the advanced threats and optimize the incident response. Executing GID on both static and streaming data shows that GID is efficient (processes about 2 million records per minute) and accurate for intrusion detection.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {707–715},
numpages = {9},
keywords = {graph modeling, intrusion detection, enterprise security system, anomaly detection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132917,
author = {Zhang, Yating and Jatowt, Adam and Tanaka, Katsumi},
title = {Temporal Analog Retrieval Using Transformation over Dual Hierarchical Structures},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132917},
doi = {10.1145/3132847.3132917},
abstract = {In recent years, we have witnessed a rapid increase of text con- tent stored in digital archives such as newspaper archives or web archives. Many old documents have been converted to digital form and made accessible online. Due to the passage of time, it is however difficult to effectively perform search within such collections. Users, especially younger ones, may have problems in finding appropriate keywords to perform effective search due to the terminology gap arising between their knowledge and the unfamiliar domain of archival collections. In this paper, we provide a general framework to bridge different domains across-time and, by this, to facilitate search and comparison as if carried in user's familiar domain (i.e., the present). In particular, we propose to find analogical terms across temporal text collections by applying a series of transformation procedures. We develop a cluster-biased transformation technique which makes use of hierarchical cluster structures built on the temporally distributed document collections. Our methods do not need any specially prepared training data and can be applied to diverse collections and time periods. We test the performance of the proposed approaches on the collections separated by both short (e.g., 20 years) and long time gaps (70 years), and we report improvements in range of 18%-27% over short and 56%-92% over long periods when compared to state-of-the-art baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {717–726},
numpages = {10},
keywords = {cluster-biased, heterogeneous document collections, dual hierarchical structure, temporal analog},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133035,
author = {Williams, Kyle and Zitouni, Imed},
title = {Does That Mean You're Happy? RNN-Based Modeling of User Interaction Sequences to Detect Good Abandonment},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133035},
doi = {10.1145/3132847.3133035},
abstract = {Queries for which there are no clicks are known as abandoned queries. Differentiating between good and bad abandonment queries has become an important task in search engine evaluation since it allows for better measurement of search engine features that do not require users to click. Examples of these features include answers on the SERP and detailed Web result snippets. In this paper, we investigate how sequences of user interactions on the SERP differ between good and bad abandonment. To do this, we study the behavior patterns on a labeled dataset of abandoned queries and find that they differ in several ways, such as in the number of user interactions and the nature of those interactions. Based on this insight, we frame good abandonment detection as a sequence classification problem. We use a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) to model the sequence of user interactions and show that it performs significantly better than other baselines when detecting good abandonment, achieving 71% accuracy. Our findings have implications for search engine evaluation.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {727–736},
numpages = {10},
keywords = {lstm, satisfaction, good abandonment, mouse movements, user interaction modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133001,
author = {Mehrotra, Rishabh and Awadallah, Ahmed Hassan and Shokouhi, Milad and Yilmaz, Emine and Zitouni, Imed and El Kholy, Ahmed and Khabsa, Madian},
title = {Deep Sequential Models for Task Satisfaction Prediction},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133001},
doi = {10.1145/3132847.3133001},
abstract = {Detecting and understanding implicit signals of user satisfaction are essential for experimentation aimed at predicting searcher satisfaction. As retrieval systems have advanced, search tasks have steadily emerged as accurate units not only to capture searcher's goals but also in understanding how well a system is able to help the user achieve that goal. However, a major portion of existing work on modeling searcher satisfaction has focused on query level satisfaction. The few existing approaches for task satisfaction prediction have narrowly focused on simple tasks aimed at solving atomic information needs.In this work we go beyond such atomic tasks and consider the problem of predicting user's satisfaction when engaged in complex search tasks composed of many different queries and subtasks. We begin by considering holistic view of user interactions with the search engine result page (SERP) and extract detailed interaction sequences of their activity. We then look at query level abstraction and propose a novel deep sequential architecture which leverages the extracted interaction sequences to predict query level satisfaction. Further, we enrich this model with auxiliary features which have been traditionally used for satisfaction prediction and propose a unified multi-view model which combines the benefit of user interaction sequences with auxiliary features.Finally, we go beyond query level abstraction and consider query sequences issued by the user in order to complete a complex task, to make task level satisfaction predictions. We propose a number of functional composition techniques which take into account query level satisfaction estimates along with the query sequence to predict task level satisfaction. Through rigorous experiments, we demonstrate that the proposed deep sequential models significantly outperform established baselines at both query and task satisfaction prediction. Our findings have implications on metric development for gauging user satisfaction and on designing systems which help users accomplish complex search tasks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {737–746},
numpages = {10},
keywords = {user interactions, lstm, search tasks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133033,
author = {Jiang, Jiepu and Allan, James},
title = {Adaptive Persistence for Search Effectiveness Measures},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133033},
doi = {10.1145/3132847.3133033},
abstract = {Many search effectiveness evaluation measures penalize the importance of results at lower ranks. This is usually explained as an attempt to model users' persistence when sequentially examining results---lower ranked results are less important because users are less likely persistent enough to read them. The persistence parameters are usually set to cope with the target cohort and tasks. But during a particular evaluation round, the same parameters are applied to evaluate different ranked lists. In contrast, we present work that adapts the persistence factor according to the ranking and relevance of the ranked lists being evaluated. This is to model that rational users change their browsing behavior according to the search result page, e.g., users avoid wasting time (a low persistence level) if the results look apparently off-topic. Experimental results show that this approach better fits observed user behavior and correlates with users' ratings on their search performance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {747–756},
numpages = {10},
keywords = {user model, search effectiveness evaluation measure, persistence},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132850,
author = {Machmouchi, Widad and Awadallah, Ahmed Hassan and Zitouni, Imed and Buscher, Georg},
title = {Beyond Success Rate: Utility as a Search Quality Metric for Online Experiments},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132850},
doi = {10.1145/3132847.3132850},
abstract = {User satisfaction metrics are an integral part of search engine development as they help system developers to understand and evaluate the quality of the user experience. Research to date has mostly focused on predicting success or frustration as a proxy for satisfaction. However, users' search experience is more complex than merely being either successful or not. As such, using success rate as a measure of satisfaction can be limiting. In this work, we propose the use of utility as a measure of searcher satisfaction. This concept represents the fulfillment a user receives from con-suming a service and explains how users aim to gain optimal overall satisfaction. Our utility metrics measure the user satisfac-tion by aggregating all their interaction with the search engine. These interactions are represented as a timeline of actions and their dwelltimes, where each action is classified as having a posi-tive or negative effect on the user. We examine sessions mined from Bing logs, with multi-point scale assessment of searcher satisfaction and show that utility is a better proxy for satisfaction compared to success. Leveraging that data, we design metrics of searcher satisfaction that assess the overall utility accumulated by a user during her search session. We use real user traffic from millions of users in an A/B setting to compare utility metrics to success rate metrics. We show that utility is a better metric for evaluating searcher satisfaction with the search engine, and a more sensitive and accurate metric when compared to predicting success. These metrics are currently adopted as the top-level met-ric for evaluating the thousands of A/B experiments that are run on Bing each year.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {757–765},
numpages = {9},
keywords = {evaluation, utility, session., Search satisfaction, effort},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132988,
author = {Mele, Ida and Bahrainian, Seyed Ali and Crestani, Fabio},
title = {Linking News across Multiple Streams for Timeliness Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132988},
doi = {10.1145/3132847.3132988},
abstract = {Linking multiple news streams based on the reported events and analyzing the streams' temporal publishing patterns are two very important tasks for information analysis, discovering newsworthy stories, studying the event evolution, and detecting untrustworthy sources of information. In this paper, we propose techniques for cross-linking news streams based on the reported events with the purpose of analyzing the temporal dependencies among streams.Our research tackles two main issues: (1) how news streams are connected as reporting an event or the evolution of the same event and (2) how timely the newswires report related events using different publishing platforms. Our approach is based on dynamic topic modeling for detecting and tracking events over the timeline and on clustering news according to the events. We leverage the event-based clustering to link news across different streams and present two scoring functions for ranking the streams based on their timeliness in publishing news about a specific event.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {767–776},
numpages = {10},
keywords = {temporal analysis, news streams, event mining, dynamic topic modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132852,
author = {Liu, Bang and Niu, Di and Lai, Kunfeng and Kong, Linglong and Xu, Yu},
title = {Growing Story Forest Online from Massive Breaking News},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132852},
doi = {10.1145/3132847.3132852},
abstract = {We describe our experience of implementing a news content organization system at Tencent that discovers events from vast streams of breaking news and evolves news story structures in an online fashion. Our real-world system has distinct requirements in contrast to previous studies on topic detection and tracking (TDT) and event timeline or graph generation, in that we 1) need to accurately and quickly extract distinguishable events from massive streams of long text documents that cover diverse topics and contain highly redundant information, and 2) must develop the structures of event stories in an online manner, without repeatedly restructuring previously formed stories, in order to guarantee a consistent user viewing experience. In solving these challenges, we propose Story Forest, a set of online schemes that automatically clusters streaming documents into events, while connecting related events in growing trees to tell evolving stories. We conducted extensive evaluation based on 60 GB of real-world Chinese news data, although our ideas are not language-dependent and can easily be extended to other languages, through detailed pilot user experience studies. The results demonstrate the superior capability of Story Forest to accurately identify events and organize news text into a logical structure that is appealing to human readers, compared to multiple existing algorithm frameworks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {777–785},
numpages = {9},
keywords = {text clustering, information retrieval, online story tree},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132995,
author = {Lim, Wee Yong and Lee, Mong Li and Hsu, Wynne},
title = {IFACT: An Interactive Framework to Assess Claims from Tweets},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132995},
doi = {10.1145/3132847.3132995},
abstract = {Posts by users on microblogs such as Twitter provide diverse real-time updates to major events. Unfortunately, not all the information are credible. Previous works that assess the credibility of information in Twitter have focused on extracting features from the Tweets. In this work, we present an interactive framework called iFACT for assessing the credibility of claims from tweets. The proposed framework collects independent evidence from web search results (WSR) and identify the dependencies between claims. It utilizes features from the search results to determine the probabilities that a claim is credible, not credible or inconclusive. Finally, the dependencies between claims are used to adjust the likelihood estimates of a claim being credible, not credible or inconclusive. iFACT allows users to be engaged in the credibility assessment process by providing feedback as to whether the web search results are relevant, support or contradict a claim. Experiment results on multiple real world datasets demonstrate the effectiveness of WSR features and its ability to generalize to claims of new events. Case studies show the usefulness of claim dependencies and how the proposed approach can give explanation to the credibility assessment process.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {787–796},
numpages = {10},
keywords = {credibility},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132877,
author = {Ruchansky, Natali and Seo, Sungyong and Liu, Yan},
title = {CSI: A Hybrid Deep Model for Fake News Detection},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132877},
doi = {10.1145/3132847.3132877},
abstract = {The topic of fake news has drawn attention both from the public and the academic communities. Such misinformation has the potential of affecting public opinion, providing an opportunity for malicious parties to manipulate the outcomes of public events such as elections. Because such high stakes are at play, automatically detecting fake news is an important, yet challenging problem that is not yet well understood. Nevertheless, there are three generally agreed upon characteristics of fake news: the text of an article, the user response it receives, and the source users promoting it. Existing work has largely focused on tailoring solutions to one particular characteristic which has limited their success and generality.In this work, we propose a model that combines all three characteristics for a more accurate and automated prediction. Specifically, we incorporate the behavior of both parties, users and articles, and the group behavior of users who propagate fake news. Motivated by the three characteristics, we propose a model called CSI which is composed of three modules: Capture, Score, and Integrate. The first module is based on the response and text; it uses a Recurrent Neural Network to capture the temporal pattern of user activity on a given article. The second module learns the source characteristic based on the behavior of users, and the two are integrated with the third module to classify an article as fake or not. Experimental analysis on real-world data demonstrates that CSI achieves higher accuracy than existing models, and extracts meaningful latent representations of both users and articles.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {797–806},
numpages = {10},
keywords = {social networks, deep learning, neural network, group anomaly detection, fake news detection, temporal analysis},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132994,
author = {Pang, Guansong and Xu, Hongzuo and Cao, Longbing and Zhao, Wentao},
title = {Selective Value Coupling Learning for Detecting Outliers in High-Dimensional Categorical Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132994},
doi = {10.1145/3132847.3132994},
abstract = {This paper introduces a novel framework, namely SelectVC and its instance POP, for learning selective value couplings (i.e., interactions between the full value set and a set of outlying values) to identify outliers in high-dimensional categorical data. Existing outlier detection methods work on a full data space or feature subspaces that are identified independently from subsequent outlier scoring. As a result, they are significantly challenged by overwhelming irrelevant features in high-dimensional data due to the noise brought by the irrelevant features and its huge search space. In contrast, SelectVC works on a clean and condensed data space spanned by selective value couplings by jointly optimizing outlying value selection and value outlierness scoring. Its instance POP defines a value outlierness scoring function by modeling a partial outlierness propagation process to capture the selective value couplings. POP further defines a top-k outlying value selection method to ensure its scalability to the huge search space. We show that POP (i) significantly outperforms five state-of-the-art full space- or subspace-based outlier detectors and their combinations with three feature selection methods on 12 real-world high-dimensional data sets with different levels of irrelevant features; and (ii) obtains good scalability, stable performance w.r.t. k, and fast convergence rate.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {807–816},
numpages = {10},
keywords = {outlier detection, coupling learning, categorical data, high-dimensional data, feature selection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132987,
author = {Zhu, Mengxiao and Aggarwal, Charu C. and Ma, Shuai and Zhang, Hui and Huai, Jinpeng},
title = {Outlier Detection in Sparse Data with Factorization Machines},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132987},
doi = {10.1145/3132847.3132987},
abstract = {In sparse data, a large fraction of the entries take on zero values. Some examples of sparse data include short text snippets (such as tweets in Twitter) or some feature representations of categorical data sets with a large number of values, in which traditional methods for outlier detection typically fail because of the difficulty of computing distances. To address this, it is important to use the latent relations between such values. Factorization machines represent a natural methodology for this, and are naturally designed for the massive-domain setting because of their emphasis on sparse data sets. In this study, we propose an outlier detection approach for sparse data with factorization machines. Factorization machines are also efficient due to their linear complexity in the number of non-zero values. In fact, because of their efficiency, they can even be extended to traditional settings for numerical data by an appropriate feature engineering effort. We show that our approach is both effective and efficient for sparse categorical, short text and numerical data by an extensive experimental study.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {817–826},
numpages = {10},
keywords = {sparse data, outlier detection, factorization machines},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132964,
author = {Teng, Xian and Lin, Yu-Ru and Wen, Xidao},
title = {Anomaly Detection in Dynamic Networks Using Multi-View Time-Series Hypersphere Learning},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132964},
doi = {10.1145/3132847.3132964},
abstract = {Detecting anomalous patterns from dynamic and multi-attributed network systems has been a challenging problem due to the complication of temporal dynamics and the variations reflected in multiple data sources. We propose a Multi-view Time-Series Hypersphere Learning (MTHL) approach that leverages multi-view learning and support vector description to tackle this problem. Given a dynamic network with time-varying edge and node properties, MTHL projects multi-view time-series data into a shared latent subspace, and then learns a compact hypersphere surrounding normal samples with soft constraints. The learned hypersphere allows for effectively distinguishing normal and abnormal cases. We further propose an efficient, two-stage alternating optimization algorithm as a solution to the MTHL. Extensive experiments are conducted on both synthetic and real datasets. Results demonstrate that our method outperforms the state-of-the-art baseline methods in detecting three types of events that involve (i) time-varying features alone, (ii) time-aggregated features alone, as well as (iii) both features. Moreover, our approach exhibits consistent and good performance in face of issues including noises, anomaly pollution in training phase and data imbalance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {827–836},
numpages = {10},
keywords = {dynamic networks, anomaly detection, multi-view learning, time- series mining, urban computing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132933,
author = {Wu, Hao and Sun, Weiwei and Zheng, Baihua},
title = {A Fast Trajectory Outlier Detection Approach via Driving Behavior Modeling},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132933},
doi = {10.1145/3132847.3132933},
abstract = {Trajectory outlier detection is a fundamental building block for many location-based service (LBS) applications, with a large application base. We dedicate this paper on detecting the outliers from vehicle trajectories efficiently and effectively. In addition, we want our solution to be able to issue an alarm early when an outlier trajectory is only partially observed (i.e., the trajectory has not yet reached the destination). Most existing works study the problem on general Euclidean trajectories and require accesses to the historical trajectory database or computations on the distance metric that are very expensive. Furthermore, few of existing works consider some specific characteristics of vehicles trajectories (e.g., their movements are constrained by the underlying road networks), and majority of them require the input of complete trajectories. Motivated by this, we propose a vehicle outlier detection approach namely DB-TOD which is based on probabilistic model via modeling the driving behavior/preferences from the set of historical trajectories. We design outlier detection algorithms on both complete trajectory and partial one. Our probabilistic model-based approach makes detecting trajectory outlier extremely efficient while preserving the effectiveness, contributed by the relatively accurate model on driving behavior. We conduct comprehensive experiments using real datasets and the results justify both effectiveness and efficiency of our approach.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {837–846},
numpages = {10},
keywords = {trajectory data processing, outlier detection, inverse reinforcement learning, driving behavior},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133026,
author = {Zhang, Jiawei and Cui, Limeng and Yu, Philip S. and Lv, Yuanhua},
title = {BL-ECD: Broad Learning Based Enterprise Community Detection via Hierarchical Structure Fusion},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133026},
doi = {10.1145/3132847.3133026},
abstract = {Employees in companies can be divided into different social communities, and those who frequently socialize with each other will be treated as close friends and are grouped in the same community. In the enterprise context, a large amount of information about the employees is available in both (1) offline company internal sources and (2) online enterprise social networks (ESNs). Each of the information sources also contain multiple categories of employees' socialization activities at the same time. In this paper, we propose to detect the social communities of the employees in companies based on the broad learning setting with both these online and offline information sources simultaneously, and the problem is formally called the "Broad Learning based Enterprise Community Detection" (BL-ECD) problem. To address the problem, a novel broad learning based community detection framework named "HeterogeneoUs Multi-sOurce ClusteRing" (HUMOR) is introduced in this paper. Based on the various enterprise social intimacy measures introduced in this paper, HUMOR detects a set of micro community structures of the employees based on each of the socialization activities respectively. To obtain the (globally) consistent community structure of employees in the company, HUMOR further fuses these micro community structures via two broad learning phases: (1) intra-fusion of micro community structures to obtain the online and offline (locally) consistent communities respectively, and (2) inter-fusion of the online and offline communities to achieve the (globally) consistent community structure of employees. Extensive experiments conducted on real-world enterprise datasets demonstrate our method can perform very well in addressing the BL-ECD problem.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {859–868},
numpages = {10},
keywords = {community detection, heterogeneous information network, network embedding, enterprise social networks, aligned social networks, broad learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133004,
author = {Hoang, Tuan-Anh and Lim, Ee-Peng},
title = {Highly Efficient Mining of Overlapping Clusters in Signed Weighted Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133004},
doi = {10.1145/3132847.3133004},
abstract = {In many practical contexts, networks are weighted as their links are assigned numerical weights representing relationship strengths or intensities of inter-node interaction. Moreover, the links' weight can be positive or negative, depending on the relationship or interaction between the connected nodes. The existing methods for network clustering however are not ideal for handling very large signed weighted networks. In this paper, we present a novel method called LPOCSIN (short for "Linear Programming based Overlapping Clustering on Signed Weighted Networks") for efficient mining of overlapping clusters in signed weighted networks. Different from existing methods that rely on computationally expensive cluster cohesiveness measures, LPOCSIN utilizes a simple yet effective one. Using this measure, we transform the cluster assignment problem into a series of alternating linear programs, and further propose a highly efficient procedure for solving those alternating problems. We evaluate LPOCSIN and other state-of-the-art methods by extensive experiments covering a wide range of synthetic and real networks. The experiments show that LPOCSIN significantly outperforms the other methods in recovering ground-truth clusters while being an order of magnitude faster than the most efficient state-of-the-art method.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {869–878},
numpages = {10},
keywords = {weighted network, signed network, overlapping clustering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132991,
author = {Ruchansky, Natali and Bonchi, Francesco and Garcia-Soriano, David and Gullo, Francesco and Kourtellis, Nicolas},
title = {To Be Connected, or Not to Be Connected: That is the Minimum Inefficiency Subgraph Problem},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132991},
doi = {10.1145/3132847.3132991},
abstract = {We study the problem of extracting a selective connector for a given set of query vertices Q subset of V in a graph G = (V,E). A selective connector is a subgraph of G which exhibits some cohesiveness property, and contains the query vertices but does not necessarily connect them all. Relaxing the connectedness requirement allows the connector to detect multiple communities and to be tolerant to outliers. We achieve this by introducing the new measure of network inefficiency and by instantiating our search for a selective connector as the problem of finding the minimum inefficiency subgraph.We show that the minimum inefficiency subgraph problem is NP-hard, and devise efficient algorithms to approximate it. By means of several case studies in a variety of application domains (such as human brain, cancer, and food networks), we show that our minimum inefficiency subgraph produces high-quality solutions, exhibiting all the desired behaviors of a selective connector.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {879–888},
numpages = {10},
keywords = {graph mining, seed set expansion, data mining, social network analysis, biomedical and healthcare applications, brain network, community},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132967,
author = {Wang, Chun and Pan, Shirui and Long, Guodong and Zhu, Xingquan and Jiang, Jing},
title = {MGAE: Marginalized Graph Autoencoder for Graph Clustering},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132967},
doi = {10.1145/3132847.3132967},
abstract = {Graph clustering aims to discovercommunity structures in networks, the task being fundamentally challenging mainly because the topology structure and the content of the graphs are difficult to represent for clustering analysis. Recently, graph clustering has moved from traditional shallow methods to deep learning approaches, thanks to the unique feature representation learning capability of deep learning. However, existing deep approaches for graph clustering can only exploit the structure information, while ignoring the content information associated with the nodes in a graph. In this paper, we propose a novel marginalized graph autoencoder (MGAE) algorithm for graph clustering. The key innovation of MGAE is that it advances the autoencoder to the graph domain, so graph representation learning can be carried out not only in a purely unsupervised setting by leveraging structure and content information, it can also be stacked in a deep fashion to learn effective representation. From a technical viewpoint, we propose a marginalized graph convolutional network to corrupt network node content, allowing node content to interact with network features, and marginalizes the corrupted features in a graph autoencoder context to learn graph feature representations. The learned features are fed into the spectral clustering algorithm for graph clustering. Experimental results on benchmark datasets demonstrate the superior performance of MGAE, compared to numerous baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {889–898},
numpages = {10},
keywords = {graph convolutional network, graph clustering, network representation, autoencoder, graph autoencoder},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132974,
author = {Vasiloudis, Theodore and Beligianni, Foteini and De Francisci Morales, Gianmarco},
title = {BoostVHT: Boosting Distributed Streaming Decision Trees},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132974},
doi = {10.1145/3132847.3132974},
abstract = {Online boosting improves the accuracy of classifiers for unbounded streams of data by chaining them into an ensemble. Due to its sequential nature, boosting has proven hard to parallelize, even more so in the online setting. This paper introduces BoostVHT, a technique to parallelize online boosting algorithms. Our proposal leverages a recently-developed model-parallel learning algorithm for streaming decision trees as a base learner. This design allows to neatly separate the model boosting from its training. As a result, BoostVHT provides a flexible learning framework which can employ any existing online boosting algorithm, while at the same time it can leverage the computing power of modern parallel and distributed cluster environments. We implement our technique on Apache SAMOA, an open-source platform for mining big data streams that can be run on several distributed execution engines, and demonstrate order of magnitude speedups compared to the state-of-the-art.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {899–908},
numpages = {10},
keywords = {distributed systems, decision trees, boosting, online learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133042,
author = {Duffield, Nick and Xu, Yunhong and Xia, Liangzhen and Ahmed, Nesreen K. and Yu, Minlan},
title = {Stream Aggregation Through Order Sampling},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133042},
doi = {10.1145/3132847.3133042},
abstract = {This paper introduces a new single-pass reservoir weighted-sampling stream aggregation algorithm, Priority-Based Aggregation (PBA). While order sampling is a powerful and efficient method for weighted sampling from a stream of uniquely keyed items, there is no current algorithm that realizes the benefits of order sampling in the context of stream aggregation over non-unique keys. A naive approach to order sample regardless of key then aggregate the results is hopelessly inefficient. In distinction, our proposed algorithm uses a single persistent random variable across the lifetime of each key in the cache, and maintains unbiased estimates of the key aggregates that can be queried at any point in the stream. The basic approach can be supplemented with a Sample and Hold pre-sampling stage with a sampling rate adaptation controlled by PBA. This approach represents a considerable reduction in computational complexity compared with the state of the art in adapting Sample and Hold to operate with a fixed cache size. Concerning statistical properties, we prove that PBA provides unbiased estimates of the true aggregates. We analyze the computational complexity of PBA and its variants, and provide a detailed evaluation of its accuracy on synthetic and trace data. Weighted relative error is reduced by 40% to 65% at sampling rates of 5% to 17%, relative to Adaptive Sample and Hold; there is also substantial improvement for rank queries.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {909–918},
numpages = {10},
keywords = {Priority Sampling, Aggregation, Subset Sums, Heavy Hitters},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132886,
author = {Haque, Ahsanul and Wang, Zhuoyi and Chandra, Swarup and Dong, Bo and Khan, Latifur and Hamlen, Kevin W.},
title = {FUSION: An Online Method for Multistream Classification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132886},
doi = {10.1145/3132847.3132886},
abstract = {Traditional data stream classification assumes that data is generated from a single non-stationary process. On the contrary, multistream classification problem involves two independent non-stationary data generating processes. One of them is the source stream that continuously generates labeled data. The other one is the target stream that generates unlabeled test data from the same domain. The distribution represented by the source stream data is biased compared to that of the target stream. Moreover, these streams may have asynchronous concept drifts between them. The multistream classification problem is to predict the class labels of target stream instances by utilizing labeled data from the source stream. This kind of scenario is often observed in real-world applications due to scarcity of labeled data. The only existing approach for multistream classification uses separate drift detection on the streams for addressing the asynchronous concept drift problem. If a concept drift is detected in any of the streams, it uses an expensive batch technique for data shift adaptation. These add significant execution overhead, and limit its usability. In this paper, we propose an efficient solution for multistream classification by fusing drift detection into online data shift adaptation. We study the theoretical convergence rate and computational complexity of the proposed approach. Moreover, empirical results on benchmark data sets indicate significantly improved performance over the baseline methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {919–928},
numpages = {10},
keywords = {data shift adaptation, multistream classification, asynchronous concept drift, direct density ratio estimation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132907,
author = {Hu, Shuguang and Wu, Xiaowei and Chan, T-H. Hubert},
title = {Maintaining Densest Subsets Efficiently in Evolving Hypergraphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132907},
doi = {10.1145/3132847.3132907},
abstract = {In this paper we study the densest subgraph problem, which plays a key role in many graph mining applications. The goal of the problem is to find a subset of nodes that induces a graph with maximum average degree. The problem has been extensively studied in the past few decades under a variety of different settings. Several exact and approximation algorithms were proposed. However, as normal graph can only model objects with pairwise relationships, the densest subgraph problem fails in identifying communities under relationships that involve more than 2 objects, e.g., in a network connecting authors by publications.We consider in this work the densest subgraph problem in hypergraphs, which generalizes the problem to a wider class of networks in which edges might have different cardinalities and contain more than 2 nodes. We present two exact algorithms and a near-linear time r-approximation algorithm for the problem, where r is the maximum cardinality of an edge in the hypergraph. We also consider the dynamic version of the problem, in which an adversary can insert or delete an edge from the hypergraph in each round and the goal is to maintain efficiently an approximation of the densest subgraph. We present two dynamic approximation algorithms in this paper with amortized polog update time, for any ε &gt; 0. For the case when there are only insertions, the approximation ratio we maintain is r(1+ε), while for the fully dynamic case, the ratio is r2(1+ε). Extensive experiments are performed on large real datasets to validate the effectiveness and efficiency of our algorithms.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {929–938},
numpages = {10},
keywords = {densest subgraph, dynamic data structure, graph mining},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132948,
author = {Wang, Yuqi and Cao, Jiannong and He, Lifang and Li, Wengen and Sun, Lichao and Yu, Philip S.},
title = {Coupled Sparse Matrix Factorization for Response Time Prediction in Logistics Services},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132948},
doi = {10.1145/3132847.3132948},
abstract = {Nowadays, there is an emerging way of connecting logistics orders and van drivers, where it is crucial to predict the order response time. Accurate prediction of order response time would not only facilitate decision making on order dispatching, but also pave ways for applications such as supply-demand analysis and driver scheduling, leading to high system efficiency. In this work, we forecast order response time on current day by fusing data from order history and driver historical locations. Specifically, we propose Coupled Sparse Matrix Factorization (CSMF) to deal with the heterogeneous fusion and data sparsity challenges raised in this problem. CSMF jointly learns from multiple heterogeneous sparse data through the proposed weight setting mechanism therein. Experiments on real-world datasets demonstrate the effectiveness of our approach, compared to various baseline methods. The performances of many variants of the proposed method are also presented to show the effectiveness of each component.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {939–947},
numpages = {9},
keywords = {coupled matrix factorization, logistics services, sparse matrix factorization, response time prediction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132945,
author = {Shi, Qiquan and Lu, Haiping and Cheung, Yiu-ming},
title = {Tensor Rank Estimation and Completion via CP-Based Nuclear Norm},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132945},
doi = {10.1145/3132847.3132945},
abstract = {Tensor completion (TC) is a challenging problem of recovering missing entries of a tensor from its partial observation. One main TC approach is based on CP/Tucker decomposition. However, this approach often requires the determination of a tensor rank a priori. This rank estimation problem is difficult in practice. Several Bayesian solutions have been proposed but they often under/over-estimate the tensor rank while being quite slow. To address this problem of rank estimation with missing entries, we view the weight vector of the orthogonal CP decomposition of a tensor to be analogous to the vector of singular values of a matrix. Subsequently, we define a new CP-based tensor nuclear norm as the $L_1$-norm of this weight vector. We then propose Tensor Rank Estimation based on $L_1$-regularized orthogonal CP decomposition (TREL1) for both CP-rank and Tucker-rank. Specifically, we incorporate a regularization with CP-based tensor nuclear norm when minimizing the reconstruction error in TC to automatically determine the rank of an incomplete tensor. Experimental results on both synthetic and real data show that: 1) Given sufficient observed entries, TREL1 can estimate the true rank (both CP-rank and Tucker-rank) of incomplete tensors well; 2) The rank estimated by TREL1 can consistently improve recovery accuracy of decomposition-based TC methods; 3) TREL1 is not sensitive to its parameters in general and more efficient than existing rank estimation methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {949–958},
numpages = {10},
keywords = {cp-based tensor nuclear norm, tensor completion, cp decomposition, tensor rank estimation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132851,
author = {Khoa, Nguyen Lu Dang and Anaissi, Ali and Wang, Yang},
title = {Smart Infrastructure Maintenance Using Incremental Tensor Analysis: Extended Abstract},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132851},
doi = {10.1145/3132847.3132851},
abstract = {Civil infrastructures are key to the flow of people and goods in urban environments. Structural Health Monitoring (SHM) is a condition-based maintenance technology, which provides and predicts actionable information on the current and future states of infrastructures. SHM data are usually multi-way data which are produced by multiple highly correlated sensors. Tensor decomposition allows the learning from such data in temporal, spatial and feature modes at the same time. However, to facilitate a real time response for online learning, incremental tensor update need to be used when new data come in, rather than doing the decomposition in a batch manner. This work proposed a method called onlineCP-ALS to incrementally update tensor component matrices, followed by a self-tuning one-class support vector machine for online damage identification. Moreover, a robust clustering technique was applied on the tensor space for online substructure grouping and anomaly detection. These methods were applied to data from lab-based structures and also data collected from the Sydney Harbour Bridge in Australia. We obtained accurate damage detection accuracies for all these datasets. Damage locations were also captured correctly, and different levels of damage severity were well estimated. Furthermore, the clustering technique was able to detect spatial anomalies, which were associated with sensor and instrumentation issues. Our proposed method was efficient and much faster than the batch approach.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {959–967},
numpages = {9},
keywords = {incremental tensor analysis, anomaly detection, smart infrastructures, structural health monitoring, clustering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132862,
author = {Das, Ariyam and Upadhyaya, Ishan and Meng, Xiangrui and Talwalkar, Ameet},
title = {Collaborative Filtering as a Case-Study for Model Parallelism on Bulk Synchronous Systems},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132862},
doi = {10.1145/3132847.3132862},
abstract = {Industrial-scale machine learning applications often train and maintain massive models that can be on the order of hundreds of millions to billions of parameters. Model parallelism thus plays a significant role to support these machine learning tasks. Recent work in this area has been dominated by parameter server architectures that follow an asynchronous computation model, introducing added complexity and approximation in order to scale to massive workloads. In this work, we explore model parallelism in the distributed bulk-synchronous parallel (BSP) setting, leveraging some recent progress made in the area of high performance computing, in order to address these complexity and approximation issues. Using collaborative filtering as a case-study, we introduce an efficient model parallel industrial scale algorithm for alternating least squares (ALS), along with a highly optimized implementation of ALS that serves as the default implementation in MLlib, Apache Spark's machine learning library. Our extensive empirical evaluation demonstrates that our implementation in MLlib compares favorably to the leading open-source parameter server framework, and our implementation scales to massive problems on the order of 50 billion ratings and close to 1 billion parameters.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {969–977},
numpages = {9},
keywords = {bulk synchronous parallel (bsp) systems, model parallelism, apache spark, parameter servers, collaborative filtering, alternating least squares (als)},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132965,
author = {Shi, Yuling and Peng, Zhiyong and Wang, Hongning},
title = {Modeling Student Learning Styles in MOOCs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132965},
doi = {10.1145/3132847.3132965},
abstract = {The recorded student activities in Massive Open Online Course (MOOC) provide us a unique opportunity to model their learning behaviors, identify their particular learning intents, and enable personalized assistance and guidance in online education. In this work, based on a thorough qualitative study of students' behaviors recorded in two MOOC courses with large student enrollments, we develop a non-parametric Bayesian model to capture students' sequential learning activities in a generative manner. Homogeneity of students' learning behaviors is captured by clustering them into latent student groups, where shared model structure characterizes the transitional patterns, intensity and temporal distribution of their learning activities. In the meanwhile, heterogeneity is captured by clustering students into different groups. Both qualitative and quantitative studies on those two MOOC courses confirmed the effectiveness of the proposed model in identifying students' learning behavior patterns and clustering them into related groups for predictive analysis. The identified student groups accurately predict student retention, course satisfaction and demographics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {979–988},
numpages = {10},
keywords = {sequential data mining, moocs, probabilistic modeling, behavior modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132929,
author = {Chen, Yuying and Liu, Qi and Huang, Zhenya and Wu, Le and Chen, Enhong and Wu, Runze and Su, Yu and Hu, Guoping},
title = {Tracking Knowledge Proficiency of Students with Educational Priors},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132929},
doi = {10.1145/3132847.3132929},
abstract = {Diagnosing students' knowledge proficiency, i.e., the mastery degrees of a particular knowledge point in exercises, is a crucial issue for numerous educational applications, e.g., targeted knowledge training and exercise recommendation. Educational theories have converged that students learn and forget knowledge from time to time. Thus, it is necessary to track their mastery of knowledge over time. However, traditional methods in this area either ignored the explanatory power of the diagnosis results on knowledge points or relied on a static assumption. To this end, in this paper, we devise an explanatory probabilistic approach to track the knowledge proficiency of students over time by leveraging educational priors. Specifically, we first associate each exercise with a knowledge vector in which each element represents an explicit knowledge point by leveraging educational priors (i.e., Q-matrix ). Correspondingly, each student is represented as a knowledge vector at each time in a same knowledge space. Second, given the student knowledge vector over time, we borrow two classical educational theories (i.e., Learning curve and Forgetting curve ) as priors to capture the change of each student's proficiency over time. After that, we design a probabilistic matrix factorization framework by combining student and exercise priors for tracking student knowledge proficiency. Extensive experiments on three real-world datasets demonstrate both the effectiveness and explanatory power of our proposed model.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {989–998},
numpages = {10},
keywords = {knowledge diagnosis, explanatory power, educational priors, dynamic modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132882,
author = {Chen, Zhe and Dadiomov, Sasha and Wesley, Richard and Xiao, Gang and Cory, Daniel and Cafarella, Michael and Mackinlay, Jock},
title = {Spreadsheet Property Detection With Rule-Assisted Active Learning},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132882},
doi = {10.1145/3132847.3132882},
abstract = {Spreadsheets are a critical and widely-used data management tool. Converting spreadsheet data into relational tables would bring benefits to a number of fields, including public policy, public health, and economics. Research to date has focused on designing domain-specific languages to describe transformation processes or automatically converting a specific type of spreadsheets. To handle a larger variety of spreadsheets, we have to identify various spreadsheet properties, which correspond to a series of transformation programs that contribute towards a general framework that converts spreadsheets to relational tables.In this paper, we focus on the problem of spreadsheet property detection. We propose a hybrid approach of building a variety of spreadsheet property detectors to reduce the amount of required human labeling effort. Our approach integrates an active learning framework with crude, easy-to-write, user-provided rules to save human labeling effort by generating additional high-quality labeled data especially in the initial training stage. Using a bagging-like technique, Our approach can also tolerate lower-quality user-provided rules. Our experiments show that when compared to a standard active learning approach, we reduced the training data needed to reach the performance plateau by 34-44% when a human provides relatively high-quality rules, and by a comparable amount with low-quality rules. A study on a large-scale web-crawled spreadsheet dataset demonstrates that it is crucial to detect a variety of spreadsheet properties in order to transform a large portion of the spreadsheets into a relational form.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {999–1008},
numpages = {10},
keywords = {spreadsheets, data cleaning, active learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132939,
author = {Zhou, Xiaofei and Zhu, Qiannan and Liu, Ping and Guo, Li},
title = {Learning Knowledge Embeddings by Combining Limit-Based Scoring Loss},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132939},
doi = {10.1145/3132847.3132939},
abstract = {In knowledge graph embedding models, the margin-based ranking loss as the common loss function is usually used to encourage discrimination between golden triplets and incorrect triplets, which has proved effective in many translation-based models for knowledge graph embedding. However, we find that the loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation. In this paper, we present a limit-based scoring loss to provide lower scoring of a golden triplet, and then to extend two basic translation models TransE and TransH, separately to TransE-RS and TransH-RS by combining limit-based scoring loss with margin-based ranking loss. Both the presented models have low complexities of parameters benefiting for application on large scale graphs. In experiments, we evaluate our models on two typical tasks including triplet classification and link prediction, and also analyze the scoring distributions of positive and negative triplets by different models. Experimental results show that the introduced limit-based scoring loss is effective to improve the capacities of knowledge graph embedding.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1009–1018},
numpages = {10},
keywords = {data mining, image recognition, machine learning, nlp},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132947,
author = {Huang, Zhengjie and Ye, Zi and Li, Shuangyin and Pan, Rong},
title = {Length Adaptive Recurrent Model for Text Classification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132947},
doi = {10.1145/3132847.3132947},
abstract = {In recent years, recurrent neural networks have been widely used for various text classification tasks. However, most of the recurrent architectures will not assign a class label to a text until they read the last word, while human beings are able to determine the text class before reading the whole text. In this paper, we propose a Length Adaptive Recurrent Model (LARM) which can automatically determine the minimum text length that is necessary to perform the classification. With three parts includingReader, Predictor andAgent, our model is designed to read a text word by word, and terminate the process when the adequate information has been caught for the text classification task. The experimental results show that our model has comparable or even better performance compared to the vanilla LSTM when both are fed with partial text input. Besides, we can speed up text classification by truncating the text when sufficient evidence is found for classification. Furthermore, we also visualize our model and show that our model works like human beings, who can gradually come up with the general idea of a text while reading texts sequentially.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1019–1027},
numpages = {9},
keywords = {recurrent neural network, text classification},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132937,
author = {Tay, Yi and Tuan, Luu Anh and Phan, Minh C. and Hui, Siu Cheung},
title = {Multi-Task Neural Network for Non-Discrete Attribute Prediction in Knowledge Graphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132937},
doi = {10.1145/3132847.3132937},
abstract = {Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete attributes for each entity. Intuitively, these attributes such as height, price or population count are able to richly characterize entities in knowledge graphs. This additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs. Unfortunately, many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs. In this paper, we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete attribute information in a relational setting. Specifically, we train a neural network for triplet prediction along with a separate network for attribute value regression. Via multi-task learning, we are able to learn representations of entities, relations and attributes that encode information about both tasks. Moreover, such attributes are not only central to many predictive tasks as an information source but also as a prediction target. Therefore, models that are able to encode, incorporate and predict such information in a relational learning context are highly attractive as well. We show that our approach outperforms many state-of-the-art methods for the tasks of relational triplet classification and attribute value prediction.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1029–1038},
numpages = {10},
keywords = {knowledge graphs, entities, artificial intelligence, relational learning, neural networks, machine learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132922,
author = {Chen, Jie and Shao, Jie and Shen, Fumin and He, Chengkun and Gao, Lianli and Shen, Heng Tao},
title = {Movie Fill in the Blank with Adaptive Temporal Attention and Description Update},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132922},
doi = {10.1145/3132847.3132922},
abstract = {Recently, a new type of video understanding task called Movie-Fill-in-the-Blank (MovieFIB) has attracted many research attentions. Given a pair of movie clip and description with one blank word as input, MovieFIB aims to automatically predict the blank word. Because of the advantage in processing sequence data, Long-Short Term Memory (LSTM) has been used as a key component in existing MovieFIB methods to generate representations of videos and descriptions. However, most of these methods fail to emphasize the salient parts of videos. To address this problem, in this paper we propose to use a novel LSTM network called LSTM with Linguistic gate (LSTMwL), which exploits adaptive temporal attention for MovieFIB. Specifically, we first use LSTM to produce video features, which are then used to update the text representation. Finally, we put the updated text into two opposite directional LSTMwL layers to infer the blank word. Experimental results demonstrate that our approach outperforms state-of-the-art models for MovieFIB.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1039–1048},
numpages = {10},
keywords = {description update, question answering, adaptive temporal attention},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132866,
author = {Khandpur, Rupinder Paul and Ji, Taoran and Jan, Steve and Wang, Gang and Lu, Chang-Tien and Ramakrishnan, Naren},
title = {Crowdsourcing Cybersecurity: Cyber Attack Detection Using Social Media},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132866},
doi = {10.1145/3132847.3132866},
abstract = {Social media is often viewed as a sensor into various societal events such as disease outbreaks, protests, and elections. We describe the use of social media as a crowdsourced sensor to gain insight into ongoing cyber-attacks. Our approach detects a broad range of cyber-attacks (e.g., distributed denial of service (DDoS) attacks, data breaches, and account hijacking) in a weakly supervised manner using just a small set of seed event triggers and requires no training or labeled samples. A new query expansion strategy based on convolution kernels and dependency parses helps model semantic structure and aids in identifying key event characteristics. Through a large-scale analysis over Twitter, we demonstrate that our approach consistently identifies and encodes events, outperforming existing methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1049–1057},
numpages = {9},
keywords = {social media, twitter, dynamic query expansion, cyber attacks, event detection, cyber security},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133002,
author = {Han, Tao and Sun, Hailong and Song, Yangqiu and Wang, Zizhe and Liu, Xudong},
title = {Budgeted Task Scheduling for Crowdsourced Knowledge Acquisition},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133002},
doi = {10.1145/3132847.3133002},
abstract = {Knowledge acquisition (e.g. through labeling) is one of the most successful applications in crowdsourcing. In practice, collecting as specific as possible knowledge via crowdsourcing is very useful since specific knowledge can be generalized easily if we have a knowledge base, but it is difficult to infer specific knowledge from general knowledge. Meanwhile, tasks for acquiring more specific knowledge can be more difficult for workers, thus need more answers to infer high-quality results. Given a limited budget, assigning workers to difficult tasks will be more effective for the goal of specific knowledge acquisition. However, existing crowdsourcing task scheduling cannot incorporate the specificity of workers' answers. In this paper, we present a new framework for task scheduling with the limited budget, targeting an effective solution to more specific knowledge acquisition. We propose novel criteria for evaluating the quality of specificity-dependent answers and result inference algorithms to aggregate more specific answers with budget constraints. We have implemented our framework with real crowdsourcing data and platform, and have achieved significant performance improvement compared with existing approaches.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1059–1068},
numpages = {10},
keywords = {crowdsourcing, knowledge acquisition, task scheduling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132971,
author = {Li, Jiyi and Baba, Yukino and Kashima, Hisashi},
title = {Hyper Questions: Unsupervised Targeting of a Few Experts in Crowdsourcing},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132971},
doi = {10.1145/3132847.3132971},
abstract = {Quality control is one of the major problems in crowdsourcing. One of the primary approaches to rectify this issue is to assign the same task to different workers and then aggregate their answers to obtain a reliable answer. In addition to simple aggregation approaches such as majority voting, various sophisticated probabilistic models have been proposed. However, given that most of the existing methods operate by strengthening the opinions of the majority, these models often fail when the tasks require highly specialized knowledge and the ability of a large majority of the workers is inadequate. In this paper, we focus on an important class of answer aggregation problems in which majority voting fails and propose the concept of hyper questions to devise effective aggregation methods. A hyper question is a set of single questions, and our key idea is that experts are more likely to provide correct answers to all of the single questions included in a hyper question than non-experts. Thus, experts are more likely to reach consensus on the hyper questions than non-experts, which strengthen their influences. We incorporate the concept of hyper questions into existing answer aggregation methods. The results of our experiments conducted using both synthetic datasets and real datasets demonstrate that our simple and easily usable approach works effectively in cases where only a few experts are available.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1069–1078},
numpages = {10},
keywords = {heterogeneous-answer multiple-choice questions, hyper question, answer aggregation, crowdsourcing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132958,
author = {Lin, Yusan and Yin, Peifeng and Lee, Wang-Chien},
title = {Modeling Menu Bundle Designs of Crowdfunding Projects},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132958},
doi = {10.1145/3132847.3132958},
abstract = {Offering products in the forms of menu bundles is a common practice in marketing to attract customers and maximize revenues. In crowdfunding platforms such as Kickstarter, rewards also play an important part in influencing project success. Designing rewards consisting of the appropriate items is a challenging yet crucial task for the project creators. However, prior research has not considered the strategies project creators take to offer and bundle the rewards, making it hard to study the impact of reward designs on project success. In this paper, we raise a novel research question: understanding project creators' decisions of reward designs to level their chance to succeed. We approach this by modeling the design behavior of project creators, and identifying the behaviors that lead to project success. We propose a probabilistic generative model, Menu-Offering-Bundle (MOB) model, to capture the offering and bundling decisions of project creators based on collected data of 14K crowdfunding projects and their 149K reward bundles across a half-year period. Our proposed model is shown to capture the offering and bundling topics, outperform the baselines in predicting reward designs. We also find that the learned offering and bundling topics carry distinguishable meanings and provide insights of key factors on project success.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1079–1088},
numpages = {10},
keywords = {menu bundle, bundling decision, offering decision, crowdfunding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133017,
author = {Parmar, Krunal and Bushi, Samuel and Bhattacharya, Sourangshu and Kumar, Surender},
title = {Forecasting Ad-Impressions on Online Retail Websites Using Non-Homogeneous Hawkes Processes},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133017},
doi = {10.1145/3132847.3133017},
abstract = {Promotional listing of products or advertisements is a major source of revenue for online retail companies. These advertisements are often sold in the guaranteed delivery market, serving of which critically depends on the ability to predict supply or potential impressions from a target segment of users. In this paper, we study the problem of predicting user visits or potential ad-impressions to online retail websites, based on historical time-stamps. We explore the time-series and temporal point process models. We find that a successful model must encompass three properties of the data: (1) temporally non-homgeneous rates, (2) self excitation and (3) handling special events. We propose a novel non-homogeneous Hawkes process based model for the same, and new algorithm for fitting this model without overfitting the self-excitation part. We validate the proposed model and algorithm using mulitple large scale ad-serving dataset from a top online retail company in India.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1089–1098},
numpages = {10},
keywords = {online advertising, supply forecasting, hawkes processes, non-stationary time-series forecasting},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132853,
author = {Song, Yuxuan and Ren, Kan and Cai, Han and Zhang, Weinan and Yu, Yong},
title = {Volume Ranking and Sequential Selection in Programmatic Display Advertising},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132853},
doi = {10.1145/3132847.3132853},
abstract = {Programmatic display advertising, which enables advertisers to make real-time decisions on individual ad display opportunities so as to achieve a precise audience marketing, has become a key technique for online advertising. However, the constrained budget setting still restricts unlimited ad impressions. As a result, a smart strategy for ad impression selection is necessary for the advertisers to maximize positive user responses such as clicks or conversions, under the constraints of both ad volume and campaign budget. In this paper, we borrow in the idea of top-N ranking and filtering techniques from information retrieval and propose an effective ad impression volume ranking method for each ad campaign, followed by a sequential selection strategy considering the remaining ad volume and budget, to smoothly deliver the volume filtering while maximizing campaign efficiency. The extensive experiments on two benchmarking datasets and a commercial ad platform demonstrate large performance superiority of our proposed solution over traditional methods, especially under tight budgets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1099–1107},
numpages = {9},
keywords = {noise contrastive estimation, article recommendation, transfer learning, text representation, word2vec},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132884,
author = {Yan, Huan and Lin, Tzu-Heng and Wang, Gang and Li, Yong and Zheng, Haitao and Jin, Depeng and Zhao, Ben Y.},
title = {On Migratory Behavior in Video Consumption},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132884},
doi = {10.1145/3132847.3132884},
abstract = {Today's video streaming market is crowded with various content providers (CPs). For individual CPs, understanding user behavior, in particular how users migrate among different CPs, is crucial for improving users' on-site experience and the CP's chance of success. In this paper, we take a data-driven approach to analyze and model user migration behavior in video streaming, i.e., users switching content provider during active sessions. Based on a large ISP dataset over two months (6 major content providers, 3.8 million users, and 315 million video requests), we study common migration patterns and reasons of migration. We find that migratory behavior is prevalent: 66% of users switch CPs with an average switching frequency of 13%. In addition, migration behaviors are highly diverse: regardless large or small CPs, they all have dedicated groups of users who like to switch to them for certain types of videos. Regarding reasons of migration, we find CP service quality rarely causes migration, while a few popular videos play a bigger role. Nearly 60% of cross-site migrations are landed to 0.14% top videos. Finally, we validate our findings by building an accurate regression model to predict user migration frequency, and discuss the implications of our results to CPs.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1109–1118},
numpages = {10},
keywords = {video consumption, migratory behavior},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132883,
author = {Li, Sha and Gao, Xiaofeng and Bao, Weiming and Chen, Guihai},
title = {FM-Hawkes: A Hawkes Process Based Approach for Modeling Online Activity Correlations},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132883},
doi = {10.1145/3132847.3132883},
abstract = {Understanding and predicting user behavior on online platforms has proved to be of significant value, with applications spanning from targeted advertising, political campaigning, anomaly detection to user self-monitoring. With the growing functionality and flexibility of online platforms, users can now accomplish a variety of tasks online. This advancement has rendered many previous works that focus on modeling a single type of activity obsolete. In this work, we target this new problem by modeling the interplay between the time series of different types of activities and apply our model to predict future user behavior. Our model, FM-Hawkes, stands for Fourier-based kernel multi-dimensional Hawkes process. Specifically, we model the multiple activity time series as a multi-dimensional Hawkes process. The correlations between different types of activities are then captured by the influence factor. As for the temporal triggering kernel, we observe that the intensity function consists of numerous kernel functions with time shift. Thus, we employ a Fourier transformation based non-parametric estimation. Our model is not bound to any particular platform and explicitly interprets the causal relationship between actions. By applying our model to real-life datasets, we confirm that the mutual excitation effect between different activities prevails among users. Prediction results show our superiority over models that do not consider action types and flexible kernels},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1119–1128},
numpages = {10},
keywords = {point processes, time series analysis, user activity modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133031,
author = {Zohrevand, Zahra and Gl\"{a}sser, Uwe and Tayebi, Mohammad A. and Shahir, Hamed Yaghoubi and Shirmaleki, Mehdi and Shahir, Amir Yaghoubi},
title = {Deep Learning Based Forecasting of Critical Infrastructure Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133031},
doi = {10.1145/3132847.3133031},
abstract = {Intelligent monitoring and control of critical infrastructure such as electric power grids, public water utilities and transportation systems produces massive volumes of time series data from heterogeneous sensor networks. Time Series Forecasting (TSF) is essential for system safety and security, and also for improving the efficiency and quality of service delivery. Being highly dependent on various external factors, the observed system behavior is usually stochastic, which makes the next value prediction a tricky and challenging task that usually needs customized methods. In this paper we propose a novel deep learning based framework for time series analysis and prediction by ensembling parametric and nonparametric methods. Our approach takes advantage of extracting features at different time scales, which improves accuracy without compromising reliability in comparison with the state-of-the-art methods. Our experimental evaluation using real-world SCADA data from a municipal water management system shows that our proposed method outperforms the baseline methods evaluated here.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1129–1138},
numpages = {10},
keywords = {deep learning, multimodal learning, time series forecasting, critical infrastructure protection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132972,
author = {Lee, Wonsung and Song, Kyungwoo and Moon, Il-Chul},
title = {Augmented Variational Autoencoders for Collaborative Filtering with Auxiliary Information},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132972},
doi = {10.1145/3132847.3132972},
abstract = {Recommender systems offer critical services in the age of mass information. A good recommender system selects a certain item for a specific user by recognizing why the user might like the item. This awareness implies that the system should model the background of the items and the users. This background modeling for recommendation is tackled through the various models of collaborative filtering with auxiliary information. This paper presents variational approaches for collaborative filtering to deal with auxiliary information. The proposed methods encompass variational autoencoders through augmenting structures to model the auxiliary information and to model the implicit user feedback. This augmentation includes the ladder network and the generative adversarial network to extract the low-dimensional representations influenced by the auxiliary information. These two augmentations are the first trial in the venue of the variational autoencoders, and we demonstrate their significant improvement on the performances in the applications of the collaborative filtering.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1139–1148},
numpages = {10},
keywords = {collaborative filtering, deep learning, variational autoencoders, generative adversarial networks, recommender systems},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132973,
author = {Cao, Qi and Shen, Huawei and Cen, Keting and Ouyang, Wentao and Cheng, Xueqi},
title = {DeepHawkes: Bridging the Gap between Prediction and Understanding of Information Cascades},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132973},
doi = {10.1145/3132847.3132973},
abstract = {Online social media remarkably facilitates the production and delivery of information, intensifying the competition among vast information for users' attention and highlighting the importance of predicting the popularity of information. Existing approaches for popularity prediction fall into two paradigms: feature-based approaches and generative approaches. Feature-based approaches extract various features (e.g., user, content, structural, and temporal features), and predict the future popularity of information by training a regression/classification model. Their predictive performance heavily depends on the quality of hand-crafted features. In contrast, generative approaches devote to characterizing and modeling the process that a piece of information accrues attentions, offering us high ease to understand the underlying mechanisms governing the popularity dynamics of information cascades. But they have less desirable predictive power since they are not optimized for popularity prediction. In this paper, we propose DeepHawkes to combat the defects of existing methods, leveraging end-to-end deep learning to make an analogy to interpretable factors of Hawkes process --- a widely-used generative process to model information cascade. DeepHawkes inherits the high interpretability of Hawkes process and possesses the high predictive power of deep learning methods, bridging the gap between prediction and understanding of information cascades. We verify the effectiveness of DeepHawkes by applying it to predict retweet cascades of Sina Weibo and citation cascades of a longitudinal citation dataset. Experimental results demonstrate that DeepHawkes outperforms both feature-based and generative approaches.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1149–1158},
numpages = {10},
keywords = {hawkes process, end-to-end deep learning, popularity prediction, interpretable factors, information cascade},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132962,
author = {Hu, Meng and Li, Zhixu and Shen, Yongxin and Liu, An and Liu, Guanfeng and Zheng, Kai and Zhao, Lei},
title = {CNN-IETS: A CNN-Based Probabilistic Approach for Information Extraction by Text Segmentation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132962},
doi = {10.1145/3132847.3132962},
abstract = {Information Extraction by Text Segmentation (IETS) aims at segmenting text inputs to extract implicit data values contained in them.The state-of-art IETS approaches mainly rely on machine learning techniques, either supervised or unsupervised.However, while the supervised approaches require a large labelled training data, the performance of the unsupervised ones could be unstable on different data sets.To overcome their weaknesses, this paper introduces CNN-IETS, a novel unsupervised probabilistic approach that takes the advantages of pre-existing data and a Convolution Neural Network (CNN)-based probabilistic classification model. While using the CNN model can ease the burden of selecting high-quality features in associating text segments with attributes of a given domain, the pre-existing data as a domain knowledge base can provide training data with a comprehensive list of features for building the CNN model.Given an input text, we do initial segmentation (according to the occurrences of these words in the knowledge base) to generate text segments for CNN classification with probabilities. Then, based on the probabilistic CNN classification results, we work on finding the most probable labelling way to the whole input text.As a complementary, a bidirectional sequencing model learned on-demand from test data is finally deployed to do further adjustment to some problematic labelled segments.Our experimental study conducted on several real data collections shows that CNN-IETS improves the extraction quality of state-of-art approaches by more than 10%.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1159–1168},
numpages = {10},
keywords = {convolution neural network, iets, information extraction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132859,
author = {Liu, Zitao and Hauskrecht, Milos},
title = {A Personalized Predictive Framework for Multivariate Clinical Time Series via Adaptive Model Selection},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132859},
doi = {10.1145/3132847.3132859},
abstract = {Building of an accurate predictive model of clinical time series for a patient is critical for understanding of the patient condition, its dynamics, and optimal patient management. Unfortunately, this process is not straightforward. First, patient-specific variations are typically large and population-based models derived or learned from many different patients are often unable to support accurate predictions for each individual patient. Moreover, time series observed for one patient at any point in time may be too short and insufficient to learn a high-quality patient-specific model just from the patient's own data. To address these problems we propose, develop and experiment with a new adaptive forecasting framework for building multivariate clinical time series models for a patient and for supporting patient-specific predictions. The framework relies on the adaptive model switching approach that at any point in time selects the most promising time series model out of the pool of many possible models, and consequently, combines advantages of the population, patient-specific and short-term individualized predictive models. We demonstrate that the adaptive model switching framework is very promising approach to support personalized time series prediction, and that it is able to outperform predictions based on pure population and patient-specific models, as well as, other patient-specific model adaptation strategies.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1169–1177},
numpages = {9},
keywords = {multivariate time series, personalized medicine, forecasting},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132924,
author = {Kim, Yejin and Choi, Jingyun and Chong, Yosep and Jiang, Xiaoqian and Yu, Hwanjo},
title = {DiagTree: Diagnostic Tree for Differential Diagnosis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132924},
doi = {10.1145/3132847.3132924},
abstract = {Differential diagnosis is detection of one disease among similar diseases using evidence such as pathologic tests. A Partially Observed Markov Decision Process (POMDP) formulates the complex differential diagnosis process into a probabilistic decision-making model. However, differential diagnosis is not often fully formulated as POMDP because model construction does not consider the cost (or time) to finish the diagnosis process, or the practical convention on clinical tests. We propose a Diagnostic Tree (DiagTree), a new framework for diagnosing diseases, which combines several tests to reduce the diagnosis time and to incorporate real-world constraints into discrete optimization. DiagTree consists of multiple tests in internal nodes and posterior probabilities ("confidences") that the patient suffers the disease listed at each leaf node. The confidences are computed after a series of test results is applied in internal nodes. DiagTree is built to maximize the confidences at leaf nodes and to minimize the decision process time. We formulate this problem as integer programming and solve it by the Branch-and-Bound method and a greedy approach. We apply DiagTree to immunohistochemistry profiles to detect lymphoid neoplasms. We evaluate the accuracy and cost of the diagnosis rules from DiagTree compared to those obtained using rules that clinicians derived from their experience. DiagTree detected diseases with high accuracy and also reduced the diagnosis cost (or time) compared to the existing rules of clinicians. DiagTree can support clinicians by suggesting a simple diagnosis process with high accuracy and low cost among test candidates.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1179–1188},
numpages = {10},
keywords = {decision process, early diagnosis, discrete optimization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133022,
author = {Ni, Jiazhi and Liu, Jie and Zhang, Chenxin and Ye, Dan and Ma, Zhirou},
title = {Fine-Grained Patient Similarity Measuring Using Deep Metric Learning},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133022},
doi = {10.1145/3132847.3133022},
abstract = {Patient similarity measuring plays a significant role in many healthcare applications, such as cohort study and treatment comparative effectiveness research. Existing methods mainly rely on supervised metric learning method to study patient similarity from Electronic Health Records (EHRs), facing the challenge of differentiating patients with a large number of fine-grained disease categories. Deep metric learning has gained noticeable success in fine-grained image categorization problem, however, it cannot be directly applied to classification of patients with hierarchical disease labels. In this paper, we present a novel three layer patient similarity deep metric learning framework (PSDML) by optimizing quadruple loss improved from triplet loss, to learn an embedding distance for disease classification among the patients. The context semantic relation of multi diagnosis labels encoding by ICD-10 is taken into account to compute the supervised distance of patients. To solve the diagnosis class imbalance, patient tuples that violate deep metric learning framework loss constraints are chosen prior as samples to accelerate the convergence of the neural network. We conducted KNN multi label classification experiment using the learned similarity metric on the real EHRs about stroke disease collected by Chinese Stroke Data Center. The results demonstrate substantial improvement over the baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1189–1198},
numpages = {10},
keywords = {Multi Label Classification, Distance Metric Learning, Patient Similarity, Deep Metric Learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132928,
author = {Nguy\^{e}n, Th\^{o}ng T. and Hui, Siu Cheung},
title = {Differentially Private Regression for Discrete-Time Survival Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132928},
doi = {10.1145/3132847.3132928},
abstract = {In survival analysis, regression models are used to understand the effects of explanatory variables (e.g., age, sex, weight, etc.) to the survival probability. However, for sensitive survival data such as medical data, there are serious concerns about the privacy of individuals in the data set when medical data is used to fit the regression models. The closest work addressing such privacy concerns is the work on Cox regression which linearly projects the original data to a lower dimensional space. However, the weakness of this approach is that there is no formal privacy guarantee for such projection. In this work, we aim to propose solutions for the regression problem in survival analysis with the protection of differential privacy which is a golden standard of privacy protection in data privacy research. To this end, we extend the Output Perturbation and Objective Perturbation approaches which are originally proposed to protect differential privacy for the Empirical Risk Minimization (ERM) problems. In addition, we also propose a novel sampling approach based on the Markov Chain Monte Carlo (MCMC) method to practically guarantee differential privacy with better accuracy. We show that our proposed approaches achieve good accuracy as compared to the non-private results while guaranteeing differential privacy for individuals in the private data set.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1199–1208},
numpages = {10},
keywords = {discrete-time models, regression models, differential privacy, survival analysis},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132998,
author = {Wang, Huandong and Gao, Chen and Li, Yong and Zhang, Zhi-Li and Jin, Depeng},
title = {From Fingerprint to Footprint: Revealing Physical World Privacy Leakage by Cyberspace Cookie Logs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132998},
doi = {10.1145/3132847.3132998},
abstract = {It is well-known that online services resort to various cookies to track users through users' online service identifiers (IDs) - in other words, when users access online services, various "fingerprints" are left behind in the cyberspace. As they roam around in the physical world while accessing online services via mobile devices, users also leave a series of "footprints" -- i.e., hints about their physical locations - in the physical world. This poses a potent new threat to user privacy: one can potentially correlate the "fingerprints" left by the users in the cyberspace with "footprints" left in the physical world to infer and reveal leakage of user physical world privacy, such as frequent user locations or mobility trajectories in the physical world - we refer to this problem as user physical world privacy leakage via user cyberspace privacy leakage. In this paper we address the following fundamental question: what kind - and how much - of user physical world privacy might be leaked if we could get hold of such diverse network datasets even without any physical location information. In order to conduct an in-depth investigation of these questions, we utilize the network data collected via a DPI system at the routers within one of the largest Internet operator in Shanghai, China over a duration of one month. We decompose the fundamental question into the three problems: i) linkage of various online user IDs belonging to the same person via mobility pattern mining; ii) physical location classification via aggregate user mobility patterns over time; and iii) tracking user physical mobility. By developing novel and effective methods for solving each of these problems, we demonstrate that the question of user physical world privacy leakage via user cyberspace privacy leakage is not hypothetical, but indeed poses a real potent threat to user privacy.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1209–1218},
numpages = {10},
keywords = {cookie, cyber-physical systems, trajectories, privacy},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132990,
author = {Lyu, Lingjuan and He, Xuanli and Law, Yee Wei and Palaniswami, Marimuthu},
title = {Privacy-Preserving Collaborative Deep Learning with Application to Human Activity Recognition},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132990},
doi = {10.1145/3132847.3132990},
abstract = {The proliferation of wearable devices has contributed to the emergence of mobile crowdsensing, which leverages the power of the crowd to collect and report data to a third party for large-scale sensing and collaborative learning. However, since the third party may not be honest, privacy poses a major concern. In this paper, we address this concern with a two-stage privacy-preserving scheme called RG-RP: the first stage is designed to mitigate maximum a posteriori (MAP) estimation attacks by perturbing each participant's data through a nonlinear function called repeated Gompertz (RG); while the second stage aims to maintain accuracy and reduce transmission energy by projecting high-dimensional data to a lower dimension, using a row-orthogonal random projection (RP) matrix. The proposed RG-RP scheme delivers better recovery resistance to MAP estimation attacks than most state-of-the-art techniques on both synthetic and real-world datasets. For collaborative learning, we proposed a novel LSTM-CNN model combining the merits of Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN). Our experiments on two representative movement datasets captured by wearable sensors demonstrate that the proposed LSTM-CNN model outperforms standalone LSTM, CNN and Deep Belief Network. Together, RG+RP and LSTM-CNN provide a privacy-preserving collaborative learning framework that is both accurate and privacy-preserving.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1219–1228},
numpages = {10},
keywords = {privacy-preserving, deep learning, mobile crowdsensing, collaborative learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132970,
author = {Mondal, Sutapa and Shukla, Manish and Lodha, Sachin},
title = {Privacy Aware Temporal Profiling of Emails in Distributed Setup},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132970},
doi = {10.1145/3132847.3132970},
abstract = {The enterprise email promises to be a rich source for knowledge discovery. This is made possible due to the direct nature of communication, support for diverse media types, active participation of entities and presence of chronological ordering of messages. Also, the enterprise emails are more trustworthy than external emails due to their formal nature. This data source has not been fully tapped. In fact, the existing work on profiling of emails focuses primarily on expertise identification and retrieval. Even in these studies, the researchers have made some restrictive assumptions. For instance, in many of the formulations, the underlying system assumes a centralized data repository, and the communication network is complete. They do not account for individual biases in an email while mining and aggregating results. Furthermore, email holds fair amount of personal and organizational sensitive information. None of the existing work on email profiling suggests anything on alleviating the individual and organizational privacy concerns.In this paper, we propose a system for building an individual's perceived knowledge profile "What she knows?" ), trends profile "In which direction and how far her expertise has grown?" ), and team profile "What all her teammates know?"). The proposed system operates in a distributed network and performs analysis of emails residing on a time-varying local email database, with no prior assumptions about the environment. It also takes care of missing nodes in a partial communication network, by deducing their profile from perceived profiles of its peers and their common interest. We developed a two-pass aggregation algorithm for combining results from individual nodes and drawing useful insights. A graph based algorithm is used for calculating spread (reach) and popularity (recall) for further improving the output of the aggregation algorithm. The results show that the two pass aggregation step is sufficient in majority of the cases, and a hybrid of email content and graph-based approach works well in a distributed setup.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1229–1238},
numpages = {10},
keywords = {information extraction, distributed retrieval, temporal profiling, enterprise search, data privacy},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132873,
author = {Zhang, Baichuan and Al Hasan, Mohammad},
title = {Name Disambiguation in Anonymized Graphs Using Network Embedding},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132873},
doi = {10.1145/3132847.3132873},
abstract = {In real-world, our DNA is unique but many people share names. This phenomenon often causes erroneous aggregation of documents of multiple persons who are namesake of one another. Such mistakes deteriorate the performance of document retrieval, web search, and more seriously, cause improper attribution of credit or blame in digital forensic. To resolve this issue, the name disambiguation task is designed which aims to partition the documents associated with a name reference such that each partition contains documents pertaining to a unique real-life person. Existing solutions to this task substantially rely on feature engineering, such as biographical feature extraction, or construction of auxiliary features from Wikipedia. However, for many scenarios, such features may be costly to obtain or unavailable due to the risk of privacy violation. In this work, we propose a novel name disambiguation method. Our proposed method is non-intrusive of privacy because instead of using attributes pertaining to a real-life person, our method leverages only relational data in the form of anonymized graphs. In the methodological aspect, the proposed method uses a novel representation learning model to embed each document in a low dimensional vector space where name disambiguation can be solved by a hierarchical agglomerative clustering algorithm. Our experimental results demonstrate that the proposed method is significantly better than the existing name disambiguation methods working in a similar setting.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1239–1248},
numpages = {10},
keywords = {name disambiguation, clustering, neural network embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133020,
author = {Dong, Rui and Sun, Yizhou and Wang, Lu and Gu, Yupeng and Zhong, Yuan},
title = {Weakly-Guided User Stance Prediction via Joint Modeling of Content and Social Interaction},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133020},
doi = {10.1145/3132847.3133020},
abstract = {Social media websites have become a popular outlet for online users to express their opinions on controversial issues, such as gun control and abortion. Understanding users' stances and their arguments is a critical task for policy-making process and public deliberation. Existing methods rely on large amounts of human annotation for predicting stance on issues of interest, which is expensive and hard to scale to new problems. In this work, we present a weakly-guided user stance modeling framework which simultaneously considers two types of information: what do you say (via stance-based content generative model) and how do you behave (via social interaction-based graph regularization). We experiment with two types of social media data: news comments and discussion forum posts. Our model uniformly outperforms a logistic regression-based supervised method on stance-based link prediction for unseen users on news comments. Our method also achieves better or comparable stance prediction performance for discussion forum users, when compared with state-of-the-art supervised systems. Meanwhile, separate word distributions are learned for users of opposite stances. This potentially helps with better understanding and interpretation of conflicting arguments for controversial issues.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1249–1258},
numpages = {10},
keywords = {social media, user stance prediction, online behavior mining, social computing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132857,
author = {Fan, Yujie and Zhang, Yiming and Ye, Yanfang and li, Xin and Zheng, Wanhong},
title = {Social Media for Opioid Addiction Epidemiology: Automatic Detection of Opioid Addicts from Twitter and Case Studies},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132857},
doi = {10.1145/3132847.3132857},
abstract = {Opioid (e.g., heroin and morphine) addiction has become one of the largest and deadliest epidemics in the United States. To combat such deadly epidemic, there is an urgent need for novel tools and methodologies to gain new insights into the behavioral processes of opioid abuse and addiction. The role of social media in biomedical knowledge mining has turned into increasingly significant in recent years. In this paper, we propose a novel framework named AutoDOA to automatically detect the opioid addicts from Twitter, which can potentially assist in sharpening our understanding toward the behavioral process of opioid abuse and addiction. In AutoDOA, to model the users and posted tweets as well as their rich relationships, a structured heterogeneous information network (HIN) is first constructed. Then meta-path based approach is used to formulate similarity measures over users and different similarities are aggregated using Laplacian scores. Based on HIN and the combined meta-path, to reduce the cost of acquiring labeled examples for supervised learning, a transductive classification model is built for automatic opioid addict detection. To the best of our knowledge, this is the first work to apply transductive classification in HIN into drug-addiction domain. Comprehensive experiments on real sample collections from Twitter are conducted to validate the effectiveness of our developed system AutoDOA in opioid addict detection by comparisons with other alternate methods. The results and case studies also demonstrate that knowledge from daily-life social media data mining could support a better practice of opioid addiction prevention and treatment.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1259–1267},
numpages = {9},
keywords = {heterogeneous information network, opioid addict detection, transductive classification, social media},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133019,
author = {Wang, Zhiwei and Derr, Tyler and Yin, Dawei and Tang, Jiliang},
title = {Understanding and Predicting Weight Loss with Mobile Social Networking Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133019},
doi = {10.1145/3132847.3133019},
abstract = {It has become increasingly popular to use mobile social networking applications for weight loss and management. Users can not only create profiles and maintain their records but also perform a variety of social activities that shatter the barrier to share or seek information. Due to the open and connected nature, these applications produce massive data that consists of rich weight-related information which offers immense opportunities for us to enable advanced research on weight loss. In this paper, we conduct the initial investigation to understand weight loss with a large-scale mobile social networking dataset with near 10 million users. In particular, we study individual and social factors related to weight loss and reveal a number of interesting findings that help us build a meaningful model to predict weight loss automatically. The experimental results demonstrate the effectiveness of the proposed model and the significance of social factors in weight loss.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1269–1278},
numpages = {10},
keywords = {social network analysis, weight loss, mobile applications},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132906,
author = {Chong, Wen-Haw and Lim, Ee-Peng},
title = {Tweet Geolocation: Leveraging Location, User and Peer Signals},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132906},
doi = {10.1145/3132847.3132906},
abstract = {Which venue is a tweet posted from? We referred this as fine-grained geolocation. To solve this problem effectively, we develop novel techniques to exploit each posting user's content history. This is motivated by our finding that most users do not share their visitation history, but have ample content history from tweet posts.  We formulate fine-grained geolocation as a ranking problem whereby given a test tweet, we rank candidate venues. We propose several models that leverage on three types of signals from locations, users and peers. Firstly, the location signals are words that are indicative of venues. We propose a location-indicative weighting scheme to capture this. Next we exploit user signals from each user's content history to enrich the very limited content of their tweets which have been targeted for geolocation. The intuition is that the user's other tweets may have been from the test venue or related venues, thus providing informative words. In this regard, we propose query expansion as the enrichment approach. Finally, we exploit the signals from peer users who have similar content history and thus potentially similar visitation behavior as the users of the test tweets. This suggests collaborative filtering where visitation information is propagated via content similarities. We proposed several models incorporating different combinations of the three signals. Our experiments show that the best model incorporates all three signals. It performs 6% to 40% better than the baselines depending on the metric and dataset.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1279–1288},
numpages = {10},
keywords = {collaborative filtering, query expansion, tweet geolocation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132940,
author = {Liu, Bin and Zhang, Min and Ma, Weizhi and Li, Xin and Liu, Yiqun and Ma, Shaoping},
title = {A Two-Step Information Accumulation Strategy for Learning from Highly Imbalanced Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132940},
doi = {10.1145/3132847.3132940},
abstract = {Highly imbalanced data is common in the real world and it is important but difficult to train an effective classifier. In this paper, Our major point is that the imbalance is the observed phenomenon but not the cause of the problem. The challenge is that useful information is been overshadowed in the large scale of data in both majority and minority classes. We propose a novel two-step strategy, Information Accumulation, which first selects the most discriminative data by the Zooming-in phase, and then leverages unlabeled data by pseudo active learning and self-training in the phase of Learning from Learned Results. Comparative experiments are conducted on large-scale highly imbalanced real customer service data on complaint detection task (where less than 2% of data is positive). The results on eight state-of-the-art classification algorithms show that significant improvements are observed on the performances of all algorithms with Information Accumulation(for example, the F-Measure score of Xgboost is increased by 197% from 0.115 to 0.347), which demonstrates the effectiveness and general applicability of the proposed strategy. This work explores a new idea on dealing with highly imbalanced data that we do not aim to balance the training examples as usual, but focus on finding the most discriminative information from labeled data and the learning results of unlabeled data.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1289–1298},
numpages = {10},
keywords = {imbalanced learning, complaint call detection, text classification},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132954,
author = {Yan, Cong and Cheung, Alvin and Yang, Junwen and Lu, Shan},
title = {Understanding Database Performance Inefficiencies in Real-World Web Applications},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132954},
doi = {10.1145/3132847.3132954},
abstract = {Many modern database-backed web applications are built upon Object Relational Mapping (ORM) frameworks. While such frame- works ease application development by abstracting persistent data as objects, such convenience comes with a performance cost. In this paper, we studied 27 real-world open-source applications built on top of the popular Ruby on Rails ORM framework, with the goal to understand the database-related performance inefficiencies in these applications. We discovered a number of inefficiencies rang- ing from physical design issues to how queries are expressed in the application code. We applied static program analysis to identify and measure how prevalent these issues are, then suggested techniques to alleviate these issues and measured the potential performance gain as a result. These techniques significantly reduce database query time (up to 91%) and the webpage response time (up to 98%). Our study provides guidance to the design of future database engines and ORM frameworks to support database application that are performant yet without sacrificing programmability.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1299–1308},
numpages = {10},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132860,
author = {Vu, Hoang Dung and Chai, Kok Soon and Keating, Bryan and Tursynbek, Nurislam and Xu, Boyan and Yang, Kaige and Yang, Xiaoyan and Zhang, Zhenjie},
title = {Data Driven Chiller Plant Energy Optimization with Domain Knowledge},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132860},
doi = {10.1145/3132847.3132860},
abstract = {Refrigeration and chiller optimization is an important and well studied topic in mechanical engineering, mostly taking advantage of physical models, designed on top of over-simplified assumptions, over the equipments. Conventional optimization techniques using physical models make decisions of online parameter tuning, based on very limited information of hardware specifications and external conditions, e.g., outdoor weather. In recent years, new generation of sensors is becoming essential part of new chiller plants, for the first time allowing the system administrators to continuously monitor the running status of all equipments in a timely and accurate way. The explosive growth of data flowing to databases, driven by the increasing analytical power by machine learning and data mining, unveils new possibilities of data-driven approaches for real-time chiller plant optimization. This paper presents our research and industrial experience on the adoption of data models and optimizations on chiller plant and discusses the lessons learnt from our practice on real world plants. Instead of employing complex machine learning models, we emphasize the incorporation of appropriate domain knowledge into data analysis tools, which turns out to be the key performance improver over state-of-the-art deep learning techniques by a significant margin. Our empirical evaluation on a real world chiller plant achieves savings by more than 7% on daily power consumption.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1309–1317},
numpages = {9},
keywords = {optimization, energy saving, data driven, chiller plant},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132903,
author = {Gollapudi, Sreenivas and Kumar, Ravi and Panigrahy, Debmalya and Panigrahy, Rina},
title = {Partitioning Orders in Online Shopping Services},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132903},
doi = {10.1145/3132847.3132903},
abstract = {The rapid growth of the Internet has led to the widespread use of newer and richer models of online shopping and delivery services. The race to efficient large scale on-demand delivery has transformed such services into complex networks of shoppers (typically working in the stores), stores, and consumers. The efficiency of processing orders in stores is critical to the profitability of the business model. Motivated by this setting, we consider the following problem: given a set of shopping orders each consisting of a few items, how to best partition the orders among a given number of shoppers working for an online shopping service? Formulating this as an optimization problem, we propose a family of simple and efficient algorithms that admit natural constraints such as number of items a shopper can process in this setting. In addition to showing provable guarantees for the algorithms, we also demonstrate their efficiency in practice on real-world data, outperforming strong baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1319–1328},
numpages = {10},
keywords = {order partitioning, e-commerce, vehicle routing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133041,
author = {Gupta, Amit and Lebret, R\'{e}mi and Harkous, Hamza and Aberer, Karl},
title = {Taxonomy Induction Using Hypernym Subsequences},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133041},
doi = {10.1145/3132847.3133041},
abstract = {We propose a novel, semi-supervised approach towards domain taxonomy induction from an input vocabulary of seed terms. Unlike all previous approaches, which typically extract direct hypernym edges for terms, our approach utilizes a novel probabilistic framework to extract hypernym subsequences. Taxonomy induction from extracted subsequences is cast as an instance of the minimum-cost flow problem on a carefully designed directed graph. Through experiments, we demonstrate that our approach outperforms state-of-the-art taxonomy induction approaches across four languages. Importantly, we also show that our approach is robust to the presence of noise in the input vocabulary. To the best of our knowledge, this robustness has not been empirically proven in any previous approach.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1329–1338},
numpages = {10},
keywords = {taxonomy induction, flow networks, knowledge acquisition, algorithms, minimum-cost flow optimization, term taxonomies},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133023,
author = {Krishnan, Adit and Sankar, Aravind and Zhi, Shi and Han, Jiawei},
title = {Unsupervised Concept Categorization and Extraction from Scientific Document Titles},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133023},
doi = {10.1145/3132847.3133023},
abstract = {This paper studies the automated categorization and extraction of scientific concepts from titles of scientific articles, in order to gain a deeper understanding of their key contributions and facilitate the construction of a generic academic knowledgebase. Towards this goal, we propose an unsupervised, domain-independent, and scalable two-phase algorithm to type and extract key concept mentions into aspects of interest (e.g., Techniques, Applications, etc.). In the first phase of our algorithm we proposePhraseType, a probabilistic generative model which exploits textual features and limited POS tags to broadly segment text snippets into aspect-typed phrases. We extend this model to simultaneously learn aspect-specific features and identify academic domains in multi-domain corpora, since the two tasks mutually enhance each other. In the second phase, we propose an approach based on adaptor grammars to extract fine grained concept mentions from the aspect-typed phrases without the need for any external resources or human effort, in a purely data-driven manner. We apply our technique to study literature from diverse scientific domains and show significant gains over state-of-the-art concept extraction techniques. We also present a qualitative analysis of the results obtained.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1339–1348},
numpages = {10},
keywords = {concept extraction, adaptor grammar, probabilistic model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132956,
author = {Zhang, Yuxiang and Chang, Yaocheng and Liu, Xiaoqing and Gollapalli, Sujatha Das and Li, Xiaoli and Xiao, Chunjing},
title = {MIKE: Keyphrase Extraction by Integrating Multidimensional Information},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132956},
doi = {10.1145/3132847.3132956},
abstract = {Traditional supervised keyphrase extraction models depend on the features of labelled keyphrases while prevailing unsupervised models mainly rely on structure of the word graph, with candidate words as nodes and edges capturing the co-occurrence information between words. However, systematically integrating all these multidimensional heterogeneous information into a unified model is relatively unexplored. In this paper, we focus on how to effectively exploit multidimensional information to improve the keyphrase extraction performance (MIKE). Specifically, we propose a random-walk parametric model, MIKE, that learns the latent representation for a candidate keyphrase that captures the mutual influences among all information, and simultaneously optimizes the parameters and ranking scores of candidates in the word graph. We use the gradient-descent algorithm to optimize our model and show the comprehensive experiments with two publicly-available WWW and KDD datasets in Computer Science. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art graph-based keyphrase extraction approaches.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1349–1358},
numpages = {10},
keywords = {parametric model, graph-based keyphrase extraction approach, keyphrase extraction, multidimensional information},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132934,
author = {Tang, Yixuan and Huang, Weilong and Liu, Qi and Tung, Anthony K.H. and Wang, Xiaoli and Yang, Jisong and Zhang, Beibei},
title = {QALink: Enriching Text Documents with Relevant Q&amp;A Site Contents},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132934},
doi = {10.1145/3132847.3132934},
abstract = {With rapid development of Q&amp;A sites such as Quora and StackExchange, high quality question-answer pairs have been produced by users. These Q&amp;A contents cover a wide range of topics, and they are useful for users to resolve queries and obtain new knowledge. Meanwhile, when people are reading digital documents, they may encounter reading problems such as lack of background information and unclear illustration of concepts. We believe that Q&amp;A sites offer high-quality contents which can serve as rich supplements to digital documents. In this paper, we devise a rigorous formulation of the novel text enrichment problem, and design an end-to-end system named QALink which assigns the most relevant Q&amp;A contents to the corresponding section of the document. We first present a new segmentation approach to model each document with a hierarchical structure. Based on the hierarchy, queries are constructed to retrieve and rank related question-answer pairs. Both syntactical and semantic features are adopted in our system. The empirical evaluation results indicate that QALink is able to effectively enrich text documents with relevant Q&amp;A contents to help people better understand the documents.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1359–1368},
numpages = {10},
keywords = {q&amp;a sites, hierarchical text segmentation, text enrichment, learning to rank, probabilistic information retrieval},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132952,
author = {Zheng, Yanan and Wen, Lijie and Wang, Jianmin and Yan, Jun and Ji, Lei},
title = {Sequence Modeling with Hierarchical Deep Generative Models with Dual Memory},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132952},
doi = {10.1145/3132847.3132952},
abstract = {Deep Generative Models (DGMs) are able to extract high-level representations from massive unlabeled data and are explainable from a probabilistic perspective. Such characteristics favor sequence modeling tasks. However, it still remains a huge challenge to model sequences with DGMs. Unlike real-valued data that can be directly fed into models, sequence data consist of discrete elements and require being transformed into certain representations first. This leads to the following two challenges. First, high-level features are sensitive to small variations of inputs as well as the way of representing data. Second, the models are more likely to lose long-term information during multiple transformations. In this paper, we propose a Hierarchical Deep Generative Model With Dual Memory to address the two challenges. Furthermore, we provide a method to efficiently perform inference and learning on the model. The proposed model extends basic DGMs with an improved hierarchically organized multi-layer architecture. Besides, our model incorporates memories along dual directions, respectively denoted as broad memory and deep memory. The model is trained end-to-end by optimizing a variational lower bound on data log-likelihood using the improved stochastic variational method. We perform experiments on several tasks with various datasets and obtain excellent results. The results of language modeling show our method significantly outperforms state-of-the-art results in terms of generative performance. Extended experiments including document modeling and sentiment analysis, prove the high-effectiveness of dual memory mechanism and latent representations. Text random generation provides a straightforward perception for advantages of our model.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1369–1378},
numpages = {10},
keywords = {inference and learning, hierarchical deep generative models, dual memory mechanism, sequence modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132949,
author = {Qian, Kun and Popa, Lucian and Sen, Prithviraj},
title = {Active Learning for Large-Scale Entity Resolution},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132949},
doi = {10.1145/3132847.3132949},
abstract = {Entity resolution (ER) is the task of identifying different representations of the same real-world object across datasets. Designing and tuning ER algorithms is an error-prone, labor-intensive process, which can significantly benefit from data-driven, automated learning methods. Our focus is on "big data'' scenarios where the primary challenges include 1) identifying, out of a potentially massive set, a small subset of informative examples to be labeled by the user, 2) using the labeled examples to efficiently learn ER algorithms that achieve both high precision and high recall, and 3) executing the learned algorithm to determine duplicates at scale. Recent work on learning ER algorithms has employed active learning to partially address the above challenges by aiming to learn ER rules in the form of conjunctions of matching predicates, under precision guarantees. While successful in learning a single rule, prior work has been less successful in learning multiple rules that are sufficiently different from each other, thus missing opportunities for improving recall. In this paper, we introduce an active learning system that learns, at scale, multiple rules each having significant coverage of the space of duplicates, thus leading to high recall, in addition to high-precision. We show the superiority of our system on real-world ER scenarios of sizes up to tens of millions of records, over state-of-the-art active learning methods that learn either rules or committees of statistical classifiers for ER, and even over sophisticated methods based on first-order probabilistic models.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1379–1388},
numpages = {10},
keywords = {large-scale data cleansing, entity resolution},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132913,
author = {Le, Dung D. and Lauw, Hady W.},
title = {Indexable Bayesian Personalized Ranking for Efficient Top-k Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132913},
doi = {10.1145/3132847.3132913},
abstract = {Top-k recommendation seeks to deliver a personalized recommendation list of k items to a user. The dual objectives are (1) accuracy in identifying the items a user is likely to prefer, and (2) efficiency in constructing the recommendation list in real time. One direction towards retrieval efficiency is to formulate retrieval as approximate k nearest neighbor (kNN) search aided by indexing schemes, such as locality-sensitive hashing, spatial trees, and inverted index. These schemes, applied on the output representations of recommendation algorithms, speed up the retrieval process by automatically discarding a large number of potentially irrelevant items when given a user query vector. However, many previous recommendation algorithms produce representations that may not necessarily align well with the structural properties of these indexing schemes, eventually resulting in a significant loss of accuracy post-indexing. In this paper, we introduce Indexable Bayesian Personalized Ranking (IBPR) that learns from ordinal preference to produce representation that is inherently compatible with the aforesaid indices. Experiments on publicly available datasets show superior performance of the proposed model compared to state-of-the-art methods on top-k recommendation retrieval task, achieving significant speedup while maintaining high accuracy.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1389–1398},
numpages = {10},
keywords = {top-k recommendation, retrieval efficiency, indexing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132865,
author = {Grover, Aman and Arya, Dhruv and Venkataraman, Ganesh},
title = {Latency Reduction via Decision Tree Based Query Construction},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132865},
doi = {10.1145/3132847.3132865},
abstract = {LinkedIn as a professional network serves the career needs of 450 Million plus members. The task of job recommendation system is to nd the suitable job among a corpus of several million jobs and serve this in real time under tight latency constraints. Job search involves nding suitable job listings given a user, query and context. Typical scoring function for both search and recommendations involves evaluating a function that matches various elds in the job description with various elds in the member pro le. This in turn translates to evaluating a function with several thousands of features to get the right ranking. In recommendations, evaluating all the jobs in the corpus for all members is not possible given the latency constraints. On the other hand, reducing the candidate set could potentially involve loss of relevant jobs. We present a way to model the underlying complex ranking function via decision trees. The branches within the decision trees are query clauses and hence the decision trees can be mapped on to real time queries. We developed an o ine framework which evaluates the quality of the decision tree with respect to latency and recall. We tested the approach on job search and recommendations on LinkedIn and A/B tests show signi cant improvements in member engagement and latency. Our techniques helped reduce job search latency by over 67% and our recommendations latency by over 55%. Our techniques show 3.5% improvement in applications from job recommendations primarily due to reduced timeouts from upstream services. As of writing the approach powers all of job search and recommendations on LinkedIn.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1399–1407},
numpages = {9},
keywords = {information retrieval, recommender systems, personalized search},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132976,
author = {Zhu, Junxing and Zhang, Jiawei and He, Lifang and Wu, Quanyuan and Zhou, Bin and Zhang, Chenwei and Yu, Philip S.},
title = {Broad Learning Based Multi-Source Collaborative Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132976},
doi = {10.1145/3132847.3132976},
abstract = {Anchor links connect information entities, such as entities of movies or products, across networks from different sources, and thus information in these networks can be transferred directly via anchor links. Therefore, anchor links have great value to many cross-network applications, such as cross-network social link prediction and cross-network recommendation. In this paper, we focus on studying the recommendation problem that can provide ratings of items or services. To address the problem, we propose a Cross-network Collaborative Matrix Factorization (CCMF) recommendation framework based on broad learning setting, which can effectively integrate multi-source information and alleviate the sparse information problem in each individual network. Based on item anchor links CCMF can fuse item similarity information and item latent information across networks from different sources. And different from most of the traditional works, CCMF can make multi-source recommendation tasks collaborate together via the information transfer based on the broad learning setting. During the transfer process, a novel cross-network similarity transfer method is applied to keep the consistency of item similarities between two different networks, and a domain adaptation matrix is used to overcome the domain difference problem. We conduct experiments to compare the proposed CCMF method with both classic and state-of-the-art recommendation techniques. The experimental results illustrate that CCMF outperforms other methods in different experimental circumstances, and has great advantages on dealing with different data sparse problems.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1409–1418},
numpages = {10},
keywords = {anchor links, recommendation, matrix factorization, cross-network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132926,
author = {Li, Jing and Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun and Lian, Tao and Ma, Jun},
title = {Neural Attentive Session-Based Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132926},
doi = {10.1145/3132847.3132926},
abstract = {Given e-commerce scenarios that user profiles are invisible, session-based recommendation is proposed to generate recommendation results from short sessions. Previous work only considers the user's sequential behavior in the current session, whereas the user's main purpose in the current session is not emphasized. In this paper, we propose a novel neural networks framework, i.e., Neural Attentive Recommendation Machine (NARM), to tackle this problem. Specifically, we explore a hybrid encoder with an attention mechanism to model the user's sequential behavior and capture the user's main purpose in the current session, which are combined as a unified session representation later. We then compute the recommendation scores for each candidate item with a bi-linear matching scheme based on this unified session representation. We train NARM by jointly learning the item and session representations as well as their matchings. We carried out extensive experiments on two benchmark datasets. Our experimental results show that NARM outperforms state-of-the-art baselines on both datasets. Furthermore, we also find that NARM achieves a significant improvement on long sessions, which demonstrates its advantages in modeling the user's sequential behavior and main purpose simultaneously.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1419–1428},
numpages = {10},
keywords = {sequential behavior, session-based recommendation, recurrent neural networks, attention mechanism},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133036,
author = {Manotumruksa, Jarana and Macdonald, Craig and Ounis, Iadh},
title = {A Deep Recurrent Collaborative Filtering Framework for Venue Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133036},
doi = {10.1145/3132847.3133036},
abstract = {Venue recommendation is an important application for Location-Based Social Networks (LBSNs), such as Yelp, and has been extensively studied in recent years. Matrix Factorisation (MF) is a popular Collaborative Filtering (CF) technique that can suggest relevant venues to users based on an assumption that similar users are likely to visit similar venues. In recent years, deep neural networks have been successfully applied to tasks such as speech recognition, computer vision and natural language processing. Building upon this momentum, various approaches for recommendation have been proposed in the literature to enhance the effectiveness of MF-based approaches by exploiting neural network models such as: word embeddings to incorporate auxiliary information (e.g. textual content of comments); and Recurrent Neural Networks (RNN) to capture sequential properties of observed user-venue interactions. However, such approaches rely on the traditional inner product of the latent factors of users and venues to capture the concept of collaborative filtering, which may not be sufficient to capture the complex structure of user-venue interactions. In this paper, we propose a Deep Recurrent Collaborative Filtering framework (DRCF) with a pairwise ranking function that aims to capture user-venue interactions in a CF manner from sequences of observed feedback by leveraging Multi-Layer Perception and Recurrent Neural Network architectures. Our proposed framework consists of two components: namely Generalised Recurrent Matrix Factorisation (GRMF) and Multi-Level Recurrent Perceptron (MLRP) models. In particular, GRMF and MLRP learn to model complex structures of user-venue interactions using element-wise and dot products as well as the concatenation of latent factors. In addition, we propose a novel sequence-based negative sampling approach that accounts for the sequential properties of observed feedback and geographical location of venues to enhance the quality of venue suggestions, as well as alleviate the cold-start users problem. Experiments on three large checkin and rating datasets show the effectiveness of our proposed framework by outperforming various state-of-the-art approaches.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1429–1438},
numpages = {10},
keywords = {deep recurrent collaborative filtering framework, dynamic preferences, static preferences},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133034,
author = {Christakopoulou, Konstantina and Kawale, Jaya and Banerjee, Arindam},
title = {Recommendation with Capacity Constraints},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133034},
doi = {10.1145/3132847.3133034},
abstract = {In many recommendation settings, the candidate items for recommendation are associated with a maximum capacity, i.e., number of seats in a Point-of-Interest (POI) or number of item copies in the inventory. However, despite the prevalence of the capacity constraint in the recommendation process, the existing recommendation methods are not designed to optimize for respecting such a constraint. Towards closing this gap, we propose Recommendation with Capacity Constraints -- a framework that optimizes for both recommendation accuracy and expected item usage that respects the capacity constraints. We show how to apply our method to three state-of-the-art latent factor recommendation models: probabilistic matrix factorization (PMF), bayesian personalized ranking (BPR) for item recommendation, and geographical matrix factorization (GeoMF) for POI recommendation. Our experiments indicate that our framework is effective for providing good recommendations while taking the limited resources into consideration. Interestingly, our methods are shown in some cases to further improve the top-N recommendation quality of the respective unconstrained models.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1439–1448},
numpages = {10},
keywords = {capacity constraints, recommendation systems, point-of-interest recommendation, latent factor recommendation, user propensity},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132892,
author = {Zhang, Yongfeng and Ai, Qingyao and Chen, Xu and Croft, W. Bruce},
title = {Joint Representation Learning for Top-N Recommendation with Heterogeneous Information Sources},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132892},
doi = {10.1145/3132847.3132892},
abstract = {The Web has accumulated a rich source of information, such as text, image, rating, etc, which represent different aspects of user preferences. However, the heterogeneous nature of this information makes it difficult for recommender systems to leverage in a unified framework to boost the performance. Recently, the rapid development of representation learning techniques provides an approach to this problem. By translating the various information sources into a unified representation space, it becomes possible to integrate heterogeneous information for informed recommendation.  In this work, we propose a Joint Representation Learning (JRL) framework for top-N recommendation. In this framework, each type of information source (review text, product image, numerical rating, etc) is adopted to learn the corresponding user and item representations based on available (deep) representation learning architectures. Representations from different sources are integrated with an extra layer to obtain the joint representations for users and items. In the end, both the per-source and the joint representations are trained as a whole using pair-wise learning to rank for top-N recommendation. We analyze how information propagates among different information sources in a gradient-descent learning paradigm, based on which we further propose an extendable version of the JRL framework (eJRL), which is rigorously extendable to new information sources to avoid model re-training in practice. By representing users and items into embeddings offline, and using a simple vector multiplication for ranking score calculation online, our framework also has the advantage of fast online prediction compared with other deep learning approaches to recommendation that learn a complex prediction network for online calculation.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1449–1458},
numpages = {10},
keywords = {representation learning, recommender systems, top-n recommendation, heterogeneous information processing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133005,
author = {Pei, Wenjie and Yang, Jie and Sun, Zhu and Zhang, Jie and Bozzon, Alessandro and Tax, David M.J.},
title = {Interacting Attention-Gated Recurrent Networks for Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133005},
doi = {10.1145/3132847.3133005},
abstract = {Capturing the temporal dynamics of user preferences over items is important for recommendation. Existing methods mainly assume that all time steps in user-item interaction history are equally relevant to recommendation, which however does not apply in real-world scenarios where user-item interactions can often happen accidentally. More importantly, they learn user and item dynamics separately, thus failing to capture their joint effects on user-item interactions. To better model user and item dynamics, we present the Interacting Attention-gated Recurrent Network (IARN) which adopts the attention model to measure the relevance of each time step. In particular, we propose a novel attention scheme to learn the attention scores of user and item history in an interacting way, thus to account for the dependencies between user and item dynamics in shaping user-item interactions. By doing so, IARN can selectively memorize different time steps of a user's history when predicting her preferences over different items. Our model can therefore provide meaningful interpretations for recommendation results, which could be further enhanced by auxiliary features. Extensive validation on real-world datasets shows that IARN consistently outperforms state-of-the-art methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1459–1468},
numpages = {10},
keywords = {attention model, user-item interaction, feature-based recommendation, recurrent neural network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132985,
author = {Manotumruksa, Jarana and Macdonald, Craig and Ounis, Iadh},
title = {A Personalised Ranking Framework with Multiple Sampling Criteria for Venue Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132985},
doi = {10.1145/3132847.3132985},
abstract = {Recommending a ranked list of interesting venues to users based on their preferences has become a key functionality in Location-Based Social Networks (LBSNs) such as Yelp and Gowalla. Bayesian Personalised Ranking (BPR) is a popular pairwise recommendation technique that is used to generate the ranked list of venues of interest to a user, by leveraging the user's implicit feedback such as their check-ins as instances of positive feedback, while randomly sampling other venues as negative instances. To alleviate the sparsity that affects the usefulness of recommendations by BPR for users with few check-ins, various approaches have been proposed in the literature to incorporate additional sources of information such as the social links between users, the textual content of comments, as well as the geographical location of the venues. However, such approaches can only readily leverage one source of additional information for negative sampling. Instead, we propose a novel Personalised Ranking Framework with Multiple sampling Criteria (PRFMC) that leverages both geographical influence and social correlation to enhance the effectiveness of BPR. In particular, we apply a multi-centre Gaussian model and a power-law distribution method, to capture geographical influence and social correlation when sampling negative venues, respectively. Finally, we conduct comprehensive experiments using three large-scale datasets from the Yelp, Gowalla and Brightkite LBSNs. The experimental results demonstrate the effectiveness of fusing both geographical influence and social correlation in our proposed PRFMC framework and its superiority in comparison to BPR-based and other similar ranking approaches. Indeed, our PRFMC approach attains a 37% improvement in MRR over a recently proposed approach that identifies negative venues only from social links.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1469–1478},
numpages = {10},
keywords = {negative sampling criterion, social correlation, geographical influences},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132941,
author = {Ding, Daizong and Zhang, Mi and Li, Shao-Yuan and Tang, Jie and Chen, Xiaotie and Zhou, Zhi-Hua},
title = {BayDNN: Friend Recommendation with Bayesian Personalized Ranking Deep Neural Network},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132941},
doi = {10.1145/3132847.3132941},
abstract = {Friendship is the cornerstone to build a social network. In online social networks, statistics show that the leading reason for user to create a new friendship is due to recommendation. Thus the accuracy of recommendation matters. In this paper, we propose a Bayesian Personalized Ranking Deep Neural Network (BayDNN) model for friend recommendation in social networks. With BayDNN, we achieve significant improvement on two public datasets: Epinions and Slashdot. For example, on Epinions dataset, BayDNN significantly outperforms the state-of-the-art algorithms, with a 5% improvement on NDCG over the best baseline. The advantages of the proposed BayDNN mainly come from its underlying convolutional neural network (CNN), which offers a mechanism to extract latent deep structural feature representations of the complicated network data, and a novel Bayesian personalized ranking idea, which precisely captures the users' personal bias based on the extracted deep features. To get good parameter estimation for the neural network, we present a fine-tuned pre-training strategy for the proposed BayDNN model based on Poisson and Bernoulli probabilistic models.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1479–1488},
numpages = {10},
keywords = {probabilistic model, bayesian personalized ranking deep neural network, pre-training strategy},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132942,
author = {Jiang, Haixin and Zhou, Rui and Zhang, Limeng and Wang, Hua and Zhang, Yanchun},
title = {A Topic Model Based on Poisson Decomposition},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132942},
doi = {10.1145/3132847.3132942},
abstract = {Determining appropriate statistical distributions for modeling text corpora is important for accurate estimation of numerical characteristics. Based on the validity of the test on a claim that the data conforms to Poisson distribution we propose Poisson decomposition model (PDM), a statistical model for modeling count data of text corpora, which can straightly capture each document's multidimensional numerical characteristics on topics. In PDM, each topic is represented as a parameter vector with multidimensional Poisson distribution, which can be easily normalized to multinomial term probabilities and each document is represented as measurements on topics and thereby reduced to a measurement vector on topics. We use gradient descent methods and sampling algorithm for parameter estimation. We carry out extensive experiments on the topics produced by our models. The results demonstrate our approach can extract more coherent topics and is competitive in document clustering by using the PDM-based features, compared to PLSI and LDA.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1489–1498},
numpages = {10},
keywords = {topic model, topic coherence, poisson decomposition, text classification, statistical testing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132984,
author = {Wang, Rui and Liu, Wei and McDonald, Chris},
title = {A Matrix-Vector Recurrent Unit Model for Capturing Compositional Semantics in Phrase Embeddings},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132984},
doi = {10.1145/3132847.3132984},
abstract = {The meaning of a multi-word phrase not only depends on the meaning of its constituent words, but also the rules of composing them to give the so-called compositional semantic. However, many deep learning models for learning compositional semantics target specific NLP tasks such as sentiment classification. Consequently, the word embeddings encode the lexical semantics, the weights of the networks are optimised for the classification task. Such models have no mechanisms to explicitly encode the compositional rules, and hence they are insufficient in capturing the semantics of phrases. We present a novel recurrent computational mechanism that specifically learns the compositionality by encoding the compositional rule of each word into a matrix. The network uses a recurrent architecture to capture the order of words for phrases with various lengths without requiring extra preprocessing such as part-of-speech tagging. The model is thoroughly evaluated on both supervised and unsupervised NLP tasks including phrase similarity, noun-modifier questions, sentiment distribution prediction, and domain specific term identification tasks. We demonstrate that our model consistently outperforms the LSTM and CNN deep learning models, simple algebraic compositions, and other popular baselines on different datasets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1499–1507},
numpages = {9},
keywords = {compositional semantics, deep learning, phrase embeddings},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132878,
author = {Azarbonyad, Hosein and Dehghani, Mostafa and Beelen, Kaspar and Arkut, Alexandra and Marx, Maarten and Kamps, Jaap},
title = {Words Are Malleable: Computing Semantic Shifts in Political and Media Discourse},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132878},
doi = {10.1145/3132847.3132878},
abstract = {Recently, researchers started to pay attention to the detection of temporal shifts in the meaning of words. However, most (if not all) of these approaches restricted their efforts to uncovering change over time, thus neglecting other valuable dimensions such as social or political variability. We propose an approach for detecting semantic shifts between different viewpoints---broadly defined as a set of texts that share a specific metadata feature, which can be a time-period, but also a social entity such as a political party. For each viewpoint, we learn a semantic space in which each word is represented as a low dimensional neural embedded vector. The challenge is to compare the meaning of a word in one space to its meaning in another space and measure the size of the semantic shifts. We compare the effectiveness of a measure based on optimal transformations between the two spaces with a measure based on the similarity of the neighbors of the word in the respective spaces. Our experiments demonstrate that the combination of these two performs best. We show that the semantic shifts not only occur over time but also along different viewpoints in a short period of time. For evaluation, we demonstrate how this approach captures meaningful semantic shifts and can help improve other tasks such as the contrastive viewpoint summarization and ideology detection (measured as classification accuracy) in political texts. We also show that the two laws of semantic change which were empirically shown to hold for temporal shifts also hold for shifts across viewpoints. These laws state that frequent words are less likely to shift meaning while words with many senses are more likely to do so.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1509–1518},
numpages = {10},
keywords = {ideology detection, semantic shifts, word stability, word embeddings},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132989,
author = {Singh, Gaurav and Marshall, Iain J. and Thomas, James and Shawe-Taylor, John and Wallace, Byron C.},
title = {A Neural Candidate-Selector Architecture for Automatic Structured Clinical Text Annotation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132989},
doi = {10.1145/3132847.3132989},
abstract = {We consider the task of automatically annotating free texts describing clinical trials with concepts from a controlled, structured medical vocabulary. Specifically, we aim to build a model to infer distinct sets of (ontological) concepts describing complementary clinically salient aspects of the underlying trials: the populations enrolled, the interventions administered and the outcomes measured, i.e., the PICO elements. This important practical problem poses a few key challenges. One issue is that the output space is vast, because the vocabulary comprises many unique concepts. Compounding this problem, annotated data in this domain is expensive to collect and hence sparse. Furthermore, the outputs (sets of concepts for each PICO element) are correlated: specific populations (e.g., diabetics) will render certain intervention concepts likely (insulin therapy) while effectively precluding others (radiation therapy). Such correlations should be exploited. We propose a novel neural model that addresses these challenges. We introduce a Candidate-Selector architecture in which the model considers setes of candidate concepts for PICO elements, and assesses their plausibility conditioned on the input text to be annotated. This relies on a 'candidate set' generator, which may be learned or relies on heuristics. A conditional discriminative neural model then jointly selects candidate concepts, given the input text. We compare the predictive performance of our approach to strong baselines, and show that it outperforms them. Finally, we perform a qualitative evaluation of the generated annotations by asking domain experts to assess their quality.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1519–1528},
numpages = {10},
keywords = {text mining, biomedical informatics, deep learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133039,
author = {Yuan, Dong and Li, Guoliang and Li, Qi and Zheng, Yudian},
title = {Sybil Defense in Crowdsourcing Platforms},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133039},
doi = {10.1145/3132847.3133039},
abstract = {Crowdsourcing platforms have been widely deployed to solve many computer-hard problems, e.g., image recognition and entity resolution. Quality control is an important issue in crowdsourcing, which has been extensively addressed by existing quality-control algorithms, e.g., voting-based algorithms and probabilistic graphical models. However, these algorithms cannot ensure quality under sybil attacks, which leverages a large number of sybil accounts to generate results for dominating answers of normal workers. To address this problem, we propose a sybil defense framework for crowdsourcing, which can help crowdsourcing platforms to identify sybil workers and defense the sybil attack. We develop a similarity function to quantify worker similarity. Based on worker similarity, we cluster workers into different groups such that we can utilize a small number of golden questions to accurately identify the sybil groups. We also devise online algorithms to instantly detect sybil workers to throttle the attacks. Our method also has ability to detect multi-attackers in one task. To the best of our knowledge, this is the first framework for sybil defense in crowdsourcing. Experimental results on real-world datasets demonstrate that our method can effectively identify and throttle sybil workers.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1529–1538},
numpages = {10},
keywords = {crowdsourcing, sybil defense},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133018,
author = {Liu, Shenghua and Hooi, Bryan and Faloutsos, Christos},
title = {HoloScope: Topology-and-Spike Aware Fraud Detection},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133018},
doi = {10.1145/3132847.3133018},
abstract = {As online fraudsters invest more resources, including purchasing large pools of fake user accounts and dedicated IPs, fraudulent attacks become less obvious and their detection becomes increasingly challenging. Existing approaches such as average degree maximization suffer from the bias of including more nodes than necessary, resulting in lower accuracy and increased need for manual verification. Hence, we propose HoloScope, which introduces a novel metric "contrast suspiciousness" integrating information from graph topology and spikes to more accurately detect fraudulent users and objects. Contrast suspiciousness dynamically emphasizes the contrast patterns between fraudsters and normal users, making HoloScope capable of distinguishing the synchronized and anomalous behaviors of fraudsters on topology, bursts and drops, and rating scores. In addition, we provide theoretical bounds for how much this increases the time cost needed for fraudsters to conduct adversarial attacks. Moreover, HoloScope has a concise framework and sub-quadratic time complexity, making the algorithm reproducible and scalable. Extensive experiments showed that HoloScope achieved significant accuracy improvements on synthetic and real data, compared with state-of-the-art fraud detection methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1539–1548},
numpages = {10},
keywords = {graph mining, burst and drop, time series, fraud detection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132951,
author = {Anindya, Imrul Chowdhury and Roy, Harichandan and Kantarcioglu, Murat and Malin, Bradley},
title = {Building a Dossier on the Cheap: Integrating Distributed Personal Data Resources Under Cost Constraints},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132951},
doi = {10.1145/3132847.3132951},
abstract = {A wide variety of personal data is routinely collected by numerous organizations that, in turn, share and sell their collections for analytic investigations (e.g., market research). To preserve privacy, certain identifiers are often redacted, perturbed or even removed. A substantial number of attacks have shown that, if care is not taken, such data can be linked to external resources to determine the explicit identifiers (e.g., personal names) or infer sensitive attributes (e.g., income) for the individuals from whom the data was collected. As such, organizations increasingly rely upon record linkage methods to assess the risk such attacks pose and adopt countermeasures accordingly. Traditional linkage methods assume only two datasets would be linked (e.g., linking de-identified hospital discharge to identified voter registration lists), but with the advent of a multi-billion dollar data broker industry, modern adversaries have access to a massive data stash of multiple datasets that can be leveraged. Still, realistic adversaries have budget constraints that prevent them from obtaining and integrating all relevant datasets. Thus, in this work, we investigate a novel privacy risk assessment framework, based on adversaries who plan an integration of datasets for the most accurate estimate of targeted sensitive attributes under a certain budget. To solve this problem, we introduce a graph-based formulation of the problem and predictive modeling methods to prioritize data resources for linkage. We perform an empirical analysis using real world voter registration data from two different U.S. states and show that the methods can be used efficiently to accurately estimate potentially sensitive information disclosure risks even under a non-trivial amount of noise.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1549–1558},
numpages = {10},
keywords = {probabilistic model, identity disclosure, data privacy, record linkage, data integration, heuristic approach, data brokers},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132848,
author = {Li, Yuhong and Hou, Dongmei and Pan, Aimin and Gong, Zhiguo},
title = {DeMalC: A Feature-Rich Machine Learning Framework for Malicious Call Detection},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132848},
doi = {10.1145/3132847.3132848},
abstract = {Malicious phone call is a plague, in which unscrupulous salesmen or criminals make to acquire money illegally from the victims. As a result, there has been broad interest in deveploing systems to make the end-users vigilant when receiving such phone calls. Typically, these systems justify the phone numbers either by the crowd-generated blacklist or exploiting the features via machine learning techniques. However, the former is frail due to the rare and lazy crowd, while the later suffers from the scarcity of effective features. In this work, we propose a solution named DeMalC to address those problems by applying the machine learning algorithmm on a novel set of discriminative features. These features consist of properties and behaviors that are powerful enough to characterize phone numbers from different perspectives. We extensively evaluated our solution, i.e., DeMalC, using massive call detail records. The experimental result shows the effectiveness of our extracted features. Capable of achieving 91.86% overall accuracy and 79.34% F1-score on the detection of malicious phone numbers, the DeMalC has been deployed online and demonstrated to be a competitive solution for detecting malicious calls.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1559–1567},
numpages = {9},
keywords = {Antifraud APP, Data Mining for Social Security, Malicious Call Detection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132938,
author = {Zehlike, Meike and Bonchi, Francesco and Castillo, Carlos and Hajian, Sara and Megahed, Mohamed and Baeza-Yates, Ricardo},
title = {FA*IR: A Fair Top-k Ranking Algorithm},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132938},
doi = {10.1145/3132847.3132938},
abstract = {In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n » k candidates, maximizing utility (i.e., select the "best" candidates) subject to group fairness criteria.Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-k should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above.An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1569–1578},
numpages = {10},
keywords = {ranking, bias in computer systems, algorithmic fairness, top-k selection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132944,
author = {Zheng, Kaiping and Wang, Wei and Gao, Jinyang and Ngiam, Kee Yuan and Ooi, Beng Chin and Yip, Wei Luen James},
title = {Capturing Feature-Level Irregularity in Disease Progression Modeling},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132944},
doi = {10.1145/3132847.3132944},
abstract = {Disease progression modeling (DPM) analyzes patients' electronic medical records (EMR) to predict the health state of patients, which facilitates accurate prognosis, early detection and treatment of chronic diseases. However, EMR are irregular because patients visit hospital irregularly based on the need of treatment. For each visit, they are typically given different diagnoses, prescribed various medications and lab tests. Consequently, EMR exhibit irregularity at the feature level. To handle this issue, we propose a model based on the Gated Recurrent Unit by decaying the effect of previous records using fine-grained feature-level time span information, and learn the decaying parameters for different features to take into account their different behaviours like decaying speeds under irregularity. Extensive experimental results in both an Alzheimer's disease dataset and a chronic kidney disease dataset demonstrate that our proposed model of capturing feature-level irregularity can effectively improve the accuracy of DPM.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1579–1588},
numpages = {10},
keywords = {healthcare, gated recurrent unit, time series, data analytics},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132946,
author = {Halder, Kishaloy and Kan, Min-Yen and Sugiyama, Kazunari},
title = {Health Forum Thread Recommendation Using an Interest Aware Topic Model},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132946},
doi = {10.1145/3132847.3132946},
abstract = {We introduce a general, interest-aware topic model (IATM), in which known higher-level interests on topics expressed by each user can be modeled. We then specialize the IATM for use in consumer health forum thread recommendation by equating each user's self-reported medical conditions as interests and topics as symptoms of treatments for recommendation. The IATM additionally models the implicit interests embodied by users' textual descriptions in their profiles. To further enhance the personalized nature of the recommendations, we introduce jointly normalized collaborative topic regression (JNCTR) which captures how users interact with the various symptoms belonging to the same clinical condition.  In our experiments on two real-world consumer health forums, our proposed model significantly outperforms competitive state-of-the-art baselines by over 10% in recall. Importantly, we show that our IATM+JNCTR pipeline also imbues the recommendation process with added transparency, allowing a recommendation system to justify its recommendation with respect to each user's interest in certain health conditions.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1589–1598},
numpages = {10},
keywords = {recommender systems, graphical model, topic models, collaborative filtering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132867,
author = {Chen, Liangzhe and Xu, Xinfeng and Lee, Sangkeun and Duan, Sisi and Tarditi, Alfonso G. and Chinthavali, Supriya and Prakash, B. Aditya},
title = {HotSpots: Failure Cascades on Heterogeneous Critical Infrastructure Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132867},
doi = {10.1145/3132847.3132867},
abstract = {Critical Infrastructure Systems such as transportation, water and power grid systems are vital to our national security, economy, and public safety. Recent events, like the 2012 hurricane Sandy, show how the interdependencies among different CI networks lead to catastrophic failures among the whole system. Hence, analyzing these CI networks, and modeling failure cascades on them becomes a very important problem. However, traditional models either do not take multiple CIs or the dynamics of the system into account, or model it simplistically. In this paper, we study this problem using a heterogeneous network viewpoint. We first construct heterogeneous CI networks with multiple components using national-level datasets. Then we study novel failure maximization problems on these networks, to compute critical nodes in such systems. We then provide HotSpots, a scalable and effective algorithm for these problems, based on careful transformations. Finally, we conduct extensive experiments on real CIS data from multiple US states, and show that our method HotSpots outperforms non-trivial baselines, gives meaningful results and that our approach gives immediate benefits in providing situational-awareness during large-scale failures.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1599–1607},
numpages = {9},
keywords = {failure cascade modeling, heterogeneous graph, critical infrastructure systems, failure maximization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133007,
author = {Dhakad, Lucky and Das, Mrinal and Bhattacharyya, Chiranjib and Datta, Samik and Kale, Mihir and Mehta, Vivek},
title = {SOPER: Discovering the Influence of Fashion and the Many Faces of User from Session Logs Using Stick Breaking Process},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133007},
doi = {10.1145/3132847.3133007},
abstract = {Recommending lifestyle articles is of immediate interest to the e-commerce industry and is beginning to attract research attention. Often followed strategies, such as recommending popular items are inadequate for this vertical because of two reasons. Firstly, users have their own personal preference over items, referred to as personal styles, which lead to the long-tail phenomenon. Secondly, each user displays multiple personas, each persona has a preference over items which could be dictated by a particular occasion, e.g. dressing for a party would be different from dressing to go to office. Recommendation in this vertical is crucially dependent on discovering styles for each of the multiple personas. There is no literature which addresses this problem.We posit a generative model which describes each user by a Simplex Over PERsona, SOPER, where a persona is described as the individuals preferences over prevailing styles modelled as topics over items. The choice of simplex and the long-tail nature necessitates the use of stick-breaking process. The main technical contribution is an efficient collapsed Gibbs sampling based algorithm for solving the attendant inference problem.Trained on large-scale interaction logs spanning more than half-a-million sessions collected from an e-commerce portal, SOPER outperforms previous baselines such as [9] by a large margin of 35% in identifying persona. Consequently it outperforms several competitive baselines comprehensively on the task of recommending from a catalogue of roughly 150 thousand lifestyle articles, by improving the recommendation quality as measured by AUC by a staggering 12.23%, in addition to aiding the interpretability of uncovered personal and fashionable styles thus advancing our precise understanding of the underlying phenomena.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1609–1618},
numpages = {10},
keywords = {bayesian nonparametrics, fashion, lifestyle, stick-breaking process, topic models},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132968,
author = {Zheng, Xin and Sun, Aixin and Wang, Sibo and Han, Jialong},
title = {Semi-Supervised Event-Related Tweet Identification with Dynamic Keyword Generation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132968},
doi = {10.1145/3132847.3132968},
abstract = {Twitter provides us a convenient channel to get access to the immediate information about major events. However, it is challenging to acquire a clean and complete set of event-related data due to the characteristics of tweets, eg short and noisy. In this paper, we propose a semi-supervised method to obtain high quality event-related tweets from Twitter stream, in terms of precision and recall. Specifically, candidate event-related tweets are selected based on a set of keywords. We propose to generate and update these keywords dynamically along the event development. To be included in this keyword set, words are evaluated based on single word properties, property based on co-occurred words, and changes of word importance over time. Our solution is capable of capturing keywords of emerging aspects or aspects with increasing importance along event evolvement. By leveraging keyword importance information and a few labeled tweets, we propose a semi-supervised expectation maximization process to identify event-related tweets. This process significantly reduces human effort in acquiring high quality tweets. Experiments on three real world datasets show that our solution outperforms state-of-the-art approaches by up to 10% in F1 measure.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1619–1628},
numpages = {10},
keywords = {dynamic keyword generation, event-related tweet identification},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133029,
author = {Wang, Chenguang and Song, Yangqiu and Li, Haoran and Sun, Yizhou and Zhang, Ming and Han, Jiawei},
title = {Distant Meta-Path Similarities for Text-Based Heterogeneous Information Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133029},
doi = {10.1145/3132847.3133029},
abstract = {Measuring network similarity is a fundamental data mining problem. The mainstream similarity measures mainly leverage the structural information regarding to the entities in the network without considering the network semantics. In the real world, the heterogeneous information networks (HINs) with rich semantics are ubiquitous. However, the existing network similarity doesn't generalize well in HINs because they fail to capture the HIN semantics. The meta-path has been proposed and demonstrated as a right way to represent semantics in HINs. Therefore, original meta-path based similarities (e.g., PathSim and KnowSim) have been successful in computing the entity proximity in HINs. The intuition is that the more instances of meta-path(s) between entities, the more similar the entities are. Thus the original meta-path similarity only applies to computing the proximity of two neighborhood (connected) entities. In this paper, we propose the distant meta-path similarity that is able to capture HIN semantics between two distant (isolated) entities to provide more meaningful entity proximity. The main idea is that even there is no shared neighborhood entities of (i.e., no meta-path instances connecting) the two entities, but if the more similar neighborhood entities of the entities are, the more similar the two entities should be. We then find out the optimum distant meta-path similarity by exploring the similarity hypothesis space based on different theoretical foundations. We show the state-of-the-art similarity performance of distant meta-path similarity on two text-based HINs and make the datasets public available.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1629–1638},
numpages = {10},
keywords = {distant similarity, text similarity, knowledge graph, heterogeneous information network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132999,
author = {An, Shuai and Wang, Jun and Wei, Jinmao and Yang, Zhenglu},
title = {Unsupervised Feature Selection with Joint Clustering Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132999},
doi = {10.1145/3132847.3132999},
abstract = {Unsupervised feature selection has raised considerable interests in the past decade, due to its remarkable performance in reducing dimensionality without any prior class information. Preserving reliable locality information and achieving excellent cluster separation are two critical issues for unsupervised feature selection. However, existing methods cannot tackle two issues simultaneously. To address the problems, we propose a novel unsupervised approach that integrates sparse feature selection and robust joint clustering analysis. The joint clustering analysis seamlessly unifies the spectral clustering and the orthogonal basis clustering. Specifically, a probabilistic neighborhood graph is utilized to preserve reliable locality information in the spectral clustering, and an orthogonal basis matrix is incorporated to achieve excellent cluster separation in the orthogonal basis clustering. A compact and effective iterative algorithm is designed to optimize the proposed selection framework. Extensive experiments on both synthetic data and real-world data validate the effectiveness of our approach under various evaluation indices.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1639–1648},
numpages = {10},
keywords = {cluster separation, locality preserving, joint clustering analysis, unsupervised feature selection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132858,
author = {Braytee, Ali and Liu, Wei and Catchpoole, Daniel R. and Kennedy, Paul J.},
title = {Multi-Label Feature Selection Using Correlation Information},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132858},
doi = {10.1145/3132847.3132858},
abstract = {High-dimensional multi-labeled data contain instances, where each instance is associated with a set of class labels and has a large number of noisy and irrelevant features. Feature selection has been shown to have great benefits in improving the classification performance in machine learning. In multi-label learning, to select the discriminative features among multiple labels, several challenges should be considered: interdependent labels, different instances may share different label correlations, correlated features, and missing and flawed labels. This work is part of a project at The Children's Hospital at Westmead (TB-CHW), Australia to explore the genomics of childhood leukaemia. In this paper, we propose a CMFS (Correlated- and Multi-label Feature Selection method), based on non-negative matrix factorization (NMF) for simultaneously performing feature selection and addressing the aforementioned challenges. Significantly, a major advantage of our research is to exploit the correlation information contained in features, labels and instances to select the relevant features among multiple labels. Furthermore, l2,1 -norm regularization is incorporated in the objective function to undertake feature selection by imposing sparsity on the feature matrix rows. We employ CMFS to decompose the data and multi-label matrices into a low-dimensional space. To solve the objective function, an efficient iterative optimization algorithm is proposed with guaranteed convergence. Finally, extensive experiments are conducted on high-dimensional multi-labeled datasets. The experimental results demonstrate that our method significantly outperforms state-of-the-art multi-label feature selection methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1649–1656},
numpages = {8},
keywords = {multi-label classification, new application, high dimensional data, multi-label feature selection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132855,
author = {Li, Yiyang and Tao, Guanyu and Zhang, Weinan and Yu, Yong and Wang, Jun},
title = {Content Recommendation by Noise Contrastive Transfer Learning of Feature Representation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132855},
doi = {10.1145/3132847.3132855},
abstract = {Personalized recommendation has been proved effective as a content discovery tool for many online news publishers. As fresh news articles are frequently coming to the system while the old ones are fading away quickly, building a consistent and coherent feature representation over the ever-changing articles pool is fundamental to the performance of the recommendation. However, learning a good feature representation is challenging, especially for some small publishers that have normally fewer than 10,000 articles each year. In this paper, we consider to transfer knowledge from a larger text corpus. In our proposed solution, an effective article recommendation engine can be established with a small number of target publisher articles by transferring knowledge from a large corpus of text with a different distribution. Specifically, we leverage noise contrastive estimation techniques to learn the word conditional distribution given the context words, where the noise conditional distribution is pre-trained from the large corpus. Our solution has been deployed in a commercial recommendation service. The large-scale online A/B testing on two commercial publishers demonstrates up to 9.97% relative overall performance gain of our proposed model on the recommendation click-though rate metric over the non-transfer learning baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1657–1665},
numpages = {9},
keywords = {word2vec, text representation, transfer learning, noise contrastive estimation, article recommendation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132963,
author = {Phan, Minh C. and Sun, Aixin and Tay, Yi and Han, Jialong and Li, Chenliang},
title = {NeuPL: Attention-Based Semantic Matching and Pair-Linking for Entity Disambiguation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132963},
doi = {10.1145/3132847.3132963},
abstract = {Entity disambiguation, also known as entity linking, is the task of mapping mentions in text to the corresponding entities in a given knowledge base, e.g. Wikipedia. Two key challenges are making use of mention's context to disambiguate (i.e. local objective), and promoting coherence of all the linked entities (i.e. global objective). In this paper, we propose a deep neural network model to effectively measure the semantic matching between mention's context and target entity. We are the first to employ the long short-term memory (LSTM) and attention mechanism for entity disambiguation. We also propose Pair-Linking, a simple but effective and significantly fast linking algorithm. Pair-Linking iteratively identifies and resolves pairs of mentions, starting from the most confident pair. It finishes linking all mentions in a document by scanning the pairs of mentions at most once. Our neural network model combined with Pair-Linking, named NeuPL, outperforms state-of-the-art systems over different types of documents including news, RSS, and tweets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1667–1676},
numpages = {10},
keywords = {entity disambiguation, pair-linking, semantic matching},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132992,
author = {Li, Jia and Cao, Yang and Ma, Shuai},
title = {Relaxing Graph Pattern Matching With Explanations},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132992},
doi = {10.1145/3132847.3132992},
abstract = {Traditional graph pattern matching is based on subgraph isomorphism, which is often too restrictive to identify meaningful matches. To handle this, taxonomy subgraph isomorphism has been proposed to relax the label constraints in the matching. Nonetheless, there are many cases that cannot be covered. In this study, we first formalize taxonomy simulation, a natural matching semantics combing graph simulation with taxonomy, and propose its pattern relaxation to enrich graph pattern matching results with taxonomy information. We also design topological ranking and diversified topological ranking for top-k relaxations. We then study the top-k pattern relaxation problems, by providing their static analyses, and developing algorithms and optimization for finding and evaluating top-k pattern relaxations. We further propose a notion of explanations for answers to the relaxations and develop algorithms to compute explanations. These together give us a framework for enriching the results of graph pattern matching. Using real-life datasets, we experimentally verify that our framework and techniques are effective and efficient for identifying meaningful matches in practice.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1677–1686},
numpages = {10},
keywords = {taxonomy simulation, pattern relaxation, query result explanation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132983,
author = {Malmi, Eric and Gionis, Aristides and Terzi, Evimaria},
title = {Active Network Alignment: A Matching-Based Approach},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132983},
doi = {10.1145/3132847.3132983},
abstract = {Network alignment is the problem of matching the nodes of two graphs, maximizing the similarity of the matched nodes and the edges between them. This problem is encountered in a wide array of applications---from biological networks to social networks to ontologies---where multiple networked data sources need to be integrated. Due to the difficulty of the task, an accurate alignment can rarely be found without human assistance. Thus, it is of great practical importance to develop network alignment algorithms that can optimally leverage experts who are able to provide the correct alignment for a small number of nodes. Yet, only a handful of existing works address this active network alignment setting.The majority of the existing active methods focus on absolute queries ("are nodes a and b the same or not?"), whereas we argue that it is generally easier for a human expert to answer relative queries ("which node in the set b1,...,bn is the most similar to node a?"). This paper introduces two novel relative-query strategies, TopMatchings and GibbsMatchings, which can be applied on top of any network alignment method that constructs and solves a bipartite matching problem. Our methods identify the most informative nodes to query by sampling the matchings of the bipartite graph associated to the network-alignment instance.We compare the proposed approaches to several commonly-used query strategies and perform experiments on both synthetic and real-world datasets. Our sampling-based strategies yield the highest overall performance, outperforming all the baseline methods by more than 15 percentage points in some cases. In terms of accuracy, TopMatchings and GibbsMatchings perform comparably. However, GibbsMatchings is significantly more scalable, but it also requires hyperparameter tuning for a temperature parameter.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1687–1696},
numpages = {10},
keywords = {graph matching, network alignment, active learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133014,
author = {Namaki, Mohammad Hossein and Wu, Yinghui and Song, Qi and Lin, Peng and Ge, Tingjian},
title = {Discovering Graph Temporal Association Rules},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133014},
doi = {10.1145/3132847.3133014},
abstract = {Detecting regularities between complex events in temporal graphs is critical for emerging applications. This paper proposes graph temporal association rules (GTAR). A GTAR extends traditional association rules to discover temporal associations for complex events captured by a class of temporal pattern queries. We introduce notions of support and confidence for GTARS and formalize the discovery problem for GTARS. We show that despite the enhanced expressive power, GTARS discovery is feasible over large temporal graphs. We develop an effective rule discovery algorithm, which integrates event mining and rule discovery as a single process, and reduces the redundant computation by leveraging their interaction. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms. Our case study also verifies that GTARS demonstrate highly interpretable associations in real-world networks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1697–1706},
numpages = {10},
keywords = {temporal association rules, large temporal graph, approximate pattern matching},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133013,
author = {Golshan, Behzad and Terzi, Evimaria},
title = {Minimizing Tension in Teams},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133013},
doi = {10.1145/3132847.3133013},
abstract = {In large organizations (e.g., companies, universities, etc.) individual experts with different work habits are asked to work together in order to complete projects or tasks. Oftentimes, the differences in the inherent work habits of these experts causes tension among them, which can prove detrimental for the organization's performance and functioning. The question we consider in this paper is the following: "can this tension be reduced by providing incentives to individuals to change their work habits?" We formalize this question in the definition of the k- AlterHabit problem. To the best of our knowledge we are the first to define this problem and analyze its properties. Although we show that k- AlterHabit is NP-hard, we devise polynomial-time algorithms for solving it in practice. Our algorithms are based on interesting connections that we draw between our problem and other combinatorial problems. Our experimental results demonstrate both the efficiency and the efficacy of our algorithmic techniques on a collection of real data.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1707–1715},
numpages = {9},
keywords = {algorithms, collaboration networks, teams, experimentation, theory},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132969,
author = {Sun, Jiabao and Xu, Jiajie and Zheng, Kai and Liu, Chengfei},
title = {Interactive Spatial Keyword Querying with Semantics},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132969},
doi = {10.1145/3132847.3132969},
abstract = {Conventional spatial keyword queries confront the difficulty of returning desired objects that are synonyms but morphologically different to query keywords. To overcome this flaw, this paper investigates the interactive spatial keyword querying with semantics. It aims to enhance the conventional queries by not only making sense of the query keywords, but also refining the understanding of query semantics through interactions. On top of the probabilistic topic model, a novel interactive strategy is proposed to precisely infer the latent query semantics by learning from user feedbacks. In each interaction, the returned objects are carefully selected to ensure effective inference of user intended query semantics. Query processing is carried out on a small candidate object set at each round of interaction, and the whole querying process terminates when the latent query semantics learned from user feedback becomes explicit enough. The experimental results on real check-in dataset demonstrates that the quality of results has been significantly improved through limited number of interactions.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1727–1736},
numpages = {10},
keywords = {interactive query, spatial database, spatial keyword query},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132869,
author = {Ha-Thuc, Viet and Yan, Yan and Wu, Xianren and Dialani, Vijay and Gupta, Abhishek and Sinha, Shakti},
title = {From Query-By-Keyword to Query-By-Example: LinkedIn Talent Search Approach},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132869},
doi = {10.1145/3132847.3132869},
abstract = {One key challenge in talent search is to translate complex criteria of a hiring position into a search query, while it is relatively easy for a searcher to list examples of suitable candidates for a given position. To improve search e ciency, we propose the next generation of talent search at LinkedIn, also referred to as Search By Ideal Candidates. In this system, a searcher provides one or several ideal candidates as the input to hire for a given position. The system then generates a query based on the ideal candidates and uses it to retrieve and rank results. Shifting from the traditional Query-By-Keyword to this new Query-By-Example system poses a number of challenges: How to generate a query that best describes the candidates? When moving to a completely di erent paradigm, how does one leverage previous product logs to learn ranking models and/or evaluate the new system with no existing usage logs? Finally, given the di erent nature between the two search paradigms, the ranking features typically used for Query-By-Keyword systems might not be optimal for Query- By-Example. This paper describes our approach to solving these challenges. We present experimental results con rming the e ectiveness of the proposed solution, particularly on query building and search ranking tasks. As of writing this paper, the new system has been available to all LinkedIn members.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1737–1745},
numpages = {9},
keywords = {personalization, query-by-example, learning to rank},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133010,
author = {Dehghani, Mostafa and Rothe, Sascha and Alfonseca, Enrique and Fleury, Pascal},
title = {Learning to Attend, Copy, and Generate for Session-Based Query Suggestion},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133010},
doi = {10.1145/3132847.3133010},
abstract = {Users try to articulate their complex information needs during search sessions by reformulating their queries. To make this process more effective, search engines provide related queries to help users in specifying the information need in their search process. In this paper, we propose a customized sequence-to-sequence model for session-based query suggestion. In our model, we employ a query-aware attention mechanism to capture the structure of the session context. is enables us to control the scope of the session from which we infer the suggested next query, which helps not only handle the noisy data but also automatically detect session boundaries. Furthermore, we observe that, based on the user query reformulation behavior, within a single session a large portion of query terms is retained from the previously submitted queries and consists of mostly infrequent or unseen terms that are usually not included in the vocabulary. We therefore empower the decoder of our model to access the source words from the session context during decoding by incorporating a copy mechanism. Moreover, we propose evaluation metrics to assess the quality of the generative models for query suggestion. We conduct an extensive set of experiments and analysis. e results suggest that our model outperforms the baselines both in terms of the generating queries and scoring candidate queries for the task of query suggestion.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1747–1756},
numpages = {10},
keywords = {sequence to sequence model, query suggestion, query-aware attention, copy mechanism},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132856,
author = {Liao, Zhen and Song, Xinying and Shen, Yelong and Lee, Saekoo and Gao, Jianfeng and Liao, Ciya},
title = {Deep Context Modeling for Web Query Entity Disambiguation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132856},
doi = {10.1145/3132847.3132856},
abstract = {In this paper, we presented a new study for Web query entity disambiguation (QED), which is the task of disambiguating different candidate entities in a knowledge base given their mentions in a query. QED is particularly challenging because queries are often too short to provide rich contextual information that is required by traditional entity disambiguation methods. In this paper, we propose several methods to tackle the problem of QED. First, we explore the use of deep neural network (DNN) for capturing the character level textual information in queries. Our DNN approach maps queries and their candidate reference entities to feature vectors in a latent semantic space where the distance between a query and its correct reference entity is minimized. Second, we utilize the Web search result information of queries to help generate large amounts of weakly supervised training data for the DNN model. Third, we propose a two-stage training method to combine large-scale weakly supervised data with a small amount of human labeled data, which can significantly boost the performance of a DNN model. The effectiveness of our approach is demonstrated in the experiments using large-scale real-world datasets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1757–1765},
numpages = {9},
keywords = {CLSM, Query Entity Disambiguation, Two-Stage Training},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133021,
author = {Qu, Meng and Tang, Jian and Shang, Jingbo and Ren, Xiang and Zhang, Ming and Han, Jiawei},
title = {An Attention-Based Collaboration Framework for Multi-View Network Representation Learning},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133021},
doi = {10.1145/3132847.3133021},
abstract = {Learning distributed node representations in networks has been attracting increasing attention recently due to its effectiveness in a variety of applications. Existing approaches usually study networks with a single type of proximity between nodes, which defines a single view of a network. However, in reality there usually exists multiple types of proximities between nodes, yielding networks with multiple views. This paper studies learning node representations for networks with multiple views, which aims to infer robust node representations across different views. We propose a multi-view representation learning approach, which promotes the collaboration of different views and lets them vote for the robust representations. During the voting process, an attention mechanism is introduced, which enables each node to focus on the most informative views. Experimental results on real-world networks show that the proposed approach outperforms existing state-of-the-art approaches for network representation learning with a single view and other competitive approaches with multiple views.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1767–1776},
numpages = {10},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132961,
author = {Tan, Zhen and Zhao, Xiang and Wang, Wei},
title = {Representation Learning of Large-Scale Knowledge Graphs via Entity Feature Combinations},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132961},
doi = {10.1145/3132847.3132961},
abstract = {Knowledge graphs are typical large-scale multi-relational structures, which comprise a large amount of fact triplets. Nonetheless, existing knowledge graphs are still sparse and far from being complete. To refine the knowledge graphs, representation learning is widely used to embed fact triplets into low-dimensional spaces. Many existing knowledge graph embedding models either focus on learning rich features from entities but fail to extract good features of relations, or employ sophisticated models that have rather high time and memory-space complexities. In this paper, we propose a novel knowledge graph embedding model, CombinE. It exploits entity features from two complementary perspectives via the plus and minus combinations. We start with the plus combination, where we use shared features of entity pairs participating in a relation to convey its relation features. To also allow differences of each pairs of entities participating in a relation, we also use the minus combination, where we concentrate on individual entity features, and regard relations as a channel to offset the divergence and preserve the prominence between head and tail entities. Compared with the state-of-the-art models, our experimental results demonstrate that CombinE outperforms existing ones and has low time and memory-space complexities.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1777–1786},
numpages = {10},
keywords = {link prediction, knowledge graphs, representation learning, feature combinations},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132959,
author = {Abu-El-Haija, Sami and Perozzi, Bryan and Al-Rfou, Rami},
title = {Learning Edge Representations via Low-Rank Asymmetric Projections},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132959},
doi = {10.1145/3132847.3132959},
abstract = {We propose a new method for embedding graphs while preserving directed edge information. Learning such continuous-space vector representations (or embeddings) of nodes in a graph is an important first step for using network information (from social networks, user-item graphs, knowledge bases, etc.) in many machine learning tasks. Unlike previous work, we (1) explicitly model an edge as a function of node embeddings, and we (2) propose a novel objective, the graph likelihood, which contrasts information from sampled random walks with non-existent edges. Individually, both of these contributions improve the learned representations, especially when there are memory constraints on the total size of the embeddings. When combined, our contributions enable us to significantly improve the state-of-the-art by learning more concise representations that better preserve the graph structure. We evaluate our method on a variety of link-prediction task including social networks, collaboration networks, and protein interactions, showing that our proposed method learn representations with error reductions of up to 76% and 55%, on directed and undirected graphs. In addition, we show that the representations learned by our method are quite space efficient, producing embeddings which have higher structure-preserving accuracy but are 10 times smaller.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1787–1796},
numpages = {10},
keywords = {graph, link prediction, edge learning, random walk, representation learning, embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132953,
author = {Fu, Tao-yang and Lee, Wang-Chien and Lei, Zhen},
title = {HIN2Vec: Explore Meta-Paths in Heterogeneous Information Networks for Representation Learning},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132953},
doi = {10.1145/3132847.3132953},
abstract = {In this paper, we propose a novel representation learning framework, namely  HIN2Vec, for heterogeneous information networks (HINs). The core of the proposed framework is a neural network model, also called HIN2Vec, designed to capture the rich semantics embedded in HINs by exploiting different types of relationships among nodes. Given a set of relationships specified in forms of meta-paths in an HIN, HIN2Vec carries out multiple prediction training tasks jointly based on a target set of relationships to learn latent vectors of nodes and meta-paths in the HIN. In addition to model design, several issues unique to HIN2Vec, including regularization of meta-path vectors, node type selection in negative sampling, and cycles in random walks, are examined. To validate our ideas, we learn latent vectors of nodes using four large-scale real HIN datasets, including Blogcatalog, Yelp, DBLP and U.S. Patents, and use them as features for multi-label node classification and link prediction applications on those networks. Empirical results show that HIN2Vec soundly outperforms the state-of-the-art representation learning models for network data, including DeepWalk, LINE, node2vec, PTE, HINE and ESim, by 6.6% to 23.8% of $micro$-$f_1$ in multi-label node classification and 5% to 70.8% of $MAP$ in link prediction.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1797–1806},
numpages = {10},
keywords = {representation learning, heterogeneous information network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132993,
author = {Galimberti, Edoardo and Bonchi, Francesco and Gullo, Francesco},
title = {Core Decomposition and Densest Subgraph in Multilayer Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132993},
doi = {10.1145/3132847.3132993},
abstract = {Multilayer networks are a powerful paradigm to model complex systems, where various relations might occur among the same set of entities. Despite the keen interest in a variety of problems, algorithms, and analysis methods in this type of network, the problem of extracting dense subgraphs has remained largely unexplored.As a first step in this direction, in this work we study the problem of core decomposition of a multilayer network. Unlike the single-layer counterpart in which cores are all nested into one another and can be computed in linear time, the multilayer context is much more challenging as no total order exists among multilayer cores; rather, they form a lattice whose size is exponential in the number of layers. In this setting we devise three algorithms which differ in the way they visit the core lattice and in their pruning techniques. We assess time and space efficiency of the three algorithms on a large variety of real-world multilayer networks.We then move a step forward and showcase an application of the multilayer core-decomposition tool to the problem of densest-subgraph extraction from multilayer networks. We introduce a definition of multilayer densest subgraph that trades-off between high density and number of layers in which the high density holds, and show how multilayer core decomposition can be exploited to approximate this problem with quality guarantees.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1807–1816},
numpages = {10},
keywords = {graph mining, multilayer networks, core decomposition, densest subgraph},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132966,
author = {Nasir, Muhammad Anis Uddin and Gionis, Aristides and Morales, Gianmarco De Francisci and Girdzijauskas, Sarunas},
title = {Fully Dynamic Algorithm for Top-<i>k</i> Densest Subgraphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132966},
doi = {10.1145/3132847.3132966},
abstract = {Given a large graph,the densest-subgraph problem asks to find a subgraph with maximum average degree. When considering the top-k version of this problem, a na\"{\i}ve solution is to iteratively find the densest subgraph and remove it in each iteration. However, such a solution is impractical due to high processing cost. The problem is further complicated when dealing with dynamic graphs, since adding or removing an edge requires re-running the algorithm. In this paper, we study the top-k densest-subgraph problem in the sliding-window model and propose an efficient fully-dynamic algorithm. The input of our algorithm consists of an edge stream, and the goal is to find the node-disjoint subgraphs that maximize the sum of their densities. In contrast to existing state-of-the-art solutions that require iterating over the entire graph upon any update, our algorithm profits from the observation that updates only affect a limited region of the graph. Therefore, the top-k densest subgraphs are maintained by only applying local updates. We provide a theoretical analysis of the proposed algorithm and show empirically that the algorithm often generates denser subgraphs than state-of-the-art competitors. Experiments show an improvement in efficiency of up to five orders of magnitude compared to state-of-the-art solutions.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1817–1826},
numpages = {10},
keywords = {fully-dynamic graph algorithm, sliding window, top-k densest subgraph, community detection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132931,
author = {Rong, Yu and Cheng, Hong},
title = {Minimizing Dependence between Graphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132931},
doi = {10.1145/3132847.3132931},
abstract = {In recent years, modeling the relation between two graphs has received unprecedented attention from researchers due to its wide applications in many areas, such as social analysis and bioinformatics. The nature of relations between two graphs can be divided into two categories: the vertex relation and the link relation. Many studies focus on modeling the vertex relation between graphs and try to find the vertex correspondence between two graphs. However, the link relation between graphs has not been fully studied. Specifically, we model the cross-graph link relation as cross-graph dependence, which reflects the dependence of a vertex in one graph on a vertex in the other graph. A generic problem, called Graph Dependence Minimization (GDM), is defined as: given two graphs with cross-graph dependence, how to select a subset of vertexes from one graph and copy them to the other, so as to minimize the cross-graph dependence. Many real applications can benefit from the solution to GDM. Examples include reducing the cross-language links in online encyclopedias, optimizing the cross-platform communication cost between different cloud services, and so on. This problem is trivial if we can select as many vertexes as we want to copy. But what if we can only choose a limited number of vertexes to copy so as to make the two graphs as independent as possible? We formulate GDM with a budget constraint into a combinatorial optimization problem, which is proven to be NP-hard. We propose two algorithms to solve GDM. Firstly, we prove the submodularity of the objective function of GDM and adopt the size-constrained submodular minimization (SSM) algorithm to solve it. Since the SSM-based algorithm cannot scale to large graphs, we design a heuristic algorithm with a provable approximation guarantee. We prove that the error achieved by the heuristic algorithm is bounded by an additive factor which is proportional to the square of the given budget. Extensive experiments on both synthetic and real-world graphs show that the proposed algorithms consistently outperform the well-studied graph centrality measure based solutions. Furthermore, we conduct a case study on the Wikipedia graphs with millions of vertexes and links to demonstrate the potential of GDM to solve real-world problems.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1827–1836},
numpages = {10},
keywords = {submodular minimization, graph analytics, graph dependence minimization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132986,
author = {Ghalwash, Mohamed and Li, Ying and Zhang, Ping and Hu, Jianying},
title = {Exploiting Electronic Health Records to Mine Drug Effects on Laboratory Test Results},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132986},
doi = {10.1145/3132847.3132986},
abstract = {The proliferation of Electronic Health Records (EHRs) challenges data miners to discover potential and previously unknown patterns from a large collection of medical data. One of the tasks that we address in this paper is to reveal previously unknown effects of drugs on laboratory test results. We propose a method that leverages drug information to find a meaningful list of drugs that have an effect on the laboratory result. We formulate the problem as a convex non smooth function and develop a proximal gradient method to optimize it. The model has been evaluated on two important use cases: lowering low-density lipoproteins and glycated hemoglobin test results. The experimental results provide evidence that the proposed method is more accurate than the state-of-the-art method, rediscover drugs that are known to lower the levels of laboratory test results, and most importantly, discover additional potential drugs that may also lower these levels.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1837–1846},
numpages = {10},
keywords = {proximal methods, drug similarity, drug effects},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132879,
author = {Baskaran, Sridevi and Keller, Alexander and Chiang, Fei and Golab, Lukasz and Szlichta, Jaroslaw},
title = {Efficient Discovery of Ontology Functional Dependencies},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132879},
doi = {10.1145/3132847.3132879},
abstract = {Functional Dependencies (FDs) define attribute relationships based on syntactic equality, and, when used in data cleaning, they erroneously label syntactically different but semantically equivalent values as errors. We enhance dependency-based data cleaning with Ontology Functional Dependencies (OFDs), which express semantic attribute relationships such as synonyms and is-a hierarchies defined by an ontology. Our technical contributions are twofold: 1) theoretical foundations for OFDs, including a set of sound and complete axioms and a linear-time inference procedure, and 2) an algorithm for discovering OFDs (exact ones and ones that hold with some exceptions) from data that uses the axioms to prune the exponential search space in the number of attributes. We demonstrate the efficiency of our techniques on real datasets, and we show that OFDs can significantly reduce the number of false positive errors in data cleaning techniques that rely on traditional FDs.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1847–1856},
numpages = {10},
keywords = {dependency discovery, data cleaning, ontology functional dependency, functional dependency},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132899,
author = {Xie, Chenhao and Chen, Lihan and Liang, Jiaqing and Zhang, Kezun and Xiao, Yanghua and Tong, Hanghang and Wang, Haixun and Wang, Wei},
title = {Automatic Navbox Generation by Interpretable Clustering over Linked Entities},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132899},
doi = {10.1145/3132847.3132899},
abstract = {Rare efforts have been devoted to generating the structured Navigation Box (Navbox) for Wikipedia articles. A Navbox is a table in Wikipedia article page that provides a consistent navigation system for related entities. Navbox is critical for the readership and editing efficiency of Wikipedia. In this paper, we target on the automatic generation of Navbox for Wikipedia articles. Instead of performing information extraction over unstructured natural language text directly, an alternative avenue is explored by focusing on a rich set of semi-structured data in Wikipedia articles: linked entities. The core idea of this paper is as follows: If we cluster the linked entities and interpret them appropriately, we can construct a high-quality Navbox for the article entity. We propose a clustering-then-labeling algorithm to realize the idea. Experiments show that the proposed solutions are effective. Ultimately, our approach enriches Wikipedia with 1.95 million new Navboxes of high quality.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1857–1865},
numpages = {9},
keywords = {navbox generation, clustering-thenlabeling, interpretable clustering, knowledge extraction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132890,
author = {Ponza, Marco and Ferragina, Paolo and Chakrabarti, Soumen},
title = {A Two-Stage Framework for Computing Entity Relatedness in Wikipedia},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132890},
doi = {10.1145/3132847.3132890},
abstract = {Introducing a new dataset with human judgments of entity relatedness, we present a thorough study of all entity relatedness measures in recent literature based on Wikipedia as the knowledge graph. No clear dominance is seen between measures based on textual similarity and graph proximity. Some of the better measures involve expensive global graph computations. We then propose a new, space-efficient, computationally lightweight, two-stage framework for relatedness computation. In the first stage, a small weighted subgraph is dynamically grown around the two query entities; in the second stage, relatedness is derived based on computations on this subgraph. Our system shows better agreement with human judgment than existing proposals both on the new dataset and on an established one. We also plug our relatedness algorithm into a state-of-the-art entity linker and observe an increase in its accuracy and robustness.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1867–1876},
numpages = {10},
keywords = {wikipedia, algorithm, entity relatedness},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132881,
author = {He, Yuan and Wang, Cheng and Jiang, Changjun},
title = {Incorporating the Latent Link Categories in Relational Topic Modeling},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132881},
doi = {10.1145/3132847.3132881},
abstract = {The soaring of social media services has greatly propelled the prevalence of document networks. Rather than a set of plain texts, documents are nodes in graphs. An observable link connects the documents at its two ends, thus it implicitly reflects the semantic association between the document pair. Previous work assumes that only similar documents tend to be connected, which neglects the rich connective patterns in the topological structure. In this paper, we introduce a latent correlation factor to categorize the links into several categories, and each category corresponds to a unique kind of association. By fitting the data, the relational information (e.g., homophily and heterophily) can be comprehensively captured. By resorting to Canonical Correlation Analysis (CCA), we maximize the correlation between all pairs of linked documents. We propose a pure generative model and derive efficient learning algorithms based on the variational EM methods. Experiments on three different datasets demonstrate that the proposed model is competitive and usually better than the state-of-the-art baselines on both topic modeling and link prediction.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1877–1886},
numpages = {10},
keywords = {canonical correlation analysis, relational topic modeling, link prediction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132864,
author = {Yin, Peifeng and Liu, Zhe and Xu, Anbang and Nakamura, Taiga},
title = {Tone Analyzer for Online Customer Service: An Unsupervised Model with Interfered Training},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132864},
doi = {10.1145/3132847.3132864},
abstract = {Emotion analysis of online customer service conservation is important for good user experience and customer satisfaction. However, conventional metrics do not fit this application scenario. In this work, by collecting and labeling online conversations of customer service on Twitter, we identify 8 new metrics, named as tones, to describe emotional information. To better interpret each tone, we extend the Latent Dirichlet Allocation (LDA) model to Tone LDA (T-LDA). In T-LDA, each latent topic is explicitly associated with one of three semantic categories, i.e., tone-related, domain-specific and auxiliary. By integrating tone label into learning, T-LDA can interfere the original unsupervised training process and thus is able to identify representative tone-related words. In evaluation, T-LDA shows better performance than baselines in predicting tone intensity. Also, a case study is conducted to analyze each tone via T-LDA output.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1887–1895},
numpages = {9},
keywords = {topic modeling, emotion, online customer service, tone},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133008,
author = {Ye, Junting and Han, Shuchu and Hu, Yifan and Coskun, Baris and Liu, Meizhu and Qin, Hong and Skiena, Steven},
title = {Nationality Classification Using Name Embeddings},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133008},
doi = {10.1145/3132847.3133008},
abstract = {Nationality identification unlocks important demographic information, with many applications in biomedical and sociological research. Existing name-based nationality classifiers use name substrings as features and are trained on small, unrepresentative sets of labeled names, typically extracted from Wikipedia. As a result, these methods achieve limited performance and cannot support fine-grained classification.We exploit the phenomena of homophily in communication patterns to learn name embeddings, a new representation that encodes gender, ethnicity, and nationality which is readily applicable to building classifiers and other systems. Through our analysis of 57M contact lists from a major Internet company, we are able to design a fine-grained nationality classifier covering 39 groups representing over 90% of the world population. In an evaluation against other published systems over 13 common classes, our F1 score (0.795) is substantial better than our closest competitor Ethnea (0.580). To the best of our knowledge, this is the most accurate, fine-grained nationality classifier available.As a social media application, we apply our classifiers to the followers of major Twitter celebrities over six different domains. We demonstrate stark differences in the ethnicities of the followers of Trump and Obama, and in the sports and entertainments favored by different groups. Finally, we identify an anomalous political figure whose presumably inflated following appears largely incapable of reading the language he posts in.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1897–1906},
numpages = {10},
keywords = {nationality classification, ethnicity classification, name embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132932,
author = {Jin, Shengmin and Zafarani, Reza},
title = {Emotions in Social Networks: Distributions, Patterns, and Models},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132932},
doi = {10.1145/3132847.3132932},
abstract = {Understanding the role emotions play in social interactions has been a central research question in the social sciences. However, the challenge of obtaining large-scale data on human emotions has left the most fundamental questions on emotions less explored: How do emotions vary across individuals, evolve over time, and are connected to social ties?We address these questions using a large-scale dataset of users that contains both their emotions and social ties. Using this dataset, we identify patterns of human emotions on five different network levels, starting from the user-level and moving up to the whole-network level. At the user-level, we identify how human emotions are distributed and vary over time. At the ego-network level, we find that assortativity is only observed with respect to positive moods. This observation allows us to introduce emotional balance, the "dual'' of structural balance theory. We show that emotional balance has a natural connection to structural balance theory. At the community-level, we find that community members are emotionally-similar and that this similarity is stronger in smaller communities. Structural properties of communities, such as their sparseness or isolatedness, are also connected to the emotions of their members. At the whole-network level, we show that there is a tight connection between the global structure of a network and the emotions of its members. As a result, we demonstrate how one can accurately predict the proportion of positive/negative users within a network by only looking at the network structure. Based on our observations, we propose the Emotional-Tie model -- a network model that can simulate the formation of friendships based on emotions. This model generates graphs that exhibit both patterns of human emotions identified in this work and those observed in real-world social networks, such as having a high clustering coefficient. Our findings can help better understand the interplay between emotions and social ties.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1907–1916},
numpages = {10},
keywords = {sentiments, network models, signed networks, emotions},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132912,
author = {Zhuang, Yan and Li, Guoliang and Zhong, Zhuojian and Feng, Jianhua},
title = {Hike: A Hybrid Human-Machine Method for Entity Alignment in Large-Scale Knowledge Bases},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132912},
doi = {10.1145/3132847.3132912},
abstract = {With the vigorous development of the World Wide Web, many large-scale knowledge bases (KBs) have been generated. To improve the coverage of KBs, an important task is to integrate the heterogeneous KBs. Several automatic alignment methods have been proposed which achieve considerable success. However, due to the inconsistency and uncertainty of large-scale KBs, automatic techniques for KBs alignment achieve low quality (especially recall). Thanks to the open crowdsourcing platforms, we can harness the crowd to improve the alignment quality. To achieve this goal, in this paper we propose a novel hybrid human-machine framework for large-scale KB integration. We rst partition the entities of different KBs into many smaller blocks based on their relations. We then construct a partial order on these partitions and develop an inference model which crowdsources a set of tasks to the crowd and infers the answers of other tasks based on the crowdsourced tasks. Next we formulate the question selection problem, which, given a monetary budget B, selects B crowdsourced tasks to maximize the number of inferred tasks. We prove that this problem is NP-hard and propose greedy algorithms to address this problem with an approximation ratio of 1--1/e. Our experiments on real-world datasets indicate that our method improves the quality and outperforms state-of-the-art approaches.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1917–1926},
numpages = {10},
keywords = {knowledge base, crowdsourcing, entity alignment},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133025,
author = {Wu, Qingyun and Wang, Hongning and Hong, Liangjie and Shi, Yue},
title = {Returning is Believing: Optimizing Long-Term User Engagement in Recommender Systems},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133025},
doi = {10.1145/3132847.3133025},
abstract = {In this work, we propose to improve long-term user engagement in a recommender system from the perspective of sequential decision optimization, where users' click and return behaviors are directly modeled for online optimization. A bandit-based solution is formulated to balance three competing factors during online learning, including exploitation for immediate click, exploitation for expected future clicks, and exploration of unknowns for model estimation. We rigorously prove that with a high probability our proposed solution achieves a sublinear upper regret bound in maximizing cumulative clicks from a population of users in a given period of time, while a linear regret is inevitable if a user's temporal return behavior is not considered when making the recommendations. Extensive experimentation on both simulations and a large-scale real-world dataset collected from Yahoo frontpage news recommendation log verified the effectiveness and significant improvement of our proposed algorithm compared with several state-of-the-art online learning baselines for recommendation.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1927–1936},
numpages = {10},
keywords = {user long-term engagement modeling, content recommendation, contextual bandit algorithm},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132908,
author = {Zhang, Qizhen and Ye, Tengyuan and Essaidi, Meryem and Agarwal, Shivani and Liu, Vincent and Loo, Boon Thau},
title = {Predicting Startup Crowdfunding Success through Longitudinal Social Engagement Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132908},
doi = {10.1145/3132847.3132908},
abstract = {A key ingredient to a startup's success is its ability to raise funding at an early stage. Crowdfunding has emerged as an exciting new mechanism for connecting startups with potentially thousands of investors. Nonetheless, little is known about its effectiveness, nor the strategies that entrepreneurs should adopt in order to maximize their rate of success. In this paper, we perform a longitudinal data collection and analysis of AngelList - a popular crowdfunding social platform for connecting investors and entrepreneurs. Over a 7-10 month period, we track companies that are actively fund-raising on AngelList, and record their level of social engagement on AngelList, Twitter, and Facebook. Through a series of measures on social en- gagement (e.g. number of tweets, posts, new followers), our analysis shows that active engagement on social media is highly correlated to crowdfunding success. In some cases, the engagement level is an order of magnitude higher for successful companies. We further apply a range of machine learning techniques (e.g. decision tree, SVM, KNN, etc) to predict the ability of a company to success- fully raise funding based on its social engagement and other metrics. Since fund-raising is a rare event, we explore various techniques to deal with class imbalance issues. We observe that some metrics (e.g. AngelList followers and Facebook posts) are more signi cant than other metrics in predicting fund-raising success. Furthermore, despite the class imbalance, we are able to predict crowdfunding success with 84% accuracy.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1937–1946},
numpages = {10},
keywords = {social networks, crowdfunding, machine learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132849,
author = {Gupta, Rupesh and Liang, Guanfeng and Rosales, Romer},
title = {Optimizing Email Volume For Sitewide Engagement},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132849},
doi = {10.1145/3132847.3132849},
abstract = {In this paper we focus on the problem of optimizing email volume for maximizing sitewide engagement of an online social networking service. Email volume optimization approaches published in the past have proposed optimization of email volume for maximization of engagement metrics which are impacted exclusively by email; for example, the number of sessions that begin with clicks on links within emails. The impact of email on such downstream engagement metrics can be estimated easily because of the ease of attribution of such an engagement event to an email. However, this framework is limited in its view of the ecosystem of the networking service which comprises of several tools and utilities that contribute towards delivering value to members; with email being just one such utility. Thus, in this paper we depart from previous approaches by exploring and optimizing the contribution of email to this ecosystem. In particular, we present and contrast the differential impact of email on sitewide engagement metrics for various types of users. We propose a new email volume optimization approach which maximizes sitewide engagement metrics, such as the total number of active users. This is in sharp contrast to the previous approaches whose objective has been maximization of downstream engagement metrics. We present details of our prediction function for predicting the impact of emails on a user's activeness on the mobile or web application. We describe how certain approximations to this prediction function can be made for solving the volume optimization problem, and present results from online A/B tests.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1947–1955},
numpages = {9},
keywords = {optimization, Machine learning, email},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3132978,
author = {Zhuang, Mengdie and Demartini, Gianluca and Toms, Elaine G.},
title = {Understanding Engagement through Search Behaviour},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132978},
doi = {10.1145/3132847.3132978},
abstract = {Evaluating user engagement with search is a critical aspect of understanding how to assess and improve information retrieval systems. While standard techniques for measuring user engagement use questionnaires, these are obtrusive to user interaction, and can only be collected at acceptable intervals. The problem we address is whether there is a less obtrusive and more automatic way to assess how users perceive the search process and outcome. Log files collect behavioural signals (e.g., clicks, queries) from users on a large scale. In this paper, we investigate the potential to predict how users perceive engagement with search by modelling behavioural signals from log files using supervised learning methods. We focus on different engagement dimensions (Perceived Usability, Felt Involvement, Endurability and Novelty) and examine how 37 behavioural features can inform these dimensions. Our results, obtained from 377 in-lab participants undergoing goal-based search tasks, support the connection between perceived engagement and search behaviour. More specifically, we show that time- and query-related features are best suited for predicting user perceived engagement, and suggest that different behavioural features better reflect specific dimensions. We demonstrate the possibility of predicting user-perceived engagement using search behavioural features.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1957–1966},
numpages = {10},
keywords = {search behaviour, engagement prediction, user engagement},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133074,
author = {An, Dong and Gao, Liangcai and Jiang, Zhuoren and Liu, Runtao and Tang, Zhi},
title = {Citation Metadata Extraction via Deep Neural Network-Based Segment Sequence Labeling},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133074},
doi = {10.1145/3132847.3133074},
abstract = {Citation metadata extraction plays an important role in academic information retrieval and knowledge management. Current works on this task generally use rule-based, template-based or learning-based approaches but these methods usually either rely on handcrafted features or are limited with domains. Recently, neural networks have shown strong ability in addressing sequence labeling tasks.In this paper, we propose a sequence labeling model for citation metadata extraction, called segment sequence labeling. Instead of inferring at word level, the input sequence is first divided into segments, and then features of the segments are computed to infer the label sequence of the segments. We first run experiments to validate the effectiveness of different parts of the model by comparing it with a CRF-based model and a neural network-based model. Experimental results show our model beats both models on most fields. Besides, our model is evaluated on public datasets UMass and Cora and has achieved significant performance improvement. Our model was trained on the data which were generated from BibTeX files collected on the Web and annotated automatically.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1967–1970},
numpages = {4},
keywords = {academic information extraction, citation metadata extraction, information retrieval, sequence labeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133117,
author = {Anwar, Samiul and Nabila, Shuha and Hashem, Tanzima},
title = {A Novel Approach for Efficient Computation of Community Aware Ridesharing Groups},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133117},
doi = {10.1145/3132847.3133117},
abstract = {The evolution of ridesharing services has reduced the road traffic congestions in recent years. However, a major concern for ridesharing services is sharing rides with strangers. To address this issue, a few ridesharing approaches have considered social closeness of group members for identifying a ridesharing group. Again, users do not feel comfortable to disclose such personal data (e.g, friendship information) with an untrusted service provider for privacy reasons. We propose a novel way to form ridesharing groups that reveals user social data in community levels, and ensures that a group member shares at least k common communities with at least other m members in the ridesharing group, where k and m are personalized parameters of every group member. We formulate a Community aware Ridesharing Group (CaRG) query that satisfies the constraints of m and k, and returns a ridesharing group with the minimum cost in terms of the spatial proximity of riders from the driver. We show in experiments that our approach to process CaRG queries outperforms a baseline approach with a large margin.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1971–1974},
numpages = {4},
keywords = {ridesharing, query processing, community, location-based services},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133141,
author = {Arora, Jatin and Agrawal, Sumit and Goyal, Pawan and Pathak, Sayan},
title = {Extracting Entities of Interest from Comparative Product Reviews},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133141},
doi = {10.1145/3132847.3133141},
abstract = {This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1975–1978},
numpages = {4},
keywords = {deep learning, opinion extraction, comparison mining},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133083,
author = {Bai, Ting and Wen, Ji-Rong and Zhang, Jun and Zhao, Wayne Xin},
title = {A Neural Collaborative Filtering Model with Interaction-Based Neighborhood},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133083},
doi = {10.1145/3132847.3133083},
abstract = {Recently, deep neural networks have been widely applied to recommender systems. A representative work is to utilize deep learning for modeling complex user-item interactions. However, similar to traditional latent factor models by factorizing user-item interactions, they tend to be ineffective to capture localized information. Localized information, such as neighborhood, is important to recommender systems in complementing the user-item interaction data. Based on this consideration, we propose a novel Neighborhood-based Neural Collaborative Filtering model (NNCF). To the best of our knowledge, it is the first time that the neighborhood information is integrated into the neural collaborative filtering methods. Extensive experiments on three real-world datasets demonstrate the effectiveness of our model for the implicit recommendation task.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1979–1982},
numpages = {4},
keywords = {deep neural network, neighborhood information, recommender systems},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133155,
author = {Berti-Equille, Laure and Zhauniarovich, Yury},
title = {Profiling DRDoS Attacks with Data Analytics Pipeline},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133155},
doi = {10.1145/3132847.3133155},
abstract = {A large amount of Distributed Reflective Denial-of-Service (DRDoS) attacks are launched every day, and our understanding of the modus operandi of their perpetrators is yet very limited as we are submerged with so Big Data to analyze and do not have reliable and complete ways to validate our findings. In this paper, we propose a first analytic pipeline that enables us to cluster and characterize attack campaigns into several main profiles that exhibit similarities. These similarities are due to common technical properties of the underlying infrastructures used to launch these attacks. Although we do not have access to the ground truth and we do not know how many perpetrators are acting behind the scene, we can group their attacks based on relevant commonalities with cluster ensembling to estimate their number and capture their profiles over time. Specifically, our results show that we can repeatably identify and group together common profiles of attacks while considering domain expert's constraint in the cluster ensembles. From the obtained consensus clusters, we can generate comprehensive rules that characterize past campaigns and that can be used for classifying the next ones despite the evolving nature of the attacks. Such rules can be further used to filter out garbage traffic in Internet Service Provider networks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1983–1986},
numpages = {4},
keywords = {clustering, ensembling, profiling, analytics, distributed reflective denial-of-service, pipeline},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133089,
author = {Bian, Weijie and Li, Si and Yang, Zhao and Chen, Guang and Lin, Zhiqing},
title = {A Compare-Aggregate Model with Dynamic-Clip Attention for Answer Selection},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133089},
doi = {10.1145/3132847.3133089},
abstract = {Answer selection for question answering is a challenging task, since it requires effective capture of the complex semantic relations between questions and answers. Previous remarkable approaches mainly adopt general Compare-Aggregate framework that performs word-level comparison and aggregation. In this paper, unlike previous Compare-Aggregate models which utilize the traditional attention mechanism to generate corresponding word-level vector before comparison, we propose a novel attention mechanism named Dynamic-Clip Attention which is directly integrated into the Compare-Aggregate framework. Dynamic-Clip Attention focuses on filtering out noise in attention matrix, in order to better mine the semantic relevance of word-level vectors. At the same time, different from previous Compare-Aggregate works which treat answer selection task as a pointwise classification problem, we propose a listwise ranking approach to model this task to learn the relative order of candidate answers. Experiments on TrecQA and WikiQA datasets show that our proposed model achieves the state-of-the-art performance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1987–1990},
numpages = {4},
keywords = {dynamic-clip attention, deep learning, question answering, listwise},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133051,
author = {Bouadjenek, Mohamed Reda and Verspoor, Karin and Zobel, Justin},
title = {Learning Biological Sequence Types Using the Literature},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133051},
doi = {10.1145/3132847.3133051},
abstract = {We explore in this paper automatic biological sequence type classification for records in biological sequence databases. The sequence type attribute provides important information about the nature of a sequence represented in a record, and is often used in search to filter out irrelevant sequences. However, the sequence type attribute is generally a non-mandatory free-text field, and thus it is subject to many errors including typos, mis-assignment, and non-assignment. In GenBank, this problem concerns roughly 18% of records, an alarming number that should worry the biocuration community. To address this problem of automatic sequence type classification, we propose the use of literature associated to sequence records as an external source of knowledge that can be leveraged for the classification task. We define a set of literature-based features and train a machine learning algorithm to classify a record into one of six primary sequence types. The main intuition behind using the literature for this task is that sequences appear to be discussed differently in scientific articles, depending on their type. The experiments we have conducted on the PubMed Central collection show that the literature is indeed an effective way to address this problem of sequence type classification. Our classification method reached an accuracy of 92.7%, and substantially outperformed two baseline approaches used for comparison.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1991–1994},
numpages = {4},
keywords = {data cleansing, data analysis, biological databases, data quality},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133050,
author = {Cai, Chiyu and Li, Linjing and Zeng, Daniel},
title = {Detecting Social Bots by Jointly Modeling Deep Behavior and Content Information},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133050},
doi = {10.1145/3132847.3133050},
abstract = {Bots are regarded as the most common kind of malwares in the era of Web 2.0. In recent years, Internet has been populated by hundreds of millions of bots, especially on social media. Thus, the demand on effective and efficient bot detection algorithms is more urgent than ever. Existing works have partly satisfied this requirement by way of laborious feature engineering. In this paper, we propose a deep bot detection model aiming to learn an effective representation of social user and then detect social bots by jointly modeling social behavior and content information. The proposed model learns the representation of social behavior by encoding both endogenous and exogenous factors which affect user behavior. As to the representation of content, we regard the user content as temporal text data instead of just plain text as be treated in other existing works to extract semantic information and latent temporal patterns. To the best of our knowledge, this is the first trial that applies deep learning in modeling social users and accomplishing social bot detection. Experiments on real world dataset collected from Twitter demonstrate the effectiveness of the proposed model.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1995–1998},
numpages = {4},
keywords = {bot detection, behavior factors, temporal content, deep learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133087,
author = {Cao, Yingjie and Zhang, Yangyang and Li, Jianxin},
title = {PMS: An Effective Approximation Approach for Distributed Large-Scale Graph Data Processing and Mining},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133087},
doi = {10.1145/3132847.3133087},
abstract = {Recently, large-scale graph data processing and mining has drawn great attention, and many distributed graph processing systems have been proposed. However, large-scale graph processing remains a challenging problem. Because the computation time in some cases is still unacceptable especially when the time is limited. As illustrated in Table 1, nearly three hours are needed when running Single-Source Shortest Path algorithm on the USA-road dataset using performant open-source distributed graph processing systems.In this paper, we propose an effective priority-based message sampling (PMS ) approach to further improve the performance of distributed graph processing at the cost of some accuracy loss. Noticing that the passing and processing of messages dominates the computation time, our approach works by eliminating those less useful messages directly without passing them which can effectively reduce the computation overhead. We implement our approach basing on Apache Giraph, a popular open-source implementation of Google's Pregel and report the primary results of our prototype system. The experimental results show that our approach can achieve reasonable accuracy with much less computation time.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1999–2002},
numpages = {4},
keywords = {distributed system, approximate computation, large-scale graph processing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133104,
author = {Cha, Miriam and Gwon, Youngjune and Kung, H. T.},
title = {Language Modeling by Clustering with Word Embeddings for Text Readability Assessment},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133104},
doi = {10.1145/3132847.3133104},
abstract = {We present a clustering-based language model using word embeddings for text readability prediction. Presumably, an Euclidean semantic space hypothesis holds true for word embeddings whose training is done by observing word co-occurrences. We argue that clustering with word embeddings in the metric space should yield feature representations in a higher semantic space appropriate for text regression. Also, by representing features in terms of histograms, our approach can naturally address documents of varying lengths. An empirical evaluation using the Common Core Standards corpus reveals that the features formed on our clustering-based language model significantly improve the previously known results for the same corpus in readability prediction. We also evaluate the task of sentence matching based on semantic relatedness using the Wiki-SimpleWiki corpus and find that our features lead to superior matching performance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2003–2006},
numpages = {4},
keywords = {readability assessment, clustering-based language model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133070,
author = {Chai, Jing and Liu, Weiwei and Tsang, Ivor W. and Shen, Xiaobo},
title = {Compact Multiple-Instance Learning},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133070},
doi = {10.1145/3132847.3133070},
abstract = {The weakly supervised Multiple-Instance Learning (MIL) problem has been successfully applied in information retrieval tasks. Two related issues might affect the performance of MIL algorithms: how to cope with label ambiguities and how to deal with non-discriminative components, and we propose COmpact MultiPle-Instance LEarning (COMPILE) to consider them simultaneously. To treat label ambiguities, COMPILE seeks ground-truth positive instances in positive bags. By using weakly supervised information to learn data's short binary representations, COMPILE enhances discrimination via strengthening discriminative components and suppressing non-discriminative ones. We adapt block coordinate descent to optimize COMPILE efficiently. Experiments on text categorization empirically show: 1) COMPILE unifies disambiguation and data preprocessing successfully; 2) it generates short binary representations efficiently to enhance discrimination at significantly reduced storage cost.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2007–2010},
numpages = {4},
keywords = {disambiguation, storage cost, multiple-instance learning, text categorization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133066,
author = {Chao, Chih-Yu and Chu, Yi-Fan and Yang, Hsiu-Wei and Wang, Chuan-Ju and Tsai, Ming-Feng},
title = {Text Embedding for Sub-Entity Ranking from User Reviews},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133066},
doi = {10.1145/3132847.3133066},
abstract = {This paper attempts to conduct analysis for one certain type of user reviews; that is, the reviews on a super-entity (e.g., restaurant) involve descriptions for many sub-entities (e.g., dishes). To deal with such analysis, we propose a text embedding framework for ranking sub-entities from user reviews of a given super-entity. Experiments on two real-world datasets show that our method outperforms three baselines by a statistically significant amount. Intriguing cases from the experiments are discussed in the paper.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2011–2014},
numpages = {4},
keywords = {co-occurrence network, user reviews, ranking, text embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133111,
author = {Chavary, Elaheh Alipour and Erfani, Sarah M. and Leckie, Christopher},
title = {Summarizing Significant Changes in Network Traffic Using Contrast Pattern Mining},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133111},
doi = {10.1145/3132847.3133111},
abstract = {Extracting knowledge from the massive volumes of network traffic is an important challenge in network and security management. In particular, network managers require concise reports about significant changes in their network traffic. While most existing techniques focus on summarizing a single traffic dataset, the problem of finding significant differences between multiple datasets is an open challenge. In this paper, we focus on finding important differences between network traffic datasets, and preparing a summarized and interpretable report for security managers. We propose the use of contrast pattern mining, which finds patterns whose support differs significantly from one dataset to another. We show that contrast patterns are highly effective at extracting meaningful changes in traffic data. We also propose several evaluation metrics that reflect the interpretability of patterns for security managers. Our experimental results show that with the proposed unsupervised approach, the vast majority of extracted patterns are pure, i.e., most changes are either attack traffic or normal traffic, but not a mixture of both.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2015–2018},
numpages = {4},
keywords = {dataset summarization, closed patterns, contrast patterns},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133125,
author = {Chen, Chengyao and Wang, Zhitao and Li, Wenjie},
title = {Modeling Opinion Influence with User Dual Identity},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133125},
doi = {10.1145/3132847.3133125},
abstract = {Exploring the mechanism that explains how a user's opinion changes under the influence of his/her neighbors is of practical importance (e.g., for predicting the sentiment of his/her future opinion) and has attracted wide attention from both enterprises and academics.Though various opinion influence models have been proposed for opinion prediction, they only consider users' personal identities, but ignore their social identities with which people behave to fit the expectations of the others in the same group. In this work, we explore users' dual identities, including both personal identities and social identities to build a more comprehensive opinion influence model for a better understanding of opinion behaviors. A novel joint learning framework is proposed to simultaneously model opinion dynamics and detect social identity in a unified model. The effectiveness of the proposed approach is demonstrated through the experiments conducted on Twitter datasets},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2019–2022},
numpages = {4},
keywords = {joint learning, opinion influence modeling, dual identity},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133151,
author = {Chen, Ruey-Cheng and Azzopardi, Leif and Scholer, Falk},
title = {An Empirical Analysis of Pruning Techniques: Performance, Retrievability and Bias},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133151},
doi = {10.1145/3132847.3133151},
abstract = {Prior work on using retrievability measures in the evaluation of information retrieval (IR) systems has laid out the foundations for investigating the relation between retrieval performance and retrieval bias. While various factors influencing retrievability have been examined, showing how the retrieval model may influence bias, no prior work has examined the impact of the index (and how it is optimized) on retrieval bias. Intuitively, how the documents are represented, and what terms they contain, will influence whether they are retrievable or not. In this paper, we investigate how the retrieval bias of a system changes as the inverted index is optimized for efficiency through static index pruning. In our analysis, we consider four pruning methods and examine how they affect performance and bias on the TREC GOV2 Collection. Our results show that the relationship between these factors is varied and complex - and very much dependent on the pruning algorithm. We find that more pruning results in relatively little change or a slight decrease in bias up to a point, and then a dramatic increase. The increase in bias corresponds to a sharp decrease in early precision such as NDCG@10 and is also indicative of a large decrease in MAP. The findings suggest that the impact of pruning algorithms can be quite varied - but retrieval bias could be used to guide the pruning process. Further work is required to determine precisely which documents are most affected and how this impacts upon performance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2023–2026},
numpages = {4},
keywords = {pruning, retrievability, indexing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133047,
author = {Cui, Baiyun and Li, Yingming and Zhang, Yaqing and Zhang, Zhongfei},
title = {Text Coherence Analysis Based on Deep Neural Network},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133047},
doi = {10.1145/3132847.3133047},
abstract = {In this paper, we propose a novel deep coherence model (DCM) using a convolutional neural network architecture to capture the text coherence. The text coherence problem is investigated with a new perspective of learning sentence distributional representation and text coherence modeling simultaneously. In particular, the model captures the interactions between sentences by computing the similarities of their distributional representations. Further, it can be easily trained in an end-to-end fashion. The proposed model is evaluated on a standard Sentence Ordering task. The experimental results demonstrate its effectiveness and promise in coherence assessment showing a significant improvement over the state-of-the-art by a wide margin.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2027–2030},
numpages = {4},
keywords = {coherence analysis, deep coherence model, distributional representation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133114,
author = {Dang, Shaobo and Cai, Xiongcai and Wang, Yang and Zhang, Jianjia and Chen, Fang},
title = {Unsupervised Matrix-Valued Kernel Learning For One Class Classification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133114},
doi = {10.1145/3132847.3133114},
abstract = {This paper is concerned with the one class classification(OCC) problem. By introducing the vector-valued function with regularizations in Y-valued Reproducing Hilbert Kernel Space(RHKS), we build an unsupervised classifier and discover the outliers and inliers simultaneously. Manifold regularization is employed to preserve the local similarity of data in input space. Experimental results of the proposed and comparing methods on OCC data sets demonstrate the performance of the proposed algorithm.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2031–2034},
numpages = {4},
keywords = {unsupervised learning, outlier detection, one class classification, kernel learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133132,
author = {Dargahi Nobari, Arash and Reshadatmand, Negar and Neshati, Mahmood},
title = {Analysis of Telegram, An Instant Messaging Service},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133132},
doi = {10.1145/3132847.3133132},
abstract = {Telegram has become one of the most successful instant messaging services in recent years. In this paper, we developed a crawler to gather its public data. To the best of our knowledge, this paper is the first attempt to analyze the structural and topical aspects of messages published in Telegram instant messaging service using crawled data. We also extracted the mention graph and page rank of our data collection which indicates important differences between linking patterns of Telegram nodes and other usual networks. We also classified messages to detect advertisement and spam messages.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2035–2038},
numpages = {4},
keywords = {spam detection, classification, instant messaging, telegram, pagerank},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133131,
author = {Das, Supratim and Mishra, Arunav and Berberich, Klaus and Setty, Vinay},
title = {Estimating Event Focus Time Using Neural Word Embeddings},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133131},
doi = {10.1145/3132847.3133131},
abstract = {Time associated with news events has been leveraged as a complementary dimension to text in several applications such as temporal information retrieval, news event linking, etc. Short textual event descriptions (e.g., single sentences) are prevalent in web documents (also considered as inputs in the above applications) and often lack explicit temporal expressions for grounding them to a precise time period. For example, the event description, "France swears in Emmanuel Macron as the 25th President", lacks temporal cues to indicate that the event occurred in the year "2017". Thus, we address the problem of estimating event focus time defined as a time interval with maximum association thereby indicating its occurrence period. We propose several estimators that leverage distributional event and time representations learned from large external document collections by adapting the word2vec paradigm. Extensive experiments using two real-world datasets and 100 Wikipedia events show that our method outperforms several state-of-the-art baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2039–2042},
numpages = {4},
keywords = {event vectors, neural word embeddings, word2vec, time vectors, pseudo relevance feedback, event focus time},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133052,
author = {Deng, Xiang and Cui, Chaoran and Fang, Huidi and Nie, Xiushan and Yin, Yilong},
title = {Personalized Image Aesthetics Assessment},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133052},
doi = {10.1145/3132847.3133052},
abstract = {Automatically assessing image quality from an aesthetic perspective is of great interest to the high-level vision research community. Existing methods are typically non-personalized and quantify image aesthetics with a universal label. However, given the fact that aesthetics is a subjective perception, how to understand user aesthetic perceptions poses a formidable challenge to image aesthetics assessment. In this paper, we propose to model user aesthetic perceptions using a set of exemplar images from social media platforms, and realize personalized aesthetics assessment by transferring this knowledge to adapt the results of the trained generic model. In this way, image aesthetics is measured from both aspects of visual quality and user tastes. Extensive experiments on two benchmark datasets well verified the potential of our approach for personalized image aesthetics assessment.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2043–2046},
numpages = {4},
keywords = {social media, personalization, image aesthetics assessment},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133130,
author = {Ding, Danhao and Li, Hui and Huang, Zhipeng and Mamoulis, Nikos},
title = {Efficient Fault-Tolerant Group Recommendation Using Alpha-Beta-Core},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133130},
doi = {10.1145/3132847.3133130},
abstract = {Fault-tolerant group recommendation systems based on subspace clustering successfully alleviate high-dimensionality and sparsity problems. However, the cost of recommendation grows exponentially with the size of dataset. To address this issue, we model the fault-tolerant subspace clustering problem as a search problem on graphs and present an algorithm, GraphRec, based on the concept of α-\ss{}-core. Moreover, we propose two variants of our approach that use indexes to improve query latency. Our experiments on different datasets demonstrate that our methods are extremely fast compared to the state-of-the-art.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2047–2050},
numpages = {4},
keywords = {subspace clustering, fault tolerance, group recommendation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133086,
author = {Duong-Trung, Nghia and Schmidt-Thieme, Lars},
title = {On Discovering the Number of Document Topics via Conceptual Latent Space},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133086},
doi = {10.1145/3132847.3133086},
abstract = {Topic modeling is a widely used technique in knowledge discovery and data mining. However, finding the right number of topics in a given text source has remained a challenging issue. In this paper, we study the concept of conceptual stability via nonnegative matrix factorization. Based on this finding, we propose a method to identify the correct number of topics and offer empirical evidence in its favor in terms of classification accuracy and the number of topics that are naturally present in the text sources. Experiments on real-world text corpora demonstrate that the proposed method has outperformed state-of-the-art latent Dirichlet allocation and nonnegative matrix factorization models.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2051–2054},
numpages = {4},
keywords = {nonnegative matrix factorization, stability analysis, topic modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133088,
author = {E, Shijia and Xiang, Yang},
title = {Chinese Named Entity Recognition with Character-Word Mixed Embedding},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133088},
doi = {10.1145/3132847.3133088},
abstract = {Named Entity Recognition (NER) is an important basis for the tasks in natural language processing such as relation extraction, entity linking and so on. The common method of existing Chinese NER systems is to use the character sequence as the input, and the intention is to avoid the word segmentation. However, the character sequence cannot express enough semantic information, so that the recognition accuracy of Chinese NER is not as good as western language such as English. To solve this issue, we propose a Chinese NER method based on Character-Word Mixed Embedding (CWME), and the method is in accord with the pipeline of Chinese natural language processing. Our experiments show that incorporating CWME can effectively improve the performance for the Chinese corpus with state-of-the-art neural architectures widely used in NER, and the proposed method yields nearly 9% absolute improvement over previously results.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2055–2058},
numpages = {4},
keywords = {word embedding, named entity recognition, character embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133138,
author = {Ensan, Faezeh and Bagheri, Ebrahim and Zouaq, Amal and Kouznetsov, Alexandre},
title = {An Empirical Study of Embedding Features in Learning to Rank},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133138},
doi = {10.1145/3132847.3133138},
abstract = {This paper explores the possibility of using neural embedding features for enhancing the effectiveness of ad hoc document ranking based on learning to rank models. We have extensively introduced and investigated the effectiveness of features learnt based on word and document embeddings to represent both queries and documents. We employ several learning to rank methods for document ranking using embedding-based features, keyword-based features as well as the interpolation of the embedding-based features with keyword-based features. The results show that embedding features have a synergistic impact on keyword based features and are able to provide statistically significant improvement on harder queries.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2059–2062},
numpages = {4},
keywords = {learning to rank, ad hoc retrieval, neural embeddings},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133140,
author = {Eslami, Sedigheh and Biega, Asia J. and Saha Roy, Rishiraj and Weikum, Gerhard},
title = {Privacy of Hidden Profiles: Utility-Preserving Profile Removal in Online Forums},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133140},
doi = {10.1145/3132847.3133140},
abstract = {Users who wish to leave an online forum often do not have the freedom to erase their data completely from the service providers' (SP) system. The primary reason behind this is that analytics on such user data form a core component of many online providers' business models. On the other hand, if the profiles reside in the SP's system in an unchanged form, major privacy violations may occur if the infrastructure is compromised, or the SP is acquired by another organization. In this work, we investigate an alternative solution to standard profile removal, where posts of different users are split and merged into synthetic mediator profiles. The goal of our framework is to preserve the SP's data mining utility as far as possible, while minimizing users' privacy risks. We present several mechanisms of assigning user posts to such mediator accounts and show the effectiveness of our framework using data from StackExchange and various health forums.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2063–2066},
numpages = {4},
keywords = {privacy-utility tradeoff, user privacy, profile removal, mediator accounts, split and merge, provider utility},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133045,
author = {Fang, Zhou and Yu, Tong and Mengshoel, Ole J. and Gupta, Rajesh K.},
title = {QoS-Aware Scheduling of Heterogeneous Servers for Inference in Deep Neural Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133045},
doi = {10.1145/3132847.3133045},
abstract = {Deep neural networks (DNNs) are popular in diverse fields such as computer vision and natural language processing. DNN inference tasks are emerging as a service provided by cloud computing environments. However, cloud-hosted DNN inference faces new challenges in workload scheduling for the best Quality of Service (QoS), due to dependence on batch size, model complexity and resource allocation. This paper represents the QoS metric as a utility function of response delay and inference accuracy. We first propose a simple and effective heuristic approach that keeps low response delay and satisfies the requirement on processing throughput. Then we describe an advanced deep reinforcement learning (RL) approach that learns to schedule from experience. The RL scheduler is trained to maximize QoS, using a set of system statuses as the input to the RL policy model. Our approach performs scheduling actions only when there are free GPUs, thus reduces scheduling overhead over common RL schedulers that run at every continuous time step. We evaluate the schedulers on a simulation platform and demonstrate the advantages of RL over heuristics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2067–2070},
numpages = {4},
keywords = {reinforcement learning, deep reinforcement learning, web service, qos aware scheduling, deep neural networks inference},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133147,
author = {Fourney, Adam and Racz, Miklos Z. and Ranade, Gireeja and Mobius, Markus and Horvitz, Eric},
title = {Geographic and Temporal Trends in Fake News Consumption During the 2016 US Presidential Election},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133147},
doi = {10.1145/3132847.3133147},
abstract = {We present an analysis of traffic to websites known for publishing fake news in the months preceding the 2016 US presidential election. The study is based on the combined instrumentation data from two popular desktop web browsers: Internet Explorer 11 and Edge. We find that social media was the primary outlet for the circulation of fake news stories and that aggregate voting patterns were strongly correlated with the average daily fraction of users visiting websites serving fake news. This correlation was observed both at the state level and at the county level, and remained stable throughout the main election season. We propose a simple model based on homophily in social networks to explain the linear association. Finally, we highlight examples of different types of fake news stories: while certain stories continue to circulate in the population, others are short-lived and die out in a few days.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2071–2074},
numpages = {4},
keywords = {browsing data, fake news, elections, social media},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133146,
author = {Garcia, Felan Carlo C. and Macabebe, Erees Queen B.},
title = {Inferring Appliance Energy Usage from Smart Meters Using Fully Convolutional Encoder Decoder Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133146},
doi = {10.1145/3132847.3133146},
abstract = {Energy management presents one of the principal sustainability challenges within urban centers given that they account for 75% of the energy consumption worldwide. In the context of a smart city framework, the use of intelligent urban systems provides a key opportunity in addressing the energy sustainability issue as an informatics problem where the goal is to deliver energy usage feedback to the users as a means of enabling behavioral change towards energy sustainability. In this paper we present a method to provide appliance energy usage feedback from smart meters using energy disaggregation. We put energy disaggregation in the context of a source separation and signal reconstruction problem in which we train a fully convolutional encoder decoder network to separate appliance energy usage from aggregate whole house electricity consumption data. The results show that the proposed fully convolutional encoder decoder model can achieve competitive accuracy compared with several state-of-the-art methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2075–2078},
numpages = {4},
keywords = {ambient intelligence, energy management, energy disaggregation, smart city, deep learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133118,
author = {Gaur, Garima and Bedathur, Srikanta J. and Bhattacharya, Arnab},
title = {Tracking the Impact of Fact Deletions on Knowledge Graph Queries Using Provenance Polynomials},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133118},
doi = {10.1145/3132847.3133118},
abstract = {Critical business applications in domains ranging from technical support to healthcare increasingly rely on large-scale, automatically constructed knowledge graphs. These applications use the results of complex queries over knowledge graphs in order to help users in taking crucial decisions such as which drug to administer, or whether certain actions are compliant with all the regulatory requirements and so on. However, these knowledge graphs constantly evolve, and the newer versions may adversely impact the results of queries that the previously taken business decisions were based on. We propose a framework based on provenance polynomials to track the impact of knowledge graph changes on arbitrary SPARQL query results. Focusing on the deletion of facts, we show how to efficiently determine the queries impacted by the change, develop ways to incrementally maintain these polynomials, and present an efficient implementation on top of RDF graph databases. Our experimental evaluation over large-scale RDF/SPARQL benchmarks show the effectiveness of our proposal.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2079–2082},
numpages = {4},
keywords = {fact deletion, provenance polynomial, knowledge graph},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133062,
author = {Gu, Lei and Zhang, Liying and Zhao, Yang},
title = {An Euclidean Distance Based on the Weighted Self-Information Related Data Transformation for Nominal Data Clustering},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133062},
doi = {10.1145/3132847.3133062},
abstract = {Numerical data clustering is a tractable task since well-defined numerical measures like traditional Euclidean distance can be directly used for it, but nominal data clustering is a very difficult problem because there exists no natural relative ordering between nominal attribute values. This paper mainly aims to make the Euclidean distance measure appropriate to nominal data clustering, and the core idea is to transform each nominal attribute value into numerical. This transformation method consists of three steps. In the first step, the weighted self-information, which can quantify the amount of information in attribute values, is calculated for each value in each nominal attribute. In the second step, we find k nearest neighbors for each object because k nearest neighbors of one object have close similarities with it. In the last step, the weighted self-information of each attribute value in each nominal object is modified according to the object's k nearest neighbors. To evaluate the effectiveness of our proposed method, experiments are done on 10 data sets. Experimental results demonstrate that our method not only enables the Euclidean distance to be used for nominal data clustering, but also can acquire the better clustering performance than several existing state-of-the-art approaches.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2083–2086},
numpages = {4},
keywords = {euclidean distance, self-information, nominal data clustering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133061,
author = {Gupta, Mukul and Kumar, Pradeep and Mishra, Rajhans},
title = {Interest Diffusion in Heterogeneous Information Network for Personalized Item Ranking},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133061},
doi = {10.1145/3132847.3133061},
abstract = {Personalized item ranking for recommending top-N items of interest to a user is an interesting and challenging problem in e-commerce. Researchers and practitioner are continuously trying to devise new methodologies to improve the accuracy of recommendations. Recommendation problem becomes more challenging for sparse binary implicit feedback, due to the absence of explicit signals of interest and sparseness of data. In this paper, we deal with the problem of the sparseness of data and accuracy of recommendations. To address the issue, we propose an interest diffusion methodology in heterogeneous information network for items to be recommended using the meta-information related to items. In this heterogeneous information network, graph regularized interest diffusion is performed to generate personalized recommendations of top-N items. For interest diffusion, personalized weight learning is performed for different meta-information object types in the network. The experimental evaluation and comparison of the proposed methodology with the state-of-the-art techniques using the real-world datasets show the effectiveness of the proposed approach},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2087–2090},
numpages = {4},
keywords = {meta-information, implicit feedback, interest diffusion, heterogeneous information network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133097,
author = {Hagen, Matthias and Potthast, Martin and Adineh, Payam and Fatehifar, Ehsan and Stein, Benno},
title = {Source Retrieval for Web-Scale Text Reuse Detection},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133097},
doi = {10.1145/3132847.3133097},
abstract = {The first step of text reuse detection addresses the source retrieval problem: given a suspicious document, a set of candidate sources from which text might have been reused have to be retrieved by querying a search engine. Afterwards, in a second step, the retrieved candidates run through a text alignment with the suspicious document in order to identify reused passages. Obviously, any true source of text reuse that is not retrieved during the source retrieval step reduces the overall recall of a reuse detector. Hence, source retrieval is a recall-oriented task, a fact ignored even by experts: Only 3 of 20 teams participating in a respective task at PAN 2012-2016 managed to find more than half of the sources, the best one achieving a recall of only~0.59. We propose a new approach that reaches a recall of~0.89---a performance gain of~51%.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2091–2094},
numpages = {4},
keywords = {pan, text reuse detection, query formulation, plagiarism detection, source retrieval, recall-oriented retrieval},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133101,
author = {Hansen, Casper and Hansen, Christian and Alstrup, Stephen and Lioma, Christina},
title = {Smart City Analytics: Ensemble-Learned Prediction of Citizen Home Care},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133101},
doi = {10.1145/3132847.3133101},
abstract = {We present an ensemble learning method that predicts large increases in the hours of home care received by citizens. The method is supervised, and uses different ensembles of either linear (logistic regression) or non-linear (random forests) classifiers. Experiments with data available from 2013 to 2017 for every citizen in Copenhagen receiving home care (27,775 citizens) show that prediction can achieve state of the art performance as reported in similar health related domains (AUC=0.715). We further find that competitive results can be obtained by using limited information for training, which is very useful when full records are not accessible or available. Smart city analytics does not necessarily require full city records. To our knowledge this preliminary study is the first to predict large increases in home care for smart city analytics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2095–2098},
numpages = {4},
keywords = {home care, ensemble learning, smart city analytics},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133091,
author = {Hu, Qinghao and Wu, Jiaxiang and Bai, Lu and Zhang, Yifan and Cheng, Jian},
title = {Fast K-Means for Large Scale Clustering},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133091},
doi = {10.1145/3132847.3133091},
abstract = {K-means algorithm has been widely used in machine learning and data mining due to its simplicity and good performance. However, the standard k-means algorithm would be quite slow for clustering millions of data into thousands of or even tens of thousands of clusters. In this paper, we propose a fast k-means algorithm named multi-stage k-means (MKM) which uses a multi-stage filtering approach. The multi-stage filtering approach greatly accelerates the k-means algorithm via a coarse-to-fine search strategy. To further speed up the algorithm, hashing is introduced to accelerate the assignment step which is the most time-consuming part in k-means. Extensive experiments on several massive datasets show that the proposed algorithm can obtain up to 600X speed-up over the k-means algorithm with comparable accuracy.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2099–2102},
numpages = {4},
keywords = {hashing, k-means, clustering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133124,
author = {Hu, Ruiqi and Pan, Shirui and Jiang, Jing and Long, Guodong},
title = {Graph Ladder Networks for Network Classification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133124},
doi = {10.1145/3132847.3133124},
abstract = {Numerous network representation-based algorithms for network classification have emerged in recent years, but many suffer from two limitations. First, they separate the network representation learning and node classification in networks into two steps, which may result in sub-optimal results because the node representation may not fit the classification model well, and vice versa. Second, they are mostly shallow methods that can only capture the linear and simple relationships in the data. In this paper, we propose an effective deep learning model, Graph Ladder Networks (GLN), for node classification in networks. Our model learns a ladder network which unifies the representation learning and network classification into one single framework by exploiting both labeled and unlabeled nodes in a network. To integrate both structure and node content information in the networks, the most recently developed graph convolution network, is further employed. The experiments on the most popular academic network dataset, Citeseer, demonstrate that our approach reaches outstanding performance compared to other state-of-the-art algorithms.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2103–2106},
numpages = {4},
keywords = {ladder network, network classification., network representation, graph convolutional network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133112,
author = {Hu, Xu and Huang, Jun and Qiu, Minghui},
title = {A Communication Efficient Parallel DBSCAN Algorithm Based on Parameter Server},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133112},
doi = {10.1145/3132847.3133112},
abstract = {Recent benchmark studies show that MPI-based distributed implementations of DBSCAN, e.g., PDSDBSCAN, outperform other implementations such as apache Spark etc. However, the communication cost of MPI DBSCAN increases drastically with the number of processors, which makes it inefficient for large scale problems.In this paper, we propose PS-DBSCAN, a parallel DBSCAN algorithm that combines the disjoint-set data structure and Parameter Server framework, to minimize communication cost. Since data points within the same cluster may be distributed over different workers which result in several disjoint-sets, merging them incurs large communication costs. In our algorithm, we employ a fast global union approach to union the disjoint-sets to alleviate the communication burden. Experiments over the datasets of different scales demonstrate that PS-DBSCAN outperforms the PDSDBSCAN with 2-10 times speedup on communication efficiency. We have released our PS-DBSCAN in an algorithm platform called Platform of AI (PAI) in Alibaba Cloud.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2107–2110},
numpages = {4},
keywords = {parameter server, parallel dbscan, density-based clustering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133123,
author = {Huang, Longtao and Zhao, Lin and Lv, Shangwen and Lu, Fangzhou and Zhai, Yue and Hu, Songlin},
title = {KIEM: A Knowledge Graph Based Method to Identify Entity Morphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133123},
doi = {10.1145/3132847.3133123},
abstract = {An entity on the web can be referred by numerous morphs that are always ambiguous, implicit and informal, which makes it challenging to accurately identify all the morphs corresponding to a specific entity. In this paper, we introduce a novel method based on knowledge graph, which takes advantage of both knowledge reasoning and statistic learning. First, we present a model to build a knowledge graph for the given entity. The knowledge graph integrates the fragmented knowledge on how humans create morphs. Then, the candidate morphs are generated based on the rules summarized from the knowledge graph. At last, we use a classification method to filter the useless candidates and identify the target morphs. The experiments conducted on real world dataset demonstrate efficiency of our proposed method in terms of precision and recall.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2111–2114},
numpages = {4},
keywords = {knowledge graph, web mining, language understanding, entity morphs},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133113,
author = {Huang, Xin and Choi, Byron and Xu, Jianliang and Cheung, William K. and Zhang, Yanchun and Liu, Jiming},
title = {Ontology-Based Graph Visualization for Summarized View},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133113},
doi = {10.1145/3132847.3133113},
abstract = {Data summarization that presents a small subset of a dataset to users has been widely applied in numerous applications and systems. Many datasets are coded with hierarchical terminologies, e.g., the international classification of Diseases-9, Medical Subject Heading, and Gene Ontology, to name a few. In this paper, we study the problem of selecting a diverse set of k elements to summarize an input dataset with hierarchical terminologies, and visualize the summary in an ontology structure. We propose an efficient greedy algorithm to solve the problem with (1-1/e)≈ 62%-approximation guarantee. Preliminary experimental results on real-world datasets show the effectiveness and efficiency of the proposed algorithm for data summarization.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2115–2118},
numpages = {4},
keywords = {graph visualization, ontology structure, approximation algorithm, top-k diversification, data summarization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133072,
author = {Huang, Zai and Pan, Zhen and Liu, Qi and Long, Bai and Ma, Haiping and Chen, Enhong},
title = {An Ad CTR Prediction Method Based on Feature Learning of Deep and Shallow Layers},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133072},
doi = {10.1145/3132847.3133072},
abstract = {In online advertising, Click-Through Rate (CTR) prediction is a crucial task, as it may benefit the ranking and pricing of online ads. To the best of our knowledge, most of the existing CTR prediction methods are shallow layer models (e.g., Logistic Regression and Factorization Machines) or deep layer models (e.g., Neural Networks). Unfortunately, the shallow layer models cannot capture or utilize high-order nonlinear features in ad data. On the other side, the deep layer models cannot satisfy the necessity of updating CTR models online efficiently due to their high computational complexity. To address the shortcomings above, in this paper, we propose a novel hybrid method based on feature learning of both Deep and Shallow Layers (DSL). In DSL, we utilize Deep Neural Network as a deep layer model trained offline to learn high-order nonlinear features and use Factorization Machines as a shallow layer model for CTR prediction. Furthermore, we also develop an online learning implementation based on DSL, i.e., onlineDSL. Extensive experiments on large-scale real-world datasets clearly validate the effectiveness of our DSL method and onlineDSL algorithm compared with several state-of-the-art baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2119–2122},
numpages = {4},
keywords = {online advertising, feature learning, ctr prediction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133115,
author = {Kang, Yoonsuk and Jo, Yong-Yeon and Cha, Jaehyuk and Bae, Wan D. and Kim, Sang-Wook},
title = {A Framework for Estimating Execution Times of IO Traces on SSDs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133115},
doi = {10.1145/3132847.3133115},
abstract = {With the NAND flash memory technology of solid-state drives (SSDs), the usage of SSDs is expanded to various devices. Due to the cost and time limitations of measuring the actual execution time of each application on SSDs, it is difficult for users to determine the best SSD for their most commonly used applications. In this paper, we propose a framework of estimating the execution time of an application IO trace (i.e., a query IO trace) on a target SSD without its real execution. Our framework is based on the observation that if two IO traces are similar in their IO behavior, their execution times tend to be similar when executed on the same SSD. The performance of the framework is evaluated through extensive experiments on real applications. The results show that our framework is accurate in estimating the execution time of an IO trace on SSDs.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2123–2126},
numpages = {4},
keywords = {execution time estimation, application io trace, solid-state drive (ssd)},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133059,
author = {Kawasaki, Mami and Kang, Inho and Sakai, Tetsuya},
title = {Ranking Rich Mobile Verticals Based on Clicks and Abandonment},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133059},
doi = {10.1145/3132847.3133059},
abstract = {We consider the problem of ranking rich verticals, which we call "cards," for a given mobile search query. Examples of card types include "SHOP" (showing access and contact information of a shop), "WEATHER" (showing a weather forecast for a particular location), and "TV" (showing information about a TV programme). These cards can be highly visual and/or concise, and may often satisfy the user's information need without making her click on them. While this "good abandonment" of the search engine result page is ideal especially for mobile environments where the interaction between the user and the search engine should be minimal, it poses a challenge for search engine companies whose ranking algorithms rely heavily on click data. In order to provide the right card types to the user for a given query, we propose a graph-based approach which extends a click-based automatic relevance estimation algorithm of Agrawal et al., by incorporating an abandonment-based preference rule. Using a real mobile query log from a commercial search engine, we constructed a data set containing 2,472 pairwise card type preferences covering 992 distinct queries, by hiring three independent assessors. Our proposed method outperforms a click-only baseline by 53-68% in terms of card type preference accuracy. The improvement is also statistically highly significant, with p ≈ 0.0000 according to the paired randomisation test.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2127–2130},
numpages = {4},
keywords = {vertical ranking, mobile search, click data, good abandonment},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133159,
author = {Kharlamov, Evgeny and Savkovi\'{y}, Ognjen and Xiao, Guohui and Penaloza, Rafael and Mehdi, Gulnar and Roshchin, Mikhail and Horrocks, Ian},
title = {Semantic Rules for Machine Diagnostics: Execution and Management},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133159},
doi = {10.1145/3132847.3133159},
abstract = {Rule-based diagnostics of equipment is an important task in industry. In this paper we present how semantic technologies can enhance diagnostics. In particular, we present our semantic rule language sigRL that is inspired by the real diagnostic languages used in Siemens. SigRL allows to write compact yet powerful diagnostic programs by relying on a high level data independent vocabulary, diagnostic ontologies, and queries over these ontologies. We study computational complexity of SigRL: execution of diagnostic programs, provenance computation, as well as automatic verification of redundancy and inconsistency in diagnostic programs.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2131–2134},
numpages = {4},
keywords = {diagnostic systems, sensor signals, ontologies, complexity, rules},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133120,
author = {Kim, Jaehyung and Park, Jinuk and Park, Sanghyun},
title = {Machine Learning Based Performance Modeling of Flash SSDs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133120},
doi = {10.1145/3132847.3133120},
abstract = {Flash memory based solid state drives(SSDs) have alleviated the I/O bottleneck by exploiting its data parallel design. In an enterprise environment, Flash SSD used in the form of a hybrid storage architecture to achieve the better performance with lower cost. In this architecture, I/O load balancing is one of the important factors. However, the internal parallelism distorts the performance measures of the flash SSDs. Despite the criticality of load balancing on I/O intensive environments, these studies have rarely been addressed. In this paper, we examine the effectiveness of applying classification method using machine learning techniques to the I/O saturation estimation by using Linux kernel I/O statistics instead of the utilization measure that is currently used for HDDs. We conclude that machine learning techniques that we employed (Support Vector Machine and LASSO Generalized Linear Model) performs well compared to the existing utilization measure even we cannot collect the internal information of the flash SSDs.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2135–2138},
numpages = {4},
keywords = {Load balancing, Flash SSD, Machine Learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133105,
author = {Kwon, Sunjae and Ko, Youngjoong and Seo, Jungyun},
title = {A Robust Named-Entity Recognition System Using Syllable Bigram Embedding with Eojeol Prefix Information},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133105},
doi = {10.1145/3132847.3133105},
abstract = {Korean named-entity recognition (NER) systems have been developed mainly on the morphological-level, and they are commonly based on a pipeline framework that identifies named-entities (NEs) following the morphological analysis. However, this framework can mean that the performance of NER systems is degraded, because errors from the morphological analysis propagate into NER systems. This paper proposes a novel syllable-level NER system, which does not require a morphological analysis and can achieve a similar or better performance compared with the morphological-level NER systems. In addition, because the proposed system does not require a morphological analysis step, its processing speed is about 1.9 times faster than those of the previous morphological-level NER systems.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2139–2142},
numpages = {4},
keywords = {eojeol prefix information, korean syllable-level named-entity recognition, syllable bigram embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133158,
author = {Lee, Jae-woong and Lee, Jongwuk},
title = {IDAE: Imputation-Boosted Denoising Autoencoder for Collaborative Filtering},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133158},
doi = {10.1145/3132847.3133158},
abstract = {In recent years, while deep neural networks have shown impressive performance to solve various recognition and classification problems, collaborative filtering (CF) received relatively little attention to utilize deep neural networks. Because of inherent data sparsity, it remains a challenging problem for deep neural networks. In this paper, we propose a new CF model, namely the imputation-boosted denoising autoencoder (IDAE), for top-N recommendation. Specifically, IDAE consists of two steps: imputing positive values and learning with imputed values. First, it infers and imputes positive user feedback from missing values. Then, the correlation between items is learned by using the denoising autoencoder (DAE) with imputed values. Unlike the existing DAE that randomly corrupts the input, the key characteristic of IDAE is that original user values are taken as the input, and imputed values are reflected as the corrupted output. Our experimental results demonstrate that IDAE significantly outperforms state-of-the-art CF algorithms using autoencoders (by up to 5%) on the MovieLens datasets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2143–2146},
numpages = {4},
keywords = {denoising autoencoders, collaborative filtering, data imputation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133093,
author = {Lee, Kwang Hee and Kim, Myoung Ho},
title = {Computing Betweenness Centrality in B-Hypergraphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133093},
doi = {10.1145/3132847.3133093},
abstract = {The directed hypergraph (especially B-hypergraph) has hyperedges that represent relations of a set of source nodes to a single target node. Author-cited networks and cellular signaling pathways can be modeled as a B-hypergraph. In this paper every source node of a hyperedge in the shortest path p in a B-hypergraph is considered a participant of p. We propose a betweenness centrality in the B-hypergraph that measures the number of shortest paths in which a node participates. The algorithm for computing the approximated betweenness centrality scores is also proposed. Through various performance experiments such as attack robustness and reachability tests, we show that our proposed betweenness centrality is a more appropriate measure in real-world B-hypergraph applications than ordinary betweenness centrality.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2147–2150},
numpages = {4},
keywords = {betweenness centrality, directed hypergraph, b-hypergraph},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133152,
author = {Lee, Yang-Yin and Yen, Ting-Yu and Huang, Hen-Hsen and Chen, Hsin-Hsi},
title = {Structural-Fitting Word Vectors to Linguistic Ontology for Semantic Relatedness Measurement},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133152},
doi = {10.1145/3132847.3133152},
abstract = {With the aid of recently proposed word embedding algorithms, the study of semantic relatedness has progressed and advanced rapidly. In this research, we propose a novel structural-fitting method that utilizes the linguistic ontology into vector space representations. The ontological information is applied in two ways. The fine2coarse approach refines the word vectors from fine-grained to coarse-grained terms (word types), while the coarse2fine approach refines the word vectors from coarse-grained to fine-grained terms. In the experiments, we show that our proposed methods outperform previous approaches in seven publicly available benchmark datasets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2151–2154},
numpages = {4},
keywords = {retrofitting, linguistic ontology, structural-fitting, Word embedding, semantic relatedness},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133100,
author = {Lei, Yu and Li, Wenjie and Lu, Ziyu and Zhao, Miao},
title = {Alternating Pointwise-Pairwise Learning for Personalized Item Ranking},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133100},
doi = {10.1145/3132847.3133100},
abstract = {Pointwise and pairwise collaborative ranking are two major classes of algorithms for personalized item ranking. This paper proposes a novel joint learning method named alternating pointwise-pairwise learning (APPL) to improve ranking performance. APPL combines the ideas of both pointwise and pairwise learning, and is able to produce a more effective prediction model. The extensive experiments with both explicit and implicit feedback settings on four real-world datasets demonstrate that APPL performs significantly better than the state-of-the-art methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2155–2158},
numpages = {4},
keywords = {collaborative ranking, item recommendation, personalized item ranking},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133084,
author = {Li, Tong and Gao, Sheng and Xu, Yajing},
title = {Deep Multi-Similarity Hashing for Multi-Label Image Retrieval},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133084},
doi = {10.1145/3132847.3133084},
abstract = {mage retrieval based on deep hashing methods has attracted more and more attentions from both academic and industry, due to the out-standing performance of deep neural network in various tasks of computer vision. However, most of the hashing methods are designed to learn simple similarity only for single-label image retrieval, thus cannot work well for the multi-label cases. In this paper, we proposed a framework named Deep Multi-Similarity Hashing (DMSH) method to learn semantic binary representations for multi-label image retrieval task. In the proposed model, a convolutional architecture is incorporated with hash function to learn compact binary representations from every pair of images with multiple labels. On the purposed of learning semantic structure of multi-label images, we define the pairwise loss for multi-label image pairs, which is influenced by zero-loss interval under the control of the number of common labels. The objective loss function consists of hashing quantification loss and pairwise loss for multi-label images, which pays more attention to high-level similarity than low-level similarity during the training process. Furthermore, our proposed model is flexible to be implemented with various deep networks. Experiments on large scale dataset NUS-WIDE have proved the state-of-the-art performance of our proposed DMSH model in the task of multi-label image retrieval.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2159–2162},
numpages = {4},
keywords = {common labels, content based image retrieval, deep hashing method, multilabel image retrieval},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133060,
author = {Li, Yuqi and Chen, Weizheng and Yan, Hongfei},
title = {Learning Graph-Based Embedding For Time-Aware Product Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133060},
doi = {10.1145/3132847.3133060},
abstract = {In this paper, we propose a novel Product Graph Embedding (PGE) model to investigate time-aware product recommendation by leveraging the network representation learning technique. Our model captures the sequential influences of products by transforming the historical purchase records into a product graph. Then the product can be transformed into a low dimensional vector by the network embedding model. Once products are projected into the latent space, we present a novel method to compute user's latest preferences, which projects users into the same latent space as products. This method is based on time-decay functions and the embedding of sequential products that the user purchased. Thus, relatedness between a product and a user can be measured by the similarity between the embedding vectors which represent the product and the user's preferences. The experimental results on purchase records crawled from JINGDONG, show the superiority of our proposed framework for personalized product recommendation.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2163–2166},
numpages = {4},
keywords = {network embedding, time aware, product recommendation, dynamic user embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133145,
author = {Lin, Junjie and Mao, Wenji and Zhang, Yuhao},
title = {An Enhanced Topic Modeling Approach to Multiple Stance Identification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133145},
doi = {10.1145/3132847.3133145},
abstract = {People often publish online texts to express their stances, which reflect the essential viewpoints they stand. Stance identification has been an important research topic in text analysis and facilitates many applications in business, public security and government decision making. Previous work on stance identification solely focuses on classifying the supportive or unsupportive attitude towards a certain topic/entity. The other important type of stance identification, multiple stance identification, was largely ignored in previous research. In contrast, multiple stance identification focuses on identifying different standpoints of multiple parties involved in online texts. In this paper, we address the problem of recognizing distinct standpoints implied in textual data. As people are inclined to discuss the topics favorable to their standpoints, topics thus can provide distinguishable information of different standpoints. We propose a topic-based method for standpoint identification. To acquire more distinguishable topics, we further enhance topic model by adding constraints on document-topic distributions. We finally conduct experimental studies on two real datasets to verify the effectiveness of our approach to multiple stance identification.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2167–2170},
numpages = {4},
keywords = {Multiple stance identification, topic modeling, constrained Nonnegative Matrix Factorization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133077,
author = {Liu, Hao and Ji, Yudian and Xiao, Jiang and Tan, Haoyu and Luo, Qiong and Ni, Lionel M.},
title = {TICC: Transparent Inter-Column Compression for Column-Oriented Database Systems},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133077},
doi = {10.1145/3132847.3133077},
abstract = {In this paper, we present TICC, an automatic data compression component that can transparently eliminate data redundancies across columns in column-oriented database systems. We further propose two approaches to integrate inter-column compression into existing database systems. One approach is to use User Defined Functions (UDFs), and the other is native. We implement these two approaches on top of Hive based on the ORC file, a common data format in column stores, and evaluate the performance of TICC using real-world datasets. The experimental results demonstrate that TICC can significantly reduce the storage overhead and process a variety of queries over large-scale data with up to 20% performance improvement over the original Hive.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2171–2174},
numpages = {4},
keywords = {cross-column redundancy, column store, data compression},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133071,
author = {Liu, Shen and Liu, Hongyan},
title = {Exploiting User Consuming Behavior for Effective Item Tagging},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133071},
doi = {10.1145/3132847.3133071},
abstract = {Automatic tagging techniques are important for many applications such as searching and recommendation, which has attracted many researchers' attention in recent years. Existing methods mainly rely on users' tagging behavior or items' content information for tagging, yet users' consuming behavior is ignored. In this paper, we propose to leverage such information and introduce a probabilistic model called joint-tagging LDA to improve tagging accuracy. An effective algorithm based on Zero-Order Collapsed Variational Bayes is developed. Experiments conducted on a real dataset demonstrate that joint-tagging LDA outperforms existing competing methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2175–2178},
numpages = {4},
keywords = {tag recommendation, generative model, user behavior modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133073,
author = {Luo, Siqiang and Hu, Jiafeng and Cheng, Reynold and Yan, Jing and Kao, Ben},
title = {SEQ: Example-Based Query for Spatial Objects},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133073},
doi = {10.1145/3132847.3133073},
abstract = {Spatial object search is prevalent in map services (e.g., Google Maps). To rent an apartment, for example, one will take into account its nearby facilities, such as supermarkets, hospitals, and subway stations. Traditional keyword search solutions, such as the nearby function in Google Maps, are insufficient in expressing the often complex attribute/spatial requirements of users. Those require- ments, however, are essential to reflect the user search intention. In this paper, we propose the Spatial Exemplar Query (SEQ), which allows the user to input a result example over an interface inside the map service. We then propose an effective similarity measure to evaluate the proximity between a candidate answer and the given example. We conduct a user study to validate the effectiveness of SEQ. Our result shows that more than 88% of users would like to have an example assisted search in map services. Moreover, SEQ gets a user satisfactory score of 4.3/5.0, which is more than 2 times higher than that of a baseline solution.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2179–2182},
numpages = {4},
keywords = {spatial query, exemplar query, query by example},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133069,
author = {Lyu, Shanshan and Ouyang, Wentao and Shen, Huawei and Cheng, Xueqi},
title = {Truth Discovery by Claim and Source Embedding},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133069},
doi = {10.1145/3132847.3133069},
abstract = {Information gathered from multiple sources on the Web often exhibits conflicts. This phenomenon motivates the need of truth discovery, which aims to automatically find the true claim among multiple conflicting claims. Existing truth discovery methods are mainly based on iterative updates or probabilistic models. In particular, iterative methods specify rules that govern how credibility flows from sources to claims and then back to sources. However, these manually-defined rules tend to be ad hoc and are difficult to adapt and analyze. Probabilistic methods model a few latent factors that impact how sources make claims, such as randomly choosing, guessing, or mistaking. However, these manually-defined factors may not well reflect the underlying data distributions. Given these limitations, we propose a new, unsupervised model for truth discovery in this paper. Our model first constructs a heterogenous network that exploits both source-claim and source-source relationships. It then embeds the network into a low dimensional space through a principled algorithm such that trustworthy sources and true claims (meanwhile, unreliable sources and false claims) are close. In this way, truth discovery can be conveniently performed in the embedding space. Compared with existing methods, our model does not need manually-defined rules or factors. Rather, it learns the embeddings automatically from data. Experiments on two real-world datasets demonstrate that our model outperforms existing state-of-the-art methods for truth discovery.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2183–2186},
numpages = {4},
keywords = {crowdsourcing, truth discovery, representation learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133102,
author = {Mandal, Arpan and Ghosh, Kripabandhu and Pal, Arindam and Ghosh, Saptarshi},
title = {Automatic Catchphrase Identification from Legal Court Case Documents},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133102},
doi = {10.1145/3132847.3133102},
abstract = {Automatically identifying catchphrases from legal court case documents is an important problem in Legal Information Retrieval, which has not been extensively studied. In this work, we propose an unsupervised approach for extraction and ranking of catchphrases from court case documents, by focusing on noun phrases. Using a dataset of gold standard catchphrases created by legal experts from real-life court documents, we compare the proposed approach with several unsupervised and supervised baselines. We show that the proposed methodology achieves statistically significantly better performance compared to all the baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2187–2190},
numpages = {4},
keywords = {court cases, legal ir, catchphrase extraction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133129,
author = {Mansouri, Behrooz and Zahedi, Mohammad Sadegh and Rahgozar, Maseud and Oroumchian, Farhad and Campos, Ricardo},
title = {Learning Temporal Ambiguity in Web Search Queries},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133129},
doi = {10.1145/3132847.3133129},
abstract = {Time has strong influence on web search. The temporal intent of the searcher adds an important dimension to the relevance judgments of web queries. However, lack of understanding their temporal requirements increases the ambiguity of the queries, turning retrieval effectiveness improvements into a complex task. In this paper, we propose an approach to classify web queries into four different categories considering their temporal ambiguity. For each query, we develop features from its search volumes and related queries using Google trends and its related top Wikipedia pages. Our experiment results show that these features can determine temporal ambiguity of a given query with high accuracy. We have demonstrated that a Multilayer Perceptron Networks can achieve better results in classifying temporal class of queries in comparison to other classifiers.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2191–2194},
numpages = {4},
keywords = {Query Intent, Temporal Query Classification, Temporal IR},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133053,
author = {Markov, Ilya and Borisov, Alexey and de Rijke, Maarten},
title = {Online Expectation-Maximization for Click Models},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133053},
doi = {10.1145/3132847.3133053},
abstract = {Click models allow us to interpret user click behavior in search interactions and to remove various types of bias from user clicks. Existing studies on click models consider a static scenario where user click behavior does not change over time. We show empirically that click models deteriorate over time if retraining is avoided. We then adapt online expectation-maximization (EM) techniques to efficiently incorporate new click/skip observations into a trained click model. Our instantiation of Online EM for click models is orders of magnitude more efficient than retraining the model from scratch using standard EM, while loosing little in quality. To deal with outdated click information, we propose a variant of online EM called EM with Forgetting, which surpasses the performance of complete retraining while being as efficient as Online EM.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2195–2198},
numpages = {4},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133098,
author = {Mehrotra, Rishabh and Yilmaz, Emine},
title = {Task Embeddings: Learning Query Embeddings Using Task Context},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133098},
doi = {10.1145/3132847.3133098},
abstract = {Continuous space word embedding have been shown to be highly effective in many information retrieval tasks. Embedding representation models make use of local information available in immediately surrounding words to project nearby context words closer in the embedding space. With rising multi-tasking nature of web search sessions, users often try to accomplish different tasks in a single search session. Consequently, the search context gets polluted with queries from different unrelated tasks which renders the context heterogeneous. In this work, we hypothesize that task information provides better context for IR systems to learn from. We propose a novel task context embedding architecture to learn representation of queries in low-dimensional space by leveraging their task context information from historical search logs using neural embedding models. In addition to qualitative analysis, we empirically demonstrate the benefit of leveraging task context to learn query representations.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2199–2202},
numpages = {4},
keywords = {query representations, neural embeddings, search tasks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133110,
author = {Meng, Zhao and Mou, Lili and Jin, Zhi},
title = {Hierarchical RNN with Static Sentence-Level Attention for Text-Based Speaker Change Detection},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133110},
doi = {10.1145/3132847.3133110},
abstract = {Speaker change detection (SCD) is an important task in dialog modeling. Our paper addresses the problem of text-based SCD, which differs from existing audio-based studies and is useful in various scenarios, for example, processing dialog transcripts where speaker identities are missing (e.g., OpenSubtitle), and enhancing audio SCD with textual information. We formulate text-based SCD as a matching problem of utterances before and after a certain decision point; we propose a hierarchical recurrent neural network (RNN) with static sentence-level attention. Experimental results show that neural networks consistently achieve better performance than feature-based approaches, and that our attention-based model significantly outperforms non-attention neural networks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2203–2206},
numpages = {4},
keywords = {sentence-level attention, speaker change detection, hierarchical recurrent neural network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133058,
author = {Menon, Aditya Krishna and Lee, Young},
title = {Predicting Short-Term Public Transport Demand via Inhomogeneous Poisson Processes},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133058},
doi = {10.1145/3132847.3133058},
abstract = {Forecasting short term passenger demand for public transport is a core problem in urban mobility. Typically, this is addressed using Poisson regression or homogeneous Poisson processes. However, such approaches have several limitations, including susceptibility to noise at fine time granularities, and the inability to capture complex non-stationary trends. In this paper, we show how such short term demand can be accurately modelled with an inhomogeneous Poisson process, using a neural network as the underlying intensity. This choice of intensity subsumes existing models as special cases, and is powerful enough to capture certain stylised facts of real-world demand. Experiments on real-world bus arrival data from a large metropolitan area in Australia validate our approach.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2207–2210},
numpages = {4},
keywords = {point process, urban mobility, neural network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133144,
author = {Meuschke, Norman and Schubotz, Moritz and Hamborg, Felix and Skopal, Tomas and Gipp, Bela},
title = {Analyzing Mathematical Content to Detect Academic Plagiarism},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133144},
doi = {10.1145/3132847.3133144},
abstract = {This paper presents, to our knowledge, the first study on analyzing mathematical expressions to detect academic plagiarism. We make the following contributions. First, we investigate confirmed cases of plagiarism to categorize the similarities of mathematical content commonly found in plagiarized publications. From this investigation, we derive possible feature selection and feature comparison strategies for developing math-based detection approaches and a ground truth for our experiments. Second, we create a test collection by embedding confirmed cases of plagiarism into the NTCIR-11 MathIR Task dataset, which contains approx. 60 million mathematical expressions in 105,120 documents from arXiv.org. Third, we develop a first math-based detection approach by implementing and evaluating different feature comparison approaches using an open source parallel data processing pipeline built using the Apache Flink framework. The best performing approach identifies all but two of our real-world test cases at the top rank and achieves a mean reciprocal rank of 0.86. The results show that mathematical expressions are promising text-independent features to identify academic plagiarism in large collections. To facilitate future research on math-based plagiarism detection, we make our source code and data available.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2211–2214},
numpages = {4},
keywords = {plagiarism detection, mathematical information retrieval},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133095,
author = {Moon, Changsung and Jones, Paul and Samatova, Nagiza F.},
title = {Learning Entity Type Embeddings for Knowledge Graph Completion},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133095},
doi = {10.1145/3132847.3133095},
abstract = {Missing data is a severe problem for algorithms that operate over knowledge graphs (KGs). Most previous research in KG completion has focused on the problem of inferring missing entities and missing relation types between entities. However, in addition to these, many KGs also suffer from missing entity types (i.e. the category labels for entities, such as /music/artist). Entity types are a critical enabler for many NLP tasks that use KGs as a reference source, and inferring missing entity types remains an important outstanding obstacle in the field of KG completion. Inspired by recent work to build a contextual KG embedding model, we propose a novel approach to address the entity type prediction problem. We compare the performance of our method with several state-of-the-art KG embedding methods, and show that our approach gives higher prediction accuracy compared to baseline algorithms on two real-world datasets. Our approach also produces consistently high accuracy when inferring entities and relation types, as well as the primary task of inferring entity types. This is in contrast to many of the baseline methods that specialize in one prediction task or another. We achieve this while preserving linear scalability with the number of entity types. Source code and datasets from this paper can be found at (https://github.ncsu.edu/cmoon2/kg).},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2215–2218},
numpages = {4},
keywords = {knowledge graph completion, kg embedding method, entity type prediction, vector embedding},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133126,
author = {Mumtaz, Sara and Wang, Xiaoyang},
title = {Identifying Top-K Influential Nodes in Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133126},
doi = {10.1145/3132847.3133126},
abstract = {Network Centrality is one of the core concepts in network analysis, which ranks the importance of a node in a network. A considerably extensive range of centrality measures exist that serve the purpose of quantifying the importance of a node according to its application and domain. One such measure is the Betweenness Centrality (BC) which computes the importance of a node in terms of total number of shortest paths that pass through that node. However, these computations are very expensive and pose different challenges for large scale networks. With an attempt to deal with these challenges, our paper presents an approximate algorithm for BC maximization problem, which tries to find a set of nodes with largest BC. The core of our algorithm is the estimation technique, which is based on progressive sampling with early stopping conditions. The reduction in sample size results not only in small computations overhead, but also scales well with large networks. We experimentally evaluate our technique using different datasets to confirm the performance of the developed techniques.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2219–2222},
numpages = {4},
keywords = {betweenness centrality, sampling, influential nodes},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133106,
author = {Nayeem, Mir Tafseer and Chali, Yllias},
title = {Paraphrastic Fusion for Abstractive Multi-Sentence Compression Generation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133106},
doi = {10.1145/3132847.3133106},
abstract = {This paper presents a first attempt towards finding an abstractive compression generation system for a set of related sentences which jointly models sentence fusion and paraphrasing using continuous vector representations. Our paraphrastic fusion system improves the informativity and the grammaticality of the generated sentences. Our system can be applied to various real world applications such as text simplification, microblog, opinion and newswire summarization. We conduct our experiments on human generated multi-sentence compression datasets and evaluate our system on several newly proposed Machine Translation (MT) evaluation metrics. Our experiments demonstrate that our method brings significant improvements over the state of the art systems across different metrics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2223–2226},
numpages = {4},
keywords = {abstractive compression generation, multi-sentence compression, lexical paraphrasing, sentence fusion},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133090,
author = {Nguyen, Dat Ba and Theobald, Martin and Weikum, Gerhard},
title = {J-REED: Joint Relation Extraction and Entity Disambiguation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133090},
doi = {10.1145/3132847.3133090},
abstract = {Information extraction (IE) from text sources can either be performed as Model-based IE (i.e, by using a pre-specified domain of target entities and relations) or as Open IE (i.e., with no particular assumptions about the target domain). While Model-based IE has limited coverage, Open IE merely yields triples of surface phrases which are usually not disambiguated into a canonical set of entities and relations. This paper presents J-REED: a joint approach for entity disambiguation and relation extraction that is based on probabilistic graphical models. J-REED merges ideas from both Model-based and Open IE by mapping surface names to a background knowledge base, and by making surface relations as crisp as possible.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2227–2230},
numpages = {4},
keywords = {entity disambiguation, open relation extraction, joint inference},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133128,
author = {Nguyen, Trong T. and Lauw, Hady W.},
title = {Collaborative Topic Regression with Denoising AutoEncoder for Content and Community Co-Representation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133128},
doi = {10.1145/3132847.3133128},
abstract = {Personalized recommendation of items frequently faces scenarios where we have sparse observations on users' adoption of items. In the literature, there are two promising directions. One is to connect sparse items through similarity in content. The other is to connect sparse users through similarity in social relations. We seek to integrate both types of information, in addition to the adoption information, within a single integrated model. Our proposed method models item content via a topic model, and user communities via an autoencoder model, while bridging a user's community-based preference to her topic-based preference. Experiments on public real-life data showcase the utility of the model, particularly when there is significant compatibility between communities and topics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2231–2234},
numpages = {4},
keywords = {autoencoder, collaborative deep learning, topic model, cold-start recommendation, social collaborative filtering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133156,
author = {Nicosia, Massimo and Moschitti, Alessandro},
title = {Accurate Sentence Matching with Hybrid Siamese Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133156},
doi = {10.1145/3132847.3133156},
abstract = {Recent neural network approaches to sentence matching compute the probability of two sentences being similar by minimizing a logistic loss. In this paper, we learn sentence representations by means of a siamese network, which: (i) uses encoders that share parameters; and (ii) enables the comparison between two sentences in terms of their euclidean distance, by minimizing a contrastive loss. Moreover, we add a multilayer perceptron in the architecture to simultaneously optimize the contrastive and the logistic losses. This way, our network can exploit a more informative feedback, given by the logistic loss, which is also quantified by the distance that the two sentences have according to their representation in the euclidean space. We show that jointly minimizing the two losses yields higher accuracy than minimizing them independently. We verify this finding by evaluating several baseline architectures in two sentence matching tasks: question paraphrasing and textual entailment recognition. Our network approaches the state of the art, while being much simpler and faster to train, and with less parameters than its competitors.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2235–2238},
numpages = {4},
keywords = {natural language processing, question similarity, siamese network, sentence similarity, sentence matching, joint loss, neural networks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133079,
author = {Niu, Shuzi and Zhang, Rongzhi},
title = {Collaborative Sequence Prediction for Sequential Recommender},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133079},
doi = {10.1145/3132847.3133079},
abstract = {With the surge of deep learning, more and more attention has been put on the sequential recommender. It can be casted as sequence prediction problem, where we will predict the next item given the previous items. RNN approaches are able to capture the global sequential features from the data compared with the local features derived in Markov Chain methods. However, both approaches rely on the independence of users' sequences, which are not true in practice. We propose to formulate the sequential recommendation problem as collaborative sequence prediction problem to take the dependency of users' sequences into account. In order to solve the collaborative sequence prediction problem, we define the dynamic neighborhood relationship between users and introduce manifold regularization to RNN on the basis of the multi-facets of collaborative filtering, referred to as MrRNN. Experimental results on benchmark datasets show that our approach outperforms the state-of-the-art baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2239–2242},
numpages = {4},
keywords = {sequential recommender, recurrent networks, collaborative sequence prediction, manifold regularization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133054,
author = {Osicka, Petr and Trnecka, Martin},
title = {Boolean Matrix Decomposition by Formal Concept Sampling},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133054},
doi = {10.1145/3132847.3133054},
abstract = {Finding interesting patterns is a classical problem in data mining. Boolean matrix decomposition is nowadays a standard tool that can find a set of patterns-also called factors-in Boolean data that explain the data well. We describe and experimentally evaluate a probabilistic algorithm for Boolean matrix decomposition problem. The algorithm is derived from GreCon algorithm which uses formal concepts-maximal rectangles or tiles-as factors in order to find a decomposition. We change the core of GreCon by substituting a sampling procedure for a deterministic computation of suitable formal concepts. This allows us to alleviate the greedy nature of GreCon, creates a possibility to bypass some of the its pitfalls and to preserve its features, e.g. an ability to explain the entire data.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2243–2246},
numpages = {4},
keywords = {formal concept analysis, boolean matrix decomposition, randomized algorithm},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133143,
author = {Pal, Soumajit and Urbani, Jacopo},
title = {Enhancing Knowledge Graph Completion By Embedding Correlations},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133143},
doi = {10.1145/3132847.3133143},
abstract = {Despite their large sizes, modern Knowledge Graphs (KGs) are still highly incomplete. Statistical relational learning methods can detect missing links by "embedding" the nodes and relations into latent feature tensors. Unfortunately, these methods are unable to learn good embeddings if the nodes are not well-connected. Our proposal is to learn embeddings for correlations between subgraphs and add a post-prediction phase to counter the lack of training data. This technique, applied on top of methods like TransE or HolE, can significantly increase the predictions on realistic KGs.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2247–2250},
numpages = {4},
keywords = {statistical relational learning, knowledge graphs, rdf, link prediction},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133096,
author = {Pang, Meng and Cheung, Yiu-ming and Wang, Binghui and Liu, Risheng},
title = {Robust Heterogeneous Discriminative Analysis for Single Sample Per Person Face Recognition},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133096},
doi = {10.1145/3132847.3133096},
abstract = {Single sample face recognition is one of the most challenging problems in face recognition (FR), where only one single sample per person (SSPP) is enrolled in the gallery set for training. Although patch-based methods have achieved great success in FR with SSPP, they still have significant limitations. In this work, we propose a new patch-based method, namely Robust Heterogeneous Discriminative Analysis (RHDA), to tackle FR with SSPP. Compared with the existing patch-based methods, RHDA can enhance the robustness against complex facial variations from two aspects. First, we develop a novel Fisher-like criterion, which incorporates two manifold embeddings, to learn heterogeneous discriminative representations of image patches. Specifically, for each patch, the Fisher-like criterion is able to preserve the reconstruction relationship of neighboring patches from the same person, while suppressing neighboring patches from different persons. Second, we present two distance metrics, i.e., patch-to-patch distance and patch-to-manifold distance, and develop a fusion strategy to combine the recognition outputs of above two distance metrics via joint majority voting for identification. Experimental results on the AR and FERET benchmark datasets demonstrate the efficacy of the proposed method.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2251–2254},
numpages = {4},
keywords = {single sample face recognition, representation learning, heterogeneous subspace analysis, joint majority voting.},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133154,
author = {Park, Keunchan and Lee, Jisoo and Choi, Jaeho},
title = {Deep Neural Networks for News Recommendations},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133154},
doi = {10.1145/3132847.3133154},
abstract = {A fundamental role of news websites is to recommend articles that are interesting to read. The key challenge of news recommendation is to recommend newly published articles. Unlike other domains, outdated items are considered to be irrelevant in the news recommendation task. Another challenge is that the recommendation candidates are not seen in the training phase. In this paper, we introduce deep neural network models to overcome these challenges. we propose a modified session-based Recurrent Neural Network (RNN) model tailored to news recommendation as well as a history-based RNN model that spans the whole user's past histories. Finally, we propose a Convolutional Neural Network (CNN) model to capture user preferences and to personalize recommendation results. Experimental results on real-world news dataset shows that our model outperforms competitive baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2255–2258},
numpages = {4},
keywords = {deep neural networks, recommender systems},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133150,
author = {Patwari, Ayush and Goldwasser, Dan and Bagchi, Saurabh},
title = {TATHYA: A Multi-Classifier System for Detecting Check-Worthy Statements in Political Debates},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133150},
doi = {10.1145/3132847.3133150},
abstract = {Fact-checking political discussions has become an essential clog in computational journalism. This task encompasses an important sub-task---identifying the set of statements with 'check-worthy' claims. Previous work has treated this as a simple text classification problem discounting the nuances involved in determining what makes statements check-worthy. We introduce a dataset of political debates from the 2016 US Presidential election campaign annotated using all major fact-checking media outlets and show that there is a need to model conversation context, debate dynamics and implicit world knowledge. We design a multi-classifier system TATHYA, that models latent groupings in data and improves state-of-art systems in detecting check-worthy statements by 19.5% in F1-score on a held-out test set, gaining primarily gaining in Recall.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2259–2262},
numpages = {4},
keywords = {natural language processing, computational journalism, clustering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133107,
author = {Rafailidis, Dimitrios and Crestani, Fabio},
title = {A Collaborative Ranking Model for Cross-Domain Recommendations},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133107},
doi = {10.1145/3132847.3133107},
abstract = {With the advent of social media, generating high quality cross-domain recommendations has become more and more important for users of heterogeneous domains. In this study, we propose a collaborative ranking model to generate cross-domain recommendations. Given a target domain, we design an objective function aimed at performing push of relevant items at the top of a recommendation list. Also, as users may have different behaviours in multiple domains in our collaborative ranking model we propose a weighting strategy to control the influence of user preferences from auxiliary domains when producing the recommendation lists. Our experiments on ten cross-domain recommendation tasks show that the proposed approach achieves higher recommendation accuracy than other state-of-the-art methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2263–2266},
numpages = {4},
keywords = {cross-domain recommendation, recommendation systems, collaborative ranking},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133103,
author = {Roy, Anurag and Ghorai, Trishnendu and Ghosh, Kripabandhu and Ghosh, Saptarshi},
title = {Combining Local and Global Word Embeddings for Microblog Stemming},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133103},
doi = {10.1145/3132847.3133103},
abstract = {Stemming is a vital step employed to improve retrieval performance through efficient unification of morphological variants of a word. We propose an unsupervised, context-specific stemming algorithm for microblogs, based on both local and global word embeddings, which is capable of handling the informal, noisy vocabulary of microblogs. Experiments on two standard microblog data collections (TREC 2016 and FIRE 2016) show that, the proposed stemmer enables significantly better retrieval performance than several state-of-the-art stemming algorithms, for the same queries.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2267–2270},
numpages = {4},
keywords = {stemming, word embedding, word2vec, glove, microblog},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133085,
author = {Roy, Dwaipayan},
title = {An Improved Test Collection and Baselines for Bibliographic Citation Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133085},
doi = {10.1145/3132847.3133085},
abstract = {The problem of recommending bibliographic citations to an author who is writing an article has been well-studied. However, different researchers have used different datasets to evaluate proposed techniques, and have sometimes reported contradictory findings regarding the relative effectiveness of various approaches. In addition, these datasets are problematic in one way or another (e.g., in terms of size or availability), precluding the possibility of adopting one (or some) of them as standard benchmarks. A recently created test collection that makes use of data from CiteSeerx is large, heterogenous, and publicly available, but has certain other limitations. In this paper, we propose a way to modify this test collection to address these limitations. We also use the improved test collection to establish a set of baseline results using elementary content-based techniques, as well as reference directed indexing.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2271–2274},
numpages = {4},
keywords = {bibliographic citations, recommender systems, test collections},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133157,
author = {Salah, Aghiles and Ailem, Melissa and Nadif, Mohamed},
title = {A Way to Boost Semi-NMF for Document Clustering},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133157},
doi = {10.1145/3132847.3133157},
abstract = {Semi-Non Negative Matrix Factorization (Semi-NMF) is one of the most popular extensions of NMF, it extends the applicable range of NMF models, to data having mixed signs, as well as strengthens their relation to clustering. However, Semi-NMF has been found to perform somewhat less than NMF, in terms of clustering, when applied to positive data such as text, which we focus on. Inspired by the recent success of neural word embedding models, e.g., word2vec, in learning high quality real valued vector representations of words, we propose to integrate a word embedding model into Semi-NMF. This allows Semi-NMF to capture more semantic relationships among words and, thereby, to infer document factors that are even better for clustering. The combination of Semi-NMF and word embedding noticeably improves the performance of NMF models, in terms of both clustering and embedding, as illustrated in our experiments.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2275–2278},
numpages = {4},
keywords = {document clustering, word embedding, semi-nonnegative matrix factorization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133137,
author = {Sanjo, Satoshi and Katsurai, Marie},
title = {Recipe Popularity Prediction with Deep Visual-Semantic Fusion},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133137},
doi = {10.1145/3132847.3133137},
abstract = {Predicting the popularity of user-created recipes has great potential to be adopted in several applications on recipe-sharing websites. To ensure timely prediction when a recipe is uploaded, a prediction model needs to be trained based on the recipe's content features (i.e., its visual and semantic features). This paper presents a novel approach to predicting recipe popularity using deep visual-semantic fusion. We first pre-train a deep model that predicts the popularity of recipes based on each single modality. We insert additional layers to the two models and concatenate their activations. Finally, we train a network comprising fully connected (FC) layers on the fused features to learn more powerful features, which are used for training a regressor. Based on experiments conducted on more than 150K recipes collected from the Cookpad website, we present a comprehensive comparison with several baselines to verify the effectiveness of our method. The best practice for the proposed method is also described.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2279–2282},
numpages = {4},
keywords = {multi-modal fusion, recipe popularity prediction, recipe features},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133148,
author = {Saravanou, Antonia and Katakis, Ioannis and Valkanas, George and Kalogeraki, Vana and Gunopulos, Dimitrios},
title = {Revealing the Hidden Links in Content Networks: An Application to Event Discovery},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133148},
doi = {10.1145/3132847.3133148},
abstract = {Social networks have become the de facto online resource for people to share, comment on and be informed about events pertinent to their interests and livelihood, ranging from road traffic or an illness to concerts and earthquakes, to economics and politics. This has been the driving force behind research endeavors that analyse such data. In this paper, we focus on how Content Networks can help us identify events effectively. Content Networks incorporate both structural and content-related information of a social network in a unified way, at the same time, bringing together two disparate lines of research: graph-based and content-based event discovery in social media. We model interactions of two types of nodes, users and content, and introduce an algorithm that builds heterogeneous, dynamic graphs, in addition to revealing content links in the network's structure. By linking similar content nodes and tracking connected components over time, we can effectively identify different types of events. Our evaluation on social media streaming data suggests that our approach outperforms state-of-the-art techniques, while showcasing the significance of hidden links to the quality of the results.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2283–2286},
numpages = {4},
keywords = {graph mining, event discovery, anomaly detection, social network analysis},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133065,
author = {Sathanur, Arun V. and Choudhury, Sutanay and Joslyn, Cliff and Purohit, Sumit},
title = {When Labels Fall Short: Property Graph Simulation via Blending of Network Structure and Vertex Attributes},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133065},
doi = {10.1145/3132847.3133065},
abstract = {Property graphs can be used to represent heterogeneous networks with labeled (attributed) vertices and edges. Given a property graph, simulating another graph with same or greater size with the same statistical properties with respect to the labels and connectivity is critical for privacy preservation and benchmarking purposes. In this work we tackle the problem of capturing the statistical dependence of the edge connectivity on the vertex labels and using the same distribution to regenerate property graphs of the same or expanded size in a scalable manner. However, accurate simulation becomes a challenge when the attributes do not completely explain the network structure. We propose the Property Graph Model (PGM) approach that uses a label augmentation strategy to mitigate the problem and preserve the vertex label and the edge connectivity distributions as well as their correlation, while also replicating the degree distribution. Our proposed algorithm is scalable with a linear complexity in the number of edges in the target graph. We illustrate the efficacy of the PGM approach in regenerating and expanding the datasets by leveraging two distinct illustrations. Our open-source implementation is available on GitHub.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2287–2290},
numpages = {4},
keywords = {joint distribution, graph generation, label-topology correlation, label augmentation, attributed graphs, property graphs},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133080,
author = {Scells, Harrisen and Zuccon, Guido and Koopman, Bevan and Deacon, Anthony and Azzopardi, Leif and Geva, Shlomo},
title = {Integrating the Framing of Clinical Questions via PICO into the Retrieval of Medical Literature for Systematic Reviews},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133080},
doi = {10.1145/3132847.3133080},
abstract = {The PICO process is a technique used in evidence based practice to frame and answer clinical questions. It involves structuring the question around four types of clinical information: population, intervention, control or comparison and outcome. The PICO framework is used extensively in the compilation of systematic reviews as the means of framing research questions. However, when a search strategy (comprising of a large Boolean query) is formulated to retrieve studies for inclusion in the review, PICO is often ignored. This paper evaluates how PICO annotations can be applied and integrated into retrieval to improve the screening of studies for inclusion in systematic reviews. The task is to increase precision while maintaining the high level of recall essential to ensure systematic reviews are representative and unbiased. Our results show that restricting the search strategies to match studies using PICO annotations improves precision, however recall is slightly reduced, when compared to the non-PICO baseline. This can lead to both time and cost savings when compiling systematic reviews.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2291–2294},
numpages = {4},
keywords = {systematic reviews, pico framework, information retrieval},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133121,
author = {Seo, Jung Hyuk and Kim, Myoung Ho},
title = {Pm-SCAN: An I/O Efficient Structural Clustering Algorithm for Large-Scale Graphs},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133121},
doi = {10.1145/3132847.3133121},
abstract = {Most existing algorithms for graph clustering, including SCAN, are not designed to cope with large volumes of data that cannot fit in main memory. When there is not enough memory, those algorithms will incur thrashing, i.e. result in huge I/O costs. We propose an I/O-efficient algorithm for structural clustering, pm-SCAN. The main idea of our scheme is to partition a large graph into several subgraphs that can fit into main memory. We first find clusters in each subgraph, and then merge them to produce final clustering of the input graph. Experimental results show that while other existing algorithms are not scalable to the graph size, our proposed method produces scalable performance for limited memory space.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2295–2298},
numpages = {4},
keywords = {structural graph clustering, i/o-efficient algorithm, graph},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133119,
author = {Shi, Jun and Gao, Huan and Qi, Guilin and Zhou, Zhangquan},
title = {Knowledge Graph Embedding with Triple Context},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133119},
doi = {10.1145/3132847.3133119},
abstract = {Knowledge graph embedding, which aims to represent entities and relations in vector spaces, has shown outstanding performance on a few knowledge graph completion tasks. Most existing methods are based on the assumption that a knowledge graph is a set of separate triples, ignoring rich graph features, i.e., structural information in the graph. In this paper, we take advantages of structures in knowledge graphs, especially local structures around a triple, which we refer to as triple context. We then propose a Triple-Context-based knowledge Embedding model (TCE). For each triple, two kinds of structure information are considered as its context in the graph; one is the outgoing relations and neighboring entities of an entity and the other is relation paths between a pair of entities, both of which reflect various aspects of the triple. Triples along with their contexts are represented in a unified framework, in which way structural information in triple contexts can be embodied. The experimental results show that our model outperforms the state-of-the-art methods for link prediction.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2299–2302},
numpages = {4},
keywords = {representation learning, knowledge graph, triple context},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133127,
author = {Singh, Abhishek Kumar and Gupta, Manish and Varma, Vasudeva},
title = {Hybrid MemNet for Extractive Summarization},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133127},
doi = {10.1145/3132847.3133127},
abstract = {Extractive text summarization has been an extensive research problem in the field of natural language understanding. While the conventional approaches rely mostly on manually compiled features to generate the summary, few attempts have been made in developing data-driven systems for extractive summarization. To this end, we present a fully data-driven end-to-end deep network which we call as Hybrid MemNet for single document summarization task. The network learns the continuous unified representation of a document before generating its summary. It jointly captures local and global sentential information along with the notion of summary worthy sentences. Experimental results on two different corpora confirm that our model shows significant performance gains compared with the state-of-the-art baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2303–2306},
numpages = {4},
keywords = {summarization, natural language, deep learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133149,
author = {Soldaini, Luca and Yates, Andrew and Goharian, Nazli},
title = {Denoising Clinical Notes for Medical Literature Retrieval with Convolutional Neural Model},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133149},
doi = {10.1145/3132847.3133149},
abstract = {The rapid increase of medical literature poses a significant challenge for physicians, who have repeatedly reported to struggle to keep up to date with developments in research. This gap is one of the main challenges in integrating recent advances in clinical research with day-to-day practice. Thus, the need for clinical decision support (CDS) search systems that can retrieve highly relevant medical literature given a clinical note describing a patient has emerged. However, clinical notes are inherently noisy, thus not being fit to be used as queries as-is. In this work, we present a convolutional neural model aimed at improving clinical notes representation, making them suitable for document retrieval. The system is designed to predict, for each clinical note term, its importance in relevant documents. The approach was evaluated on the 2016 TREC CDS dataset, where it achieved a 37% improvement in infNDCG over state-of-the-art query reduction methods and a 27% improvement over the best known method for the task.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2307–2310},
numpages = {4},
keywords = {query reduction, medical informatics, clinical decision support systems, convolutional neural networks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133082,
author = {Song, Xingshen and Yang, Yuexiang and Li, Xiaoyong},
title = {SIMD-Based Multiple Sets Intersection with Dual-Scale Search Algorithm},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133082},
doi = {10.1145/3132847.3133082},
abstract = {Conjunctive Boolean query is one fundamental operation for document retrieval in many information systems and databases. Various algorithms have been put up in terms of maximizing the query efficiency. In recent years, researchers began to exploit the parallel advantage of single-instruction-multiple-data (SIMD) instructions to accelerate the intersection procedure and achieved substantial gains over previous scalar algorithms. However, these works only focus on intersecting two sets at a time and ignore the scenario of multiple sets intersection. We present a flexible search algorithm which balances non-SIMD and SIMD comparisons in order to provide efficient and effective intersection.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2311–2314},
numpages = {4},
keywords = {algorithm optimization, vectorized processing, set intersection, performance evaluation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133162,
author = {Srivastava, Avikalp and Datt, Madhav},
title = {Soft Seeded SSL Graphs for Unsupervised Semantic Similarity-Based Retrieval},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133162},
doi = {10.1145/3132847.3133162},
abstract = {Semantic similarity based retrieval is playing an increasingly important role in many IR systems such as modern web search, question-answering, similar document retrieval etc. Improvements in retrieval of semantically similar content are very significant to applications like Quora, Stack Overflow, Siri etc. We propose a novel unsupervised model for semantic similarity based content retrieval, where we construct semantic flow graphs for each query, and introduce the concept of "soft seeding" in graph based semi-supervised learning (SSL) to convert this into an unsupervised model.We demonstrate the effectiveness of our model on an equivalent question retrieval problem on the Stack Exchange QA dataset, where our unsupervised approach significantly outperforms the state-of-the-art unsupervised models, and produces comparable results to the best supervised models. Our research provides a method to tackle semantic similarity based retrieval without any training data, and allows seamless extension to different domain QA communities, as well as to other semantic equivalence tasks.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2315–2318},
numpages = {4},
keywords = {topic model application, soft seeded semi-supervised learning graphs, similar question retrieval, semantic similarity, document representation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133068,
author = {Stanojevic, Rade},
title = {How Safe is Your (Taxi) Driver?},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133068},
doi = {10.1145/3132847.3133068},
abstract = {For an auto insurer, understanding the risk of individual drivers is a critical factor in building a healthy and profitable portfolio. For decades, assessing the risk of drivers has relied on demographic information which allows the insurer to segment the market in several risk groups priced with an appropriate premium. In the recent years, however, some insurers started experimenting with so called Usage-Based Insurance (UBI) in which the insurer monitors a number of additional variables (mostly related to the location) and uses them to better assess the risk of the drivers. While several studies have reported results on the UBI trials these studies keep the studied data confidential (for obvious privacy and business concerns) which inevitably limits their reproducibility and interest by the data-mining community. In this paper we discuss a methodology for studying driver risk assessment using a public dataset of 173M taxi rides in NYC with over 40K drivers. Our approach for risk assessment utilizes not only the location data (which is significantly sparser than what is normally exploited in UBI) but also the revenue, tips and overall activity of the drivers (as proxies of their behavioral traits) and obtain risk scoring accuracy on par with the reported results on non-professional driver cohorts in spite of sparser location data and no demographic information about the drivers.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2319–2322},
numpages = {4},
keywords = {car insurance, data analytics, user modeling},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133153,
author = {Tan, Jiaxing and Kotov, Alexander and Pir Mohammadiani, Rojiar and Huo, Yumei},
title = {Sentence Retrieval with Sentiment-Specific Topical Anchoring for Review Summarization},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133153},
doi = {10.1145/3132847.3133153},
abstract = {We propose Topic Anchoring-based Review Summarization (TARS), a two-step extractive summarization method, which creates review summaries from the sentences that represent the most important aspects of a review. In the first step, the proposed method utilizes Topic Aspect Sentiment Model (TASM), a novel sentiment-topic model, to identify aspects of sentiment-specific topics in a collection of reviews. The output of TASM is utilized in the second step of TARS to rank review sentences based on how representative of the most important review aspects their words are. Qualitative and quantitative evaluation of review summaries using two collections indicate the effectiveness of structuring review summaries around aspects of sentiment-specific topics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2323–2326},
numpages = {4},
keywords = {topic models, opinion mining, text summarization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133108,
author = {Tian, Shixin and Cai, Ying},
title = {Visualizing Deep Neural Networks with Interaction of Super-Pixels},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133108},
doi = {10.1145/3132847.3133108},
abstract = {An effective way to visualize the prediction of deep neural networks on an image is to decompose the prediction into the contribution of units (pixels or patches). In the existing works, these units are largely considered independently, thus limiting the performance of visualization. In this paper, we propose a new predication visualization method that uses super-pixel as a contribution unit. Moreover, our method takes into consideration of the interaction of adjacent super-pixels. We implement our technique and evaluate its performance with various images. Our results show its excellent performance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2327–2330},
numpages = {4},
keywords = {image classification, visualization, deep neural networks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133046,
author = {Ueda, Saki and Yamaguchi, Yuto and Kitagawa, Hiroyuki},
title = {Collecting Non-Geotagged Local Tweets via Bandit Algorithms},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133046},
doi = {10.1145/3132847.3133046},
abstract = {How can we collect non-geotagged tweets posted by users in a specific location as many as possible in a limited time span? How can we find such users if we do not have much information about the specified location? Although there are varieties of methods to estimate the locations of users, these methods are not directly applicable to this problem because they require collecting a large amount of random tweets and then filter them to obtain a small amount of tweets from such users. In this paper, we propose a framework that incrementally finds such users and continuously collects tweets from them. Our framework is based on the bandit algorithm that adjusts the trade-off between exploration and exploitation, in other words, it simultaneously finds new users in the specified location and collects tweets from already-found users. The experimental results show that the bandit algorithm works well on this problem and outperforms the carefully-designed baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2331–2334},
numpages = {4},
keywords = {twitter, location estimation, focused crawling, bandit algorithm},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133116,
author = {Veyseh, Amir Pouran Ben and Ebrahimi, Javid and Dou, Dejing and Lowd, Daniel},
title = {A Temporal Attentional Model for Rumor Stance Classification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133116},
doi = {10.1145/3132847.3133116},
abstract = {Rumor stance classification is the task of determining the stance towards a rumor in text. This is the first step in effective rumor tracking on social media which is an increasingly important task. In this work, we analyze Twitter users' stance toward a rumorous tweet, in which users could support, deny, query, or comment upon the rumor. We propose a deep attentional CNN-LSTM approach, which takes the sequence of tweets in a thread of conversation as the input. We use neighboring tweets in the timeline as context vectors to capture the temporal dynamism in users' stance evolution. In addition, we use extra features such as friendship, to leverage useful relational features that are readily available in social media. Our model achieves the state-of-the-art results on rumor stance classification on a recent SemEval dataset, improving accuracy and F1 score by 3.6% and 4.2% respectively.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2335–2338},
numpages = {4},
keywords = {temporal attention, twitter, lstm, rumor stance classification},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133122,
author = {Wang, Cheng and Fang, Yujuan and Tan, Zheng and He, Yuan},
title = {Improving the Gain of Visual Perceptual Behaviour on Topic Modeling for Text Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133122},
doi = {10.1145/3132847.3133122},
abstract = {Internet information services have been greatly improved profiting from the growing performance of interest mining technology. Visual perceptual behaviours, a new hotspot of mining user's interests, have resulted in great gains in some typical Internet information services, e.g., information retrieval and recommendation. It is validated that combining the subjective visual perceptual behaviours with the objective contents can significantly improve these services' performance. However, the existing methods usually treat the contents and visual perceptual behaviours as two independent parts in the calculating process. The gain of visual perceptual behaviours has not been fully exploited. In this paper, we mainly aim at improving the gain of visual perceptual behaviour for text recommendation, by integrating the objective contents with subjective visual perceptual behaviours. We investigate the correlation between user's reading interests and records of real-time interaction on texts, and then design a real-time visual perceptual behaviour based method for text recommendation, which is able to: (1) build a joint interest model, called ViP-LDA (Visual Perceptual LDA), by integrating the user's visual perceptual behaviours into topic model; (2) make more accurate text recommendation based on ViP-LDA with feedback adjustment. Several experiments on a real data set are implemented to demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2339–2342},
numpages = {4},
keywords = {text recommendation, interest model, visual perceptual behaviour, eye tracking, vip-lda, lda},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133075,
author = {Wang, Yan and Qin, Zongxu and Pang, Jun and Zhang, Yang and Xin, Jin},
title = {Semantic Annotation for Places in LBSN through Graph Embedding},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133075},
doi = {10.1145/3132847.3133075},
abstract = {With the prevalence of location-based social networks (LBSNs), automated semantic annotation for places plays a critical role in many LBSN-related applications. Although a line of research continues to enhance labeling accuracy, there is still a lot of room for improvement. The crucial problem is to find a high-quality representation for each place. In previous works, the representation is usually derived directly from observed patterns of places or indirectly from calculated proximity amongst places or their combination. In this paper, we also exploit the combination to represent places but present a novel semi-supervised learning framework based on graph embedding, called Predictive Place Embedding (PPE). For place proximity, PPE first learns user embeddings from a user-tag bipartite graph by minimizing supervised loss in order to preserve the similarity of users visiting analogous places. User similarity is then transformed into place proximity by optimizing each place embedding as the centroid of the vectors of its check-in users. Our underlying idea is that a place can be considered as a representative of all its visitors. For observed patterns, a place-temporal bipartite graph is used to further adjust place embeddings by reducing unsupervised loss. Extensive experiments on real large LBSNs show that PPE outperforms state-of-the-art methods significantly.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2343–2346},
numpages = {4},
keywords = {graph representation, semantic tag, deep learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133109,
author = {Wang, Yiren and Seyler, Dominic and Santu, Shubhra Kanti Karmaker and Zhai, ChengXiang},
title = {A Study of Feature Construction for Text-Based Forecasting of Time Series Variables},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133109},
doi = {10.1145/3132847.3133109},
abstract = {Time series are ubiquitous in the world since they are used to measure various phenomena (e.g., temperature, spread of a virus, sales, etc.). Forecasting of time series is highly beneficial (and necessary) for optimizing decisions, yet is a very challenging problem; using only the historical values of the time series is often insufficient. In this paper, we study how to construct effective additional features based on related text data for time series forecasting. Besides the commonly used n-gram features, we propose a general strategy for constructing multiple topical features based on the topics discovered by a topic model. We evaluate feature effectiveness using a data set for predicting stock price changes where we constructed additional features from news text articles for stock market prediction. We found that: 1) Text-based features outperform time series-based features, suggesting the great promise of leveraging text data for improving time series forecasting. 2) Topic-based features are not very effective stand-alone, but they can further improve performance when added on top of n-gram features. 3) The best topic-based feature appears to be a long-term aggregation of topics over time with high weights on recent topics.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2347–2350},
numpages = {4},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133161,
author = {Wang, Yiwei and Carman, Mark James and Li, Yuan-Fang},
title = {Using Knowledge Graphs to Explain Entity Co-Occurrence in Twitter},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133161},
doi = {10.1145/3132847.3133161},
abstract = {Modern Knowledge Graphs such as DBPedia contain significant information regarding Named Entities and the logical relationships which exist between them. Twitter on the other hand, contains important information on the popularity and frequency with which these entities are mentioned and discussed in combination with one another. In this paper we investigate whether these two sources of information can be used to complement and explain one another. In particular, we would like to know whether the logical relationships (a.k.a. semantic paths) which exist between pairs of known entities can help to explain the frequency with which those entities co-occur with one another in Twitter. To do this we train a ranking function over semantic paths between pairs of entities. The aim of the ranker is to identify the path that most likely explains why a particular pair of entities have appeared together in a particular tweet. We train the ranking model using a number of lexical, graph-embedding and popularity-based features over semantic paths containing a single intermediate entity and demonstrate the efficacy of the model for determining why pairs of entities occur together in tweets.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2351–2354},
numpages = {4},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133136,
author = {Wang, Yutong and Xu, Yixin and Yang, Min and Zhao, Zhou and Xiao, Jun and Zhuang, Yueting},
title = {Integrating Side Information for Boosting Machine Comprehension},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133136},
doi = {10.1145/3132847.3133136},
abstract = {Machine Reading and Comprehension recently has drawn a fair amount of attention in the field of natural language processing. In this paper, we consider integrating side information to improve machine comprehension on answering cloze-style questions more precisely. To leverage the external information, we present a novel attention-based architecture which could feed the side information representations into word level embeddings to explore the comprehension performance. Our experiments show consistent improvements of our model over various baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2355–2358},
numpages = {4},
keywords = {Question answering, Machine comprehension, Text understanding, Machine reading},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133055,
author = {Wei, Xiaokai and Cao, Bokai and Yu, Philip S.},
title = {Unsupervised Feature Selection with Heterogeneous Side Information},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133055},
doi = {10.1145/3132847.3133055},
abstract = {Compared to supervised feature selection, unsupervised feature selection tends to be more challenging due to the lack of guidance from class labels. Along with the increasing variety of data sources, many datasets are also equipped with certain side information of heterogeneous structure. Such side information can be critical for feature selection when class labels are unavailable. In this paper, we propose a new feature selection method, SideFS, to exploit such rich side information. We model the complex side information as a heterogeneous network and derive instance correlations to guide subsequent feature selection. Representations are learned from the side information network and the feature selection is performed in a unified framework. Experimental results show that the proposed method can effectively enhance the quality of selected features by incorporating heterogeneous side information.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2359–2362},
numpages = {4},
keywords = {unsupervised learning, side information, heterogeneous information network, feature selection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133133,
author = {Whang, Joyce Jiyoung},
title = {An Empirical Study of Community Overlap: Ground-Truth, Algorithmic Solutions, and Implications},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133133},
doi = {10.1145/3132847.3133133},
abstract = {In real-world social networks, communities tend to be overlapped with each other because a vertex can belong to multiple communities. To identify these overlapping communities, a number of overlapping community detection methods have been proposed over the recent years. However, there have been very few studies on the characteristics and the implications of the community overlap. In this paper, we investigate the properties of the nodes and the edges placed within the overlapped regions between the communities using the ground-truth communities as well as algorithmic communities derived from the state-of-the-art overlapping community detection methods. We find that the overlapped nodes and the overlapped edges play different roles from the ones that are not in the overlapped regions. Using real-world data, we empirically show that the highly overlapped nodes are involved in structure holes of a network. Also, we show that the overlapped nodes and edges play an important role in forming new links in evolving networks and diffusing information through a network.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2363–2366},
numpages = {4},
keywords = {community detection, overlap, social network analysis},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133078,
author = {Whang, Joyce Jiyoung and Dhillon, Inderjit S.},
title = {Non-Exhaustive, Overlapping Co-Clustering},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133078},
doi = {10.1145/3132847.3133078},
abstract = {The goal of co-clustering is to simultaneously identify a clustering of the rows as well as the columns of a two dimensional data matrix. Most existing co-clustering algorithms are designed to find pairwise disjoint and exhaustive co-clusters. However, many real-world datasets might contain not only a large overlap between co-clusters but also outliers which should not belong to any co-cluster. We formulate the problem of Non-Exhaustive, Overlapping Co-Clustering where both of the row and column clusters are allowed to overlap with each other and the outliers for each dimension of the data matrix are not assigned to any cluster. To solve this problem, we propose an intuitive objective function, and develop an efficient iterative algorithm which we call the NEO-CC algorithm. We theoretically show that the NEO-CC algorithm monotonically decreases the proposed objective function. Experimental results show that the NEO-CC algorithm is able to effectively capture the underlying co-clustering structure of real-world data, and thus outperforms state-of-the-art clustering and co-clustering methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2367–2370},
numpages = {4},
keywords = {clustering, overlap, co-clustering, outlier, k-means},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133160,
author = {White, Jerome and Oard, Douglas W.},
title = {Simulating Zero-Resource Spoken Term Discovery},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133160},
doi = {10.1145/3132847.3133160},
abstract = {If search engines are ever to index all of the spoken content in the world, they will need to handle hundreds of languages for which no automatic speech recognition systems exist. Zero-resource spoken term discovery, in which repeated content is detected in some acoustic representation, offers a potentially useful source of indexing features. This paper describes a text-based simulation of a zero-resource spoken term discovery system that allows any information retrieval test collection to be used as a basis for early development of information retrieval techniques. It is proposed that these techniques can be later applied to actual zero-resource spoken term discovery results.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2371–2374},
numpages = {4},
keywords = {zero resource term discovery, simulation, n-gram retrieval},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133135,
author = {Wilkie, Colin and Azzopardi, Leif},
title = {Algorithmic Bias: Do Good Systems Make Relevant Documents More Retrievable?},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133135},
doi = {10.1145/3132847.3133135},
abstract = {Algorithmic bias presents a difficult challenge within Information Retrieval. Long has it been known that certain algorithms favour particular documents due to attributes of these documents that are not directly related to relevance. The evaluation of bias has recently been made possible through the use of retrievability, a quantifiable measure of bias. While evaluating bias is relatively novel, the evaluation of performance has been common since the dawn of the Cranfield approach and TREC. To evaluate performance, a pool of documents to be judged by human assessors is created from the collection. This pooling approach has faced accusations of bias due to the fact that the state of the art algorithms were used to create it, thus the inclusion of biases associated with these algorithms may be included in the pool. The introduction of retrievability has provided a mechanism to evaluate the bias of these pools. This work evaluates the varying degrees of bias present in the groups of relevant and non-relevant documents for topics. The differentiating power of a system is also evaluated by examining the documents from the pool that are retrieved for each topic. The analysis finds that the systems that perform better, tend to have a higher chance of retrieving a relevant document rather than a non-relevant document for a topic prior to retrieval, indicating that retrieval systems which perform better at TREC are already predisposed to agree with the judgements regardless of the query posed.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2375–2378},
numpages = {4},
keywords = {performance, retrievability, bias},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133163,
author = {Wu, Chen and Yan, Ming},
title = {Session-Aware Information Embedding for E-Commerce Product Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133163},
doi = {10.1145/3132847.3133163},
abstract = {Most of the existing recommender systems assume that user's visiting history can be constantly recorded. However, in recent online services, the user identification may be usually unknown and only limited online user behaviors can be used. It is of great importance to model the temporal online user behaviors and conduct recommendation for the anonymous users. In this paper, we propose a list-wise deep neural network based architecture to model the limited user behaviors within each session. To train the model efficiently, we first design a session embedding method to pre-train a session representation, which incorporates different kinds of user search behaviors such as clicks and views. Based on the learnt session representation, we further propose a list-wise ranking model to generate the recommendation result for each anonymous user session. We conduct quantitative experiments on a recently published dataset from an e-commerce company. The evaluation results validate the effectiveness of the proposed method, which can outperform the state-of-the-art.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2379–2382},
numpages = {4},
keywords = {product embedding, e-commerce recommendation, session-aware},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133134,
author = {Wu, Siyuan and U, Leong Hou and Bhowmick, Sourav S. and Gatterbauer, Wolfgang},
title = {Conflict of Interest Declaration and Detection System in Heterogeneous Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133134},
doi = {10.1145/3132847.3133134},
abstract = {Peer review is the most critical process in evaluating an article to be accepted for publication in an academic venue. When assigning a reviewer to evaluate an article, the assignment should be aware of conflicts of interest (COIs) such that the reviews are fair to everyone. However, existing conference management systems simply ask reviewers and authors to declare their explicit COIs through a plain search user interface guided by some simple conflict rules. We argue that such declaration system is not enough to discover all latent COI cases. In this work, we study a graphical declaration system that visualizes the relationships of authors and reviewers based on a heterogeneous co-authorship network. With the help of the declarations, we attempt to detect the latent COIs automatically based on the meta-paths of a heterogeneous network.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2383–2386},
numpages = {4},
keywords = {conflict of interest, heterogeneous network, peer review process},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133092,
author = {Xiang, Changsheng and Jin, Xiaoming},
title = {Common-Specific Multimodal Learning for Deep Belief Network},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133092},
doi = {10.1145/3132847.3133092},
abstract = {Multimodal Deep Belief Network has been widely used to extract representations for multimodal data by fusing the high-level features of each data modality into common representations. Such straightforward fusion strategy can benefit the classification and information retrieval tasks. However, it may introduce noise in case the high-level features are not naturally common hence non-fusable for different modalities. Intuitively, each modality may have its own specific features and corresponding representation capabilities thus should not be simply fused. Therefore, it is more reasonable to fuse only the common features and represent the multimodal data by both the fused features and the modality-specific features. To distinguish common features from modal-specific features is a challenging task for traditional DBN models where all features are crudely mixed. This paper proposes the Common-Specific Multimodal Deep Belief Network (CSDBN) to solve the problem. CS-DBN automatically separates common features from modal-specific features and fuses only the common ones for data representation. Experimental results demonstrate the superiority of CS-DBN for classification tasks compared with the baseline approaches.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2387–2390},
numpages = {4},
keywords = {deep belief network, multimodal data, representation learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133048,
author = {Xiong, Chenyan and Liu, Zhengzhong and Callan, Jamie and Hovy, Eduard},
title = {JointSem: Combining Query Entity Linking and Entity Based Document Ranking},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133048},
doi = {10.1145/3132847.3133048},
abstract = {Entity-based ranking systems often employ entity linking systems to align entities to query and documents. Previously, entity linking systems were not designed specifically for search engines and were mostly used as a preprocessing step. This work presents JointSem, a joint semantic ranking system that combines query entity linking and entity-based document ranking. In JointSem, the spotting and linking signals are used to describe the importance of candidate entities in the query, and the linked entities are utilized to provide additional ranking features for the documents. The linking signals and the ranking signals are combined by a joint learning-to-rank model, and the whole system is fully optimized towards end-to-end ranking performance. Experiments on TREC Web Track datasets demonstrate the effectiveness of joint learning of entity linking and entity-based ranking.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2391–2394},
numpages = {4},
keywords = {entity-based search, entity linking, document ranking},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133049,
author = {Xu, Bo and Lin, Hongfei and Lin, Yuan and Xu, Kan},
title = {Learning to Rank with Query-Level Semi-Supervised Autoencoders},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133049},
doi = {10.1145/3132847.3133049},
abstract = {Learning to rank utilizes machine learning methods to solve ranking problems by constructing ranking models in a supervised way, which needs fixed-length feature vectors of documents as inputs, and outputs the ranking models learned by iteratively reducing the pre-defined ranking loss. The document features are always extracted based on classic textual statistics, and different features contribute differently to ranking performance. Given that well-defined features would contribute more to the retrieval performance, we investigate the usage of autoencoders to enrich the feature representations of documents. Autoencoders, as basic building blocks of deep neural networks, have been successfully used in many text mining tasks for generating effective features. To enrich the feature space for learning to rank, we introduce supervision into the loss functions of autoencoders. Specifically, we first train a linear ranking model on the training data, and then incorporate the learned weights into the reconstruction costs of an autoencoder. Meanwhile, we accumulate the costs of documents for a given query with query-level constraints for producing more useful features. We evaluate the effectiveness of our model on three LETOR datasets, and show that our model can generate effective document features to improve the retrieval performance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2395–2398},
numpages = {4},
keywords = {learning to rank, semi-supervised learning, autoencoders},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133142,
author = {Xu, Nan and Mao, Wenji},
title = {MultiSentiNet: A Deep Semantic Network for Multimodal Sentiment Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133142},
doi = {10.1145/3132847.3133142},
abstract = {With the prevalence of more diverse and multiform user-generated content in social networking sites, multimodal sentiment analysis has become an increasingly important research topic in recent years. Previous work on multimodal sentiment analysis directly extracts feature representation of each modality and fuse these features for classification. Consequently, some detailed semantic information for sentiment analysis and the correlation between image and text have been ignored. In this paper, we propose a deep semantic network, namely MultiSentiNet, for multimodal sentiment analysis. We first identify object and scene as salient detectors to extract deep semantic features of images. We then propose a visual feature guided attention LSTM model to extract words that are important to understand the sentiment of whole tweet and aggregate the representation of those informative words with visual semantic features, object and scene. The experiments on two public available sentiment datasets verify the effectiveness of our MultiSentiNet model and show that our extracted semantic features demonstrate high correlations with human sentiments.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2399–2402},
numpages = {4},
keywords = {attentional mechanism, visual semantic features, multimodal sentiment analysis, deep neural network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133081,
author = {Xu, Qiongkai and Wang, Qing and Xu, Chenchen and Qu, Lizhen},
title = {Attentive Graph-Based Recursive Neural Network for Collective Vertex Classification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133081},
doi = {10.1145/3132847.3133081},
abstract = {Vertex classification is a critical task in graph analysis, where both contents and linkage of vertices are incorporated during classification. Recently, researchers proposed using deep neural network to build an end-to-end framework, which can capture both local content and structure information. These approaches were proved effective in incorporating semantic meanings of neighbouring vertices, while the usefulness of this information was not properly considered. In this paper, we propose an Attentive Graph-based Recursive Neural Network (AGRNN), which exerts attention on neural network to make our model focus on vertices with more relevant semantic information. We evaluated our approach on three real-world datasets and also datasets with synthetic noise. Our experimental results show that AGRNN achieves the state-of-the-art performance, in terms of effectiveness and robustness. We have also illustrated some attention weight samples to demonstrate the rationality of our model.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2403–2406},
numpages = {4},
keywords = {attention model, recursive neural network, collective vertex classification},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133076,
author = {Yang, Hongxia},
title = {Bayesian Heteroscedastic Matrix Factorization for Conversion Rate Prediction},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133076},
doi = {10.1145/3132847.3133076},
abstract = {Display Advertising has generated billions of revenue and originated hundreds of scientific papers and patents, yet the accuracy of prediction technologies leaves much to be desired. Conversion rates (CVR) predictions can often be formulated as a matrix or tensor completion problem where each dimension consists of thousands or even hundreds of thousands of levels. Observed entries are typically extremely sparse, comprising only 0.01% to 1% of the entire matrix or tensor with highly unevenly distributed conversion as well as impression sizes.To deal with these issues, we propose an extension of matrix factorization, namely Bayesian Heteroscedastic Matrix Factorization (BHMF), with three key features. First, BHMF accounts for the fact that each observed entry of a matrix has different magnitude of errors depending on the corresponding impression sizes. We extend the previous research on empirical instance-wise weighted matrix factorization with rigorous probabilistic modelling framework. Second, BHMF is amenable to an efficient Bayesian inference algorithm that is scalable to high dimensional matrices. Compared to the optimization based training, it is more robust to the choices of dimensions of the latent factors as well as regularization parameters. Last, the Bayesian approach provides predictive uncertainty estimations for unseen entries that is capable of dealing with cold-start problems. This can potentially affect a good amount of revenue in the real time bidding (RTB) environment. We focus on matrix CVR predictions in this paper but the proposed BHMF can be naturally extended and applied to higher dimensional tensors. We demonstrate the substantial improvement of our model in predictive capabilities on Yahoo! demand side platform (DSP) BrightRoll.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2407–2410},
numpages = {4},
keywords = {heteroscedastic, conversion rate prediction, bayesian matrix factorization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133056,
author = {Yao, Di and Zhang, Chao and Huang, Jianhui and Bi, Jingping},
title = {SERM: A Recurrent Model for Next Location Prediction in Semantic Trajectories},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133056},
doi = {10.1145/3132847.3133056},
abstract = {Predicting the next location a user tends to visit is an important task for applications like location-based advertising, traffic planning, and tour recommendation. We consider the next location prediction problem for semantic trajectory data, wherein each GPS record is attached with a text message that describes the user's activity. In semantic trajectories, the confluence of spatiotemporal transitions and textual messages indicates user intents at a fine granularity and has great potential in improving location prediction accuracies. Nevertheless, existing methods designed for GPS trajectories fall short in capturing latent user intents for such semantics-enriched trajectory data. We propose a method named semantics-enriched recurrent model (SERM). SERM jointly learns the embeddings of multiple factors (user, location, time, keyword) and the transition parameters of a recurrent neural network in a unified framework. Therefore, it effectively captures semantics-aware spatiotemporal transition regularities to improve location prediction accuracies. Our experiments on two real-life semantic trajectory datasets show that SERM achieves significant improvements over state-of-the-art methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2411–2414},
numpages = {4},
keywords = {location prediction, semantic trajectory, rnn},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133057,
author = {Yu, Chia-An and Chan, Tak-Shing and Yang, Yi-Hsuan},
title = {Low-Rank Matrix Completion over Finite Abelian Group Algebras for Context-Aware Recommendation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133057},
doi = {10.1145/3132847.3133057},
abstract = {The incorporation of contextual information is an important part of context-aware recommendation. Many context-aware recommendation systems adopt tensor completion to include contextual information. However, the symmetries between dimensions of a tensor induce an unreasonable assumption that users, items and contexts should be treated equally in recommender systems. In this paper, we address this by using matrices over finite abelian group algebra (AGA) to model context-aware interactions between users and items. Specifically, we formulate context-aware recommendation as a low-rank matrix completion problem over AGA (MC-AGA) and derive a new algorithm using the inexact augmented Lagrange multiplier method. We then test MC-AGA on two real-world datasets: one containing implicit feedback and one with explicit feedback. Experiment results show that MC-AGA outperforms not only existing tensor completion algorithms but also recommendation systems with other context-aware representations.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2415–2418},
numpages = {4},
keywords = {group algebra, matrix/tensor completion, low-rank modeling, context-aware recommendation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133139,
author = {Yuan, Shuhan and Wu, Xintao and Li, Jun and Lu, Aidong},
title = {Spectrum-Based Deep Neural Networks for Fraud Detection},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133139},
doi = {10.1145/3132847.3133139},
abstract = {In this paper, we focus on fraud detection on a signed graph with only a small set of labeled training data. We propose a novel framework that combines deep neural networks and spectral graph analysis. In particular, we use the node projection (called as spectral coordinate) in the low dimensional spectral space of the graph's adjacency matrix as the input of deep neural networks. Spectral coordinates in the spectral space capture the most useful topology information of the network. Due to the small dimension of spectral coordinates (compared with the dimension of the adjacency matrix derived from a graph), training deep neural networks becomes feasible. We develop and evaluate two neural networks, deep autoencoder and convolutional neural network, in our fraud detection framework. Experimental results on a real signed graph show that our spectrum based deep neural networks are effective in fraud detection.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2419–2422},
numpages = {4},
keywords = {fraud detection, spectrum, deep neural networks},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133067,
author = {Zhang, Yu and Wei, Wei and Huang, Binxuan and Carley, Kathleen M. and Zhang, Yan},
title = {RATE: Overcoming Noise and Sparsity of Textual Features in Real-Time Location Estimation},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133067},
doi = {10.1145/3132847.3133067},
abstract = {Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that "one-hot" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2423–2426},
numpages = {4},
keywords = {text mining, location inference, real-time, microblog},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133094,
author = {Zhao, Zhi-Lin and Wang, Chang-Dong and Lin, Kun-Yu and Lai, Jian-Huang},
title = {Missing Value Learning},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133094},
doi = {10.1145/3132847.3133094},
abstract = {Missing value is common in many machine learning problems and much effort has been made to handle missing data to improve the performance of the learned model. Sometimes, our task is not to train a model using those unlabeled/labeled data with missing value but process examples according to the values of some specified features. So, there is an urgent need of developing a method to predict those missing values. In this paper, we focus on learning from the known values to learn missing value as close as possible to the true one. It's difficult for us to predict missing value because we do not know the structure of the data matrix and some missing values may relate to some other missing values. We solve the problem by recovering the complete data matrix under the three reasonable constraints: feature relationship, upper recovery error bound and class relationship. The proposed algorithm can deal with both unlabeled and labeled data and generative adversarial idea will be used in labeled data to transfer knowledge. Extensive experiments have been conducted to show the effectiveness of the proposed algorithms.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2427–2430},
numpages = {4},
keywords = {supervised learning, missing value, unsupervised learning, generative adversarial},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133099,
author = {Zheng, Jing and Zhuang, Fuzhen and Shi, Chuan},
title = {Local Ensemble across Multiple Sources for Collaborative Filtering},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133099},
doi = {10.1145/3132847.3133099},
abstract = {Recently, Transfer Collaborative Filtering (TCF) methods across multiple source domains, which employ knowledge from different source domains to improve the recommendation performance in the target domain, have been applied in recommender systems. The existing multi-source TCF methods either require overlapping objects in different domains or simply re-weight domains to merge them together. In this paper, we propose a novel LO cal EN semble framework across multiple source domains for collaborative filtering (called LOEN for short), where weights of multiple sources for each missing rating in the target domain are determined according to their corresponding local structures. Compared with the previous TCF methods, LOEN does not require overlapping data and considers the divergence of sources through exploiting the local structures of ratings, which allows LOEN to be more general and effective. Experiments conducted on real datasets validate the effectiveness of LOEN, especially for knowledge transfer across unrelated source domains.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2431–2434},
numpages = {4},
keywords = {transfer collaborative filtering, recommender system, local ensemble},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133063,
author = {Zhu, Endong and Rao, Yanghui and Xie, Haoran and Liu, Yuwei and Yin, Jian and Wang, Fu Lee},
title = {Cluster-Level Emotion Pattern Matching for Cross-Domain Social Emotion Classification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133063},
doi = {10.1145/3132847.3133063},
abstract = {This paper addresses the task of cross-domain social emotion classification of online documents. The cross-domain task is formulated as using abundant labeled documents from a source domain and a small amount of labeled documents from a target domain, to predict the emotion of unlabeled documents in the target domain. Although several cross-domain emotion classification algorithms have been proposed, they require that feature distributions of different domains share a sufficient overlapping, which is hard to meet in practical applications. This paper proposes a novel framework, which uses the emotion distribution of training documents at the cluster level, to alleviate the aforementioned issue. Experimental results on two datasets show the effectiveness of our proposed model on cross-domain social emotion classification.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2435–2438},
numpages = {4},
keywords = {cross-domain classification, emotion detection, clustering},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133064,
author = {Zhu, Shuguang and Cheng, Xiang and Su, Sen and Lang, Shuang},
title = {Knowledge-Based Question Answering by Jointly Generating, Copying and Paraphrasing},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133064},
doi = {10.1145/3132847.3133064},
abstract = {With the development of large-scale knowledge bases, people are building systems which give simple answers to questions based on consolidate facts. In this paper, we focus on simple questions, which ask about only a subject and relation in the knowledge base. Observing that certain parts of a question usually overlap with names of its corresponding subject and relation in the knowledge base, we argue that a question is formed by a mixture of copying and generation. To model that, we propose a sequence-to-sequence (seq2seq) architecture which encodes a candidate subject-relation pair and decodes it into the given question, where the decoding probability is used to select the best candidate. In our decoder, the copying mode points the subject or relation and duplicates its name, while the generating mode summarizes the meaning of the subject-relation pair and produces a word to smooth the question. Realizing that although sometimes a subject or relation is pointed, different names or keywords might be used, we also incorporate a paraphrasing mode to supplement the copying mode using an automatically mined lexicon. Extensive experiments on the largest dataset exhibit our better performance compared with the state-of-the-art methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2439–2442},
numpages = {4},
keywords = {question answering, neural network, knowledge base},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133176,
author = {Amsterdamer, Yael and Goldreich, Oded},
title = {PODIUM: Procuring Opinions from Diverse Users in a Multi-Dimensional World},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133176},
doi = {10.1145/3132847.3133176},
abstract = {The procurement of opinions is an important task in many contexts. When selecting members of a certain population to ask for their opinions, diversity inside the selected subset is a central consideration. People with diverse profiles are assumed to provide a wider range of opinions and thus to better represent the opinions of the entire population. However, in platforms with a large user base such as crowdsourcing applications and social networks, defining and realizing notions of diversity are both nontrivial. The profiles of users typically contain information that is high-dimensional and semantically rich. We present PODIUM, a tool for opinion procurement that accounts for complex user profiles and enables customizable user selection. Beyond selecting a subset of users with diverse profiles, PODIUM produces explanation for the choice of each user and visual aids to compare the selected subset to the entire population on different dimensions. We demonstrate the use of PODIUM on the TripAdvisor user base, which further enables us to examine the ability of our system to predict diverse opinions in user reviews.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2443–2446},
numpages = {4},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133190,
author = {Arman, Arif and Ali, Mohammed Eunus and Choudhury, Farhana Murtaza and Abdullah, Kaysar},
title = {VizQ: A System for Scalable Processing of Visibility Queries in 3D Spatial Databases},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133190},
doi = {10.1145/3132847.3133190},
abstract = {In this demonstration, we present VizQ, an efficient, scalable, and interactive system to process and visualize a comprehensive collection of novel visibility queries in the presence of obstacles in 3D space. Specifically, we demonstrate four types of query processing: (i) k Maximum Visibility Query (kMVQ), that finds k locations with the maximum visibility of a target object (ii) Visibility Color Map (VCM), where each point in the space is assigned a color value denoting the visibility measure of the target (iii) Continuous Maximum Visibility (CMV) that continuously finds the location that provides the best view of a moving target, and (iv) Text Visibility Color Map (TVCM), where VCM is generated considering readability of text data displayed on a target. We are the first to propose efficient algorithms to run all of the above four types of visibility queries in the context of a large number of 3D obstacle database. We exploit human visibility metrics to design our data structures and algorithms to efficiently process queries, and our approaches outperform baseline approaches in several order of magnitude both in terms of I/Os and processing time. The link of our demonstration video is https://youtu.be/rcizJtFvQfU.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2447–2450},
numpages = {4},
keywords = {visibility query, spatial database, location based services},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133171,
author = {Beheshti, Amin and Benatallah, Boualem and Nouri, Reza and Chhieng, Van Munin and Xiong, HuangTao and Zhao, Xu},
title = {CoreDB: A Data Lake Service},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133171},
doi = {10.1145/3132847.3133171},
abstract = {The continuous improvement in connectivity, storage and data processing capabilities allow access to a data deluge from sensors, social-media, news, user-generated, government and private data sources. Accordingly, in a modern data-oriented landscape, with the advent of various data capture and management technologies, organizations are rapidly shifting to datafication of their processes. In such an environment, analysts may need to deal with a collection of datasets, from relational to NoSQL, that holds a vast amount of data gathered from various private/open data islands, i.e. Data Lake. Organizing, indexing and querying the growing volume of internal data and metadata, in a data lake, is challenging and requires various skills and experiences to deal with dozens of new databases and indexing technologies: How to store information items? What technology to use for persisting the data? How to deal with the large volume of streaming data? How to trace and persist information about data? What technology to use for indexing the data? How to query the data lake? To address the above mentioned challenges, we present CoreDB - an open source data lake service - which offers researchers and developers a single REST API to organize, index and query their data and metadata. CoreDB manages multiple database technologies and offers a built-in design for security and tracing.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2451–2454},
numpages = {4},
keywords = {data lake, database service, data api},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133174,
author = {Ekron, Maya and Milo, Tova and Youngmann, Brit},
title = {SimMeme: Semantic-Based Meme Search},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133174},
doi = {10.1145/3132847.3133174},
abstract = {With the proliferation of social image-sharing applications, image search becomes an increasingly common activity. In this work, we focus on a particular class of images that convey semantic meaning beyond the visual appearance, and whose search presents particular challenges. A prominent example is Memes, an emerging popular type of captioned pictures, which we will use in this demo to demonstrate our solution. Unlike in conventional image-search, visually similar Memes may reflect different concepts. The intent is sometimes captured by user annotations, but these too are often incomplete and ambiguous. Thus, a deeper analysis of the semantic relations among Memes is required for an accurate search. To address this problem, we present SimMeme, a semantic aware search engine for Memes. SimMeme uses a generic graph-based data model that aligns all the information available about the Memes with a semantic ontology. A novel similarity measure that interweaves common image, textual, structural and semantic similarities into one holistic measure is employed to effectively answer user queries. We will demonstrate the operation of SimMeme over a large repository of real-life annotated Memes which we have constructed by web crawling and crowd annotations, allowing users to appreciate the quality of the search results as well as the execution efficiency.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2455–2458},
numpages = {4},
keywords = {semantic, similarity, information-network, image-search},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133183,
author = {Feigenblat, Guy and Boni, Odellia and Roitman, Haggai and Konopnicki, David},
title = {SummIt: A Tool for Extractive Summarization, Discovery and Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133183},
doi = {10.1145/3132847.3133183},
abstract = {We propose to demonstrate SummIt -- a tool for extractive summarization, discovery and analysis. The main goal of SummIt is to provide consumable summaries that are driven by users' information intents. To this end, SummIt discovers and analyzes potential intents that can be used for summarization. Given an intent, SummIt generates a summary based on a novel unsupervised, query-focused, extractive, multi-document summarization approach. Using visualization aids, SummIt further allows to analyze a given summary and explore both its narrow and broader context.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2459–2462},
numpages = {4},
keywords = {unsupervised, query-focused, summarization, discovery, search intents},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133170,
author = {Freitas, Scott and Tong, Hanghang and Cao, Nan and Xia, Yinglong},
title = {Rapid Analysis of Network Connectivity},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133170},
doi = {10.1145/3132847.3133170},
abstract = {This research focuses on accelerating the computational time of two base network algorithms (k-simple shortest paths and minimum spanning tree for a subset of nodes)---cornerstones behind a variety of network connectivity mining tasks---with the goal of rapidly finding networkpathways andtrees using a set of user-specific query nodes. To facilitate this process we utilize: (1) multi-threaded algorithm variations, (2) network re-use for subsequent queries and (3) a novel algorithm, Key Neighboring Vertices (KNV), to reduce the network search space. The proposed KNV algorithm serves a dual purpose: (a) to reduce the computation time for algorithmic analysis and (b) to identify key vertices in the network (textit   ). Empirical results indicate this combination of techniques significantly improves the baseline performance of both algorithms. We have also developed a web platform utilizing the proposed network algorithms to enable researchers and practitioners to both visualize and interact with their datasets (PathFinder: http://www.path-finder.io.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2463–2466},
numpages = {4},
keywords = {mst, network visualization, k-simple shortest paths, seed nodes, multi-threading, search space reduction, parallel processing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133167,
author = {Hubig, Nina and Passing, Linnea and Sch\"{u}le, Maximilian E. and Vorona, Dimitri and Kemper, Alfons and Neumann, Thomas},
title = {HyPerInsight: Data Exploration Deep Inside HyPer},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133167},
doi = {10.1145/3132847.3133167},
abstract = {Nowadays we are drowning in data of various varieties. For all these mixed types and categories of data there exist even more different analysis approaches, often done in single hand-written solutions. We propose to extend HyPer, a main memory database system to a uniform data agent platform following the one system fits all approach for solving a wide variety of data analysis problems. We achieve this by applying a flexible operator concept to a set of various important data exploration algorithms. With that, HyPer solves analytical questions using clustering, classification, association rule mining and graph mining besides standard HTAP (Hybrid Transaction and Analytical Processing) workloads on the same database state. It enables to approach the full variety and volume of HTAP extended for data exploration (HTAPx), and only needs knowledge of already introduced SQL extensions that are automatically optimized by the database's standard optimizer. In this demo we will focus on the benefits and flexibility we create by using the SQL extensions for several well-known mining workloads. In our interactive webinterface for this project named HyPerInsight we demonstrate how HyPer outperforms the best open source competitor Apache Spark in common use cases in social media, geo-data, recommender systems and several other.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2467–2470},
numpages = {4},
keywords = {k-means, dbscan, query processing, hyper, apriori, database operators, sql},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133166,
author = {Jatowt, Adam and Campos, Ricardo},
title = {Interactive System for Reasoning about Document Age},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133166},
doi = {10.1145/3132847.3133166},
abstract = {Recently, many historical texts have become digitized and made accessible for search and browsing. Professionals who work with collections of such texts often need to verify the correctness of documents' key metadata - their creation dates. In this paper, we demonstrate an interactive system for estimating the age of documents. It may be useful not only for tagging a large number of undated documents, but also for verifying already known timestamps. In order to infer probable dates, we rely on a large scale lexical corpora, Google Books Ngrams. Besides estimating the document creation year, the system also outputs evidences to support age detection and reasoning process and allows testing different hypotheses about document's age.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2471–2474},
numpages = {4},
keywords = {document metadata, document timestamping, historical texts},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133192,
author = {Kharlamov, Evgeny and Giacomelli, Luca and Sherkhonov, Evgeny and Grau, Bernardo Cuenca and Kostylev, Egor V. and Horrocks, Ian},
title = {SemFacet: Making Hard Faceted Search Easier},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133192},
doi = {10.1145/3132847.3133192},
abstract = {Faceted search is a prominent search paradigm that became the standard in many Web applications and has also been recently proposed as a suitable paradigm for exploring and querying RDF graphs. One of the main challenges that hampers usability of faceted search systems especially in the RDF context is information overload, that is, when the size of faceted interfaces becomes comparable to the size of the data over which the search is performed. In this demo we present (an extension of) our faceted search system SemFacet and focus on features that address the information overload: ranking, aggregation, and reachability. The demo attendees will be able to try our system on an RDF graph that models online shopping over a catalogs with up to millions of products.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2475–2478},
numpages = {4},
keywords = {recursion, faceted search, ranking, aggregation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133180,
author = {Kruse, Sebastian and Hahn, David and Walter, Marius and Naumann, Felix},
title = {Metacrate: Organize and Analyze Millions of Data Profiles},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133180},
doi = {10.1145/3132847.3133180},
abstract = {Databases are one of the great success stories in IT. However, they have been continuously increasing in complexity, hampering operation, maintenance, and upgrades. To face this complexity, sophisticated methods for schema summarization, data cleaning, information integration, and many more have been devised that usually rely on data profiles, such as data statistics, signatures, and integrity constraints. Such data profiles are often extracted by automatic algorithms, which entails various problems: The profiles can be unfiltered and huge in volume; different profile types require different complex data structures; and the various profile types are not integrated with each other. We introduce Metacrate, a system to store, organize, and analyze data profiles of relational databases, thereby following the proven design of databases. In particular, we (i) propose a logical and a physical data model to store all kinds of data profiles in a scalable fashion; (ii) describe an analytics layer to query, integrate, and analyze the profiles efficiently; and (iii) implement on top a library of established algorithms to serve use cases, such as schema discovery, database refactoring, and data cleaning.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2483–2486},
numpages = {4},
keywords = {data profiling, metadata management, database administration},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133181,
author = {Le, Tuan M. V. and Lauw, Hady W.},
title = {SemVis: Semantic Visualization for Interactive Topical Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133181},
doi = {10.1145/3132847.3133181},
abstract = {Exploratory analysis of a text corpus is an important task that can be aided by informative visualization. One spatially-oriented form of document visualization is a scatterplot, whereby every document is associated with a coordinate, and relationships among documents can be perceived through their spatial distances. Semantic visualization further infuses the visualization space with latent semantics, by incorporating a topic model that has a representation in the visualization space, allowing users to also perceive relationships between documents and topics spatially. We illustrate how a semantic visualization system called SemVis could be used to navigate a text corpus interactively and topically via browsing and searching.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2487–2490},
numpages = {4},
keywords = {interactive topical analysis, semantic visualization, topic model},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133179,
author = {Leblay, Julien and Chen, Weiling and Lynden, Steven},
title = {Exploring the Veracity of Online Claims with BackDrop},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133179},
doi = {10.1145/3132847.3133179},
abstract = {Using the Web to assess the validity of claims presents many challenges. Whether the data comes from social networks or established media outlets, individual or institutional data publishers, one has to deal with scale and heterogeneity, as well as with incomplete, imprecise and sometimes outright false information. All of these are closely studied issues. Yet in many situations, the claims under scrutiny, and the data itself, have some inherent context-dependency making them impossible to completely disprove, or evaluate through a simple (e.g. scalar) measure. While data models used on the Web typically deal with universal knowledge, we believe the time has come to put context, such as time or provenance, at the forefront and watch knowledge through multiple lenses. We present BackDrop, an application that enables annotating knowledge and ontologies found online to explore how the veracity of claims varies with context. BackDrop comes in the form of a Web interface, in which users can interactively populate and annotate knowledge bases, and explore under which circumstances certain claims are more or less credible.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2491–2494},
numpages = {4},
keywords = {fact checking, web data, contextual reasoning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133169,
author = {Li, Feng-Lin and Qiu, Minghui and Chen, Haiqing and Wang, Xiongwei and Gao, Xing and Huang, Jun and Ren, Juwei and Zhao, Zhongzhou and Zhao, Weipeng and Wang, Lei and Jin, Guwei and Chu, Wei},
title = {<i>AliMe Assist </i>: An Intelligent Assistant for Creating an Innovative E-Commerce Experience},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133169},
doi = {10.1145/3132847.3133169},
abstract = {We present AliMe Assist, an intelligent assistant designed for creating an innovative online shopping experience in E-commerce. Based on question answering (QA), AliMe Assist offers assistance service, customer service, and chatting service. It is able to take voice and text input, incorporate context to QA, and support multi-round interaction. Currently, it serves millions of customer questions per day and is able to address 85% of them. In this paper, we demonstrate the system, present the underlying techniques, and share our experience in dealing with real-world QA in the E-commerce field.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2495–2498},
numpages = {4},
keywords = {knowledge graph, rerank, convolutional neural network, question answering, sequence-to-sequence, semantic normalization},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133173,
author = {Li, Guanyao and Chen, Chun-Jie and Huang, Sheng-Yun and Chou, Ai-Jou and Gou, Xiaochuan and Peng, Wen-Chih and Yi, Chih-Wei},
title = {Public Transportation Mode Detection from Cellular Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133173},
doi = {10.1145/3132847.3133173},
abstract = {Public transportation is essential in people's daily life and it is crucial to understand how people move around the city. Some prior works have exploited GPS, Wi-Fi or bluetooth to collect data, in which extra sensors or devices were needed. Other works utilized data from smart card systems. However, some public transportation systems have their own smart card system and the smart card data cannot include all kinds of transportation modes, which makes it unsuitable for our study.Nowadays, each user has his/her own mobile phones and from the cellular data of mobile phone service providers, it is possible to know the uses' transportation mode and the fine-grained crowd flows. As such, given a set of cellular data, we propose a system for public transportation mode detection, crowd density estimation, and crowd flow estimation. Note that we only have cellular data, no extra sensor data collected from users' mobile phones. In this paper, we refer to some external data sources (e.g., the bus routing networks) to identify transportation modes. Users' cellular data sometimes have uncertainty about user location information. Thus, we propose two approaches for different transportation mode detection considering the cell tower properties, spatial and temporal factors. We demonstrate our system using the data from Chunghwa Telecom, which is the largest telecommunication company in Taiwan, to show the usefulness of our system.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2499–2502},
numpages = {4},
keywords = {smart cities, transportation mode detection, urban computing, crowd density and flow estimation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133177,
author = {Liu, Mengxiong and Liu, Zhengchao and Zhang, Chao and Zhang, Keyang and Yuan, Quan and Hanratty, Tim and Han, Jiawei},
title = {Urbanity: A System for Interactive Exploration of Urban Dynamics from Streaming Human Sensing Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133177},
doi = {10.1145/3132847.3133177},
abstract = {With the urbanization process worldwide, modeling the dynamics of people's activities in urban environments has become a crucial socioeconomic task. We present Urbanity, a novel system that leverages geo-tagged social media streams for modeling urban dynamics. Urbanity automatically discovers the spatial and temporal hotspots where people's activities concentrate; and captures the cross-modal correlations among location, time, and text by jointly mapping different units into the same latent space. With Urbanity, the end users are able to use flexible query schemes to retrieve different resources (e.g., POIs, hotspots, hours, activities) that meet their needs. Furthermore, Urbanity can handle continuous streams to update the learned model, thus revealing up-to-date patterns of urban activities.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2503–2506},
numpages = {4},
keywords = {multimodal embedding, urban computing, spatiotemporal data, activity modeling, social media},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133191,
author = {Mehdi, Gulnar and Kharlamov, Evgeny and Savkovi\'{c}, Ognjen and Xiao, Guohui and Kalayci, Elem G\"{u}zel and Brandt, Sebastian and Horrocks, Ian and Roshchin, Mikhail and Runkler, Thomas},
title = {SemDia: Semantic Rule-Based Equipment Diagnostics Tool},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133191},
doi = {10.1145/3132847.3133191},
abstract = {Rule-based diagnostics of power generating equipment is an important task in industry. In this demo we present how semantic technologies can enhance diagnostics. In particular, we present our semantic rule language sigRL that is inspired by the real diagnostic languages in Siemens. SigRL allows to write compact yet powerful diagnostic programs by relying on a high level data independent vocabulary, diagnostic ontologies, and queries over these ontologies. We present our diagnostic system SemDia. The attendees will be able to write diagnostic programs in SemDia using sigRL over 50 Siemens turbines. We also present how such programs can be automatically verified for redundancy and inconsistency. Moreover, the attendees will see the provenance service that SemDia provides to trace the origin of diagnostic results.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2507–2510},
numpages = {4},
keywords = {ontologies, diagnostic systems, rules, turbines},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133193,
author = {Paramonov, Sergey and Kolb, Samuel and Guns, Tias and De Raedt, Luc},
title = {TaCLe: Learning Constraints in Tabular Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133193},
doi = {10.1145/3132847.3133193},
abstract = {Spreadsheet data is widely used today by many different people and across industries. However, writing, maintaining and identifying good formulae for spreadsheets can be time consuming and error-prone. To address this issue we have introduced the TaCLe system (Tabular Constraint Learner). The system tackles an inverse learning problem: given a plain comma separated file, it reconstructs the spreadsheet formulae that hold in the tables. Two important considerations are the number of cells and constraints to check, and how to deal with multiple formulae for the same cell. Our system reasons over entire rows and columns and has an intuitive user interface for interacting with the learned constraints and data. It can be seen as an intelligent assistance tool for discovering formulae from data. As a result, the user obtains a spreadsheet that can automatically recompute dependent cells when updating or adding data.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2511–2514},
numpages = {4},
keywords = {constraint learning, spreadsheets, relational learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133164,
author = {Persia, Fabio and Bettini, Fabio and Helmer, Sven},
title = {An Interactive Framework for Video Surveillance Event Detection and Modeling},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133164},
doi = {10.1145/3132847.3133164},
abstract = {We present a framework for high-level event detection in video streams based on a novel temporal extension of relational algebra. With the help of intuitive and interactive graphical user interfaces, a user can have a look at the different layers of our system to gain insights into the inner workings of the system, as well as create new events on the fly and track their processing through the system. As a proof-of-concept we have predefined events on three video surveillance data sets, but we also plan to run a demo with a live video stream generated by a local webcam.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2515–2518},
numpages = {4},
keywords = {high-level event detection, video stream, event query languages, surveillance, intervals},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133186,
author = {Remus, Steffen and Kaufmann, Manuel and Ballweg, Kathrin and von Landesberger, Tatiana and Biemann, Chris},
title = {Storyfinder: Personalized Knowledge Base Construction and Management by Browsing the Web},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133186},
doi = {10.1145/3132847.3133186},
abstract = {This paper presents Storyfinder, an application which consists of a browser plugin and a web server backend with the goal to highlight and manage the information contained in web pages by combining techniques from natural language processing and visual analytics. Webpages are analyzed while visiting them by means of natural language processing components, and metadata in the form of named entities and keywords are extracted and stored for further reference. The extracted information is instantaneously highlighted in the web page and stored in a graph of entities and relations. The graph can be inspected and modified. The investigational scope can be set to a single web page, multiple web pages, or the complete set of analyzed web pages in a user's history. The graph view is designed to adhere to standards of visual analytics and information visualization. Storyfinder is available as an open source application. Its benefit for information access is evaluated in a small user study.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2519–2522},
numpages = {4},
keywords = {knowledge management, information extraction, visual analytics},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133184,
author = {Saleem, Muhammad Aamir and Kumar, Rohit and Calders, Toon and Xie, Xike and Pedersen, Torben Bach},
title = {IMaxer: A Unified System for Evaluating Influence Maximization in Location-Based Social Networks},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133184},
doi = {10.1145/3132847.3133184},
abstract = {Due to the popularity of social networks with geo-tagged activities, so-called location-based social networks (LBSN), a number of methods have been proposed for influence maximization for applications such as word-of-mouth marketing (WOMM), and out-of-home marketing (OOH). It is thus important to analyze and compare these different approaches. In this demonstration, we present a unified system IMaxer that both provides a complete pipeline of state-of-the-art and novel models and algorithms for influence maximization (IM) as well as allows to evaluate and compare IM techniques for a particular scenario. IMaxer allows to select and transform the required data from raw LBSN datasets. It further provides a unified model that utilizes interactions of nodes in an LBSN, i.e., users and locations, for capturing diverse types of information propagations. On the basis of these interactions, influential nodes can be found and their potential influence can be simulated and visualized using Google Maps and graph visualization APIs. Thus, IMaxer allows users to compare and pick the most suitable IM method in terms of effectiveness and cost.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2523–2526},
numpages = {4},
keywords = {influence maximization, location-based social networks, unified system},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133165,
author = {Shaikh, Salman Ahmed and Kitagawa, Hiroyuki},
title = {StreamingCube: A Unified Framework for Stream Processing and OLAP Analysis},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133165},
doi = {10.1145/3132847.3133165},
abstract = {In most streaming applications, the data streams need to be analyzed continuously to make instant decisions exploiting latest information. Often data streams are multidimensional and are at the low-level of abstraction, whereas analysts are interested in multi-level interactive analysis of data streams across several dimensions. On-line analytical processing (OLAP) is a proven technique for such analysis of static data and has also been studied by some researchers for data streams. Traditionally this is achieved by coupling a stream processing engine with an OLAP engine. We believe that coupling multiple systems is not an efficient solutions as it results in lower performance (due to the transfer of data between multiple systems), resource wastage (due to replication of data for each coupled system) and increased complexity and maintenance cost. To this end, we present StreamingCube, a unified framework for data stream processing and its interactive OLAP analysis. The proposed framework possesses all the essential operators to process data streams and introduces a new operator, cubify, to maintain OLAP lattice nodes (materialized views) incrementally. The novelty of the introduced cubify operator lies in the incremental maintenance of the materialized views. To demonstrate StreamingCube, a web-based GUI has been developed which enables users to register continuous queries (CQs). Once a CQ has been registered, users can perform different OLAP operations through the GUI for the interactive analysis. The results of the OLAP queries/operations are displayed in the form of tables and graphs.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2527–2530},
numpages = {4},
keywords = {Unified framework, Incremental view maintenance, Online analytical processing, Stream processing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133175,
author = {Skopal, Tom\'{a}\v{s} and Pe\v{s}ka, Ladislav and Koval\v{c}\'{\i}k, Gregor and Grosup, Tom\'{a}\v{s} and Loko\v{c}, Jakub},
title = {Product Exploration Based on Latent Visual Attributes},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133175},
doi = {10.1145/3132847.3133175},
abstract = {In this demo paper, we present a prototype web application of a product search engine of a fashion e-shop. Although e-shop products consist of full-text description, relational attributes (e.g., price, type, size, color, etc.) as well as visual information (product photo), traditional search engines in e-shops only provide full-text and relational attributes for product filtering. In our retrieval model, we incorporate also the visual information into the search by extracting visual-semantic features using deep convolutional neural networks. Furthermore, visual exploration of the product space using the visual-semantic features (multi-example queries) is used to dynamically discover latent visual attributes that could enhance the original relational schema by fuzzy attributes (e.g., a floral pattern in product). In the demo, we show how these latent attributes could be used to recommend the user preferred products and even outfits (e.g., shoes, bag, jacket) that fit a certain visual style.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2531–2534},
numpages = {4},
keywords = {outfit recommendation, convolutional neural networks, latent visual attributes},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133185,
author = {Suzanna, Sia Xin Yun and Anthony, Li Lianjie},
title = {Hierarchical Module Classification in Mixed-Initiative Conversational Agent System},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133185},
doi = {10.1145/3132847.3133185},
abstract = {Our operational context is a task-oriented dialog system where no single module satisfactorily addresses the range of conversational queries from humans. Such systems must be equipped with a range of technologies to address semantic, factual, task-oriented, open domain conversations using rule-based, semantic-web, traditional machine learning and deep learning. This raises two key challenges. First, the modules need to be managed and selected appropriately. Second, the complexity of troubleshooting on such systems is high. We address these challenges with a mixed-initiative model that controls conversational logic through hierarchical classification. We also developed an interface to increase interpretability for operators and to aggregate module performance.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2535–2538},
numpages = {4},
keywords = {language modeling, conversational agent, machine learning, dialogue systems},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133172,
author = {Vo, Hoang Tam and Mehedy, Lenin and Mohania, Mukesh and Abebe, Ermyas},
title = {Blockchain-Based Data Management and Analytics for Micro-Insurance Applications},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133172},
doi = {10.1145/3132847.3133172},
abstract = {In this paper, we demonstrate a blockchain-based solution for transparently managing and analyzing data in a pay-as-you-go car insurance application. This application allows drivers who rarely use cars to only pay insurance premium for particular trips they would like to travel. One of the key challenges from database perspective is how to ensure all the data pertaining to the actual trip and premium payment made by the users are transparently recorded so that every party in the insurance contract including the driver, the insurance company, and the financial institution is confident that the data are tamper-proof and traceable.  Another challenge from information retrieval perspective is how to perform entity matching and pattern matching on customer data as well as their trip and claim history recorded on the blockchain for intelligent fraud detection. Last but not least, the drivers' trip history, once have been collected sufficiently, can be much valuable for the insurance company to do offline analysis and build statistics on past driving behaviour and past vehicle runtime. These statistics enable the insurance company to offer the users with transparent and individualized insurance quotes. Towards this end, we develop a blockchain-based solution for micro-insurance applications that transparently keeps records and executes smart contracts depending on runtime conditions while also connecting with off-chain analytic databases.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2539–2542},
numpages = {4},
keywords = {blockchain, information retrieval, data management, information management, data analytics},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133187,
author = {Wang, Hongzhi and Ding, Xiaoou and Chen, Xiangying and Li, Jianzhong and Gao, Hong},
title = {CleanCloud: Cleaning Big Data on Cloud},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133187},
doi = {10.1145/3132847.3133187},
abstract = {We describe CleanCloud, a system for cleaning big data based on Map-Reduce paradigm in cloud. Using Map-Reduce paradigm, the system detects and repairs various data quality problems in big data. We demonstrate the following features of CleanCloud: (a) the support for cleaning multiple data quality problems in big data; (b) a visual tool for watching the status of big data cleaning process and tuning the parameters for data cleaning; (c) the friendly interface for data input and setting as well as cleaned data collection for big data. CleanCloud is a promising system that provides scalable and effect data cleaning mechanism for big data in either files or databases.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2543–2546},
numpages = {4},
keywords = {entity resolution, parallel computing, data cleaning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133189,
author = {Wei, Mingrui and Cao, Lei and Cormier, Chris and Zheng, Hui and Rundensteiner, Elke A.},
title = {Interactive Analytics System for Exploring Outliers},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133189},
doi = {10.1145/3132847.3133189},
abstract = {ONION is the first system with rich interactive support for efficiently analyzing outliers. ONION features an innovative exploration model that offers an "outlier-centric panorama'' into big datasets. The ONION system is composed of an offline preprocessing phase followed by an online exploration phase that supports rich classes of novel exploration operations. As our demonstration illustrates, this enables analysts to interactively explore outliers at near real-time speed even over large datasets. We demonstrate ONION's capabilities with urban planning applications use cases on the Open Street Maps dataset.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2547–2550},
numpages = {4},
keywords = {parameter exploration, interactive analytics, outlier detection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133178,
author = {Xu, Jianqiu and G\"{u}ting, Ralf Hartmut},
title = {Query and Animate Multi-Attribute Trajectory Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133178},
doi = {10.1145/3132847.3133178},
abstract = {The widespread use of GPS-enabled devices has led to huge amounts of trajectory data. In addition to location and time, trajectories are associated with descriptive attributes representing different aspects of real entities, called multi-attribute trajectories. This comes from the combination of several data sources and enables a range of new applications in which users can find interesting trajectories and discover potential relationships that cannot be determined solely based on GPS data. In this demo, we provide the motivation scenario and introduce a system that is developed to integrate standard trajectories (a sequence of timestamped locations) and attributes into one unified framework. The system is able to answer a range of interesting queries on multi-attribute trajectories that are not handled by standard trajectories. The system supports both standard trajectories and multi-attribute trajectories. We demonstrate how to form queries and animate multi-attribute trajectories in the system. To our knowledge, existing moving objects prototype systems do not support multi-attribute trajectories.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2551–2554},
numpages = {4},
keywords = {multi-attribute trajectories, animation, queries, index structure},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133182,
author = {Zhi, Shi and Sun, Yicheng and Liu, Jiayi and Zhang, Chao and Han, Jiawei},
title = {ClaimVerif: A Real-Time Claim Verification System Using the Web and Fact Databases},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133182},
doi = {10.1145/3132847.3133182},
abstract = {Our society is increasingly digitalized. Every day, a tremendous amount of information is being created, shared, and digested through all kinds of cyber channels. Although people can easily acquire information from various sources (social media, news articles, etc.), the truthfulness of most received information remains unverified. In many real-life scenarios, false information has become the de facto cause that leads to detrimental decision makings, and techniques that can automatically filter false information are highly demanded. However, verifying whether a piece of information is trustworthy is difficult because: (1) selecting candidate snippets for fact checking is nontrivial; and (2) detecting supporting evidences, i.e. stances, suffers from the difficulty of measuring the similarity between claims and related evidences. We build ClaimVerif, a claim verification system that not only provides credibility assessment for any user-given query claim, but also rationales the assessment results with supporting evidences. ClaimVerif can automatically select the stances from millions of documents and employs two-step training to justify the opinions of the stances. Furthermore, combined with the credibility of stances sources, ClaimVerif degrades the score of stances from untrustworthy sources and alleviates the negative effects from rumor spreaders. Our empirical evaluations show that ClaimVerif achieves both high accuracy and efficiency in different claim verification tasks. It can be highly useful in practical applications by providing multi-dimension analysis for the suspicious statements, including the stances, opinions, source credibility and estimated judgements.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2555–2558},
numpages = {4},
keywords = {fact checking, text mining, source credibility analysis, rumor detection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133168,
author = {Zhong, Ping and Li, Zhanhuai and Chen, Qun and Wang, Yanyan and Wang, Lianping and Ahmed, Murtadha HM and Fan, Fengfeng},
title = {POOLSIDE: An Online Probabilistic Knowledge Base for Shopping Decision Support},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133168},
doi = {10.1145/3132847.3133168},
abstract = {We present POOLSIDE, an online PrObabilistic knOwLedge base for ShoppIng DEcision support, that provides with the on-target recommendation service based on explicit user requirement. With a natural language interface, POOLSIDE can answer question in real-time. We present how to construct the knowledge base and how to enable real-time response in POOLSIDE. Finally, we demonstrate that Poolside can give high-quality product recommendations with high efficiency.(The demo video can be accessed via the link:https://www.youtube.com/watch?v=D8ALi11CUcc)},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2559–2562},
numpages = {4},
keywords = {decision support system, knowledge base, markov logic network},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133197,
author = {Hasanuzzaman, Mohammed and Dias, Ga\"{e}l and Jatowt, Adam and D\"{u}ring, Marten and van den Bosch, Antal},
title = {Overview of the 4th HistoInformatics Workshop},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133197},
doi = {10.1145/3132847.3133197},
abstract = {In line with global trends, historical records are increasingly available in forms that computer can process. These ever expanding records (such as scanned books, large-scale corpora, academic papers, maps, photos, audios, videos)---either digitally born or reconstructed through digitization pipelines---are too big to be read or viewed manually. Historians, like other humanities researchers, have a keen interest in computational approaches to process and study digitized historical information for research, writing, and dissemination of historical knowledge. In Computer Science, experimental tools and methods are challenged to be validated regarding their relevance for real-world questions and applications. The HistoInformatics workshop series is focused on the challenges and opportunities of data-driven humanities and brings together scientists and scholars at the forefront of this emerging field, at the interface between History, Anthropology, Archaeology, Computer Science and associated disciplines as well as the cultural heritage sector. The 4th HistoInformatics Workshop was a half day workshop co-located with the 26th ACM International Conference on Information and Knowledge Management (CIKM 2017) in Singapore.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2563–2564},
numpages = {2},
keywords = {cultural heritage, computational history, digital history},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133198,
author = {Hu, Xia and Ji, Shuiwang},
title = {IDM 2017: Workshop on Interpretable Data Mining -- Bridging the Gap between Shallow and Deep Models},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133198},
doi = {10.1145/3132847.3133198},
abstract = {Intelligent systems built upon complex machine learning and data mining models (e.g., deep neural networks) have shown superior performances on various real-world applications. However, their effectiveness is limited by the difficulty in interpreting the resultant prediction mechanisms or how the results are obtained. In contrast, the results of many simple or shallow models, such as rule-based or tree-based methods, are explainable but not sufficiently accurate. Model interpretability enables the systems to be clearly understood, properly trusted, effectively managed and widely adopted by end users. Interpretations are necessary in applications such as medical diagnosis, fraud detection and object recognition where valid reasons would be significantly helpful, if not necessary, before taking actions based on predictions. This workshop is about interpreting the prediction mechanisms or results of the complex computational models for data mining by taking advantage of simple models which are easier to understand. We wish to exchange ideas on recent approaches to the challenges of model interpretability, identify emerging fields of applications for such techniques, and provide opportunities for relevant interdisciplinary research or projects.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2565–2566},
numpages = {2},
keywords = {interpretability, data mining, deep models, shallow models, machine learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3133199,
author = {Sinha, Manjira and He, Xiangnan and Bozzon, Alessandro and Mannarswamy, Sandya and Murukannaiah, Pradeep and Mukherjee, Tridib},
title = {SMASC 2017: First International Workshop on Social Media Analytics for Smart Cities},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133199},
doi = {10.1145/3132847.3133199},
abstract = {In an increasingly digital urban setting, connected &amp; concerned Citizens typically voice their opinions on various civic topics via social media. Efficient and scalable analysis of these citizen voices on social media to derive actionable insights is essential to the development of smart cities. The very nature of the data: heterogeneity and dynamism, the scarcity of gold standard annotated corpora, and the need for multi-dimensional analysis across space, time and semantics, makes urban social media analytics challenging. This workshop is dedicated to the theme of social media analytics for smart cities, with the aim of focusing the interest of CIKM research community on the challenges in mining social media data for urban informatics. The workshop hopes to foster collaboration between researchers working in information retrieval, social media analytics, linguistics; social scientists, and civic authorities, to develop scalable and practical systems for capturing and acting upon real world issues of cities as voiced by their citizens in social media. The aim of this workshop is to encourage researchers to develop techniques for urban analytics of social media data, with specific focus on applying these techniques to practical urban informatics applications for smart cities.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2567–2568},
numpages = {2},
keywords = {social media analytics, text mining, urban informatics},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3132847.3152359,
author = {Winslett, Marianne},
title = {Additional Workshops Co-Located with CIKM 2017},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3152359},
doi = {10.1145/3132847.3152359},
abstract = {Summary of three workshops co-located with CIKM 2017.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2569–2570},
numpages = {2},
keywords = {cikm 2017 workshops},
location = {Singapore, Singapore},
series = {CIKM '17}
}

