@inproceedings{10.1145/1183614.1183616,
author = {Garcia-Molina, Hector},
title = {Pair-Wise Entity Resolution: Overview and Challenges},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183616},
doi = {10.1145/1183614.1183616},
abstract = {Information integration is one of the oldest and most important computer science problems: Information from diverse sources must be combined, so that users can access and manipulate the information in a unified way. One of the central problems in information integration is that of Entity Resolution (ER) (sometimes referred to as deduplication). ER is the process of identifying and merging incoming records judged to represent the same real-world entity.For example, consider a company that has different customer databases (e.g., one for each subsidiary), and would like to integrate them. Identifying matching records is challenging because there are no unique identifiers across the different sources or databases. A given customer may appear in different ways in each database, and there is a fair amount of guesswork in determining which customers match. Deciding if records match is often computationally expensive, e.g., may involve finding maximal common subsequences in two strings. How to combine matching records is often also application dependent. For example, say different phone numbers appear in two records to be merged. In some cases we may wish to keep both of them, while in others we may want to pick just one as the "consolidated" number.Another source of complexity is that newly merged records may match with other records. For instance, when we combine records r1 and r2 we may obtain a record r12 that now matches r3. The original records, r1 and r2, may not match with r3, but because r12 contains more information about the same real-word entity that r1 and r2 represent, the "connection" to r3 may now be apparent. Such "chained" matches imply that new merged records must be recursively compared to all records.There are many ways to perform ER, but in this talk I will explore only one general approach, where the decision of what records represent the same real-world entity is done in a pair-wise fashion. Furthermore, we assume that the matching is done by a "black-box" function, which makes our approach generic and applicable to many domains. Thus, given two records, r1 and r2, the match function M(r1, r2) returns true if there is enough evidence in the two records that they both refer to the same real-world entity. We also assume a black-box merge function that combines a pair of matching records.In this talk I will discuss the advantages and disadvantages of such a generic, pair-wise approach to ER. And even though the approach is relatively simple, there are still many interesting challenges. For instance, how can one minimize the number of invocations to the match and merge black-boxes? Are there any properties of the functions that can significantly reduce the number of calls? If one has available multiple processors, how can one distribute the computational load? If records have confidences associated with them, how does the problem complexity change, and how can we efficiently find the confidence of the resolved records? In the talk I will address these challenges, and report on some preliminary work we have done at Stanford. (This Stanford work in joint with Omar Benjelloun, Tyson Condie, Johnson (Heng) Gong, Jeff Jonas, Hideki Kawai, Tait E. Larson, David Menestrina, Nicolas Pombourcq, Qi Su, Steven Whang, Jennifer Widom.For additional information on ER and our Stanford SERF Project, please visit http://www-db.stanford.edu/serf/.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {1},
numpages = {1},
keywords = {entity resolution, data cleaning},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183617,
author = {Flake, Gary William},
title = {How I Learned to Stop Worrying and Love the Imminent Internet Singularity},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183617},
doi = {10.1145/1183614.1183617},
abstract = {In 1993, Verner Vinge [3] introduced the notion of the Singularity -- a step function to nearly unlimited technological capability -- which would be realized if the acceleration of scientific progress continues to produce such things as strong AI, nanotechnology, and super-human intelligence. Since its introduction, the idea of the Singularity has been met with both evangelism (by Ray Kurzweil [2]) and apocalyptic warnings (by Bill Joy [1]).In this talk, I will introduce a more modest version of the idea, which I call the Internet Singularity. Like the original, the Internet Singularity suggests continued acceleration of progress, but makes greater emphasis on our ability to improve science, analytic methods, and engineering on data as opposed to the physical world. I make the case for the Internet Singularity in four steps.First, there is a general trend of more capabilities being more available to more people. These increasing capabilities span content creation, community, and commerce, yielding more power to today's "amateur" than yesterday's "professional". As a result, the boundary between producers and consumers is becoming increasingly blurred over time.Second, in many parts of the Internet we see power law distributions with a heavy tail. One implication of heavy tail distributions is that the aggregate impact of "small" participants can be greater than that of the "large" participants.Third, with the Internet comes entirely new means for authoring new and derivative works: aggregations, mashups, tagging, remixing, etc. The greater emphasis on collaboration and sharing yields direct and indirect network effects. Network effects, can produce entirely new utility, making online activities potentially more efficient or valuable than the offline equivalent.Fourth, on the Internet, advances are effectively decoupled from the physical constraints of the offline world: startups costs are smaller; customer, collaborator, and audience pools are dramatically larger; and improvements happen in more of a continuous rather than discreet manner. As a result the effective "clock cycle" of progress is potentially much faster online.Putting these four pieces together reveals a compelling pattern: more people contribute to the collective pool; the collective pool contains entirely new value that is derived from its data; and the new value from the data increases individual and aggregate capabilities. In combination, these components mutually reinforce one another, forming something of a virtuous cycle. This is the Internet Singularity.Conceptually, if we consider engineering to be the ability to create artifacts, mathematical analysis to be the ability to analyze numerical properties, and science to be the pursuit of knowledge, then each of these activities -- when focused on digital objects as they exist on the Internet -- can be amplified in a manner consistent with the Internet Singularity.The implications for the Internet Singularity are profound as they suggest nothing less than the evolution of the scientific method itself. Moreover, these trends also imply that now may be the best possible moment in the history of the universe to be a computer scientist.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {2},
numpages = {1},
keywords = {technological singularity, internet, scientific computing},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183618,
author = {Kielman, Joseph},
title = {The Real-Time Nature and Value of Homeland Security Information},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183618},
doi = {10.1145/1183614.1183618},
abstract = {Ensuring the security of our homeland depends in large measure on two quite distinct factors: having the knowledge necessary to prevent, predict, prepare for, or respond, if necessary, to any manner of terrorist attack or a natural or manmade disaster and collaborating or sharing knowledge with a broad range of international, federal, state, local, and tribal agencies, as well as other private or public organizations. The essential problem with adequately addressing these factors, despite the many advancements made in the past decade, is twofold. First, it is not so much the mass but rather the diffuse nature and complexity of the data, information, and knowledge required for understanding terrorism and accounting for the manifold consequences of disasters that make possession of the right knowledge difficult. And, second, that diffuseness and complexity is magnified by the extreme diversity and wide distribution of the many potential homeland security collaborators. Retrospective analysis, and even knowledge discovery, is less useful under these conditions than prospective, real-time synthesis of information for multiple users. Also, privacy is as important as, if not more important than, security. This suggests that database designs and techniques for information retrieval, and knowledge management must take advantage of such technologies as semantic nets, visualization, and discrete mathematics to build knowledge systems capable for homeland security applications.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {3},
numpages = {1},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183620,
author = {Traina, Caetano and Traina, Agma J. M. and Vieira, Marcos R. and Arantes, Adriano S. and Faloutsos, Christos},
title = {Efficient Processing of Complex Similarity Queries in RDBMS through Query Rewriting},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183620},
doi = {10.1145/1183614.1183620},
abstract = {Multimedia and complex data are usually queried by similarity predicates. Whereas there are many works dealing with algorithms to answer basic similarity predicates, there are not generic algorithms able to efficiently handle similarity complex queries combining several basic similarity predicates. In this work we propose a simple and effective set of algorithms that can be combined to answer complex similarity queries, and a set of algebraic rules useful to rewrite similarity query expressions into an adequate format for those algorithms. Those rules and algorithms allow relational database management systems to turn complex queries into efficient query execution plans. We present experiments that highlight interesting scenarios. They show that the proposed algorithms are orders of magnitude faster than the traditional similarity algorithms. Moreover, they are linearly scalable considering the database size.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {4–13},
numpages = {10},
keywords = {similarity predicates, query rewriting},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183621,
author = {Zeinalipour-Yazti, Demetrios and Lin, Song and Gunopulos, Dimitrios},
title = {Distributed Spatio-Temporal Similarity Search},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183621},
doi = {10.1145/1183614.1183621},
abstract = {In this paper we introduce the distributed spatio-temporal similarity search problem: given a query trajectory Q, we want to find the trajectories that follow a motion similar to Q, when each of the target trajectories is segmented across a number of distributed nodes. We propose two novel algorithms, UB-K and UBLB-K, which combine local computations of lower and upper bounds on the matching between the distributed subsequences and Q. Such an operation generates the desired result without pulling together all the distributed subsequences over the fundamentally expensive communication medium. Our solutions find applications in a wide array of domains, such as cellular networks, wild life monitoring and video surveillance. Our experimental evaluation using realistic data demonstrates that our framework is both efficient and robust to a variety of conditions.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {14–23},
numpages = {10},
keywords = {spatio-temporal similarity search, top-K query processing},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183622,
author = {Marsolo, Keith and Parthasarathy, Srinivasan and Ramamohanarao, Kotagiri},
title = {Structure-Based Querying of Proteins Using Wavelets},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183622},
doi = {10.1145/1183614.1183622},
abstract = {The ability to retrieve molecules based on structural similarity has use in many applications, from disease diagnosis and treatment to drug discovery and design. In this paper, we present a method to represent protein molecules that allows for the fast, flexible and efficient retrieval of similar structures, based on either global or local attributes. We begin by computing the pair-wise distance between amino acids, transforming each 3D structure into a 2D distance matrix. We normalize this matrix to a specific size and apply a 2D wavelet decomposition to generate a set of approximation coefficients, which serves as our global feature vector. This transformation reduces the overall dimensionality of the data while still preserving spatial features and correlations. We test our method by running queries on three different protein data sets that have been used previously in the literature, basing our comparisons on labels taken from the SCOP database. We find that our method significantly outperforms existing approaches, in terms of retrieval accuracy, memory utilization and execution time. Specifically, using a k-d tree and running a 10-nearest-neighbor search on a dataset of 33,000 proteins against itself, we see an average accuracy of 89% at the SCOP SuperFamily level and a total query time that is up to 350 times faster than previously published techniques. In addition to processing queries based on global similarity, we also propose innovative extensions to effectively match proteins based solely on shared local substructures, allowing for a more flexible query interface.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {24–33},
numpages = {10},
keywords = {wavelets, protein structure similarity, indexing},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183623,
author = {Takasu, Atsuhiro},
title = {An Approximate Multi-Word Matching Algorithm for Robust Document Retrieval},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183623},
doi = {10.1145/1183614.1183623},
abstract = {Document generation from low level data and its utilization is one of the most challenging tasks in document engineering. Word occurrence detection is a fundamental problem in the recognized document utilization obtained by a recognizer, such as OCR and speech recognition. Given a set of words, such as a dictionary, this paper proposes an efficient dynamic programming (DP) algorithm to find the occurrences of each word in a text. In this paper, the string similarity is measured by a statistical similarity model that enables a definition of the similarities in the character level as well as edit operation level. The proposed algorithm uses tree structures to measure similarities in order to avoid measuring similarities of the same substrings appearing in different parts of the text and words. The time complexity of the proposed algorithm is O(|W|⋅|S|⋅|Q|), where |W| (resp. |S|) denote the number of nodes in the trees representing the word set (resp. the text), and |Q| donotes the number of the states of the model used for string similarity. This paper shows the proposed algorithm is experimentally about six times faster than a naive DP algorithm.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {34–42},
numpages = {9},
keywords = {dynamic programming, suffix tree, statistical model},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183625,
author = {Zhuang, Li and Jing, Feng and Zhu, Xiao-Yan},
title = {Movie Review Mining and Summarization},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183625},
doi = {10.1145/1183614.1183625},
abstract = {With the flourish of the Web, online review is becoming a more and more useful and important information resource for people. As a result, automatic review mining and summarization has become a hot research topic recently. Different from traditional text summarization, review mining and summarization aims at extracting the features on which the reviewers express their opinions and determining whether the opinions are positive or negative. In this paper, we focus on a specific domain - movie review. A multi-knowledge based approach is proposed, which integrates WordNet, statistical analysis and movie knowledge. The experimental results show the effectiveness of the proposed approach in movie review mining and summarization.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {43–50},
numpages = {8},
keywords = {summarization, review mining},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183626,
author = {Zhang, Zhu and Varadarajan, Balaji},
title = {Utility Scoring of Product Reviews},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183626},
doi = {10.1145/1183614.1183626},
abstract = {We identify a new task in the ongoing research in text sentiment analysis: predicting utility of product reviews, which is orthogonal to polarity classification and opinion extraction. We build regression models by incorporating a diverse set of features, and achieve highly competitive performance for utility scoring on three real-world data sets.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {51–57},
numpages = {7},
keywords = {regression, text sentiment analysis, utility},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183627,
author = {Qamra, Arun and Tseng, Belle and Chang, Edward Y.},
title = {Mining Blog Stories Using Community-Based and Temporal Clustering},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183627},
doi = {10.1145/1183614.1183627},
abstract = {In recent years, weblogs, or blogs for short, have become an important form of online content. The personal nature of blogs, online interactions between bloggers, and the temporal nature of blog entries, differentiate blogs from other kinds of Web content. Bloggers interact with each other by linking to each other's posts, thus forming online communities. Within these communities, bloggers engage in discussions of certain issues, through entries in their blogs. Since these discussions are often initiated in response to online or offline events, a discussion typically lasts for a limited time duration. We wish to extract such temporal discussions, or stories, occurring within blogger communities, based on some query keywords. We propose a Content-Community-Time model that can leverage the content of entries, their timestamps, and the community structure of the blogs, to automatically discover stories. Doing so also allows us to discover hot stories. We demonstrate the effectiveness of our model through several case studies using real-world data collected from the blogosphere.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {58–67},
numpages = {10},
keywords = {online-communities, weblogs, time-sensitive clustering},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183628,
author = {Chi, Yun and Tseng, Belle L. and Tatemura, Junichi},
title = {Eigen-Trend: Trend Analysis in the Blogosphere Based on Singular Value Decompositions},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183628},
doi = {10.1145/1183614.1183628},
abstract = {The blogosphere - the totality of blog-related Web sites - has become a great source of trend analysis in areas such as product survey, customer relationship, and marketing. Existing approaches are based on simple counts, such as the number of entries or the number of links. In this paper, we introduce a novel concept, coined eigen-trend, to represent the temporal trend in a group of blogs with common interests and propose two new techniques for extracting eigen-trends in blogs. First, we propose a trend analysis technique based on the singular value decomposition. Extracted eigen-trends provide new insights into multiple trends on the same keyword. Second, we propose another trend analysis technique based on a higher-order singular value decomposition. This analyzes the blogosphere as a dynamic graph structure and extracts eigen-trends that reflect the structural changes of the blogosphere over time. Experimental studies based on synthetic data sets and a real blog data set show that our new techniques can reveal a lot of interesting trend information and insights in the blogosphere that are not obtainable from traditional count-based methods.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {68–77},
numpages = {10},
keywords = {blog, higher-order singular value decomposition, blogosphere, singular value ecomposition, trend analysis},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183630,
author = {Robertson, Stephen},
title = {On GMAP: And Other Transformations},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183630},
doi = {10.1145/1183614.1183630},
abstract = {As an alternative to the usual Mean Average Precision, some use is currently being made of the Geometric Mean Average Precision (GMAP) as a measure of average search effectiveness across topics. GMAP is specifically used to emphasise the lower end of the average precision scale, in order to shed light on poor performance of search engines. This paper discusses the status of this measure and how it should be understood.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {78–83},
numpages = {6},
keywords = {effectiveness measures, evaluation},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183631,
author = {Ogilvie, Paul and Lalmas, Mounia},
title = {Investigating the Exhaustivity Dimension in Content-Oriented XML Element Retrieval Evaluation},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183631},
doi = {10.1145/1183614.1183631},
abstract = {INEX, the evaluation initiative for content-oriented XML retrieval, has since its establishment defined the relevance of an element according to two graded dimensions, exhaustivity and specificity. The former measures how exhaustively an XML element discusses the topic of request, whereas specificity measures how focused the element is on the topic of request. The reason for having two dimensions was to provide a more stable measure of relevance than if assessors were asked to rate the relevance of an element on a single scale. However, obtaining relevance assessments is a costly task. as each document must be assessed for relevance by a human assessor. In XML retrieval this problem is exacerbated as the elements of the document must also be assessed with respect to the exhaustivity and specificity dimensions. A continuous discussion in INEX has been whether such a sophisticated definition of relevance, and in particular the exhaustivity dimension, was needed. This paper attempts to answer this question through extensive statistical tests to compare the conclusions about system performance that could be made under different assessment scenarios.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {84–93},
numpages = {10},
keywords = {relevance, XML evaluation, INEX, statistical tests},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183632,
author = {Thomas, Paul and Hawking, David},
title = {Evaluation by Comparing Result Sets in Context},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183632},
doi = {10.1145/1183614.1183632},
abstract = {Familiar evaluation methodologies for information retrieval (IR) are not well suited to the task of comparing systems in many real settings. These systems and evaluation methods must support contextual, interactive retrieval over changing, heterogeneous data collections, including private and confidential information.We have implemented a comparison tool which can be inserted into the natural IR process. It provides a familiar search interface, presents a small number of result sets in side-by-side panels, elicits searcher judgments, and logs interaction events. The tool permits study of real information needs as they occur, uses the documents actually available at the time of the search, and records judgments taking into account the instantaneous needs of the searcher.We have validated our proposed evaluation approach and explored potential biases by comparing different whole-of-Web search facilities using a Web-based version of the tool. In four experiments, one with supplied queries in the laboratory and three with real queries in the workplace, subjects showed no discernable left-right bias and were able to reliably distinguish between high- and low-quality result sets. We found that judgments were strongly predicted by simple implicit measures.Following validation we undertook a case study comparing two leading whole-of-Web search engines. The approach is now being used in several ongoing investigations.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {94–101},
numpages = {8},
keywords = {embedded comparisons, evaluation},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183633,
author = {Yilmaz, Emine and Aslam, Javed A.},
title = {Estimating Average Precision with Incomplete and Imperfect Judgments},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183633},
doi = {10.1145/1183614.1183633},
abstract = {We consider the problem of evaluating retrieval systems using incomplete judgment information. Buckley and Voorhees recently demonstrated that retrieval systems can be efficiently and effectively evaluated using incomplete judgments via the bpref measure [6]. When relevance judgments are complete, the value of bpref is an approximation to the value of average precision using complete judgments. However, when relevance judgments are incomplete, the value of bpref deviates from this value, though it continues to rank systems in a manner similar to average precision evaluated with a complete judgment set. In this work, we propose three evaluation measures that (1) are approximations to average precision even when the relevance judgments are incomplete and (2) are more robust to incomplete or imperfect relevance judgments than bpref. The proposed estimates of average precision are simple and accurate, and we demonstrate the utility of these estimates using TREC data.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {102–111},
numpages = {10},
keywords = {Bpref, average precision, evaluation, incomplete judgments, sampling},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183635,
author = {Ojewole, Adegoke and Zhu, Qiang and Hou, Wen-Chi},
title = {Window Join Approximation over Data Streams with Importance Semantics},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183635},
doi = {10.1145/1183614.1183635},
abstract = {Load shedding techniques generate approximate sliding window join results when memory constraints prevent exact computation. The previously proposed random load shedding method drops input tuples without consideration for the number of outputs created, while the recently proposed semantic load shedding technique aims to produce the largest possible result set. We consider a new model in which data stream tuples contain numerical importance values relevant to the query source and seek to maximize the importance of the approximate join result. We show that both random load shedding and semantic load shedding are sub-optimal in this situation, while the techniques presented in this paper satisfy the objective function by considering both tuple importance and join attribute distributions. We extend the existing offline semantic approximation technique to make it compatible with our objective function and show that it is less space and time efficient than our new optimal offline algorithm for small and large join memory allotments. We also introduce four efficient online algorithms, which are quite promising in maximizing the importance of the approximate join result without foreknowledge of input streams.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {112–121},
numpages = {10},
keywords = {data streams, sliding window join, importance semantics, load shedding, approximation algorithms},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183636,
author = {Jain, Ankur and Zhang, Zhihua and Chang, Edward Y.},
title = {Adaptive Non-Linear Clustering in Data Streams},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183636},
doi = {10.1145/1183614.1183636},
abstract = {Data stream clustering has emerged as a challenging and interesting problem over the past few years. Due to the evolving nature, and one-pass restriction imposed by the data stream model, traditional clustering algorithms are inapplicable for stream clustering. This problem becomes even more challenging when the data is high-dimensional and the clusters are not linearly separable in the input space. In this paper, we propose a nonlinear stream clustering algorithm that adapts to the stream's evolutionary changes. Using the kernel methods for dealing with the non-linearity of data separation, we propose a novel 2-tier stream clustering architecture. Tier-1 captures the temporal locality in the stream, by partitioning it into segments, using a kernel-based novelty detection approach. Tier-2 exploits this segment structure to continuously project the streaming data nonlinearly onto a low-dimensional space (LDS), before assigning them to a cluster. We demonstrate the effectiveness of our approach through extensive experimental evaluation on various real-world datasets.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {122–131},
numpages = {10},
keywords = {stream clustering, data streams, kernel methods, stream mining, dimension reduction},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183637,
author = {Xu, Yabo and Wang, Ke and Fu, Ada Wai-Chee and She, Rong and Pei, Jian},
title = {Classification Spanning Correlated Data Streams},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183637},
doi = {10.1145/1183614.1183637},
abstract = {In many applications, classifiers need to be built based on multiple related data streams. For example, stock streams and news streams are related, where the classification patterns may involve features from both streams. Thus instead of mining on a single isolated stream, we need to examine multiple related data streams in order to find such patterns and build an accurate classifier. Other examples of related streams include traffic reports and car accidents, sensor readings of different types or at different locations, etc. In this paper, we consider the classification problem defined over sliding-window join of several input data streams. As the data streams arrive in fast pace and the many-to-many join relationship blows up the data arrival rate even more, it is impractical to compute the join and then build the classifier each time the window slides forward. We present an efficient algorithm to build a Na\"{\i}ve Bayesian classifier in such context. Our method does not need to perform the join operations but is still able to build exactly the same classifier as if built on the joined result. It only examines each input tuple twice, independent of the number of tuples it joins in other streams, therefore, is able to keep pace with the fast arriving data streams in the presence of many-to-many join relationships. The experiments confirmed that our classification algorithm is more efficient than conventional methods while maintaining good classification accuracy.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {132–141},
numpages = {10},
keywords = {classification, stream data, algorithm, Na\"{\i}ve Bayesian model, join},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183639,
author = {Couto, Francisco M. and Silva, M\'{a}rio J. and Coutinho, Pedro M.},
title = {Validating Associations in Biological Databases},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183639},
doi = {10.1145/1183614.1183639},
abstract = {Erroneous data can often be found in databases, and detecting it is normally a non-trivial task. For example, To cope with the large amount of biological sequences being produced, a significant number of genes and proteins have been annotated by automated tools. A protein annotation is an association between a protein and a term describing its role. These tools have produced a significant number of misannotations that are now present in biological databases. This paper proposes a new method for automatically scoring associations by comparing them to preexisting curated associations. An association is a pair that links two entities. The score can be used to filter incorrect or uncommon associations.We evaluated the method using the automated protein annotations submitted to BioCreAtIvE, an international evaluation of state-of-the-art text-mining systems in Biology. The method scored each of these annotations and those scored below a certain threshold were discarded. The results have shown a small trade-off in recall for a large improvement in precision. For example, we were able to discard 44.6%, 66.8% and 81% of the misannotations, maintaining 96.9%, 84.2%, and 47.8% of the correct annotations, respectively. Moreover, we were able to outperform each individual submission to BioCreAtIvE by proper adjustment of the threshold.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {142–151},
numpages = {10},
keywords = {filtering associations, biological databases, knowledge management},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183640,
author = {Zhang, Jian and Feigenbaum, Joan},
title = {Finding Highly Correlated Pairs Efficiently with Powerful Pruning},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183640},
doi = {10.1145/1183614.1183640},
abstract = {We consider the problem of finding highly correlated pairs in a large data set. That is, given a threshold not too small, we wish to report all the pairs of items (or binary attributes) whose (Pearson) correlation coefficients are greater than the threshold. Correlation analysis is an important step in many statistical and knowledge-discovery tasks. Normally, the number of highly correlated pairs is quite small compared to the total number of pairs. Identifying highly correlated pairs in a naive way by computing the correlation coefficients for all the pairs is wasteful. With massive data sets, where the total number of pairs may exceed the main-memory capacity, the computational cost of the naive method is prohibitive. In their KDD'04 paper [15], Hui Xiong et al. address this problem by proposing the TAPER algorithm. The algorithm goes through the data set in two passes. It uses the first pass to generate a set of candidate pairs whose correlation coefficients are then computed directly in the second pass. The efficiency of the algorithm depends greatly on the selectivity (pruning power) of its candidate-generating stage.In this work, we adopt the general framework of the TAPER algorithm but propose a different candidate-generation method. For a pair of items, TAPER's candidate-generation method considers only the frequencies (supports) of individual items. Our method also considers the frequency (support) of the pair but does not explicitly count this frequency (support). We give a simple randomized algorithm whose false-negative probability is negligible. The space and time complexities of generating the candidate set in our algorithm are asymptotically the same as TAPER's. We conduct experiments on synthesized and real data. The results show that our algorithm produces a greatly reduced candidate set - one that can be several orders of magnitude smaller than that generated by TAPER. Because of this, our algorithm uses much less memory and can be faster. The former is critical for dealing with massive data.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {152–161},
numpages = {10},
keywords = {correlation, massive data set, statistical computing},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183641,
author = {Gonzalez, Hector and Han, Jiawei and Li, Xiaolei},
title = {Mining Compressed Commodity Workflows from Massive RFID Data Sets},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183641},
doi = {10.1145/1183614.1183641},
abstract = {Radio Frequency Identification (RFID) technology is fast becoming a prevalent tool in tracking commodities in supply chain management applications. The movement of commodities through the supply chain forms a gigantic workflow that can be mined for the discovery of trends, flow correlations and outlier paths, that in turn can be valuable in understanding and optimizing business processes.In this paper, we propose a method to construct compressed probabilistic workflows that capture the movement trends and significant exceptions of the overall data sets, but with a size that is substantially smaller than that of the complete RFID workflow. Compression is achieved based on the following observations: (1) only a relatively small minority of items deviate from the general trend, (2)only truly non-redundant deviations, ie, those that substantially deviate from the previously recorded ones, are interesting, and (3) although RFID data is registered at the primitive level, data analysis usually takes place at a higher abstraction level. Techniques for workflow compression based on non-redundant transition and emission probabilities are derived; and an algorithm for computing approximate path probabilities is developed. Our experiments demonstrate the utility and feasibility of our design, data structure, and algorithms.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {162–171},
numpages = {10},
keywords = {workflow induction, RFID},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183643,
author = {Michel, Sebastian and Bender, Matthias and Ntarmos, Nikos and Triantafillou, Peter and Weikum, Gerhard and Zimmer, Christian},
title = {Discovering and Exploiting Keyword and Attribute-Value Co-Occurrences to Improve P2P Routing Indices},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183643},
doi = {10.1145/1183614.1183643},
abstract = {Peer-to-Peer (P2P) search requires intelligent decisions for query routing: selecting the best peers to which a given query, initiated at some peer, should be forwarded for retrieving additional search results. These decisions are based on statistical summaries for each peer, which are usually organized on a per-keyword basis and managed in a distributed directory of routing indices. Such architectures disregard the possible correlations among keywords. Together with the coarse granularity of per-peer summaries, which are mandated for scalability, this limitation may lead to poor search result quality.This paper develops and evaluates two solutions to this problem, sk-STAT based on single-key statistics only, and mk-STAT based on additional multi-key statistics. For both cases, hash sketch synopses are used to compactly represent a peer's data items and are efficiently disseminated in the P2P network to form a decentralized directory. Experimental studies with Gnutella and Web data demonstrate the viability and the trade-offs of the approaches.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {172–181},
numpages = {10},
keywords = {key co-occurrences, distributed IR, query routing, peer-to-peer information systems},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183644,
author = {B\"{u}ttcher, Stefan and Clarke, Charles L. A.},
title = {A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183644},
doi = {10.1145/1183614.1183644},
abstract = {We present a static index pruning method, to be used in ad-hoc document retrieval tasks, that follows a document-centric approach to decide whether a posting for a given term should remain in the index or not. The decision is made based on the term's contribution to the document's Kullback-Leibler divergence from the text collection's global language model. Our technique can be used to decrease the size of the index by over 90%, at only a minor decrease in retrieval effectiveness. It thus allows us to make the index small enough to fit entirely into the main memory of a single PC, even for large text collections containing millions of documents. This results in great efficiency gains, superior to those of earlier pruning methods, and an average response time around 20 ms on the GOV2 document collection.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {182–189},
numpages = {8},
keywords = {information retrieval, KL divergence, index pruning},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183645,
author = {Anh, Vo Ngoc and Moffat, Alistair},
title = {Pruning Strategies for Mixed-Mode Querying},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183645},
doi = {10.1145/1183614.1183645},
abstract = {Web information retrieval systems face a range of unique challenges, not the least of which is the sheer scale of the data that must be handled. Also specific to web retrieval is that queries may be a mix of Boolean and ranked features, and documents may have static score components that must also be factored into the ranking process. In this paper we consider a range of query semantics used in web retrieval systems, and show that impact-sorted indexes provide support for dynamic pruning mechanisms and in doing so allow fast document-at-a-time resolution of typical mixed-mode queries, even on relatively large volumes of data. Our techniques also extend to more complex query semantics, including the use of phrase, proximity, and structural constraints.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {190–197},
numpages = {8},
keywords = {web querying, text indexing, index compression},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183647,
author = {A\ss{}falg, Johannes and Borgwardt, Karsten M. and Kriegel, Hans-Peter},
title = {3DString: A Feature String Kernel for 3D Object Classification on Voxelized Data},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183647},
doi = {10.1145/1183614.1183647},
abstract = {Classification of 3D objects remains an important task in many areas of data management such as engineering, medicine or biology. As a common preprocessing step in current approaches to classification of voxelized 3D objects, voxel representations are transformed into a feature vector description.In this article, we introduce an approach of transforming 3D objects into feature strings which represent the distribution of voxels over the voxel grid. Attractively, this feature string extraction can be performed in linear runtime with respect to the number of voxels. We define a similarity measure on these feature strings that counts common k-mers in two input strings, which is referred to as the spectrum kernel in the field of kernel methods. We prove that on our feature strings, this similarity measure can be computed in time linear to the number of different characters in these strings. This linear runtime behavior makes our kernel attractive even for large datasets that occur in many application domains. Furthermore, we explain that our similarity measure induces a metric which allows to combine it with an M-tree for handling of large volumes of data. Classification experiments on two published benchmark datasets show that our novel approach is competitive with the best state-of-the-art methods for 3D object classification.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {198–207},
numpages = {10},
keywords = {3D object classification, string kernel},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183648,
author = {Anagnostopoulos, Aris and Broder, Andrei Z. and Punera, Kunal},
title = {Effective and Efficient Classification on a Search-Engine Model},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183648},
doi = {10.1145/1183614.1183648},
abstract = {Traditional document classification frameworks, which apply the learned classifier to each document in a corpus one by one, are infeasible for extremely large document corpora, like the Web or large corporate intranets. We consider the classification problem on a corpus that has been processed primarily for the purpose of searching, and thus our access to documents is solely through the inverted index of a large scale search engine. Our main goal is to build the "best" short query that characterizes a document class using operators normally available within large engines. We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. Moreover, we show that optimizing the efficiency of query execution by careful selection of these terms can further reduce the query costs. More precisely, we show that on our set-up the best 10 terms query canachieve 90% of the accuracy of the best SVM classifier (14000 terms), and if we are willing to tolerate a reduction to 86% of the best SVM, we can build a 10 terms query that can be executed more than twice as fast as the best 10 terms query.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {208–217},
numpages = {10},
keywords = {query efficiency, search engine, WAND, feature selection, text classification},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183649,
author = {Veloso, Adriano and Meira, Wagner and Cristo, Marco and Gon\c{c}alves, Marcos and Zaki, Mohammed},
title = {Multi-Evidence, Multi-Criteria, Lazy Associative Document Classification},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183649},
doi = {10.1145/1183614.1183649},
abstract = {We present a novel approach for classifying documents that combines different pieces of evidence (e.g., textual features of documents, links, and citations) transparently, through a data mining technique which generates rules associating these pieces of evidence to predefined classes. These rules can contain any number and mixture of the available evidence and are associated with several quality criteria which can be used in conjunction to choose the "best" rule to be applied at classification time. Our method is able to perform evidence enhancement by link forwarding/backwarding (i.e., navigating among documents related through citation), so that new pieces of link-based evidence are derived when necessary. Furthermore, instead of inducing a single model (or rule set) that is good on average for all predictions, the proposed approach employs a lazy method which delays the inductive process until a document is given for classification, therefore taking advantage of better qualitative evidence coming from the document. We conducted a systematic evaluation of the proposed approach using documents from the ACM Digital Library and from a Brazilian Web directory. Our approach was able to outperform in both collections all classifiers based on the best available evidence in isolation as well as state-of-the-art multi-evidence classifiers. We also evaluated our approach using the standard WebKB collection, where our approach showed gains of 1% in accuracy, being 25 times faster. Further, our approach is extremely efficient in terms of computational performance, showing gains of more than one order of magnitude when compared against other multi-evidence classifiers.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {218–227},
numpages = {10},
keywords = {classification, data mining, lazy algorithms},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183650,
author = {Qi, Xiaoguang and Davison, Brian D.},
title = {Knowing a Web Page by the Company It Keeps},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183650},
doi = {10.1145/1183614.1183650},
abstract = {Web page classification is important to many tasks in information retrieval and web mining. However, applying traditional textual classifiers on web data often produces unsatisfying results. Fortunately, hyperlink information provides important clues to the categorization of a web page. In this paper, an improved method is proposed to enhance web page classification by utilizing the class information from neighboring pages in the link graph. The categories represented by four kinds of neighbors (parents, children, siblings and spouses) are combined to help with the page in question. In experiments to study the effect of these factors on our algorithm, we find that the method proposed is able to boost the classification accuracy of common textual classifiers from around 70% to more than 90% on a large dataset of pages from the Open Directory Project, and outperforms existing algorithms. Unlike prior techniques, our approach utilizes same-host links and can improve classification accuracy even when neighboring pages are unlabeled. Finally, while all neighbor types can contribute, sibling pages are found to be the most important.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {228–237},
numpages = {10},
keywords = {rainbow, web page classification, SVM, neighboring},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183652,
author = {Li, Xiaoyan and Croft, W. Bruce},
title = {Improving Novelty Detection for General Topics Using Sentence Level Information Patterns},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183652},
doi = {10.1145/1183614.1183652},
abstract = {The detection of new information in a document stream is an important component of many potential applications. In this work, a new novelty detection approach based on the identification of sentence level information patterns is proposed. First, the information-pattern concept for novelty detection is presented with the emphasis on new information patterns for general topics (queries) that cannot be simply turned into specific questions whose answers are specific named entities (NEs). Then we elaborate a thorough analysis of sentence level information patterns on data from the TREC novelty tracks, including sentence lengths, named entities, sentence level opinion patterns. This analysis provides guidelines in applying those patterns in novelty detection particularly for the general topics. Finally, a unified pattern-based approach is presented to novelty detection for both general and specific topics. The new method for dealing with general topics will be the focus. Experimental results show that the proposed approach significantly improves the performance of novelty detection for general topics as well as the overall performance for all topics from the 2002-2004 TREC novelty tracks.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {238–247},
numpages = {10},
keywords = {novelty detection, information patterns, named entities},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183653,
author = {Zhou, Ding and Ji, Xiang and Zha, Hongyuan and Giles, C. Lee},
title = {Topic Evolution and Social Interactions: How Authors Effect Research},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183653},
doi = {10.1145/1183614.1183653},
abstract = {We propose a method for discovering the dependency relationships between the topics of documents shared in social networks using the latent social interactions, attempting to answer the question: given a seemingly new topic, from where does this topic evolve? In particular, we seek to discover the pair-wise probabilistic dependency in topics of documents which associate social actors from a latent social network, where these documents are being shared. By viewing the evolution of topics as a Markov chain, we estimate a Markov transition matrix of topics by leveraging social interactions and topic semantics. Metastable states in a Markov chain are applied to the clustering of topics. Applied to the CiteSeer dataset, a collection of documents in academia, we show the trends of research topics, how research topics are related and which are stable. We also show how certain social actors, authors, impact these topics and propose new ways for evaluating author impact.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {248–257},
numpages = {10},
keywords = {markov chains, social network analysis, text data mining, clustering},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183654,
author = {Vieira, Karane and da Silva, Altigran S. and Pinto, Nick and de Moura, Edleno S. and Cavalcanti, Jo\~{a}o M. B. and Freire, Juliana},
title = {A Fast and Robust Method for Web Page Template Detection and Removal},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183654},
doi = {10.1145/1183614.1183654},
abstract = {The widespread use of templates on the Web is considered harmful for two main reasons. Not only do they compromise the relevance judgment of many web IR and web mining methods such as clustering and classification, but they also negatively impact the performance and resource usage of tools that process web pages. In this paper we present a new method that efficiently and accurately removes templates found in collections of web pages. Our method works in two steps. First, the costly process of template detection is performed over a small set of sample pages. Then, the derived template is removed from the remaining pages in the collection. This leads to substantial performance gains when compared to previous approaches that combine template detection and removal. We show, through an experimental evaluation, that our approach is effective for identifying terms occurring in templates - obtaining F-measure values around 0.9, and that it also boosts the accuracy of web page clustering and classification methods.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {258–267},
numpages = {10},
keywords = {web page noise removal, web template extraction},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183655,
author = {Girju, Roxana},
title = {Out-of-Context Noun Phrase Semantic Interpretation with Cross-Linguistic Evidence},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183655},
doi = {10.1145/1183614.1183655},
abstract = {The acquisition of semantic knowledge is paramount for any application that requires a deep understanding of natural language text. Motivated by the problem of building a noun phrase-level semantic parser and adapting it to various applications, such as machine translation and multilingual question answering, in this paper we present a domain-independent model for noun phrase semantic interpretation. We investigate the problem based on cross-linguistic evidence from a set of four Romance languages: Spanish, Italian, French, and Romanian. The focus on Romance languages is well motivated. It is generally the case that English noun phrases translate into constructions of the form "N P N" in Romance languages where, as we will show, the P (preposition) varies in ways that correlate with the semantics. Thus, based on a set of 22 semantic interpretation categories (such as PART-WHOLE, AGENT, POSSESSION) we present empirical observations regarding the distribution of these semantic categories in a cross-lingual corpus and their mapping to various syntactic constructions in English and Romance. Furthermore, given a training set of English noun phrases along with their translations in the four Romance languages, our algorithm automatically learns classification rules and applies them to unseen noun phrase instances for semantic interpretation. Experimental results are compared against a state-of-the-art model reported in the literature.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {268–276},
numpages = {9},
keywords = {semantic relations, SVM, computational semantics, classification},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183657,
author = {Boydell, Ois\'{\i}n and Smyth, Barry},
title = {Capturing Community Search Expertise for Personalized Web Search Using Snippet-Indexes},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183657},
doi = {10.1145/1183614.1183657},
abstract = {We describe and evaluate an approach to capturing and re-using search expertise within a community of like minded searchers, such as the employees of a company or organisation. Within knowledge based industries, search expertise - the ability to quickly and accurately locate information according to a specific information need - is an important corporate asset and in our approach we attempt to capture this knowledge by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our assumption is that the snippet text of a result must play a role in helping users to judge the initial relevance of that result and that the snippet terms of selected results must contain especially informative terms about the goals and preferences of the searchers. In other words, results are selected because the user recognises certain combinations of terms in their snippets which are related to their information needs. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by some underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {277–286},
numpages = {10},
keywords = {snippet, personalization, web search, community},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183658,
author = {Chirita, Paul-Alexandru and Firan, Claudiu S. and Nejdl, Wolfgang},
title = {Summarizing Local Context to Personalize Global Web Search},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183658},
doi = {10.1145/1183614.1183658},
abstract = {The PC Desktop is a very rich repository of personal information, efficiently capturing user's interests. In this paper we propose a new approach towards an automatic personalization of web search in which the user specific information is extracted from such local desktops, thus allowing for an increased quality of user profiling, while sharing less private information with the search engine. More specifically, we investigate the opportunities to select personalized query expansion terms for web search using three different desktop oriented approaches: summarizing the entire desktop data, summarizing only the desktop documents relevant to each user query, and applying natural language processing techniques to extract dispersive lexical compounds from relevant desktop resources. Our experiments with the Google API showed at least the latter two techniques to produce a very strong improvement over current web search.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {287–296},
numpages = {10},
keywords = {personalized web search, relevance feedback, desktop summarization, user profile},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183659,
author = {White, Ryen W. and Kelly, Diane},
title = {A Study on the Effects of Personalization and Task Information on Implicit Feedback Performance},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183659},
doi = {10.1145/1183614.1183659},
abstract = {While Implicit Relevance Feedback (IRF) algorithms exploit users' interactions with information to customize support offered to users of search systems, it is unclear how individual and task differences impact the effectiveness of such algorithms. In this paper we describe a study on the effect on retrieval performance of using additional information about the user and their search tasks when developing IRF algorithms. We tested four algorithms that use document display time to estimate relevance, and tailored the threshold times (i.e., the time distinguishing relevance from non-relevance) to the task, the user, a combination of both, or neither. Interaction logs gathered during a longitudinal naturalistic study of online information-seeking behavior are used as stimuli for the algorithms. The findings show that tailoring display time thresholds based on task information improves IRF algorithm performance, but doing so based on user information worsens performance. This has implications for the development of effective IRF algorithms.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {297–306},
numpages = {10},
keywords = {implicit feedback, evaluation},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183660,
author = {Zha, Hongyuan and Zheng, Zhaohui and Fu, Haoying and Sun, Gordon},
title = {Incorporating Query Difference for Learning Retrieval Functions in World Wide Web Search},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183660},
doi = {10.1145/1183614.1183660},
abstract = {We discuss information retrieval methods that aim at serving a diverse stream of user queries such as those submitted to commercial search engines. We propose methods that emphasize the importance of taking into consideration of query difference in learning effective retrieval functions. We formulate the problem as a multi-task learning problem using a risk minimization framework. In particular, we show how to calibrate the empirical risk to incorporate query difference in terms of introducing nuisance parameters in the statistical models, and we also propose an alternating optimization method to simultaneously learn the retrieval function and the nuisance parameters. We work out the details for both L1 and L2 regularization cases, and provide convergence analysis for the alternating optimization method for the special case when the retrieval functions belong to a reproducing kernel Hilbert space. We illustrate the effectiveness of the proposed methods using modeling data extracted from a commercial search engine. We also point out how the current framework can be extended in future research.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {307–316},
numpages = {10},
keywords = {query specific feature, regularization, WWW search, relevance, alternating optiminization, relevance judgment, discounted cumulative gain, retrieval function, gradient boosting, least-squares regression, machine learning, risk minimization, quadratic programming, query dependence, query document feature},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183662,
author = {Aly, Mohamed and Pruhs, Kirk and Chrysanthis, Panos K.},
title = {KDDCS: A Load-Balanced in-Network Data-Centric Storage Scheme for Sensor Networks},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183662},
doi = {10.1145/1183614.1183662},
abstract = {We propose an In-Network Data-Centric Storage (INDCS) scheme for answering ad-hoc queries in sensor networks. Previously proposed In-Network Storage (INS) schemes suffered from Storage Hot-Spots that are formed if either the sensors' locations are not uniformly distributed over the coverage area, or the distribution of sensor readings is not uniform over the range of possible reading values. Our K-D tree based Data-Centric Storage (KDDCS) scheme maintains the invariant that the storage of events is distributed reasonably uniformly among the sensors. KDDCS is composed of a set of distributed algorithms whose running time is within a poly-log factor of the diameter of the network. The number of messages any sensor has to send, as well as the bits in those messages, is poly-logarithmic in the number of sensors. Load balancing in KDDCS is based on defining and distributively solving a theoretical problem that we call the Weighted Split Median problem. In addition to analytical bounds on KDDCS individual algorithms, we provide experimental evidence of our scheme's general efficiency, as well as its ability to avoid the formation of storage hot-spots of various sizes, unlike all previous INDCS schemes.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {317–326},
numpages = {10},
keywords = {distributed algorithms, power-aware, sensor network},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183663,
author = {Hung, Hao-Ping and Chen, Ming-Syan},
title = {Efficient Range-Constrained Similarity Search on Wavelet Synopses over Multiple Streams},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183663},
doi = {10.1145/1183614.1183663},
abstract = {Due to the resource limitation in the data stream environment, it has been reported that answering user queries according to the wavelet synopsis of a stream is an essential ability of a Data Stream Management System (DSMS). In this paper, motivated by the fact that a user may be interested in an arbitrary range of the data streams, we investigate two important types of range-constrained queries in time series streaming environments: the distance queries (which aim at obtaining the Euclidean distance between two streams) and the kNN queries (which aim at discovering k nearest neighbors to a reference stream). To achieve high efficiency in processing these two types of queries, we propose procedure RED (standing for Range-constrained Euclidean Distance) and algorithm EKS (standing for Enhanced KNN Search). Compared to the existing methods in the prior research, the advantageous features of our approaches are in two folds. First, our approaches are capable of processing the queries directly from the wavelet synopses retained in the main memory without using IDWT to reconstruct the data cells. This feature allows us to save the complexity in both memory and time. Moreover, our approaches enable the users to query the DSMS within their range of interest. Unlike the conventional methods which only support the full-range query processing, this feature will enhance the flexibility at the client side. We evaluate procedure RED and algorithm EKS on live and synthetic datasets empirically and show that the proposed approaches are efficient in similarity search and kNN discovery within arbitrary ranges in the time series streaming environments.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {327–336},
numpages = {10},
keywords = {wavelet synopses, data stream, similarity search},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183664,
author = {Bai, Yijian and Thakkar, Hetal and Wang, Haixun and Luo, Chang and Zaniolo, Carlo},
title = {A Data Stream Language and System Designed for Power and Extensibility},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183664},
doi = {10.1145/1183614.1183664},
abstract = {By providing an integrated and optimized support for user-defined aggregates (UDAs), data stream management systems (DSMS) can achieve superior power and generality while preserving compatibility with current SQL standards. This is demonstrated by the Stream Mill system that, through is Expressive Stream Language (ESL), efficiently supports a wide range of applications - including very advanced ones such as data stream mining, streaming XML processing, time-series queries, and RFID event processing. ESL supports physical and logical windows (with optional slides and tumbles) on both built-in aggregates and UDAs, using a simple framework that applies uniformly to both aggregate functions written in an external procedural languages and those natively written in ESL. The constructs introduced in ESL extend the power and generality of DSMS, and are conducive to UDA-specific optimization and efficient execution as demonstrated by several experiments.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {337–346},
numpages = {10},
keywords = {data stream, query language, data stream management system},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183666,
author = {Goldin, Dina and Mardales, Ricardo and Nagy, George},
title = {In Search of Meaning for Time Series Subsequence Clustering: Matching Algorithms Based on a New Distance Measure},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183666},
doi = {10.1145/1183614.1183666},
abstract = {Recent papers have claimed that the result of K-means clustering for time series subsequences (STS clustering) is independent of the time series that created it. Our paper revisits this claim. In particular, we consider the following question: Given several time series sequences and a set of STS cluster centroids from one of them (generated by the K-means algorithm), is it possible to reliably determine which of the sequences produced these cluster centroids? While recent results suggest that the answer should be NO, we answer this question in the affirmative.We present cluster shape distance, an alternate distance measure for time series subsequence clusters, based on cluster shapes. Given a set of clusters, its shape is the sorted list of the pairwise Euclidean distances between their centroids. We then present two algorithms based on this distance measure, which match a set of STS cluster centroids with the time series that produced it. While the first algorithm creates DQG reuse this term more smaller "fingerprints" for the sequences, the second is more accurate. In our experiments with a dataset of 10 sequences, it produced a correct match 100% of the time.Furthermore, we offer an analysis that explains why our cluster shape distance provides a reliable way to match STS clusters to the original sequences, whereas cluster set distance fails to do so. Our work establishes for the first time a strong relation between the result of K-means STS clustering and the time series sequence that created it, despite earlier predictions that this is not possible.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {347–356},
numpages = {10},
keywords = {K-means subsequence clustering, cluster shape distance, meaningless},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183667,
author = {Sahoo, Nachiketa and Callan, Jamie and Krishnan, Ramayya and Duncan, George and Padman, Rema},
title = {Incremental Hierarchical Clustering of Text Documents},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183667},
doi = {10.1145/1183614.1183667},
abstract = {Incremental hierarchical text document clustering algorithms are important in organizing documents generated from streaming on-line sources, such as, Newswire and Blogs. However, this is a relatively unexplored area in the text document clustering literature. Popular incremental hierarchical clustering algorithms, namely Cobweb and Classit, have not been widely used with text document data. We discuss why, in the current form, these algorithms are not suitable for text clustering and propose an alternative formulation that includes changes to the underlying distributional assumption of the algorithm in order to conform with the data. Both the original Classit algorithm and our proposed algorithm are evaluated using Reuters newswire articles and Ohsumed dataset.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {357–366},
numpages = {10},
keywords = {incremental clustering, hierarchical clustering, text clustering},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183668,
author = {Yan, Hua and Chen, Keke and Liu, Ling},
title = {Efficiently Clustering Transactional Data with Weighted Coverage Density},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183668},
doi = {10.1145/1183614.1183668},
abstract = {It is widely recognized that developing efficient and fully automated algorithms for clustering large transactional datasets is a challenging problem. In this paper, we propose a fast, memory-efficient, and scalable clustering algorithm for analyzing transactional data. Our approach has three unique features. First, we use the concept of Weighted Coverage Density as a categorical similarity measure for efficient clustering of transactional datasets. The concept of weighted coverage density is intuitive and allows the weight of each item in a cluster to be changed dynamically according to the occurrences of items. Second, we develop two transactional data clustering specific evaluation metrics based on the concept of large transactional items and the coverage density respectively. Third, we implement the weighted coverage density clustering algorithm and the two clustering validation metrics using a fully automated transactional clustering framework, called SCALE (Sampling, Clustering structure Assessment, cLustering and domain-specific Evaluation). The SCALE framework is designed to combine the weighted coverage density measure for clustering over a sample dataset with self-configuring methods that can automatically tune the two important parameters of the clustering algorithms: (1) the candidates of the best number K of clusters; and (2) the application of two domain-specific cluster validity measures to find the best result from the set of clustering results. We have conducted experimental evaluation using both synthetic and real datasets and our results show that the weighted coverage density approach powered by the SCALE framework can efficiently generate high quality clustering results in a fully automated manner.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {367–376},
numpages = {10},
keywords = {SCALE, weighted coverage density, LISR, AMI},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183670,
author = {Chen, Le and Zhang, Lei and Jing, Feng and Deng, Ke-Feng and Ma, Wei-Ying},
title = {Ranking Web Objects from Multiple Communities},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183670},
doi = {10.1145/1183614.1183670},
abstract = {Vertical search is a promising direction as it leverages domain-specific knowledge and can provide more precise information for users. In this paper, we study the Web object-ranking problem, one of the key issues in building a vertical search engine. More specifically, we focus on this problem in cases when objects lack relationships between different Web communities, and take high-quality photo search as the test bed for this investigation. We proposed two score fusion methods that can automatically integrate as many Web communities (Web forums) with rating information as possible. The proposed fusion methods leverage the hidden links discovered by a duplicate photo detection algorithm, and aims at minimizing score differences of duplicate photos in different forums. Both intermediate results and user studies show the proposed fusion methods are practical and efficient solutions to Web object ranking in cases we have described. Though the experiments were conducted on high-quality photo ranking, the proposed algorithms are also applicable to other ranking problems, such as movie ranking and music ranking.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {377–386},
numpages = {10},
keywords = {ranking, web objects, image search},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183671,
author = {Macdonald, Craig and Ounis, Iadh},
title = {Voting for Candidates: Adapting Data Fusion Techniques for an Expert Search Task},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183671},
doi = {10.1145/1183614.1183671},
abstract = {In an expert search task, the users' need is to identify people who have relevant expertise to a topic of interest. An expert search system predicts and ranks the expertise of a set of candidate persons with respect to the users' query. In this paper, we propose a novel approach for predicting and ranking candidate expertise with respect to a query. We see the problem of ranking experts as a voting problem, which we model by adapting eleven data fusion techniques.We investigate the effectiveness of the voting approach and the associated data fusion techniques across a range of document weighting models, in the context of the TREC 2005 Enterprise track. The evaluation results show that the voting paradigm is very effective, without using any collection specific heuristics. Moreover, we show that improving the quality of the underlying document representation can significantly improve the retrieval performance of the data fusion techniques on an expert search task. In particular, we demonstrate that applying field-based weighting models improves the ranking of candidates. Finally, we demonstrate that the relative performance of the adapted data fusion techniques for the proposed approach is stable regardless of the used weighting models.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {387–396},
numpages = {10},
keywords = {expertise modelling, expert finding, ranking, data fusion, information retrieval, voting, expert search},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183672,
author = {Zigoris, Philip and Zhang, Yi},
title = {Bayesian Adaptive User Profiling with Explicit &amp; Implicit Feedback},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183672},
doi = {10.1145/1183614.1183672},
abstract = {Research in information retrieval is now moving into a personalized scenario where a retrieval or filtering system maintains a separate user profile for each user. In this framework, information delivered to the user can be automatically personalized and catered to individual user's information needs. However, a practical concern for such a personalized system is the "cold start problem": any user new to the system must endure poor initial performance until sufficient feedback from that user is provided.To solve this problem, we use both explicit and implicit feedback to build a user's profile and use Bayesian hierarchical methods to borrow information from existing users. We analyze the usefulness of implicit feedback and the adaptive performance of the model on two data sets gathered from user studies where users' interaction with a document, or implicit feedback, were recorded along with explicit feedback. Our results are two-fold: first, we demonstrate that the Bayesian modeling approach effectively trades off between shared and user-specific information, alleviating poor initial performance for each user. Second, we find that implicit feedback has very limited unstable predictive value by itself and only marginal value when combined with explicit feedback.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {397–404},
numpages = {8},
keywords = {Bayesian statistics, information retrieval, user modeling, implicit feedback},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183674,
author = {Bartolini, Ilaria and Ciaccia, Paolo and Patella, Marco},
title = {SaLSa: Computing the Skyline without Scanning the Whole Sky},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183674},
doi = {10.1145/1183614.1183674},
abstract = {Skyline queries compute the set of Pareto-optimal tuples in a relation, ie those tuples that are not dominated by any other tuple in the same relation. Although several algorithms have been proposed for efficiently evaluating skyline queries, they either require to extend the relational server with specialized access methods (which is not always feasible) or have to perform the dominance tests on all the tuples in order to determine the result. In this paper we introduce SaLSa (Sort and Limit Skyline algorithm), which exploits the sorting machinery of a relational engine to order tuples so that only a subset of them needs to be examined for computing the skyline result. This makes SaLSa particularly attractive when skyline queries are executed on top of systems that do not understand skyline semantics or when the skyline logic runs on clients with limited power and/or bandwidth.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {405–414},
numpages = {10},
keywords = {monotone functions, client/server architecture, skyline},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183675,
author = {Dellis, Evangelos and Vlachou, Akrivi and Vladimirskiy, Ilya and Seeger, Bernhard and Theodoridis, Yannis},
title = {Constrained Subspace Skyline Computation},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183675},
doi = {10.1145/1183614.1183675},
abstract = {In this paper we introduce the problem of Constrained Subspace Skyline Queries. This class of queries can be thought of as a generalization of subspace skyline queries using range constraints. Although both constrained skyline queries and subspace skyline queries have been addressed previously, the implications of constrained subspace skyline queries has not been examined so far. Constrained skyline queries are usually more expensive than regular skylines. In case of constrained subspace skyline queries additional performance degradation is caused through the projection. In order to support constrained skylines for arbitrary subspaces, we present approaches exploiting multiple low-dimensional indexes instead of relying on a single high-dimensional index. Effective pruning strategies are applied to discard points from dominated regions. An important ingredient of our approach is the workload-adaptive strategy for determining the number of indexes and the assignment of dimensions to the indexes. Extensive performance evaluation shows the superiority of our proposed technique compared to its most related competitors.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {415–424},
numpages = {10},
keywords = {high-dimensional indexing, skyline queries},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183676,
author = {Hose, Katja and Lemke, Christian and Sattler, Kai-Uwe},
title = {Processing Relaxed Skylines in PDMS Using Distributed Data Summaries},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183676},
doi = {10.1145/1183614.1183676},
abstract = {Peer Data Management Systems (PDMS) are a natural extension of heterogeneous database systems. One of the main tasks in such systems is efficient query processing. Insisting on complete answers, however, leads to asking almost every peer in the network. Relaxing these completeness requirements by applying approximate query answering techniques can significantly reduce costs. Since most users are not interested in the exact answers to their queries, rank-aware query operators like top-k or skyline play an important role in query processing. In this paper, we present the novel concept of relaxed skylines that combines the advantages of both rank-aware query operators and approximate query processing techniques. Furthermore, we propose a strategy for processing relaxed skylines in distributed environments that allows for giving guarantees for the completeness of the result using distributed data summaries as routing indexes.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {425–434},
numpages = {10},
keywords = {QTree, PDMS, indexing multidimensional data, relaxed skylines, distributed data summaries, query processing, P2P},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183678,
author = {Nanavati, Amit A. and Gurumurthy, Siva and Das, Gautam and Chakraborty, Dipanjan and Dasgupta, Koustuv and Mukherjea, Sougata and Joshi, Anupam},
title = {On the Structural Properties of Massive Telecom Call Graphs: Findings and Implications},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183678},
doi = {10.1145/1183614.1183678},
abstract = {With ever growing competition in telecommunications markets, operators have to increasingly rely on business intelligence to offer the right incentives to their customers. Toward this end, existing approaches have almost solely focussed on the individual behaviour of customers. Call graphs, that is, graphs induced by people calling each other, can allow telecom operators to better understand the interaction behaviour of their customers, and potentially provide major insights for designing effective incentives.In this paper, we use the Call Detail Records of a mobile operator from four geographically disparate regions to construct call graphs, and analyse their structural properties. Our findings provide business insights and help devise strategies for Mobile Telecom operators. Another goal of this paper is to identify the shape of such graphs. In order to do so, we extend the well-known reachability analysis approach with some of our own techniques to reveal the shape of such massive graphs. Based on our analysis, we introduce the Treasure-Hunt model to describe the shape of mobile call graphs. The proposed techniques are general enough for analysing any large graph. Finally, how well the proposed model captures the shape of other mobile call graphs needs to be the subject of future studies.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {435–444},
numpages = {10},
keywords = {graph analysis},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183679,
author = {Theodoratos, Dimitri and Souldatos, Stefanos and Dalamagas, Theodore and Placek, Pawel and Sellis, Timos},
title = {Heuristic Containment Check of Partial Tree-Pattern Queries in the Presence of Index Graphs},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183679},
doi = {10.1145/1183614.1183679},
abstract = {The wide adoption of XML has increased the interest of the database community on tree-structured data management techniques. Querying capabilities are provided through tree-pattern queries. The need for querying tree-structured data sources when their structure is not fully known, and the need to integrate multiple data sources with different tree structures have driven, recently, the suggestion of query languages that relax the complete specification of a tree pattern. In this paper, we use a query language which allows partial tree-pattern queries (PTPQs). The structure in a PTPQ can be flexibly specified fully, partially or not at all. To evaluate a PTPQ, we exploit index graphs which generate an equivalent set of "complete" tree-pattern queries.In order to process PTPQs, we need to efficiently solve the PTPQ satisfiability and containment problems. These problems become more complex in the context of PTPQs because the partial specification of the structure allows new, non-trivial, structural expressions to be derived from those explicitly specified in a PTPQ. We address the problem of PTPQ satisfiability and containment in the absence and in the presence of index graphs, and we provide necessary and sufficient conditions for each case. To cope with the high complexity of PTPQ containment in the presence of index graphs,we study a family of heuristic approaches for PTPQ containment based on structural information extracted from the index graph in advance and on-the-fly. We implement our approaches and we report on their extensive experimental evaluation and comparison.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {445–454},
numpages = {10},
keywords = {partial tree-pattern query, tree-structured data, query containment},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183680,
author = {Tatikonda, Shirish and Parthasarathy, Srinivasan and Kurc, Tahsin},
title = {TRIPS and TIDES: New Algorithms for Tree Mining},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183680},
doi = {10.1145/1183614.1183680},
abstract = {Recent research in data mining has progressed from mining frequent itemsets to more general and structured patterns like trees and graphs. In this paper, we address the problem of frequent subtree mining that has proven to be viable in a wide range of applications such as bioinformatics, XML processing, computational linguistics, and web usage mining. We propose novel algorithms to mine frequent subtrees from a database of rooted trees. We evaluate the use of two popular sequential encodings of trees to systematically generate and evaluate the candidate patterns. The proposed approach is very generic and can be used to mine embedded or induced subtrees that can be labeled, unlabeled, ordered, unordered, or edge-labeled. Our algorithms are highly cache-conscious in nature because of the compact and simple array-based data structures we use. Typically, L1 and L2 hit rates above 99% are observed. Experimental evaluation showed that our algorithms can achieve up to several orders of magnitude speedup on real datasets when compared to state-of-the-art tree mining algorithms.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {455–464},
numpages = {10},
keywords = {frequent patterns, embedding lists, depth first order codes, tree mining, Prufer sequences},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183682,
author = {Ziegler, Cai-Nicolas and Simon, Kai and Lausen, Georg},
title = {Automatic Computation of Semantic Proximity Using Taxonomic Knowledge},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183682},
doi = {10.1145/1183614.1183682},
abstract = {Taxonomic measures of semantic proximity allow us to compute the relatedness of two concepts. These metrics are versatile instruments required for diverse applications, e.g., the Semantic Web, linguistics, and also text mining. However, most approaches are only geared towards hand-crafted taxonomic dictionaries such as WordNet, which only feature a limited fraction of real-world concepts. More specific concepts, and particularly instances of concepts, i.e., names of artists, locations, brand names, etc., are not covered.The contributions of this paper are two fold. First, we introduce a framework based on Google and the Open Directory Project (ODP), enabling us to derive the semantic proximity between arbitrary concepts and instances. Second, we introduce a new taxonomy-driven proximity metric tailored for our framework. Studies with human subjects corroborate our hypothesis that our new metric outperforms benchmark semantic proximity metrics and comes close to human judgement.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {465–474},
numpages = {10},
keywords = {data extraction, accuracy, taxonomy, semantic similarity, metrics},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183683,
author = {Gollapudi, Sreenivas and Panigrahy, Rina},
title = {Exploiting Asymmetry in Hierarchical Topic Extraction},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183683},
doi = {10.1145/1183614.1183683},
abstract = {Topic or feature extraction is often used as an important step in document classification and text mining. Topics are succinct representation of content in a document collection and hence are very effective when used as content identifiers in peer-to-peer systems and other large scale distributed content management systems. Effective topic extraction is dependent on the accuracy of term clustering that often has to deal with problems like synonymy and polysemy. Retrieval techniques based on spectral analysis like Latent Semantic Indexing (LSI) are often used to effectively solve these problems. Most of the spectral retrieval schemes produce term similarity measures that are symmetric and often, not an accurate characterization of term relationships. Another drawback of LSI is its running time that is polynomial in the dimensions of the m x n matrix, A. This can get prohibitively large for some IR applications. In this paper, we present efficient algorithms using the technique of Locality-Sensitive Hashing (LSH) to extract topics from a document collection based on the asymmetric relationships between terms in a collection. The relationship is characterized by the term co-occurrences and other higher-order similarity measures. Our LSH based scheme can be viewed as a simple alternative to LSI. We show the efficacy of our algorithms via experiments on a set of large documents. An interesting feature of our algorithms is that it produces a natural hierarchical decomposition of the topic space instead of a flat clustering.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {475–482},
numpages = {8},
keywords = {locality-sensitive hashing, latent semantic indexing},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183684,
author = {Kim, Jong Wook and Candan, K. Sel#231;uk},
title = {CP/CV: Concept Similarity Mining without Frequency Information from Domain Describing Taxonomies},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183684},
doi = {10.1145/1183614.1183684},
abstract = {Domain specific ontologies are heavily used in many applications. For instance, these form the bases on which similarity/dissimilarity between keywords are extracted for various knowledge discovery and retrieval tasks. Existing similarity computation schemes can be categorized as (a) structure- or (b) information-based approaches. Structure based approaches compute dissimilarity between keywords using a (weighted) count of edges between two keywords. Information-base approaches, on the other hand, leverage available corpora to extract additional information, such as keyword frequency, to achieve better performance in similarity computation than structure-based approaches. Unfortunately, in many application domains (such as applications that rely on unique-keys in a relational database), frequency information required by information-based approaches does not exist. In this paper, we note that there is a third way of computing similarity: if each node in a given hierarchy can be represented as a vector of related concepts, these vectors could be compared to compute similarities. This requires mapping concept-nodes in a given hierarchy onto a concept space. In this paper, we propose a concept propagation (CP) scheme, which relies on the semantical relationships between concepts implied by the structure of the hierarchy to annotate each concept-node with a concept-vector (CV). We refer to this approach as CP/CV. Comparison of keyword similarity results shows that CP/CV provides significantly better (upto 33%) results than existing structure-based schemes. Also, even if CP/CV does not assume the availability of an appropriate corpus to extract keyword frequency information, our approach matches (and slightly improves on) the performance of information-based approaches.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {483–492},
numpages = {10},
keywords = {concept hierarchies, concept propagation, mining keyword similarities},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183686,
author = {Bailey, Peter and Hawking, David and Matson, Brett},
title = {Secure Search in Enterprise Webs: Tradeoffs in Efficient Implementation for Document Level Security},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183686},
doi = {10.1145/1183614.1183686},
abstract = {Document level security (DLS) -- enforcing permissions prevailing at the time of search -- is specified as a mandatory requirement in many enterprise search applications. Unfortunately, depending upon implementation details and values of key parameters, DLS may come at a high price in increased query processing time, leading to an unacceptably slow search experience. In this paper we present a model and a method for carrying out secure search in the presence of DLS within enterprise webs. We report on two alternative commercial DLS search implementations. Using a 10,000 document experimental DLS environment, we graph the dependence of query processing time on result set size and visibility density for different classes of user. Scaled up to collections of tens of thousands of documents, our results suggest that query times will be unacceptable if exact counts of matching documents are required and also for users who can view only a small proportion of documents. We show that the time to conduct access checks is dramatically increased if requests must be sent off-server, even on a local network, and discuss methods for reducing the cost of security checks. We conclude that enterprises can effectively reduce DLS overheads by organizing documents in such a way that most access checking can be at collection rather than document level, by forgoing accurate match counts, by using caching, batching or hierarchical methods to cut costs of DLS checking and, if applicable, by using a single portal both to access and search documents.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {493–502},
numpages = {10},
keywords = {collection level security, access control, document level security, security models, scalability, caching, performance, enterprise search, file systems},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183687,
author = {Ordonez, Carlos and Garc\'{\i}a-Garc\'{\i}a, Javier},
title = {Vector and Matrix Operations Programmed with UDFs in a Relational DBMS},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183687},
doi = {10.1145/1183614.1183687},
abstract = {In general, a relational DBMS provides limited capabilities to perform multidimensional statistical analysis, which requires manipulating vectors and matrices. In this work, we study how to extend a DBMS with basic vector and matrix operators by programming User-Defined Functions (UDFs). We carefully analyze UDF features and limitations to implement vector and matrix operations commonly used in statistics, machine learning and data mining, paying attention to DBMS, operating system and computer architecture constraints. UDFs represent a C programming interface that allows the definition of scalar and aggregate functions that can be used in SQL. UDFs have several advantages and limitations. A UDF allows fast evaluation of arithmetic expressions, memory manipulation, using multidimensional arrays and exploiting all C language control statements. Nevertheless, a UDF cannot perform disk I/O, the amount of heap and stack memory that can be allocated is small and the UDF code must consider specific architecture characteristics of the DBMS. We experimentally compare UDFs and SQL with respect to performance, ease of use, flexibility and scalability. We profile UDFs based on call overhead, memory management and interleaved disk access. We show UDFs are faster than standard SQL aggregations and as fast as SQL arithmetic expressions.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {503–512},
numpages = {10},
keywords = {UDF, SQL, matrix, vector},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183688,
author = {Pioch, Nicholas J. and Everett, John O.},
title = {POLESTAR: Collaborative Knowledge Management and Sensemaking Tools for Intelligence Analysts},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183688},
doi = {10.1145/1183614.1183688},
abstract = {In this paper, we describe POLESTAR (POLicy Explanation using STories and ARguments), an integrated suite of knowledge management and collaboration tools for intelligence analysts.POLESTAR provides built-in support for analyst workflow, including collection of textual facts from source documents, structured argumentation, and automatic citation in analytic product documents. Underlying POLESTAR is a scalable dependency repository, which provides traceability from product documents to source snippets. The repository's notification engine allows POLESTAR to alert analysts when dependent sources are discredited and aid them in repairing affected arguments. The paper then discusses recent extensions to POLESTAR to support collaborative analysis through community-of-interest finding, portfolio sharing, and peer review of arguments. We conclude with a preview of future research and summary of POLESTAR's primary benefits from the point of view of its deployed users.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {513–521},
numpages = {9},
keywords = {knowledge management, argument structuring, workflow management, intelligence analysis, knowledge representation, sensemaking, collaboration},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183689,
author = {Holz, Harald and Rostanin, Oleg and Dengel, Andreas and Suzuki, Takeshi and Maeda, Kaoru and Kanasaki, Katsumi},
title = {Task-Based Process Know-How Reuse and Proactive Information Delivery in TaskNavigator},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183689},
doi = {10.1145/1183614.1183689},
abstract = {Knowledge management approaches for weakly-structured, adhoc knowledge work processes need to be lightweight, i.e., they cannot rely on high upfront modeling efforts. This paper presents TaskNavigator, a novel prototype to support weakly-structured processes by integrating a standard task list application with a state-of-the-art document classification system. The resulting system allows for a task-oriented view on office workers' personal knowledge spaces in order to realize a proactive and contextsensitive information support during daily, knowledge-intensive tasks. Moreover, TaskNavigator supports process know-how reuse by proactively suggesting similar tasks or relevant process models, based on textual similarities. Finally, we report on a feasibility test and a case study that have been conducted in order to evaluate the system in the context of daily research task management and software requirements analysis.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {522–531},
numpages = {10},
keywords = {proactive information delivery, agile workflows, process-oriented knowledge management},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183691,
author = {Ye, Jieping and Xiong, Tao and Li, Qi and Janardan, Ravi and Bi, Jinbo and Cherkassky, Vladimir and Kambhamettu, Chandra},
title = {Efficient Model Selection for Regularized Linear Discriminant Analysis},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183691},
doi = {10.1145/1183614.1183691},
abstract = {Classical Linear Discriminant Analysis (LDA) is not applicable for small sample size problems due to the singularity of the scatter matrices involved. Regularized LDA (RLDA) provides a simple strategy to overcome the singularity problem by applying a regularization term, which is commonly estimated via cross-validation from a set of candidates. However, cross-validation may be computationally prohibitive when the candidate set is large. An efficient algorithm for RLDA is presented that computes the optimal transformation of RLDA for a large set of parameter candidates, with approximately the same cost as running RLDA a small number of times. Thus it facilitates efficient model selection for RLDA.An intrinsic relationship between RLDA and Uncorrelated LDA (ULDA), which was recently proposed for dimension reduction and classification is presented. More specifically, RLDA is shown to approach ULDA when the regularization value tends to zero. That is, RLDA without any regularization is equivalent to ULDA. It can be further shown that ULDA maps all data points from the same class to a common point, under a mild condition which has been shown to hold for many high-dimensional datasets. This leads to the overfitting problem in ULDA, which has been observed in several applications. Thetheoretical analysis presented provides further justification for the use of regularization in RLDA. Extensive experiments confirm the claimed theoretical estimate of efficiency. Experiments also show that, for a properly chosen regularization parameter, RLDA performs favorably in classification, in comparison with ULDA, as well as other existing LDA-based algorithms and Support Vector Machines (SVM).},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {532–539},
numpages = {8},
keywords = {dimension reduction, linear discriminant analysis, model selection, regularization},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183692,
author = {Yan, Xin and Song, Dawei and Li, Xue},
title = {Concept-Based Document Readability in Domain Specific Information Retrieval},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183692},
doi = {10.1145/1183614.1183692},
abstract = {Domain specific information retrieval has become in demand. Not only domain experts, but also average non-expert users are interested in searching domain specific (e.g., medical and health) information from online resources. However, a typical problem to average users is that the search results are always a mixture of documents with different levels of readability. Non-expert users may want to see documents with higher readability on the top of the list. Consequently the search results need to be re-ranked in a descending order of readability. It is often not practical for domain experts to manually label the readability of documents for large databases. Computational models of readability needs to be investigated. However, traditional readability formulas are designed for general purpose text and insufficient to deal with technical materials for domain specific information retrieval. More advanced algorithms such as textual coherence model are computationally expensive for re-ranking a large number of retrieved documents. In this paper, we propose an effective and computationally tractable concept-based model of text readability. In addition to textual genres of a document, our model also takes into account domain specific knowledge, i.e., how the domain-specific concepts contained in the document affect the document's readability. Three major readability formulas are proposed and applied to health and medical information retrieval. Experimental results show that our proposed readability formulas lead to remarkable improvements in terms of correlation with users' readability ratings over four traditional readability measures.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {540–549},
numpages = {10},
keywords = {document ranking, document scope and cohesion, document readability},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183693,
author = {Shakery, Azadeh and Zhai, ChengXiang},
title = {A Probabilistic Relevance Propagation Model for Hypertext Retrieval},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183693},
doi = {10.1145/1183614.1183693},
abstract = {A major challenge in developing models for hypertext retrieval is to effectively combine content information with the link structure available in hypertext collections. Although several link-based ranking methods have been developed to improve retrieval results, none of them can fully exploit the discrimination power of contents as well as fully exploit all useful link structures. In this paper, we propose a general relevance propagation framework for combining content and link information. The framework gives a probabilistic score to each document defined based on a probabilistic surfing model. Two main characteristics of our framework are our probabilistic view on the relevance propagation model and propagation through multiple sets of neighbors. We compare eight different models derived from the probabilistic relevance propagation framework on two standard TREC Web test collections. Our results show that all the eight relevance propagation models can outperform the baseline content only ranking method for a wide range of parameter values, indicating that the relevance propagation framework provides a general, effective and robust way of exploiting link information. Our experiments also show that using multiple neighbor sets outperforms using just one type of neighbors significantly and taking a probabilistic view of propagation provides guidance on setting propagation parameters.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {550–558},
numpages = {9},
keywords = {content and link ranking, probabilistic relevance propagation, hypertext retrieval model, web information retrieval},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183694,
author = {Pickens, Jeremy and MacFarlane, Andrew},
title = {Term Context Models for Information Retrieval},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183694},
doi = {10.1145/1183614.1183694},
abstract = {At their heart, most if not all information retrieval models utilize some form of term frequency.The notion is that the more often a query term occurs in a document, the more likely it is that document meets an information need. We examine an alternative. We propose a model which assesses the presence of a term in a document not by looking at the actual occurrence of that term, but by a set of non-independent supporting terms, i.e. context. This yields a weighting for terms in documents which is different from and complementary to tf-based methods, and is beneficial for retrieval.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {559–566},
numpages = {8},
keywords = {context-based retrieval, conditional random fields, maximum entropy},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183696,
author = {Zhou, Yun and Croft, W. Bruce},
title = {Ranking Robustness: A Novel Framework to Predict Query Performance},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183696},
doi = {10.1145/1183614.1183696},
abstract = {In this paper, we introduce the notion of ranking robustness, which refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of uncertainty in the ranked documents. We propose a statistical measure called the robustness score to quantify this notion. We demonstrate that the robustness score significantly and consistently correlates with query performance in a variety of TREC test collections including the GOV2 collection. We compare the robustness score with the clarity score method which is the state-of-the-art technique for query performance prediction. Our experimental results show that the robustness score performs better than or at least as good as the clarity score. We find that the clarity score is barely correlated with query performance on the GOV2 collection while the correlation between the robustness score and query performance remains significant. We also notice that a combination of the two usually results in more prediction power.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {567–574},
numpages = {8},
keywords = {ranking robustness, query performance prediction},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183697,
author = {Su, Weifeng and Wang, Jiying and Huang, Qiong and Lochovsky, Fred},
title = {Query Result Ranking over E-Commerce Web Databases},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183697},
doi = {10.1145/1183614.1183697},
abstract = {To deal with the problem of too many results returned from an E-commerce Web database in response to a user query, this paper proposes a novel approach to rank the query results. Based on the user query, we speculate how much the user cares about each attribute and assign a corresponding weight to it. Then, for each tuple in the query result, each attribute value is assigned a score according to its "desirableness" to the user. These attribute value scores are combined according to the attribute weights to get a final ranking score for each tuple. Tuples with the top ranking scores are presented to the user first. Our ranking method is domain independent and requires no user feedback. Experimental results demonstrate that this ranking method can effectively capture a user's preferences.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {575–584},
numpages = {10},
keywords = {e-commerce, attribute weight assignment, query result ranking},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183698,
author = {Taylor, Michael and Zaragoza, Hugo and Craswell, Nick and Robertson, Stephen and Burges, Chris},
title = {Optimisation Methods for Ranking Functions with Multiple Parameters},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183698},
doi = {10.1145/1183614.1183698},
abstract = {Optimising the parameters of ranking functions with respect to standard IR rank-dependent cost functions has eluded satisfactory analytical treatment. We build on recent advances in alternative differentiable pairwise cost functions, and show that these techniques can be successfully applied to tuning the parameters of an existing family of IR scoring functions (BM25), in the sense that we cannot do better using sensible search heuristics that directly optimize the rank-based cost function NDCG. We also demonstrate how the size of training set affects the number of parameters we can hope to tune this way.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {585–593},
numpages = {9},
keywords = {ranking, scoring, effectiveness measures, evaluation, optimisation},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183699,
author = {Broder, Andrei and Fontura, Marcus and Josifovski, Vanja and Kumar, Ravi and Motwani, Rajeev and Nabar, Shubha and Panigrahy, Rina and Tomkins, Andrew and Xu, Ying},
title = {Estimating Corpus Size via Queries},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183699},
doi = {10.1145/1183614.1183699},
abstract = {We consider the problem of estimating the size of a collection of documents using only a standard query interface. Our main idea is to construct an unbiased and low-variance estimator that can closely approximate the size of any set of documents defined by certain conditions, including that each document in the set must match at least one query from a uniformly sampleable query pool of known size, fixed in advance.Using this basic estimator, we propose two approaches to estimating corpus size. The first approach requires a uniform random sample of documents from the corpus. The second approach avoids this notoriously difficult sample generation problem, and instead uses two fairly uncorrelated sets of terms as query pools; the accuracy of the second approach depends on the degree of correlation among the two sets of terms.Experiments on a large TREC collection and on three major search engines demonstrates the effectiveness of our algorithms.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {594–603},
numpages = {10},
keywords = {estimator, random sampling, corpus size},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183701,
author = {Reeve, Lawrence H. and Han, Hyoil and Nagori, Saya V. and Yang, Jonathan C. and Schwimmer, Tamara A. and Brooks, Ari D.},
title = {Concept Frequency Distribution in Biomedical Text Summarization},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183701},
doi = {10.1145/1183614.1183701},
abstract = {Text summarization is a data reduction process. The use of text summarization enables users to reduce the amount of text that must be read while still assimilating the core information. The data reduction offered by text summarization is particularly useful in the biomedical domain, where physicians must continuously find clinical trial study information to incorporate into their patient treatment efforts. Such efforts are often hampered by the high-volume of publications. Our contribution is two-fold: 1) to propose the frequency of domain concepts as a method to identify important sentences within a full-text; and 2) propose a novel frequency distribution model and algorithm for identifying important sentences based on term or concept frequency distribution. An evaluation of several existing summarization systems using biomedical texts is presented in order to determine a performance baseline. For domain concept comparison, a recent high-performing frequency-based algorithm using terms is adapted to use concepts and evaluated using both terms and concepts. It is shown that the use of concepts performs closely with the use of terms for sentence selection. Our proposed frequency distribution model and algorithm outperforms a state-of-the-art approach.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {604–611},
numpages = {8},
keywords = {text summarization, concept frequency, biomedicine},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183702,
author = {M\"{u}ller, Heiko and Freytag, Johann-Christoph and Leser, Ulf},
title = {Describing Differences between Databases},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183702},
doi = {10.1145/1183614.1183702},
abstract = {We study the novel problem of efficiently computing the update distance for a pair of relational databases. In analogy to the edit distance of strings, we define the update distance of two databases as the minimal number of set-oriented insert, delete and modification operations necessary to transform one database into the other. We show how this distance can be computed by traversing a search space of database instances connected by update operations. This insight leads to a family of algorithms that compute the update distance or approximations of it. In our experiments we observed that a simple heuristic performs surprisingly well in most considered cases.Our motivation for studying distance measures for databases stems from the field of scientific databases. There, replicas of a single database are often maintained at different sites, which typically leads to (accidental or planned) divergence of their content. To re-create a consistent view, these differences must be resolved. Such an effort requires an understanding of the process that produced them. We found that minimal update sequences of set-oriented update operations are a proper and concise representation of systematic errors, thus giving valuable clues to domain experts responsible for conflict resolution.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {612–621},
numpages = {10},
keywords = {contradicting databases, conflict resolution, update distance},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183703,
author = {Varadarajan, Ramakrishna and Hristidis, Vagelis},
title = {A System for Query-Specific Document Summarization},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183703},
doi = {10.1145/1183614.1183703},
abstract = {There has been a great amount of work on query-independent summarization of documents. However, due to the success of Web search engines query-specific document summarization (query result snippets) has become an important problem, which has received little attention. We present a method to create query-specific summaries by identifying the most query-relevant fragments and combining them using the semantic associations within the document. In particular, we first add structure to the documents in the preprocessing stage and convert them to document graphs. Then, the best summaries are computed by calculating the top spanning trees on the document graphs. We present and experimentally evaluate efficient algorithms that support computing summaries in interactive time. Furthermore, the quality of our summarization method is compared to current approaches using a user survey.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {622–631},
numpages = {10},
keywords = {keyword search, query-specific summarization, Steiner tree problem, user survey},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183705,
author = {Cong, Gao and Fan, Wenfei and Geerts, Floris},
title = {Annotation Propagation Revisited for Key Preserving Views},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183705},
doi = {10.1145/1183614.1183705},
abstract = {This paper revisits the analysis of annotation propagation from source databases to views defined in terms of conjunctive (SPJ) queries. Given a source database D, an SPJ query Q, the view Q(D) and a tuple ΔV in the view, the view (resp. source) side-effect problem is to find a minimal set ΔD of tuples such that the deletion of ΔD from D results in the deletion of ΔV from Q(D) while minimizing the side effects on the view (resp. the source). A third problem, referred to as the annotation placement problem, is to find a single base tuple ΔD such that annotation in a field of ΔD propagates to ΔV while minimizing the propagation to other fields in the view Q(D). These are important for data provenance and the management of view updates. However important, these problems are unfortunately NP-hard for most subclasses of SPJ views [5].To make the annotation propagation analysis feasible in practice, we propose a key preserving condition on SPJ views, which requires that the projection fields of an SPJ view Q retain a key of each base relation involved in Q. While this condition is less restrictive than other proposals [11, 14], it often simplifies the annotation propagation analysis. Indeed, for key-preserving SPJ views the annotation placement problem coincides with the view side-effect problem, and the view and source side-effect problems become tractable. In addition we generalize the setting of [5] by allowing ΔV to be a group of tuples to be deleted, and investigate the insertion of tuples to the view. We show that group updates make the analysis harder: these problems become NP-hard for several subclasses of SPJ views. We also show that for SPJ views the source and view side-effect problems are NP-hard for single-tuple insertion, but are tractable for some subclasses of SPJ for group insertions, in the presence or in the absence of the key preservation condition.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {632–641},
numpages = {10},
keywords = {provenance, annotations, view updates},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183706,
author = {Chirkova, Rada and Sadri, Fereidoon},
title = {Query Optimization Using Restructured Views},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183706},
doi = {10.1145/1183614.1183706},
abstract = {We study optimization of relational queries using materialized views, where views may be regular or restructured. In a restructured view, some data from the base table(s) are represented as metadata - that is, schema information, such as table and attribute names - or vice versa.Using restructured views in query optimization opens up a new spectrum of views that were not previously available, and can result in significant additional savings in query-evaluation costs. These savings can be obtained due to a significantly larger set of views to choose from, and may involve reduced table sizes, elimination of self-joins, clustering produced by restructuring, and horizontal partitioning.In this paper we propose a general query-optimization framework that treats regular and restructured views in a uniform manner and is applicable to SQL select-project-join queries and views with or without aggregation. Within the framework we provide (1) algorithms to determine when a view (regular or restructured) is usable in answering a query, and (2) algorithms to rewrite a query using usable views.Semantic information, such as knowledge of the key of a view, can be used to further optimize a rewritten query. Within our general query-optimization framework, we develop techniques for determining the key of a (regular or restructured) view, and show how this information can be used to further optimize a rewritten query. It is straightforward to integrate all our algorithms and techniques into standard query-optimization algorithms.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {642–651},
numpages = {10},
keywords = {materialized views, restructured views, query optimization},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183707,
author = {Wang, Xiaoyu and Cherniack, Mitch},
title = {Improving Query I/O Performance by Permuting and Refining Block Request Sequences},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183707},
doi = {10.1145/1183614.1183707},
abstract = {The I/O performance of query processing can be improved using two complementary approaches: improve the buffer and the file system management policies of the DB buffer manager and the OS file system manager (e.g. page replacement), or improve the sequence of requests that are submitted to a file system manager and that lead to actual I/O's (block request sequences). This paper takes the latter approach. Exploiting common file system practices as found in Linux, we propose four techniques for permuting and refining block request sequences: Block-Level I/O Grouping, File-Level I/O Grouping, I/O Ordering, and Block Recycling. To manifest these techniques, we create two new plan operations, MMS and SHJ, each of which adopts some of the block request refinement techniques above. We implement the new plan operations on top of Postgres running on Linux, and show experimental results that demonstrate up to a factor of 4 performance benefit from the use of these techniques.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {652–661},
numpages = {10},
keywords = {I/O sequence, block request sequence},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183709,
author = {Sch\"{u}tze, Hinrich and Velipasaoglu, Emre and Pedersen, Jan O.},
title = {Performance Thresholding in Practical Text Classification},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183709},
doi = {10.1145/1183614.1183709},
abstract = {In practical classification, there is often a mix of learnable and unlearnable classes and only a classifier above a minimum performance threshold can be deployed. This problem is exacerbated if the training set is created by active learning. The bias of actively learned training sets makes it hard to determine whether a class has been learned. We give evidence that there is no general and efficient method for reducing the bias and correctly identifying classes that have been learned. However, we characterize a number of scenarios where active learning can succeed despite these difficulties.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {662–671},
numpages = {10},
keywords = {practical text classification, accuracy estimation, active learning, learnability},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183710,
author = {Shen, Dou and Sun, Jian-Tao and Yang, Qiang and Chen, Zheng},
title = {Text Classification Improved through Multigram Models},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183710},
doi = {10.1145/1183614.1183710},
abstract = {Classification algorithms and document representation approaches are two key elements for a successful document classification system. In the past, much work has been conducted to find better ways to represent documents. However, most of the attempts rely on certain extra resources such as WordNet, or they face the problem of extremely high dimension. In this paper, we propose a new document representation approach based on n-multigram language models. This approach can automatically discover the hidden semantic sequences in the documents under each category. Based on n-multigram language models and n-gram language models, we put forward two text classification algorithms. The experiments on RCV1 show that our proposed algorithm based on n-multigram models alone can achieve the similar or even better classification performance compared with the classifier based on n-gram models but the model size of our algorithm is much smaller than that of the latter. Another proposed algorithm based on the combination of n-multigram models and n-gram models improves the micro-F1 and macro-F1 values from 89.5% to 92.6% and 87.2% to 91.1% respectively. All these observations support the validity of our proposed document representation approach.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {672–681},
numpages = {10},
keywords = {document representation, N-gram models, N-multigram models, text classification},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183711,
author = {Lu, Yumao and Peng, Fuchun and Li, Xin and Ahmed, Nawaaz},
title = {Coupling Feature Selection and Machine Learning Methods for Navigational Query Identification},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183711},
doi = {10.1145/1183614.1183711},
abstract = {It is important yet hard to identify navigational queries in Web search due to a lack of sufficient information in Web queries, which are typically very short. In this paper we study several machine learning methods, including naive Bayes model, maximum entropy model, support vector machine (SVM), and stochastic gradient boosting tree (SGBT), for navigational query identification in Web search. To boost the performance of these machine techniques, we exploit several feature selection methods and propose coupling feature selection with classification approaches to achieve the best performance. Different from most prior work that uses a small number of features, in this paper, we study the problem of identifying navigational queries with thousands of available features, extracted from major commercial search engine results, Web search user click data, query log, and the whole Web's relational content. A multi-level feature extraction system is constructed.Our results on real search data show that 1) Among all the features we tested, user click distribution features are the most important set of features for identifying navigational queries. 2) In order to achieve good performance, machine learning approaches have to be coupled with good feature selection methods. We find that gradient boosting tree, coupled with linear SVM feature selection is most effective. 3) With carefully coupled feature selection and classification approaches, navigational queries can be accurately identified with 88.1% F1 score, which is 33% error rate reduction compared to the best uncoupled system, and 40% error rate reduction compared to a well tuned system without feature selection.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {682–689},
numpages = {8},
keywords = {machine learning, navigational query classification},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183713,
author = {Yang, Lingpeng and Ji, Donghong and Zhou, Guodong and Nie, Yu and Xiao, Guozheng},
title = {Document Re-Ranking Using Cluster Validation and Label Propagation},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183713},
doi = {10.1145/1183614.1183713},
abstract = {This paper proposes a novel document re-ranking approach in information retrieval, which is done by a label propagation-based semi-supervised learning algorithm to utilize the intrinsic structure underlying in the large document data. Since no labeled relevant or irrelevant documents are generally available in IR, our approach tries to extract some pseudo labeled documents from the ranking list of the initial retrieval. For pseudo relevant documents, we determine a cluster of documents from the top ones via cluster validation-based k-means clustering; for pseudo irrelevant ones, we pick a set of documents from the bottom ones. Then the ranking of the documents can be conducted via label propagation. Evaluation on benchmark corpora shows that the approach can achieve significant improvement over standard baselines and performs better than other related approaches.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {690–697},
numpages = {8},
keywords = {document re-ranking, information retrieval, data manifold structure, label propagation},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183714,
author = {Leuski, Anton and Lavrenko, Victor},
title = {Tracking Dragon-Hunters with Language Models},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183714},
doi = {10.1145/1183614.1183714},
abstract = {We are interested in the problem of understanding the connections between human activities and the content of textual information generated in regard to those activities. Firstly, we define and motivate this problem as an important part in making sense of various life events. Secondly, we introduce the domain of massive online collaborative environments, specifically online virtual worlds, where people meet, exchange messages, and perform actions as a rich data source for such an analysis. Finally, we outline three experimental tasks and show how statistical language modeling and text clustering techniques may allow us to explore those connections successfully.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {698–707},
numpages = {10},
keywords = {virtual worlds, activity detection, massively multiplayer online role-playing game, MMORPG},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183715,
author = {Varde, Aparna S. and Rundensteiner, Elke A. and Ruiz, Carolina and Brown, David C. and Maniruzzaman, Mohammmed and Sisson, Richard D.},
title = {Designing Semantics-Preserving Cluster Representatives for Scientific Input Conditions},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183715},
doi = {10.1145/1183614.1183715},
abstract = {In scientific domains, knowledge is often discovered from experiments by grouping or clustering them based on the similarity of their output. The causes of similarity are analyzed based on the input conditions characterizing a given type of output, i.e., a given cluster. This analysis helps in applications such as decision support in industry. Cluster representatives form at-a-glance depictions for such applications. Randomly selecting a set of conditions in a cluster as its representative is not sufficient since distinct combinations of inputs could lead to the same cluster. In this paper, an approach called DesCond is proposed to design semantics-preserving cluster representatives for scientific input conditions. We define a notion of distance for conditions to capture semantics based on the types of their attributes and their relative importance. Using this distance, methods of building candidate cluster representatives with different levels of detail are proposed. Candidates are compared using the DesCond Encoding proposed in this paper that assesses their complexity and information loss, given user interests. The candidate with the lowest encoding for each cluster is returned as its designed representative. DesCond is evaluated with real data from Materials Science. Evaluation with domain expert interviews and formal user surveys shows that designed representatives consistently outperform randomly selected ones and different candidates suit different users.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {708–717},
numpages = {10},
keywords = {post-processing, minimum description length, domain knowledge, visual displays, decision trees, distance metrics},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183717,
author = {He, Bingsheng and Luo, Qiong},
title = {Cache-Oblivious Nested-Loop Joins},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183717},
doi = {10.1145/1183614.1183717},
abstract = {We propose to adapt the newly emerged cache-oblivious model to relational query processing. Our goal is to automatically achieve an overall performance comparable to that of fine-tuned algorithms on a multi-level memory hierarchy. This automaticity is because cache-oblivious algorithms assume no knowledge about any specific parameter values, such as the capacity and block size of each level of the hierarchy. As a first step, we propose recursive partitioning to implement cache-oblivious nested-loop joins (NLJs) without indexes, and recursive clustering and buffering to implement cache-oblivious NLJs with indexes. Our theoretical results and empirical evaluation on three different architectures show that our cache-oblivious NLJs match the performance of their manually optimized, cache-conscious counterparts.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {718–727},
numpages = {10},
keywords = {recursive clustering, buffering, recursive partitioning, cache-oblivious, nested-loop join},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183718,
author = {Terrovitis, Manolis and Passas, Spyros and Vassiliadis, Panos and Sellis, Timos},
title = {A Combination of Trie-Trees and Inverted Files for the Indexing of Set-Valued Attributes},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183718},
doi = {10.1145/1183614.1183718},
abstract = {Set-valued attributes frequently occur in contexts like market-basked analysis and stock market trends. Late research literature has mainly focused on set containment joins and data mining without considering simple queries on set valued attributes. In this paper we address superset, subset and equality queries and we propose a novel indexing scheme for answering them on set-valued attributes. The proposed index superimposes a trie-tree on top of an inverted file that indexes a relation with set-valued data. We show that we can efficiently answer the aforementioned queries by indexing only a subset of the most frequent of the items that occur in the indexed relation. Finally, we show through extensive experiments that our approach outperforms the state of the art mechanisms and scales gracefully as database size grows.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {728–737},
numpages = {10},
keywords = {HTI, inverted files, tries, containment queries},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183719,
author = {Cheng, Reynold and Singh, Sarvjeet and Prabhakar, Sunil and Shah, Rahul and Vitter, Jeffrey Scott and Xia, Yuni},
title = {Efficient Join Processing over Uncertain Data},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183719},
doi = {10.1145/1183614.1183719},
abstract = {In many applications data values are inherently uncertain. This includes moving-objects, sensors and biological databases. There has been recent interest in the development of database management systems that can handle uncertain data. Some proposals for such systems include attribute values that are uncertain. In particular, an attribute value can be modeled as a range of possible values, associated with a probability density function. Previous efforts for this type of data have only addressed simple queries such as range and nearest-neighbor queries. Queries that join multiple relations have not been addressed in earlier work despite the significance of joins in databases. In this paper we address join queries over uncertain data. We propose a semantics for the join operation, define probabilistic operators over uncertain data, and propose join algorithms that provide efficient execution of probabilistic joins. The paper focuses on an important class of joins termed probabilistic threshold joins that avoid some of the semantic complexities of dealing with uncertain data. For this class of joins we develop three sets of optimization techniques: item-level, page-level, and index-level pruning. These techniques facilitate pruning with little space and time overhead, and are easily adapted to most join algorithms. We verify the performance of these techniques experimentally.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {738–747},
numpages = {10},
keywords = {joins, imprecise data, uncertainty management},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183721,
author = {Gkoulalas-Divanis, Aris and Verykios, Vassilios S.},
title = {An Integer Programming Approach for Frequent Itemset Hiding},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183721},
doi = {10.1145/1183614.1183721},
abstract = {The rapid growth of transactional data brought, soon enough, into attention the need of its further exploitation. In this paper, we investigate the problem of securing sensitive knowledge from being exposed in patterns extracted during association rule mining. Instead of hiding the produced rules directly, we decide to hide the sensitive frequent itemsets that may lead to the production of these rules. As a first step, we introduce the notion of distance between two databases and a measure for quantifying it. By trying to minimize the distance between the original database and its sanitized version (that can safely be released), we propose a novel, exact algorithm for association rule hiding and evaluate it on real world datasets demonstrating its effectiveness towards solving the problem.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {748–757},
numpages = {10},
keywords = {association rule mining, optimization, integer programming, sensitive itemset hiding, rivacy preserving data mining},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183722,
author = {Kapoor, V. and Poncelet, P. and Trousset, F. and Teisseire, M.},
title = {Privacy Preserving Sequential Pattern Mining in Distributed Databases},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183722},
doi = {10.1145/1183614.1183722},
abstract = {Research in the areas of privacy preserving techniques in databases and subsequently in privacy enhancement technologies have witnessed an explosive growth-spurt in recent years. This escalation has been fueled by the growing mistrust of individuals towards organizations collecting and disbursing their Personally Identifiable Information (PII). Digital repositories have become increasingly susceptible to intentional or unintentional abuse, resulting in organizations to be liable under the privacy legislations that are being adopted by governments the world over. These privacy concerns have necessitated new advancements in the field of distributed data mining wherein, collaborating parties may be legally bound not to reveal the private information of their customers. In this paper, we present a new algorithm PriPSeP (Privacy Preserving SEquential Patterns) for the mining of sequential patterns from distributed databases while preserving privacy. A salient feature of PriPSeP is that due to its flexibility it is more pertinent to mining operations for real world applications in terms of efficiency and functionality. Under some reasonable assumptions, we prove that our architecture and protocol employed by our algorithm for multi-party computation is secure.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {758–767},
numpages = {10},
keywords = {privacy mining},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183723,
author = {Gollapudi, Sreenivas and Panigrahy, Rina},
title = {A Dictionary for Approximate String Search and Longest Prefix Search},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183723},
doi = {10.1145/1183614.1183723},
abstract = {In this paper we propose a dictionary data structure for string search with errors where the query string may didiffer from the expected matching string by a few edits. This data structure can also be used to find the database string with the longest common prefix with few errors. Specifically, with a database of n random strings, each of length of O(m), we show how to perform string search on a query string that differs from its closest match by k edits using a data structure of linear size and query time equal to \~{O}(log n 2 log n klog a 2m over 2m). This means that if k &lt; m over log a 2m log n, then the query time is \~{O}(1). This is of significant in practice as there are several applications where k is small relative to m. Our approach converts strings into bit vectors so that similar strings can map to similar bit vectors with small hamming distance. A simple reduction can be used to obtain similar results for approximate longest prefix search.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {768–775},
numpages = {8},
keywords = {hamming space, embeddings},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183725,
author = {Xiao, Xiangye and Luo, Qiong and Xie, Xing and Ma, Wei-Ying},
title = {A Comparative Study on Classifying the Functions of Web Page Blocks},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183725},
doi = {10.1145/1183614.1183725},
abstract = {In this paper, we study the problem of learning block classification models to estimate block functions. We distinguish general models, which are learned across multiple sites, and site-specific models, which are learned within individual sites. We further consider several factors that affect the learning process and model effectiveness. These factors include the layout features, the content features, the classifiers, and the term selection methods. We have empirically evaluated the performance of the models when the factors are varied. Our main results are that layout features do better than content features for learning both general and site-specific models.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {776–777},
numpages = {2},
keywords = {block function, feature selection, block classification model, web page block, layout features, content features},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183726,
author = {Angelova, Ralitsa and Siersdorfer, Stefan},
title = {A Neighborhood-Based Approach for Clustering of Linked Document Collections},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183726},
doi = {10.1145/1183614.1183726},
abstract = {This paper addresses the problem of automatically structuring linked document collections by using clustering. In contrast to traditional clustering, we study the clustering problem in the light of available link structure information for the data set (e.g., hyperlinks among web documents or co-authorship among bibliographic data entries). Our approach is based on iterative relaxation of cluster assignments, and can be built on top of any clustering algorithm. This technique results in higher cluster purity, better overall accuracy, and make self-organization more robust.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {778–779},
numpages = {2},
keywords = {clustering, exploiting link structure},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183727,
author = {Hlaoua, Lobna and Sauvagnat, Karen and Boughanem, Mohand},
title = {A Structure-Oriented Relevance Feedback Method for XML Retrieval},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183727},
doi = {10.1145/1183614.1183727},
abstract = {Relevance Feedback (RF) is a technique allowing to enrich an initial query according to the user feedback. The goal is to express more precisily the user's needs. Some open issues appear when considering semi-structured documents like XML documents. Most of the RF approaches proposed in XML retrieval are simple adaptations of traditional RF to the new granularity of information. They enrich queries by adding terms extracted from relevant elements instead of terms extracted from whole documents. In this paper we show how structural constraints can also be used in RF. We propose a new approach that is able to extend the initial query by adding one or more generative structures. This approach is applied to unstructured queries. Experiments are carried out on INEX collection and results show the interest of our method.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {780–781},
numpages = {2},
keywords = {XML retrieval, relevance feedback, structure},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183728,
author = {Qian, Tieyun and Xiong, Hui and Wang, Yuanzhen and Chen, Enhong},
title = {Adapting Association Patterns for Text Categorization: Weaknesses and Enhancements},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183728},
doi = {10.1145/1183614.1183728},
abstract = {The use of association patterns for text categorization has attracted great interest and a variety of useful methods have been developed. However, the key characteristics of pattern-based text categorization remain unclear. Indeed, there are still no concrete answers for the following two questions: First, what kind of association patterns are the best candidate for pattern-based text categorization? Second, what is the most desirable way to use patterns for text categorization? In this paper, we focus on answering the above two questions. Specifically, we show that hyperclique patterns are more desirable than frequent patterns for text categorization. Along this line, we develop an algorithm for text categorization using hyperclique patterns. The experimental results show that our method provides better performance than state-of-the-art methods in terms of both computational performance and classification accuracy.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {782–783},
numpages = {2},
keywords = {hyperclique patterns, text categorization},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183729,
author = {Potamias, Michalis and Patroumpas, Kostas and Sellis, Timos},
title = {Amnesic Online Synopses for Moving Objects},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183729},
doi = {10.1145/1183614.1183729},
abstract = {We present a hierarchical tree structure for online maintenance of time-decaying synopses over streaming data. We exemplify such an amnesic behavior over streams of locations taken from numerous moving objects in order to obtain reliable trajectory approximations as well as affordable estimates regarding distinct count spatiotemporal queries.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {784–785},
numpages = {2},
keywords = {compression, data streams, continuous queries, trajectories, moving objects},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183730,
author = {Jiang, Zhewei and Luo, Cheng and Hou, Wen-Chi},
title = {An Efficient One-Phase Holistic Twig Join Algorithm for XML Data},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183730},
doi = {10.1145/1183614.1183730},
abstract = {In view of the inefficiency of the traditional two-phase Twig-Stack algorithm, we propose a single-phase holistic twig pattern matching method based on the TwigStack algorithm by applying a novel stack structure.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {786–787},
numpages = {2},
keywords = {XML, query optimization, twig query},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183731,
author = {Achtert, Elke and B\"{o}hm, Christian and Kr\"{o}ger, Peer and Kunath, Peter and Pryakhin, Alexey and Renz, Matthias},
title = {Approximate Reverse K-Nearest Neighbor Queries in General Metric Spaces},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183731},
doi = {10.1145/1183614.1183731},
abstract = {In this paper, we propose an approach for efficient approximative RkNN search in arbitrary metric spaces where the value of k is specified at query time. Our method uses an approximation of the nearest-neighbor-distances in order to prune the search space. In several experiments, our solution scales significantly better than existing non-approximative approaches while producing an approximation of the true query result with a high recall.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {788–789},
numpages = {2},
keywords = {approximative similarity search, reverse nearest neighbor},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183732,
author = {Tao, Tao and Zhai, ChengXiang},
title = {Best-k Queries on Database Systems},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183732},
doi = {10.1145/1183614.1183732},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {790–791},
numpages = {2},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183733,
author = {Eguchi, Koji and Croft, W. Bruce},
title = {Boosting Relevance Model Performance with Query Term Dependence},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183733},
doi = {10.1145/1183614.1183733},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {792–793},
numpages = {2},
keywords = {term dependence, language models, information retrieval},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183734,
author = {Nasraoui, Olfa and Cerwinske, Jeff and Rojas, Carlos and Gonzalez, Fabio},
title = {Collaborative Filtering in Dynamic Usage Environments},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183734},
doi = {10.1145/1183614.1183734},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {794–795},
numpages = {2},
keywords = {mining evolving streams, collaborative filtering, evaluation, recommendation systems},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183735,
author = {Barbosa, Luciano and Freire, Juliana},
title = {Automatically Constructing Collections of Online Database Directories},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183735},
doi = {10.1145/1183614.1183735},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {796–797},
numpages = {2},
keywords = {modular classifiers, online databases, focused web crawling},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183736,
author = {Olsson, J. Olsson Scott and Oard, Douglas W.},
title = {Combining Feature Selectors for Text Classification},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183736},
doi = {10.1145/1183614.1183736},
abstract = {We introduce several methods of combining feature selectors for text classification. Results from a large investigation of these combinations are summarized. Easily constructed combinations of feature selectors are shown to improve peak R-precision and F1 at statistically significant levels.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {798–799},
numpages = {2},
keywords = {text classification, feature selection},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183737,
author = {Cao, Guihong and Nie, Jian-Yun and Bai, Jing},
title = {Constructing Better Document and Query Models with Markov Chains},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183737},
doi = {10.1145/1183614.1183737},
abstract = {Document and query expansions have been used separately in previous studies to enhance the representation of documents and queries. In this paper, we propose a general method that integrates both of them. Expansion is carried out using multi-stage Markov chains. Our experiments show that this method significantly outperforms the existing approaches.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {800–801},
numpages = {2},
keywords = {markov chain, language model, expansion, information retrieval},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183738,
author = {Hristidis, Vagelis and Valdivia, Oscar and Vlachos, Michail and Yu, Philip S.},
title = {Continuous Keyword Search on Multiple Text Streams},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183738},
doi = {10.1145/1183614.1183738},
abstract = {In this paper we address the issue of continuous keyword queries on multiple textual streams. This line of work represents a significant departure from previous keyword search models that assumed a static database. In our model the user poses a query comprised by a collection of keywords, which is subsequently applied on multiple text streams (these can be RSS news feeds, TV closed captions, emails, etc). A result to a query is a combination of streams "sufficiently correlated" to each other that collectively contain all query keywords within a specified time span.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {802–803},
numpages = {2},
keywords = {streams, correlation, keyword search},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183739,
author = {Chu, Yi-Hong and Huang, Jen-Wei and Chuang, Kun-Ta and Chen, Ming-Syan},
title = {On Subspace Clustering with Density Consciousness},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183739},
doi = {10.1145/1183614.1183739},
abstract = {In this paper, a problem, called "the density divergence problem" is explored. This problem is related to the phenomenon that the densities of the clusters vary in different subspace cardinalities. We take the densities into consideration in subspace clustering and explore an algorithm to adaptively determine different density thresholds to discover clusters in different subspace cardinalities.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {804–805},
numpages = {2},
keywords = {subspace clustering, density divergence problem},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183740,
author = {Peng, Yefei and He, Daqing},
title = {Direct Comparison of Commercial and Academic Retrieval System: An Initial Study},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183740},
doi = {10.1145/1183614.1183740},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {806–807},
numpages = {2},
keywords = {baseline, enterprise email retrieval, Google desktop, Indri},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183741,
author = {Greco, Sergio and Ruffolo, Massimiliano and Tagarelli, Andrea},
title = {Effective and Efficient Similarity Search in Time Series},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183741},
doi = {10.1145/1183614.1183741},
abstract = {We present DSA - Derivative time series Segment Approximation, a novel representation model for time series designed for effective and efficient similarity search. DSA substantially exploits derivative estimation, segmentation and dimensionality reduction to meet at least the requirements of high sensitivity to main features (trends) of time series and robustness to outliers. Experiments show that DSA is drastically faster and still as good or better than the prominent state-of-the-art similarity methods.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {808–809},
numpages = {2},
keywords = {dynamic time warping, time series representation model, dimensionality reduction, segmentation},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183742,
author = {Kunkle, Daniel and Zhang, Donghui and Cooperman, Gene},
title = {Efficient Mining of Max Frequent Patterns in a Generalized Environment},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183742},
doi = {10.1145/1183614.1183742},
abstract = {This poster paper summarizes our solution for mining max frequent generalized itemsets (g-itemsets), a compact representation for frequent patterns in the generalized environment.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {810–811},
numpages = {2},
keywords = {max frequent itemsets, data mining},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183743,
author = {Metzler, Donald},
title = {Estimation, Sensitivity, and Generalization in Parameterized Retrieval Models},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183743},
doi = {10.1145/1183614.1183743},
abstract = {In this work we investigate three important aspects of parameterized retrieval models: estimation, sensitivity, and generalization. Since all parameterized models, even those based on heuristics, have inherent uncertainty, we study these issues using statistical tools.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {812–813},
numpages = {2},
keywords = {generalization, estimation, sensitivity},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183744,
author = {Shi, Lixin and Nie, Jian-Yun},
title = {Filtering or Adapting: Two Strategies to Exploit Noisy Parallel Corpora for Cross-Language Information Retrieval},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183744},
doi = {10.1145/1183614.1183744},
abstract = {Noisy parallel corpora have been widely used for Cross-language information retrieval (CLIR). However, the previous studies only focus on truly parallel corpus. In this paper, we examine two possible approaches to exploit noisy corpora: filtering out noise from the corpora or adapting the training process of translation model to the noise corpora. Our experiments show that the second approach is better suited to CLIR.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {814–815},
numpages = {2},
keywords = {parallel corpus, noise filtering, alignment score},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183745,
author = {Wang, Ling and Rundensteiner, Elke A. and Mani, Murali and Jiang, Ming},
title = {HUX: A Schemacentric Approach for Updating XML Views},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183745},
doi = {10.1145/1183614.1183745},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {816–817},
numpages = {2},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183746,
author = {Kadri, Youssef and Nie, Jian-Yun},
title = {Improving Query Translation with Confidence Estimation for Cross Language Information Retrieval},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183746},
doi = {10.1145/1183614.1183746},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {818–819},
numpages = {2},
keywords = {confidence estimation, CLIR},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183747,
author = {Ranganathan, Anand and Liu, Zhen},
title = {Information Retrieval from Relational Databases Using Semantic Queries},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183747},
doi = {10.1145/1183614.1183747},
abstract = {Relational databases are widely used today as a mechanism for providing access to structured data. They, however, are not suitable for typical information finding tasks of end users. There is often a semantic gap between the queries users want to express and the queries that can be answered by the database. In this paper, we propose a system that bridges this semantic gap using domain knowledge contained in ontologies. Our system extends relational databases with the ability to answer semantic queries that are represented in SPARQL, an emerging Semantic Web query language. Users express their queries in SPARQL, based on a semantic model of the data, and they get back semantically relevant results. We define different categories of results that are semantically relevant to the users' query and show how our system retrieves these results. We evaluate the performance of our system on sample relational databases, using a combination of standard and custom ontologies.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {820–821},
numpages = {2},
keywords = {ontologies, knowledge management, information retrieval, semantic queries, relational databases, semantic relevance},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183748,
author = {Liu, Shaorong and Wang, Fusheng and Liu, Peiya},
title = {Integrated RFID Data Modeling: An Approach for Querying Physical Objects in Pervasive Computing},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183748},
doi = {10.1145/1183614.1183748},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {822–823},
numpages = {2},
keywords = {sensors, RFID, pervasive computing, data modeling, EPC},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183749,
author = {Hu, Xiaohua and Zhang, Xiaodan and Zhou, Xiaohua},
title = {Integration of Cluster Ensemble and EM Based Text Mining for Microarray Gene Cluster Identification and Annotation},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183749},
doi = {10.1145/1183614.1183749},
abstract = {In this paper, we design and develop a unified system GE-Miner (Gene Expression Miner) to integrate cluster ensemble, text clustering and multi document summarization and provide an environment for comprehensive gene expression data analysis. We present a novel cluster ensemble approach to generate high quality gene cluster. In our text summarization module, given a gene cluster, our Expectation Maximization (EM) based algorithm can automatically identify subtopics and extract most probable terms for each topic. Then, the extracted top k topical terms from each subtopic are combined to form the biological explanation of each gene cluster. Experimental results demonstrate that our system can obtain high quality clusters and provide informative key terms for the gene clusters.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {824–825},
numpages = {2},
keywords = {text mining, summarization, gene cluster identification, gene cluster annotation, cluster ensemble, EM},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183750,
author = {Mokhtaripour, Alireza and Jahanpour, Saber},
title = {Introduction to a New Farsi Stemmer},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183750},
doi = {10.1145/1183614.1183750},
abstract = {In this poster, a new Farsi (also called Persian) stemmer which works without dictionary is introduced. Evaluation results show significant improvement in performance (precision/recall) of the Information Retrieval (IR) system using this stemmer.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {826–827},
numpages = {2},
keywords = {information retrieval, stemmer, Farsi, language, Persian},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183751,
author = {Bai, Bing and Kantor, Paul and Cornea, Nicu and Silver, Deborah},
title = {IR Principles for Content-Based Indexing and Retrieval of Functional Brain Images},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183751},
doi = {10.1145/1183614.1183751},
abstract = {In this paper, we explore the concept of a "library of brain images", which implies not only a repository of brain images, but also efficient search and retrieval mechanisms that are based on models derived from IR practice. As a preliminary study, we have worked with a collection of functional MRI brain images assembled in the study of several distinct cognitive tasks. We adapt several classical IR methods (inverted indexing, TFIDF and Latent Semantic Indexing(LSI)) to content-based brain image retrieval. Our results show that efficient and accurate retrieval of brain images is possible, and that representations motivated by the IR perspective are somewhat more effective than are methods based on retaining the full image information.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {828–829},
numpages = {2},
keywords = {fMRI, TFIDF, functional brain image, latent semantic indexing, inverted files, content-based retrieval},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183752,
author = {David, J\'{e}r\^{o}me and Guillet, Fabrice and Briand, Henri},
title = {Matching Directories and OWL Ontologies with AROMA},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183752},
doi = {10.1145/1183614.1183752},
abstract = {This paper presents a simple and adaptable matching method dealing with web directories, catalogs and OWL ontologies. By using a well-known Knowledge Discovery in Databases model, such as the association rule paradigm, this method has the originality to be both extensional and asymmetric. It works at the terminological level (by selecting concept-relevant terms contained in documents) and permits to discover equivalence and also subsumption relations holding between entities (concepts and properties). This method relies on the implication intensity measure, a probabilistic model of deviation from independence. Selection of significant rules between concepts (or properties) is lead by two criteria permitting to assess respectively the implication quality and the generativity of the rule. Finally, the proposed method is evaluated on two benchmarks. The first contains two conceptual hierarchies populated with textual documents and the second one is composed of OWL ontologies.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {830–831},
numpages = {2},
keywords = {semantic web, association rules, data-mining, ontology matching},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183753,
author = {Kuntschke, Richard and Kemper, Alfons},
title = {Matching and Evaluation of Disjunctive Predicates for Data Stream Sharing},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183753},
doi = {10.1145/1183614.1183753},
abstract = {New optimization techniques, e.g., in data stream management systems (DSMSs), make the treatment of disjunctive predicates a necessity. In this paper, we introduce and compare methods for matching and evaluating disjunctive predicates.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {832–833},
numpages = {2},
keywords = {predicate matching, predicate evaluation, data stream sharing, disjunctive predicates},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183754,
author = {Stanoi, Ioana and Mihaila, George and Palpanas, Themis and Lang, Christian},
title = {Maximizing the Sustained Throughput of Distributed Continuous Queries},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183754},
doi = {10.1145/1183614.1183754},
abstract = {Monitoring systems today often involve continuous queries over streaming data, in a distributed collaborative system. The distribution of query operators over a network of processors, and their processing sequence, form a query configuration with inherent constraints on the throughput it can support. In this paper we propose to optimize stream queries with respect to a version of throughput measure, the profiled input throughput. This measure is focused on matching the expected behavior of the input streams. To prune the search space we used hill-climbing techniques that proved to be efficient and effective.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {834–835},
numpages = {2},
keywords = {continuous queries, data management, query optimization, stream},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183755,
author = {Liu, Bing and Jones, Rosie and Klinkner, Kristina Lisa},
title = {Measuring the Meaning in Time Series Clustering of Text Search Queries},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183755},
doi = {10.1145/1183614.1183755},
abstract = {We use a combination of proven methods from time series analysis and machine learning to explore the relationship between temporal and semantic similarity in web query logs; we discover that the combination of correlation and cycles is a good, but not perfect, sign of semantic relationship.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {836–837},
numpages = {2},
keywords = {query log analysis, time series, query clustering, semantic similarity, web search},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183756,
author = {Zhang, Xiang and Wang, Wei},
title = {Mining Coherent Patterns from Heterogeneous Microarray Data},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183756},
doi = {10.1145/1183614.1183756},
abstract = {Microarray technology is a powerful tool for geneticists to monitor interactions among tens of thousands of genes simultaneously. There has been extensive research on coherent subspace clustering of gene expressions measured under consistent experimental settings. However, these methods assume that all experiments are run using the same batch of microarray chips with similar characteristics of noise. Algorithms developed under this assumption may not be applicable for analyzing data collected from heterogeneous settings, where the set of genes being monitored may be different and expression levels may be not directly comparable even for the same gene. In this paper, we propose a model, F-cluster, for mining subspace coherent patterns from heterogeneous gene expression data. We compare our model with previously proposed models. We analyze the search space of the problem and give a na\"{\i}ve solution for it.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {838–839},
numpages = {2},
keywords = {heterogenous data, microarray analysis, subspace clustering},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183757,
author = {Xiong, Li and Chitti, Subramanyam and Liu, Ling},
title = {K Nearest Neighbor Classification across Multiple Private Databases},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183757},
doi = {10.1145/1183614.1183757},
abstract = {Distributed privacy preserving data mining tools are critical for mining multiple databases with a minimum information disclosure. We present a framework including a general model as well as multi-round algorithms for mining horizontally partitioned databases using a privacy preserving k Nearest Neighbor (kNN) classifier.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {840–841},
numpages = {2},
keywords = {distributed databases, classification, privacy, k nearest neighbor},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183758,
author = {Badue, Claudine and Baeza-Yates, Ricardo and Ribeiro-Neto, Berthier and Ziviani, Artur and Ziviani, Nivio},
title = {Modeling Performance-Driven Workload Characterization of Web Search Systems},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183758},
doi = {10.1145/1183614.1183758},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {842–843},
numpages = {2},
keywords = {parallel query processing, performance analysis, search engines, workload characterization},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183759,
author = {Golab, Lukasz and Bijay, Kumar Gaurav and \"{O}zsu, M. Tamer},
title = {Multi-Query Optimization of Sliding Window Aggregates by Schedule Synchronization},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183759},
doi = {10.1145/1183614.1183759},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {844–845},
numpages = {2},
keywords = {persistent queries, multi-query optimization, data streams, sliding windows},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183760,
author = {Sun, Bingjun and Zhou, Ding and Zha, Hongyuan and Yen, John},
title = {Multi-Task Text Segmentation and Alignment Based on Weighted Mutual Information},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183760},
doi = {10.1145/1183614.1183760},
abstract = {Text segmentation is important for text analysis, while text alignment is to determine shared sub-topics among similar documents. Multi-task text segmentation and alignment is the extension of single-task segmentation to utilize information of multi-source documents. In this paper we introduce a novel domain-independent unsupervised method for multi-task segmentation and alignment based on the idea that the optimal segmentation and alignment maximizes weighted mutual information, mutual information with term weights. The experiment results show that our approach works well.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {846–847},
numpages = {2},
keywords = {multi-task, text alignment, weighted mutual information, text segmentation},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183761,
author = {Li, Te and Shao, Qihong and Chen, Yi},
title = {PEPX: A Query-Friendly Probabilistic XML Database},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183761},
doi = {10.1145/1183614.1183761},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {848–849},
numpages = {2},
keywords = {XML, probabilistic data models, query processing},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183762,
author = {Huang, Jen-Wei and Tseng, Chi-Yao and Ou, Jian-Chih and Chen, Ming-Syan},
title = {On Progressive Sequential Pattern Mining},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183762},
doi = {10.1145/1183614.1183762},
abstract = {When sequential patterns are generated, the newly arriving patterns may not be identified as frequent sequential patterns due to the existence of old data and sequences. In practice, users are usually more interested in the recent data than the old ones. To capture the dynamic nature of data addition and deletion, we propose a general model of sequential pattern mining with a progressive database. In addition, we present a progressive concept to progressively discover sequential patterns in recent time period of interest.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {850–851},
numpages = {2},
keywords = {sequential patterns, progressive},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183763,
author = {Yang, Yanjiang and Deng, Robert H. and Bao, Feng},
title = {Practical Private Data Matching Deterrent to Spoofing Attacks},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183763},
doi = {10.1145/1183614.1183763},
abstract = {Private data matching between the data sets of two potentially distrusted parties has a wide range of applications. However, existing solutions have substantial weaknesses and do not meet the needs of many practical application scenarios. In particular, practical private data matching applications often require discouraging the matching parties from spoofing their private inputs. In this paper, we address this challenge by forcing the matching parties to "escrow" the data they use for matching to an auditorial agent, and in the "after-the-fact" period, they undertake the liability to attest the genuineness of the escrowed data.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {852–853},
numpages = {2},
keywords = {data privacy, secure multi-party computation},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183764,
author = {Wu, H. C. and Luk, R. W. P. and Wong, K. F. and Kwok, K. L.},
title = {Probabilistic Document-Context Based Relevance Feedback with Limited Relevance Judgments},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183764},
doi = {10.1145/1183614.1183764},
abstract = {This paper presents our novel relevance feedback (RF) algorithm that uses the probabilistic document-context based retrieval model with limited relevance judgments for document re-ranking. Probabilities of the document-context based retrieval model are estimated from the top N (=20) documents in the initial retrieval. We use document-context based cosine similarity measure to find similar data for better probability estimation in order to reduce the data scarcity problem and the negative weighting problem. Our RF algorithm is promising because its mean average precision is statistically significantly better than the baseline using TREC-6 and TREC-7 data collections.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {854–855},
numpages = {2},
keywords = {document-context, model},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183765,
author = {Tomasic, Anthony and Simmons, Isaac and Zimmerman, John},
title = {Processing Information Intent via Weak Labeling},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183765},
doi = {10.1145/1183614.1183765},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {856–857},
numpages = {2},
keywords = {information intent, weak labeling, domestication},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183766,
author = {Shi, Shuming and Xing, Fei and Zhu, Mingjie and Nie, Zaiqing and Wen, Ji-Rong},
title = {Pseudo-Anchor Text Extraction for Searching Vertical Objects},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183766},
doi = {10.1145/1183614.1183766},
abstract = {This paper examines the problem of utilizing pseudo-anchor text to help ranking Web objects in vertical search. We adopt a machine learning based approach to extract pseudo-anchor text for a vertical object from its candidate anchor blocks. Experiments in academic search domain indicate that our approach is able to dramatically improve search performance.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {858–859},
numpages = {2},
keywords = {vertical search, pseudo-anchor, pseudo-URL, anchor extraction},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183767,
author = {Zhuang, Ziming and Cucerzan, Silviu},
title = {Re-Ranking Search Results Using Query Logs},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183767},
doi = {10.1145/1183614.1183767},
abstract = {This work addresses two common problems in search, frequently occurring with underspecified user queries: the top-ranked results for such queries may not contain documents relevant to the user's search intent, and fresh and relevant pages may not get high ranks for an underspecified query due to their freshness and to the large number of pages that match the query, despite the fact that a large number of users have searched for parts of their content recently. We propose a novel method, Q-Rank, to effectively refine the ranking of search results for any given query by constructing the query context from search query logs. Evaluation results show that Q-Rank gains a considerable advantage over the current ranking system of a large-scale commercial Web search engine, being able to improve the relevance of search results for 82% of the queries.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {860–861},
numpages = {2},
keywords = {ranking, query logs, search engine, relevance feedback},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183768,
author = {Cheng, Pu-Jeng and Tsai, Ching-Hsiang and Hung, Chen-Ming and Chien, Lee-Feng},
title = {Query Taxonomy Generation for Web Search},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183768},
doi = {10.1145/1183614.1183768},
abstract = {We propose an approach that organizes the search-result clusters into a hierarchical structure, called a query taxonomy, from the user's perspective. The proposed approach is based on an unsupervised classification method, which uses the dynamic Web as the training corpus. With query taxonomy, users can browse relevant Web documents more conveniently and comprehensibly. Our experimental results verify the feasibility and the effectiveness of the proposed approach to query taxonomy generation in Web search.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {862–863},
numpages = {2},
keywords = {query taxonomy, web search},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183769,
author = {Berberich, Klaus and Bedathur, Srikanta and Weikum, Gerhard},
title = {Rank Synopses for Efficient Time Travel on the Web Graph},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183769},
doi = {10.1145/1183614.1183769},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {864–865},
numpages = {2},
keywords = {web archive search, pagerank, web graph, web dynamics},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183770,
author = {Melucci, Massimo},
title = {Ranking in Context Using Vector Spaces},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183770},
doi = {10.1145/1183614.1183770},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {866–867},
numpages = {2},
keywords = {information retrieval in context},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183771,
author = {Shah, Chirag and Croft, W. Bruce and Jensen, David},
title = {Representing Documents with Named Entities for Story Link Detection (SLD)},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183771},
doi = {10.1145/1183614.1183771},
abstract = {Several information organization, access, and filtering systems can benefit from different kind of document representations than those used in traditional Information Retrieval (IR). Topic Detection and Tracking (TDT) is an example of such an application. In this paper we demonstrate that named entities serve as better choices of units for document representation over all words. In order to test this hypothesis we study the effect of words-based and entity-based representations on Story Link Detection (SLD) - a core task in TDT research. The experiments on TDT corpora show that entity-based representations give significant improvements for SLD. We also propose a mechanism to expand the set of named entities used for document representation, which enhances the performance in some cases. We then take a step further and analyze the limitations of using only named entities for the document representation. Our studies and experiments indicate that adding additional topical terms can help in addressing such limitations.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {868–869},
numpages = {2},
keywords = {document representation, named entities, topic detection and tracking, story link detection},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183772,
author = {Heinz, Christoph and Seeger, Bernhard},
title = {Resource-Aware Kernel Density Estimators over Streaming Data},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183772},
doi = {10.1145/1183614.1183772},
abstract = {A fundamental building block of many data mining and analysis approaches is density estimation as it provides a comprehensive statistical model of a data distribution. For that reason, its application to transient data streams is highly desirable. A convenient, nonparametric method for density estimation utilizes kernels. However, its computational complexity collides with the rigid processing requirements of data streams. In this work, we present a new approach to this problem that combines linear processing cost with a constant amount of allocated memory. Our approach also supports a dynamic memory adaptation to changing system resources.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {870–871},
numpages = {2},
keywords = {kernel density estimation, data streams},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183773,
author = {Ahlgren, Per and Gr\"{o}nqvist, Leif},
title = {Retrieval Evaluation with Incomplete Relevance Data: A Comparative Study of Three Measures},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183773},
doi = {10.1145/1183614.1183773},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {872–873},
numpages = {2},
keywords = {Cranfield evaluation model, TREC, rank effectiveness, incomplete judgments, retrieval effectiveness},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183774,
author = {Parthasarathy, S. and Mehta, S. and Srinivasan, S.},
title = {Robust Periodicity Detection Algorithms},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183774},
doi = {10.1145/1183614.1183774},
abstract = {Periodicity detection is an important pre-processing step for many time series algorithms. It provides important information about the structural properties of a time series. Feature vectors based on periodicity can be used for clustering, classification, abnormality detection, and human motion understanding. The periodicity detection task is not difficult in case of simple and uncontaminated signal. Unfortunately, most of the real datasets exhibit one or more of the following properties: i) non-stationarity, ii) interlaced cyclic patterns and iii) data contamination, which makes the period detection extremely challenging. A seemingly straightforward solution is to develop individual specialized algorithms for handling each case separately. However, determining if a time series is non-stationary or is contaminated in itself is an extremely difficult task. In this article, we propose generic algorithms which can detect periods in complex, noisy and incomplete datasets. The algorithm leverages the frequency characterization and autocorrelation structure inherent in a time series to estimate its periodicity. We extend the methods to handle non-stationary time series by tracking the candidate periods using a Kalman filter. We also address the interesting problem of finding multiple interlaced periodicities.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {874–875},
numpages = {2},
keywords = {pre-processing, non-stationary time series, interlaced periodicity, time series, periodicity},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183775,
author = {Chitrapura, Krishna P and Joshi, Sachindra and Krishnapuram, Raghu},
title = {Search Result Summarization and Disambiguation via Contextual Dimensions},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183775},
doi = {10.1145/1183614.1183775},
abstract = {Topic hierarchies are a popular method of summarizing the results obtained in response to a query in various search applications. However, topic hierarchies are rigid when they are pre-defined and somewhat unintuitive when they are dynamically generated by statistical techniques. In this paper, we propose an alternative approach to query disambiguation and result summarization by placing the results in set of contextual dimensions which can be viewed as facets. For the generic search scenario, we illustrate our approach by using three types of contextual dimensions, namely, concepts, features, and specializations. We use NLP techniques and a data mining algorithm to select distinct contexts.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {876–877},
numpages = {2},
keywords = {information retrieval, summarization},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183776,
author = {Ramadoss, Balakrishnan and Rajkumar, Kannan},
title = {Semi-Automatic Annotation and MPEG-7 Authoring of Dance Videos},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183776},
doi = {10.1145/1183614.1183776},
abstract = {This paper presents a system (DanVideo) that is implemented using J2SE and JMF to annotate manually the macro and micro features of the dance videos by the dance experts. As MPEG-7 has reached a matured state for the description of the multimedia structure and semantics through the Descriptors and Description Schemes, DanVideo generates a MPEG-7 instance that conforms to the MPEG-7 schema, semi-automatically and effortlessly from the dance annotations.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {878–879},
numpages = {2},
keywords = {dance videos, video annotation, agents, multimedia applications, MPEG-7 MDS},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183777,
author = {Puppin, Diego and Silvestri, Fabrizio},
title = {The Query-Vector Document Model},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183777},
doi = {10.1145/1183614.1183777},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {880–881},
numpages = {2},
keywords = {document partitioning, document model, collection selection},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183778,
author = {Liu, Shixia and Cao, Nan and Lv, Hao and Su, Hui},
title = {The Visual Funding Navigator: Analysis of the NSF Funding Information},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183778},
doi = {10.1145/1183614.1183778},
abstract = {This paper presents an interactive visualization toolkit for navigating and analyzing the National Science Foundation (NSF) funding information. Our design builds upon an improved 2.5D treemap layout and the stacked graph to contribute customized techniques for visually navigating and interacting with the hierarchical data of NSF programs and proposals. Furthermore, an incremental layout method is adopted to handle information on a large scale. The improved treemap visualization will help to visually analyze the static funding related data and the stacked graph is utilized to analyze the time-series data. Through these visual analysis techniques, research trends of NSF, popular NSF programs are quickly identified.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {882–883},
numpages = {2},
keywords = {cascaded rectangles, treemaps, time series data visualization},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183779,
author = {Zhuang, Yi and Zhuang, Yueting and Li, Qing and Chen, Lei},
title = {Towards Interactive Indexing for Large Chinese Calligraphic Character Databases},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183779},
doi = {10.1145/1183614.1183779},
abstract = {In this paper, based on a novel shape-similarity-based retrieval method, we propose an interactive partial-distance-map (PDM)- based high-dimensional indexing scheme to speed up the retrieval performance of the large Chinese calligraphic character databases. Specifically, we use the approximate minimal bounding hyper- sphere of query character to search the PDM and utilize the users' relevance feedback to refine the search process. We conduct comprehensive experiments to testify the efficiency and effectiveness of the proposed method.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {884–885},
numpages = {2},
keywords = {high-dimensional index, Chinese calligraphic character},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@inproceedings{10.1145/1183614.1183780,
author = {Dang, E. K. F. and Luk, R. W. P. and Lee, D. L. and Ho, K. S. and Chan, S. C. F.},
title = {Query-Specific Clustering of Search Results Based on Document-Context Similarity Scores},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183780},
doi = {10.1145/1183614.1183780},
abstract = {This paper presents a pilot study of query-specific clustering that uses our novel document-context based similarity scores as compared with document similarity scores. Clustering is applied to the top 1000 retrieved documents for a given query. Clustering effectiveness is evaluated based on the MK1 score for TREC-2, TREC-6 and TREC-7 test collections. Encouraging results were obtained whereby document-context clustering produces better MK1 scores than document clustering with a 95% confidence level if precision and recall are equally important.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {886–887},
numpages = {2},
keywords = {context-based model, experimentations, document clustering},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

