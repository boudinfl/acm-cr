@inproceedings{10.1109/JCDL.2019.00011,
author = {Figueira, Pablo and Bel\'{e}m, Fabiano and Almeida, Jussara M. and Gon\c{c}alves, Marcos A.},
title = {Automatic Generation of Initial Reading Lists: Requirements and Solutions},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00011},
doi = {10.1109/JCDL.2019.00011},
abstract = {Researchers who start delving into a new research area often resort to reading lists of scientific articles in order to familiarize themselves with the existing literature. Generally, these reading lists are manually built and recommended by experts. In this paper we tackle the problem of automatically generating these lists. We start by stating five requirements for a comprehensive initial reading list, four of which were previously proposed and one is a contribution of ours. We then assess the extent to which these requirements are redundant or complementary. By performing a correlation analysis on a large dataset, we find that the five requirements are indeed mostly conflicting, which suggests that simultaneously meeting all of them is a difficult task. We then perform an extensive set of experiments, comparing twenty-five different approaches to automatically generate initial reading lists, most of which are new proposals of ours which exploit learning to rank (L2R) and aggregation methods to combine multiple pieces of evidence and objectives. Our experimental results indicate that, though no method outperforms the others in all five requirements, our new L2R and aggregation methods significantly outperform the state-of-the-art when jointly considering all requirements. Moreover, we identify a subset of six new techniques which offer the best tradeoff (in a Pareto-efficient sense) across all five requirements.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {1–10},
numpages = {10},
keywords = {learning to rank, ranking aggregation, reading list recommendation},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00012,
author = {McKenna, Lucy and Debruyne, Christophe and O'Sullivan, Declan},
title = {NAISC: An Authoritative Linked Data Interlinking Approach for the Library Domain},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00012},
doi = {10.1109/JCDL.2019.00012},
abstract = {By interlinking internal Linked Data (LD) entities to related LD entities published by authoritative creators and holders of data, libraries have the potential to expose their collections to a larger audience and to allow for richer user searches. While increasing numbers libraries are devoting time to publishing LD, the full potential of these datasets has not been explored due to limited LD interlinking. In 2018 we conducted a survey which explored the position of Information Professionals (IPs), such as librarians, archivists and cataloguers, with regards to LD. Results indicated that IPs find the process of data interlinking to be a particularly challenging step in the creation of Five Star LD. Consequently, we developed NAISC, an interlinking approach designed specifically for the library domain aimed at facilitating increased IP engagement in the LD interlinking process. Our paper provides an overview of the design and user-evaluation of NAISC. Results indicated that IPs found NAISC easy-to-use and useful for creating LD interlinks.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {11–20},
numpages = {10},
keywords = {library, semantic web, interlinking, usability testing, linked data},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00013,
author = {Ambavi, Heer and Garg, Ayush and Garg, Ayush and Nitiksha and Sharma, Mridul and Sharma, Rohit and Choudhari, Jayesh and Singh, Mayank},
title = {BioGen: Automated Biography Generation},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00013},
doi = {10.1109/JCDL.2019.00013},
abstract = {A biography of a person is the detailed description of several life events including his education, work, relationships and death. Wikipedia, the free web-based encyclopedia, consists of millions of manually curated biographies of eminent politicians, film and sports personalities, etc. However, manual curation efforts, even though efficient, suffers from significant delays. In this work, we propose an automatic biography generation framework BioGen. BioGen generates a short collection of biographical sentences clustered into multiple events of life. Evaluation results show that biographies generated by BioGen are significantly closer to manually written biographies in Wikipedia. A working model of this framework is available at nlpbiogen.herokuapp.com/home/},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {21–24},
numpages = {4},
keywords = {summarization, English wikipedia, biography generation},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00014,
author = {Hahn, Udo and Duan, Tinghui},
title = {Corpus Assembly as Text Data Integration from Digital Libraries and the Web},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00014},
doi = {10.1109/JCDL.2019.00014},
abstract = {We here explore a new corpus construction workflow which exploits the inherent potential of the growing number of Digital Libraries worldwide and the ever-expanding Internet Archive. Rather than building corpora from scratch (which typically consumes a huge amount of resources), we search the Web for fragments of relevant digitized contents scattered across the world, check their digitization quality, select those digital versions with highest quality, and finally assemble from those pieces an integrated corpus with a maximum coverage of the targeted resource. As a use case within the framework of Digital Humanities, we illustrate this approach for the Allgemeine Literatur-Zeitung (General Literature Gazette, ALZ) published from 1785 to 1849, which is considered as one of the most important text collections from the Romantic Age in Germany. With lots of incomplete and overlapping fragments physically scattered over many Web sites, we started to assemble these fragments, to bind these pieces together using a homogeneous format, and thus constructed the first (almost) complete corpus of ALZ, now accessible (in XML format obeying to TEI standards) as a whole for in-depth scientific investigations.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {25–28},
numpages = {4},
keywords = {digital humanities, german romanticism, allgemeine literatur-zeitung, digital libraries, internet archive, document management},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00015,
author = {Nguyen, Thi-Tuyet-Hai and Jatowt, Adam and Coustaty, Mickael and Nguyen, Nhu-Van and Doucet, Antoine},
title = {Deep Statistical Analysis of OCR Errors for Effective Post-OCR Processing},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00015},
doi = {10.1109/JCDL.2019.00015},
abstract = {Post-OCR is an important processing step that follows optical character recognition (OCR) and is meant to improve the quality of OCR documents by detecting and correcting residual errors. This paper describes the results of a statistical analysis of OCR errors on four document collections. Five aspects related to general OCR errors are studied and compared with human-generated misspellings, including edit operations, length effects, erroneous character positions, real-word vs. non-word errors, and word boundaries. Based on the observations from the analysis we give several suggestions related to the design and implementation of effective OCR post-processing approaches.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {29–38},
numpages = {10},
keywords = {OCR post-processing, OCR errors, post-OCR text correction},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00016,
author = {Post, Colin and Chassanoff, Alexandra and Lee, Christopher A. and Rabkin, Andrew and Zhang, Yinglong and Skinner, Katherine and Meister, Sam},
title = {Digital Curation at Work: Modeling Workflows for Digital Archival Materials},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00016},
doi = {10.1109/JCDL.2019.00016},
abstract = {This paper describes and compares digital curation workflows from 12 cultural heritage institutions that vary in size, nature of digital collections, available resources, and level of development of digital curation activities. While the research and practice of digital curation continues to mature in the cultural heritage sector, relatively little empirical, comparative research on digital curation activities has been conducted to date. The present research aims to advance knowledge about digital curation as it is currently practiced in the field, principally by modeling digital curation workflows from different institutional contexts. This greater understanding can contribute to the advancement of digital curation software, practices, and technical skills. In particular, the project focuses on the role of open-source software systems, as these systems already have strong support in the cultural heritage sector and can readily be further developed through these existing communities. This research has surfaced similarities and differences in digital curation activities, as well as broader sociotechnical factors impacting digital curation work, including the degree of formalization of digital curation activities, the nature of collections being acquired, and the level of institutional support for various software environments.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {39–48},
numpages = {10},
keywords = {workflows, open-source software, digital curation},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00017,
author = {Pakstis, Julianna and Calkins, Hannah and Dobrzynski, Christiana and Lamm, Spencer and McNamara, Laura},
title = {Advancing Reproducibility through Shared Data: Bridging Archival and Library Practice},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00017},
doi = {10.1109/JCDL.2019.00017},
abstract = {At the Children's Hospital of Philadelphia (CHOP), a team of librarians and archivists is implementing a new biomedical research data archives and a data discovery catalog as part of Arcus, a multi-year strategic initiative of the CHOP Research Institute dedicated to making research data more broadly available within the institution. Arcus presents the Library Science team with the opportunity to develop new methods and solutions for addressing a number of important issues. This paper serves to highlight three areas of focus. First, archival appraisal methods will be used for CHOP's large-scale biomedical research data, which presents issues for quality data management and preservation. The Archives will employ expanded appraisal workflows to encompass strong selection and prioritization criteria before a collection is ingested, as opposed to current archival research data frameworks, which situate appraisal after a collection is acquired. Second, CHOP research data efforts will be organized as archival collections because they offer a framework for encapsulating research and mapping the relationships between a dataset, software, protocols and other contextual information critical to data reproducibility. Third, a custom descriptive metadata schema is being developed because archival item-level descriptive metadata doesn't address the complex discovery needs of diverse biomedical data. This will require addressing tension between archival arrangement and the item-level descriptive metadata required for discovery.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {49–52},
numpages = {4},
keywords = {appraisal, reproducibility, pediatric healthcare research data, data sharing, digital curation},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00018,
author = {Lu\v{c}i\'{c}, Ana and Burke, Robin and Shanahan, John},
title = {Unsupervised Clustering with Smoothing for Detecting Paratext Boundaries in Scanned Documents},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00018},
doi = {10.1109/JCDL.2019.00018},
abstract = {Digital humanities scholars are developing new techniques of literary study using non-consumptive processing of large collections of scanned text. A crucial step in working with such collections is to separate the main text of a work from the surrounding paratext, the content of which may distort word counts, location references, sentiment scores, and other important outputs. Simple heuristic methods have been devised, but are not accurate for some texts and some methodological needs. This study describes a method for paratext detection based on smoothed unsupervised clustering. We show that this method is more accurate than simple heuristics, especially for non-fiction works, and edited works with larger amounts of paratext. We also show that a more accurate detection of paratext boundaries improves the accuracy of subsequent text processing, as exemplified by a readability metric.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {53–56},
numpages = {4},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00019,
author = {Mansouri, Behrooz and Zanibbi, Richard and Oard, Douglas W.},
title = {Characterizing Searches for Mathematical Concepts},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00019},
doi = {10.1109/JCDL.2019.00019},
abstract = {Although there has been considerable interest in recent years in the development of specialized mathematical digital libraries that can index and search for mathematical documents, little is yet known about how people will search for mathematical concepts. To begin to gain some insight into that question, this paper examines the nature of queries for mathematical concepts that were created by users of a search engine. A total of 392,586 queries that contained at least one distinctive mathematical term (e.g., "Taylor Series") were identified in a two-year query log from the Parsijoo search engine; these queries were each issued in one of 69,014 search sessions. Descriptive and comparative analysis indicates that math search sessions are typically longer and less successful than general search sessions. Queries for mathematical concepts exhibit greater diversity than do queries in general - essentially the query distribution is nearly all "tail." The use of well-formed questions as queries (e.g., "How can I prove that ...") is also surprisingly common, as is the creation of queries by copying text from a document. Among the implications of this study for the design of search engines with specialized functions for handling mathematical notation are that robust support for query refinement and reformulation could prove beneficial.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {57–66},
numpages = {10},
keywords = {query log analysis, math search, user behavior},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00020,
author = {Barifah, Maram and Landoni, Monica},
title = {Exploring Usage Patterns of a Large-Scale Digital Library},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00020},
doi = {10.1109/JCDL.2019.00020},
abstract = {Log file (LF) analysis has been widely used in Interactive Information Retrieval (IIR) as a means to uncover information about users and their behaviour when searching for information. Various studies targeted small-scale IIR systems; Online Public Access Catalogue (OPAC) is an example. Instead this research considers a large-scale digital library that services heterogeneous population and provides divers digital contents. The aim of this study is to explore the hidden usage patterns (UP) of a large-scale digital library (DL) by analysing the LFs. A dataset of 28 million records obtained from the RERO Doc DL was adopted for our study. We identify and discuss three main distinct UPs: item seekers, navigators, and searchers. The UPs were characterised by session duration, session starting points, discovering content actions, functions used, and determination session points. We conclude that LF reveals valuable information that might assist DL developers and designers to improve user experiences.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {67–76},
numpages = {10},
keywords = {clustering, search experience, digital library, usage pattern, log file, interaction},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00021,
author = {Sumikawa, Yasunobu and Jatowt, Adam and Doucet, Antoine and Moreux, Jean-Philippe},
title = {Large Scale Analysis of Semantic and Temporal Aspects in Cultural Heritage Collection's Search},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00021},
doi = {10.1109/JCDL.2019.00021},
abstract = {Cultural Heritage (CH) collections store and represent numerous historical objects such as images and documents, which are often of unique type and of high cultural and historical value. While the maintained objects are typically well-understood and analyzed, relatively less is known about what type of content is actually interesting to the searching users and how they find such content. These kinds of analyses could help us to understand for what purposes users use cultural heritage collections and could lead to user intent classification and understanding. In this paper we report the results of a large-scale exploratory analysis based on a 15-month long snapshot of query logs generated at the online portal of the National Library of France. Besides understanding the nature of content search in cultural heritage collections and digital libraries, the results of our study can be used for designing content recommendation systems and could help to improve time-aware search applications.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {77–86},
numpages = {10},
keywords = {digital library, temporal query analysis, digital history},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00022,
author = {Marshall, Catherine C. and Shipman, Frank M.},
title = {The Ownership and Control of Online Photos and Game Data: Patterns, Trends, and Keeping Pace with Evolving Circumstances},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00022},
doi = {10.1109/JCDL.2019.00022},
abstract = {Using a series of media type-specific studies, we have identified social norms associated with the ownership and control of user-contributed content and data stored in online collections. These norms constrain everyday reuse of this material, as well as prefiguring public reactions to institutional archiving efforts. In this paper, we describe two studies, one of photos and the other of multiplayer games, originally run in 2010 and 2013 respectively, that we reran in 2018 to establish whether specific norms are stable or evolving. Analysis of data from 455 new participants, 235 collected via the photo study and 220 via the games study, revealed that many previous results are stable, but there are important changes too that seem to stem from increased online experience, as well as additional experience with the media type in question. Overall, the barriers to institutional archiving of collections of user-contributed online material seem to be diminishing over time. At the same time, participants exhibit a greater sensitivity to privacy issues---along with some privacy fatigue (which has resulted in a looser grasp on ownership)---and more sophisticated reasoning about potential reuse situations, both benign and malicious.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {87–96},
numpages = {10},
keywords = {privacy, multiplayer games, social norms, reuse, photos, user studies, information rights, institutional archives},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00023,
author = {Shipman, Frank M. and Monteiro, Caio D. D.},
title = {Crawling and Classification Strategies for Generating a Multi-Language Corpus of Sign Language Video},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00023},
doi = {10.1109/JCDL.2019.00023},
abstract = {Although there is considerable sign language content available online, it can be hard to locate content in a specific sign language on a particular topic. The Sign Language Digital Library (SLaDL) aims to improve access through the generation of a multi-language corpus of sign language video. SLaDL uses a combination of crawling to collect potential sign language content and applying multimodal sign language detection and identification classifiers to winnow the collected videos to those believed to be in a particular sign language. Here we compare the quantity and variety of sign language videos located via breadth-first, depth-first, and focused crawling strategies. Then we examine the accuracy of different approaches to combining textual metadata and video features for the 3-way classification task of identifying videos in American Sign Language (ASL), British Sign Language (BSL), and without-sign language. Finally, due to the high computational cost of generating the video features used for classification, we explore the tradeoffs when using a cascading classifier and when generating features based on motion in sampled frames on classifier accuracy.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {97–106},
numpages = {10},
keywords = {sign language, multimodal classification, metadata extraction, crawling techniques, video sharing, collection generation},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00024,
author = {Shen, Aili and Salehi, Bahar and Baldwin, Timothy and Qi, Jianzhong},
title = {A Joint Model for Multimodal Document Quality Assessment},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00024},
doi = {10.1109/JCDL.2019.00024},
abstract = {The quality of a document is affected by various factors, including grammaticality, readability, stylistics, and expertise depth, making the task of document quality assessment a complex one. In this paper, we explore this task in the context of assessing the quality of Wikipedia articles. Observing that the visual rendering of a document can capture implicit quality indicators that are not present in the document text --- such as images, font choices, and visual layout --- we propose a joint model that combines the text content with a visual rendering of the document for document quality assessment. Experimental results over a Wikipedia dataset reveal that textual and visual features are complementary, achieving state-of-the-art results. Further experiments on a Peer Review dataset verify the general applicability of our proposed model.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {107–110},
numpages = {4},
keywords = {textual embeddings, multimodal, document quality assessment, wikipedia articles, visual embeddings},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00025,
author = {Elekes, \'{A}bel and Stefano, Antonino Simone Di and Sch\"{a}ler, Martin and B\"{o}hm, Klemens and Keller, Matthias},
title = {Learning from Few Samples: Lexical Substitution with Word Embeddings for Short Text Classification},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00025},
doi = {10.1109/JCDL.2019.00025},
abstract = {Text classification helps to categorize large number of documents in Digital Libraries. Text classification results highly depend on the quality of labeled training data. In practice, the process of manually annotating documents is a hidden cost that is often overlooked. We propose a general preprocessing method for scenarios in which training data is scarce. It clusters semantically similar terms by including both a semantic distance measure and a probabilistic model of any task-specific term distributions. By preprocessing the training data with our method, one increases the mean classification performance of all tested classification approaches in text classification tasks having 500 or 1000 training samples. The largest observed increase is 15%. When more training samples are available, we report significant improvements in most scenarios as well.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {111–119},
numpages = {9},
keywords = {text classification, word embedding models, lexical substitution},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00026,
author = {Meuschke, Norman and Stange, Vincent and Schubotz, Moritz and Kramer, Michael and Gipp, Bela},
title = {Improving Academic Plagiarism Detection for STEM Documents by Analyzing Mathematical Content and Citations},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00026},
doi = {10.1109/JCDL.2019.00026},
abstract = {Identifying academic plagiarism is a pressing task for educational and research institutions, publishers, and funding agencies. Current plagiarism detection systems reliably find instances of copied and moderately reworded text. However, reliably detecting concealed plagiarism, such as strong paraphrases, translations, and the reuse of nontextual content and ideas is an open research problem. In this paper, we extend our prior research on analyzing mathematical content and academic citations. Both are promising approaches for improving the detection of concealed academic plagiarism primarily in Science, Technology, Engineering and Mathematics (STEM). We make the following contributions: i) We present a two-stage detection process that combines similarity assessments of mathematical content, academic citations, and text. ii) We introduce new similarity measures that consider the order of mathematical features and outperform the measures in our prior research. iii) We compare the effectiveness of the math-based, citation-based, and text-based detection approaches using confirmed cases of academic plagiarism. iv) We demonstrate that the combined analysis of math-based and citation-based content features allows identifying potentially suspicious cases in a collection of 102K STEM documents. Overall, we show that analyzing the similarity of mathematical content and academic citations is a striking supplement for conventional text-based detection approaches for academic literature in the STEM disciplines. The data and code of our study are openly available at https://purl.org/hybridPD},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {120–129},
numpages = {10},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00027,
author = {Collins, Andrew and Beel, Joeran},
title = {Document Embeddings vs. Keyphrases vs. Terms for Recommender Systems: A Large-Scale Online Evaluation},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00027},
doi = {10.1109/JCDL.2019.00027},
abstract = {Many recommendation algorithms are available to digital library recommender system operators. The effectiveness of algorithms is largely unreported by way of online evaluation. We compare a standard term-based recommendation approach to two promising approaches for related-article recommendation in digital libraries: document embeddings, and keyphrases. We evaluate the consistency of their performance across multiple scenarios. Through our recommender-system as-a-service Mr. DLib, we delivered 33.5M recommendations to users of Sowiport and Jabref over the course of 19 months, from March 2017 to October 2018. The effectiveness of the algorithms differs significantly between Sowiport and Jabref (Wilcoxon rank-sum test; p &lt; 0.05). There is a ~400% difference in effectiveness between the best and worst algorithm in both scenarios separately. The best performing algorithm in Sowiport (terms) is the worst performing in Jabref. The best performing algorithm in Jabref (keyphrases) is 70% worse in Sowiport, than Sowiport's best algorithm (click-through rate; 0.1% terms, 0.03% keyphrases).},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {130–133},
numpages = {4},
keywords = {TF-IDF, document embeddings, recommender systems, keyphrases},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00028,
author = {Breitinger, Corinna and Gipp, Bela and Wortner, Patrick and Reiterer, Harald},
title = { 'Too Late to Collaborate': Challenges to the Discovery of in-Progress Research},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00028},
doi = {10.1109/JCDL.2019.00028},
abstract = {Identifying the ongoing research in one's field is an essential yet time-intensive information seeking task. Today's digital libraries support researchers in the search and discovery of published academic research. However, they are unable to support researchers in the discovery of ongoing research, i.e., research that has not yet been published. The discovery of ongoing research thus remains a manual information seeking task lacking standardized processes or automated support systems. We present findings from an initial qualitative study on how computer science researchers from four disciplines currently go about identifying ongoing research projects within their fields and the challenges they face. A major challenge we identify is what we term the discovery-confidentiality trade-off. On the one hand, researchers express a need to discover ongoing research projects in their domain to identify collaboration partners and to avoid performing duplicate research. However, at the same time, researchers are hesitant to reveal details about their own in-progress research for fear of idea plagiarism. We discuss several key factors influencing this trade-off, such as trust and timeliness. We argue that these factors must be accounted for in the design of future academic search and recommendation solutions to support researchers in the timely identification of ongoing research.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {134–137},
numpages = {4},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00029,
author = {Fenlon, Katrina},
title = {Modeling Digital Humanities Collections as Research Objects},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00029},
doi = {10.1109/JCDL.2019.00029},
abstract = {Advancing digital libraries to increase the sustainability and usefulness of digital scholarship depends on identifying and developing data models capable of representing increasingly complex scholarly products. This paper considers the potential for an emergent model of scientific communication, the research objects data model, to accommodate the complexities of digital humanities collections. Digital humanities collections aggregate and enrich diverse sources of evidence and context, serving simultaneously as "publications" and dynamic, interactive platforms for research. The research objects model is an alternative to traditional formats of publication, facilitating aggregation and description of all of the inputs and outputs of a research process, ranging from datasets to papers to executable code. This model increasingly underpins research infrastructures in some scientific domains, yet its efficacy for representing humanities scholarship, and for undergirding humanities cyberinfrastructure, remains largely untested. This study offers a qualitative content analysis of digital humanities collections relying on a content/context analytical framework for characterizing collection components and their interrelationships. This study then maps those components and relationships into a research objects model to identify the model's strengths and limitations for representing diverse digital humanities scholarship.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {138–147},
numpages = {10},
keywords = {research objects, digital humanities, digital libraries, data models},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00030,
author = {Hienert, Daniel and Kern, Dagmar and Boland, Katarina and Zapilko, Benjamin and Mutschke, Peter},
title = {A Digital Library for Research Data and Related Information in the Social Sciences},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00030},
doi = {10.1109/JCDL.2019.00030},
abstract = {In the social sciences, researchers search for information on the Web, but this is most often distributed on different websites, search portals, digital libraries, data archives, and databases. In this work, we present an integrated search system for social science information that allows finding information around research data in a single digital library. Users can search for research data sets, publications, survey variables, questions from questionnaires, survey instruments, and tools. Information items are linked to each other so that users can see, for example, which publications contain data citations to research data. The integration and linking of different kinds of information increase their visibility so that it is easier for researchers to find information for re-use. In a log-based usage study, we found that users search across different information types, that search sessions contain a high rate of positive signals and that link information is often explored.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {148–157},
numpages = {10},
keywords = {open data, social sciences, research data, digital library},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00031,
author = {Weber, Nicholas and Norlander, Bree},
title = {Open Data Publishing by Public Libraries},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00031},
doi = {10.1109/JCDL.2019.00031},
abstract = {Public libraries in the USA are part of a broad civic information ecosystem that is rapidly adopting transparency legislation aimed at publishing structured open data for public reuse. In this preliminary results paper we look specifically at the open data publishing practices of 85 public libraries in the USA. We find that less than half of these libraries have published any open data, and that there is no relationship between revenue nor staff size and open data publishing practices. Categorizing public library open data by type we find overwhelmingly the most frequent type of open data published by libraries are geospatial (map) information. We plan use these findings to develop a proposal for public libraries to engage in publishing a core set of open data, and conclude by discussing the potential for reuse of open public library data.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {158–161},
numpages = {4},
keywords = {civics, open data, public libraries},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00032,
author = {Aturban, Mohamed and Alam, Sawood and Nelson, Michael L. and Weigle, Michele C.},
title = {Archive Assisted Archival Fixity Verification Framework},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00032},
doi = {10.1109/JCDL.2019.00032},
abstract = {The number of public and private web archives has increased, and we implicitly trust content delivered by these archives. Fixity is checked to ensure an archived resource has remained unaltered since the time it was captured. Some web archives do not allow users to access fixity information and, more importantly, even if fixity information is available, it is provided by the same archive from which the archived resources are requested. In this research, we propose two approaches, namely Atomic and Block, to establish and check fixity of archived resources. In the Atomic approach, the fixity information of each archived web page is stored in a JSON file (or a manifest), and published in a well-known web location (an Archival Fixity server) before it is disseminated to several on-demand web archives. In the Block approach, we first batch together fixity information of multiple archived pages in a single binary-searchable file (or a block) before it is published and disseminated to archives. In both approaches, the fixity information is not obtained directly from archives. Instead, we compute the fixity information (e.g., hash values) based on the playback of archived resources. One advantage of the Atomic approach is the ability to verify fixity of archived pages even with the absence of the Archival Fixity server. The Block approach requires pushing fewer resources into archives, and it performs fixity verification faster than the Atomic approach. On average, it takes about 1.25X, 4X and 36X longer to disseminate a manifest to perma.cc, archive.org, and webcitation.org, respectively, than archive.is, while it takes 3.5X longer to disseminate a block to archive.org than perma.cc. The Block approach performs 4.46X faster than the Atomic approach on verifying the fixity of archived pages.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {162–171},
numpages = {10},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00033,
author = {Alam, Sawood and Weigle, Michele C. and Nelson, Michael L. and Melo, Fernando and Bicho, Daniel and Gomes, Daniel},
title = {Mementomap Framework for Flexible and Adaptive Web Archive Profiling},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00033},
doi = {10.1109/JCDL.2019.00033},
abstract = {In this work we proposed MementoMap, a flexible and adaptive framework to summarize holdings of a web archive efficiently. We described a simple, yet extensible, file format suitable for MementoMap. We used the complete index of the Arquivo.pt comprising 5B mementos (archived web pages/files) to understand the nature and shape of its holdings. We generated MementoMaps with varying amount of detail from its HTML pages that have an HTTP status code of 200 OK. Additionally, we designed a single-pass, memory-efficient, and parallelization-friendly algorithm to compact a large MementoMap into a small one and an in-file binary search method for efficient lookup. We analyzed more than three years of MemGator (a Memento aggregator) logs to understand the response behavior of 14 public web archives. We evaluated MementoMaps by measuring their Accuracy using 3.3M unique URIs from MemGator logs. We found that a MementoMap of less than 1.5% Relative Cost (as compared to the comprehensive listing of all the unique original URIs) can correctly identify the presence or absence of 60% of the lookup URIs in the corresponding archive while maintaining 100% Recall (i.e., zero false negatives).},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {172–181},
numpages = {10},
keywords = {archive profiling, web archiving, mementomap, memento},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00034,
author = {Klein, Martin and Balakireva, Lyudmila and Shankar, Harihar},
title = {Evaluating Memento Service Optimizations},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00034},
doi = {10.1109/JCDL.2019.00034},
abstract = {Services and applications based on the Memento Aggregator can suffer from slow response times due to the federated search across web archives performed by the Memento infrastructure. In an effort to decrease the response times, we established a cache system and experimented with machine learning models to predict archival holdings. We reported on the experimental results in previous work and can now, after these optimizations have been in production for two years, evaluate their efficiency, based on long-term log data. During our investigation we find that the cache is very effective with a 70 -- 80% cache hit rate for human-driven services. The machine learning prediction operates at an acceptable average recall level of 0.727 but our results also show that a more frequent retraining of the models is needed to further improve prediction accuracy.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {182–185},
numpages = {4},
keywords = {aggregation, machine learning, memento, evaluation},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00035,
author = {Boukhers, Zeyd and Ambhore, Shriharsh and Staab, Steffen},
title = {An End-to-End Approach for Extracting and Segmenting High-Variance References from Pdf Documents},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00035},
doi = {10.1109/JCDL.2019.00035},
abstract = {This paper addresses the problem of extracting and segmenting references from PDF documents. The novelty of the presented approach lies in its capability to discover highly varying references mainly in terms of content, length and location in the document. Unlike existing works, the proposed method does not follow the classical pipeline that consists of sequential phases. It rather learns the different characteristics of references to be used in a coherent scheme that reduces the error accumulation by following a probabilistic approach. Contrary to conventional references, mentioning the sources of information in some publications, such as those of social science, is not subject to the same specifications such as being located in a unique reference section. Therefore, the proposed method aims to extract references of highly varying reference characteristics by relaxing the restrictions of existing methods. Additionally, we present in this paper a new challenging dataset of annotated references in German social science publications. The main purpose of this work is to serve the indexation of missing references by extracting them from challenging publications such as those of German social science. The effectiveness of the presented methods in terms of both extraction and segmentation is evaluated on different datasets, including the German social science set.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {186–195},
numpages = {10},
keywords = {conditional random fields, reference segmentation, random forest, reference extraction},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00036,
author = {Hamborg, Felix and Zhukova, Anastasia and Gipp, Bela},
title = {Automated Identification of Media Bias by Word Choice and Labeling in News Articles},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00036},
doi = {10.1109/JCDL.2019.00036},
abstract = {Media bias can strongly impact the individual and public perception of news events. One difficult-to-detect, yet powerful form of slanted news coverage is bias by word choice and labeling (WCL). Bias by WCL can occur when journalists refer to the same concept, yet use different terms, which results in different sentiments being sparked in the readers, such as the terms "economic migrants" vs. "refugees." We present an automated approach to identify bias by WCL that employs models and manual analysis approaches from the social sciences, a research domain in which media bias has been studied for decades. This paper makes three contributions. First, we present NewsWCL50, the first open evaluation dataset for the identification of bias by WCL consisting of 8,656 manual annotations in 50 news articles. Second, we propose a method capable of extracting instances of bias by WCL while outperforming state-of-the-art methods, such as coreference resolution, which currently cannot resolve very broadly defined or abstract coreferences used by journalists. We evaluate our method on the NewsWCL50 dataset, achieving an F1=45.7% compared to F1=29.8% achieved by the best performing state-of-the-art technique. Lastly, we present a prototype demonstrating the effectiveness of our approach in finding frames caused by bias by WCL.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {196–205},
numpages = {10},
keywords = {emotions, automated content analysis, news bias, NLP, entity perception, automated frame analysis, news slant, CAS, CAQDAS},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00037,
author = {Herrmannova, Drahomira and Pontika, Nancy and Knoth, Petr},
title = {Do Authors Deposit on Time? Tracking Open Access Policy Compliance},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00037},
doi = {10.1109/JCDL.2019.00037},
abstract = {Recent years have seen fast growth in the number of policies mandating Open Access (OA) to research outputs. We conduct a large-scale analysis of over 800 thousand papers from repositories around the world published over a period of 5 years to investigate: a) if the time lag between the date of publication and date of deposit in a repository can be effectively tracked across thousands of repositories globally, and b) if introducing deposit deadlines is associated with a reduction of time from acceptance to public availability of research outputs. We show that after the introduction of the UK REF 2021 OA policy, this time lag has decreased significantly in the UK and that the policy introduction might have accelerated the UK's move towards immediate OA1 compared to other countries. This supports the argument for the inclusion of a time-limited deposit requirement in OA policies.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {206–216},
numpages = {11},
keywords = {data mining, open access, research evaluation, research excellence framework, scholarly data, REF},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00038,
author = {Pinto, Jos\'{e} Mar\'{\i}a Gonz\'{a}lez and Wawrzinek, Janus and Balke, Wolf-Tilo},
title = {What Drives Research Efforts? Find Scientific Claims That Count!},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00038},
doi = {10.1109/JCDL.2019.00038},
abstract = {Researchers often struggle to solve a common problem: how does one know whether a research hypothesis is worth investigating? Given the increasing number of research publications, it is complicated to guide such decisions. Previous work has shown how predicting generally emerging research topics can provide some help. Yet, in specialized scientific domains, only little is known about how to provide a service that allows users to ease the identification of scientific claims worth investigating. Scientific claims here means a natural language sentence that expresses a relationship between two entities. In particular, how one of them affects, manipulates, or causes the other entity. In this paper, we propose a data-driven approach aiming at filling this gap and empowering users at query level: given the results of a query, we deliver a characterization of clusters of the query results to discover the contextualization of scientific claims and the identification of those claims that may be worth more research efforts. To do so, we cluster documents with scientific claims that share the same context by leveraging co-clustering. After that, we characterize the clusters to annotate them. Our annotation focuses on two core aspects: controversy and diversity of claims in a given cluster. Controversy arises when two or more claims semantically contradict each other; diversity means the presence of different semantics of the claims that do not contradict each other but provide different insights expressed by some paper. To evaluate the benefits of our approach, we performed an extensive retrospective analysis on PubMed.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {217–226},
numpages = {10},
keywords = {scientific claims, cluster characterization, metadata generation},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00039,
author = {Ghosal, Tirthankar and Raj, Ashish and Ekbal, Asif and Saha, Sriparna and Bhattacharyya, Pushpak},
title = {A Deep Multimodal Investigation to Determine the <i>Appropriateness</i> of Scholarly Submissions},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00039},
doi = {10.1109/JCDL.2019.00039},
abstract = {Present day peer review is a time-consuming process and is still the only gatekeeper of scientific knowledge and wisdom. However, the rapid increase in research article submissions these days across different fields is posing significant challenges to the current system. Hence the incorporation of Artificial Intelligence (AI) techniques to better streamline the existing peer review system is an immediate need in this age of rapid scientific progress. Among many, one particular challenge these days is that the journal editors and conference program chairs are overwhelmed with the ever-increasing rise in article submissions. Studies show that a lot many submissions are not well-informed and do not fit within the scope of the intended journal or conference. Here in this work, we embark on to investigate how an AI could assist the editors and program chairs to identify potential out-of-scope submissions based on the past accepted papers of the particular journal or conference. We design a multimodal deep neural architecture and investigate the role of every possible channel of information in a research article (full-text, bibliography, images) to determine its appropriateness to the concerned venue. Our approach does not involve any handcrafted features, solely depends on the past accepting activity of the venue, and thereby achieves significant performance on two real-life datasets. Our findings suggest that a system of this kind is possible and with reasonable accuracy could assist the editors/chairs in flagging out inappropriate submissions.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {227–236},
numpages = {10},
keywords = {deep learning, multimodality, scope of a journal, appropriateness of a research article, peer review},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00040,
author = {Ghosal, Tirthankar and Sonam, Ravi and Ekbal, Asif and Saha, Sriparna and Bhattacharyya, Pushpak},
title = {Is the Paper within Scope? Are You Fishing in the Right Pond? Addressing the Appropriateness of a Manuscript to a Journal in the Peer Review Workflow},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00040},
doi = {10.1109/JCDL.2019.00040},
abstract = {Outright rejection from the editors' desk, better known as pre-screening or desk-rejection is an unfortunate yet common occurrence in academic peer review. In spite of having merit, many papers are rejected from the desk merely because they are a misfit to the scope of the journal. However, this phenomena costs a considerable time of both the editors and the authors. In this work, we present an investigation towards automation of desk rejection for out-of-scope submissions. We model the problem as a binary classification decision of an article being within scope or outside. We carry our experiments on six different Elsevier Computer Science journals. Our approach based on supervised machine learning outperforms a state-of-the-art by a wide margin in terms of accuracy (at least ~8%). We believe that our proposed method is generic, and with requisite set-up could be applied to articles of other journals. An appropriate system developed with our features could also help prospective authors to check beforehand whether they are submitting to the right venue.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {237–240},
numpages = {4},
keywords = {scope detection, scholarly texts, bibliographic analysis, content analysis, desk rejection},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00-68,
author = {Wildemann, Sergej and Holzmann, Helge},
title = {Towards Temporal URI Collections for Named Entities},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00-68},
doi = {10.1109/JCDL.2019.00-68},
abstract = {Web archives represent crucial endeavors in preserving the Web from the past and provide a valuable resource for researchers of different disciplines. Due to their size, navigation in these collections is often limited to specifying an URI and the desired date. However, typical research questions often revolve around the evolution of entities instead of specific websites. Although full-text search often seems to be the first choice to look up web pages, while it provides a quick way to yield the best match with a keyword, its diversified ranking is not made for compiling reliable entity related collections. Further, it generally ignores the temporal relevance that is needed to find pages from the past, e.g., in web archives.In this paper, we present a collection of ranked resource identifiers, characterizing named entities over time. For this purpose, different datasets were collected and evaluated by comparing each with a combination of others. Benchmarked against web search engines, our approach achieves a remarkable precision of 83.3% and shows promising results for high-quality lookups and temporal collection building. To not only rely on existing datasets, we have implemented an interactive platform to get humans in the loop to expand the collection by contributing URIs, metadata and temporal information as well as to correct errors.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {241–250},
numpages = {10},
keywords = {web archives, collaborative knowledge, temporal information retrieval},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00042,
author = {Nwala, Alexander C. and Weigle, Michele C. and Nelson, Michael L.},
title = {Using Micro-Collections in Social Media to Generate Seeds for Web Archive Collections},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00042},
doi = {10.1109/JCDL.2019.00042},
abstract = {In a Web plagued by disappearing resources, Web archive collections provide a valuable means of preserving Web resources important to the study of past events ranging from elections to disease outbreaks. These archived collections start with seed URIs (Uniform Resource Identifiers) hand-selected by curators. Curators produce high quality seeds by removing non-relevant URIs and adding URIs from credible and authoritative sources, but this ability comes at a cost: it is time consuming to collect these seeds. Two main strategies adopted by curators for discovering seeds include scraping Web (e.g., Google) Search Engine Result Pages (SERPs) and social media (e.g., Twitter) SERPs. In this work, we studied three social media platforms in order to provide some insight on the characteristics of seeds generated from different sources. First, we developed a simple vocabulary for describing social media posts across different platforms. Second, we introduced a novel source for generating seeds from URIs in the threaded conversations of social media posts created by single or multiple users. Users on social media sites routinely create and share posts about news events consisting of hand-selected URIs of news stories, tweets, videos, etc. In this work, we call these posts micro-collections, whether shared on Reddit or Twitter, and we consider them as an important source for seeds. This is because, the effort taken to create micro-collections is an indication of editorial activity and a demonstration of domain expertise. Third, we generated 23,112 seed collections with text and hashtag queries from 449,347 social media posts from Reddit, Twitter, and Scoop.it. We collected in total 120,444 URIs from the conventional scraped SERP posts and micro-collections. We characterized the resultant seed collections across multiple dimensions including the distribution of URIs, precision, ages, diversity of webpages, etc. We showed that seeds generated by scraping SERPs had a higher median probability (0.63) of producing relevant URIs than micro-collections (0.5). However, micro-collections were more likely to produce seeds with a higher precision than conventional SERP collections for Twitter collections generated with hashtags. Also, micro-collections were more likely to produce older webpages and more non-HTML documents.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {251–260},
numpages = {10},
keywords = {social media, crawling, collection building, web archiving, seeds},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00043,
author = {Deschamps, Ryan and Fritz, Samantha and Lin, Jimmy and Milligan, Ian and Ruest, Nick},
title = {The Cost of a WARC: Analyzing Web Archives in the Cloud},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00043},
doi = {10.1109/JCDL.2019.00043},
abstract = {The value of web archives to support scholarship in the humanities and social sciences is slowly being realized by the increasing availability of scalable tools and platforms. The cost of providing scholarly access is a critical component of developing a long-term sustainability strategy. This paper attempts to answer a straight-forward question: How much does it cost to analyze web archives in the cloud? To make this question more concrete, we examine the creation of three derivatives (extraction of collection statistics, full text, and the webgraph) that serve as the starting points of many scholarly inquiries. Our analysis shows that these typical derivatives costs around US$7 per TB using our Archives Unleashed Toolkit. We describe in detail the methodology and assumptions made to arrive at this figure. To our knowledge, we are the first to quantify the economics of scholarly access to web archives, and we believe that this information is valuable for service planning by archives, libraries, and other institutions.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {261–264},
numpages = {4},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00044,
author = {Milligan, Ian and Casemajor, Nathalie and Fritz, Samantha and Lin, Jimmy and Ruest, Nick and Weber, Matthew S. and Worby, Nicholas},
title = {Building Community and Tools for Analyzing Web Archives through Datathons},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00044},
doi = {10.1109/JCDL.2019.00044},
abstract = {Starting in March 2016, the Archives Unleashed team and our collaborators have brought together social scientists, humanists, archivists, librarians, computer scientists, and other stakeholders to explore web archives as research objects. Three objectives motivated our team to develop and organize these events: facilitating scholarly access, community building, and skills training. We believe that we have been successful on all three fronts. For each event, over the course of two to three days, participants formed interdisciplinary teams and explored web archives using a variety of methods and tools. This paper details our experiences in designing these "datathons", with an intent to share lessons learned, highlight interdisciplinary approaches to research and education on web archives, and describe future opportunities.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {265–268},
numpages = {4},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00045,
author = {Zhang, Baichuan and Dundar, Murat and Dave, Vachik and Hasan, Mohammad Al},
title = {Dirichlet Process Gaussian Mixture for Active Online Name Disambiguation by Particle Filter},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00045},
doi = {10.1109/JCDL.2019.00045},
abstract = {The name disambiguation task partitions a collection of records pertaining to a given name, such that there is a one-to-one correspondence between the partitions and a group of people, all sharing that given name. Most existing solutions for this task are proposed for static data. However, more realistic scenarios stipulate emergence of records in a streaming fashion where records may belong to known as well as unknown persons all sharing the same name. This requires a flexible name disambiguation algorithm that can not only classify records of known persons represented in the training data by their existing records but can also identify records of new ambiguous persons with no existing records included in the initial training dataset. Toward achieving this objective, in this paper we present a non-parametric Bayesian framework that utilizes a Dirichlet Process Gaussian Mixture Model (DPGMM) as a core engine for online name disambiguation task. A Sequential Importance Sampling with Resampling (SISR) technique, also known as particle filtering, is proposed for inference to simultaneously perform online classification and new class discovery. Specifically, for each online record, we approximate its class conditional posterior distribution by a set of particles and their weights, which are updated in a sequential manner without the need to re-access previously observed records. We also propose an interactive version of our online name disambiguation method, which improves the prediction accuracy by exploiting user feedback.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {269–278},
numpages = {10},
keywords = {online name disambiguation, digital library},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00046,
author = {Katsurai, Marie and Ohmukai, Ikki},
title = {Author Matching across Different Academic Databases: Aggregating Simple Feature-Based Rankings},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00046},
doi = {10.1109/JCDL.2019.00046},
abstract = {Profiling the research accomplishments (e.g., papers, research grants, awards, and dissertations) of individual researchers is needed for evaluating their research activity and academic networking. These research accomplishments are generally scattered across separate databases that have different schema. This paper explores an approach for author matching across different academic databases, automatically integrating the records and regarding them as an individual researcher's accomplishments. Given an author identifier with a certain full name in a source database and its counterpart candidates with the same full name in a different target database, we first extract six types of simple features based on the attributes that are easily available in any type of scholarly contribution. Each feature ranks all the namesakes according to similarity with the target author. Finally, we apply unsupervised aggregation to all of the ranked lists, providing an improved ranked list to help manual inspections. In experiments that match researchers in Japan to their PhD dissertations, we demonstrate that the proposed aggregated ranking achieved the best performance over single rankings.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {279–282},
numpages = {4},
keywords = {rank aggregation, academic databases, author matching},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00047,
author = {Anthony, Poonam and Bhowmick, Plaban Kumar},
title = {Learning to Retrieve Related Resources in a Bibliographic Information Network},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00047},
doi = {10.1109/JCDL.2019.00047},
abstract = {With the advancement in semantic technologies, structured data in different domains is being modelled as knowledge graphs. Bibliographic information has proven rather acquiescent to being formalized as knowledge graphs or information networks. In this paper, we address the problem of retrieving related resources in a bibliographic information network like SciGraph 1. Discovery of path patterns representing a set of example pairs of related resources forms the basis of our solution. We have adopted a bi-directional search strategy to accommodate state-of-the-art similarity measure (viz. HeteSim), relevant to Heterogeneous Information Networks (HIN). The proposed method has been evaluated based on precision and execution time. On experimenting with different datasets (YAGO and Springer Nature SciGraph), we observe an improvement in performance, compared to forward search algorithm, which is based on Path Constrained Random Walk similarity measure.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {283–286},
numpages = {4},
keywords = {entity similarity, heterogeneous information network, meta-path, bibliographic network},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00048,
author = {Buchanan, George and McKay, Dana},
title = {One Way or Another i'm Gonna Find Ya: The Influence of Input Mechanism on Scrolling in Complex Digital Collections},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00048},
doi = {10.1109/JCDL.2019.00048},
abstract = {Creating effective interfaces for browsing is a key challenge in digital libraries. In a desktop computing-based world, large screen real estate and vertical-only scrolling were major drivers of the design of DL interfaces. Mobile computing presents new challenges and opportunities for DLs: screens are smaller, but touch screens afford interactions that have not been possible in the past. In this paper we compare the effect of input modality---touch vs. scrolling---on navigation in interfaces designed for book browsing. We specifically address effectiveness and user satisfaction with horizontal and two-dimensional scrolling, which has traditionally had low acceptability to users and thus drastically limited the information presentation options available in digital libraries.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {287–296},
numpages = {10},
keywords = {digital libraries, touch interfaces, scrolling, browsing},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00049,
author = {Hienert, Daniel and Kern, Dagmar},
title = {Recognizing Topic Change in Search Sessions of Digital Libraries Based on Thesaurus and Classification System},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00049},
doi = {10.1109/JCDL.2019.00049},
abstract = {Log analysis in Web search showed that user sessions often contain several different topics. This means sessions need to be segmented into parts which handle the same topic in order to give appropriate user support based on the topic, and not on a mixture of topics. Different methods have been proposed to segment a user session to different topics based on timeouts, lexical analysis, query similarity or external knowledge sources. In this paper, we study the problem in a digital library for the social sciences. We present a method based on a thesaurus and a classification system which are typical knowledge organization systems in digital libraries. Five experts evaluated our approach and rated it as good for the segmentation of search sessions into parts that treat the same topic.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {297–300},
numpages = {4},
keywords = {digital library, topic detection, session segmentation, task, search session},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00050,
author = {Freeman, Cole and Roy, Mrinal Kanti and Fattoruso, Michele and Alhoori, Hamed},
title = {Shared Feelings: Understanding Facebook Reactions to Scholarly Articles},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00050},
doi = {10.1109/JCDL.2019.00050},
abstract = {Research on social-media platforms has tended to rely on textual analysis to perform research tasks. While text-based approaches have significantly increased our understanding of online behavior and social dynamics, they overlook features on these platforms that have grown in prominence in the past few years: click-based responses to content. In this paper, we present a new dataset of Facebook Reactions to scholarly content. We give an overview of its structure, analyze some of the statistical trends in the data, and use it to train and test two supervised learning algorithms. Our preliminary tests suggest the presence of stratification in the number of users following pages, divisions that seem to fall in line with distinctions in the subject matter of those pages.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {301–304},
numpages = {4},
keywords = {social clicks, research community, facebook reactions, data collection, altmetrics, supervised learning, social media analytics},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00051,
author = {Mohapatra, Dattatreya and Maiti, Abhishek and Bhatia, Sumit and Chakraborty, Tanmoy},
title = {Go Wide, Go Deep: Quantifying the Impact of Scientific Papers through Influence Dispersion Trees},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00051},
doi = {10.1109/JCDL.2019.00051},
abstract = {Despite a long history of the use of 'citation count' as a measure of scientific impact, the evolution of the follow-up work inspired by the paper and their interactions through citation links have rarely been explored to quantify how the paper enriches the depth and breadth of a research field. We propose a novel data structure, called Influence Dispersion Tree (IDT), to model the organization of follow-up papers and their dependencies through citations. We also propose the notion of an ideal IDT for every paper and show that an ideal (highly influential) paper should increase the knowledge of a field vertically and horizontally. We study the structural properties of IDT (both theoretically and empirically) and propose two metrics, namely Influence Dispersion Index (IDI) and Normalized Influence Divergence (NID) to quantify the influence of a paper. Our theoretical analysis shows that an ideal IDT configuration should have equal depth and breadth (and thus minimize the NID value). We establish the superiority of NID as a better influence measure in two experimental settings. First, on a large real-world bibliographic dataset, we show that NID outperforms raw citation count as an early predictor of the number of new citations a paper will receive within a certain period after publication. Second, we show that NID is superior to the raw citation count at identifying the papers recognized as highly influential through 'Test of Time Award' among all their contemporary papers (published in the same venue).},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {305–314},
numpages = {10},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00052,
author = {Keselman, Leonid},
title = {Venue Analytics: A Simple Alternative to Citation-Based Metrics},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00052},
doi = {10.1109/JCDL.2019.00052},
abstract = {We present a method for automatically organizing and evaluating the quality of different publishing venues in Computer Science. Since this method only requires paper publication data as its input, we can demonstrate our method on a large portion of the DBLP dataset, spanning 50 years, with millions of authors and thousands of publishing venues. By formulating venue authorship as a regression problem and targeting metrics of interest, we obtain venue scores for every conference and journal in our dataset. The obtained scores can also provide a per-year model of conference quality, showing how fields develop and change over time. Additionally, these venue scores can be used to evaluate individual academic authors and academic institutions. We show that using venue scores to evaluate both authors and institutions produces quantitative measures that are comparable to approaches using citations or peer assessment. In contrast to many other existing evaluation metrics, our use of large-scale, openly available data enables this approach to be repeatable and transparent.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {315–324},
numpages = {10},
keywords = {venues, regression, citation, metrics, NSF, ranking, conferences, DBLP},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00053,
author = {Hahn, Jim and Ward, David and Cabada, Elisandro and Wallace, Rob and Kurt, Eric and Mischo, Bill},
title = {Institutionalizing and Sustaining Virtual Reality Experiences},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00053},
doi = {10.1109/JCDL.2019.00053},
abstract = {The speakers on this panel will discuss the curricular uses, partnership opportunities, infrastructure requirements, and planning results of institutionalizing for the sustained support of Virtual Reality experiences for teaching and learning in library settings. The panel will provide an opportunity for those with some familiarity of emerging technologies to extend their understanding of the field, and help develop ideas for new research and teaching areas related to information science and digital librarianship. The panel will also motivate JCDL audience participation and discussion for potential Virtual Reality development uses in research settings beyond academic libraries. Supporting Virtual Reality research, pedagogy, and content creation requires extensive cross-campus collaboration, and the session will highlight the conference theme of connecting people to achieve extraordinary results.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {325–326},
numpages = {2},
keywords = {virtual reality, policy, multimedia, research collaborations, emerging technology},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00054,
author = {He, Daqing and Wu, Dan and Graves, Wayne and Klein, Martin},
title = {Creation of a DL by the Communities and for the Communities},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00054},
doi = {10.1109/JCDL.2019.00054},
abstract = {The ACM Digital Library (DL) serves as a repository for high-quality literature on computing literature, and provides rich interlinking relationships among authors, works, institutions, and special interest groups within the ACM community. Therefore, it is important for the digital library research community to work with ACM to identify critical existing barriers and potential important directions to enable further developments of ACM DL. The goal of this panel is to initiate a collaborative relationship between the DL community and ACM DL. The panel will include two main activities: first, several presentations about recent efforts to enhance ACM's DL for better serving academic and professional communities, and second, a series of interactive discussions engaging the conference audience in defining ways to initiate more participation among the DL community in guiding the future development of ACM DL.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {327–328},
numpages = {2},
keywords = {ACM DL, digital libraries},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00055,
author = {Pride, David and Harag, Jozef and Knoth, Petr},
title = {ACT: An Annotation Platform for Citation Typing at Scale},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00055},
doi = {10.1109/JCDL.2019.00055},
abstract = {In this paper we introduce the Academic Citation Typing (ACT) Platform, a highly scalable online tool that takes as its input any full text research paper and which then enables rapid annotation and classification of in-text citations according to purpose and influence. In contrast to previous work, we employ first authors as annotators. Our evaluation shows that these authors are able to quickly classify the citations within their own papers. Over 200 authors have thus far annotated their papers using the ACT platform. This approach has already enabled the collection of the largest dataset of citations annotated according to their purposes and influence on the citing paper. Furthermore, this process is ongoing and the dataset will continue to expand following this initial phase.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {329–330},
numpages = {2},
keywords = {citation typing, open access, scholarly data, citation classification, research evaluation, data mining},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00056,
author = {Cunningham, Sally Jo},
title = {An Analysis of Arabic Language Music Queries: Design Considerations for an Arabic Music Digital Library},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00056},
doi = {10.1109/JCDL.2019.00056},
abstract = {This paper presents an analysis of 100 music related questions from the Arabic language Q&amp;A website Alsahar. The questions and the most highly rated answer for each question were automatically translated to English using Google Translate, and then the translations were verified and manually translated by a native Arabic speaker. These question/answer pairs are categorized by the types of detail used to characterize the poster's information need, the type of music information requested, and additional social and contextual elements present in the postings. We explore the music information system design implications that emerge from our analyses and classifications.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {331–332},
numpages = {2},
keywords = {multilingual music information seeking, cross-cultural music information seeking, user behaviors},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00057,
author = {Hamdi, Ahmed and Jean-Caurant, Axel and Sidere, Nicolas and Coustaty, Micka\"{e}l and Doucet, Antoine},
title = {An Analysis of the Performance of Named Entity Recognition over Ocred Documents},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00057},
doi = {10.1109/JCDL.2019.00057},
abstract = {The use of digital libraries requires an easy accessibility to documents which is strongly impacted by the quality of document indexing. Named entities are among the most important information to index digital documents. According to a recent study, 80% of the top 500 queries sent to a digital library portal contained at least one named entity [2]. However most digitized documents are indexed through their OCRed version which includes numerous errors that may hinder the access to them.Named Entity Recognition (NER) is the task that aims to locate important names in a given text and to categorize them into a set of predefined classes (person, location, organization). This paper aims to estimate the performance of NER systems through OCRed data. It exhaustively discusses NER errors arising from OCR errors; we studied the correlation between NER accuracy and OCR error rates and estimated the cost of character insertion, deletion and substitution in named entities. Results show that even if the OCR engine does contaminate named entities with errors, NER systems can overcome this issue and correctly recognize some of them.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {333–334},
numpages = {2},
keywords = {OCR, digital libraries, extraction, indexing, named entity},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00058,
author = {Wei, Jingzhu and Liu, Rui},
title = {An Approach of Constructing Knowledge Graph of the Hundred Schools of Thought in Ancient China},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00058},
doi = {10.1109/JCDL.2019.00058},
abstract = {This paper takes the creation of knowledge graph of the Hundred Schools of Thought an example to discuss the application value and realization path of knowledge graph in knowledge organization of digital humanities. We have formed the idea of establishing the knowledge graph of the Hundred Schools of Thought, which consists of four steps: knowledge representation, knowledge extraction, knowledge storage, semantic search and knowledge visualization. At present, this paper has completed the ontology construction of Confucius, Laozi and Mozi, which are the three core figures of the Hundred Schools of Thought. This knowledge graph will be further realized by using web crawler and neo4j.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {335–336},
numpages = {2},
keywords = {knowledge graph, the hundred school of thought, digital humanities},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00059,
author = {Deschamps, Ryan and Ruest, Nick and Lin, Jimmy and Fritz, Samantha and Milligan, Ian},
title = {The Archives Unleashed Notebook: Madlibs for Jumpstarting Scholarly Exploration of Web Archives},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00059},
doi = {10.1109/JCDL.2019.00059},
abstract = {This paper introduces the Archives Unleashed Notebook, which is designed to work with derivative datasets from the Archives Unleashed Cloud, a platform for analyzing web archives. These datasets contain common starting points for scholarly inquiry, including full text content and the domain-level webgraph. Our notebooks interactively walk a scholar through the process of interrogating a collection using a fill-in-the-blanks 'madlibs' approach to promote engagement. Scholars start with a notebook populated with common analyses, in which they can make minor changes to variables to alter the subject of study in systematic ways.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {337–338},
numpages = {2},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00060,
author = {Song, Ningyuan and Cheng, Hanghang and Zhou, Huimin and Wang, Xiaoguang},
title = {Argument Structure Mining in Scientific Articles: A Comparative Analysis},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00060},
doi = {10.1109/JCDL.2019.00060},
abstract = {Scientific articles serve as examples to demonstrate the standard functional units of argumentation, extracting its argument structure would benefit the knowledge discovery, representation and retrieval. This study proposes a coding schema based on the argumentation ontologies, and uses the schema to annotate 40 articles in two research field, then combines with sequential pattern mining to find argument structure model. The results expose a similar linear construction with differences in the specific argument structures of each knowledge domain. Therefore, our models have strong applicability in these two fields.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {339–340},
numpages = {2},
keywords = {models, sequential pattern mining, scientific argument},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00061,
author = {Aalberg, Trond},
title = {Branch Filtering of Tree-Structured Search Results},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00061},
doi = {10.1109/JCDL.2019.00061},
abstract = {The use of categories to filter results is a major feature in modern search systems. With the introduction of entity-centric information models for bibliographic data such as the IFLA Library Reference Model (LRM), the items returned for library search typically have an internal tree-structure and filtering needs to be applied on branches. This contribution presents a solution that is demonstrated in a search system for LRM-based data, but the approach is generic and can be adapted to the filtering of any tree-based search results and combination logic.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {341–342},
numpages = {2},
keywords = {faceted filtering, bibliographic models, user interface, search},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00062,
author = {Ricupero, Bryan and Taylor, Glory and Lehman, Amanda},
title = {Building and Enhancing Stereographic Digitial Collections: Working across Departments to Augment Reality},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00062},
doi = {10.1109/JCDL.2019.00062},
abstract = {On July 18, 2018, the University of Wyoming Libraries were awarded a grant through the State Historical Records Advisory Board (SHRAB). Funded through the National Historical Publications and Records Commission (NHPRC), the project scope includes the digitization of a multi-institutional stereoview collection, and its eventual augmentation to near 3D representation. Collection contributions will come from the William Robertson Coe Library and the Laramie Plains Museum.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {343–344},
numpages = {2},
keywords = {stereograph, digitization, stereoscope, augmented reality, metadata, collaboration, programming},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00063,
author = {Lyu, Lucheng and Han, Tao},
title = {A Comparative Study of Chinese Patent Literature Automatic Classification Based on Deep Learning},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00063},
doi = {10.1109/JCDL.2019.00063},
abstract = {Patent literature automatic classification is of great significance to literature retrieval and management. In this study, deep learning technique is used for Chinese patent literature classification, and automatic classification accuracy rates of one existing model and six deep learning models are compared. Compared with "TFIDF+Logistic Regression" model, the deep learning model has better effect of patent literature automatic classification. Furthermore, the "Word2Vec+GRU+TextCNN" model in seven models has the highest classification accuracy rate, and attention mechanism has little effect on classification results.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {345–346},
numpages = {2},
keywords = {word embedding, patent text mining, patent literature, deep learning, automatic classification},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00064,
author = {Shen, Hongzhou and Shi, Junpeng},
title = {CrowdEIM: Crowdsourcing Emergency Information Management Tasks to the Mobile Social Media Users},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00064},
doi = {10.1109/JCDL.2019.00064},
abstract = {Crowdsourcing some of the emergency information management (EIM) tasks to the general public has been proven to be effective during emergency events. In order to speed up the diffusion of EIM crowdsourcing tools and quickly gather more participants, this research proposed a solution that takes full advantages of mobile social media platform and its users. Specifically, a mobile crowdsourcing tool named CrowdEIM was designed based on WeChat open platform, and the paper prototypes were evaluated through user study. The findings indicated that the functions of CrowdEIM were clear and easy-to-use. However, there are also some features need to be improved, especially the classification and display of emergency information.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {347–348},
numpages = {2},
keywords = {WeChat, crowdsourcing, user study, emergency information management, mobile social media},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00065,
author = {Kiesel, Johannes and Hubricht, Fabienne and Stein, Benno and Potthast, Martin},
title = {A Dataset for Content Error Detection in Web Archives},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00065},
doi = {10.1109/JCDL.2019.00065},
abstract = {The World Wide Web is the single largest repository of digital culture and knowledge. Given both ubiquitous availability and the constant stream of new and noteworthy content competing for the attention of web users, one can easily miss that, at any time, reams of "old" content disappears---because web pages as well as entire websites are updated, restructured, and deleted. To preserve this content, large-scale web archiving initiatives have been started [2, 7]. The Internet Archive1 is among the most prominent, long-lasting, and largest of such organizations; as of April 2019, its web archive collection surpassed the gigantic amount of 730 billion archived web pages.2},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {349–350},
numpages = {2},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00066,
author = {Wu, Mei Mei and Liu, Ying-Hsang},
title = {Exploring Ontologies for Collection Protection in Second Sino-Japanese War},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00066},
doi = {10.1109/JCDL.2019.00066},
abstract = {The actions are usually trivial, implicit yet significant for the protection of collection during the wartime. Records of those actions, events, and particularly, the related persons, organizations may be scattered but deserve further attention since the records are precious to human knowledge protection efforts. Searching, identifying and collecting the related resources from the scattered data sets is not as complicated as developing an ontology for this domain of collection protection during the wartime period. This study has selected 1939--41, in the Second Sino-Japanese War during WWII when there were figures working on the collection protection projects that have been documented in the historical archives and many other resources, including oral history, journals, digital documentation, conference papers, literature reviews, etc. The preliminary ontology models for the protection of wartime period collection have been developed based on one of the oral history. The current research has further tuned and finalized the ontology of collection protection during the wartime period by analyzing the multi-sets of data resources. The implications of the research results are two folds: firstly, it could shed lights on the classification system for a digital library of this domain, and secondly, it could benefit the historical researchers for providing new clues in this line of collection protection during the wartime period research.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {351–352},
numpages = {2},
keywords = {cultural heritage, ontologies, collection protection, digital scholarship},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00067,
author = {Zhao, Rongying and Li, Xinlai and Li, Danyang},
title = {Research on Scientific Strategy of Google Based on Funded Papers},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00067},
doi = {10.1109/JCDL.2019.00067},
abstract = {Science funding plays a guiding role in the development of science innovation. Enterprises influence the development of science and technology (S&amp;T) through offering their support to various research. Thus, their development strategy in S&amp;T can be revealed by analyzing their funded papers. Taking Google as the example, the paper proposes an analysis method of funded papers by the combination of co-word analysis, clusters analysis and social network analysis, so as to explore the strategy and collaboration preference. The total 2,162 valid bibliometric records from the Web of Science (WOS) are divided into four groups according to discipline clusters using Hierarchical Clustering Algorithm (HCA) in the study. Then social network analysis is conducted to detect the communities among keywords and institutions. The results demonstrate that the major factor of deciding the support fields that they support is the business area where the enterprises located. It can be founded that Google's development strategy in S&amp;T not only includes the traditional areas like AI, information retrieval, data management, computer algorithm, human behaviour, and social systems, but also includes the emerging area like biological gene, ecological environment, economic phenomena forecast, economic behaviour, disease transmission trends and cancer treatment.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {353–354},
numpages = {2},
keywords = {S&amp;T strategy, hierarchical cluster analysis, collaboration preferences, co-word analysis, funded paper, social network analysis},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00068,
author = {Jayawardana, Yasith and Jayarathna, Sampath},
title = {DFS: A Dataset File System for Data Discovering Users},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00068},
doi = {10.1109/JCDL.2019.00068},
abstract = {Many research questions can be answered quickly and efficiently using data already collected for previous research. This practice is called secondary data analysis, and has gained popularity due to lower costs and improved research efficiency. In this paper we propose DFS, a file system to standardize the metadata representation of datasets, and DDU, a scalable architecture based on DFS for semi-automated metadata generation and data recommendation on the cloud, and explores their implications on datasets stored in digital libraries.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {355–356},
numpages = {2},
keywords = {meta data, data recommendation, data discovering users},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00069,
author = {Joo, Soohyung and Oh, Kyong Eun},
title = {Differences in the Research Domains of Knowledge Organization between Academic Researchers and Library Practitioners: Preliminary Results},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00069},
doi = {10.1109/JCDL.2019.00069},
abstract = {This poster presents preliminary findings from a study that explores the research domain in the field of Knowledge Organization (KO) by comparing the publications contributed by academic researchers and library practitioners. We analyzed 914 articles in three representative KO journals using text mining. The preliminary findings reveal the distinct gaps in research topics between the two groups. Academic researchers contributed more to the realm of theories and fundamental concepts while library practitioners were more interested in specific library practices.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {357–358},
numpages = {2},
keywords = {academic researchers, knowledge organization, domain analysis, library practitioners, knowledge analysis, LDA topic modeling},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00070,
author = {Siriaraya, Panote and Sakata, Haruka and Wang, Yuanyuan and Kawai, Yukiko},
title = {A Document Summary Method Based on the Relative Spatial Length between Sentences},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00070},
doi = {10.1109/JCDL.2019.00070},
abstract = {This work presents a document summary method which utilizes the relative position between sentences to generate summaries of documents with different levels of details for users. Sentence sets with different spatial distances centered around the searched keywords are extracted and used to generate the summaries. Afterward, we discuss the results of a feasibility study carried out on Japanese news articles, highlighting how this approach could be used to generate summaries with different levels of details.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {359–360},
numpages = {2},
keywords = {document analysis, text summation, document search},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00071,
author = {Organisciak, Peter and Therrell, Grace and Ryan, Maggie and Schmidt, Benjamin MacDonald},
title = {Examining Patterns of Text Reuse in Digitized Text Collections},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00071},
doi = {10.1109/JCDL.2019.00071},
abstract = {Repeating text presents challenges to access, retrieval, and mining of large-scale digital libraries. We discuss the various forms of duplicate works in scanned text collections and show the patterns of how those variations manifest within the structure of a book. Understanding these structural similarities is an important step to aligning and controlling variant and duplicate works in digital libraries.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {361–362},
numpages = {2},
keywords = {digital libraries, text structure, text mining},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00072,
author = {Deng, Shengli and Jiang, Yuting and Xia, Sudi},
title = {Exploring the Impact of Behaviors on User's Popularity in Sqa Community},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00072},
doi = {10.1109/JCDL.2019.00072},
abstract = {Based on social exchange theory, this study explored the participation behavior that affects user popularity. The experimental data were collected from Zhihu.com, the most popular online social question &amp; answer community in China. The results from a multiple linear regression model indicated that the effect of hosting live broadcasts on improving user popularity is the most significant. Moreover, answering questions, writing articles and asking questions also have a significant impact on users' popularity. But users' non-content contribution behavior, such as sharing content, and participation in live broadcasts has little effect on increasing user popularity. The findings can help guide users to adjust their behavior in the knowledge-sharing community to become more popular, thus promoting their continued contribution behavior. In addition, inspired by the research results, this paper recommends more effective knowledge service model of digital library.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {363–365},
numpages = {3},
keywords = {user popularity, knowledge popularize mode, multiple linear regression, user activity},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00073,
author = {Iddriss, Zainab and Sarraj, Amirah Al},
title = {Exploring Trends in Open Access Repositories: The Case of Higher Education Institutions in Nigeria, Ghana, Cabo Verde, and Senegal},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00073},
doi = {10.1109/JCDL.2019.00073},
abstract = {The use of electronic resources in academic research remains low in many developing countries for various reasons, including inadequate funding [1]. Access to these resources are often limited by high prices, resulting in a wide range of scholarly research not reaching its full potential to impact on people's lives. The Open Access (OA) movement developed in response to such conditions, advocating worldwide unrestricted access to scholarly publications [2]. OA resources are thus a major enabler of open science because they empower researchers to share their ideas globally and provide an economical solution to developing countries [3].One way for academic libraries to provide more OA resources to their communities is to develop institutional repositories (IR). These repositories are a platform for collecting, storing, and providing access to an institution's own research output [4]. This gives them intellectual control over locally published works, instead of relinquishing rights to a third-party publisher. IRs can create higher visibility for the institution and lower-barrier access to scholarly literature [4].Considering these merits, why haven't more academic institutions in West Africa shown more involvement in institutional repositories? As of February 2019, according to statistics on OpenDOAR [5] there are only 30 OA institutional repositories in four West African countries, namely: Nigeria (21), Ghana (5), Cabo Verde (2), and Senegal (2). However, the four countries combined have hundreds of higher education institutions, with many in Ghana and Nigeria recognised by their national accreditation bodies. There are 152 accredited tertiary institutions in Nigeria [6] and 200 in Ghana [7]. It was difficult to find an accurate source for a clear number of accredited institutions in Cabo Verde and Senegal, although some sources listed 8 universities existing in the former [8] and up to 75 in the latter [9]. These numbers bring up questions about the level of awareness of Open Access resources in West Africa.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {366–367},
numpages = {2},
keywords = {Senegal, open access (OA), Ghana, OpenDOAR, cabo verde, institutional repositories, Nigeria, West Africa, digital resources},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00074,
author = {Newton, Glen and Korol, Oksana and L\'{e}vesque, Andr\'{e} and Favrin, Robert and Graefenham, Tom},
title = {Extracting Pest Risk Information from Risk Assessment Documents},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00074},
doi = {10.1109/JCDL.2019.00074},
abstract = {Outbreaks of plant pests and pathogens have the potential to significantly harm the Canadian economy, damage the environment, detrimentally affect the health of citizens, and threaten national food security. Loss of trees caused by the Emerald Ash Borer pest had a significant public health impact due to an increase in mortality related to cardiovascular and respiratory-tract illness [2]. Outbreaks of UG99 wheat rust strain can cause up to 100% crop losses of wheat and is viewed as a potential worldwide disaster with massive implications for global food security [6]. In Canada, an outbreak of Potato Wart disease in 2000 resulted in $280 million in costs to the provincial economy of Prince Edward Island [1].The Canadian Food Inspection Agency (CFIA) has completed decades of risk analyses for various crop commodities and specific pests/pathogens. These analyses characterize the risk of exotic pests to Canada and support regulatory decision-making and response actions. This information is currently stored and maintained in a myriad of disconnected databases, spreadsheets, and technical documents under the control of the Plant Health Risk Assessment Unit. This private corpus represents a unique source of invaluable analyses tailored to the Canadian context, which are not found anywhere else in the world. Additional risk information for other countries is drawn from the European and Mediterranean Plant Protection Organization (EPPO)1 global database of country regulated pests and the U.S. Department of Agriculture\^{a}\u{A}undefineds (USDA) Animal and Plant Health Inspection Service (APHIS) regulated plant pest list2.The cost of next generation sequencing (NGS) of DNA has decreased exponentially [10]. As a consequence, academic institutions are aggressively staffing positions in environmental metagenomics, and while these researchers may not be specifically looking for plant pests or pathogens, or even human pathogens, large volumes of data with a wide range of species names will be automatically generated and be made publicly available and searchable on the web. The generation of large amount of DNA sequence data from Canadian samples creates exciting opportunities. At the same time, there is an inherent risk to trade should a quarantine organism in a Canadian sample be detected that is not currently known to exist in Canada. Interpretation or misinterpretation of species identification from such studies at their face value could potentially trigger trade issues.The study seeks to evaluate the benefits and means to actively link quantitative Next-Generation Sequencing (NGS) sample data with qualitative risk assessment information. This will guide policy and operational decision-making for rapid detection, surveillance and response against plant health threats and to set a new scientific standard for knowledge-based international trade actions for the agriculture, forestry and fishing industries.One of the goals of this study is to evaluate the use of natural language processing and machine learning techniques to extract pest risk information from the CFIA risk corpus. This study is limited to fungi that are pathogens, partially to limit its initial scope. The fungi are taxonomic group that arguably have some of the most complex taxonomic naming problems due to (among other things) the anamorph-teleomorph-holomorph[8] issue3. Collecting the fungal names synonyms is needed to canonicalize the fungal names found both in the risks sources and the NGS identification results.Previous work on risk has not looked at risk such as in this study which examines the risk of pathogenic species on the trade and agriculture. This previous work has looked at the broader topic of event extraction for decision support[3], corporate risk[4]. More recent work has examined risk in a more granular fashion, such as supplier selection[7] and more specifically, assessing the likelihood (risk) of child labour by suppliers[9],.The poster will provide early results of this stage of the project, primarily the risk extraction from the CFIA (Canadian) corpus, the risk and organism names for other countries from the EPPO and the USDA, and the collection and rationalization of the fungal synonym naming from Mycobank and Index Fungorum.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {368–369},
numpages = {2},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00075,
author = {Cong, Ting and Chen, Bikun and Ming, Wei},
title = {Global Science Discussed in Local Social Media: WeChat and Its Comparison with Web of Science},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00075},
doi = {10.1109/JCDL.2019.00075},
abstract = {Communicating scientific research on social media has become increasingly popular in recent years. While most current studies focus on global science discussed in global platforms, global top research discussed in local social media platforms were seldom studied. Our research collects data from Nature WeChat Official Account, which is one of the most famous global scientific brands in China. Statistical results show that there is very weak negative correlation of usage counts of the same articles with different languages between WeChat and Web of Science(WoS). To better understand what Chinese social media users' interest about global science, the Top100 articles in both platforms are further explored. Our content analysis shows that the articles to highlight the surprising facts of the paper without professional terms in titles are more attractive for local social media users than WoS users. And the agenda settings by WeChat account also have obvious impact on social media users' attention.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {370–371},
numpages = {2},
keywords = {nature, WeChat, scientific communication, social media, web of science},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00076,
author = {Veytsman, Boris},
title = {How to Measure the Consistency of the Tagging of Scientific Papers?},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00076},
doi = {10.1109/JCDL.2019.00076},
abstract = {Boris Feldman, a formidable Principal University Librarian I have met in 1980s, loved to say that a scientific library without an index is a huge pile of used paper. To make such index we add tags to the publications: concepts, topics, keywords, etc., often by computers. It is customary to test machine produced tags by comparing them to a "golden set" of manually tagged papers. There are certain problems with this approach. First, human tagging is expensive---even more so for scientific papers. Second, even the best human taggers' results are inconsistent [9, 10]. The situation is exacerbated when the tagging dictionary is large. For example, the popular US National Library of Medicine database of Medical Subject Headings (MeSH, https://www.nlm.nih.gov/mesh/) has about 30 000 entries. A superset of MeSH, Unified Medical Language System (UMLS, https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/release/statistics.html) contains a staggering amount of 3 822 832 distinct concepts. It is doubtful a human can do a good job choosing the right tags from such a huge collection.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {372–373},
numpages = {2},
keywords = {topic modeling, tagging evaluation, tagging},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00077,
author = {Ghosal, Tirthankar and Chakraborty, Ananya and Sonam, Ravi and Ekbal, Asif and Saha, Sriparna and Bhattacharyya, Pushpak},
title = {Incorporating Full Text and Bibliographic Features to Improve Scholarly Journal Recommendation},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00077},
doi = {10.1109/JCDL.2019.00077},
abstract = {Selecting an appropriate venue to communicate one's research is the very first step in scholarly communication. Many papers are simply rejected from the editor's desk on not being submitted to the right journal. Existing journal recommender systems extract keywords only from the title and abstract sections of candidate articles and produce journal recommendations based on their weighted-match with a domain-specific vocabulary. Here in this work, we investigate a simple yet effective approach by incorporating additional information from bibliography and body section of academic manuscripts and show their potency to yield a better recommendation. On a closed set of ten different journals, our content-based recommender achieves significant improvement over the usual baselines (at least ~ 10%). Our preliminary approach is simple yet promising and if suitably applied could efficiently recommend journals from a larger pool as well.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {374–375},
numpages = {2},
keywords = {scholarly communications, full-text processing, journal recommendation},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00078,
author = {Srinivasan, Krishna and Tan, Marcus Lee Kiang and Peh, Derrick Chin Kiat and Goh, Dion Hoe-Lian and Lee, Chei Sian},
title = {Information Seeking in Pokemon Go: A Preliminary Study},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00078},
doi = {10.1109/JCDL.2019.00078},
abstract = {This paper presents a preliminary study of issues pertaining to the information needs and sources of Pokemon Go players. A semi-structured interview revealed three categories of information needs. Interestingly, participants were found to be passive in the fulfillment of their information needs, relying on friends, and online chat groups as major sources of information. There was also a surprisingly high trust in these information sources.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {376–377},
numpages = {2},
keywords = {information behavior, information sources, information needs, Pokemon Go, mobile games},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00079,
author = {Agata, Teru and Ueda, Shuichi},
title = {Comparison of Digitized Book Index among Japan, the United States, and the United Kingdom},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00079},
doi = {10.1109/JCDL.2019.00079},
abstract = {Some reports of publishers' sales data on trends in e-books have been released, but no attempts have been made to precisely indicate the particular titles of the printed versions that were digitized. Therefore, the Digitized Book Index (DBI) has been developed, comprising titles in the national catalog databases of Japan, the United States, and the United Kingdom. The DBI is composed of titles listed at major e-book sites, on e-book platforms, and in electronic libraries. It was found that: 1) the DBI is generally increasing in volume; 2) there are differences among the three countries; and 3) the DBIs are different according to the classification item.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {378–379},
numpages = {2},
keywords = {library catalogs, Google, bibliometrics, open data, e-books},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00080,
author = {Thomas, Cristel G. and Klein, Martin},
title = {Less than 10% of Library Service Users Ask for Help},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00080},
doi = {10.1109/JCDL.2019.00080},
abstract = {User engagement and support is getting increasing attention in the world of institutional repositories as user-driven improvements are becoming the norm [4]. As the Los Alamos National Laboratory Research Library prepared for an interface overhaul intended to modernize current online tools, we developed a pipeline to track and analyze user feedback. We focused our efforts on emails specific to one tool sent to the helpdesk, from its launch in March 2012 to November 2018. We analyzed both email volume and email contents and found that while email volume is stable over time, contents vary in correlation to changes in tool workflows, capabilities, and policies dictating tool use. Our developed methods offer insights into user behavior and guidance for future tool development.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {380–381},
numpages = {2},
keywords = {user-driven development, user feedback analysis},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00081,
author = {Joo, Soohyung and Peters, Christie},
title = {Librarians' Perceptions on Skills/Knowledge and Resources Needed for Research Data Services: Preliminary Results},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00081},
doi = {10.1109/JCDL.2019.00081},
abstract = {This poster presents partial findings from a survey study that explored academic librarians' perceptions on research data services. Preliminary results reveal that librarians perceive knowledge and skills related to education and consultation more important than technical skills, and they find collaboration with other organizations on campus and full-time data librarians the most important resources needed for research data services.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {382–383},
numpages = {2},
keywords = {academic libraries, data librarianship, research data services, skills and resources for data services, RDS},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00082,
author = {Deng, Shengli and Xia, Sudi and Fu, Shaoxiong},
title = {Measuring the Interdisciplinary Degree of Information Behavior Research},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00082},
doi = {10.1109/JCDL.2019.00082},
abstract = {This article explores the interdisciplinarity of this field using co-word analysis and diversity measures based on 5299 publications related to information behavior research. Findings show that information behavior research is becoming more interdisciplinary in recent ten years. The interdisciplinary collaborations mostly exist in neighboring disciplines, while collaborations between different disciplines are relatively few. In addition, the strength of interdisciplinary collaborations needs to be enhanced in the future. Suggestions are made to promote interdisciplinary collaborations of this field, which is beneficial for researchers of digital library to provide personalized service to meet user need.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {384–385},
numpages = {2},
keywords = {interdisciplinarity, diversity measures, co-word analysis},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00083,
author = {Lauscher, Anne and Song, Yide and Gashteovski, Kiril},
title = {MinScIE: Citation-Centered Open Information Extraction},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00083},
doi = {10.1109/JCDL.2019.00083},
abstract = {Acknowledging the importance of citations in scientific literature, in this work we present MinScIE, an Open Information Extraction system which provides structured knowledge enriched with semantic information about citations. By comparing our system to it's original core, MinIE, we show that our approach improves extraction precision by 3 percentage points.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {386–387},
numpages = {2},
keywords = {open information extraction, scitorics, citation purpose, citation analysis, citation polarity, suppport vector machines},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00084,
author = {Ying, Minglei and Zhou, Lihong and Huang, Ruhua},
title = {Mistaken Identity: Knowledge Service in China's Academic Libraries},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00084},
doi = {10.1109/JCDL.2019.00084},
abstract = {The term knowledge service (KS) appeared in China's library field in approximately 2000 and has now been widely provided in China's academic libraries [1]. It is believed that traditional information services are overly generic, ineffective, and obsolete; whereas KS is much more elevated, intelligent and effective in responding to users' requirements [1]. KS can be simply defined as the service of knowledge, not information [2]. It offers golden opportunities for libraries to promote and reinvent themselves to survive in the knowledge economy [3]. KS is deeply rooted in the principles and theories of knowledge management and Nonaka and Takeuchi's SECI model should be adopted as the basis for KS [4].However, the philosophical basis of library KS is neither completely rational nor convincing. Two basic philosophical assumptions underpin KS. The first believes that knowledge can be viewed as an object and as an absolute and universal truth. Hence, knowledge can be stored and manipulated and is separated from the knower [5]. This view has been constantly criticised and challenged by the practice-based view of knowledge, which posits that knowledge cannot be easily transferred and disseminated. Instead, knowledge is generated, shared, and utilised through continuous interaction with the social and physical world rather than being transferred as an object [6]. If this notion is true, then KS is not valid because the transfer of knowledge through a simple handover is virtually impossible.This poster presents a literature analysis, which aims to provide a holistic overview of the KS research field in China. Specifically, this research aims to identify the main themes, correlations and trend of development in KS research field.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {388–389},
numpages = {2},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00085,
author = {Zhang, Zhixiong and Liu, Huan and Ding, Liangping and Wu, Pengmin and Yu, Gaihong},
title = {Moves Recognition in Abstract of Research Paper Based on Deep Learning},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00085},
doi = {10.1109/JCDL.2019.00085},
abstract = {The purpose of this work is to explore the applicability and effectiveness of deep learning methods for the task------moves recognition in abstract of research paper. We firstly build a large corpus for moves recognition. Then we choose the traditional machine learning method SVM as a benchmark, and develop four moves recognition methods based on DNN, LSTM, Attention-BiLSTM and BERT. Finally, we design two groups of experiments with sample size 10,000 and 50,000 and then compare experimental results. The results show that most of the deep learning methods outperform the traditional machine learning method SVM especially in large-scale sample experiments, in which the BERT with a re-pre-trained model achieves the best results in both groups of experiments. Deep learning methods are proved applicable and effective for moves recognition in research paper abstracts.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {390–391},
numpages = {2},
keywords = {neural network, moves recognition, deep learning, research paper},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00086,
author = {Ghosal, Tirthankar and Dey, Debomit and Dutta, Avik and Ekbal, Asif and Saha, Sriparna and Bhattacharyya, Pushpak},
title = {A Multiview Clustering Approach to Identify <i>out-of-Scope</i> Submissions in Peer Review},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00086},
doi = {10.1109/JCDL.2019.00086},
abstract = {Despite criticisms, peer review is still the only widely accepted method for research validation. The first stage in academic peer review begins at the editor's desk where one essential job of the editor is to identify and reject inappropriate out-of-scope submissions. Here in this work, we investigate if we could assist the editor in identifying potential out-of-scope submissions. We view a paper from multiple perspectives and devise a multiview clustering approach to group in-scope and out-of-scope articles. Our semi-supervised approach requires less training data yet achieves high performance. Our initial investigation yields promising results and has the potential to reduce the first turn-around time for journal submissions.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {392–393},
numpages = {2},
keywords = {peer review, desk rejection, multiview clustering},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00122,
author = {Su, Xuan and Prasad, Animesh and Kan, Min-Yen and Sugiyama, Kazunari},
title = {Neural Multi-Task Learning for Citation Function and Provenance},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00122},
doi = {10.1109/JCDL.2019.00122},
abstract = {Citation function and provenance are two cornerstone tasks in citation analysis. Given a citation, the former task determines its rhetorical role, while the latter locates the text in the cited paper that contains the relevant cited information. We hypothesize that these two tasks are synergistically related, and build a model that validates this claim. For both tasks, we show that a single-layer convolutional neural network (CNN) outperforms existing state-of-the-art baselines. More importantly, we show that the two tasks are indeed synergistic: by jointly training both tasks using multi-task learning, we demonstrate additional performance gains.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {394–395},
numpages = {2},
keywords = {citation analysis, neural networks, multi-task learning},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00087,
author = {Cain, Brian J. and Klein, Martin and Finnell, Joshua},
title = {Nucleus: Deploying Research Data Management Infrastructure at the Los Alamos National Laboratory},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00087},
doi = {10.1109/JCDL.2019.00087},
abstract = {Research data management (RDM) efforts, including the implementation of tools, development of best practices, and training of scholars, have taken center stage in many academic libraries. Evaluating and serving the RDM needs at federally funded organizations have also become a priority since the release of the 2013 U.S. Office of Science and Technology Policy memo. At the Los Alamos National Laboratory, a U.S. Department of Energy laboratory, the Research Library has launched a collaborative data management pilot called "Nucleus", based on a local installation of the open source software Open Science Framework. In this poster we present a preliminary assessment of Nucleus' implementation, including user feedback and lessons learned.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {396–397},
numpages = {2},
keywords = {open science, research data management, data collaboration},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00088,
author = {Yu, Qianqian and Zhang, Jianyong and Qian, Li and Dong, Zhipeng and Huang, Yongwen and Jianhua, Liu},
title = {Practice of Constructing Name Authority Database Based on Multi-Source Data Integration},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00088},
doi = {10.1109/JCDL.2019.00088},
abstract = {Name authority is a common issue in digital library. This paper mainly summarizes the practice of constructing name authority database based on multi-source data in NSTL. Firstly, we load, integrate different source data and convert them into unified structure. Then, we extract scientific entities and relationships from these data, according to metadata model. For different entities, we use different disambiguation rules and algorithms. As a result, we have constructed author name authority database, institution authority database, journal authority database, and fund authority database. Compared with Incites, taking six institutions name authority data as a sample, the result shows that the average accuracy can reach 86.8%.1},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {398–399},
numpages = {2},
keywords = {multi-source, name disambiguation, NSTL, name authority},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00089,
author = {Shaikh, Abdul Rahman and Alhoori, Hamed},
title = {Predicting Patent Citations to Measure Economic Impact of Scholarly Research},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00089},
doi = {10.1109/JCDL.2019.00089},
abstract = {A crucial goal of funding research and development has always been to advance economic development. On this basis, a considerable body of research undertaken with the purpose of determining what exactly constitutes economic impact and how to accurately measure that impact has been published. Numerous indicators have been used to measure economic impact, although no single indicator has been widely adapted. Based on patent data collected from Altmetrics we predict patent citations through various social media features using several classification models. Patents citing a research paper implies the potential it has for direct application in its field. These predictions can be utilized by researchers in determining the practical applications for their work when applying for patents.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {400–401},
numpages = {2},
keywords = {patents, altmetrics, social media, economic impact},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00090,
author = {Kyozuka, Momo and Xu, Yang and Tajima, Keishi},
title = {A Ranking Method for Relaxed Queries in Book Search},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00090},
doi = {10.1109/JCDL.2019.00090},
abstract = {In this paper, we propose a ranking method for keyword-based book search systems. Because we do not have full text data of all books, we use a database of brief descriptions of books. Such a brief description of the target book may not include all the query keywords given by the user. In addition, a query formulated by a user based on a vague memory may include wrong keywords. In both cases, the target book does not match with the query. To solve that problem, our method ranks candidate books in two steps. We first generate relaxed queries by removing some keywords from the given original query, and rank the queries based on how likely the remaining keywords are to appear in the brief descriptions. In the second step, we rank the results of each query. For each query, we retrieve matching books, find words in the description of each book that are most likely to be mistaken for the removed keywords by the user, and rank the books based on how likely they are mistaken for the removed keywords. By combining these two rankings, i.e., the ranking of relaxed queries, and the ranking of books matching with each query, we produce the final ranking. In this paper, we focus on the ranking method for the second step. Our experiment shows that our method improves ranking for some queries where the original query includes many keywords that do not appear in the description of the target book.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {402–403},
numpages = {2},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00091,
author = {Park, Young},
title = {Recommending Personalized Search Terms for Assisting Exploratory Website Search},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00091},
doi = {10.1109/JCDL.2019.00091},
abstract = {When visiting a website, the main task is to search for relevant information available on the site. The search is typically conducted using search terms. Choosing the right search terms is crucial to a successful search, but is often difficult, especially for exploratory searches. In this paper, we propose a recommender system called mySearchCLUE that recommends personalized search terms within a website search engine via collaborative filtering of similar visitors. The proposed recommender system helps website visitors explore and find better personalized search terms compared with no recommendations, and thus can be used to aid the personalized lookup and/or exploratory search process.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {404–405},
numpages = {2},
keywords = {collaborative filtering, website search, exploratory search, personalized search term recommendation, recommender systems},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00092,
author = {Dinh, Ly and Cheng, Yi-Yun and Parulian, Nikolaus},
title = {ReTracker: An Open-Source Plugin for Automated and Standardized Tracking of Retracted Scholarly Publications},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00092},
doi = {10.1109/JCDL.2019.00092},
abstract = {In this paper we present ReTracker, an open-source plugin for tracking retracted scientific journal articles. ReTracker helps Zotero users detect any papers in their current libraries that were retracted on PubMed. At its first release, ReTracker plugin adds metadata for each entry on its "retraction status" as the plugin performs local cache checking and sends a query to the tool's database. If there is a match in the paper's full title between the query and ReTracker's database, the retraction status metadata will be updated. We believe that ReTracker, as an open-source, free tool, is a necessary solution that is distinctive from existing tools (e.g. Open Retractions, Cross-Mark, etc.), in that it enables researchers to manage their existing citation libraries without extra efforts to repetitively search for and manually flag retracted papers from external database(s). With future releases, the tool aims to expand its functionalities in terms of standardizing retraction tags, adding retraction data for other item types, and providing pop-up notices for web browser plugins.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {406–407},
numpages = {2},
keywords = {scholarly communication, scientific retraction, metadata improvement, publishing ethics},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00093,
author = {Han, Wenting and Song, Shijie and Zhao, Yuxiang and Zhu, Qinghua},
title = {The Role of Self-Efficacy and Familiarity in Digital Humanity Crowdsourcing: A Preliminary Study from Transcribe-Sheng Project},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00093},
doi = {10.1109/JCDL.2019.00093},
abstract = {In this work, we employed regression analysis based on Transcribe-Sheng Project, a typical Chinese crowdsourcing project in culture heritage, to explore the influencing effects of the two predicting factors---self-efficacy and familiarity, on the volunteers' task completion and task performance respectively. The preliminary study found that: 1) Familiarity (including familiarity of background knowledge and familiarity of transcription platform) was the main factor that influenced task completion; 2) Familiarity of background knowledge significantly influenced the task performance; 3) The effects of self-efficacy on both task completion and task performance were not significant.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {408–409},
numpages = {2},
keywords = {self-efficacy, familiarity, transcribe sheng project, digital humanity, crowdsourcing},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00094,
author = {Luo, Xiaoxi and Tan, Xu and Wang, Xiaoguang},
title = {Semantically Enriched Presentation for Cultural Heritage Image: A POI-Based Perspective},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00094},
doi = {10.1109/JCDL.2019.00094},
abstract = {Images of cultural heritage contain rich historical and cultural connotations, while existing image presentation modes ignore the revelation of fine-grained semantic information. Semantic enrichment is a strategy for enhancing cultural heritage data and supporting digital humanities. The study constructed a framework to aggregate and display image semantic information units. We further took a Dunhuang mural as an example to implement a visualization platform based on POI(point of interest) perspective, presenting a new interactive method of image presentation.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {410–411},
numpages = {2},
keywords = {semantic enrichment, visualization platform, cultural heritage, digital humanities},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00095,
author = {Nanni, Federico and Menini, Stefano and Tonelli, Sara and Ponzetto, Simone Paolo},
title = {Semantifying the UK Hansard (1918--2018)},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00095},
doi = {10.1109/JCDL.2019.00095},
abstract = {The transcripts of UK parliamentary debates, offered by the Hansard Online collection are a major resource for historians and political scientists. To foster their use, we provide a) semantic annotations of over one hundred years of debated motions in the form of disambiguated and entity-linked speakers, b) topic annotations, and c) topical-clusters of the most frequently addressed issues.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {412–413},
numpages = {2},
keywords = {entity linking, topic extraction, parliamentary corpora, UK Hansard},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00096,
author = {Ghosal, Tirthankar and Verma, Rajeev and Ekbal, Asif and Bhattacharyya, Pushpak},
title = {A Sentiment Augmented Deep Architecture to Predict Peer Review Outcomes},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00096},
doi = {10.1109/JCDL.2019.00096},
abstract = {Peer review texts reflect the overall impression of the reviewers towards a candidate research paper and by far are the most important artifact used by editors and program chairs to determine the prospective inclusion of a manuscript in a given journal or a conference. Here in this work, we study how we could make use of the sentiment information embedded within peer review texts to help editors or program chairs to make better editorial decisions. We design an efficient deep neural architecture that takes into account: the paper, the corresponding reviews, and sentiment polarity of the reviews to predict the recommendation score of reviewers and well as to anticipate the final decision. Our results show that we achieve significant improvement over the baselines (~ 29% error reduction) proposed in a recently released dataset of peer reviews.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {414–415},
numpages = {2},
keywords = {peer review, review sentiment analysis, deep neural network},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00097,
author = {Lee, Chei Sian and Goh, Dion Hoe-Lian and Osop, Hamzah},
title = {To Help without Expectation: Investigating Social Exchanges on a Mobile Crowdsourcing Platform for a Smart City},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00097},
doi = {10.1109/JCDL.2019.00097},
abstract = {Drawing from the social exchange theory, this study investigates social exchanges on a mobile crowdsourcing platform (MCP) for a smart city. Specifically, we investigate how users' perceptions of features on a MCP influence their helping behaviors. Preliminary analysis shows that there are three main categories of features (i.e. information, positive emotions and ease of use) and two types of social exchange structures (i.e. reciprocal and productive). Results from regression analyses indicate that positive emotions and ease of use have effects on reciprocal exchange structure while information features have effects on productive exchange structure.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {416–417},
numpages = {2},
keywords = {crowdsourcing, smart city, social exchange theory, mobile applications, helping},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00098,
author = {Choi, Yunseon and Joo, Soohyung},
title = {Topic Detection of Online Book Reviews: Preliminary Results},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00098},
doi = {10.1109/JCDL.2019.00098},
abstract = {This study is part of a larger research project which aims to investigate whether online reviews on children's books would represent significant factors which are useful for selecting appropriate books for children. This paper presents the preliminary results on topic detection of online book reviews. Topic modeling using Latent Dirichlet Allocation (LDA) generated several topic terms from online reviews, and we categorized those topic terms into eleven categories. Sentiment analysis was applied to examine the emotional aspects of the reviews. We examined that sentiment words which have a powerful effect on polarity values to determine whether those sentiment words appear as topic words extracted by the LDA topic modeling. The results of our study have a significant implication on understanding user behavior in online book reviews.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {418–419},
numpages = {2},
keywords = {online reviews, topic modeling, social media, sentiment analysis},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00099,
author = {Mischo, William H. and Norman, Michael A. and Schlembach, Mary C.},
title = {Transaction Log Analysis within a Bento Discovery System},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00099},
doi = {10.1109/JCDL.2019.00099},
abstract = {Academic libraries have introduced bento-style discovery systems to address identified user issues with web-scale discovery systems. The bento systems partition search results into discrete areas on the screen grouped by content format type and/or local service results and address key user needs such as known-item search and streamlined full-text delivery. This poster reports on a transaction log study on the bento discovery system employed at the University of Illinois at Urbana-Champaign Library. The analysis looks at known-item vs exploratory searches, search argument characteristics, and user clickthrough behaviors.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {420–421},
numpages = {2},
keywords = {bento discovery systems, user clickthrough behaviors, user search behaviors, transaction log analysis},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00100,
author = {Otani, Yasuharu and Agata, Teru and Hashizume, Akiko and Eto, Masaki and Agata, Mari and Sugie, Noriko},
title = {Using VIAF Dataset and the National Bibliography for Identifying and Listing Comics and Manga Authors},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00100},
doi = {10.1109/JCDL.2019.00100},
abstract = {We propose a method for a comprehensive listing of comics and manga authors by combining a VIAF dataset, which internationally shares authority information created in each country, and a national bibliography. Specifically, it combines the dataset based on the descriptions of VIAF and the set of comics and manga authors extracted from comic titles in the Japanese national bibliography. The proposed method revealed that there are Japanese comics and manga authors who are not recognized as such in the Japanese national bibliography and that comics written by many Japanese comics authors are distributed only in Japan.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {422–423},
numpages = {2},
keywords = {national bibliography, authority file, manga, VIAF, comics},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00101,
author = {Wang, Xinyue and Xie, Zhiwu},
title = {Web Archive Analysis Using Hive and SparkSQL},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00101},
doi = {10.1109/JCDL.2019.00101},
abstract = {Analyzing large web archive collections incurs high computational costs. We propose an analytic framework based on Apache Hive and SparkSQL with integrated data storage and processing. This method achieves a more balanced performance on typical web archives analysis tasks from searching, filtering, extracting to deriving.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {424–425},
numpages = {2},
keywords = {web archive, distributed computation, big data},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00102,
author = {Gryk, Michael R. and Schuyler, Adam D. and Wedell, Jon and Maciejewski, Mark W. and Weatherby, Gerard and Romero, Pedro},
title = {When Should FAIR Begin? Applying FAIR during Data Creation},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00102},
doi = {10.1109/JCDL.2019.00102},
abstract = {Scientific data repositories exist to provide access to research data as well as to promote their reuse. Research data communities currently recognize the FAIR Data Principles [1] as the standard for supporting the aforementioned goals. FAIR consists of fifteen principles which are meant to assist in making the contents of a repository Findable, Accessible, Interoperable and Re-usable [1].While the FAIR guidelines are only a set of recommendations, open both to critique and possible improvements [2], they represent an important first step towards addressing the issues of accessibility and reuse of data in digital repositories. The topic of this poster is whether the FAIR guidelines could or should be applied to other data stores than bone fide digital repositories? Particularly, we explore whether there is value in considering the FAIR guidelines at the initial point of data creation, in addition to the end points of data curation and preservation.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {426–427},
numpages = {2},
keywords = {FAIR, virtual machines, data curation, data reuse},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00103,
author = {Prasad, Animesh and Ahuja, Saumya and Jiang, Shenhao and Wadhwa, Bimlesh and Kan, Min-Yen},
title = {ChairVisE: An Analytic Lens for Conference Submission Data},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00103},
doi = {10.1109/JCDL.2019.00103},
abstract = {We introduce an analytic and interactive visualization framework for conference submission metadata. Our system, ChairVisE, is a workflow tool that imports submission metadata, allows the crossing and linking between key submission metadata objects, and the integration of both sensitive data in the anonymized form and user-contributed metadata. It supports common statistics that are often reported by general and program chairs and caters to several common submission systems. We demonstrate ChairVisE through two use cases of different scales, on the JCDL and ACL 2018 conferences.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {428–429},
numpages = {2},
keywords = {analytics, visualization, conference meta-data},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00104,
author = {Nanni, Federico and Zhang, Jingyi and Betz, Ferdinand and Gashteovski, Kiril},
title = {EAL: A Toolkit and Dataset for Entity-Aspect Linking},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00104},
doi = {10.1109/JCDL.2019.00104},
abstract = {We present a toolkit and dataset for entity-aspect linking. The tool takes as input a sentence and provides the most relevant aspect for each mentioned entity; it is implemented in Python and available as a script and via an online demo. It is accompanied by the first large dataset of entity-aspects, comprising more than 20,000 entities manually linked to the most relevant aspect, given a sentence as context. Each is expressed in structured manner as Open Information Extraction (OIE) triples (Subject, Relation, Object), having semantic information for polarity, modality, quantity and attributions.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {430–431},
numpages = {2},
keywords = {entity-aspects, entities, knowledge base, information retrieval, open information extraction},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00105,
author = {Hosseini, Azam and Ghavimi, Behnam and Boukhers, Zeyd and Mayr, Philipp},
title = {EXCITE: A Toolchain to Extract, Match and Publish Open Literature References},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00105},
doi = {10.1109/JCDL.2019.00105},
abstract = {This demo paper presents a generic toolchain to extract, segment and match literature references from full text PDF files in the project EXCITE. The aim of EXCITE is extracting and matching citations from social science publications and making more citation data available to researchers. Each single step in the EXCITE pipeline and the open source tools used to accomplish the tasks are explained. The public demo system which integrates all components of the toolchain under an user-friendly interface is put forward and illustrated. As a final step, a special component is introduced which is capable to ingest the extracted and matched references into the Open Citation Corpus.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {432–433},
numpages = {2},
keywords = {reference extraction, open citations, reference matching, demo},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00106,
author = {Page, Kevin R. and Lewis, David and Weigl, David M.},
title = {MELD: A Linked Data Framework for Multimedia Access to Music Digital Libraries},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00106},
doi = {10.1109/JCDL.2019.00106},
abstract = {We present MELD, the Music Encoding and Linked Data framework, and its application to musicological articles containing dynamic, multi-path explorations of multimedia music digital library resources. MELD uses Linked Data to combine music-related materials including text, audio, video, images, facsimiles, and music scores. To maximise flexibility across this multimedia publication platform, and increase the potential for scholarly reuse of underlying data, we use RDF to model the nature of the resources being deployed, the relationships between them and, separately, how these resources should be visualised and interacted with.Crucially, this gives the ability to relate resources and parts of resources using semantically appropriate terms. Rather than only relating resources by a seconds- or beat-based spine, we encode richer musicological relationships drawing concepts from a Music Encoding Initiative (MEI) document or the Music Ontology, enabling the use of a wide range of music notational concepts.We describe the framework, demonstrating a JavaScript reference implementation and its utility when publishing musicological scholarship. As a case study, we show a MELD-enhanced version of an article written for the British Library on the performance of works by Frederick Delius. The article integrates TEI text, IIIF-served images, MEI notation and recordings of audio and video. We describe the semantic annotations which underpin this realisation, and how they relate users' experiences of moving between the content to the musicological argument being marshalled. Through this example we illustrate how connecting diverse media types using musically-meaningful semantics can support a richer form of interaction with music digital library materials, beyond the current state of the art.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {434–435},
numpages = {2},
keywords = {linked data, multimedia, publishing, digital libraries, musicology},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00107,
author = {Yang, Hsiu-Wei and Liu, Linqing and Milligan, Ian and Ruest, Nick and Lin, Jimmy},
title = {Scalable Content-Based Analysis of Images in Web Archives with TensorFlow and the Archives Unleashed Toolkit},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00107},
doi = {10.1109/JCDL.2019.00107},
abstract = {We demonstrate the integration of the Archives Unleashed Toolkit, a scalable platform for exploring web archives, with Google's Tensor-Flow deep learning toolkit to provide scholars with content-based image analysis capabilities. By applying pretrained deep neural networks for object detection, we are able to extract images of common objects from a 4TB web archive of GeoCities, which we then compile into browsable collages. This case study illustrates the types of interesting analyses enabled by combining big data and deep learning capabilities.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {436–437},
numpages = {2},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00108,
author = {F\"{a}rber, Michael and Nishioka, Chifumi and Jatowt, Adam},
title = {ScholarSight: Visualizing Temporal Trends of Scientific Concepts},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00108},
doi = {10.1109/JCDL.2019.00108},
abstract = {In this paper, we present a system for exploring the temporal trends of scientific concepts. Scientific concepts were captured by extracting noun phrases and entities from all computer science papers of arXiv.org. Our system allows users to review the time series of numerous concepts and to identify positively and negatively trending concepts. By applying clustering techniques and cluster analysis visualizations, it can also present concepts which share the same usage patterns over time. Our system can be beneficial for both ordinary researchers of any field and for researchers working in bibliometrics and scientometrics in order to investigate the evolution of scientific concepts.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {438–439},
numpages = {2},
keywords = {trend detection, bibliometrics, time series, scholarly data},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00109,
author = {Wildemann, Sergej and Holzmann, Helge},
title = {Tempurion: A Collaborative Temporal URI Collection for Named Entities},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00109},
doi = {10.1109/JCDL.2019.00109},
abstract = {Web archives preserve the history of the Web and help users to access resources that may not be discoverable anymore by traditional web search engines due to changes or deletion. Navigating these vast archives without knowing the exact URI of interest has proven to be challenging. When typical information needs revolve around named entities, the retrieval and temporal ranking of related archived resources cannot be solved by simple full-text searches.In this paper, we demonstrate a web platform that provides an ordered and annotated collection of URIs that characterize named entities over specific time frames. To not only rely on existing datasets, we have implemented interactive mechanisms to get humans in the loop to expand the collection by contributing URIs, metadata and temporal information as well as to correct errors.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {440–441},
numpages = {2},
keywords = {temporal information retrieval, collaborative knowledge, web archives},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00110,
author = {Ruest, Nick and Milligan, Ian and Lin, Jimmy},
title = {Warclight: A Rails Engine for Web Archive Discovery},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00110},
doi = {10.1109/JCDL.2019.00110},
abstract = {This paper describes the development of Warclight, a portmanteau of the open-source Blacklight platform and the ISO-standard Web ARChive file format. Warclight allows users to explore web archives that have been indexed into Apache Solr using the UK Web Archive's Web Archive Discovery tool. Referencing previous work, we explain how the standard search engine results page is inadequate to support scholarly inquiries. Instead, Warclight provides full-text and faceted search, as well as faceted browsing, to enable exploration and discovery. Given the large sizes of many web archives, we share experiences with deploying our tool at scale using a federated architecture.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {442–443},
numpages = {2},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00111,
author = {Fox, Edward A.},
title = {Introduction to Digital Libraries},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00111},
doi = {10.1109/JCDL.2019.00111},
abstract = {This tutorial is a thorough and deep introduction to the Digital Libraries (DL) field, providing a firm foundation: covering key concepts and terminology, as well as services, systems, technologies, methods, standards, projects, issues, and practices. It introduces and builds upon a firm theoretical foundation (starting with the '5S' set of intuitive aspects: Streams, Structures, Spaces, Scenarios, Societies), giving careful definitions and explanations of all the key parts of a 'minimal digital library', and expanding from that basis to cover key DL issues. Illustrations come from a set of case studies, including from multiple current projects, including with webpages, tweets, and social networks. Attendees will be exposed to four Morgan and Claypool books that elaborate on 5S, published 2012-2014. Complementing the coverage of '5S' will be an overview of key aspects of the DELOS Reference Model and DL.org activities. Further, use of a Hadoop cluster supporting big data DLs will be described.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {444–445},
numpages = {2},
keywords = {structures, big data, scenarios, streams, 5S, societies, spaces},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00112,
author = {Crocken, Todd and Washington, Anne},
title = {Strategies and Tools for Digital Repository Selection and Migration},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00112},
doi = {10.1109/JCDL.2019.00112},
abstract = {As digital repositories evolve, so to do the needs of institutions who employ them. Increasingly, institutions are faced with the daunting task of migrating content from one repository to another. But what strategies exist to help institutions identify suitable repositories and effectively and efficiently plan and execute a migration? This workshop aims to explore the issues and strategies of repository (re)selection and migration.Participants will learn about the different phases of a migration process including: system evaluation and selection, migration planning, and implementation strategies and tools. Throughout the workshop, participants will actively explore these phases as they relate to their organizational context and come away with questions and next steps for planning for system selection and/or a migration at their own institution.Target Audience: Digital repository managers, metadata librarians, digital services librariansFormat: Half Day TutorialExpected Number of Participants: 20Tutorial History: Completed one session at the 2019 Digital Initiatives Symposium},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {446–447},
numpages = {2},
keywords = {digital system evaluation, repository migration, digital libraries, migration strategies},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00113,
author = {Bainbridge, David},
title = {Building Digital Library Collections with Greenstone 3},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00113},
doi = {10.1109/JCDL.2019.00113},
abstract = {This tutorial is designed for those who want an introduction to building a digital library using an open source software program. The tutorial will focus on the Greenstone digital library software. In particular, participants will work with the Greenstone Librarian Interface, a flexible graphical user interface designed for developing and managing digital library collections. Attendees do not require programming expertise, however they should be familiar with HTML and the Web, and be aware of representation standards such as Unicode, Dublin Core and XML. The Greenstone software has a pedigree of more than two decades, with over 1 million downloads from SourceForge. The premier version of the software has, for many years, been Greenstone 2. This tutorial will introduce users to Greenstone 3 --- a redesign and reimplementation of the original software to take better advantage of newer standards and web technologies that have been developed since the original implementation of Greenstone. Written in Java, the software is more modular in design to increase the flexibility and extensibility of the software design. Emphasis in the tutorial is placed on where Greenstone 3 goes beyond what Greenstone 2 can do. Through the hands-on practical exercises participants will, for example, build collections where geo-tagged metadata embedded in photos is automatically extracted and used to provide a map-based view in the digital library of the collection.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {448},
numpages = {1},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00114,
author = {Clyburne-Sherin, April and Fei, Xu},
title = {Preparing Code and Data for Computational Reproducibility},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00114},
doi = {10.1109/JCDL.2019.00114},
abstract = {Computational analyses are playing an increasingly central role in research. Journals, funders, and researchers are calling for published research to include associated data and code. However, many involved in research have not received training in best practices and tools for sharing code and data. This workshop aims to address this gap in training while also providing those who support researchers with curated best practices guidance and tools.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {449–450},
numpages = {2},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00115,
author = {Koehl, Eleanor Dickson and Dubnicek, Ryan},
title = {Text Mining with HathiTrust},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00115},
doi = {10.1109/JCDL.2019.00115},
abstract = {This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {451–452},
numpages = {2},
keywords = {text mining, digital collections, digital pedagogy},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00116,
author = {Weigl, David M.},
title = {Workshop on Requirements, Use Cases, and User Studies in Digital Music Libraries and Archives (RUCUS) 2019: A Half-Day Workshop},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00116},
doi = {10.1109/JCDL.2019.00116},
abstract = {Music has widespread appeal as a subject for research, drawing on diverse perspectives from a large variety of academic fields. Digital libraries, serving as centres for interdisciplinary exchange [1], enable collaborative interactions with multimodal music resources between users and across use contexts. A thorough understanding of such interactions on a human level, incorporating concerns around user expectations, requirements, and information behaviours is critical in order to evaluate and inform the development of information systems to fulfil this purpose [2].The proposed half-day workshop on Requirements, Use Cases, and User Studies in Digital Music Libraries and Archives (RUCUS) provides a concerted platform for intellectual exchange between scholars, practitioners, and system developers concerned with user-centric issues of music information research, a topic of primary concern for the development and understanding of Digital Music Libraries and Archives},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {453–454},
numpages = {2},
keywords = {user studies, digital music archives, digital music libraries},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00041,
author = {Klein, Martin and Xie, Zhiwu and Fox, Edward A.},
title = {Web Archiving and Digital Libraries (WADL)},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00041},
doi = {10.1109/JCDL.2019.00041},
abstract = {The 2019 edition of the Workshop on Web Archiving and Digital Libraries (WADL) will explore the integration of web archiving and digital libraries. The workshop aims at addressing aspects covering the entire life cycle of digital resources and will also explore areas such as archiving processes and tools for "non-traditional" resources such as scholarly and government data, 3D objects, and digital online art.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {455–456},
numpages = {2},
keywords = {digital preservation, web archiving, community building},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00117,
author = {Weber, Nicholas and Fenlon, Katrina and Organisciak, Peter and Thomer, Andrea K.},
title = {Conceptual Models in Digital Libraries, Archives, and Museums},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00117},
doi = {10.1109/JCDL.2019.00117},
abstract = {This workshop addresses the development, use, and evolution of conceptual models in the context of digital libraries, archives, and museums. The workshop will convene domain practitioners and researchers in order to formalize a research agenda for conceptual modelling in digital libraries, and foster a cooperative research community to make progress on these topics over the coming decade. Activities at the workshop will include paper presentations, round-table discussions, and a keynote from an expert in the conceptual and logical foundations of information organization systems. Workshop papers as well as a summary of the proceedings will be published in a free and openly accessible repository.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {457–458},
numpages = {2},
keywords = {digital libraries, conceptual models},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00118,
author = {Chen, Jiangping and Lu, Wei and Zavalina, Oksana},
title = {Organizing Data, Information, and Knowledge in Big Data Environments},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00118},
doi = {10.1109/JCDL.2019.00118},
abstract = {The explosion of information and the availability of big data provide new significant challenges for digital libraries. For example, libraries face the "cyberinfrastructural challenge" and the need to develop better understanding of research data to support curation, sharing and reuse of data generated by data-intensive science [9][6]. The synergy between information science and data science is emerging to address these Big Data environment challenges. The most fruitful collaboration areas for this synergy include those related to information organization: "big metadata, smart metadata" the ways to leverage the "metadata capital"[4]. The proposed one-day workshop will seek to support the interdisciplinary collaboration in this important area. It will focus on the challenges and opportunities provided by Big Data environment for information and computing professionals to explore innovative strategies and solutions for organizing data, information, and knowledge.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {459–460},
numpages = {2},
keywords = {information organization, knowledge organization, big data analytics, data management},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00123,
author = {Wacholder, Nina and Valenti, M. Grey and Provenzano, Nicholas},
title = {A Graphical Interface for the Dodge Poetry Festival Archive},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00123},
doi = {10.1109/JCDL.2019.00123},
abstract = {The Dodge Poetry Festival is a biennial four-day event at which American and international poets read their own poetry out loud to relatively large, attentive audiences. In this paper, we report on a prototype graphical interface designed to let users access high quality video recordings of these poetry readings. The goal is to create an interface that draws high school students in particular into the experience of listening to poetry. This prototype is the first step in a plan to build a working system for a small portion of the Archive collection that can then be realistically evaluated. From a historical perspective, the archive provides a high-quality record of poets of the late 20th and early 21st centuries reading their own poems and and discussing their work with festival attendees.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {461–462},
numpages = {2},
keywords = {visual and audio text, digital humanities, graphical interface, poetry},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1109/JCDL.2019.00124,
author = {Bainbridge, David and Nichols, David M. and Hinze, Annika and Downie, J. Stephen},
title = {Using the HTRC Data Capsule Model to Promote Reuse and Evolution of Experimental Analysis of Digital Library Data: A Case Study of Topic Modeling},
year = {2019},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00124},
doi = {10.1109/JCDL.2019.00124},
abstract = {We report on a case-study to independently reproduce the work given in a publicly available blog on how to develop a topic model sourced from a collection of texts, where both the data set and source code used are readily available. More specifically, we detail the steps necessary---and the challenges that had to be overcome---to replicate the work using the HathiTrust Research Center's virtual machine Data Capsule platform. From this we make recommendations for authors to follow, based on the lessons learned. We also show that the Data Capsule model can be put to work in a way that is of benefit to those interested in supporting computational reproducibility within their organizations.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {463–464},
numpages = {2},
keywords = {experimental reproducibility, virtual machine, digital libraries},
location = {Champaign, Illinois},
series = {JCDL '19}
}

