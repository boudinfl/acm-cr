@inproceedings{10.1145/3342558.3351871,
author = {Siegel, Erik},
title = {Introduction to XProc 3.0: Tutorial Description},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3351871},
doi = {10.1145/3342558.3351871},
abstract = {XProc is an XML based programming language for complex document processing. Documents flow through pipelines in which steps perform processing like conversion, validation, split, merge, report, etc. It's an almost perfect fit for the kind of processing necessary in document engineering. This tutorial will be an introduction to the main concepts of the language.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {1},
numpages = {2},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3351872,
author = {Reichardt, Marcus},
title = {SGML to the Rescue: Using SGML with Modern HTML},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3351872},
doi = {10.1145/3342558.3351872},
abstract = {This tutorial will explore techniques for parsing and processing HTML 5 using SGML, the original markup meta-language on which both XML and HTML are based. The tutorial will re-introduce requisite SGML concepts and introduce a new SGML Document Type Definition (DTD) grammar covering W3C's current HTML specification text.The main part of the tutorial is dedicated to hands-on exercises for practical tasks in working with HTML markup. The tutorial will conclude with a discussion on current developments in HTML standardization and potential consequences for the markup and standardization communities.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {2},
numpages = {2},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3351873,
author = {von Seggern, Dietrich and Posselt, Klaas and Hassan, Tamir and Zellmann, Thomas},
title = {More than Just Digital Paper-Hidden Features of the PDF Format},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3351873},
doi = {10.1145/3342558.3351873},
abstract = {PDF has long been established as the de facto format for the exchange of print-oriented documents and is known for its robust visual presentation across a variety of operating systems and platforms.However, relatively few users are familiar with the format's newer features, such as tagging, forms and security. This tutorial aims to give an overview of the most important of these features and demonstrate the benefits of creating and exchanging PDF files that make use of them.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {3},
numpages = {2},
keywords = {Document formats, PDF, Machine-readable structure, Workflows, Tagging},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3351874,
author = {Lins, Rafael Dueire and Mello, Rafael Ferreira and Simske, Steve},
title = {DocEng'19 Competition on Extractive Text Summarization},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3351874},
doi = {10.1145/3342558.3351874},
abstract = {The DocEng'19 Competition on Extractive Text Summarization assessed the performance of two new and fourteen previously published extractive text sumarization methods. The competitors were evaluated using the CNN-Corpus, the largest test set available today for single document extractive summarization.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {4},
numpages = {2},
keywords = {CNN Corpus, NLP, Text summarization, text documents},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345402,
author = {Baggio, Cecilia and Cecchini, Roc\'{\i}o L. and Maguitman, Ana G. and Milios, Evangelos E.},
title = {Multi-Objective GP Strategies for Topical Search Integrating Wikipedia Concepts},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345402},
doi = {10.1145/3342558.3345402},
abstract = {Genetic Programming techniques have demonstrated great potential in dealing with the problem of query generation. This work explores different Multi-Objective Genetic Programming strategies for evolving a collection of topic-based Boolean queries. It compares three approaches to build topical Boolean queries: using terms, incorporating Wikipedia semantics (Wikipedia concepts) and a hybrid approach, using a combination of both terms and concepts. In addition, different fitness functions are combined giving rise to seven multi-objective schemes. In particular, we investigate the use of the proposed strategies in conjunction with novel fitness functions aimed at attaining high diversity based on the information-theoretic notion of entropy and Jaccard similarity. Experiments were completed using 25 topics from a dataset consisting of approximately 350,000 webpages classified into 448 topics. The results reveal that the use of Wikipedia concepts does not result in statistically significant improvements in precision, global recall or diversity when compared to the term-based approaches. However, the use of concepts has a positive effect on query interpretability since the use of terms leads to artificial queries that are hard to interpret by humans. In the meantime, concept-based queries contain a smaller number of operands than the term-based ones, hence resulting in better execution times without a loss in retrieval performance.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {5},
numpages = {10},
keywords = {Recall Maximization, Topical Recommendation, Query Optimization, Similarity Measures, Wikification},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345397,
author = {Boyer, John M.},
title = {On the Expressive Power of Declarative Constructs in Interactive Document Scripts},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345397},
doi = {10.1145/3342558.3345397},
abstract = {It is difficult to generally compare the succinctness of declarative versus imperative programming as source code size varies. In imperative programs, basic operations have constant cost, but they tend to be more verbose than declarative programs, which increases the potential for defects. This paper presents a novel approach for a generalized comparison by transforming the problem into comparing executed code size of a benchmark imperative algorithm with a partially declarative variant of the same algorithm. This allows input size variation to substitute for source code size variation. For implementation, we use a multiparadigm language called XForms that contains both declarative XPath expressions and imperative script actions for interacting with XML data within web and office documents. A novel partially declarative variant of the quicksort is presented. Amortized analysis shows that only O(n) imperative actions are executed, so the expressive power of the declarative constructs is at least Î©(log n). In general, declarative constructs can have an order of magnitude expressive power advantage compared with only using basic imperative operations. The performance cost factor of the expressive power advantage was determined to be O(log2 n) based on a novel dynamic projection from the generalized tree structure of XML data to a height balanced binary tree.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {6},
numpages = {10},
keywords = {Rich web applications, Document models and structures, Algorithm analysis, Multiparadigm languages, Document representation and standards, Interactive documents},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345398,
author = {Lepiane, Cristiane Dias and Pereira, Fernando Lauro and Pieri, Giovani and Martins, Douglas and Martina, Jean Everson and Rabelo, Mauro Luiz},
title = {Digital Degree Certificates for Higher Education in Brazil: A Technical Policy Specification},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345398},
doi = {10.1145/3342558.3345398},
abstract = {Higher Education Degree Certificates in Brazil are a tool for social mobility. Access to higher education is still an issue in a developing economy with continental size and historic inequalities. Some people see this combination as an opportunity to exploit the system, producing fake degree certificates, or issuing official degree certificates to people that did not enrol in courses. Degree certificates can be easily bought in the country and they produce the desired social ascension. To tackle that, the Brazilian Ministry of Education enacted a regulation instituting the Digital Degree Certificate for higher education. The regulation only specifies that the degree certificates must be digitally signed with the country's official PKI. This regulation does not bring the technical details of how this can be implemented. We aim to discuss these problems of the Degree Certificates' black-market in Brazil, its social consequences, and how a technical specification for that regulation can be conceived and put in practice following the ministerial regulation. The outcome of this research is a proposal for implementing digitally signed degree certificates that fulfil the legal requirements in Brazil, as well as can be easily integrated with computerized information systems and that can be maintained securely in the long term.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {7},
numpages = {10},
keywords = {public-key infrastructures, digital government, Keywords Digital documents},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345393,
author = {El Vaigh, Cheikh Brahim and Goasdou\'{e}, Fran\c{c}ois and Gravier, Guillaume and S\'{e}billot, Pascale},
title = {Using Knowledge Base Semantics in Context-Aware Entity Linking},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345393},
doi = {10.1145/3342558.3345393},
abstract = {Entity linking is a core task in textual document processing, which consists in identifying the entities of a knowledge base (KB) that are mentioned in a text. Approaches in the literature consider either independent linking of individual mentions or collective linking of all mentions. Regardless of this distinction, most approaches rely on the Wikipedia encyclopedic KB in order to improve the linking quality, by exploiting its entity descriptions (web pages) or its entity interconnections (hyperlink graph of web pages). In this paper, we devise a novel collective linking technique which departs from most approaches in the literature by relying on a structured RDF KB. This allows exploiting the semantics of the interrelationships that candidate entities may have at disambiguation time rather than relying on raw structural approximation based on Wikipedia's hyperlink graph. The few approaches that also use an RDF KB simply rely on the existence of a relation between the candidate entities to which mentions may be linked. Instead, we weight such relations based on the RDF KB structure and propose an efficient decoding strategy for collective linking. Experiments on standard benchmarks show significant improvement over the state of the art.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {8},
numpages = {10},
keywords = {Entity linking, RDF, entity relatedness measure, collective entity linking, knowledge base semantics},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345389,
author = {Flagg, Cristopher and Frieder, Ophir},
title = {Searching Document Repositories Using 3D Model Reconstruction},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345389},
doi = {10.1145/3342558.3345389},
abstract = {A common representation of a three dimensional object is a multi-view collection of two dimensional images showing the object from multiple angles. This technique is often used with document repositories such as collections of engineering drawings and governmental repositories of design patents and 3D trademarks. It is rare for the original physical artifact to be available. When the original physical artifact is modeled as a set of images, the resulting multi-view collection of images may be indexed and retrieved using traditional image retrieval techniques. Consequently, massive repositories of multi-view collections exist. While these repositories are in use and easy to construct, the conversion of a physical object into multi-view images results in a degraded representation of both the original three dimensional artifact and the resulting document repository. We propose an alternative approach where the archived multi-view representation of the physical artifact is used to reconstruct the 3D model, and the reconstructed model is used for retrieval against a database of 3D models. We demonstrate that document retrieval using the reconstructed 3D model achieves higher accuracy than document retrieval using a document image against a collection of degraded multi-view images. The Princeton Shape Benchmark 3D model database and the ShapeNet Core 3D model database are used as ground truth for the 3D image collection. Traditional indexing and retrieval is simulated using the multi-view images generated from the 3D models. A more accurate 3D model search is then considered using a reconstruction of the original 3D models from the multi-view archive, and this model is searched against the 3D model database.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {9},
numpages = {10},
keywords = {3D Modeling, Patent Documents, Fast Point Feature Histograms, SIFT, Model Reconstruction, Document Repository, Point Cloud},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345401,
author = {Dias, Amanda Gon\c{c}alves and Milios, Evangelos E. and de Oliveira, Maria Cristina Ferreira},
title = {TRIVIR: A Visualization System to Support Document Retrieval with High Recall},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345401},
doi = {10.1145/3342558.3345401},
abstract = {In this paper, we propose TRIVIR, a novel interactive visualization tool powered by an Information Retrieval (IR) engine that implements an active learning protocol to support IR with high recall. The system integrates multiple graphical views in order to assist the user identifying the relevant documents in a collection, including a content-based similarity map obtained with multidimensional projection techniques. Given representative documents as queries, users can interact with the views to label documents as relevant/not relevant, and this information is used to train a machine learning (ML) algorithm which suggests other potentially relevant documents on demand. TRIVIR offers two major advantages over existing visualization systems for IR. First, it merges the ML algorithm output into the visualization, while supporting several user interactions in order to enhance and speed up its convergence. Second, it tackles the problem of vocabulary mismatch, by providing term's synonyms and a view that conveys how the terms are used within the collection. Besides, TRIVIR has been developed as a flexible front-end interface that can be associated with distinct text representations and multidimensional projection techniques. We describe two use cases conducted with collaborators who are potential users of TRIVIR. Results show that the system simplified the search for relevant documents in large collections, based on the context in which the terms occur.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {10},
numpages = {10},
keywords = {total recall, visualization, information retrieval, machine learning, vocabulary mismatch},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345400,
author = {Guedes, \'{A}lan L. V. and de A. Azevedo, Roberto G. and Colcher, S\'{e}rgio and Barbosa, Simone D. J.},
title = {Modeling Multimodal-Multiuser Interactions in Declarative Multimedia Languages},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345400},
doi = {10.1145/3342558.3345400},
abstract = {Recent advances in hardware and software technologies have given rise to a new class of human-computer interfaces that both explores multiple modalities and allows for multiple collaborating users. When compared to the development of traditional single-user WIMP (windows, icons, menus, pointer)-based applications, however, applications supporting the seamless integration of multimodal-multiuser interactions bring new specification and runtime requirements. With the aim of assisting the specification of multimedia applications that integrate multimodal-multiuser interactions, this paper: (1) proposes the MMAM (Multimodal-Multiuser Authoring Model); (2) presents three different instantiations of it (in NCL, HTML, and a block-based syntax); and (3) evaluates the proposed model through a task-based user study. MMAM enables programmers to design and ponder different solutions for applications with multimodal-multiuser requirements. The proposed instantiations served as proofs of concept about the feasibility of our model implementation and provided the basis for practical experimentation, while the performed user study focused on capturing evidence of both the user understanding and the user acceptance of the proposed model. We asked developers to perform tasks using MMAM and then answer a TAM (Technology Acceptance Model)-based questionnaire focused on both the model and its instances. As results, the study indicates that the participants easily understood the model (most of them performed the required tasks with minor or no errors) and found it both useful and easy to use. 94.47% of the participants gave positive answers to the block-based representation TAM questions, whereas 75.17% gave positive answers to the instances-related questions.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {11},
numpages = {10},
keywords = {MUI, HTML, NCL, Multiuser User Interactions, Multimodal User Interactions, Multimedia Languages},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345390,
author = {de Souza, Richard Henrique and Dorneles, Carina Friedrich},
title = {Searching and Ranking Questionnaires: An Approach to Calculate Similarity between Questionnaires},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345390},
doi = {10.1145/3342558.3345390},
abstract = {Questionnaires are useful tools for research purposes and are generally used for collecting information about a population of interest, by focusing on different intentions. During the questionnaire project, or for sharing data purposes, it may be useful to check if there is already a questionnaire with the same intention as that being carried out. Well-designed questions can induce respondents to provide better answers. However, examining research questionnaires is not a trivial task since a question can be structured in different ways. In this paper, we propose a similarity measure to match questionnaires that are characterized by the heterogeneity of their questions and to provide a ranking method based on variations of a given query. In determining the effectiveness of this approach, we evaluated it through an experimental study, using recall, precision, f-value, MAP and NDGC, and this enabled us to obtain more effective results than other proposals.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {12},
numpages = {9},
keywords = {ranking, searching, similarity, Questionnaire},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345396,
author = {Jessen, Morten and B\"{o}schen, Falk and Scherp, Ansgar},
title = {Text Localization in Scientific Figures Using Fully Convolutional Neural Networks on Limited Training Data},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345396},
doi = {10.1145/3342558.3345396},
abstract = {Text extraction from scientific figures has been addressed in the past by different unsupervised approaches due to the limited amount of training data. Motivated by the recent advances in Deep Learning, we propose a two-step neural-network-based pipeline to localize and extract text using Fully Convolutional Networks. We improve the localization of the text bounding boxes by applying a novel combination of a Residual Network with the Region Proposal Network based on Faster R-CNN. The predicted bounding boxes are further pre-processed and used as input to the of-the-shelf optical character recognition engine Tesseract 4.0. We evaluate our improved text localization method on five different datasets of scientific figures and compare it with the best unsupervised pipeline. Since only limited training data is available, we further experiment with different data augmentation techniques for increasing the size of the training datasets and demonstrate their positive impact. We use Average Precision and F1 measure to assess the text localization results. In addition, we apply Gestalt Pattern Matching and Levenshtein Distance for evaluating the quality of the recognized text. Our extensive experiments show that our new pipeline based on neural networks outperforms the best unsupervised approach by a large margin of 19-20%.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {13},
numpages = {10},
keywords = {OCR, training data augmentation, neural networks},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345391,
author = {Kassaie, Besat and Tompa, Frank Wm.},
title = {Predictable and Consistent Information Extraction},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345391},
doi = {10.1145/3342558.3345391},
abstract = {Information extraction programs (extractors) can be applied to documents to isolate structured versions of some content, that is, to create tabular records corresponding to facts found in the documents. If the data in an extracted table needs to be updated for any reason (for example, as a result of data cleaning), the source document will no longer be synchronized with the data. But documents are the principal medium for sharing information among humans. We therefore wish to ensure that changes to extracted tables are reflected correctly in their source documents.In this work, we characterize extractors for which we are able to predict the effects that updates to source documents will have on extracted records. We introduce three general properties for extractors that, if satisfied, can guarantee that consistency will be maintained if the lineage of extracted records is respected when changing the documents. We propose a property verification process that uses static analysis for a substantial subset of JAPE, a well-established rule-based extraction language, and illustrate it through an example based on a freely-available extractor library.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {14},
numpages = {10},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345399,
author = {Lin, Jason and Wang, Xing and Wang, Zelun and Beyette, Donald and Liu, Jyh-Charn},
title = {Prediction of Mathematical Expression Declarations Based on Spatial, Semantic, and Syntactic Analysis},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345399},
doi = {10.1145/3342558.3345399},
abstract = {Mathematical expressions (ME) and words are carefully bonded together in most science, technology, engineering, and mathematics (STEM) documents. They respectively give quantitative and qualitative descriptions of a system model under discussion. This paper proposes a general model for finding the co-reference relations between words and MEs, based on which we developed a novel algorithm for predicting the natural language declarations of MEs--the ME-Dec. The prediction algorithm is applied in a three-level framework, where the first level is a customized tagger to identify the syntactic roles of MEs and the part-of-speech (POS) tags of words in the ME-word mixed sentences. The second level screens the ME-Dec candidates based on the hypothesis that most ME-Dec are noun phrases (NP). A shallow chunker is trained from the fuzzy process mining algorithm, which uses the labeled POS tag series in the NTCIR-10 dataset as input to mine for the frequent syntactic patterns of NP. In the third level, using distance, word stem, and POS tag respectively as the spatial, semantic, and syntactic features, the bonding model between MEs and ME-Dec candidates is trained on the NTCIR-10 training set. The final prediction results are made upon the majority votes of an ensemble of Na\"{\i}ve Bayesian classifiers based on the three features. Evaluation of the model on the NTCIR-10 test set, the proposed algorithm achieved 75% and 71% average F1 score in soft matching and strict matching, respectively, which outperforms the state-of-the-art solutions by a margin of 5-18%.1},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {15},
numpages = {10},
keywords = {Mathematical expression, Declaration extraction, Co-reference},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345388,
author = {Lins, Rafael Dueire and Oliveira, Hilario and Cabral, Luciano and Batista, Jamilson and Tenorio, Bruno and Ferreira, Rafael and Lima, Rinaldo and de Fran\c{c}a Pereira e Silva, Gabriel and Simske, Steven J.},
title = {The CNN-Corpus: A Large Textual Corpus for Single-Document Extractive Summarization},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345388},
doi = {10.1145/3342558.3345388},
abstract = {This paper details the features and the methodology adopted in the construction of the CNN-corpus, a test corpus for single document extractive text summarization of news articles. The current version of the CNN-corpus encompasses 3,000 texts in English, and each of them has an abstractive and an extractive summary. The corpus allows quantitative and qualitative assessments of extractive summarization strategies.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {16},
numpages = {10},
keywords = {Corpus, Single-document Summarization, Extractive Summarization, Multi-language Summarization},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345395,
author = {Miller, Matthias and Bonnici, Alexandra and El-Assady, Mennatallah},
title = {Augmenting Music Sheets with Harmonic Fingerprints},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345395},
doi = {10.1145/3342558.3345395},
abstract = {Common Music Notation (CMN) is the well-established foundation for the written communication of musical information, such as rhythm or harmony. CMN suffers from the complexity of its visual encoding and the need for extensive training to acquire proficiency and legibility. While alternative notations using additional visual variables (e.g., color to improve pitch identification) have been proposed, the community does not readily accept notation systems that vary widely from the CMN. Therefore, to support student musicians in understanding harmonic relationships, instead of replacing the CMN, we present a visualization technique that augments digital sheet music with a harmonic fingerprint glyph. Our design exploits the circle of fifths, a fundamental concept in music theory, as visual metaphor. By attaching such glyphs to each bar of a composition we provide additional information about the salient harmonic features available in a musical piece. We conducted a user study to analyze the performance of experts and non-experts in an identification and comparison task of recurring patterns. The evaluation shows that the harmonic fingerprint supports these tasks without the need for close-reading, as when compared to a not-annotated music sheet.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {17},
numpages = {10},
keywords = {Analysis, Sheet Music, Visualization, Harmony},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345394,
author = {Wang, Jingwen and Zhang, Hao and Zhang, Cheng and Yang, Wenjing and Shao, Liqun and Wang, Jie},
title = {An Effective Scheme for Generating An Overview Report over A Very Large Corpus of Documents},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345394},
doi = {10.1145/3342558.3345394},
abstract = {How to efficiently generate an accurate, well-structured overview report (ORPT) over thousands of documents is challenging. A well-structured ORPT is divided into sections of multiple levels (e.g., a two-level structure consists of sections and subsections). None of the existing multi-document summarization (MDS) algorithms is suitable for accomplishing this task. To overcome this obstacle, we devise NDORGS (Numerous Documents' Overview Report Generation Scheme) that integrates text filtering, keyword scoring, single-document summarization (SDS), topic modeling, MDS, and title generation to generate a coherent, well-structured ORPT. We then present a multi-criteria evaluation method using techniques of text mining and multi-attribute decision making on a combination of human judgments, running time, information coverage, and topic diversity. We evaluate ORPTs generated by NDORGS on two large corpora of documents, where one is classified and the other unclassified. We show that, using Saaty's pairwise comparison 9-point scale and TOPSIS, the ORPTs generated on SDS's with the length of 20% of the original documents are the best overall on both datasets.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {18},
numpages = {11},
keywords = {document generation, TOPSIS, topic clustering, multi-document summarization},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345403,
author = {Wen, Elliott and Warren, Jim and Weber, Gerald},
title = {PaperWork: Exploring the Potential of Electronic Paper on Office Work},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345403},
doi = {10.1145/3342558.3345403},
abstract = {Electronic paper (e-paper) is a display technology that aims to imitate conventional paper. Currently most e-paper applications on handheld devices are restricted to digital book readers. Few studies explore the potential of e-paper on input oriented applications. In this paper, we introduce a novel e-paper application PaperWork, which allows users to offload their commonly used office applications from a PC to an e-paper device remotely. There are considerable challenges when building a system for a resource constrained e-paper device which we will highlight in this work. In addition to presenting a new e-paper system we also conduct a usability study of this system.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {19},
numpages = {10},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345416,
author = {Correia, Fernando A. and Nunes, Jos\'{e} Luiz and de Almeida, Guilherme F. C. F. and Almeida, Alexandre A. A. and Lopes, H\'{e}lio},
title = {An Exploratory Analysis of Precedent Relevance in the Brazilian Supreme Court Rulings},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345416},
doi = {10.1145/3342558.3345416},
abstract = {The new Brazilian Code of Civil Procedure (CPC) has elevated the importance of precedents in the legal decision-making process. This increased the need to find relevant precedents for a given issue or dispute. Precedents play a central role in judicial thinking by providing information to judges about the legal relevance of particular facts and by establishing legal rules. Precedents are also an important argumentative tool, enabling lawyers to present arguments based on previous decisions. The automated search for relevant precedents is an unattended issue in the Brazilian scenario, partly due to the court's massive production of decisions -- only in 2018 the Brazilian Supreme Court (STF) produced more than 121.000 new rulings -- and partly due to the technical challenges arising from the unstructured nature of the court's practices. In this paper, we present a study of precedent relevance, taking into account the uniqueness of the Brazilian legal system and of STF. To do so, we conducted an exploratory investigation over the precedent network extracted from 1.152.963 decisions published by the STF between 2008 and 2018. This exploratory analysis, although interesting in itself, reveals important challenges that need to be overcome by future research in order for the technology to have the kind of impact it can have on legal practice and academia. In our conclusion, we set out possible paths forward, briefly considering some of the most promising ways to sort out the signal from the noise.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {20},
numpages = {4},
keywords = {Brazilian Law, Legal Arguments, Legal Documents, Precedents, Legal Rulings, Network Analysis},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345418,
author = {Dewalkar, Swapnil and Desarkar, Maunendra Sankar},
title = {Multi-Context Information for Word Representation Learning},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345418},
doi = {10.1145/3342558.3345418},
abstract = {Word embedding techniques in literature are mostly based on Bag of Words models where words that co-occur with each other are considered to be related. However, it is not necessary for similar or related words to occur in the same context window. In this paper, we propose a new approach to combine different types of resources for training word embeddings. The lexical resources used in this work are Dependency Parse Tree and WordNet. Apart from the co-occurrence information, the use of these additional resources helps us in including the semantic and syntactic information from the text in learning the word representations. The learned representations are evaluated on multiple evaluation tasks like Semantic Textual Similarity, Word Similarity. Results of the experimental analyses highlight the usefulness of the proposed methodology.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {21},
numpages = {4},
keywords = {Representation Learning, Word embedding, WordNet, Dependency Parsing},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345406,
author = {Di Iorio, Angelo and Spinaci, Gianmarco and Vitali, Fabio},
title = {Multi-Layered Edits for Meaningful Interpretation of Textual Differences},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345406},
doi = {10.1145/3342558.3345406},
abstract = {The way humans and algorithms look at and understand differences between versions and variants of the same text may be very different. While correctness and overall byte length are fundamental aspects of good outputs of diff algorithms, they do not usually provide immediately interesting values for humans trying to make sense of the events that lead from one version to another of a text.In this paper we propose 3-edit, a layered model to group and organize individual differences (i.e., edits) between document versions in a conceptual value-based scaffolding that provides an easier and more approachable characterization of the modifications occurred to a text document. Through the structural and semantic classification of the individual edits, it becomes possible to differentiate between modifications, so as to show them differently, show only some of them, or emphasize some of them, so that the human mind can more easily identify the types of modifications that matter for its reading purpose.An algorithm that provides structural and semantic grouping of basic mechanical INS/DEL edits is described as well.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {22},
numpages = {4},
keywords = {textual differences, diff, interpretation of changes, Change detection, versioning},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345413,
author = {Faigenbaum-Golovin, Shira and Levin, David and Piasetzky, Eli and Finkelstein, Israel},
title = {Writer Characterization and Identification of Short Modern and Historical Documents: Reconsidering Paleographic Tables},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345413},
doi = {10.1145/3342558.3345413},
abstract = {Handwriting is considered a unique "fingerprint" that characterizes a scribe (it is even used as evidence in modern forensics). In paleography (the study of ancient writing), it is presumed that each writer has a one prototype for each letter in the alphabet. Commonly, for ancient inscriptions, letters are organized into paleographic tables (where the rows are the alphabet letters, and the columns represent the examined inscriptions). These tables play a significant role in dating inscriptions based on their resemblance to columns in the table. In this paper, we argue that each scribe "fingerprint" is not represented by a single character prototype, but in fact by a distribution of characters. We introduce a framework for automatically identifying the writer style and constructing paleographic tables based on character histograms. Subsequently, we propose a method for comparing short documents utilizing letter distribution. We demonstrate the validity of the methods on two handwritten datasets: Modern and Ancient Hebrew pertaining to the First Temple period. Our methodology on the ancient dataset enables us to provide additional evidence concerning the level of literacy in the kingdom of Judah ca. 600 BCE.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {23},
numpages = {4},
keywords = {epigraphy, Hebrew ostraca, Judah, Iron Age, paleographic tables, Handwriting comparison, historical documents},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345411,
author = {Foppiano, Luca and Romary, Laurent and Ishii, Masashi and Tanifuji, Mikiko},
title = {Automatic Identification and Normalisation of Physical Measurements in Scientific Literature},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345411},
doi = {10.1145/3342558.3345411},
abstract = {We present Grobid-quantities, an open-source application for extracting and normalising measurements from scientific and patent literature. Tools of this kind, aiming to understand and make unstructured information accessible, represent the building blocks for large-scale Text and Data Mining (TDM) systems. Grobid-quantities is a module built on top of Grobid [6] [13], a machine learning framework for parsing and structuring PDF documents. Designed to process large quantities of data, it provides a robust implementation accessible in batch mode or via a REST API. The machine learning engine architecture follows the cascade approach, where each model is specialised in the resolution of a specific task. The models are trained using CRF (Conditional Random Field) algorithm [12] for extracting quantities (atomic values, intervals and lists), units (such as length, weight) and different value representations (numeric, alphabetic or scientific notation). Identified measurements are normalised according to the International System of Units (SI). Thanks to its stable recall and reliable precision, Grobid-quantities has been integrated as the measurement-extraction engine in various TDM projects, such as Marve (Measurement Context Extraction from Text), for extracting semantic measurements and meaning in Earth Science [10]. At the National Institute for Materials Science in Japan (NIMS), it is used in an ongoing project to discover new superconducting materials. Normalised materials characteristics (such as critical temperature, pressure) extracted from scientific literature are a key resource for materials informatics (MI) [9].},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {24},
numpages = {4},
keywords = {TDM, Machine Learning, Physical quantities, Units of measurements, Measurements},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345409,
author = {Koci, Elvis and Kuban, Dana and Luettig, Nico and Olwig, Dominik and Thiele, Maik and Gonsior, Julius and Lehner, Wolfgang and Romero, Oscar},
title = {XLIndy: Interactive Recognition and Information Extraction in Spreadsheets},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345409},
doi = {10.1145/3342558.3345409},
abstract = {Over the years, spreadsheets have established their presence in many domains, including business, government, and science. However, challenges arise due to spreadsheets being partially-structured and carrying implicit (visual and textual) information. This translates into a bottleneck, when it comes to automatic analysis and extraction of information. Therefore, we present XLIndy, a Microsoft Excel add-in with a machine learning back-end, written in Python. It showcases our novel methods for layout inference and table recognition in spreadsheets. For a selected task and method, users can visually inspect the results, change configurations, and compare different runs. This enables iterative fine-tuning. Additionally, users can manually revise the predicted layout and tables, and subsequently save them as annotations. The latter is used to measure performance and (re-)train classifiers. Finally, data in the recognized tables can be extracted for further processing. XLIndy supports several standard formats, such as CSV and JSON.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {25},
numpages = {4},
keywords = {excel, interactive, information extraction, layout inference, add-in, spreadsheets, annotation, table recognition},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345415,
author = {Lins, Rafael Dueire and Santo, Paulo Hugo Espirito and e Silva, Gabriel Pereira},
title = {Generating Digital Libraries of M.Sc. and Ph.D. Theses},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345415},
doi = {10.1145/3342558.3345415},
abstract = {Postgraduate degrees are one of the most important propellers of all areas of science. M.Sc. and Ph.D. theses witness the important developments and provide a solid and global account of research projects. This paper describes a platform developed with the aim of generating digital libraries of theses and dissertations. Printed theses have to be scanned and then processed by the platform for marginal border removal, skew and orientation correction, image segmentation and enhancement, compression and pdf file generation. Both scanned and digitally generated theses are processed in the platform to extract relevant indexing information.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {26},
numpages = {4},
keywords = {document engineering, Digital libraries, image processing},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345412,
author = {Ohta, Manabu and Yamada, Ryoya and Kanazawa, Teruhito and Takasu, Atsuhiro},
title = {A Cell-Detection-Based Table-Structure Recognition Method},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345412},
doi = {10.1145/3342558.3345412},
abstract = {If tables are automatically recognized to extract the numerical values in them, digital documents containing such tables can be augmented with graphs generated using the recognized tables. In this paper, we propose a cell-detection-based table-structure recognition method for such automatic graph generation from tables. In detecting cells in a table, ruled lines are crucial but do not necessarily surround all cells. We therefore propose a method to detect cells by estimating implicit ruled lines, where necessary, to recognize the table structure. We demonstrate the effectiveness of the proposed method by experiments using the ICDAR 2013 table competition dataset.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {27},
numpages = {4},
keywords = {XML, PDF, table-structure recognition, table-structure analysis},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345414,
author = {Piotrowski, Michael},
title = {A Vision for User-Defined Semantic Markup},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345414},
doi = {10.1145/3342558.3345414},
abstract = {Typesetting systems, such as LATEX, permit users to define custom markup and corresponding formatting to simplify authoring, ensure the consistent presentation of domain-specific recurring elements and, potentially, enable further processing, such as the generation of an index of such elements. In XML-based and similar systems, the separation of content and form is also reflected in the processing pipeline: while document authors can define custom markup, they cannot define its semantics. This could be said to be intentional to ensure structural integrity of documents, but at the same time it limits the expressivity of markup. The latter is particularly true for so-called lightweight markup languages like Mark-down, which only define very limited sets of generic elements. This vision paper sketches an approach for user-defined semantic markup that could permit authors to define the semantics of elements by formally describing the relations between its constituent parts and to other elements, and to define a formatting intent that would ensure that a default presentation is always available.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {28},
numpages = {4},
keywords = {document authoring, scholarly publishing, document models and structures, markup semantics},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345410,
author = {Scheicher, Ricardo B. and Sinoara, Roberta A. and Felinto, Jonas C. and Rezende, Solange O.},
title = {Sentiment Classification Improvement Using Semantically Enriched Information},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345410},
doi = {10.1145/3342558.3345410},
abstract = {The emergence of new and challenging text mining applications is demanding the development of novel text processing and knowledge extraction techniques. One important challenge of text mining is the proper treatment of text meaning, which may be addressed by incorporating different types of information (e.g., syntactic or semantic) into the text representation model. Sentiment classification is one of the challenging text mining applications. It may be considered more complex than the traditional topic classification since, although sentiment words are important, they may not be enough to correctly classify the sentiment expressed in a document. In this work, we propose a novel and straightforward method to improve sentiment classification performance, with the use of semantically enriched information derived from domain expressions. We also propose a superior scheme for generating these expressions. We conducted an experimental evaluation applying different classification algorithms to three datasets composed by reviews of different products and services. The results indicate that the proposed method enables the improvement of classification accuracy when dealing with reviews of a narrow domain.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {29},
numpages = {4},
keywords = {Sentiment analysis, Text classification, Text semantics},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345408,
author = {Shatnawi, Ahmed S. and Munson, Ethan V.},
title = {Enhanced Automated Policy Enforcement EXchange Framework (EAPEX)},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345408},
doi = {10.1145/3342558.3345408},
abstract = {In this paper, we describe an enhancement of the Automated Policy Enforcement eXchange framework (APEX) called eAPEX. eAPEX uses version-control information as the basis for a more incremental approach to scanning document text to determine if security policies are being followed. Where APEX requires a full or deep scan of the document each time an exposure operation is invoked, eAPEX usually requires only a scan of changed elements (deltas). A new scanning approach was designed and implemented. eAPEX works by combining version-aware document technology with a policy database that functions as a cache of security policy results.eAPEX is evaluated by testing an application that was created to simulate the behavior of the proposed scannning approaches. This evaluation suggests that an incremental approach to checking document security should yield noticeable performance benefits.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {30},
numpages = {4},
keywords = {Information leakage, Secure document engineering, Version control},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345407,
author = {Tayeh, Ahmed A. O. and Tran, Ngoc and Signer, Beat},
title = {Enhanced Document Retrieval and Discovery Based on a Combination of Implicit and Explicit Document Relationships},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345407},
doi = {10.1145/3342558.3345407},
abstract = {With the rapid increase of digital information we are dealing with in our daily work, we face significant document retrieval and discovery challenges. We present a novel document retrieval and discovery framework that addresses some of the limitations of existing solutions. An innovative aspect of our solution is the combination of implicit and explicit links between documents in the retrieval as well as in the visualisation process, in order to improve document retrieval and discovery. Our framework exploits implicit relationships between documents--defined by the similarity of their content as well as their metadata--and explicit links (hyperlinks) defined between documents based on a third-party link service. Further, the software framework can be extended with arbitrary third-party visualisations. Last but not least, our search query interface offers advanced features not available in most existing document retrieval systems.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {31},
numpages = {4},
keywords = {document discovery, clustering, Document retrieval, hypertext, search interfaces, document linking, link navigation},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345417,
author = {Tsuboi, Yuroti and Suzuki, Nobutaka},
title = {An Algorithm for Extracting Shape Expression Schemas from Graphs},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345417},
doi = {10.1145/3342558.3345417},
abstract = {Unlike traditional data such as relational databases and XML documents, most of graphs do not have their own schema. However, schema is a concise representation of a graph, and if we can extract a "good" schema from a graph, we can take advantage of the extracted schema for effective graph data management. In this paper, we focus on Shape Expression Schemas (ShEx) and consider extracting ShEx schemas from RDF/graph data. To manage both efficiency and quality of extracted schema, our algorithm consists of two schema extraction steps: (i) edge-label based clustering and (ii) type-merge method for target nodes of outgoing edges. We made preliminary experiments, which result suggests that our algorithm can extract ShEx schemas appropriately.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {32},
numpages = {4},
keywords = {RDF, graph, schema extraction},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345405,
author = {Woli\'{n}ski, Marcin},
title = {Globally Optimal Page Breaking with Column Balancing: A Case Study},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345405},
doi = {10.1145/3342558.3345405},
abstract = {The paper presents a dynamic programming algorithm that finds the globally optimal sequence of page breaks for a book avoiding widows and orphans when the only source of variation is the possibility to break selected paragraphs into varying number of lines by skillful selection of line breaks. The text is set in two-columns, on each last page of a chapter the columns must be balanced. We show how the balancing process can be included in the global optimization.The algorithm is applied to a real-life problem of typesetting a small-format two-column 800 pages long dictionary. We analyze the typesetting process including the proofing phase where local changes in the text can globally influence page breaks.This problem provides an ideal test-bed for global optimization since the typographic model involved is relatively easy - the material processed is merely a stream of paragraphs. On the other hand, breaking the book under these conditions is a very tedious and frustrating job for a human typesetter, as it typically requires hours of trial and error.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {33},
numpages = {4},
keywords = {page breaking, global optimization, automatic layout, typesetting},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345404,
author = {Yousefinaghani, Samira and Dara, Rozita and Sharif, Shayan},
title = {Impact of In-Domain Vector Representations on the Classification of Disease-Related Tweets: Avian Influenza Case Study},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345404},
doi = {10.1145/3342558.3345404},
abstract = {A number of methods have been proposed for the construction of vector representations for natural language processing (NLP) tasks. These methods have been applied to various domains and each has its own pros and cons. Despite their effectiveness, the proposed approaches usually ignore the sentiment information concerning specific tasks. In this paper, we examined various types of word vectors and their impact on the performance of a sentiment classification problem in the area of infectious diseases. Vectors were used in the embedding layer of a word-based convolutional neural network (CNN) to identify tweets pertaining to avian influenza. We proposed a new approach to build effective word embeddings for the sentiment analysis task. Furthermore, the performance of the language model was compared in terms of using various corpus sizes and vector dimensions. Our experiments indicated that initializing the sentiment learning network with domain-specific word embeddings outperforms general domain embeddings. We found that the proposed method leads to a considerable improvement in the classification performance.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {34},
numpages = {4},
keywords = {word embedding, sentiment analysis, Twitter, machine learning, Convolutional neural network, avian influenza},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345426,
author = {Beyette, Donald and Wang, Zelun and Lin, Jason and Liu, Jyh-Charn},
title = {Semi-Automatic LaTeX-Based Labeling of Mathematical Objects in PDF Documents: MOP Data Set},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345426},
doi = {10.1145/3342558.3345426},
abstract = {Mathematical objects (MO) in PDF documents is paramount in understanding the ontology and mathematical essence in published science, technology, engineering, and mathematical (STEM) documents. As of now, Marmot is the only publicly available data set for optimizing and evaluating MO labeling models in PDF documents. Thus, this paper proposes a semiautomatic labeling MO algorithm that uses PDF documents and their corresponding LaTeX source files to generate a new data set consisting of MO bounding boxes (Bbox) in PDF documents, their LaTeX equation, topic, and subject. The first step in labeling each MO is to transform the LaTeX and PDF documents into a string format. Afterwards, a shortest unique string-matching technique is proposed to align PDF pages with LaTeX files. On each page, a similar shortest string-matching technique is employed to align each LaTeX MO with its PDF counterpart. Once an MO is located, the PDF and LaTeX MOs are normalized in order to match symbols between their LaTeX and PDF representations. A number of filtering rules are set to eliminate matches that are considered exceedingly inconsistent. Matches that pass these rules will have their MOs highlighted for final manual inspection. A total of 1,802 pages in the high energy physics (hep-th) field were labelled.1},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {35},
numpages = {4},
keywords = {semi-automatic labeling, ontology, ground truth, LaTeX, PDF, Mathematical object},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345420,
author = {Brito, Eduardo and Sifa, Rafet and Bauckhage, Christian and Loitz, R\"{u}diger and Lohmeier, Uwe and P\"{u}nt, Christin},
title = {A Hybrid AI Tool to Extract Key Performance Indicators from Financial Reports for Benchmarking},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345420},
doi = {10.1145/3342558.3345420},
abstract = {We present a tool that enables benchmarking of companies by means of automatic extraction of key performance indicators from publicly available financial reports. Our tool monitors companies of interest so that their reports are automatically downloaded as soon as they become available. After tables and paragraphs have been extracted from the documents using a table detection module based on convolutional neural networks, relevant key performance indicators are stored in a central database. The extracted values are finally displayed in a user-friendly web application where the user can compare time series of key performance indicators against arbitrary available companies.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {36},
numpages = {4},
keywords = {Document Analysis, Visualization, Information Extraction},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345424,
author = {Hirschmeier, Stefan and Schoder, Detlef},
title = {Combining Word Embeddings with Taxonomy Information for Multi-Label Document Classification},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345424},
doi = {10.1145/3342558.3345424},
abstract = {In business contexts, documents often need to be classified using company-specific taxonomies. Text-classification approaches based on word embeddings have become increasingly popular as they enable words, documents, and tags to be represented in a semantically robust way (as distributed representations of their contexts) and make documents and tags processable in an algebraic vector space. However, these distributed representations of contexts have their shortcomings when used for multi-label classification tasks: the more similar the contexts of two tags, the more difficult they are to separate in classification. Intensified by poor training data, poor training, or inherent limitations of the word-embedding approach, in practice, we find areas of indistinguishability, leading to false positive predictions (typically in leaf tags of a taxonomy tree). We contribute an approach to tackle the problem of indistinguishable areas for multi-label classification tasks based on word embeddings by including taxonomy information during prediction.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {37},
numpages = {4},
keywords = {multi-label document classification, taxonomy, word embeddings, text tagging, keyword identification},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345423,
author = {Lins, Rafael Dueire and Oliveira, Hilario and Cabral, Luciano and Batista, Jamilson and Tenorio, Bruno and Salcedo, Diego A. and Ferreira, Rafael and Lima, Rinaldo and de Fran\c{c}a Pereira e Silva, Gabriel and Simske, Steven J.},
title = {The CNN-Corpus in Spanish: A Large Corpus for Extractive Text Summarization in the Spanish Language},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345423},
doi = {10.1145/3342558.3345423},
abstract = {This paper details the development and features of the CNN-corpus in Spanish, possibly the largest test corpus for single document extractive text summarization in the Spanish language. Its current version encompasses 1,117 well-written texts in Spanish, each of them has an abstractive and an extractive summary. The development methodology adopted allows good-quality qualitative and quantitative assessments of summarization strategies for tools developed in the Spanish language.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {38},
numpages = {4},
keywords = {Multi-language Summarization, Extractive Summarization, CNN Corpus, Spanish, Single-document Summarization},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345422,
author = {Mariano, Ednardo and Lins, Rafael Dueire and Fan, Jian},
title = {Enhancing Document-Camera Images},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345422},
doi = {10.1145/3342558.3345422},
abstract = {Document Camera digitalization devices are low-cost, easy to use, produce good quality images, are able to digitalize pages of bound books without damaging their spine, etc. On the other hand, they may bring two serious problems. The first one appears if the document to be digitalized is printed on glossy paper. The paper reflects the different illumination sources from the environment producing a specular noise in the document image. The second problem occurs when the document to be digitalized does not lay flat on the digitalization surface. This paper presents solutions to both problems. The results obtained in almost 400 test images may be considered satisfactory.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {39},
numpages = {4},
keywords = {Document camera, document quality, image enhancement},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345419,
author = {Schubert, Svante},
title = {The Next Millennium Document Format},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345419},
doi = {10.1145/3342558.3345419},
abstract = {Most of today's leading document formats have their roots in the eighties. Their design was built upon requirements of these days: to represent the document state on one single machine or to exchange a document by floppy disc or modem. Often designed for a single purpose far narrower than their current usage. New features were often accomplished by workarounds. For example, change-tracking of any office format does not track a defined interoperable change. Only the earlier state of the changed area is stored, to be swapped back in case of rejection. Nowadays, with the rise of mobile devices, online collaboration is ubiquitous and creates challenges when dealing with documents designed for an environment from the eighties.In this paper, we lay out a concept how to evolve a new document format that allows not only collaboration, but responsiveness and interoperability by design.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {40},
numpages = {4},
keywords = {Document repositories deduplication, Interoperable Layout, Document synchronization, Agile standardization, Collaborative editing, Document versioning},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3342558.3345421,
author = {Sifa, Rafet and Ladi, Anna and Pielka, Maren and Ramamurthy, Rajkumar and Hillebrand, Lars and Kirsch, Birgit and Biesner, David and Stenzel, Robin and Bell, Thiago and L\"{u}bbering, Max and N\"{u}tten, Ulrich and Bauckhage, Christian and Warning, Ulrich and F\"{u}rst, Benedikt and Khameneh, Tim Dilmaghani and Thom, Daniel and Huseynov, Ilgar and Kahlert, Roland and Schlums, Jennifer and Ismail, Hisham and Kliem, Bernd and Loitz, R\"{u}diger},
title = {Towards Automated Auditing with Machine Learning},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345421},
doi = {10.1145/3342558.3345421},
abstract = {We present the Automated List Inspection (ALI) tool that utilizes methods from machine learning, natural language processing, combined with domain expert knowledge to automate financial statement auditing. ALI is a content based context-aware recommender system, that matches relevant text passages from the notes to the financial statement to specific law regulations. In this paper, we present the architecture of the recommender tool which includes text mining, language modeling, unsupervised and supervised methods that range from binary classification models to deep recurrent neural networks. Next to our main findings, we present quantitative and qualitative comparisons of the algorithms as well as concepts for how to further extend the functionality of the tool.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {41},
numpages = {4},
keywords = {Business Process Optimization, Text Mining, Automated Auditing},
location = {Berlin, Germany},
series = {DocEng '19}
}

