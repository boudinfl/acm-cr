@inproceedings{10.1145/3248704,
author = {Schimmler, Sonja},
title = {Session Details: Tutorials},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248704},
doi = {10.1145/3248704},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103026,
author = {Gatos, Basilis and Louloudis, Georgios and Stamatopoulos, Nikolaos and Sfikas, Giorgos},
title = {Historical Document Processing},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103026},
doi = {10.1145/3103010.3103026},
abstract = {This tutorial focuses on recent advances and ongoing developments for historical document processing. It includes the main challenges involved, the different tasks that have to be implemented as well as practices and technologies that currently exist in the literature. The focus is given on the most promising techniques, related projects as well as on existing datasets and competitions that can be proved useful to historical document processing research.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {1–2},
numpages = {2},
keywords = {segmentation, keyword spotting, recognition, historical documents, preprocessing},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103027,
author = {Nicholas, Charles},
title = {Document Engineering Issues in Malware Analysis},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103027},
doi = {10.1145/3103010.3103027},
abstract = {We present an overview of the field of malware analysis with emphasis on issues related to document engineering. We will introduce the field with a discussion of the types of malware, including executable binaries, malicious PDFs, polymorphic malware, ransomware, and exploit kits. We will conclude with our view of important research questions in the field. This is an updated version of last year's tutorial, with more information about web-based malware and malware targeting the Android market.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {3},
numpages = {1},
keywords = {malware analysis},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3109452,
author = {Marriott, Kim and Simske, Steven and Sturgill, Margaret},
title = {Understanding the User: User Studies and User Evaluation for Document Engineering},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3109452},
doi = {10.1145/3103010.3109452},
abstract = {Document engineering is all about building systems and tools that allow people to work with documents and document collections. A key aspect is the usefulness and usability of these tools. In this tutorial, we will look at the many different kinds of user studies and user evaluations that can be used to inform the design and improve utility and usability of document engineering applications. The tutorial will be based on actual studies and will also give participants a chance to explore how they might use these techniques in their research or system development.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {5},
numpages = {1},
keywords = {user study, data analysis, user evaluation},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3248705,
author = {Camilleri, Kenneth P.},
title = {Session Details: Keynote I},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248705},
doi = {10.1145/3248705},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103025,
author = {Zammit Lupi, Theresa},
title = {The Notarial Archives, Valletta: Starting from Zero},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103025},
doi = {10.1145/3103010.3103025},
abstract = {The main objective of this paper is to talk about my work as a book and paper conservator in the light of the current rehabilitation project at the Notarial Archives in St Christopher Street, Valletta. With its six centuries of manuscript material spread over two kilometres of shelving, the state of preservation of the archives has presented numerous challenges over the last years. The EU funds granted in recent months are a crucial investment that will ensure the safeguarding of the collection, but putting one's house in order is not just about money. A number of other considerations such as careful planning, multidisciplinary collaboration, clever marketing, accessibility, team-building and creating a clear vision for the future have been some of the central factors that continue to contribute to the success of this project. A discussion on the general preservation and conservation strategies that are being undertaken for the project will also be given.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {7},
numpages = {1},
keywords = {book and paper conservation, archives, multidisciplinary collaboration, preservation},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3248706,
author = {Hassan, Tamir},
title = {Session Details: Generation, Manipulation and Presentation},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248706},
doi = {10.1145/3248706},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103011,
author = {Barrellon, Vincent and Portier, Pierre-Edouard and Calabretto, Sylvie and Ferret, Olivier},
title = {Linear Extended Annotation Graphs},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103011},
doi = {10.1145/3103010.3103011},
abstract = {Multistructured (M-S) data models were introduced to allow the expression of multilevel, concurrent annotation. However, most models lack either a consistent or an efficient validation mechanism. In a former paper, we introduced extended Annotation Graphs (eAG), a cyclic-graph data model equipped with a novel schema mechanism that, by allowing validation "by construction", bypasses the typical algorithmic cost of traditional methods for the validation of graph-structured data. We introduce here LeAG, a markup syntax for eAG annotations over text data. LeAG takes the shape of a classic, inline markup model. A LeAG annotation can then be written, in a human-readable form, in any notepad application, and saved as a text file; the syntax is simple and familiar -- yet LeAG proposes a natural syntax for multilayer annotation with (self-) overlap and links. From a theoretical point of view, LeAG inaugurates a hybrid markup paradigm. Syntactically speaking, it is a full inline model, since the tags are all inserted along the annotated resources; still, we evidence that representing independent elements' co-occurring in an inline manner requires to make the annotation rest upon a notion of reference value, that is typical of stand-off markup. To our knowledge, LeAG is the first inline markup syntax to properly conceptualize the notion of elements' accidental co-occurring, that is yet fundamental in multilevel annotation.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {9–18},
numpages = {10},
keywords = {markup model, validation, multistructured data, simulation, multilayer annotation},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103016,
author = {Pinto, Hedvan Fernandes and Soares Neto, Carlos De Salles and Colcher, S\'{e}rgio and Azevedo, Roberto Gerson De Albuquerque},
title = {The F\'{a}Bulas Model for Authoring Web-Based Children's EBooks},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103016},
doi = {10.1145/3103010.3103016},
abstract = {Nowadays, tablets and smartphones are commonly used by children for both entertainment and education purposes. In special, interactive multimedia eBooks running on those devices allow a richer experience when compared to traditional text-only books, being potentially more engaging and entertaining to readers. However, to explore the most exciting features in these environments, authors are currently left alone in the sense that there is no high level (less technical) support, and these features are usually accessible only through programming or some other technical skill. In this work, we aim at extracting the main features on enhanced children's eBooks and propose a model, named F\'{a}bulas - the Portuguese word for fables -that allows authors to create interactive multimedia children's eBooks declaratively. The model was conceived by taking, as a starting point, a systematic analysis of the common concepts, with the focus on identifying and categorizing recurring characteristics and pointing out functional and non-functional requirements that establish a strong orientation towards the set of desirable abstractions of an underlying model. Moreover, the paper presents a case study for the implementation of F\'{a}bulas on the Web, and discusses the authoring of a complete interactive story over it.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {19–28},
numpages = {10},
keywords = {multimedia authoring, conceptual model, interactive ebook},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103015,
author = {Mittelbach, Frank},
title = {Effective Floating Strategies},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103015},
doi = {10.1145/3103010.3103015},
abstract = {This paper presents an extension to the general framework for globally optimized pagination described in Mittelbach (2016). The extended algorithm supports automatic placement of floats as part of the optimization. It uses a flexible constraint model that allows for the implementation of typical typographic rules that can be weighted against each other to support different application scenarios. By "flexible" we mean that the rules of typographic presentation of the content of a document element are not fixed---but neither are they completely arbitrary; also, some of these rules are absolute whereas others are in the form of preferences. It is easy to see that without restrictions the float placement possibilities grow exponentially if the number of floats has a linear relation to the document size. It is therefore important to restrict the objective function used for optimization in a way that the algorithm does not have to evaluate all theoretically possible placements while still being guaranteed to find an optimal solution. Different objective functions are being evaluated against typical typographic requirements in order to arrive at a system that is both rich in its expressiveness of modeling a large class of pagination applications and at the same time is capable of solving the optimization problem in acceptable time for realistic input data. Frank Mittelbach. 2016. A General Framework for Globally Optimized Pagination. In Proceedings of the 2016 ACM Symposium on Document Engineering (DocEng '16). ACM, New York, NY, USA, pages 11--20.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {29–38},
numpages = {10},
keywords = {pagination, typesetting, macro-typography, page breaking, automatic layout, global optimization, adaptive layout},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3248707,
author = {King, Peter R.},
title = {Session Details: Collections, Systems and Management},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248707},
doi = {10.1145/3248707},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103017,
author = {Schwarz, Monika M. and Marriott, Kim and McCormack, Jon},
title = {The Mitchell Library WordCloud: Beyond Boolean Search},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103017},
doi = {10.1145/3103010.3103017},
abstract = {Libraries are increasingly offering on-line digital access to their collections. However, traditional search-based interfaces are restrictive and do not encourage the user to explore the collection in the same way that a physical collection does. We present the Mitchell WordCloud, a novel on-line interface to the David Scott Mitchell collection of the State Library of New South Wales. Based on interface design principles for explorative search, it presents the user with a word cloud derived from the collection and a list of titles. As the user drags words from the word cloud to tell the system what they like or dislike the title list is reordered. The surrounding interface elements -- image bar, time line and Dewey bar -- provide complementary insights into the collection. The traditional vector space model for measuring text similarity was extended to take account of user dislikes and to order words in the word cloud. User studies confirmed that the Mitchell WordCloud is easy to use and encourages exploration.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {39–48},
numpages = {10},
keywords = {word cloud, explorative search, cosine similarity, library interface design},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103022,
author = {Kane, Andrew and Tompa, Frank Wm.},
title = {Small-Term Distribution for Disk-Based Search},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103022},
doi = {10.1145/3103010.3103022},
abstract = {A disk-based search system distributes a large index across multiple disks on one or more machines, where documents are typically assigned to disks at random in order to achieve load balancing. However, random distribution degrades clustering, which is required for efficient index compression. Using the GOV2 dataset, we demonstrate the effect of various ordering techniques on index compression, and then quantify the effect of various document distribution approaches on compression and load balancing. We explore runtime performance by simulating a disk-based search system for a scaled-out 10xGOV2 index over ten disks using two standard approaches, document and term distribution, as well as a hybrid approach: small-term distribution. We find that small-term distribution has the best performance, especially in the presence of list caching, and argue that this rarely discussed distribution approach can improve disk-based search performance for many real-world installations.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {49–58},
numpages = {10},
keywords = {index compression, distributed search engines, run-time efficiency, query performance, information retrieval, index partitioning},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121038,
author = {Shatnawi, Ahmed and Munson, Ethan V. and Thao, Cheng},
title = {Maintaining Integrity and Non-Repudiation in Secure Offline Documents},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121038},
doi = {10.1145/3103010.3121038},
abstract = {Securing sensitive digital documents (such as health records, legal reports, government documents, and financial assets) is a critical and challenging task. Unreliable Internet connections, viruses, and compromised file storage systems impose a significant risk on such documents and can compromise their integrity especially when shared across domains while they are shared in offline fashion.In this paper, we present a new framework for maintaining integrity in offline documents and provide a non-repudiation security feature without relying on a central repository of certificates. This framework has been implemented as a plug-in for the Microsoft Word application. It is portable because the plug-in is attached to the document itself and it is scalable because there are no fixed limits on the numbers of users who can collaborate in producing the document. Our framework provides integrity and non-repudiation guarantees for each change in the document's version history.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {59–62},
numpages = {4},
keywords = {xml document, secure document engineering, version control},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121040,
author = {Badenes-Olmedo, Carlos and Redondo-Garcia, Jos\'{e} Luis and Corcho, Oscar},
title = {Distributing Text Mining Tasks with <i>LibrAIry</i>},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121040},
doi = {10.1145/3103010.3121040},
abstract = {We present librAIry, a novel architecture to store, process and analyze large collections of textual resources, integrating existing algorithms and tools into a common, distributed, high-performance workflow. Available text mining techniques can be incorporated as independent plug&amp;play modules working in a collaborative manner into the framework. In the absence of a pre-defined flow, librAIry leverages on the aggregation of operations executed by different components in response to an emergent chain of events. Extensive use of Linked Data (LD) and Representational State Transfer (REST) principles are made to provide individually addressable resources from textual documents. We have described the architecture design and its implementation and tested its effectiveness in real-world scenarios such as collections of research papers, patents or ICT aids, with the objective of providing solutions for decision makers and experts in those domains. Major advantages of the framework and lessons-learned from these experiments are reported.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {63–66},
numpages = {4},
keywords = {large-scale text analysis, nlp, text mining, scholarly data, data integration},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3248708,
author = {Marriott, Kim},
title = {Session Details: User Interactions},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248708},
doi = {10.1145/3248708},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103013,
author = {Zaman, Loutfouz and Stuerzlinger, Wolfgang and Neugebauer, Christian},
title = {MACE: A New Interface for Comparing and Editing of Multiple Alternative Documents for Generative Design},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103013},
doi = {10.1145/3103010.3103013},
abstract = {We present a new interface for interactive comparisons of more than two alternative documents in the context of a generative design system that uses generative data-flow networks defined via directed acyclic graphs. To better show differences between such networks, we emphasize added, deleted, (un)changed nodes and edges. We emphasize differences in the output as well as parameters using highlighting and enable post-hoc merging of the state of a parameter across a selected set of alternatives. To minimize visual clutter, we introduce new difference visualizations for selected nodes and alternatives using additive and subtractive encodings, which improve readability and keep visual clutter low. We analyzed similarities in networks from a set of alternative designs produced by architecture students and found that the number of similarities outweighs the differences, which motivates use of subtractive encoding. We ran a user study to evaluate the two main proposed difference visualization encodings and found that they are equally effective.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {67–76},
numpages = {10},
keywords = {difference visualization., generative design, alternatives, exploration, parallel editing},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121048,
author = {Orlando, Alex F. and Zaine, Isabela and Pimentel, Maria G. and De Souza, Deisy G. and Teixeira, Cesar A.C.},
title = {Interactive Documents Based on Discrete Trials},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121048},
doi = {10.1145/3103010.3121048},
abstract = {Interactive documents offer users alternatives for accessing the content available. In Education, researchers employ Individualized Learning Programs as interactive multimedia documents in order to teach students a variety of subjects. In order to author such interactive document, domain experts have to control features such as pace, duration, response-based criteria, and the hierarchy of learning units. We present how individualized learning programs can be modeled as interactive documents based on discrete trials and deterministic finite automata. We also report the main numbers associated with the use of a companion system deployed in real environments by domain specialists and learners.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {77–80},
numpages = {4},
keywords = {interactive multimedia, automata, individualized learning program},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121036,
author = {Denoue, Laurent and Carter, Scott and Marlow, Jennifer and Cooper, Matthew},
title = {DocHandles: Linking Document Fragments in Messaging Apps},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121036},
doi = {10.1145/3103010.3121036},
abstract = {In this paper, we describe DocHandles, a novel system that allows users to link to specific document parts in their chat applications. As users type a message, they can invoke the tool by referring to a specific part of a document, e.g., "@fig1 needs revision". By combining text parsing and document layout analysis, DocHandles can find and present all the figures "1" inside previously shared documents, allowing users to explicitly link to the relevant "document handle". In this way, Ddocuments become first-class citizens inside the conversation stream where users can seamlessly integrate documents in their text-centric messaging application.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {81–84},
numpages = {4},
keywords = {enterprise messaging, image processing, document capture, interactive documents},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103018,
author = {Spinaci, Gianmarco and Peroni, Silvio and Di Iorio, Angelo and Poggi, Francesco and Vitali, Fabio},
title = {The RASH JavaScript Editor (RAJE): A Wordprocessor for Writing Web-First Scholarly Articles},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103018},
doi = {10.1145/3103010.3103018},
abstract = {The most used format for submitting and publishing papers in the academic domain is the Portable Document Format (PDF), since its possibility of being rendered in the same way independently from the device used for visualising it. However, the PDF format has some important issues as well, among which the lack of interactivity and the low degree of accessibility. In order to address these issues, recently some journals, conferences, and workshops have started to accept also HTML as Web-first submission/publication format. However, most of the people are not able to produce a well-formed HTML5 article from scratch, and they would, thus, need an appropriate interface, e.g. a word processor, for creating such HTML-compliant scholarly article. To provide a solution to the aforementioned issue, in this paper we introduce the RASH JavaScript Editor (a.k.a. RAJE), which is a multi platform word processor for writing scholarly articles in HTML natively. RAJE allows authors to write research papers by means of a user-friendly interface hiding the complexities of HTML5. We also discuss the outcomes of a user study where we asked some researchers to write a scientific paper using RAJE.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {85–94},
numpages = {10},
keywords = {html-based scholarly articles, research communication, research article in simplified html, raje, rash framework, rash, web-first format},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3248709,
author = {Cristina, Stefania},
title = {Session Details: Keynote II},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248709},
doi = {10.1145/3248709},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103024,
author = {Collomosse, John},
title = {Sketched Visual Narratives for Image and Video Search},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103024},
doi = {10.1145/3103010.3103024},
abstract = {The internet is transforming into a visual medium; over 80% of the internet is forecast to be visual content by 2018, and most of this content will be consumed on mobile devices featuring a touch-screen as their primary interface. Gestural interaction, such as sketch, presents an intuitive way to interact with these devices. Imagine a Google image search in which you specify your query by sketching the desired image with your finger, rather than (or in addition to) describing it with text words. Sketch offers an orthogonal perspective on visual search - enabling concise specification of appearance (via sketch) in addition to semantics (via text). In this talk, John Collomosse will present a summary of his group's work on the use of free-hand sketches for the visual search and manipulation of images and video. He will begin by describing a scalable system for sketch based search of multi-million image databases, based upon their Gradient Field HOG (GF-HOG) descriptor. He will then describe how deep learning can be used to enhance performance of the retrieval. Imagine a product catalogue in which you sketched, say an engineering part, rather than using a text or serial numbers to find it? John will then describe how scalable search of video can be similarly achieved, through the depiction of sketched visual narratives that depict not only objects but also their motion (dynamics) as a constraint to find relevant video clips. The work presented in this talk has been supported by the EPSRC and AHRC between 2012-2016.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {95},
numpages = {1},
keywords = {sketched visual narratives, gestural interaction, image search, multimedia},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121035,
author = {Ferreira, Cristiane and Salles, Carlos and Santos, Luis and Trinta, Fernanto and Viana, Windson},
title = {Towards a Model and a Textual Representation for Location-Based Games},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121035},
doi = {10.1145/3103010.3121035},
abstract = {Location-Based Mobile Games (LBMGs) are a subclass of pervasive games that make use of location technologies to consider the players' geographic position in the game rules and mechanics. This research presents LEGaL, a language to model and represent the structure and multimedia contents (e.g., video, audio, 3D objects, etc.) of LBMGs. LEGaL is an extension of NCL (Nested Context Language) that allows the modelling and representation of mission-based games by supporting spatial and temporal relationships between game elements.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {97–100},
numpages = {4},
keywords = {location-based games, ncl, game modeling, multimedia document},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121029,
author = {Boddien, Charlotte and Heitmann, Jill and Hermuth, Florian and Lokiec, Dawid and Tan, Carlos and W\"{o}lbeling, Laura and Jung, Thomas and Israel, Johann Habakuk},
title = {SketchTab3d: A Hybrid Sketch Library Using Tablets and Immersive 3D Environments},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121029},
doi = {10.1145/3103010.3121029},
abstract = {This paper proposes a 2d sketching tool and an immersive 3d sketch library as an approach to easily create and access documents (i.e. sketches). The sketch library allows users to store, arrange and assemble their own sketches and others' in theoretically unlimited space. A user can get an idea about the general activities of all users since the sketch library is updated whenever changes are made. The system provides 2d and 3d means to access the sketch library. Whereas the 2d interfaces offers a standard dash board, the 3d environment provides unrestricted spatial access to the sketch library. Furthermore, a 2d sketching interfaces is provided in order to create sketch-based documents. Possible application areas are in the fields of engineering, design, public displays, shared knowledge applications, and art. The system was evaluated among eight participants regarding its pragmatic and hedonic qualities as well as searching performance. The results suggest that the users appreciate the particular combination of 2d and 3d technologies in SketchTab3d and requested for improvement in the 3d interaction technique. No significant differences were found in the search performance, however the physical demand during searching was perceived significantly higher in the 3d condition than in the 2d condition.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {101–104},
numpages = {4},
keywords = {virtual reality, 3d user interfaces, sketch-based interaction and modelling, mixed reality, 3dui, sketching, collaboration},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121028,
author = {Gaiffe, Bertrand},
title = {A Tool for Mixing XML Annotations},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121028},
doi = {10.1145/3103010.3121028},
abstract = {XML documents, in particular critical editions are usually very heavily annotated. They usually represent abbreviations, variant readings, edition operations etc. Among such documents, only a part of the character contents of the file is the actual edition of the text. Very often, one wants to run automatic tools on this "simple" text and thereafter re-embed the result into the original file. The tool we present here is dedicated to this embedding of annotations. In order to achieve this, the tool sets the problem as an ambiguous input and parses that ambiguous input by the grammar of the XML language. It then proposes those solutions that are syntactically correct. In case there are none, the input is modified and reparsed until at least one solution is found.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {105–108},
numpages = {4},
keywords = {xml, multiple hierarchies, parsing},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121034,
author = {Tosi, Lorraine and B\'{e}nel, Aur\'{e}lien},
title = {Authenticity in a Digital Era: Still a Document Process: The Case of Laboratory Notebooks},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121034},
doi = {10.1145/3103010.3121034},
abstract = {Asymmetric cryptography brings the ability for anyone on earth to check the signature of a digital object (Diffie &amp; Hellman, 1976). From that perspective, trusted timestamping of a digital object provides very strong evidence of its author or inventor and integrity (Haber, 1991). 26 years later, one might have expected that trusted timestamping would have long ago replaced traditional paper laboratory notebooks, which has not happened yet. In this paper, we argue that the reason is that authenticity is a document process: while trusted timestamping remains a necessary part of the process, a digital object must be involved in a sociotechnical process in order to become a document. We first point out the gap, intractable with paper, between the strict administrative workflow required to create strong evidence, and the fluidity of collaborative authoring needed for creativity. This gap is relevant to laboratory notebooks, as they are commonly used by inventors to attest that they discovered elements at a specific time, in a specific context. Then we explain the design and implementation of our software system, according to document theory (Buckland, 1997), in order to reinvent the whole process to minimize the administrative burden, while preserving its well-known and valuable properties.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {109–112},
numpages = {4},
keywords = {digital archiving., user experience, document systems and workflows, collaborative authoring and editing, security and privacy},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121033,
author = {Chan, Ka-Hou and Im, Sio-Kei and Ke, Wei},
title = {Fast Binarisation with Chebyshev Inequality},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121033},
doi = {10.1145/3103010.3121033},
abstract = {In order to enhance the binarization result of degraded document images with smudged and bleed-through background, we present a fast binarization technique that applies the Chebyshev theory in the image preprocessing. We introduce the Chebyshev filter which uses the Chebyshev inequality in the segmentation of objects and background. Our result shows that the Chebyshev filter is not only effective, but also simple, robust and easy to implement. Because of its simplicity, our method is sufficiently efficient to process live image sequences in real-time. We have implemented and compared with the Document Image Binarization Contest datasets (H-DIBCO 2014) for testing and evaluation. The experimental outcomes have demonstrated that this method achieved good result in this literature.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {113–116},
numpages = {4},
keywords = {binarization, thresholding, chebyshev inequality, handwritten document, segmentation},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121032,
author = {Mei, Jie and Islam, Aminul and Moh'd, Abidalrahman and Wu, Yajing and Milios, Evangelos},
title = {Post-Processing OCR Text Using Web-Scale Corpora},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121032},
doi = {10.1145/3103010.3121032},
abstract = {We introduce a (semi-)automatic OCR post-processing system that utilizes web-scale linguistic corpora in providing high-quality correction. This paper is a comprehensive system overview with the focus on the computational procedures, applied linguistic analysis, and processing optimization.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {117–120},
numpages = {4},
keywords = {ocr error correction, ocr post-processing, statistical learning},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121031,
author = {Schurov, Ilya V.},
title = {Qqmbr and Indentml: Extensible Mathematical Publishing for Web and Paper},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121031},
doi = {10.1145/3103010.3121031},
abstract = {We present qqmbr, novel publishing system aimed at preparation of high-quality mathematical publications. One source can be converted to a single interactive webpage, multi-page website or PDF (via LaTeX). The markup language behind qqmbr entitled indentml is designed to be both human-readable and machine-readable (easily parsable). It is possible to extend basic qqmbr markup with custom tags that enrich its semantics and build plugins and applications that query qqmbr documents, extract information from them and process it in an arbitrary way without much effort.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {121–124},
numpages = {4},
keywords = {latex, markup languages, xml},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121030,
author = {Das, Sagnik and Mishra, Gaurav and Sudharshana, Akshay and Shilkrot, Roy},
title = {The Common Fold: Utilizing the Four-Fold to Dewarp Printed Documents from a Single Image},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121030},
doi = {10.1145/3103010.3121030},
abstract = {Handheld cameras are currently the device of choice for performing document digitization, due to their convenience, ubiquity and high performance at low cost. Software methods process a captured image, to rectify distortions and reconstruct the original document. Existing methods struggle to reconstruct a flattened version given a single image of a document distorted by folding. We propose a novel non-parametric page dewarping approach from a single image based on deep learning to identify creases due to folds on the paper. Our method then performs a 2D boundary method based on polynomial regression, and a Coons patch, to get a flattened reconstruction. We found our method improves OCR word accuracy by more than 2.5 times when compared to the original distorted image.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {125–128},
numpages = {4},
keywords = {folded document dewarping, document reconstruction, document image processing},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121027,
author = {Filho, Alexandre Azevedo and Munson, Ethan V. and Thao, Cheng},
title = {Improving Version-Aware Word Documents},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121027},
doi = {10.1145/3103010.3121027},
abstract = {Coakley et al. described how they developed Version Aware Word Documents, which is an enhanced document representation that includes a detailed version history that is self-contained and portable. However, they were not able to adopt the unique-ID-based techniques that have been shown to support efficient merging and differencing algorithms. This application note describes how it is possible to adapt existing features of MS Word's OOXML representation to provide a system of unique element IDs suitable for those algorithms. This requires taking over Word's Revision Save ID (RSID) system and also defining procedures for specifying ID values for elements that do not support the RSID mechanism. Important limitations remain but appear surmountable.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {129–132},
numpages = {4},
keywords = {xml document, document engineering, collaborative editing, version control},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121026,
author = {Nagao, Yuma and Suzuki, Nobutaka},
title = {Classification of MathML Expressions Using Multilayer Perceptron},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121026},
doi = {10.1145/3103010.3121026},
abstract = {MathML consists of two sets of elements: Presentation Markup and Content Markup. The former is more widely used to display math expressions in Web pages, while the latter is more suited to the calculation of math expressions. In this paper, we consider classifying math expressions in Presentation Markup. In general, a math expression in Presentation Markup cannot be uniquely converted into the corresponding expression in Content Markup. If the class of a given math expression can be identified automatically, such conversions can be done more appropriately. Moreover, identifying the class of a given math expression is useful for text-to-speech of math expression. In this paper, we propose a method for classifying math expressions in Presentation Markup by using a kind of deep learning; multilayer perceptron. Experimental results show that our method classifies math expressions with high accuracy.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {133–136},
numpages = {4},
keywords = {mathml, multilayer perceptron, classification},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3248710,
author = {Tompa, Frank},
title = {Session Details: Document Analysis: Classification and Similarity},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248710},
doi = {10.1145/3248710},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121044,
author = {Simske, Steven and Vans, Marie},
title = {Learning before Learning: Reversing Validation and Training},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121044},
doi = {10.1145/3103010.3121044},
abstract = {In the world of ground truthing--that is, the collection of highly valuable labeled training and validation data-there is a tendency to follow the path of first training on a set of data, then validating the data, and then testing the data. However, in many cases the labeled training data is of non-uniform quality, and thus of non-uniform value for assessing the accuracy and other performance indicators for analytics algorithms, systems and processes. This means that one or more of the so-labeled classes is likely a mixture of two or more clusters or sub-classes. These data may inhibit our ability to assess the classifier to use for deployment. We argue that one must learn about the labeled data before the labeled data can be used for downstream machine learning; that is, we reverse the validation and training steps in building the classifier. This "learning before learning" is assessed using a CNN corpus (cnn.com) which was hand-labeled as comprising 12 classes. We show how the suspect classes are identified using the initial validation, and how training after validation occurs. We then apply this process to the CNN corpus and show that it consists of 9 high-quality classes and three mixed-quality classes. The effects of this validation-training approach is then shown and discussed.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {137–140},
numpages = {4},
keywords = {classification, validation, ensemble methods, tf*idf, training},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121041,
author = {Iwatsuki, Kenichi and Sagara, Takeshi and Hara, Tadayoshi and Aizawa, Akiko},
title = {Detecting In-Line Mathematical Expressions in Scientific Documents},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121041},
doi = {10.1145/3103010.3121041},
abstract = {One of the issues in extracting natural language sentences from PDF documents is the identification of non-textual elements in a sentence. In this paper, we report our preliminary results on the identification of in-line mathematical expressions. We first construct a manually annotated corpus and apply conditional random field (CRF) for the math-zone identification using both layout features, such as font types, and linguistic features, such as context n-grams, obtained from PDF documents. Although our method is naive and uses a small amount of annotated training data, our method achieved an 88.95% F-measure compared with 22.81% for existing math OCR software.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {141–144},
numpages = {4},
keywords = {in-line mathematical expression detection, scientific paper mining, mathematical formula recognition, math ir, pdf structure analysis},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121039,
author = {Ai, Zichu and Mei, Jie and Moh'd, Abidalrahman and Zeh, Norbert and He, Meng and Milios, Evangelos},
title = {High-Performance Computational Framework for Phrase Relatedness},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121039},
doi = {10.1145/3103010.3121039},
abstract = {TrWP is a text relatedness measure that computes semantic similarity between words and phrases utilizing aggregated statistics from the Google Web 1T 5-gram corpus. The phrase similarity computation in TrWP is costly in terms of both time and space, making the existing implementation of TrWP impractical for real-world usage. In this work, we present an in-memory computational framework for TrWP, which optimizes the corpus search using perfect hashing and minimizes the required memory cost using variable length encoding. Evaluated using the Google Web 1T 5-gram corpus, we demonstrate that the computational speed of our framework outperforms a file-based implementation by several orders of magnitude.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {145–148},
numpages = {4},
keywords = {semantic text similarity, indexing, compression},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3248711,
author = {Brailsford, David F.},
title = {Session Details: Document Analysis: Content Analysis},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248711},
doi = {10.1145/3248711},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121043,
author = {Al-Zaidy, Rabah A. and Giles, C. Lee},
title = {Automatic Knowledge Base Construction from Scholarly Documents},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121043},
doi = {10.1145/3103010.3121043},
abstract = {The continuing growth of published scholarly content on the web ensures the availability of the most recent scientific findings to researchers. Scholarly documents, such as research articles, are easily accessed by using academic search engines that are built on large repositories of scholarly documents. Scientific information extraction from documents into a structured knowledge graph representation facilitates automated machine understanding of a document's content. Traditional information extraction approaches, that either require training samples or a preexisting knowledge base to assist in the extraction, can be challenging when applied to large repositories of digital documents. Labeled training examples for such large scale are difficult to obtain for such datasets. Also, most available knowledge bases are built from web data and do not have sufficient coverage to include concepts found in scientific articles. In this paper we aim to construct a knowledge graph from scholarly documents while addressing both these issues. We propose a fully automatic, unsupervised system for scientific information extraction that does not build on an existing knowledge base and avoids manually-tagged training data. We describe and evaluate a constructed taxonomy that contains over 15k entities resulting from applying our approach to 10k documents.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {149–152},
numpages = {4},
keywords = {taxonomy construction, scholarly documents, knowledge base},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103023,
author = {Nandhakumar, Nidhin and Sherkat, Ehsan and Milios, Evangelos E. and Gu, Hong and Butler, Michael},
title = {Clinically Significant Information Extraction from Radiology Reports},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103023},
doi = {10.1145/3103010.3103023},
abstract = {Radiology reports are one of the most important medical documents that a diagnostician looks into, especially in the emergency context. They provide the emergency physicians with critical information regarding the condition of the patient and help the physicians take immediate action on urgent conditions. However, the reports are in the form of unstructured text, which makes them time consuming for humans to interpret. We have developed a machine learning system to (a) efficiently extract the clinically significant parts and their level of importance in radiology reports, and (b) to classifies the overall report into critical or non-critical categories which help doctors to identify potential high priority reports. As a starting point, the system uses anonymized chest X-RAY reports of adults and provides three levels of importance for medical phrases. We used the Conditional Random Field (CRF) model to identify clinically significant phrases with an average f1-score of 0.75. The proposed system includes a web-based interface which highlights the medical phrases, and their level of importance to the emergency physician. The overall classification of the report is performed using the phrases extracted from the CRF model as features for the classifier. Average accuracy achieved is 85%.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {153–162},
numpages = {10},
keywords = {information extraction, radiology reports, classification},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103020,
author = {Borg, Mark and Camilleri, Kenneth P.},
title = {Towards a Transcription System of Sign Language Video Resources via Motion Trajectory Factorisation},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103020},
doi = {10.1145/3103010.3103020},
abstract = {Sign languages are visual languages used by the Deaf community for communication purposes. Whilst recent years have seen a high growth in the quantity of sign language video collections available online, much of this material is hard to access and process due to the lack of associated text-based tagging information and because 'extracting' content directly from video is currently still a very challenging problem. Also limited is the support for the representation and documentation of sign language video resources in terms of sign writing systems. In this paper, we start with a brief survey of existing sign language technologies and we assess their state of the art from the perspective of a sign language digital information processing system. We then introduce our work, focusing on vision-based sign language recognition. We apply the factorisation method to sign language videos in order to factor out the signer's motion from the structure of the hands. We then model the motion of the hands in terms of a weighted combination of linear trajectory basis and apply a set of classifiers on the basis weights for the purpose of recognising meaningful phonological elements of sign language. We demonstrate how these classification results can be used for transcribing sign videos into a written representation for annotation and documentation purposes. Results from our evaluation process indicate the validity of our proposed framework.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {163–172},
numpages = {10},
keywords = {computer vision, factorisation method, sign language recognition, sign language transcription, annotation tools},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103019,
author = {Lombardo, Vincenzo and Damiano, Rossana and Pizzo, Antonio and Terzulli, Carmi},
title = {The Intangible Nature of Drama Documents: An FRBR View},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103019},
doi = {10.1145/3103010.3103019},
abstract = {As a pervasive form of artistic expression through ages and me- dia, drama features a twofold nature of its tangible manifestations (theatrical performances, movies, books, etc.) and its intangible abstraction (the story of Cinderella underlying Disney movie and Perrault's fable). The encoding of the intangible drama abstraction of drama documents is relevant for the preservation of cultural heritage and the didactics and research on drama documents. This paper addresses the task of encoding the notion of intangible story abstraction from the drama documents. The reference model is provided by a computational ontology that formally encodes the elements that characterize a drama, for purposes of semantic link- ing and inclusion in annotation schemata. By providing a formal expression posited between drama as work and its manifestations, the ontology-based representation is compliant with the model of Functional Requirements for Bibliographic Records (FRBR).},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {173–182},
numpages = {10},
keywords = {drama annotation},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3248712,
author = {Simske, Steven J.},
title = {Session Details: Document Analysis &amp; #38; Visual Document Analysis},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248712},
doi = {10.1145/3248712},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103021,
author = {Lins, Rafael Dueire and de Almeida, Marcos Martins and Bernardino, Rodrigo Barros and Jesus, Darlisson and Oliveira, Jos\'{e} M\'{a}rio},
title = {Assessing Binarization Techniques for Document Images},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103021},
doi = {10.1145/3103010.3103021},
abstract = {Image binarization is a technique widely used for documents as monochromatic documents claim for far less space for storage and computer bandwidth for network transmission than their color or even grayscale equivalent. Paper color, texture, aging, translucidity, kind and color of ink used in handwritting, printing process, digitalization process, etc., are some of the factors that affect binarization. No algorithm is good enough to be a winner in the binarization of all kinds of documents. This paper presents a methodology to assess the performance of binarization algorithms for a wide variety of text documents, allowing a judicious quantitative choice of the best algorithms and their parameters.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {183–192},
numpages = {10},
keywords = {documents, back-to-front interference, bleeding, binarization, big-data, show through, image filtering},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121037,
author = {Fawzi, Ahmed and Pastor, Mois\'{e}s and Mart\'{\i}nez-Hinarejos, Carlos D.},
title = {Baseline Detection on Arabic Handwritten Documents},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121037},
doi = {10.1145/3103010.3121037},
abstract = {Document processing comprises different steps depending on the nature of the documents. For text documents, specially for handwritten documents, transcription of their contents is one of the main tasks. Handwritten Text Recognition (HTR) is the process of automatically obtaining the transcription of the content of a handwritten text document. In document processing, the basic unit for the acquisition process is the page image, whilst line image is the basic form for the HTR process. This is a bottle-neck which is holding back the massive industrial document processing. Baseline detection can be used not only to segment page images into line images but also for many other document processing steps. Baseline detection problem can be formulated as a clustering problem over a set of interest points. In this work, we study the use of an automatic baseline detection technique, based on interest point clustering, in Arabic handwritten documents. The experiments reveal that this technique provides promising results for this task.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {193–196},
numpages = {4},
keywords = {handwritten arabic document, extremely randomized trees, baseline text detection, dbscan},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121042,
author = {Hassan, Tamir and Verges-Llahi, Jaume and Gonzalez, Andres},
title = {High-Performance Preprocessing of Architectural Drawings for Legend Metadata Extraction via OCR},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121042},
doi = {10.1145/3103010.3121042},
abstract = {This paper describes the results of an investigation into methods of preprocessing architectural plots to enable them to be processed very quickly via OCR, detecting the region containing the relevant metadata legend and obtaining it in machine-readable form for e.g. automated folding and filenaming applications. We show how a processing pipeline adapted to this type of content can vastly decrease processing time, maintaining acceptable accuracy. Initial results show a reduction in total processing time from 2--3 minutes to around 15 seconds for most documents encountered, with the folding orientation being correctly detected in 78% of cases and the legend region being completely detected in 60% of cases, high enough for the use-case at hand.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {197–200},
numpages = {4},
keywords = {ocr preprocessing, image processing, document analysis},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103012,
author = {Bonnici, Alexandra and Cristina, Stefania and Camilleri, Kenneth P.},
title = {Preparation of Music Scores to Enable Hands-Free Page Turning Based on Eye-Gaze Tracking},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103012},
doi = {10.1145/3103010.3103012},
abstract = {Digital copies of musical scores may be saved on tablet devices, compressing volumes of scores into a single portable device. Tablet screens are however typically smaller than printed sheet music such that the score needs to be resized for readability. This necessitates additional page turning which is made more complex when repeat instructions are used since these give rise to forward and backward page turns of the music. In this paper, we tackle this problem by first performing image analysis of the score in order to identify repeat instructions and hence flatten the score. Thus, the music player is presented the score as it should be played. We then propose the use of eye-gaze tracking to provide a hands-free page turning mechanism. Thus, the player remains in full control of when the page turn occurs. Through a preliminary study, we found that our proposed score flattening and eye-gaze page turning reduced the time spent navigating the page turns by 47% in comparison to available music score reading tools.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {201–210},
numpages = {10},
keywords = {page-turning, repeat detection, eye-gaze tracking, flattened scores},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3248713,
author = {Munson, Ethan V.},
title = {Session Details: Multimedia and Mobile Documents},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248713},
doi = {10.1145/3248713},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
numpages = {1},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3103014,
author = {Abreu, Raphael and dos Santos, Joel A.F.},
title = {Using Abstract Anchors to Aid The Development of Multimedia Applications With Sensory Effects},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103014},
doi = {10.1145/3103010.3103014},
abstract = {Declarative multimedia authoring languages allows authors to combine multiple media objects, generating a range of multimedia presentations. Novel multimedia applications, focusing at improving user experience, extend multimedia applications with multisensory content. The idea is to synchronize sensory effects with the audiovisual content being presented. The usual approach for specifying such synchronization is to mark the content of a main media object (e.g. a main video) indicating the moments when a given effect has to be executed. For example, a mark may represent when snow appears in the main video so that a cold wind may be synchronized with it. Declarative multimedia authoring languages provide a way to mark subparts of a media object through anchors. An anchor indicates its begin and end times (video frames or audio samples) in relation to its parent media object. The manual definition of anchors in the above scenario is both not efficient and error prone (i) when the main media object size increases, (ii) when a given scene component appears several times and (iii) when the application requires marking scene components. This paper tackles this problem by providing an approach for creating abstract anchors in declarative multimedia documents. An abstract anchor represents (possibly) several media anchors, indicating the moments when a given scene component appears in a media object content. The author, therefore is able to define the application behavior through relationships among, for example, sensory effects and abstract anchors. Prior to executing, abstract anchors are automatically instantiated for each moment a given element appears and relationships are cloned so the application behavior is maintained. This paper presents an implementation of the proposed approach using NCL (Nested Context Language) as the target language. The abstract anchor processor is implemented in Lua and uses available APIs for video recognition in order to identify the begin and end times for abstract anchor instances. We also present an evaluation of our approach using a real world use cases.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {211–218},
numpages = {8},
keywords = {multisensory content, multimedia authoring, video recognition, anchors, ncl, mulsemedia},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121047,
author = {M. Uscamayta, A. Omar and Cunha, Bruna C.R. and Martins, Diogo S. and Pimentel, Maria G.},
title = {Opportunistic Collaborative Mobile-Based Multimedia Authoring Based on the Capture of Live Experiences},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121047},
doi = {10.1145/3103010.3121047},
abstract = {Despite recent results allowing collaborative video capture using mobile devices, there is a gap in promoting collaborative capture of media other than video. In this paper we report our collaborative model supporting amateur and opportunistic collaborative recording of multiple media using mobile devices. We present a case study carried out in the educational domain. We include a motivating scenario and related requirements, our proposed architecture and associated proof-of-concept prototype for supporting mobile amateur collaborative recording.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {219–222},
numpages = {4},
keywords = {cscw., annotation, ink, video, audio, photo, bookmark, ubicomp},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121046,
author = {Viel, Caio C. and Rodrigues, Kamila R.H. and Zaine, Isabela and Cunha, Bruna C.R. and Scalco, Leonardo F. and Pimentel, Maria G.C.},
title = {Personalized Ubiquitous Data Collection and Intervention as Interactive Multimedia Documents},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121046},
doi = {10.1145/3103010.3121046},
abstract = {The Experience Sampling Method (ESM) has been proposed as a method for collecting data about people's experiences in their everyday and natural environments. ESM-based systems offer limited authoring for interactive documents designed to collect text-based responses offered as answers to text-based questions, and integrated with the non-intrusive data collection from sensors. From a document engineering perspective, ESM brings new requirements with respect to the authoring of non-trivial interaction and navigation workflow, in particular when multiple media and collaborative tasks are concerned. Tackling existing challenges, we modeled the Experience Sampling and Programmed Intervention Method (ESPIM) by combining ESM, individualized teaching procedures and ubiquitous computing toward producing interactive personalized multimedia documents applied in data collection.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {223–226},
numpages = {4},
keywords = {mobile player., data collection, authoring, multimedia presentation},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.1145/3103010.3121045,
author = {Eichmann, Philipp and Green, Trent and Zeleznik, Robert and van Dam, Andries},
title = {NuSys: Towards a Document IDE for Knowledge Work},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3121045},
doi = {10.1145/3103010.3121045},
abstract = {Knowledge workers consume and annotate digital documents such as PDF files, videos, images and text notes - in some cases collaboratively - to form mental models and gain insight. An abundance of software solutions and utilities that were designed to assist users in stages of this process but not in the process as a whole, which makes knowledge work with documents unnecessarily inefficient. In this paper, we introduce ideas on how to streamline common knowledge worker tasks, such as collaboratively searching, gathering and freely arranging fragments of various media documents to gain understanding and then transforming emergent insights into interactive structured visualizations. Furthermore, we present NuSys, an integrated development environment (IDE) specialized for document-centric workflows, that implements the core of these ideas.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {227–230},
numpages = {4},
keywords = {knowledge work, document handling, user interface},
location = {Valletta, Malta},
series = {DocEng '17}
}

