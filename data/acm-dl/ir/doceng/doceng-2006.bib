@inproceedings{10.1145/3245601,
author = {Brailsford, D. F.},
title = {Session Details: Document Layout},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245601},
doi = {10.1145/3245601},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166165,
author = {Hurst, Nathan and Marriott, Kim and Moulder, Peter},
title = {Minimum Sized Text Containment Shapes},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166165},
doi = {10.1145/1166160.1166165},
abstract = {In many text-processing applications, we would like shapes that expand (or shrink) in size to fit their textual content. We address how to efficiently compute the minimum size for such text shapes. A variant of this problem is to take a fixed shape and determine the maximal size font that will still allow the content to fit into it. Our approach is to model the problem as a constrained optimisation problem with a single variable that controls the geometry of the text shape. We use a variant of secant search to determine the minimum area for the shape, guided by the area of the text. We represent the shape by regions that are composed of trapezoids whose coordinates are a linear function of the unknown variable. This allows us to use a novel linear time algorithm (based on computing Minkowski difference) that takes a trapezoid list and text height and determines the region in which a line of text of that height and some minimum width can start and still remain inside the shape.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {3–12},
numpages = {10},
keywords = {constrained optimization, textbox, adaptive layout},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166166,
author = {Faria, Alexis Cabeda and de Oliveira, Joao B. S.},
title = {Measuring Aesthetic Distance between Document Templates and Instances},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166166},
doi = {10.1145/1166160.1166166},
abstract = {Adaptive documents undergo many transformations during their generation, including insertion and deletion of content. One major problem in this scenario is the preservation of the aesthetic qualities of the document during those transformations.As adaptive documents are instances of a template, the aesthetic quality of an instance with respect to the template could be evaluated by aesthetic measures providing scores to any desired quality parameters. These parameters measure the deviation of the instance from the desired template. This evaluation could assure the quality of instances during their generation and final output.This paper introduces the use of document templates to support aesthetic measures of document instances. A score is assigned to a document instance according to the differences detected from the original template. Considering the original template as an ideal result, the quality of a document instance will decrease according to the number and severity of the changes applied to produce it. So, documents that are below a given threshold can be sent for further (possibly human) review, and any others are accepted.The amount of change with respect to the template will reflect the document quality, and in such a model the quality of instances can be considered as a distance from that original.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {13–21},
numpages = {9},
keywords = {aesthetics, document, measures, layout},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166167,
author = {Chao, Hui},
title = {Text Block Geometric Shape Analysis},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166167},
doi = {10.1145/1166160.1166167},
abstract = {When graphic artist designs a page, they envision a set of text blocks of arbitrary shapes constrained by page size, image blocks and graphics blocks with wrap around properties. We call this the intended shape. What is seen on an actual page depends on the particular text content and typographical constrains such as natural text line breaking and justification. We call this the apparent shape. Our goal is to create document templates by extracting the text blocks' intended shapes from the apparent shapes. The main difficulty is when the line justification is jagged the intended block shape is obfuscated. We solve this problem by analyzing the layout relation of all blocks on a page and applying an iterative process to find the maximum likelihood of the intended shapes.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {22–24},
numpages = {3},
keywords = {document geometric layout analysis, page segmentation, template creation},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166168,
author = {Macdonald, Alexander J. and Brailsford, David F. and Lumley, John},
title = {Evaluating Invariances in Document Layout Functions},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166168},
doi = {10.1145/1166160.1166168},
abstract = {With the development of variable-data-driven digital presses where each document printed is potentially unique there is a need for pre-press optimization to identify material that is invariant from document to document. In this way rasterisation can be confined solely to those areas which change between successive documents thereby alleviating a potential performance bottleneck.Given a template document specified in terms of layout functions, where actual data is bound at the last possible moment before printing, we look at deriving and exploiting the invariant properties of layout functions from their formal specifications. We propose future work on generic extraction of invariance from such properties for certain classes of layout functions.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {25–27},
numpages = {3},
keywords = {XSLT, SVG, document layout, XML, optimisation},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166169,
author = {Hurst, Nathan and Marriott, Kim and Albrecht, David},
title = {Solving the Simple Continuous Table Layout Problem},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166169},
doi = {10.1145/1166160.1166169},
abstract = {Automatic table layout is required in web applications. Unfortunately, this is NP-hard for reasonable layout requirements such as minimizing table height for a given width. One approach is to solve a continuous relaxation of the layout problem in which each cell must be large enough to contain the area of its content. The solution to this relaxed problem can then guide the solution to the original problem. We give a simple and efficient algorithm for solving this continuous relaxation for the case that cells do not span multiple columns or rows. The algorithm is not only interesting in its own right but also because it provides insight into the geometry of table layout.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {28–30},
numpages = {3},
keywords = {constrained optimization, automatic table layout},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245602,
author = {Simske, S. J.},
title = {Session Details: Poster Presentations},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245602},
doi = {10.1145/3245602},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166171,
author = {Bagley, Steven R.},
title = {COG Extractor},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166171},
doi = {10.1145/1166160.1166171},
abstract = {The Component Object Graphic (COG) model describes documents as a series of distinct, encapsulated graphical blocks (termed COGs) that are positioned on the page rather than the traditional approach (taken by formats such as PostScript, PDF and SVG) of describing each page as one monolithic block of drawing operators that create marks on the page to form the content. Previous work [1, 2] has demonstrated how this paradigm can be implemented on top of both PDF and SVG.Tools have previously been created which allow the creation of COG documents [1, 3] and manipulation of existing COGs to form new documents [1, 4]. Missing from the COG toolkit has been a method of producing COGs from already existing content. The proposed solution is a new tool entitled 'COG Extractor' that allows the user to select an area of an existing document to be extracted and converted to be a COG in PDF or SVG form.The first process is to get the documents into a format that can easily be understood. The most sensible choice is PDF since there are several tools that can convert other formats into PDF.It is then necessary to parse the PDF content stream for the page to be extracted and to derive the meaning of the operators in the page's content stream. PDF's operators can be divided into two classes: those that image content, and those that define the state (such as fill colour, line width) that content is imaged with. This causes problems since each imaging operators depends on the cumulative effect of all previous state operators. It is therefore necessary to work out exactly which operators are responsible for drawing the content.This is achieved by combining consecutive state operators together to form a state-change object, which encapsulates all changes in state at that point. Everything following the state-change object in the PDF content stream then become children of that state-change object. This produces a tree representation of the document where the drawing operators form the leaf nodes. The state of any drawing operator can then be calculated by walking back along its parents in the tree.From this tree it is then possible to calculate the bounding box of each drawing operator and see if it intersects with the area to be extracted. If they do not intersect, then that drawing operator can be pruned from the tree.An optimization phase can remove any state-change nodes that are no longer required (e.g nodes for which all children have been removed). This tree can then be exported as a COG in PDF or SVG format for future use.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {31},
numpages = {1},
keywords = {SVG, FormXObject, PDF, graphic objects, COGs},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166172,
author = {Witschel, Hans Friedrich},
title = {Carrot and Stick: Combining Information Retrieval Models},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166172},
doi = {10.1145/1166160.1166172},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {32},
numpages = {1},
keywords = {language models, weighting schemes},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166173,
author = {Gormish, Michael and Schwartz, Edward L.},
title = {Standards Based High Resolution Document Editing Using Low Resolution Proxy Images},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166173},
doi = {10.1145/1166160.1166173},
abstract = {This poster presents an implementation of the JPIP-JPM standard which allows low bandwidth access to remote high resolution documents. The implementation goes beyond the standard by allowing simple editing operations at the client without obtaining all of the compressed data. These edits form a new version of the document that can be easily uploaded over low bandwidth connections.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {33},
numpages = {1},
keywords = {JPM, JPEG2000, JPIP, partial image transmission},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245603,
author = {Nicholas, C. M.},
title = {Session Details: Document Management},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245603},
doi = {10.1145/3245603},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166175,
author = {Norrie, Moira C. and Signer, Beat and Weibel, Nadir},
title = {Print-n-Link: Weaving the Paper Web},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166175},
doi = {10.1145/1166160.1166175},
abstract = {Citations form the basis for a web of scientific publications. Search engines, embedded hyperlinks and digital libraries all simplify the task of finding publications of interest on the web and navigating to cited publications or web sites. However the actual reading of publications often takes place on paper and frequently on the move. We present a system Print-n-Link that uses technologies for interactive paper to enhance the reading process by enabling users to access digital information and/or searches for cited documents from a printed version of a publication using a digital pen for interaction. A special virtual printer driver automatically generates links from paper to digital services during the printing process based on an analysis of PDF documents. Depending on the user setting and interaction gesture, the system may retrieve metadata about the citation and inform the user through an audio channel or directly display the cited document on the user's screen.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {34–43},
numpages = {10},
keywords = {document integration, digital library, interactive paper, citation management},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166176,
author = {Wong, Sylvia C. and Crowder, Richard M. and Wills, Gary B. and Shadbolt, Nigel R.},
title = {Knowledge Engineering from Frontline Support to Preliminary Design},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166176},
doi = {10.1145/1166160.1166176},
abstract = {The design and maintenance of complex engineering systems such as a jet engine generates a significant amount of documentation. Increasingly, aerospace manufacturers are shifting their focus from selling products to providing services. As a result, when designing new engines, engineers must increasingly consider the life-cycle requirements in addition to design parameters. To identify possible areas of concern, engineers must obtain knowledge gained from the entire life of an engine. However, because of the size and distributed nature of a company's operation, engineers often do not have access to front-line maintenance data. In addition, the large number of documents accrued makes it impossible to examine thoroughly. This paper presents a prototype knowledge-based document repository for such an application. It searches and analyzes distributed document resources, and provides engineers with a summary view of the underlying knowledge. The aim is to aid engineers in creating design requirement documents that incorporate aftermarket issues. Unlike existing document repositories and digital libraries, our approach is knowledge-based, where users browse summary reports instead of following suggested links. To test the validity of our proposed architecture, we have developed and deployed a working prototype. The prototype has been demonstrated to engineers and received positive reviews.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {44–52},
numpages = {9},
keywords = {service-oriented architecture, intelligent documents, semantic web},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166177,
author = {Kuo, Y. S. and Tseng, Lendle and Hu, Hsun-Cheng and Shih, N. C.},
title = {An XML Interaction Service for Workflow Applications},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166177},
doi = {10.1145/1166160.1166177},
abstract = {Interactions with human users are a crucial part of many workflow applications. In workflow or business process management specifications, such as WSBPEL, all data are represented as XML. As a consequence, many human tasks simply support users to create or update XML data compliant with a schema. We propose the XML interaction service as a generic Web service for use by human tasks that provides HTML form-based Web interfaces for users to interact with and update schema-compliant XML data. In addition, a visual interface design tool is provided for interface designers to create and customize the Web interfaces an XML interaction service supports.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {53–55},
numpages = {3},
keywords = {XML, user interface, workflow management},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166178,
author = {Haas, Bertrand},
title = {Engineering Better Voting Systems},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166178},
doi = {10.1145/1166160.1166178},
abstract = {We consider here an election ballot as a document, a document that works as the carrier of a voter's choice in an election's accounting system for determining a winning candidate. And we consider a voting system as a way to manage both the document and its flow in compliance with the requirements of the election. Trustworthy elections are the core of a democratic spirit and engineering a voting system with requirements of convenience, privacy, integrity and reliability lies at the core of trustworthy elections. We define here some clear requirements for a trustworthy voting system and analyze the most popular classes of voting systems according to some of these requirements. We draw conclusions on the engineering of better voting systems and show two efforts in this direction.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {56–58},
numpages = {3},
keywords = {voting, ballots, electronic, mail, security},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166179,
author = {Triebsees, Thomas and Borghoff, Uwe M.},
title = {Preservation-Centric and Constraint-Based Migration of Digital Documents},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166179},
doi = {10.1145/1166160.1166179},
abstract = {We introduce a framework that supports archivists in planning and running migrations. The central idea is that -once relevant information pieces of digital documents are modeled -desired migration results can be specified by means of preservation constraints. From these constraint specifications we are able to derive migration algorithms that provably respect a set of document properties before (pre-conditions) and after migration (post-conditions). Underlying is the concept of Abstract State Machines (ASM) modeling archival states. Migrations are modeled as sequences of basic operations that change the archive's state while respecting userdefined constraints. Among others, our target scenarios comprise legal and medical documents where considerable property changes cannot be tolerated and where constraint preservation must hold over a long period of time.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {59–61},
numpages = {3},
keywords = {model, migration, preservation, digital archive},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166180,
author = {Zerida, Nadia and Lucas, Nadine and Cr\'{e}milleux, Bruno},
title = {Combining Linguistic and Structural Descriptors for Mining Biomedical Literature},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166180},
doi = {10.1145/1166160.1166180},
abstract = {This work proposes an original combination of linguistic and structural descriptors to represent the content of biomedical papers. The objective is to show the effectiveness of descriptors taking into account the structure of documents to characterise three kinds of biomedical texts (reviews, research and clinical papers). The description of text is made at various levels, from the global level to the local one. The contexts makes it possible to characterise the three classes. The characterisation of the textual resources is carried out quantitatively by using the discriminating capacity of techniques of data mining based on emerging patterns.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {62–64},
numpages = {3},
keywords = {text mining preprocessing, characterisation, categorisation},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245604,
author = {King, P. R.},
title = {Session Details: XML-Based Document Structure and Analysis},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245604},
doi = {10.1145/3245604},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166182,
author = {Genev\`{e}s, Pierre and Laya\"{\i}da, Nabil},
title = {Comparing XML Path Expressions},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166182},
doi = {10.1145/1166160.1166182},
abstract = {XPath is the standard declarative language for navigating XML data and returning a set of matching nodes. In the context of XSLT/XQuery analysis, query optimization, and XML type checking, XPath decision problems arise naturally. They notably include XPath comparisons such as equivalence (whether two queries always return the same result), and containment (whether for any tree the result of a particular query is included in the result of a second one).XPath decision problems have attracted a lot of research attention, especially for studying the computational complexity of various XPath fragments. However, what is missing at present is the constructive use of an expressive logic which would allow capturing these decision problems, while providing practically effective decision procedures.In this paper, we propose a logic-based framework for the static analysis of XPath. Specifically, we propose the alternation free modal μ-calculus with converse as the appropriate logic for effectively solving XPath decision problems. We present a translation of a large XPath fragment into μ-calculus, together with practical experiments on the containment using a state-of-the-art EXPTIME decision procedure for μ-calculus satisfiability. These preliminary experiments shed light, for the first time, on the cost of checking the containment in practice. We believe they reveal encouraging results for further static analysis of XML transformations.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {65–74},
numpages = {10},
keywords = {experimentation, XPath, analysis},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166183,
author = {Lindholm, Tancred and Kangasharju, Jaakko and Tarkoma, Sasu},
title = {Fast and Simple XML Tree Differencing by Sequence Alignment},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166183},
doi = {10.1145/1166160.1166183},
abstract = {With the advent of XML we have seen a renewed interest in methods for computing the difference between trees. Methods that include heuristic elements play an important role in practical applications due to the inherent complexity of the problem. We present a method for differencing XML as ordered trees based on mapping the problem to the domain of sequence alignment, applying simple and efficient heuristics in this domain, and transforming back to the tree domain. Our approach provides a method to quickly compute changes that are meaningful transformations on the XML tree level, and includes subtree move as a primitive operation. We evaluate the feasibility of our approach and benchmark it against a selection of existing differencing tools. The results show our approach to be feasible and to have the potential to perform on par with tools of a more complex design in terms of both output size and execution time.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {75–84},
numpages = {10},
keywords = {sequence alignment, differencing, move, XML, ordered tree},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166184,
author = {Qeli, Ermir and Freisleben, Bernd},
title = {Filtering XML Documents Using XPath Expressions and Aspect-Oriented Programming},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166184},
doi = {10.1145/1166160.1166184},
abstract = {In this paper, we present the design and implementation of a filtering approach for XML documents which is based on XPath expressions and Aspect-Oriented Programming (AOP). The class of XPath expressions used allows for branching, wildcards and descendant relationships between nodes. For the embedding of simple paths into XPath expressions, a dynamic programming approach is proposed. The AOP paradigm, which provides a means for encapsulating crosscutting concerns in software, is introduced to integrate the filtering approach in the broader context of event-based parsing of XML documents using SAX.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {85–87},
numpages = {3},
keywords = {XML, XPath, aspect-oriented programming, SAX},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166185,
author = {Qeli, Ermir and Gllavata, Julinda and Freisleben, Bernd},
title = {Customizable Detection of Changes for XML Documents Using XPath Expressions},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166185},
doi = {10.1145/1166160.1166185},
abstract = {Change detection in XML documents is an important task in the context of query systems. In this paper, we present CustX- Diff, a customizable change detection approach for XML documents based on X-Diff [6]. CustX-Diff performs the change detection operation simultaneosly with the XPath based filtering of XML document parts. The class of XPath expressions used is the tree patterns subset of XPath. For the embedding of simple paths into XPath expressions during the difference operation, a dynamic programming approach is proposed. Comparative performance results with respect to the original X-Diff [6] approach demonstrate the efficiency of the proposed method.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {88–90},
numpages = {3},
keywords = {XPath, change detection, XML},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245605,
author = {Quint, V.},
title = {Session Details: XSLT and Beyond},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245605},
doi = {10.1145/3245605},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166187,
author = {Pimentel, Maria G. and Baldochi, Laercio and Munson, Ethan V.},
title = {Modeling Context Information for Capture and Access Applications},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166187},
doi = {10.1145/1166160.1166187},
abstract = {The Contextractor is an XSLT-based transformation system that gathers information from extended UML models to produce XML Schemas that model the information captured by an application and define a query language that allows the submission of queries over the captured content.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {92–94},
numpages = {3},
keywords = {ubiquitous, UML models, XSLT, XMI, capture and access applications, computing, XML schema},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166188,
author = {Lumley, John and Gimson, Roger and Rees, Owen},
title = {Resolving Layout Interdependency with Presentational Variables},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166188},
doi = {10.1145/1166160.1166188},
abstract = {In the construction of variable data documents, the layout of component parts to build a composite section with heterogeneous layout functions can be implemented by a tree-evaluating layout processor. This handles many cases with well-scoped structure very smoothly but becomes complex when layout relationships between components cut across a strict tree. We present an approach for XML-described layouts based on a post-rendering set of single-assignment variables, analagous to XSLT, that can make this much easier, does not compromise layout extensibility and can be a target for automated interdependency analysis and generation. This is the approach used in the layout processor associated with the Document Description Framework (DDF).},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {95–97},
numpages = {3},
keywords = {SVG, document construction, functional programming, XSLT},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245606,
author = {Gormish, M.},
title = {Session Details: Document Recognition and Classification},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245606},
doi = {10.1145/3245606},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166190,
author = {Simske, Steven J. and Wright, David W. and Sturgill, Margaret},
title = {Meta-Algorithmic Systems for Document Classification},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166190},
doi = {10.1145/1166160.1166190},
abstract = {To address cost and regulatory concerns, many businesses are converting paper-based elements of their workflows into fully electronic flows that use the content of the documents. Scanning the document contents into workflows, however, is a manual, error-prone, and costly process especially when the data extraction process requires high accuracy. These manual costs are a primary barrier to widespread adoption of distributed capture solutions for business critical workflows such as insurance claims, medical records, or loan applications. Software solutions using artificial intelligence and natural language processing techniques are emerging to address these needs, but each have their individual strengths and weaknesses, and none have demonstrated a high level of accuracy across the many unstructured document types included in these business critical workflows. This paper describes how to overcome many of these limitations by intelligently combining multiple approaches for document classification using meta-algorithmic design patterns. These patterns explore the error space in multiple engines, and provide improved and "emergent" results in comparison to voting schemes and to the output of any of the individual engines. This paper considers the results of the individual engines along with traditional combinatorial techniques such as voting, before describing prototype results for a variety of novel metaalgorithmic patterns that reduce individual document error rates by up to 13% and reduce system error rates by up to 38%.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {98–106},
numpages = {9},
keywords = {document indexing, confusion matrix, engine combination, document classification, meta-algorithmics},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@dataset{10.1145/review-1166160.1166190_R41787,
author = {Olagunju, Amos O},
title = {Review ID:R41787 for DOI: 10.1145/1166160.1166190},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1166160.1166190_R41787}
}

@inproceedings{10.1145/1166160.1166191,
author = {G\'{o}mez Hidalgo, Jos\'{e} Mar\'{\i}a and Bringas, Guillermo Cajigas and S\'{a}nz, Enrique Puertas and Garc\'{\i}a, Francisco Carrero},
title = {Content Based SMS Spam Filtering},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166191},
doi = {10.1145/1166160.1166191},
abstract = {In the recent years, we have witnessed a dramatic increment in the volume of spam email. Other related forms of spam are increasingly revealing as a problem of importance, specially the spam on Instant Messaging services (the so called SPIM), and Short Message Service (SMS) or mobile spam.Like email spam, the SMS spam problem can be approached with legal, economic or technical measures. Among the wide range of technical measures, Bayesian filters are playing a key role in stopping email spam. In this paper, we analyze to what extent Bayesian filtering techniques used to block email spam, can be applied to the problem of detecting and stopping mobile spam. In particular, we have built two SMS spam test collections of significant size, in English and Spanish. We have tested on them a number of messages representation techniques and Machine Learning algorithms, in terms of effectiveness. Our results demonstrate that Bayesian filtering techniques can be effectively transferred from email to SMS spam.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {107–114},
numpages = {8},
keywords = {bayesian filter, spam, junk, receiver operating characteristic},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166192,
author = {Godlewski, Grzegorz and Piasecki, Maciej and Sas, Jerzy},
title = {Application of Syntactic Properties to Three-Level Recognition of Polish Hand-Written Medical Texts},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166192},
doi = {10.1145/1166160.1166192},
abstract = {In the paper, three-level hand-writing recognition using language syntactic properties on the upper level is presented. Isolated characters are recognized on the lowest level. The character classification from the lowest level is used in words recognition. Words are recognized using a combined classifier based on possibly incomplete unigram lexicon. Word classifier builds a rank of the most likely words. Ranks created for subsequent words are input to the syntactic classifier, which recognizes the whole sentences. Here the local syntactic constraints are used to build a syntactically consistent sentence. The method has been applied to recognition of hand-written medical texts describing fixed aspects of patient treatment. Due to narrow area of topics explained in the texts and peculiarity of style characteristic for physicians writing texts, the syntax of expected sentences is relatively simple, what makes the problem of checking the syntactic consistency simpler.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {115–121},
numpages = {7},
keywords = {PoS tagging, OCR, polish, HMM, hand-written text},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166193,
author = {Takasu, Atsuhiro and Aihara, Kenro},
title = {Quality Enhancement in Information Extraction from Scanned Documents},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166193},
doi = {10.1145/1166160.1166193},
abstract = {When constructing a large document archive, an important element is the digitizing of printed documents. Although various techniques for document image analysis such as Optical Character Recognition (OCR) have been developed, error handling is required in constructing real document archive systems. This paper discusses the problem from the quality enhancement perspective and proposes a robust reference extraction method for academic articles scanned with OCR mark-up. We applied the proposed method to articles appearing in various journals, and these experiments showed that the proposed method achieved a recognition accuracy of more than 94%. This paper also discusses manual correction and investigates experimentally the relationship between extraction accuracy and cost reduction.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {122–124},
numpages = {3},
keywords = {digital libraries, document capturing, statistical model},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166194,
author = {Lecerf, Lo\"{\i}c and Chidlovskii, Boris},
title = {Document Annotation by Active Learning Techniques},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166194},
doi = {10.1145/1166160.1166194},
abstract = {We present a system for the semantic annotation of layout-oriented documents, with an integrated learning component. We introduce probabilistic learning methods on tree-like documents and we present different active learning techniques for training document annotation models. We report some preliminary results of deploying such active learning techniques on an important case of document collection annotation.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {125–127},
numpages = {3},
keywords = {active learning, semantic annotation, maximum entropy},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245607,
author = {Roisin, C.},
title = {Session Details: Text-Based Document Models},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245607},
doi = {10.1145/3245607},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166196,
author = {Ruiz-Rico, Fernando and Vicedo, Jose Luis and Rubio-S\'{a}nchez, Mar\'{\i}a-Consuelo},
title = {NEWPAR: An Automatic Feature Selection and Weighting Schema for Category Ranking},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166196},
doi = {10.1145/1166160.1166196},
abstract = {Category ranking provides a way to classify plain text documents into a pre-determined set of categories. This work proposes to have a look at typical document collections and analyze which measures and peculiarities can help us to represent documents so that the resulting features are as much discriminative and representative as possible. Considerations such as selecting only nouns and adjectives, taking expressions rather than words, and using measures like term length, are combined into a simple feature selection and weighting method to extract, select and weight especial n-grams. Several experiments are performed to prove the usefulness of the new schema with different data sets (Reuters and OHSUMED) and two different algorithms (SVM and a simple sum of weights). After evaluation, the new approach outperforms some of the best known and most widely used categorization methods.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {128–137},
numpages = {10},
keywords = {text categorization, machine learning, SVM, text classifcation, category ranking},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@dataset{10.1145/review-1166160.1166196_R41833,
author = {Hodgson, Jonathan P. E.},
title = {Review ID:R41833 for DOI: 10.1145/1166160.1166196},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1166160.1166196_R41833}
}

@inproceedings{10.1145/1166160.1166197,
author = {Tesar, Roman and Strnad, Vaclav and Jezek, Karel and Poesio, Massimo},
title = {Extending the Single Words-Based Document Model: A Comparison of Bigrams and 2-Itemsets},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166197},
doi = {10.1145/1166160.1166197},
abstract = {The basic approach in text categorization is to represent documents by single words. However, often other features are utilized to achieve better classification results. In this paper, our attention is focused on bigrams and 2-itemsets. We compare the performance improvement in terms of classification accuracy when these features are used to extend the single words-based document representation on two standard text corpora: Reuters-21578 and 20 Newsgroups. For this comparison we use the multinomial Naive Bayes classifier and five different feature selection approaches. Algorithms for bigrams and 2-itemsets discovery are presented as well. Our results show a statistically significant improvement when bigrams and also 2-itemsets are incorporated. However, in the case of 2-itemsets it is important to use an appropriate feature selection method. On the other hand, even when a simple feature selection approach is applied to discover bigrams the classification accuracy improves. The conclusion is that, in our case, it is not very effective to extend document representation with 2-itemsets because bigrams achieve better results and discovering them is less resource-consuming.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {138–146},
numpages = {9},
keywords = {text categorization, n-grams, machine learning, document model, comparison, itemsets, bigrams, feature selection},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245608,
author = {Munson, E. V.},
title = {Session Details: Documents with Multiple Markup},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245608},
doi = {10.1145/3245608},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166199,
author = {Bruno, Emmanuel and Murisasco, Elisabeth},
title = {Describing and Querying Hierarchical XML Structures Defined over the Same Textual Data},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166199},
doi = {10.1145/1166160.1166199},
abstract = {Our work aims at representing and querying hierarchical XML structures defined over the same textual data. We call such data "multistructured textual documents".Our objectives are twofold. First, we shall define a suitable - XML compatible - data model enabling (1) to describe several independent hierarchical structures over the same textual data (represented by several XML structured documents) (2) to consider user annotations added in each structured document. Our proposal is based on the use of hedges (the foundation of the grammar language RelaxNG). Secondly, we shall propose an extension of XQuery in order to query structures and content in a concurrent way. We shall apply our proposals using a literary text written in old French.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {147–154},
numpages = {8},
keywords = {tree-like structure, textual documents, multistructure, XML, XQuery},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166200,
author = {Le Maitre, Jacques},
title = {Describing Multistructured XML Documents by Means of Delay Nodes},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166200},
doi = {10.1145/1166160.1166200},
abstract = {Multistructured documents are documents whose structure is composed of a set of concurrent hierarchical structures. In this paper, we propose a new model of multistructured documents and we show how to translate its instances into XML using a new kind of nodes: delay nodes, which we propose to add to the XDM model on which XPath and XQuery are based. A delay node is the virtual representation, by an XQuery query, of some of the children of its parent. Interest of delay nodes to manage multistructured documents is that they allow several nodes to virtually share their children nodes. In this way, it is possible to query, with XPath or XQuery, multistructured documents described in XML as if their different structures were really concurrent. Finally, we compare our model with the GODDAGbased model and the multicolored trees (MCT) model.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {155–164},
numpages = {10},
keywords = {lazy evaluation, XML, multistructured documents, XQuery},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245609,
author = {Pimentel, M. G.},
title = {Session Details: Multimedia and Hypermedia Authoring},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245609},
doi = {10.1145/3245609},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166202,
author = {de Resende Costa, Romualdo Monteiro and Moreno, M\'{a}rcio Ferreira and Rodrigues, Rog\'{e}rio Ferreira and Soares, Luiz Fernando Gomes},
title = {Live Editing of Hypermedia Documents},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166202},
doi = {10.1145/1166160.1166202},
abstract = {In some hypermedia system applications, like interactive digital TV applications, authoring and presentation of documents may have to be done concomitantly. This is the case of live programs, where not only some contents are not known a priori, but also some temporal and spatial relationships, among program media objects, may have to be established after the unknown content definition. This paper proposes a method for hypermedia document live editing, preserving not only the presentation semantics but also the logical structure semantics defined by an author. To validate this proposal, an implementation has been done for the Brazilian Digital TV System, which is also presented.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {165–172},
numpages = {8},
keywords = {NCL, SBTVD, ginga, declarative middleware, interactive digital TV},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166203,
author = {Deltour, Romain and Roisin, C\'{e}cile},
title = {The Limsee3 Multimedia Authoring Model},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166203},
doi = {10.1145/1166160.1166203},
abstract = {For most users, authoring multimedia documents remains a complex task. One solution to deal with this problem is to provide template-based authoring tools but with the drawback of limited functionality. In this paper we propose a document model dedicated to the creation of authoring tools using templates while keeping rich composition capabilities. It is based on a component oriented approach integrating homogeneously logical, time and spatial structures. Templates are defined as constraints on these structures.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {173–175},
numpages = {3},
keywords = {document models, multimedia documents, document authoring, template-based editing},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166204,
author = {Cesar, Pablo and Bulterman, D. C. A. and Jansen, A. J.},
title = {Benefits of Structured Multimedia Documents in IDTV: The End-User Enrichment System},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166204},
doi = {10.1145/1166160.1166204},
abstract = {This paper presents a system that exploits the benefits of modelling multimedia presentations as structured documents within the context of interactive digital television systems. Our work permits end-users to easily enrich multimedia content at viewing time (e.g., add images and delete scenes). Because the document is structured, the system can expose to the user the possible enrichment alternatives depending on the current state of the presentation (e.g., current story). Moreover, because the base content is wrapped as a structured document, the enrichments can be modelled as overlying layers that do not alter the original content. Finally, the user can share the enriched content (or parts of it) to specific peers within a P2P network.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {176–178},
numpages = {3},
keywords = {structured multimedia documents, content enrichment, SMIL},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166205,
author = {Joshi, Parag Mulendra and Atkins, C. Brian and Zhang, Tong},
title = {From Video to Photo Albums: Digital Publishing Workflow for Automatic Album Creation},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166205},
doi = {10.1145/1166160.1166205},
abstract = {The revolution in consumer electronics for capturing video has been followed by an explosion of video content. However, meaningful consumption models of such rich media for nonprofessional users are still emerging. In contrast to those of video cameras, the consumption models for output of still cameras have been long established and are considerably simpler. The output of a still camera is an image of sufficiently high quality and high resolution for a good quality production on paper. Due to ease of use, mobility, high quality and simplicity paper photographs are still incomparable in terms of overall human experience. On the other hand, video content by itself is not as easy to use. Consumption of video content requires computers and/or video display devices and so cannot be instantaneously displayed or shared. Rendition on paper is much more complex for video content compared to still camera images. In contrast with the simplicity of usage of still cameras, video camera output has to be edited on computer, key frames with good visual quality have to be manually extracted, digitally edited and prepared for printing before getting usable good quality photographs. Due to complexity of the video content, users often prefer to take still pictures instead of recording video clips. In this paper we describe an approach to construct an end-to-end digital publishing workflow system that automatically composes visually appealing photo albums with high quality photographic images from video content input.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {179–181},
numpages = {3},
keywords = {WSBPEL, photo album, web services, digital publishing, multimedia, video, web-to-print, workflow},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245610,
author = {Bagley, S. R.},
title = {Session Details: Demonstrations},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245610},
doi = {10.1145/3245610},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166207,
author = {Matulic, Fabrice},
title = {SmartPublisher: Document Creation on Pen-Based Systems via Document Element Reuse},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166207},
doi = {10.1145/1166160.1166207},
abstract = {SmartPublisher is a powerful, all-in-one application for pen-based devices with which users can quickly and intuitively create new documents by reusing individual image and text elements acquired from analogue and/or digital documents. The application is especially targeted at scanning devices with touch screen operating panels or tablet PCs connected to them (e.g. modern multifunction printers with large touch screen displays), as one of its main purposes is reuse of material obtained from scanned paper documents.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {182–183},
numpages = {2},
keywords = {document creation and editing, GUI, scanned document, reuse},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166208,
author = {Chidlovskii, Boris and Fuselier, J\'{e}r\^{o}me and Lecerf, Lo\"{\i}c},
title = {ALDAI: Active Learning Documents Annotation Interface},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166208},
doi = {10.1145/1166160.1166208},
abstract = {In the framework of the LegDoC project at XRCE, we present a document annotation interface with an integrated active learning component. The interface is designed for automating the annotation of layout-oriented documents with semantic labels. We describe core functionalities of the interface; we pay a particular attention to the learning component, including the feature management and different strategies of deploying the active learner in the document annotation.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {184–185},
numpages = {2},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166209,
author = {Cesar, Pablo and Bulterman, Dick C. A. and Jansen, A. J.},
title = {The Ambulant Annotator: Empowering Viewer-Side Enrichment of Multimedia Content},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166209},
doi = {10.1145/1166160.1166209},
abstract = {This paper presents a set of demos that allow viewer-side enrichment of multimedia content in a home setting. The most relevant features of our system are the following: passive authoring of content in contraposition to the traditional active PC authoring, preservation of the base content, and collaborative authoring (e.g., to share the enriched material with a peer group). These requirements are met by modelling television content as structured multimedia documents using SMIL 2.1.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {186–187},
numpages = {2},
keywords = {structured, content enrichment, multimedia documents, SMIL},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245611,
author = {Brailsford, D. F.},
title = {Session Details: Document Editing for the Web},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245611},
doi = {10.1145/3245611},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166211,
author = {Flores, Francesc Campoy and Quint, Vincent and Vatton, Ir\`{e}ne},
title = {Templates, Microformats and Structured Editing},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166211},
doi = {10.1145/1166160.1166211},
abstract = {Microformats and semantic XHTML add semantics to web pages while taking advantage of the existing (X)HTML infrastructure. This approach enables new applications that can be deployed smoothly on the web. But there is currently no way to describe rigorously this type of markup and authors of web pages have very little help for creating and encoding semantic markup. A language that addresses these issues is presented in this paper. Its role is to specify semantically rich XML languages in terms of other XML languages, such as XHTML. The language is versatile enough to represent templates that can capture the overall structure of large documents as well as the fine details of a microformat. It is supported by an editing tool for producing documents encoded in a semantically rich markup language, still fully compatible with XHTML.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {188–197},
numpages = {10},
keywords = {document authoring, microformats, document models, semantic XHTML, document templates, world wide web, structure editing},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/3245612,
author = {Bulterman, D. C. A.},
title = {Session Details: Novel Web Applications},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245612},
doi = {10.1145/3245612},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166213,
author = {Levering, Ryan and Cutler, Michal},
title = {The Portrait of a Common HTML Web Page},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166213},
doi = {10.1145/1166160.1166213},
abstract = {Web pages are not purely text, nor are they solely HTML. This paper surveys HTML web pages; not only on textual content, but with an emphasis on higher order visual features and supplementary technology. Using a crawler with an in-house developed rendering engine, data on a pseudo-random sample of web pages is collected. First, several basic attributes are collected to verify the collection process and confirm certain assumptions on web page text. Next, we take a look at the distribution of different types of page content (text, images, plug-in objects, and forms) in terms of rendered visual area. Those different types of content are broken down into a detailed view of the ways in which the content is used. This includes a look at the prevalence and usage of scripts and styles. We conclude that more complex page elements play a significant and underestimated role in the visually attractive, media rich, and highly interactive web pages that are currently being added to the World Wide Web.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {198–204},
numpages = {7},
keywords = {javascript, style, world wide web, visual, CSS, feature, HTML, script, survey},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166214,
author = {Murthy, Sudarshan and Maier, David and Delcambre, Lois},
title = {Mash-o-Matic},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166214},
doi = {10.1145/1166160.1166214},
abstract = {Web applications called mash-ups combine information of varying granularity from different, possibly disparate, sources. We describe Mash-o-matic, a utility that can extract, clean, and combine disparate information fragments, and automatically generate data for mash-ups and the mash-ups themselves. As an illustration, we generate a mash-up that displays a map of a university campus, and outline the potential benefits of using Mash-o-matic. Mash-o-matic exploits superimposed information (SI), which is new information and structure created in reference to fragments of existing information. Mashomatic is implemented using middleware called the Superimposed Pluggable Architecture for Contexts and Excerpts (SPARCE), and a query processor for SI and referenced information, both parts of our infrastructure to support SI management. We present a high-level description of the mash-up production process and discuss in detail how Mash-o-matic accelerates that process.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {205–214},
numpages = {10},
keywords = {mash-up, bi-level information, superimposed information, sidepad, document transformation, SPARCE},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166215,
author = {Stone, Roger and Dhiensa, Jatinder and Machin, Colin},
title = {Profile-Based Web Document Delivery},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166215},
doi = {10.1145/1166160.1166215},
abstract = {This work originated by considering the needs of visually impaired users but may have wider application. A profile captures some key descriptors or preferences of a user and their browsing device. Individual users may maintain any number of profiles which they can edit for use in different situations, for different tasks or with different devices. A profile is described in terms of essentiality and proficiency. Essentiality is used to control the quantity of information that is transmitted and proficiency is used to control the format. Various levels of essentiality are introduced into a document by the technique known as microformatting. Proficiency (for the visually impaired) includes a description of minimum acceptable font size, preferred font face and preferred text and background colours. A key feature of the proficiency profile is the accessibility component which captures the user's tolerance of accessibility issues in a document, for example the presence of images or the markup of tables. The document delivery tool works as a kind of filter to reduce the content to the level of essentiality requested, to make the various presentation changes and to warn of accessibility issues as specified in the user's profile. Encouraging preliminary results have been obtained from testing the prototype with subjects from the local RNIB college.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {215–217},
numpages = {3},
keywords = {usability, web accessibility, visual impairment, web-documents},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

@inproceedings{10.1145/1166160.1166216,
author = {Contessa, Diego Fraga and Moreira de Oliveira, Jos\'{e} Palazzo},
title = {An OAI Data Provider for JEMS},
year = {2006},
isbn = {1595935150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1166160.1166216},
doi = {10.1145/1166160.1166216},
abstract = {Effective scientific activity requires the open publication and diffusion of scientific works and easy access to the research results. Digital libraries aim to simplify the publication process by using the Open Archives Initiative associated with the Dublin Core metadata system. The Brazilian Computer Society - SBC - supports the JEMS system for managing the submission and evaluation of conference and journal articles. JEMS collects considerable data about papers, authors and conferences and SBC is seeking ways to provide wider access to this information. This article describes an OAI-compatible data provider that publishes metadata in both the Simple and Qualified Dublin Core formats.},
booktitle = {Proceedings of the 2006 ACM Symposium on Document Engineering},
pages = {218–220},
numpages = {3},
keywords = {digital libraries, XML, data provider, metadata generation, OAI, JEMS},
location = {Amsterdam, The Netherlands},
series = {DocEng '06}
}

