@inproceedings{10.1145/1281192.1281197,
author = {Agarwal, Deepak and Barman, Dhiman and Gunopulos, Dimitrios and Young, Neal E. and Korn, Flip and Srivastava, Divesh},
title = {Efficient and Effective Explanation of Change in Hierarchical Summaries},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281197},
doi = {10.1145/1281192.1281197},
abstract = {Dimension attributes in data warehouses are typically hierarchical (e.g., geographic locations in sales data, URLs in Web traffic logs). OLAP tools are used to summarize the measure attributes (e.g., total sales) along a dimension hierarchy, and to characterize changes (e.g., trends and anomalies) in a hierarchical summary over time. When thenumber of changes identified is large (e.g., total sales in many stores differed from their expected values), a parsimonious explanation of the most significant changes is desirable. In this paper, we propose a natural model of parsimonious explanation, as a composition of node weights along the root-to-leaf paths in a dimension hierarchy, which permits changes to be aggregated with maximal generalization along the dimension hierarchy. We formalize this model of explaining changes in hierarchical summaries and investigate the problem of identifying optimally parsimonious explanations on arbitrary rooted one dimensional tree hierarchies. We show that such explanations can be computed efficiently in time essentially proportional to the number of leaves and the depth of the hierarchy. Further, our method can produce parsimonious explanations from the output of any statistical model that provides predictions and confidence intervals, making it widely applicable. Our experiments use real data sets to demonstrate the utility and robustness of our proposed model for explaining significant changes, as well as its superior parsimony compared to alternatives.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {6–15},
numpages = {10},
keywords = {change, parsimonious explanations, OLAP, hierarchical summary, statistical model},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281198,
author = {Agarwal, Deepak and Broder, Andrei Zary and Chakrabarti, Deepayan and Diklic, Dejan and Josifovski, Vanja and Sayyadian, Mayssam},
title = {Estimating Rates of Rare Events at Multiple Resolutions},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281198},
doi = {10.1145/1281192.1281198},
abstract = {We consider the problem of estimating occurrence rates of rare eventsfor extremely sparse data, using pre-existing hierarchies to perform inference at multiple resolutions. In particular, we focus on the problem of estimating click rates for (webpage, advertisement) pairs (called impressions) where both the pages and the ads are classified into hierarchies that capture broad contextual information at different levels of granularity. Typically the click rates are low and the coverage of the hierarchies is sparse. To overcome these difficulties we devise a sampling method whereby we analyze aspecially chosen sample of pages in the training set, and then estimate click rates using a two-stage model. The first stage imputes the number of (webpage, ad) pairs at all resolutions of the hierarchy to adjust for the sampling bias. The second stage estimates clickrates at all resolutions after incorporating correlations among sibling nodes through a tree-structured Markov model. Both models are scalable and suited to large scale data mining applications. On a real-world dataset consisting of 1/2 billion impressions, we demonstrate that even with 95% negative (non-clicked) events in the training set, our method can effectively discriminate extremely rare events in terms of their click propensity.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {16–25},
numpages = {10},
keywords = {hierarchy, internet advertising, maximum entropy, contextual matching, tree-structured markov model, clickthrough rate, imputation},
location = {San Jose, California, USA},
series = {KDD '07}
}

@dataset{10.1145/review-1281192.1281198_R43798,
author = {Papadopoulos, Apostolos N},
title = {Review ID:R43798 for DOI: 10.1145/1281192.1281198},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1281192.1281198_R43798}
}

@inproceedings{10.1145/1281192.1281199,
author = {Agarwal, Deepak and Merugu, Srujana},
title = {Predictive Discrete Latent Factor Models for Large Scale Dyadic Data},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281199},
doi = {10.1145/1281192.1281199},
abstract = {We propose a novel statistical method to predict large scale dyadic response variables in the presence of covariate information. Our approach simultaneously incorporates the effect of covariates and estimates local structure that is induced by interactions among the dyads through a discrete latent factor model. The discovered latent factors provide a redictive model that is both accurate and interpretable. We illustrate our method by working in a framework of generalized linear models, which include commonly used regression techniques like linear regression, logistic regression and Poisson regression as special cases. We also provide scalable generalized EM-based algorithms for model fitting using both "hard" and "soft" cluster assignments. We demonstrate the generality and efficacy of our approach through large scale simulation studies and analysis of datasets obtained from certain real-world movie recommendation and internet advertising applications.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {26–35},
numpages = {10},
keywords = {co-clustering, generalized linear regression, dyadic data, latent factor modeling},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281200,
author = {Aggarwal, Charu C. and Yu, Philip S.},
title = {On String Classification in Data Streams},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281200},
doi = {10.1145/1281192.1281200},
abstract = {String data has recently become important because of its use in a number of applications such as computational and molecular biology, protein analysis, and market basket data. In many cases, these strings contain a wide variety of substructures which may have physical significance for that application. For example, such substructures could represent important fragments of a DNA string or an interesting portion of a fraudulent transaction. In such a case, it is desirable to determine the identity, location, and extent of that substructure in the data. This is a much more difficult generalization of the classification problem, since the latter problem labels entire strings rather than deal with the more complex task of determining string fragments with a particular kind of behavior. The problem becomes even more complicated when different kinds of substrings show complicated nesting patterns. Therefore, we define a somewhat different problem which we refer to as the generalized classification problem. We propose a scalable approach based on hidden markov models for this problem. We show how to implement the generalized string classification procedure for very large data bases and data streams. We present experimental results over a number of large data sets and data streams.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {36–45},
numpages = {10},
keywords = {string, classification, hidden markov models},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281201,
author = {Aggarwal, Charu C. and Ta, Na and Wang, Jianyong and Feng, Jianhua and Zaki, Mohammed},
title = {Xproj: A Framework for Projected Structural Clustering of Xml Documents},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281201},
doi = {10.1145/1281192.1281201},
abstract = {XML has become a popular method of data representation both on the web and in databases in recent years. One of the reasons for the popularity of XML has been its ability to encode structural information about data records. However, this structural characteristic of data sets also makes it a challenging problem for a variety of data mining problems. One such problem is that of clustering, in which the structural aspects of the data result in a high implicit dimensionality of the data representation. As a result, it becomes more difficult to cluster the data in a meaningful way. In this paper, we propose an effective clustering algorithm for XML data which uses substructures of the documents in order to gain insights about the important underlying structures. We propose new ways of using multiple sub-structuralinformation in XML documents to evaluate the quality of intermediate cluster solutions, and guide the algorithms to a final solution which reflects the true structural behavior in individual partitions. We test the algorithm on a variety of real and synthetic data sets.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {46–55},
numpages = {10},
keywords = {clustering, XML},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281202,
author = {Archak, Nikolay and Ghose, Anindya and Ipeirotis, Panagiotis G.},
title = {Show Me the Money! Deriving the Pricing Power of Product Features by Mining Consumer Reviews},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281202},
doi = {10.1145/1281192.1281202},
abstract = {The increasing pervasiveness of the Internet has dramatically changed the way that consumers shop for goods. Consumer-generated product reviews have become a valuable source of information for customers, who read the reviews and decide whether to buy the product based on the information provided. In this paper, we use techniques that decompose the reviews into segments that evaluate the individual characteristics of a product (e.g., image quality and battery life for a digital camera). Then, as a major contribution of this paper, we adapt methods from the econometrics literature, specifically the hedonic regression concept, to estimate: (a) the weight that customers place on each individual product feature, (b) the implicit evaluation score that customers assign to each feature, and (c) how these evaluations affect the revenue for a given product. Towards this goal, we develop a novel hybrid technique combining text mining and econometrics that models consumer product reviews as elements in a tensor product of feature and evaluation spaces. We then impute the quantitative impact of consumer reviews on product demand as a linear functional from this tensor product space. We demonstrate how to use a low-dimension approximation of this functional to significantly reduce the number of model parameters, while still providing good experimental results. We evaluate our technique using a data set from Amazon.com consisting of sales data and the related consumer reviews posted over a 15-month period for 242 products. Our experimental evaluation shows that we can extract actionable business intelligence from the data and better understand the customer preferences and actions. We also show that the textual portion of the reviews can improve product sales prediction compared to a baseline technique that simply relies on numeric data.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {56–65},
numpages = {10},
keywords = {opinion mining, e-commerce, internet, hedonic analysis, econometrics, consumer reviews, electronic markets, electronic commerce, user-generated content, text mining, sentiment analysis, product review},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281203,
author = {Arnold, Andrew and Liu, Yan and Abe, Naoki},
title = {Temporal Causal Modeling with Graphical Granger Methods},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281203},
doi = {10.1145/1281192.1281203},
abstract = {The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of "Granger causality", based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {66–75},
numpages = {10},
keywords = {time series data, causal modeling, graphical models},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281204,
author = {Baeza-Yates, Ricardo and Tiberi, Alessandro},
title = {Extracting Semantic Relations from Query Logs},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281204},
doi = {10.1145/1281192.1281204},
abstract = {In this paper we study a large query log of more than twenty million queries with the goal of extracting the semantic relations that are implicitly captured in the actions of users submitting queries and clicking answers. Previous query log analyses were mostly done with just the queries and not the actions that followed after them. We first propose a novel way to represent queries in a vector space based on a graph derived from the query-click bipartite graph. We then analyze the graph produced by our query log, showing that it is less sparse than previous results suggested, and that almost all the measures of these graphs follow power laws, shedding some light on the searching user behavior as well as on the distribution of topics that people want in the Web. The representation we introduce allows to infer interesting semantic relationships between queries. Second, we provide an experimental analysis on the quality of these relations, showing that most of them are relevant. Finally we sketch an application that detects multitopical URLs.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {76–85},
numpages = {10},
keywords = {query log analysis, graph mining},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281205,
author = {Becker, Hila and Arias, Marta},
title = {Real-Time Ranking with Concept Drift Using Expert Advice},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281205},
doi = {10.1145/1281192.1281205},
abstract = {In many practical applications, one is interested in generating a ranked list of items using information mined from continuous streams of data. For example, in the context of computer networks, one might want to generate lists of nodes ranked according to their susceptibility to attack. In addition, real-world data streams often exhibit concept drift, making the learning task even more challenging. We present an online learning approach to ranking with concept drift, using weighted majority techniques. By continuously modeling different snapshots of the data and tuning our measure of belief in these models over time, we capture changes in the underlying concept and adapt our predictions accordingly. We measure the performance of our algorithm on real electricity data as well as asynthetic data stream, and demonstrate that our approach to ranking from stream data outperforms previously known batch-learning methods and other online methods that do not account for concept drift.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {86–94},
numpages = {9},
keywords = {online learning, data streams, concept drift, ranking},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281206,
author = {Bell, Robert and Koren, Yehuda and Volinsky, Chris},
title = {Modeling Relationships at Multiple Scales to Improve Accuracy of Large Recommender Systems},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281206},
doi = {10.1145/1281192.1281206},
abstract = {The collaborative filtering approach to recommender systems predicts user preferences for products or services by learning past user-item relationships. In this work, we propose novel algorithms for predicting user ratings of items by integrating complementary models that focus on patterns at different scales. At a local scale, we use a neighborhood-based technique that infers ratings from observed ratings by similar users or of similar items. Unlike previous local approaches, our method is based on a formal model that accounts for interactions within the neighborhood, leading to improved estimation quality. At a higher, regional, scale, we use SVD-like matrix factorization for recovering the major structural patterns in the user-item rating matrix. Unlike previous approaches that require imputations in order to fill in the unknown matrix entries, our new iterative algorithm avoids imputation. Because the models involve estimation of millions, or even billions, of parameters, shrinkage of estimated values to account for sampling variability proves crucial to prevent overfitting. Both the local and the regional approaches, and in particular their combination through a unifying model, compare favorably with other approaches and deliver substantially better results than the commercial Netflix Cinematch recommender system on a large publicly available data set.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {95–104},
numpages = {10},
keywords = {collaborative filtering, recommender systems, netflix prize},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281207,
author = {Bhagwat, Deepavali and Eshghi, Kave and Mehra, Pankaj},
title = {Content-Based Document Routing and Index Partitioning for Scalable Similarity-Based Searches in a Large Corpus},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281207},
doi = {10.1145/1281192.1281207},
abstract = {We present a document routing and index partitioning scheme for scalable similarity-based search of documents in a large corpus. We consider the case when similarity-based search is performed by finding documents that have features in common with the query document. While it is possible to store all the features of all the documents in one index, this suffers from obvious scalability problems. Our approach is to partition the feature index into multiple smaller partitions that can be hosted on separate servers, enabling scalable and parallel search execution. When a document is ingested into the repository, a small number of partitions are chosen to store the features of the document. To perform similarity-based search, also, only a small number of partitions are queried. Our approach is stateless and incremental. The decision as to which partitions the features of the document should be routed to (for storing at ingestion time, and for similarity based search at query time) is solely based on the features of the document.Our approach scales very well. We show that executing similarity-based searches over such a partitioned search space has minimal impact on the precision and recall of search results, even though every search consults less than 3% of the total number of partitions.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {105–112},
numpages = {8},
keywords = {similarity-based search, distributed indexing, index partitioning, scalability, document routing},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281208,
author = {Chaovalitwongse, Wanpracha Art and Fan, Ya-Ju and Sachdeo, Rajesh C.},
title = {Support Feature Machine for Classification of Abnormal Brain Activity},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281208},
doi = {10.1145/1281192.1281208},
abstract = {In this study, a novel multidimensional time series classification technique, namely support feature machine (SFM), is proposed. SFM is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data. This paper also describes an application of SFM for detecting abnormal brain activity. Epilepsy is a case in point in this study. In epilepsy studies, electroencephalograms (EEGs), acquired in multidimensional time series format, have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain. From multi-dimensional EEG time series data, SFM was used to identify seizure pre-cursors and detect seizure susceptibility (pre-seizure) periods. The empirical results showed that SFM achieved over 80% correct classification of per-seizure EEG on average in 10 patients using 5-fold cross validation. The proposed optimization model of SFM is very compact and scalable, and can be implemented as an online algorithm. The outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through EEG classification.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {113–122},
numpages = {10},
keywords = {multi-dimensional time series, nearest neighbor, classification, epilepsy, optimization},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281209,
author = {Chen, Jianhui and Zhao, Zheng and Ye, Jieping and Liu, Huan},
title = {Nonlinear Adaptive Distance Metric Learning for Clustering},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281209},
doi = {10.1145/1281192.1281209},
abstract = {A good distance metric is crucial for many data mining tasks. To learn a metric in the unsupervised setting, most metric learning algorithms project observed data to a low-dimensional manifold, where geometric relationships such as pairwise distances are preserved. It can be extended to the nonlinear case by applying the kernel trick, which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space. In this paper, we propose a novel unsupervised Nonlinear Adaptive Metric Learning algorithm, called NAML, which performs clustering and distance metric learning simultaneously. NAML firstmaps the data to a high-dimensional space through a kernel function; then applies a linear projection to find a low-dimensional manifold where the separability of the data is maximized; and finally performs clustering in the low-dimensional space. The performance of NAML depends on the selection of the kernel function and the projection. We show that the joint kernel learning, dimensionality reduction, and clustering can be formulated as a trace maximization problem, which can be solved via an iterative procedure in the EM framework. Experimental results demonstrated the efficacy of the proposed algorithm.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {123–132},
numpages = {10},
keywords = {convex programming, clustering, kernel, distance metric},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281210,
author = {Chen, Yixin and Tu, Li},
title = {Density-Based Clustering for Real-Time Stream Data},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281210},
doi = {10.1145/1281192.1281210},
abstract = {Existing data-stream clustering algorithms such as CluStream arebased on k-means. These clustering algorithms are incompetent tofind clusters of arbitrary shapes and cannot handle outliers. Further, they require the knowledge of k and user-specified time window. To address these issues, this paper proposes D-Stream, a framework for clustering stream data using adensity-based approach. The algorithm uses an online component which maps each input data record into a grid and an offline component which computes the grid density and clusters the grids based on the density. The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream. Exploiting the intricate relationships between the decay factor, data density and cluster structure, our algorithm can efficiently and effectively generate and adjust the clusters in real time. Further, a theoretically sound technique is developed to detect and remove sporadic grids mapped to by outliers in order to dramatically improve the space and time efficiency of the system. The technique makes high-speed data stream clustering feasible without degrading the clustering quality. The experimental results show that our algorithm has superior quality and efficiency, can find clusters of arbitrary shapes, and can accurately recognize the evolving behaviors of real-time data streams.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {133–142},
numpages = {10},
keywords = {sporadic grids, d-stream, density-based clustering, stream data mining},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281211,
author = {Chew, Peter A. and Bader, Brett W. and Kolda, Tamara G. and Abdelali, Ahmed},
title = {Cross-Language Information Retrieval Using PARAFAC2},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281211},
doi = {10.1145/1281192.1281211},
abstract = {A standard approach to cross-language information retrieval (CLIR) uses Latent Semantic Analysis (LSA) in conjunction with a multilingual parallel aligned corpus. This approach has been shown to be successful in identifying similar documents across languages - or more precisely, retrieving the most similar document in one language to a query in another language. However, the approach has severe drawbacks when applied to a related task, that of clustering documents "language-independently", so that documents about similar topics end up closest to one another in the semantic space regardless of their language. The problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language, but on the same topic. As a result, when using multilingual LSA, documents will in practice cluster by language, not by topic.We propose a novel application of PARAFAC2 (which is a variant of PARAFAC, a multi-way generalization of the singular value decomposition [SVD]) to overcome this problem. Instead of forming a single multilingual term-by-document matrix which, under LSA, is subjected to SVD, we form an irregular three-way array, each slice of which is a separate term-by-document matrix for a single language in the parallel corpus. The goal is to compute an SVD for each language such that V (the matrix of right singular vectors) is the same across all languages. Effectively, PARAFAC2 imposes the constraint, not present in standard LSA, that the "concepts" in all documents in the parallel corpus are the same regardless of language. Intuitively, this constraint makes sense, since the whole purpose of using a parallel corpus is that exactly the same concepts are expressed in the translations.We tested this approach by comparing the performance of PARAFAC2 with standard LSA in solving a particular CLIR problem. From our results, we conclude that PARAFAC2 offers a very promising alternative to LSA not only for multilingual document clustering, but also for solving other problems in cross-language information retrieval.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {143–152},
numpages = {10},
keywords = {PARAFAC2, information retrieval, multilingual, clustering, latent semantic analysis (LSA)},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281212,
author = {Chi, Yun and Song, Xiaodan and Zhou, Dengyong and Hino, Koji and Tseng, Belle L.},
title = {Evolutionary Spectral Clustering by Incorporating Temporal Smoothness},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281212},
doi = {10.1145/1281192.1281212},
abstract = {Evolutionary clustering is an emerging research area essential to important applications such as clustering dynamic Web and blog contents and clustering data streams. In evolutionary clustering, a good clustering result should fit the current data well, while simultaneously not deviate too dramatically from the recent history. To fulfill this dual purpose, a measure of temporal smoothness is integrated in the overall measure of clustering quality. In this paper, we propose two frameworks that incorporate temporal smoothness in evolutionary spectral clustering. For both frameworks, we start with intuitions gained from the well-known k-means clustering problem, and then propose and solve corresponding cost functions for the evolutionary spectral clustering problems. Our solutions to the evolutionary spectral clustering problems provide more stable and consistent clustering results that are less sensitive to short-term noises while at the same time are adaptive to long-term cluster drifts. Furthermore, we demonstrate that our methods provide the optimal solutions to the relaxed versions of the corresponding evolutionary k-means clustering problems. Performance experiments over a number of real and synthetic data sets illustrate our evolutionary spectral clustering methods provide more robust clustering results that are not sensitive to noise and can adapt to data drifts.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {153–162},
numpages = {10},
keywords = {treserving cluster quality, temporal smoothness, preserving cluster membership, mining data streams, evolutionary spectral clustering},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281213,
author = {Chi, Yun and Zhu, Shenghuo and Song, Xiaodan and Tatemura, Junichi and Tseng, Belle L.},
title = {Structural and Temporal Analysis of the Blogosphere through Community Factorization},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281213},
doi = {10.1145/1281192.1281213},
abstract = {The blogosphere has unique structural and temporal properties since blogs are typically used as communication media among human individuals. In this paper, we propose a novel technique that captures the structure and temporal dynamics of blog communities. In our framework, a community is a set of blogs that communicate with each other triggered by some events (such as a news article). The community is represented by its structure and temporal dynamics: a community graph indicates how often one blog communicates with another, and a community intensity indicates the activity level of the community that varies over time. Our method, community factorization, extracts such communities from the blogosphere, where the communication among blogs is observed as a set of subgraphs (i.e., threads of discussion). This community extraction is formulated as a factorization problem in the framework of constrained optimization, in which the objective is to best explain the observed interactions in the blogosphere over time. We further provide a scalable algorithm for computing solutions to the constrained optimization problems. Extensive experimental studies on both synthetic and real blog data demonstrate that our technique is able to discover meaningful communities that are not detectable by traditional methods.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {163–172},
numpages = {10},
keywords = {iterative search, non-negative matrix factorization, regularization, blogosphere, blog, community factorization},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281214,
author = {Chopra, Sumit and Thampy, Trivikraman and Leahy, John and Caplin, Andrew and LeCun, Yann},
title = {Discovering the Hidden Structure of House Prices with a Non-Parametric Latent Manifold Model},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281214},
doi = {10.1145/1281192.1281214},
abstract = {In many regression problems, the variable to be predicted depends not only on a sample-specific feature vector, but also on an unknown (latent) manifold that must satisfy known constraints. An example is house prices, which depend on the characteristics of the house, and on the desirability of the neighborhood, which is not directly measurable. The proposed method comprises two trainable components. The first one is a parametric model that predicts the "intrinsic" price of the house from its description. The second one is a smooth, non-parametric model of the latent "desirability" manifold. The predicted price of a house is the product of its intrinsic price and desirability. The two components are trained simultaneously using a deterministic form of the EM algorithm. The model was trained on a large dataset of houses from Los Angeles county. It produces better predictions than pure parametric and non-parametric models. It also produces useful estimates of the desirability surface at each location.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {173–182},
numpages = {10},
keywords = {structured prediction, energy-based models, latent manifold models, expectation maximization},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281215,
author = {Cotofrei, Paul and Stoffel, Kilian},
title = {Stochastic Processes and Temporal Data Mining},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281215},
doi = {10.1145/1281192.1281215},
abstract = {This article tries to give an answer to a fundamental question intemporal data mining: "Under what conditions a temporal rule extracted from up-to-date temporal data keeps its confidence/support for future data". A possible solution is given by using, on the one hand, a temporal logic formalism which allows the definition of the main notions (event, temporal rule, support, confidence) in a formal way and, on the other hand, the stochastic limit theory. Under this probabilistic temporal framework, the equivalence between the existence of the support of a temporal rule and the law of large numbers is systematically analyzed.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {183–190},
numpages = {8},
keywords = {temporal data mining, consistency of temporal rules, temporal logic formalism, stochastic limit theory, stochastic processes},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281216,
author = {Crabtree, Daniel Wayne and Andreae, Peter and Gao, Xiaoying},
title = {Exploiting Underrepresented Query Aspects for Automatic Query Expansion},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281216},
doi = {10.1145/1281192.1281216},
abstract = {Users attempt to express their search goals through web search queries. When a search goal has multiple components or aspects, documents that represent all the aspects are likely to be more relevant than those that only represent some aspects. Current web search engines often produce result sets whose top ranking documents represent only a subset of the query aspects. By expanding the query using the right keywords, the search engine can find documents that represent more query aspects and performance improves. This paper describes AbraQ, an approach for automatically finding the right keywords to expand the query. AbraQ identifies the aspects in the query, identifies which aspects are underrepresented in the result set of the original query, and finally, for any particularly underrepresented aspect, identifies keywords that would enhance that aspect's representation and automatically expands the query using the best one. The paper presents experiments that show AbraQ significantly increases the precision of hard queries, whereas traditional automatic query expansion techniques have not improved precision. AbraQ also compared favourably against a range of interactive query expansion techniques that require user involvement including clustering, web-log analysis, relevance feedback, and pseudo relevance feedback.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {191–200},
numpages = {10},
keywords = {global document analysis, query expansion, aspect coverage, web search},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281217,
author = {Culotta, Aron and Wick, Michael and Hall, Robert and Marzilli, Matthew and McCallum, Andrew},
title = {Canonicalization of Database Records Using Adaptive Similarity Measures},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281217},
doi = {10.1145/1281192.1281217},
abstract = {It is becoming increasingly common to construct databases from information automatically culled from many heterogeneous sources. For example, a research publication database can be constructed by automatically extracting titles, authors, and conference information from online papers. A common difficulty in consolidating data from multiple sources is that records are referenced in a variety of ways (e.g. abbreviations, aliases, and misspellings). Therefore, it can be difficult to construct a single, standard representation to present to the user. We refer to the task of constructing this representation as canonicalization. Despite its importance, there is little existing work on canonicalization.In this paper, we explore the use of edit distance measures to construct a canonical representation that is "central" in the sense that it is most similar to each of the disparate records. This approach reduces the impact of noisy records on the canonical representation. Furthermore, because the user may prefer different styles of canonicalization, we show how different edit distance costs can result in different forms of canonicalization. For example, reducing the cost of character deletions can result in representations that favor abbreviated forms over expanded forms (e.g. KDD versus Conference on Knowledge Discovery and Data Mining). We describe how to learn these costs from a small amount of manually annotated data using stochastic hill-climbing. Additionally, we investigate feature-based methods to learn ranking preferences over canonicalizations. These approaches can incorporate arbitrary textual evidence to select a canonical record. We evaluate our approach on a real-world publications database and show that our learning method results in a canonicalization solution that is robust to errors and easily customizable to user preferences.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {201–209},
numpages = {9},
keywords = {data cleaning, information extraction, data mining},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281218,
author = {Dai, Wenyuan and Xue, Gui-Rong and Yang, Qiang and Yu, Yong},
title = {Co-Clustering Based Classification for out-of-Domain Documents},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281218},
doi = {10.1145/1281192.1281218},
abstract = {In many real world applications, labeled data are in short supply. It often happens that obtaining labeled data in a new domain is expensive and time consuming, while there may be plenty of labeled data from a related but different domain. Traditional machine learning is not able to cope well with learning across different domains. In this paper, we address this problem for a text-mining task, where the labeled data are under one distribution in one domain known as in-domain data, while the unlabeled data are under a related but different domain known as out-of-domain data. Our general goal is to learn from the in-domain and apply the learned knowledge to out-of-domain. We propose a co-clustering based classification (CoCC) algorithm to tackle this problem. Co-clustering is used as a bridge to propagate the class structure and knowledge from the in-domain to the out-of-domain. We present theoretical and empirical analysis to show that our algorithm is able to produce high quality classification results, even when the distributions between the two data are different. The experimental results show that our algorithm greatly improves the classification performance over the traditional learning algorithms.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {210–219},
numpages = {10},
keywords = {co-clustering, classification, Kullback-Leibler divergence, out-of-domain},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281219,
author = {Das, Kaustav and Schneider, Jeff},
title = {Detecting Anomalous Records in Categorical Datasets},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281219},
doi = {10.1145/1281192.1281219},
abstract = {We consider the problem of detecting anomalies in high aritycategorical datasets. In most applications, anomalies are defined as datapoints that are "abnormal". Quite often we have access to data which consists mostly of normal records, a long with a small percentage of unlabelled anomalous records. We are interested in the problem of unsupervised anomaly detection, where we use the unlabelled data for training, and detect records that do not follow the definition of normality.A standard approach is to create a model of normal data, and compare test records against it. A probabilistic approach builds a likelihood model from the training data. Records are tested for anomalies based on the complete record likelihood given the probability model. For categorical attributes, bayes nets give a standard representation of the likelihood. While this approach is good at finding outliers in the dataset, it often tends to detect records with attribute values that are rare. Sometimes, just detecting rare values of an attribute is not desired and such outliers are not considered as anomalies in that context. We present an alternative definition of anomalies, and propose an approach of comparing against marginal distribution of attribute subsets. We show that this is a more meaningful way of detecting anomalies, and has a better performance over semi-synthetic as well as real world datasets.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {220–229},
numpages = {10},
keywords = {anomaly detection, machine learning},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281220,
author = {Dasgupta, Anirban and Drineas, Petros and Harb, Boulos and Josifovski, Vanja and Mahoney, Michael W.},
title = {Feature Selection Methods for Text Classification},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281220},
doi = {10.1145/1281192.1281220},
abstract = {We consider feature selection for text classification both theoretically and empirically. Our main result is an unsupervised feature selection strategy for which we give worst-case theoretical guarantees on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the features. To the best of our knowledge, this is the first feature selection method with such guarantees. In addition, the analysis leads to insights as to when and why this feature selection strategy will perform well in practice. We then use the TechTC-100, 20-Newsgroups, and Reuters-RCV2 data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against two commonly-used strategies. Our empirical evaluation shows that the strategy with provable performance guarantees performs well in comparison with other commonly-used feature selection strategies. In addition, it performs better on certain datasets under very aggressive feature selection.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {230–239},
numpages = {10},
keywords = {text classification, random sampling, regularized least squares classification, feature selection},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281221,
author = {Davidson, Ian and Ravi, S. S. and Ester, Martin},
title = {Efficient Incremental Constrained Clustering},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281221},
doi = {10.1145/1281192.1281221},
abstract = {Clustering with constraints is an emerging area of data mining research. However, most work assumes that the constraints are given as one large batch. In this paper we explore the situation where the constraints are incrementally given. In this way the user after seeing a clustering can provide positive and negative feedback via constraints to critique a clustering solution. We consider the problem of efficiently updating a clustering to satisfy the new and old constraints rather than reclustering the entire data set. We show that the problem of incremental clustering under constraints is NP-hard in general, but identify several sufficient conditions which lead to efficiently solvable versions. These translate into a set of rules on the types of constraints thatcan be added and constraint set properties that must be maintained. We demonstrate that this approach is more efficient than re-clustering the entire data set and has several other advantages.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {240–249},
numpages = {10},
keywords = {constraints, clustering, algorithms, complexity},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281222,
author = {Deodhar, Meghana and Ghosh, Joydeep},
title = {A Framework for Simultaneous Co-Clustering and Learning from Complex Data},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281222},
doi = {10.1145/1281192.1281222},
abstract = {For difficult classification or regression problems, practitioners often segment the data into relatively homogenous groups and then build a model for each group. This two-step procedure usually results in simpler, more interpretable and actionable models without any lossin accuracy. We consider problems such as predicting customer behavior across products, where the independent variables can be naturally partitioned into two groups. A pivoting operation can now result in the dependent variable showing up as entries in a "customer by product" data matrix. We present a model-based co-clustering (meta)-algorithm that interleaves clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models. This algorithm provably converges to a local minimum of a suitable cost function. The framework not only generalizes co-clustering and collaborative filtering to model-basedco-clustering, but can also be viewed as simultaneous co-segmentation and classification or regression, which is better than independently clustering the data first and then building models. Moreover, it applies to a wide range of bi-modal or multimodal data, and can be easily specialized to address classification and regression problems. We demonstrate the effectiveness of our approach on both these problems through experimentation on real and synthetic data.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {250–259},
numpages = {10},
keywords = {regression, prediction models, multimodal data, co-clustering, classification},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281223,
author = {Ding, Chris and Simon, Horst D. and Jin, Rong and Li, Tao},
title = {A Learning Framework Using Green's Function and Kernel Regularization with Application to Recommender System},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281223},
doi = {10.1145/1281192.1281223},
abstract = {Green's function for the Laplace operator represents the propagation of influence of point sources and is the foundation for solving many physics problems. On a graph of pairwise similarities, the Green's function is the inverse of the combinatorial Laplacian; we resolve the zero-mode difficulty by showing its physical origin as the consequence of the Von Neumann boundary condition. We propose to use Green's function to propagate label information for both semi-supervised and unsupervised learning. We also derive this learning framework from the kernel regularization using Reproducing Kernel Hilbert Space theory at strong regularization limit. Green's function provides a well-defined distance metric on a generic weighted graph, either as the effective distance on the network of electric resistors, or the average commute time in random walks. We show that for unsupervised learning this approach is identical to Ratio Cut and Normalized Cut spectral clustering algorithms. Experiments on newsgroups and six UCI datasets illustrate the effectiveness of this approach. Finally, we propose a novel item-based recommender system using Green's function and show its effectiveness.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {260–269},
numpages = {10},
keywords = {semi-supervised learning, label propagation},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281224,
author = {Dou, Dejing and Frishkoff, Gwen and Rong, Jiawei and Frank, Robert and Malony, Allen and Tucker, Don},
title = {Development of NeuroElectroMagnetic Ontologies(NEMO): A Framework for Mining Brainwave Ontologies},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281224},
doi = {10.1145/1281192.1281224},
abstract = {Event-related potentials (ERP) are brain electrophysiological patterns created by averaging electroencephalographic (EEG) data, time-locking to events of interest (e.g., stimulus or response onset). In this paper, we propose a generic framework for mining anddeveloping domain ontologies and apply it to mine brainwave (ERP) ontologies. The concepts and relationships in ERP ontologies can be mined according to the following steps: pattern decomposition, extraction of summary metrics for concept candidates, hierarchical clustering of patterns for classes and class taxonomies, and clustering-based classification and association rules mining for relationships (axioms) of concepts. We have applied this process to several dense-array (128-channel) ERP datasets. Results suggest good correspondence between mined concepts and rules, on the one hand, and patterns and rules that were independently formulated by domain experts, on the other. Data mining results also suggest ways in which expert-defined rules might be refined to improve ontologyrepresentation and classification results. The next goal of our ERP ontology mining framework is to address some long-standing challenges in conducting large-scale comparison and integration of results across ERP paradigms and laboratories. In a more general context, this work illustrates the promise of an interdisciplinary research program, which combines data mining, neuroinformatics andontology engineering to address real-world problems.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {270–279},
numpages = {10},
keywords = {ERP, ontology mining, clustering-based classification, temporal PCA, semantic web},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281225,
author = {Druck, Gregory and Pal, Chris and McCallum, Andrew and Zhu, Xiaojin},
title = {Semi-Supervised Classification with Hybrid Generative/Discriminative Methods},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281225},
doi = {10.1145/1281192.1281225},
abstract = {We compare two recently proposed frameworks for combining generative and discriminative probabilistic classifiers and apply them to semi-supervised classification. In both cases we explore the tradeoff between maximizing a discriminative likelihood of labeled data and a generative likelihood of labeled and unlabeled data. While prominent semi-supervised learning methods assume low density regions between classes or are subject to generative modeling assumptions, we conjecture that hybrid generative/discriminative methods allow semi-supervised learning in the presence of strongly overlapping classes and reduce the risk of modeling structure in the unlabeled data that is irrelevant for the specific classification task of interest. We apply both hybrid approaches within naively structured Markov random field models and provide a thorough empirical comparison with two well-known semi-supervised learning methods on six text classification tasks. A semi-supervised hybrid generative/discriminative method provides the best accuracy in 75% of the experiments, and the multi-conditional learning hybrid approach achieves the highest overall mean accuracy across all tasks.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {280–289},
numpages = {10},
keywords = {hybrid generative/discriminative methods, text classification, semi-supervised learning},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281226,
author = {Friedland, Lisa and Jensen, David},
title = {Finding Tribes: Identifying Close-Knit Individuals from Employment Patterns},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281226},
doi = {10.1145/1281192.1281226},
abstract = {We present a family of algorithms to uncover tribes-groups of individuals who share unusual sequences of affiliations. While much work inferring community structure describes large-scale trends, we instead search for small groups of tightly linked individuals who behave anomalously with respect to those trends. We apply the algorithms to a large temporal and relational data set consisting of millions of employment records from the National Association of Securities Dealers. The resulting tribes contain individuals at higher risk for fraud, are homogenous with respect to risk scores, and are geographically mobile, all at significant levels compared to random or to other sets of individuals who share affiliations.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {290–299},
numpages = {10},
keywords = {social networks, dynamic networks, anomaly detection},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281227,
author = {Fung, Gabriel Pui Cheong and Yu, Jeffrey Xu and Liu, Huan and Yu, Philip S.},
title = {Time-Dependent Event Hierarchy Construction},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281227},
doi = {10.1145/1281192.1281227},
abstract = {In this paper, an algorithm called Time Driven Documents-partition (TDD) is proposed to construct an event hierarchy in a text corpus based on a given query. Specifically, assume that a query contains only one feature - Election. Election is directly related to the events such as 2006 US Midterm Elections Campaign, 2004 US Presidential Election Campaign and 2004 Taiwan Presidential Election Campaign, where these events may further be divided into several smaller events (e.g. the 2006 US Midterm Elections Campaign can be broken down into events such as campaign for vote, election results and the resignation of Donald H. Rumsfeld). As such, an event hierarchy is resulted. Our proposed algorithm, TDD, tackles the problem by three major steps: (1)Identify the features that are related to the query according to both the timestamps and the contents of the documents. The features identified are regarded as bursty features; (2) Extract the documents that are highly related to the bursty features based on time; (3) Partition the extracted documents to form events and organize them in a hierarchicalstructure. To the best of our knowledge, there is little works targeting for constructing a feature-based event hierarchy for a text corpus. Practically, event hierarchies can assist us to efficiently locate our target information in a text corpus easily. Again, assume that Election is used for a query. Without an event hierarchy, it is very difficult to identify what are the major events related to it, when do these events happened, as well as the features and the news articles that are related to each of these events. We have archived two-year news articles to evaluate the feasibility of TDD. The encouraging results indicated that TDD is practically sound and highly effective.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {300–309},
numpages = {10},
keywords = {time, text, clustering, hierarchies, events, presentation, retrieval},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281228,
author = {Gao, Byron J. and Ester, Martin and Cai, Jin-Yi and Schulte, Oliver and Xiong, Hui},
title = {The Minimum Consistent Subset Cover Problem and Its Applications in Data Mining},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281228},
doi = {10.1145/1281192.1281228},
abstract = {In this paper, we introduce and study the Minimum Consistent Subset Cover (MCSC) problem. Given a finite ground set X and a constraint t, find the minimum number of consistent subsets that cover X, where a subset of X is consistent if it satisfies t. The MCSC problem generalizes the traditional set covering problem and has Minimum Clique Partition, a dual problem of graph coloring, as an instance. Many practical data mining problems in the areas of rule learning, clustering, and frequent pattern mining can be formulated as MCSC instances. In particular, we discuss the Minimum Rule Set problem that minimizes model complexity of decision rules as well as some converse k-clustering problems that minimize the number of clusters satisfying certain distance constraints. We also show how the MCSC problem can find applications in frequent pattern summarization. For any of these MCSC formulations, our proposed novel graph-based generic algorithm CAG can be directly applicable. CAG starts by constructing a maximal optimal partial solution, then performs an example-driven specific-to-general search on a dynamically maintained bipartite assignment graph to simultaneously learn a set of consistent subsets with small cardinality covering the ground set. Our experiments on benchmark datasets show that CAG achieves good results compared to existing popular heuristics.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {310–319},
numpages = {10},
keywords = {minimum consistent subset cover, minimum rule set, converse k-clustering, pattern summarization},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281229,
author = {Ge, Rong and Ester, Martin and Jin, Wen and Davidson, Ian},
title = {Constraint-Driven Clustering},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281229},
doi = {10.1145/1281192.1281229},
abstract = {Clustering methods can be either data-driven or need-driven. Data-driven methods intend to discover the true structure of the underlying data while need-driven methods aims at organizing the true structure to meet certain application requirements. Thus, need-driven (e.g. constrained) clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks, privacy preservation, and market segmentation. However, the existing methods of constrained clustering require users to provide the number of clusters, which is often unknown in advance, but has a crucial impact on the clustering result. In this paper, we argue that a more natural way to generate actionable clusters is to let the application-specific constraints decide the number of clusters. For this purpose, we introduce a novel cluster model, Constraint-Driven Clustering (CDC), which finds an a priori unspecified number of compact clusters that satisfy all user-provided constraints. Two general types of constraints are considered, i.e. minimum significance constraints and minimum variance constraints, as well as combinations of these two types. We prove the NP-hardness of the CDC problem with different constraints. We propose a novel dynamic data structure, the CD-Tree, which organizes data points in leaf nodes such that each leaf node approximately satisfies the CDC constraints and minimizes the objective function. Based on CD-Trees, we develop an efficient algorithm to solve the new clustering problem. Our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {320–329},
numpages = {10},
keywords = {NP-hardness, clustering, constraints},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281230,
author = {Giannotti, Fosca and Nanni, Mirco and Pinelli, Fabio and Pedreschi, Dino},
title = {Trajectory Pattern Mining},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281230},
doi = {10.1145/1281192.1281230},
abstract = {The increasing pervasiveness of location-acquisition technologies (GPS, GSM networks, etc.) is leading to the collection of large spatio-temporal datasets and to the opportunity of discovering usable knowledge about movement behaviour, which fosters novel applications and services. In this paper, we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectories of moving objects. We introduce trajectory patterns as concise descriptions of frequent behaviours, in terms of both space (i.e., the regions of space visited during movements) and time (i.e., the duration of movements). In this setting, we provide a general formal statement of the novel mining problem and then study several different instantiations of different complexity. The various approaches are then empirically evaluated over real data and synthetic benchmarks, comparing their strengths and weaknesses.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {330–339},
numpages = {10},
keywords = {trajectory patterns, spatio-temporal data mining},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281231,
author = {Guo, Zhen and Zhang, Zhongfei and Xing, Eric and Faloutsos, Christos},
title = {Enhanced Max Margin Learning on Multimodal Data Mining in a Multimedia Database},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281231},
doi = {10.1145/1281192.1281231},
abstract = {The problem of multimodal data mining in a multimedia database can be addressed as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables. In this paper, built upon the existing literature on the max margin based learning, we develop a new max margin learning approach called Enhanced Max Margin Learning (EMML) framework. In addition, we apply EMML framework to developing an effective and efficient solution to the multimodal data mining problem in a multimedia database. The main contributions include: (1) we have developed a new max margin learning approach - the enhanced max margin learning framework that is much more efficient in learning with a much faster convergence rate, which is verified in empirical evaluations; (2) we have applied this EMML approach to developing an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale, allowing facilitating a multimodal data mining querying to a very large scale multimedia database,and excelling many existing multimodal data mining methods in the literature that do not scale up at all; this advantage is also supported through the complexity analysis as well as empirical evaluations against a state-of-the-art multimodal data mining method from the literature. While EMML is a general framework, for the evaluation purpose, we apply it to the Berkeley Drosophila embryo image database, and report the performance comparison with a state-of-the-art multimodal data mining method.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {340–349},
numpages = {10},
keywords = {image retrieval, multimodal data mining, image annotation, max margin},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281232,
author = {Heikinheimo, Hannes and Hinkkanen, Eino and Mannila, Heikki and Mielik\"{a}inen, Taneli and Sepp\"{a}nen, Jouni K.},
title = {Finding Low-Entropy Sets and Trees from Binary Data},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281232},
doi = {10.1145/1281192.1281232},
abstract = {The discovery of subsets with special properties from binary data hasbeen one of the key themes in pattern discovery. Pattern classes suchas frequent itemsets stress the co-occurrence of the value 1 in the data. While this choice makes sense in the context of sparse binary data, it disregards potentially interesting subsets of attributes that have some other type of dependency structure.We consider the problem of finding all subsets of attributes that have low complexity. The complexity is measured by either the entropy of the projection of the data on the subset, or the entropy of the data for the subset when modeled using a Bayesian tree, with downward or upward pointing edges. We show that the entropy measure on sets has a monotonicity property, and thus a levelwise approach can find all low-entropy itemsets. We also show that the tree-based measures are bounded above by the entropy of the corresponding itemset, allowing similar algorithms to be used for finding low-entropy trees. We describe algorithms for finding all subsets satisfying an entropy condition. We give an extensive empirical evaluation of the performance of the methods both on synthetic and on real data. We also discuss the search for high-entropy subsets and the computation of the Vapnik-Chervonenkis dimension of the data.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {350–359},
numpages = {10},
keywords = {local models, pattern discovery},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281233,
author = {Janssens, Frizo and Gl\"{a}nzel, Wolfgang and De Moor, Bart},
title = {Dynamic Hybrid Clustering of Bioinformatics by Incorporating Text Mining and Citation Analysis},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281233},
doi = {10.1145/1281192.1281233},
abstract = {To unravel the concept structure and dynamics of the bioinformatics field, we analyze a set of 7401 publications from the Web of Science and MEDLINE databases, publication years 1981-2004. For delineating this complex, interdisciplinary field, a novel bibliometric retrieval strategy is used. Given that the performance of unsupervised clustering and classification of scientific publications is significantly improved by deeply merging textual contents with the structure of the citation graph, we proceed with a hybrid clustering method based on Fisher's inverse chi-square. The optimal number of clusters is determined by a compound semiautomatic strategy comprising a combination of distance-based and stability-based methods. We also investigate the relationship between number of Latent Semantic Indexing factors, number of clusters, and clustering performance. The HITS and PageRank algorithms are used to determine representative publications in each cluster. Next, we develop a methodology for dynamic hybrid clustering of evolving bibliographic data sets. The same clustering methodology is applied to consecutive periods defined by time windows on the set, and in a subsequent phase chains are formed by matching and tracking clusters through time. Term networks for the eleven resulting cluster chains present the cognitive structure of the field. Finally, we provide a view on how much attention the bioinformatics community has devoted to the different subfields through time.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {360–369},
numpages = {10},
keywords = {fisher's inverse chi-square method, cluster chains},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281234,
author = {Jo, Yookyung and Lagoze, Carl and Giles, C. Lee},
title = {Detecting Research Topics via the Correlation between Graphs and Texts},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281234},
doi = {10.1145/1281192.1281234},
abstract = {In this paper we address the problem of detecting topics in large-scale linked document collections. Recently, topic detection has become a very active area of research due to its utility for information navigation, trend analysis, and high-level description of data. We present a unique approach that uses the correlation between the distribution of a term that represents a topic and the link distribution in the citation graph where the nodes are limited to the documents containing the term. This tight coupling between term and graph analysis is distinguished from other approaches such as those that focus on language models. We develop a topic score measure for each term, using the likelihood ratio of binary hypotheses based on a probabilistic description of graph connectivity. Our approach is based on the intuition that if a term is relevant to a topic, the documents containing the term have denser connectivity than a random selection of documents. We extend our algorithm to detect a topic represented by a set of terms, using the intuition that if the co-occurrence of terms represents a new topic, the citation pattern should exhibit the synergistic effect. We test our algorithm on two electronic research literature collections,arXiv and Citeseer.Our evaluation shows that the approach is effective and reveals some novel aspects of topic detection.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {370–379},
numpages = {10},
keywords = {topic detection, correlation of text and links, graph mining, citation graphs, probabilistic measure},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281235,
author = {Karras, Panagiotis and Sacharidis, Dimitris and Mamoulis, Nikos},
title = {Exploiting Duality in Summarization with Deterministic Guarantees},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281235},
doi = {10.1145/1281192.1281235},
abstract = {Summarization is an important task in data mining. A major challenge over the past years has been the efficient construction of fixed-space synopses that provide a deterministic quality guarantee, often expressed in terms of a maximum-error metric. Histograms and several hierarchical techniques have been proposed for this problem. However, their time and/or space complexities remain impractically high and depend not only on the data set size n, but also on the space budget B. These handicaps stem from a requirement to tabulate all allocations of synopsis space to different regions of the data. In this paper we develop an alternative methodology that dispels these deficiencies, thanks to a fruitful application of the solution to the dual problem: given a maximum allowed error, determine the minimum-space synopsis that achieves it. Compared to the state-of-the-art, our histogram construction algorithm reduces time complexity by (at least) a Blog2n over logε* factor and our hierarchical synopsis algorithm reduces the complexity by (at least) a factor of log2B over logε* + logn in time and B(1-log B over log n) in space, where ε* is the optimal error. These complexity advantages offer both a space-efficiency and a scalability that previous approaches lacked. We verify the benefits of our approach in practice by experimentation.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {380–389},
numpages = {10},
keywords = {synopses, efficiency, histograms, wavelets},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281236,
author = {Ke, Yiping and Cheng, James and Ng, Wilfred},
title = {Correlation Search in Graph Databases},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281236},
doi = {10.1145/1281192.1281236},
abstract = {Correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects. However, the research of correlation mining from graph databases is still lacking despite the fact that graph data, especially in various scientific domains, proliferate in recent years. In this paper, we propose a new problem of correlation mining from graph databases, called Correlated Graph Search (CGS). CGS adopts Pearson's correlation coefficient as a correlation measure to take into consideration the occurrence distributions of graphs. However, the problem poses significant challenges, since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential. We derive two necessary conditions which set bounds on the occurrence probability of a candidate in the database. With this result, we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidates. To further improve the efficiency, we develop three heuristic rules and apply them on the candidate set to further reduce the search space. Our extensive experiments demonstrate the effectiveness of our method on candidate reduction. The results also justify the efficiency of our algorithm in mining correlations from large real and synthetic datasets.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {390–399},
numpages = {10},
keywords = {correlation, graph databases, Pearson's correlation coefficient},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281237,
author = {Kolcz, Aleksander and Yih, Wen-tau},
title = {Raising the Baseline for High-Precision Text Classifiers},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281237},
doi = {10.1145/1281192.1281237},
abstract = {Many important application areas of text classifiers demand high precision andit is common to compare prospective solutions to the performance of Naive Bayes. This baseline is usually easy to improve upon, but in this work we demonstrate that appropriate document representation can make out performing this classifier much more challenging. Most importantly, we provide a link between Naive Bayes and the logarithmic opinion pooling of the mixture-of-experts framework, which dictates a particular type of document length normalization. Motivated by document-specific feature selection we propose monotonic constraints on document term weighting, which is shown as an effective method of fine-tuning document representation. The discussion is supported by experiments using three large email corpora corresponding to the problem of spam detection, where high precision is of particular importance.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {400–409},
numpages = {10},
keywords = {email spam detection, naive bayes, low false positive rates, high precision text classification},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281238,
author = {Laxman, Srivatsan and Sastry, P. S. and Unnikrishnan, K. P.},
title = {A Fast Algorithm for Finding Frequent Episodes in Event Streams},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281238},
doi = {10.1145/1281192.1281238},
abstract = {Frequent episode discovery is a popular framework for mining data available as a long sequence of events. An episode is essentially a short ordered sequence of event types and the frequency of an episode is some suitable measure of how often the episode occurs in the data sequence. Recently,we proposed a new frequency measure for episodes based on the notion of non-overlapped occurrences of episodes in the event sequence, and showed that, such a definition, in addition to yielding computationally efficient algorithms, has some important theoretical properties in connecting frequent episode discovery with HMM learning. This paper presents some new algorithms for frequent episode discovery under this non-overlapped occurrences-based frequency definition. The algorithms presented here are better (by a factor of N, where N denotes the size of episodes being discovered) in terms of both time and space complexities when compared to existing methods for frequent episode discovery. We show through some simulation experiments, that our algorithms are very efficient. The new algorithms presented here have arguably the least possible orders of spaceand time complexities for the task of frequent episode discovery.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {410–419},
numpages = {10},
keywords = {frequent episodes, event streams, non-overlapped occurrences, temporal data mining},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281239,
author = {Leskovec, Jure and Krause, Andreas and Guestrin, Carlos and Faloutsos, Christos and VanBriesen, Jeanne and Glance, Natalie},
title = {Cost-Effective Outbreak Detection in Networks},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281239},
doi = {10.1145/1281192.1281239},
abstract = {Given a water distribution network, where should we place sensors toquickly detect contaminants? Or, which blogs should we read to avoid missing important stories?.These seemingly different problems share common structure: Outbreak detection can be modeled as selecting nodes (sensor locations, blogs) in a network, in order to detect the spreading of a virus or information asquickly as possible. We present a general methodology for near optimal sensor placement in these and related problems. We demonstrate that many realistic outbreak detection objectives (e.g., detection likelihood, population affected) exhibit the property of "submodularity". We exploit submodularity to develop an efficient algorithm that scales to large problems, achieving near optimal placements, while being 700 times faster than a simple greedy algorithm. We also derive online bounds on the quality of the placements obtained by any algorithm. Our algorithms and bounds also handle cases where nodes (sensor locations, blogs) have different costs.We evaluate our approach on several large real-world problems,including a model of a water distribution network from the EPA, andreal blog data. The obtained sensor placements are provably near optimal, providing a constant fraction of the optimal solution. We show that the approach scales, achieving speedups and savings in storage of several orders of magnitude. We also show how the approach leads to deeper insights in both applications, answering multicriteria trade-off, cost-sensitivity and generalization questions.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {420–429},
numpages = {10},
keywords = {sensor placement, information cascades, virus propagation, graphs, submodular functions},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281240,
author = {Li, Jinyan and Liu, Guimei and Wong, Limsoon},
title = {Mining Statistically Important Equivalence Classes and Delta-Discriminative Emerging Patterns},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281240},
doi = {10.1145/1281192.1281240},
abstract = {The support-confidence framework is the most common measure used in itemset mining algorithms, for its antimonotonicity that effectively simplifies the search lattice. This computational convenience brings both quality and statistical flaws to the results as observed by many previous studies. In this paper, we introduce a novel algorithm that produces itemsets with ranked statistical merits under sophisticated test statistics such as chi-square, risk ratio, odds ratio, etc. Our algorithm is based on the concept of equivalence classes. An equivalence class is a set of frequent itemsets that always occur together in the same set of transactions. Therefore, itemsets within an equivalence class all share the same level of statistical significance regardless of the variety of test statistics. As an equivalence class can be uniquely determined and concisely represented by a closed pattern and a set of generators, we just mine closed patterns and generators, taking a simultaneous depth-first search scheme. This parallel approach has not been exploited by any prior work. We evaluate our algorithm on two aspects. In general, we compare to LCM and FPclose which are the best algorithms tailored for mining only closed patterns. In particular, we compare to epMiner which is the most recent algorithm for mining a type of relative risk patterns, known as minimal emerging patterns. Experimental results show that our algorithm is faster than all of them, sometimes even multiple orders of magnitude faster. These statistically ranked patterns and the efficiency have a high potential for real-life applications, especially in biomedical and financial fields where classical test statistics are of dominant interest.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {430–439},
numpages = {10},
keywords = {equivalence classes, itemsets with ranked statistical merit},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281241,
author = {Li, Ping},
title = {Very Sparse Stable Random Projections for Dimension Reduction in <i>l</i>α (0 &lt;α ≤ 2) Norm},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281241},
doi = {10.1145/1281192.1281241},
abstract = {The method of stable random projections is a useful tool for efficiently computing the lα (0 &lt; α ≤ 2) norms and distances in massive data in one pass. Consider a data matrix A ∈RnxD. If we multiply A with a projection matrix R ΕR Dxk (k« D),whose entries are i.i.d. samples of an α-stable distribution,then the projected matrix B = Ax R Ε R nxkx containsenough information to approximately recover the l α properties in A.We propose very sparse stable random projections, by replacing the α stable distribution with a (much simpler) mixture of a symmetric α Pareto distribution (with probability Β, 0 β Β 1) and a point mass at the origin(with probability 1-Β). This leads to a significant 1 over Β fold speedup for small Β when computing B = AxR and a 1 over Β-fold cost reduction in storing R. By analyzing the convergence, we show that in"reasonable" datasets Β often can be very small (e.g.,D1/2 without hurting the estimation accuracy. Some numerical evaluations are conducted, on synthetic data, Web crawldata, and gene expression microarray data.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {440–449},
numpages = {10},
keywords = {convergence, random projections, dimension reductions, sparse projections, stable Ddistributions, asymptotic analysis},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281242,
author = {Liu, Yi and Jin, Rong and Jain, Anil K.},
title = {BoostCluster: Boosting Clustering by Pairwise Constraints},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281242},
doi = {10.1145/1281192.1281242},
abstract = {Data clustering is an important task in many disciplines. A large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints. However, these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints. We present a boosting framework for data clustering,termed as BoostCluster, that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints. The key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised. The proposed framework addresses this problem by dynamically generating new data representations at each iteration that are, on the one hand, adapted to the clustering results at previous iterations by the given algorithm, and on the other hand consistent with the given side information. Our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering algorithms (K-means, partitional SingleLink, spectral clustering), and its performance is comparable to the state-of-the-art algorithms for data clustering with side information.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {450–459},
numpages = {10},
keywords = {boosting, semi-supervised learning, pairwise constraints, data clustering},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281243,
author = {Lo, David and Khoo, Siau-Cheng and Liu, Chao},
title = {Efficient Mining of Iterative Patterns for Software Specification Discovery},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281243},
doi = {10.1145/1281192.1281243},
abstract = {Studies have shown that program comprehension takes up to 45% of software development costs. Such high costs are caused by the lack-of documented specification and further aggravated by the phenomenon of software evolution. There is a need for automated tools to extract specifications to aid program comprehension. In this paper, a novel technique to efficiently mine common software temporal patterns from traces is proposed. These patterns shed light on program behaviors, and are termed iterative patterns. They capture unique characteristic of software traces, typically not found in arbitrary sequences. Specifically, due to loops, interesting iterative patterns can occur multiple times within a trace. Furthermore, an occurrence of an iterative pattern in a trace can extend across a sequence of indefinite length. Since a program behavior can be manifested in numerous ways, analyzing a single trace will not be sufficient. Iterative pattern mining extends sequential pattern and episode minings to discover frequent iterative patterns which occur repetitively both within a program trace and across multiple traces. In this paper, we present CLIPER (CLosed Iterative Pattern minER) to efficiently mine a closed set of iterative patterns. A performance study on several simulated and real datasets shows the efficiency of our mining algorithm and effectiveness of our pruning strategy. Our case study on JBoss Application Server confirms the usefulness of mined patterns in discovering interesting software behavioral specification.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {460–469},
numpages = {10},
keywords = {software specification discovery, closed iterative patterns},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281244,
author = {Long, Bo and Zhang, Zhongfei Mark and Yu, Philip S.},
title = {A Probabilistic Framework for Relational Clustering},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281244},
doi = {10.1145/1281192.1281244},
abstract = {Relational clustering has attracted more and more attention due to its phenomenal impact in various important applications which involve multi-type interrelated data objects, such as Web mining, search marketing, bioinformatics, citation analysis, and epidemiology. In this paper, we propose a probabilistic model for relational clustering, which also provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering, semi-supervised clustering, co-clustering and graph clustering. The proposed model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects. Under this model, we propose parametric hard and soft relational clustering algorithms under a large number of exponential family distributions. The algorithms are applicable to relational data of various structures and at the same time unifies a number of stat-of-the-art clustering algorithms: co-clustering algorithms, the k-partite graph clustering, Bregman k-means, and semi-supervised clustering based on hidden Markov random fields.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {470–479},
numpages = {10},
keywords = {relational data, relational clustering, clustering},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281245,
author = {Mannila, Heikki and Terzi, Evimaria},
title = {Nestedness and Segmented Nestedness},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281245},
doi = {10.1145/1281192.1281245},
abstract = {Consider each row of a 0-1 dataset as the subset of the columns for which the row has an 1. Then a dataset is nested, if for all pairs of rows one row is either a superset or subset of the other. The concept of nestedness has its origins in ecology, where approximate versions of it has been used to model the species distribution in different locations. We argue that nestedness and its extensions are interesting properties of datasets, and that they can be applied also to domains other than ecology.We first define natural measures of nestedness and study their properties. We then define the concept of k-nestedness: a dataset is (almost) k-nested if the set of columns can be partitioned to k parts so that each part is (almost) nested. We consider the algorithmic problems of computing how far a dataset is from being k-nested, and for finding a good partition of the columns into k parts. The algorithms are based on spectral partitioning, and scale to moderately large datasets. We apply the methods to real data from ecology and from other applications, and demonstrate the usefulness of the concept.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {480–489},
numpages = {10},
keywords = {0-1 matrices, presence/absence data, nestedness},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281246,
author = {Mei, Qiaozhu and Shen, Xuehua and Zhai, ChengXiang},
title = {Automatic Labeling of Multinomial Topic Models},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281246},
doi = {10.1145/1281192.1281246},
abstract = {Multinomial distributions over words are frequently used to model topics in text collections. A common, major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic. So far, such labels have been generated manually in a subjective way. In this paper, we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way. We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information between a label and a topic model. Experiments with user study have been done on two text data sets with different genres.The results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models. Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA, LDA, and their variations.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {490–499},
numpages = {10},
keywords = {topic model labeling, multinomial distribution, statistical topic models},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281247,
author = {Mimno, David and McCallum, Andrew},
title = {Expertise Modeling for Matching Papers with Reviewers},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281247},
doi = {10.1145/1281192.1281247},
abstract = {An essential part of an expert-finding task, such as matching reviewers to submitted papers, is the ability to model the expertise of a person based on documents. We evaluate several measures of the association between an author in an existing collection of research papers and a previously unseen document. We compare two language model based approaches with a novel topic model, Author-Persona-Topic (APT). In this model, each author can write under one or more "personas," which are represented as independent distributions over hidden topics. Examples of previous papers written by prospective reviewers are gathered from the Rexa database, which extracts and disambiguates author mentions from documents gathered from the web. We evaluate the models using a reviewer matching task based on human relevance judgments determining how well the expertise of proposed reviewers matches a submission. We find that the APT topic model outperforms the other models.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {500–509},
numpages = {10},
keywords = {reviewer finding, topic models},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281248,
author = {Moser, Flavia and Ge, Rong and Ester, Martin},
title = {Joint Cluster Analysis of Attribute and Relationship Data Withouta-Priori Specification of the Number of Clusters},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281248},
doi = {10.1145/1281192.1281248},
abstract = {In many applications, attribute and relationship data areavailable, carrying complementary information about real world entities. In such cases, a joint analysis of both types of data can yield more accurate results than classical clustering algorithms that either use only attribute data or only relationship (graph) data. The Connected k-Center (CkC) has been proposed as the first joint cluster analysis model to discover k clusters which are cohesive on both attribute and relationship data. However, it is well-known that prior knowledge on the number of clusters is often unavailable in applications such as community dentification and hotspot analysis. In this paper, we introduce and formalize the problem of discovering an a-priori unspecified number of clusters in the context of joint cluster analysis of attribute and relationship data, called Connected X Clusters (CXC) problem. True clusters are assumed to be compact and distinctive from their neighboring clusters in terms of attribute data and internally connected in terms of relationship data. Different from classical attribute-based clustering methods, the neighborhood of clusters is not defined in terms of attribute data but in terms of relationship data. To efficiently solve the CXC problem, we present JointClust, an algorithm which adopts a dynamic two-phase approach. In the first phase, we find so called cluster atoms. We provide a probability analysis for thisphase, which gives us a probabilistic guarantee, that each true cluster is represented by at least one of the initial cluster atoms. In the second phase, these cluster atoms are merged in a bottom-up manner resulting in a dendrogram. The final clustering is determined by our objective function. Our experimental evaluation on several real datasets demonstrates that JointClust indeed discovers meaningful and accurate clusterings without requiring the user to specify the number of clusters.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {510–519},
numpages = {10},
keywords = {clustering, hotspot analysis, graph-structured data, algorithms, joint cluster analysis, community identification},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281249,
author = {Nallapati, Ramesh M. and Ditmore, Susan and Lafferty, John D. and Ung, Kin},
title = {Multiscale Topic Tomography},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281249},
doi = {10.1145/1281192.1281249},
abstract = {Modeling the evolution of topics with time is of great value in automatic summarization and analysis of large document collections. In this work, we propose a new probabilistic graphical model to address this issue. The new model, which we call the Multiscale Topic Tomography Model (MTTM), employs non-homogeneous Poisson processes to model generation of word-counts. The evolution of topics is modeled through a multi-scale analysis using Haar wavelets. One of the new features of the model is its modeling the evolution of topics at various time-scales of resolution, allowing the user to zoom in and out of the time-scales. Our experiments on Science data using the new model uncovers some interesting patterns in topics. The new model is also comparable to LDA in predicting unseen data as demonstrated by our perplexity experiments.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {520–529},
numpages = {10},
keywords = {poisson, time-scale, temporal evolution, topic modeling},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281250,
author = {Nijssen, Siegfried and Fromont, Elisa},
title = {Mining Optimal Decision Trees from Itemset Lattices},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281250},
doi = {10.1145/1281192.1281250},
abstract = {We present DL8, an exact algorithm for finding a decision tree that optimizes a ranking function under size, depth, accuracy and leaf constraints. Because the discovery of optimal trees has high theoretical complexity, until now few efforts have been made to compute such trees for real-world datasets. An exact algorithm is of both scientific and practical interest. From a scientific point of view, it can be used as a gold standard to evaluate the performance of heuristic constraint-based decision tree learners and to gain new insight in traditional decision tree learners. From the application point of view, it can be used to discover trees that cannot be found by heuristic decision tree learners. The key idea behind our algorithm is that there is a relation between constraints on decision trees and constraints on itemsets. We show that optimal decision trees can be extracted from lattices of itemsets in linear time. We give several strategies to efficiently build these lattices. Experiments show that under the same constraints, DL8 obtains better results than C4.5, which confirms that exhaustive search does not always imply overfitting. The results also show that DL8 is a useful and interesting tool to learn decision trees under constraints.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {530–539},
numpages = {10},
keywords = {frequent itemsets, constraint-based mining, formal concepts, decision trees},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281251,
author = {Pandey, Gaurav and Steinbach, Michael and Gupta, Rohit and Garg, Tushar and Kumar, Vipin},
title = {Association Analysis-Based Transformations for Protein Interaction Networks: A Function Prediction Case Study},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281251},
doi = {10.1145/1281192.1281251},
abstract = {Protein interaction networks are one of the most promising types of biological data for the discovery of functional modules and the prediction of individual protein functions. However, it is known that these networks are both incomplete and inaccurate, i.e., they have spurious edges and lackbiologically valid edges. One way to handle this problem is by transforming the original interaction graph into new graphs that remove spurious edges, add biologically valid ones, and assign reliability scores to the edges constituting the final network. We investigate currently existing methods, as well as propose a robust association analysis-based method for this task. This method is based on the concept of h-confidence, which is a measure that can be used to extract groups of objects having high similarity with each other. Experimental evaluation on several protein interaction data sets show that hyperclique-based transformations enhance the performance of standard function prediction algorithms significantly, and thus have merit.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {540–549},
numpages = {10},
keywords = {association analysis, protein function prediction, noise reduction, h-confidence, protein interaction networks},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281252,
author = {Park, Seung-Taek and Pennock, David M.},
title = {Applying Collaborative Filtering Techniques to Movie Search for Better Ranking and Browsing},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281252},
doi = {10.1145/1281192.1281252},
abstract = {We propose a new ranking method, which combines recommender systems with information search tools for better search and browsing. Our method uses a collaborative filtering algorithm to generate personal item authorities for each user and combines them with item proximities for better ranking. To demonstrate our approach, we build a prototype movie search and browsing engine called MAD6 (Movies, Actors and Directors; 6 degrees of separation). We conduct offline and online tests of our ranking algorithm. For offline testing, we use Yahoo! Search queries that resulted in a click on a Yahoo! Movies or Internet Movie Database (IMDB) movie URL. Our online test involved 44 Yahoo! employees providing subjective assessments of results quality. In both tests, our ranking methods show significantly better recall and quality than IMDB search and Yahoo! Movies current search.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {550–559},
numpages = {10},
keywords = {collaborative filtering, movie search, information retrieval, search ranking, recommender systems},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281253,
author = {Pon, Raymond K. and Cardenas, Alfonso F. and Buttler, David and Critchlow, Terence},
title = {Tracking Multiple Topics for Finding Interesting Articles},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281253},
doi = {10.1145/1281192.1281253},
abstract = {We introduce multiple topic tracking (MTT) for iScore to better recommend news articles for users with multiple interests and to address changes in user interests over time. As an extension of the basic Rocchio algorithm, traditional topic detection and tracking, and single-pass clustering, MTT maintains multiple interest profiles to identify interesting articles for a specific user given user-feedback. Focusing on only interesting topics enables iScore to discard useless profiles to address changes in user interests and to achieve a balance between resource consumption and classification accuracy. Also by relating a topic's interestingness to an article.s interestingness, iScore is able to achieve higher quality results than traditional methods such as the Rocchio algorithm. We identify several operating parameters that work well for MTT. Using the same parameters, we show that MTT alone yields high quality results for recommending interesting articles from several corpora. The inclusion of MTT improves iScore's performance by 9% in recommending news articles from the Yahoo! News RSS feeds and the TREC11 adaptive filter article collection. And through a small user study, we show that iScore can still perform well when only provided with little user feedback.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {560–569},
numpages = {10},
keywords = {news recommendation, personalization, news filtering},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281254,
author = {Radlinski, Filip and Joachims, Thorsten},
title = {Active Exploration for Learning Rankings from Clickthrough Data},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281254},
doi = {10.1145/1281192.1281254},
abstract = {We address the task of learning rankings of documents from search enginelogs of user behavior. Previous work on this problem has relied onpassively collected clickthrough data. In contrast, we show that anactive exploration strategy can provide data that leads to much fasterlearning. Specifically, we develop a Bayesian approach for selectingrankings to present users so that interactions result in more informativetraining data. Our results using the TREC-10 Web corpus, as well assynthetic data, demonstrate that a directed exploration strategy quicklyleads to users being presented improved rankings in an online learningsetting. We find that active exploration substantially outperformspassive observation and random exploration.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {570–579},
numpages = {10},
keywords = {active exploration, learning to rank, clickthrough data, web search},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281255,
author = {Sandler, Mark},
title = {Hierarchical Mixture Models: A Probabilistic Analysis},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281255},
doi = {10.1145/1281192.1281255},
abstract = {Mixture models form one of the most widely used classes of generative models for describing structured and clustered data. In this paper we develop a new approach for the analysis of hierarchical mixture models. More specifically, using a text clustering problem as a motivation, we describe a natural generative process that creates a hierarchical mixture model for the data. In this process, an adversary starts with an arbitrary base distribution and then builds a topic hierarchy via some evolutionary process, where he controls the parameters of the process. We prove that under our assumptions, given a subset of topics that represent generalizations of one another (such as baseball → sports → base), for any document which was produced via some topic in this hierarchy, we can efficiently determine the most specialized topic in this subset, it still belongs to. The quality of the classification is independent of the total number of topics in the hierarchy and our algorithm does not need to know the total number of topics in advance. Our approach also yields an algorithm for clustering and unsupervised topical tree reconstruction. We validate our model by showing that properties predicted by our theoretical results carry over to real data. We then apply our clustering algorithm to two different datasets: (i) "20 newsgroups"[19] and (ii) a snapshot of abstracts of arXiv {2} (15 categories, ~240,000 abstracts). In both cases our algorithm performs extremely well.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {580–589},
numpages = {10},
keywords = {hierarchical clustering, mixture models, topical models},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281256,
author = {Sato, Issei and Nakagawa, Hiroshi},
title = {Knowledge Discovery of Multiple-Topic Document Using Parametric Mixture Model with Dirichlet Prior},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281256},
doi = {10.1145/1281192.1281256},
abstract = {Documents, such as those seen on Wikipedia and Folksonomy, have tended to be assigned with multiple topics as a meta-data.Therefore, it is more and more important to analyze a relationship between a document and topics assigned to the document. In this paper, we proposed a novel probabilistic generative model of documents with multiple topics as a meta-data. By focusing on modeling the generation process of a document with multiple topics, we can extract specific properties of documents with multiple topics.Proposed model is an expansion of an existing probabilistic generative model: Parametric Mixture Model (PMM). PMM models documents with multiple topics by mixing model parameters of each single topic. Since, however, PMM assigns the same mixture ratio to each single topic, PMM cannot take into account the bias of each topic within a document. To deal with this problem, we propose a model that considers Dirichlet distribution as a prior distribution of the mixture ratio.We adopt Variational Bayes Method to infer the bias of each topic within a document. We evaluate the proposed model and PMM using MEDLINE corpus.The results of F-measure, Precision and Recall show that the proposed model is more effective than PMM on multiple-topic classification. Moreover, we indicate the potential of the proposed model that extracts topics and document-specific keywords using information about the assigned topics.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {590–598},
numpages = {9},
keywords = {text clustering, dirichlet distribution, variational bayes method, multiple topic, probability model},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281257,
author = {Schickel-Zuber, Vincent and Faltings, Boi},
title = {Using Hierarchical Clustering for Learning Theontologies Used in Recommendation Systems},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281257},
doi = {10.1145/1281192.1281257},
abstract = {Ontologies are being successfully used to overcome semanticheterogeneity, and are becoming fundamental elements of the SemanticWeb. Recently, it has also been shown that ontologies can be used tobuild more accurate and more personalized recommendation systems byinferencing missing user's preferences. However, these systemsassume the existence of ontologies, without considering theirconstruction. With product catalogs changing continuously, newtechniques are required in order to build these ontologies in realtime, and autonomously from any expert intervention.This paper focuses on this problem and show that it is possible tolearn ontologies autonomously by using clustering algorithms. Results on the MovieLens and Jester data sets show that recommendersystem with learnt ontologies significantly outperform the classical recommendation approach.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {599–608},
numpages = {10},
keywords = {ontology, performance, recommendation systems},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281258,
author = {Sculley, D.},
title = {Practical Learning from One-Sided Feedback},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281258},
doi = {10.1145/1281192.1281258},
abstract = {In many data mining applications, online labeling feedback is only available for examples which were predicted to belong to the positive class. Such applications includespam filtering in the case where users never checkemails marked "spam", document retrieval where users cannotgive relevance feedback on unretrieved documents,and online advertising where user behavior cannot beobserved for unshown advertisements. One-sided feedback can cripple the performance of classical mistake-driven online learners such as Perceptron. Previous work under the Apple Tasting framework showed how to transform standard online learners into successful learners from one sided feedback. However, we find in practice that this transformation may request more labels than necessary to achieve strong performance. In this paper,we employ two active learning methods which reduce the number of labels requested in practice. One method is the use of Label Efficient active learning. The other method,somewhat surprisingly, is the use of margin-based learners without modification, which we show combines implicit active learning and a greedy strategy to managing the exploration exploitation tradeoff. Experimental results show that these methods can be significantly more effective in practice than those using the Apple Tasting transformation, even on minority class problems.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {609–618},
numpages = {10},
keywords = {data mining, streaming data, apple tasting, active learning, online learning},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281259,
author = {Shaparenko, Benyah and Joachims, Thorsten},
title = {Information Genealogy: Uncovering the Flow of Ideas in Non-Hyperlinked Document Databases},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281259},
doi = {10.1145/1281192.1281259},
abstract = {We now have incrementally-grown databases of text documents ranging back for over a decade in areas ranging from personal email, to news-articles and conference proceedings. While accessing individual documents is easy, methods for overviewing and understanding these collections as a whole are lacking in number and in scope. In this paper, we address one such global analysis task, namely the problem of automatically uncovering how ideas spread through the collection over time. We refer to this problem as Information Genealogy. In contrast to bibliometric methods that are limited to collections with explicit citation structure, we investigate content-based methods requiring only the text and timestamps of the documents. In particular, we propose a language-modeling approach and a likelihood ratio test to detect influence between documents in a statistically well-founded way. Furthermore, we show how this method can be used to infer citation graphs and to identify the most influential documents in the collection. Experiments on the NIPS conference proceedings and the Physics ArXiv show that our method is more effective than methods based on document similarity.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {619–628},
numpages = {10},
keywords = {text mining, temporal data, flow of ideas, information genealogy, citation inference, language models},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281260,
author = {Shehata, Shady and Karray, Fakhri and Kamel, Mohamed},
title = {A Concept-Based Model for Enhancing Text Categorization},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281260},
doi = {10.1145/1281192.1281260},
abstract = {Most of text categorization techniques are based on word and/or phrase analysis of the text. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes moreto the meaning of its sentences than the other term. Thus, the underlying model should indicate terms that capture these mantics of text. In this case, the model can capture terms that present the concepts of the sentence, which leads todiscover the topic of the document. A new concept-based model that analyzes terms on the sentence and document levels rather than the traditional analysis of document only is introduced. The concept-based model can effectively discriminate between non-important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed model consists of concept-based statistical analyzer, conceptual ontological graph representation,and concept extractor. The term which contributes to the sentence semantics is assigned two different weights by the concept-based statistical analyzer and the conceptual ontological graph representation. These two weights are combined into a new weight. The concepts that have maximum combined weights are selected by the concept extractor. A set of experiments using the proposed concept-basedmodel on different datasets in text categorization is conducted. The experiments demonstrate the comparison between traditional weighting and the concept-based weighting obtained by the combined approach of the concept-based statistical analyzer and the conceptual ontological graph. The evaluation of results is relied on two quality measures, the Macro-averaged F1 and the Error rate. These quality measures are improved when the newly developedconcept-based model is used to enhance the quality of thetext categorization.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {629–637},
numpages = {9},
keywords = {concepts, concept-based categorization, concept-based indexing},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281261,
author = {Sheng, Victor S. and Ling, Charles X.},
title = {Partial Example Acquisition in Cost-Sensitive Learning},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281261},
doi = {10.1145/1281192.1281261},
abstract = {It is often expensive to acquire data in real-world data mining applications. Most previous data mining and machine learning research, however, assumes that a fixed set of training examples is given. In this paper, we propose an online cost-sensitive framework that allows a learner to dynamically acquire examples as it learns, and to decide the ideal number of examples needed to minimize the total cost. We also propose a new strategy for Partial Example Acquisition (PAS), in which the learner can acquire examples with a subset of attribute values to reduce the data acquisition cost. Experiments on UCI datasets show that the new PAS strategy is an effective method in reducing the total cost for data acquisition.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {638–646},
numpages = {9},
keywords = {interactive and online data mining, induction, data acquisition, active learning, cost-sensitive learning, machine learning, active cost-sensitive learning, data mining},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281262,
author = {Shiga, Motoki and Takigawa, Ichigaku and Mamitsuka, Hiroshi},
title = {A Spectral Clustering Approach to Optimally Combining Numericalvectors with a Modular Network},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281262},
doi = {10.1145/1281192.1281262},
abstract = {We address the issue of clustering numerical vectors with a network. The problem setting is basically equivalent to constrained clustering by Wagstaff and Cardie and semi-supervised clustering by Basu et al., but our focus is more on the optimal combination of two heterogeneous data sources. An application of this setting is web pages which can be numerically vectorized by their contents, e.g. term frequencies, and which are hyperlinked to each other, showing a network. Another typical application is genes whose behavior can be numerically measured and a gene network can be given from another data source.We first define a new graph clustering measure which we call normalized network modularity, by balancing the cluster size of the original modularity. We then propose a new clustering method which integrates the cost of clustering numerical vectors with the cost of maximizing the normalized network modularity into a spectral relaxation problem. Our learning algorithm is based on spectral clustering which makes our issue an eigenvalue problem and uses k-means for final cluster assignments. A significant advantage of our method is that we can optimize the weight parameter for balancing the two costs from the given data by choosing the minimum total cost. We evaluated the performance of our proposed method using a variety of datasets including synthetic data as well as real-world data from molecular biology. Experimental results showed that our method is effective enough to have good results for clustering by numerical vectors and a network.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {647–656},
numpages = {10},
keywords = {spectral clustering, k-means, network modularity, eigenvalue problem, heterogeneous data sources},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281263,
author = {Smith, Andrew T. and Elkan, Charles},
title = {Making Generative Classifiers Robust to Selection Bias},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281263},
doi = {10.1145/1281192.1281263},
abstract = {This paper presents approaches to semi-supervised learning when the labeled training data and test data are differently distributed. Specifically, the samples selected for labeling are a biased subset of some general distribution and the test set consists of samples drawn from either that general distribution or the distribution of the unlabeled samples. An example of the former appears in loan application approval, where samples with repay/default labels exist only for approved applicants and the goal is to model the repay/default behavior of all applicants. An example of the latter appears in spam filtering, in which the labeled samples can be out-dated due to the cost of labeling email by hand, but an unlabeled set of up-to-date emails exists and the goal is to build a filter to sort new incoming email.Most approaches to overcoming such bias in the literature rely on the assumption that samples are selected for labeling depending only on the features, not the labels, a case in which provably correct methods exist. The missing labels are said to be "missing at random" (MAR). In real applications, however, the selection bias can be more severe. When the MAR conditional independence assumption is not satisfied and missing labels are said to be "missing not at random" (MNAR), and no learning method is provably always correct.We present a generative classifier, the shifted mixture model (SMM), with separate representations of the distributions of the labeled samples and the unlabeled samples. The SMM makes no conditional independence assumptions and can model distributions of semi-labeled data sets with arbitrary bias in the labeling. We present a learning method based on the expectation maximization (EM) algorithm that, while not always able to overcome arbitrary labeling bias, learns SMMs with higher test-set accuracy in real-world data sets (with MNAR bias) than existing learning methods that are proven to overcome MAR bias.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {657–666},
numpages = {10},
keywords = {sample selection bias, reject inference, semi-supervised learning, generative classifiers},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281264,
author = {Song, Xiuyao and Wu, Mingxi and Jermaine, Christopher and Ranka, Sanjay},
title = {Statistical Change Detection for Multi-Dimensional Data},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281264},
doi = {10.1145/1281192.1281264},
abstract = {This paper deals with detecting change of distribution in multi-dimensional data sets. For a given baseline data set and a set of newly observed data points, we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set. We define a test statistic that is strictly distribution-free under the null hypothesis. Our experimental results show that the density test has substantially more power than the two existing methods for multi-dimensional change detection.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {667–676},
numpages = {10},
keywords = {change detection, kernel density estimation, density test},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281265,
author = {Srihari, Rohini K. and Xu, Li and Saxena, Tushar},
title = {Use of Ranked Cross Document Evidence Trails for Hypothesis Generation},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281265},
doi = {10.1145/1281192.1281265},
abstract = {This paper focuses on detecting how concepts are linked across multiple textdocuments by generating an evidence trail explaining the connection. A traditional search involving, for example, two or more person names willattempt to find documents mentioning both of these individuals. This researchfocuses on a different interpretation of such a query: what is the best evidencetrail across documents that explains a connection between these individuals? For example, allmay be good golfers. A generalization ofthis task involves query terms representing general concepts (e.g. indictment,foreign policy). Such queries reflect a special case oftext mining. Previous attempts to solve this problem have focused on graphapproaches involving hyperlinked documents, and link analysis tools exploiting named entities. A new robust framework is presented, based on (i) generating concept chain graphs, a hybrid content representation, (ii) performing graph matching to select candidate subgraphs, and (iii) subsequently using graphical models to validate hypotheses using ranked evidence trails. We adapt the DUC data set for cross-document summarization to evaluate evidence trails generated by this approach.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {677–686},
numpages = {10},
keywords = {text mining, graph mining, cross document summarization},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281266,
author = {Sun, Jimeng and Faloutsos, Christos and Papadimitriou, Spiros and Yu, Philip S.},
title = {GraphScope: Parameter-Free Mining of Large Time-Evolving Graphs},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281266},
doi = {10.1145/1281192.1281266},
abstract = {How can we find communities in dynamic networks of socialinteractions, such as who calls whom, who emails whom, or who sells to whom? How can we spot discontinuity time-points in such streams of graphs, in an on-line, any-time fashion? We propose GraphScope, that addresses both problems, using information theoretic principles. Contrary to the majority of earlier methods, it needs no user-defined parameters. Moreover, it is designed to operate on large graphs, in a streaming fashion. We demonstrate the efficiency and effectiveness of our GraphScope on real datasets from several diverse domains. In all cases it produces meaningful time-evolving patterns that agree with human intuition.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {687–696},
numpages = {10},
keywords = {graphs, streams, MDL, mining},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281267,
author = {Tandon, Gaurav and Chan, Philip K.},
title = {Weighting versus Pruning in Rule Validation for Detecting Network and Host Anomalies},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281267},
doi = {10.1145/1281192.1281267},
abstract = {For intrusion detection, the LERAD algorithm learns a succinct set of comprehensible rules for detecting anomalies, which could be novel attacks. LERAD validates the learned rules on a separate held-out validation set and removes rules that cause false alarms. However, removing rules with possible high coverage can lead to missed detections. We propose to retain these rules and associate weights to them. We present three weighting schemes and our empirical results indicate that, for LERAD, rule weighting can detect more attacks than pruning with minimal computational overhead.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {697–706},
numpages = {10},
keywords = {machine learning, anomaly detection, rule weighting, rule pruning},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281268,
author = {Tang, Wei and Xiong, Hui and Zhong, Shi and Wu, Jie},
title = {Enhancing Semi-Supervised Clustering: A Feature Projection Perspective},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281268},
doi = {10.1145/1281192.1281268},
abstract = {Semi-supervised clustering employs limited supervision in the form of labeled instances or pairwise instance constraints to aid unsupervised clustering and often significantly improves the clustering performance. Despite the vast amount of expert knowledge spent on this problem, most existing work is not designed for handling high-dimensional sparse data. This paper thus fills this crucial void by developing a Semi-supervised Clustering method based on spheRical K-mEans via fEature projectioN (SCREEN). Specifically, we formulate the problem of constraint-guided feature projection, which can be nicely integrated with semi-supervised clustering algorithms and has the ability to effectively reduce data dimension. Indeed, our experimental results on several real-world data sets show that the SCREEN method can effectively deal with high-dimensional data and provides an appealing clustering performance.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {707–716},
numpages = {10},
keywords = {semi-supervised clustering, pairwise instance constraints, feature projection},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281269,
author = {Tantipathananandh, Chayant and Berger-Wolf, Tanya and Kempe, David},
title = {A Framework for Community Identification in Dynamic Social Networks},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281269},
doi = {10.1145/1281192.1281269},
abstract = {We propose frameworks and algorithms for identifying communities in social networks that change over time. Communities are intuitively characterized as "unusually densely knit" subsets of a social network. This notion becomes more problematic if the social interactions change over time. Aggregating social networks over time can radically misrepresent the existing and changing community structure. Instead, we propose an optimization-based approach for modeling dynamic community structure. We prove that finding the most explanatory community structure is NP-hard and APX-hard, and propose algorithms based on dynamic programming, exhaustive search, maximum matching, and greedy heuristics. We demonstrate empirically that the heuristics trace developments of community structure accurately for several synthetic and real-world examples.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {717–726},
numpages = {10},
keywords = {community identification, dynamic social networks},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281270,
author = {Teo, Choon Hui and Smola, Alex and Vishwanathan, S. V.N. and Le, Quoc Viet},
title = {A Scalable Modular Convex Solver for Regularized Risk Minimization},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281270},
doi = {10.1145/1281192.1281270},
abstract = {A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as l1 and l2 penalties. At present, our solver implements 20 different estimation problems, can be easily extended, scales to millions of observations, and is up to 10 times faster than specialized solvers for many applications. The open source code is freely available as part of the ELEFANT toolbox.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {727–736},
numpages = {10},
keywords = {optimization, algorithms, convexity},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281271,
author = {Tong, Hanghang and Faloutsos, Christos and Gallagher, Brian and Eliassi-Rad, Tina},
title = {Fast Best-Effort Pattern Matching in Large Attributed Graphs},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281271},
doi = {10.1145/1281192.1281271},
abstract = {We focus on large graphs where nodes have attributes, such as a social network where the nodes are labelled with each person's job title. In such a setting, we want to find subgraphs that match a user query pattern. For example, a "star" query would be, "find a CEO who has strong interactions with a Manager, a Lawyer,and an Accountant, or another structure as close to that as possible". Similarly, a "loop" query could help spot a money laundering ring.Traditional SQL-based methods, as well as more recent graph indexing methods, will return no answer when an exact match does not exist. This is the first main feature of our method. It can find exact-, as well as near-matches, and it will present them to the user in our proposed "goodness" order. For example, our method tolerates indirect paths between, say, the "CEO" and the "Accountant" of the above sample query, when direct paths don't exist. Its second feature is scalability. In general, if the query has nq nodes and the data graph has n nodes, the problem needs polynomial time complexity O(n n q), which is prohibitive. Our G-Ray ("Graph X-Ray") method finds high-quality subgraphs in time linear on the size of the data graph.Experimental results on the DLBP author-publication graph (with 356K nodes and 1.9M edges) illustrate both the effectiveness and scalability of our approach. The results agree with our intuition, and the speed is excellent. It takes 4 seconds on average fora 4-node query on the DBLP graph.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {737–746},
numpages = {10},
keywords = {random walk, pattern match, attributed graph},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281272,
author = {Tong, Hanghang and Faloutsos, Christos and Koren, Yehuda},
title = {Fast Direction-Aware Proximity for Graph Mining},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281272},
doi = {10.1145/1281192.1281272},
abstract = {In this paper we study asymmetric proximity measures on directed graphs, which quantify the relationships between two nodes or two groups of nodes. The measures are useful in several graph mining tasks, including clustering, link prediction and connection subgraph discovery. Our proximity measure is based on the conceptof escape probability. This way, we strive to summarize the multiple facets of nodes-proximity, while avoiding some of the pitfalls to which alternative proximity measures are susceptible. A unique feature of the measures is accounting for the underlying directional information. We put a special emphasis on computational efficiency, and develop fast solutions that are applicable in several settings. Our experimental study shows the usefulness of our proposed direction-aware proximity method for several applications, and that our algorithms achieve a significant speedup (up to 50,000x) over straight forward implementations.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {747–756},
numpages = {10},
keywords = {proximity, graph mining, random walk},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281273,
author = {Vogel, David S. and Asparouhov, Ognian and Scheffer, Tobias},
title = {Scalable Look-Ahead Linear Regression Trees},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281273},
doi = {10.1145/1281192.1281273},
abstract = {Most decision tree algorithms base their splitting decisions on a piecewise constant model. Often these splitting algorithms are extrapolated to trees with non-constant models at the leaf nodes. The motivation behind Look-ahead Linear Regression Trees (LLRT) is that out of all the methods proposed to date, there has been no scalable approach to exhaustively evaluate all possible models in the leaf nodes in order to obtain an optimal split. Using several optimizations, LLRT is able to generate and evaluate thousands of linear regression models per second. This allows for a near-exhaustive evaluation of all possible splits in a node, based on the quality of fit of linear regression models in the resulting branches. We decompose the calculation of the Residual Sum of Squares in such a way that a large part of it is pre-computed. The resulting method is highly scalable. We observe it to obtain high predictive accuracy for problems with strong mutual dependencies between attributes. We report on experiments with two simulated and seven real data sets.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {757–764},
numpages = {8},
keywords = {model tree, scalable algorithms, linear regression tree, regression, predictive model},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281274,
author = {Vreeken, Jilles and van Leeuwen, Matthijs and Siebes, Arno},
title = {Characterising the Difference},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281274},
doi = {10.1145/1281192.1281274},
abstract = {Characterising the differences between two databases is an often occurring problem in Data Mining. Detection of change over time is a prime example, comparing databases from two branches is another one. The key problem is to discover the patterns that describe the difference. Emerging patterns provide only a partial answer to this question.In previous work, we showed that the data distribution can be captured in a pattern-based model using compression [12]. Here, we extend this approach to define a generic dissimilarity measure on databases. Moreover, we show that this approach can identify those patterns that characterise the differences between two distributions.Experimental results show that our method provides a well-founded way to independently measure database dissimilarity that allows for thorough inspection of the actual differences. This illustrates the use of our approach in real world data mining.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {765–774},
numpages = {10},
keywords = {compression, temporal data mining, database dissimilarity},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281275,
author = {Wan, Li and Ng, Wee Keong and Han, Shuguo and Lee, Vincent C. S.},
title = {Privacy-Preservation for Gradient Descent Methods},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281275},
doi = {10.1145/1281192.1281275},
abstract = {Gradient descent is a widely used paradigm for solving many optimization problems. Stochastic gradient descent performs a series of iterations to minimize a target function in order to reach a local minimum. In machine learning or data mining, this function corresponds to a decision model that is to be discovered. The gradient descent paradigm underlies many commonly used techniques in data mining and machine learning, such as neural networks, Bayesian networks, genetic algorithms, and simulated annealing. To the best of our knowledge, there has not been any work that extends the notion of privacy preservation or secure multi-party computation to gradient-descent-based techniques. In this paper, we propose a preliminary approach to enable privacy preservation in gradient descent methods in general and demonstrate its feasibility in specific gradient descent methods.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {775–783},
numpages = {9},
keywords = {privacy preservation, secure multi-party computation, gradient descent method, regression},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281276,
author = {Wang, Xuanhui and Zhai, ChengXiang and Hu, Xiao and Sproat, Richard},
title = {Mining Correlated Bursty Topic Patterns from Coordinated Text Streams},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281276},
doi = {10.1145/1281192.1281276},
abstract = {Previous work on text mining has almost exclusively focused on a single stream. However, we often have available multiple text streams indexed by the same set of time points (called coordinated text streams), which offer new opportunities for text mining. For example, when a major event happens, all the news articles published by different agencies in different languages tend to cover the same event for a certain period, exhibiting a correlated bursty topic pattern in all the news article streams. In general, mining correlated bursty topic patterns from coordinated text streams can reveal interesting latent associations or events behind these streams. In this paper, we define and study this novel text mining problem. We propose a general probabilistic algorithm which can effectively discover correlated bursty patterns and their bursty periods across text streams even if the streams have completely different vocabularies (e.g., English vs Chinese). Evaluation of the proposed method on a news data set and a literature data set shows that it can effectively discover quite meaningful topic patterns from both data sets: the patterns discovered from the news data set accurately reveal the major common events covered in the two streams of news articles (in English and Chinese, respectively), while the patterns discovered from two database publication streams match well with the major research paradigm shifts in database research. Since the proposed method is general and does not require the streams to share vocabulary, it can be applied to any coordinated text streams to discover correlated topic patterns that burst in multiple streams in the same period.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {784–793},
numpages = {10},
keywords = {clustering, correlated bursty patterns, coordinated streams, reinforcement},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281277,
author = {Wang, Xuerui and Pal, Chris and McCallum, Andrew},
title = {Generalized Component Analysis for Text with Heterogeneous Attributes},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281277},
doi = {10.1145/1281192.1281277},
abstract = {We present a class of richly structured, undirected hidden variable models suitable for simultaneously modeling text along with other attributes encoded in different modalities. Our model generalizes techniques such as principal component analysis to heterogeneous data types. In contrast to other approaches, this framework allows modalities such as words, authors and timestamps to be captured in their natural, probabilistic encodings. A latent space representation for a previously unseen document can be obtained through a fast matrix multiplication using our method. We demonstrate the effectiveness of our framework on the task of author prediction from 13 years of the NIPS conference proceedings and for a recipient prediction task using a 10-month academic email archive of a researcher. Our approach should be more broadly applicable to many real-world applications where one wishes to efficiently make predictions for a large number of potential outputs using dimensionality reduction in a well defined probabilistic framework.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {794–803},
numpages = {10},
keywords = {text mining, undirected graphical models, recipient prediction, topic modeling, multimodal heterogeneous data, author prediction},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281278,
author = {Wong, Raymond Chi-Wing and Pei, Jian and Fu, Ada Wai-Chee and Wang, Ke},
title = {Mining Favorable Facets},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281278},
doi = {10.1145/1281192.1281278},
abstract = {The importance of dominance and skyline analysis has been well recognized in multi-criteria decision making applications. Most previous studies assume a fixed order on the attributes. In practice, different customers may have different preferences on nominal attributes. In this paper, we identify an interesting data mining problem, finding favorable facets, which has not been studied before. Given a set of points in a multidimensional space, for a specific target point p we want to discover with respect to which combinations of orders (e.g., customer preferences) on the nominal attributes p is not dominated by any other points. Such combinations are called the favorable facets of p.We consider both the effectiveness and the efficiency of the mining. A given point may have many favorable facets. We propose the notion of minimal disqualifying condition (MDC) which is effective in summarizing favorable facets. We develop efficient algorithms for favorable facet mining for different application scenarios. The first method computes favorable facets on the fly. The second method pre-computes all minimal disqualifying conditions so that the favorable facets can be looked up in constant time. An extensive performance study using both synthetic and real data sets is reported to verify their effectiveness and efficiency.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {804–813},
numpages = {10},
keywords = {skyline, preference, warehouse, materialization, spatial data},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281279,
author = {Wu, Junjie and Xiong, Hui and Wu, Peng and Chen, Jian},
title = {Local Decomposition for Rare Class Analysis},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281279},
doi = {10.1145/1281192.1281279},
abstract = {Given its importance, the problem of predicting rare classes in large-scale multi-labeled data sets has attracted great attentions in the literature. However, the rare-class problem remains a critical challenge, because there is no natural way developed for handling imbalanced class distributions. This paper thus fills this crucial void by developing a method for Classification using lOcal clusterinG (COG). Specifically, for a data set with an imbalanced class distribution, we perform clustering within each large class and produce sub-classes with relatively balanced sizes. Then, we apply traditional supervised learning algorithms, such as Support Vector Machines (SVMs), for classification. Indeed, our experimental results on various real-world data sets show that our method produces significantly higher prediction accuracies on rare classes than state-of-the-art methods. Furthermore, we show that COG can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {814–823},
numpages = {10},
keywords = {k-means clustering support vector machines, rare class analysis, local clustering},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281280,
author = {Xu, Xiaowei and Yuruk, Nurcan and Feng, Zhidan and Schweiger, Thomas A. J.},
title = {SCAN: A Structural Clustering Algorithm for Networks},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281280},
doi = {10.1145/1281192.1281280},
abstract = {Network clustering (or graph partitioning) is an important task for the discovery of underlying structures in networks. Many algorithms find clusters by maximizing the number of intra-cluster edges. While such algorithms find useful and interesting structures, they tend to fail to identify and isolate two kinds of vertices that play special roles - vertices that bridge clusters (hubs) and vertices that are marginally connected to clusters (outliers). Identifying hubs is useful for applications such as viral marketing and epidemiology since hubs are responsible for spreading ideas or disease. In contrast, outliers have little or no influence, and may be isolated as noise in the data. In this paper, we proposed a novel algorithm called SCAN (Structural Clustering Algorithm for Networks), which detects clusters, hubs and outliers in networks. It clusters vertices based on a structural similarity measure. The algorithm is fast and efficient, visiting each vertex only once. An empirical evaluation of the method using both synthetic and real datasets demonstrates superior performance over other methods such as the modularity-based algorithms.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {824–833},
numpages = {10},
keywords = {outliers, hubs, graph partitioning, network clustering, community Structure},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281281,
author = {Yan, Rong and Tesic, Jelena and Smith, John R.},
title = {Model-Shared Subspace Boosting for Multi-Label Classification},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281281},
doi = {10.1145/1281192.1281281},
abstract = {Typical approaches to the multi-label classification problem require learning an independent classifier for every label from all the examples and features. This can become a computational bottleneck for sizeable datasets with a large label space. In this paper, we propose an efficient and effective multi-label learning algorithm called model-shared subspace boosting (MSSBoost) as an attempt to reduce the information redundancy in the learning process. This algorithm automatically finds, shares and combines a number of base models across multiple labels, where each model is learned from random feature subspace and boots trap data samples. The decision functions for each label are jointly estimated and thus a small number of shared subspace models can support the entire label space. Our experimental results on both synthetic data and real multimedia collections have demonstrated that the proposed algorithm can achieve better classification performance than the non-ensemble baselineclassifiers with a significant speedup in the learning and prediction processes. It can also use a smaller number of base models to achieve the same classification performance as its non-model-shared counterpart.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {834–843},
numpages = {10},
keywords = {multi-label classification, random subspace methods},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281282,
author = {Yankov, Dragomir and Keogh, Eamonn and Medina, Jose and Chiu, Bill and Zordan, Victor},
title = {Detecting Time Series Motifs under Uniform Scaling},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281282},
doi = {10.1145/1281192.1281282},
abstract = {Time series motifs are approximately repeated patterns foundwithin the data. Such motifs have utility for many data mining algorithms, including rule-discovery,novelty-detection, summarization and clustering. Since the formalization of the problem and the introduction of efficient linear time algorithms, motif discovery has been successfully applied tomany domains, including medicine, motion capture, robotics and meteorology.In this work we show that most previous applications of time series motifs have been severely limited by the definition's brittleness to even slight changes of uniform scaling, the speed at which the patterns develop. We introduce a new algorithm that allows discovery of time series motifs with invariance to uniform scaling, and show that it produces objectively superior results in several important domains. Apart from being more general than all other motifdiscovery algorithms, a further contribution of our work isthat it is simpler than previous approaches, in particular we have drastically reduced the number of parameters that need to be specified.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {844–853},
numpages = {10},
keywords = {motifs, random projection, time series, uniform scaling},
location = {San Jose, California, USA},
series = {KDD '07}
}

@dataset{10.1145/review-1281192.1281282_R43196,
author = {Papadopoulos, Apostolos N},
title = {Review ID:R43196 for DOI: 10.1145/1281192.1281282},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1281192.1281282_R43196}
}

@inproceedings{10.1145/1281192.1281283,
author = {Ye, Jieping and Ji, Shuiwang and Chen, Jianhui},
title = {Learning the Kernel Matrix in Discriminant Analysis via Quadratically Constrained Quadratic Programming},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281283},
doi = {10.1145/1281192.1281283},
abstract = {The kernel function plays a central role in kernel methods. In this paper, we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in Regularized Kernel Discriminant Analysis (RKDA), which performs lineardiscriminant analysis in the feature space via the kernel trick. Previous studies have shown that this kernel learning problem can be formulated as a semidefinite program (SDP), which is however computationally expensive, even with the recent advances in interior point methods. Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a Quadratically Constrained Quadratic Programming (QCQP) formulation for the kernel learning problem, which can be solved more efficiently than SDP. While most existing work on kernel learning deal with binary-class problems only, we show that our QCQP formulation can be extended naturally to the multi-class case. Experimental results on both binary-class and multi-class benchmarkdata sets show the efficacy of the proposed QCQP formulations.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {854–863},
numpages = {10},
keywords = {kernel learning, convex optimization, quadratically constrained quadratic programming, kernel discriminant analysis, model selection},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281284,
author = {Yuan, Junsong and Wu, Ying and Yang, Ming},
title = {From Frequent Itemsets to Semantically Meaningful Visual Patterns},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281284},
doi = {10.1145/1281192.1281284},
abstract = {Data mining techniques that are successful in transaction and text data may not be simply applied to image data that contain high-dimensional features and have spatial structures. It is not a trivial task to discover meaningful visual patterns in image databases, because the content variations and spatial dependency in the visual data greatly challenge most existing methods. This paper presents a novel approach to coping with these difficulties for mining meaningful visual patterns. Specifically, the novelty of this work lies in the following new contributions: (1) a principled solution to the discovery of meaningful itemsets based on frequent itemset mining; (2) a self-supervised clustering scheme of the high-dimensional visual features by feeding back discovered patterns to tune the similarity measure through metric learning; and (3) a pattern summarization method that deals with the measurement noises brought by the image data. The experimental results in the real images show that our method can discover semantically meaningful patterns efficiently and effectively.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {864–873},
numpages = {10},
keywords = {self-supervised clustering, image data mining, meaningful itemset mining, pattern summarization},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281285,
author = {Zhang, Xian and Hao, Yu and Zhu, Xiaoyan and Li, Ming and Cheriton, David R.},
title = {Information Distance from a Question to an Answer},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281285},
doi = {10.1145/1281192.1281285},
abstract = {We provide three key missing pieces of a general theory of information distance [3, 23, 24]. We take bold steps in formulating a revised theory to avoid some pitfalls in practical applications. The new theory is then used to construct a question answering system. Extensive experiments are conducted to justify the new theory.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {874–883},
numpages = {10},
keywords = {normalized information distance, information distance, question answering system},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281286,
author = {Zhao, Hongkun and Meng, Weiyi and Yu, Clement},
title = {Mining Templates from Search Result Records of Search Engines},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281286},
doi = {10.1145/1281192.1281286},
abstract = {Metasearch engine, Comparison-shopping and Deep Web crawling applications need to extract search result records enwrapped in result pages returned from search engines in response to user queries. The search result records from a given search engine are usually formatted based on a template. Precisely identifying this template can greatly help extract and annotate the data units within each record correctly. In this paper, we propose a graph model to represent record template and develop a domain independent statistical method to automatically mine the record template for any search engine using sample search result records. Our approach can identify both template tags (HTML tags) and template texts (non-tag texts), and it also explicitly addresses the mismatches between the tag structures and the data structures of search result records. Our experimental results indicate that this approach is very effective.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {884–893},
numpages = {10},
keywords = {wrapper generation, information extraction, search engine},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281287,
author = {Zheng, Shuyi and Song, Ruihua and Wen, Ji-Rong and Wu, Di},
title = {Joint Optimization of Wrapper Generation and Template Detection},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281287},
doi = {10.1145/1281192.1281287},
abstract = {Many websites have large collections of pages generated dynamically from an underlying structured source like a database. The data of a category are typically encoded into similar pages by a common script or template. In recent years, some value-added services, such as comparison shopping and vertical search in a specific domain, have motivated the research of extraction technologies with high accuracy. Almost all previous works assume that input pages of a wrapper induction system conform to a common template and they can be easily identified in terms of a common schema of URL. However, we observed that it is hard to distinguish different templates using dynamic URLs today. Moreover, since extraction accuracy heavily depends on how consistent input pages are, we argue that it is risky to determine whether pages share a common template solely based on URLs. Instead, we propose a new approach that utilizes similarity between pages to detect templates. Our approach separates pages with notable inner differences and then generates wrappers, respectively. Experimental results show that our proposed approach is feasible and effective for improving extraction accuracy.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {894–902},
numpages = {9},
keywords = {template detection, wrapper, information extraction},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281288,
author = {Zhu, Jun and Zhang, Bo and Nie, Zaiqing and Wen, Ji-Rong and Hon, Hsiao-Wuen},
title = {Webpage Understanding: An Integrated Approach},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281288},
doi = {10.1145/1281192.1281288},
abstract = {Recent work has shown the effectiveness of leveraging layout and tag-tree structure for segmenting webpages and labeling HTML elements. However, how to effectively segment and label the text contents inside HTML elements is still an open problem. Since many text contents on a webpage are often text fragments and not strictly grammatical, traditional natural language processing techniques, that typically expect grammatical sentences, are no longer directly applicable. In this paper, we examine how to use layout and tag-tree structure in a principled way to help understand text contents on webpages. We propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model. In this model, semantic labels of page structure can be leveraged to help text content understanding, and semantic labels ofthe text phrases can be used in page structure understanding tasks such as data record detection. Thus, integration of both page structure and text content understanding leads to an integrated solution of webpage understanding. Experimental results on research homepage extraction show the feasibility and promise of our approach.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {903–912},
numpages = {10},
keywords = {text processing, conditional random fields, webpage understanding},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281290,
author = {Asur, Sitaram and Parthasarathy, Srinivasan and Ucar, Duygu},
title = {An Event-Based Framework for Characterizing the Evolutionary Behavior of Interaction Graphs},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281290},
doi = {10.1145/1281192.1281290},
abstract = {Interaction graphs are ubiquitous in many fields such as bioinformatics, sociology and physical sciences. There have been many studies in the literature targeted at studying and mining these graphs. However, almost all of them have studied these graphs from a static point of view. The study of the evolution of these graphs over time can provide tremendous insight on the behavior of entities, communities and the flow of information among them. In this work, we present an event-based characterization of critical behavioral patterns for temporally varying interaction graphs. We use non-overlapping snapshots of interaction graphs and develop a framework for capturing and identifying interesting events from them. We use these events to characterize complex behavioral patterns of individuals and communities over time. We demonstrate the application of behavioral patterns for the purposes of modeling evolution, link prediction and influence maximization. Finally, we present a diffusion model for evolving networks, based on our framework.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {913–921},
numpages = {9},
keywords = {diffusion of innovations, evolutionary analysis, interaction networks},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281291,
author = {Castano, Rebecca and Wagstaff, Kiri L. and Chien, Steve and Stough, Timothy M. and Tang, Benyang},
title = {On-Board Analysis of Uncalibrated Data for a Spacecraft at Mars},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281291},
doi = {10.1145/1281192.1281291},
abstract = {Analyzing data on-board a spacecraft as it is collected enables several advanced spacecraft capabilities, such as prioritizing observations to make the best use of limited bandwidth and reacting to dynamic events as they happen. In this paper, we describe how we addressed the unique challenges associated with on-board mining of data as it is collected: uncalibrated data, noisy observations, and severe limitations on computational and memory resources. The goal of this effort, which falls into the emerging application area of spacecraft-based data mining, was to study three specific science phenomena on Mars. Following previous work that used a linear support vector machine (SVM) on-board the Earth Observing 1 (EO-1)spacecraft, we developed three data mining techniques for use on-board the Mars Odyssey spacecraft. These methods range from simple thresholding to state-of-the-art reduced-set SVM technology. We tested these algorithms on archived data in a flight software testbed. We also describe a significant, serendipitous science discovery of this data mining effort: the confirmation of a water ice annulus around the north polar cap of Mars. We conclude with a discussion on lessons learned in developing algorithms for use on-board a spacecraft.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {922–930},
numpages = {9},
keywords = {on-board data mining, real-time data analysis, lessons learned, resource-constrained computing},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281292,
author = {Davitz, Jeffrey and Yu, Jiye and Basu, Sugato and Gutelius, David and Harris, Alexandra},
title = {<span class="smallcaps SmallerCapital">iLink</span>: Search and Routing in Social Networks},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281292},
doi = {10.1145/1281192.1281292},
abstract = {The growth of Web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks. This paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration. We propose a general interaction model for the underlying social networks and then a specific model (iLink for social search and message routing. A key contribution here is the development of a general learning framework for making such online peer production systems work at scale. The iLink model has been used to develop a system for FAQ generation in a social network (FAQtory), and experience with its application in the context of a full-scale learning-driven workflow application (CALO) is reported. We also discuss methods of adapting iLink technology for use in military knowledge sharing portals and other message routing systems. Finally, the paper shows the connection of iLink to SQM, a theoretical model for social search that is a generalization of Markov Decision Processes and the popular Pagerank model.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {931–940},
numpages = {10},
keywords = {message routing, learning, smart RSS, peer production, social search, expert identification, social FAQ generation},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281293,
author = {Fast, Andrew and Friedland, Lisa and Maier, Marc and Taylor, Brian and Jensen, David and Goldberg, Henry G. and Komoroske, John},
title = {Relational Data Pre-Processing Techniques for Improved Securities Fraud Detection},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281293},
doi = {10.1145/1281192.1281293},
abstract = {Commercial datasets are often large, relational, and dynamic. They contain many records of people, places, things, events and their interactions over time. Such datasets are rarely structured appropriately for knowledge discovery, and they often contain variables whose meanings change across different subsets of the data. We describe how these challenges were addressed in a collaborative analysis project undertaken by the University of Massachusetts Amherst and the National Association of Securities Dealers(NASD). We describe several methods for data pre-processing that we applied to transform a large, dynamic, and relational dataset describing nearly the entirety of the U.S. securities industry, and we show how these methods made the dataset suitable for learning statistical relational models. To better utilize social structure, we first applied known consolidation and link formation techniques to associate individuals with branch office locations. In addition, we developed an innovative technique to infer professional associations by exploiting dynamic employment histories. Finally, we applied normalization techniques to create a suitable class label that adjusts for spatial, temporal, and other heterogeneity within the data. We show how these pre-processing techniques combine to provide the necessary foundation for learning high-performing statistical models of fraudulent activity.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {941–949},
numpages = {9},
keywords = {relational probability trees, fraud detection, normalization, data pre-processing, statistical relational learning},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281294,
author = {Hua, Ming and Pei, Jian},
title = {Cleaning Disguised Missing Data: A Heuristic Approach},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281294},
doi = {10.1145/1281192.1281294},
abstract = {In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely, such as causing significant biases and misleading results in hypothesis tests, correlation analysis and regressions. The very limited previous studies on cleaning disguised missing data use outlier mining and distribution anomaly detection. They highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers.To tackle the problem of cleaning disguised missing data, in this paper, we first model the distribution of disguised missing data, and propose the embedded unbiased sample heuristic. Then, we develop an effective and efficient method to identify the frequently used disguise values which capture the major body of the disguised missing data. Our method does not require any domain background knowledge to find the suspicious disguise values. We report an empirical evaluation using real data sets, which shows that our method is effective - the frequently used disguise values found by our method match the values identified by the domain experts nicely. Our method is also efficient and scalable for processing large data sets.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {950–958},
numpages = {9},
keywords = {data cleaning, disguised missing data, data quality},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281295,
author = {Kohavi, Ron and Henne, Randal M. and Sommerfield, Dan},
title = {Practical Guide to Controlled Experiments on the Web: Listen to Your Customers Not to the Hippo},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281295},
doi = {10.1145/1281192.1281295},
abstract = {The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments, also called randomized experiments (single factor or factorial designs), A/B tests (and their generalizations), split tests, Control/Treatment tests, and parallel flights. Controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior. We provide a practical guide to conducting online experiments, where end-users can help guide the development of features. Our experience indicates that significant learning and return-on-investment (ROI) are seen when development teams listen to their customers, not to the Highest Paid Person's Opinion (HiPPO). We provide several examples of controlled experiments with surprising results. We review the important ingredients of running controlled experiments, and discuss their limitations (both technical and organizational). We focus on several areas that are critical to experimentation, including statistical power, sample size, and techniques for variance reduction. We describe common architectures for experimentation systems and analyze their advantages and disadvantages. We evaluate randomization and hashing techniques, which we show are not as simple in practice as is often assumed. Controlled experiments typically generate large amounts of data, which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest, leading to new hypotheses and creating a virtuous cycle of improvements. Organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses. Based on our extensive practical experience with multiple systems and organizations, we share key lessons that will help practitioners in running trustworthy controlled experiments.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {959–967},
numpages = {9},
keywords = {A/B testing, e-commerce, controlled experiments},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281296,
author = {Luo, Ping and Xiong, Hui and L\"{u}, Kevin and Shi, Zhongzhi},
title = {Distributed Classification in Peer-to-Peer Networks},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281296},
doi = {10.1145/1281192.1281296},
abstract = {This work studies the problem of distributed classification in peer-to-peer(P2P) networks. While there has been a significant amount of work in distributed classification, most of existing algorithms are not designed for P2P networks. Indeed, as server-less and router-less systems, P2P networks impose several challenges for distributed classification: (1) it is not practical to have global synchronization in large-scale P2P networks; (2)there are frequent topology changes caused by frequent failure and recovery of peers; and (3) there are frequent on-the-fly data updates on each peer.In this paper, we propose an ensemble paradigm for distributed classification in P2P networks. Under this paradigm, each peer builds its local classifiers on the local data and the results from all local classifiers are then combined by plurality voting. To build local classifiers, we adopt the learning algorithm of pasting bites to generate multiple local classifierson each peer based on the local data. To combine local results, we propose a general form of Distributed Plurality Voting (DPV) protocol in dynamic P2P networks. This protocol keeps the single-site validity for dynamic networks, and supports the computing modes of both one-shot query and continuous monitoring. We theoretically prove that the condition (BOB CHECK THIS 'C')ω0 for sending messages used in DPV0 is locally communication-optimal to achieve the above properties. Finally, experimental results on real-world P2P networks show that: (1) the proposed ensemble paradigm is effective even if there are thousands of local classifiers; (2) in most cases, the DPV0 algorithm is local in the sense that voting is processed using information gathered from a very small vicinity, whose size is independent of the network size; (3) DPV0 is significantly more communication-efficient than existing algorithms for distributed plurality voting.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {968–976},
numpages = {9},
keywords = {P2P networks, distributed classification, distributed plurality voting},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281297,
author = {Perlich, Claudia and Rosset, Saharon and Lawrence, Richard D. and Zadrozny, Bianca},
title = {High-Quantile Modeling for Customer Wallet Estimation and Other Applications},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281297},
doi = {10.1145/1281192.1281297},
abstract = {In this paper we discuss the important practical problem of customer wallet estimation, i.e., estimation of potential spending by customers(rather than their expected spending). For this purpose we utilize quantile modeling, whose goal is to estimate a quantile of the discriminative conditional distribution of the response, rather than the mean, which is the implicit goal of most standard regression approaches. We argue that a notion of wallet can be captured through high quantile modeling (e.g, estimating the 90th percentile), and describe a wallet estimation implementation within IBM's Market Alignment Program (MAP). We also discuss the wide range of domains where high-quantile modeling can be practically important: estimating opportunities in sales and marketing domains, defining 'surprising' patterns for outlier and fraud detection and more. We survey some existing approaches for quantile modeling, and propose adaptations of nearest-neighbor and regression-tree approaches to quantile modeling. We demonstrate the various models' performance in high quantile estimation in several domains, including our motivating problem of estimating the 'realistic' IT wallets of IBM customers.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {977–985},
numpages = {9},
keywords = {quantile estimation, data mining, customer wallet estimation},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281298,
author = {Zhao, Jun Hua and Dong, Zhao Yang and Zhang, Pei},
title = {Mining Complex Power Networks for Blackout Prevention},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281298},
doi = {10.1145/1281192.1281298},
abstract = {Following the recent devastating blackouts in North America, UK and Italy, blackout prevention has attracted significant attention, though it is known as a notoriously difficult task. To prevent the blackout, it is essential to accurately predict the instable status of power network components. In the large-scale power network however, existing analysis tools fail to perform accurate and in-time prediction of component instability, because of the sophisticated structure of real-world power networks and the huge amount of system variables to be analyzed. To prevent the blackout, we need an accurate and efficient method that (a) can discover interesting features and patterns relevant to the blackout, from the highly complex structure and ten thousands of system variables of a power network, and (b) can give accurate and fast prediction of system instability whenever required, so that the network operator can take necessary actions in time. In this paper, we report our tool developed for power network instability prediction. The proposed method consists of two major stages. In the first stage,a novel type of patterns namely Local Correlation Network Pattern (LCNP) is mined from the structure and system variables of the power network. Correlation rules, which are useful for the network operator to locate potentially instable components, can be further generated from the LCNP. In the second stage, a kernel based network classification method is developed to predict the system instability. By testing on a real world power network (the New England system), we demonstrate that the proposed tool is effective in predicting system instability and thus highly useful for blackout prevention.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {986–994},
numpages = {9},
keywords = {graph mining, power networks, blackout prevention},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281299,
author = {Zhao, Shubin and Betz, Jonathan},
title = {Corroborate and Learn Facts from the Web},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281299},
doi = {10.1145/1281192.1281299},
abstract = {The web contains lots of interesting factual information about entities, such as celebrities, movies or products. This paper describes a robust bootstrapping approach to corroborate facts and learn more facts simultaneously. This approach starts with retrieving relevant pages from a crawl repository for each entity in the seed set. In each learning cycle, known facts of an entity are corroborated first in a relevant page to find fact mentions. When fact mentions are found, they are taken as examples for learning new facts from the page via HTML pattern discovery. Extracted new facts are added to the known fact set for the next learning cycle. The bootstrapping process continues until no new facts can be learned. This approach is language-independent. It demonstrated good performance in experiment on country facts. Results of a large scale experiment will also be shown with initial facts imported from wikipedia.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {995–1003},
numpages = {9},
keywords = {web mining, bootstrapping, information extraction},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281300,
author = {Zhu, Guangyu and Bethea, Timothy J. and Krishna, Vikas},
title = {Extracting Relevant Named Entities for Automated Expense Reimbursement},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281300},
doi = {10.1145/1281192.1281300},
abstract = {Expense reimbursement is a time-consuming and labor-intensive process across organizations. In this paper, we present a prototype expense reimbursement system that dramatically reduces the elapsed time and costs involved, by eliminating paper from the process life cycle. Our complete solution involves (1) an electronic submission infrastructure that provides multi- channel image capture, secure transport and centralized storage of paper documents; (2) an unconstrained data mining approach to extracting relevant named entities from un-structured document images; (3) automation of auditing procedures that enables automatic expense validation with minimum human interaction.Extracting relevant named entities robustly from document images with unconstrained layouts and diverse formatting is a fundamental technical challenge to image-based data mining, question answering, and other information retrieval tasks. In many applications that require such capability, applying traditional language modeling techniques to the stream of OCR text does not give satisfactory result due to the absence of linguistic context. We present an approach for extracting relevant named entities from document images by combining rich page layout features in the image space with language content in the OCR text using a discriminative conditional random field (CRF) framework. We integrate this named entity extraction engine into our expense reimbursement solution and evaluate the system performance on large collections of real-world receipt images provided by IBM World Wide Reimbursement Center.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1004–1012},
numpages = {9},
keywords = {conditional random fields, document layout analysis, learning, named entity extraction},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281302,
author = {Aggarwal, Charu C.},
title = {A Framework for Classification and Segmentation of Massive Audio Data Streams},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281302},
doi = {10.1145/1281192.1281302},
abstract = {In recent years, the proliferation of VOIP data has created a number of applications in which it is desirable to perform quick online classification and recognition of massive voice streams. Typically such applications are encountered in real time intelligence and surveillance. In many cases, the data streams can be in compressed format, and the rate of data processing can often run at the rate of Gigabits per second. All known techniques for speaker voice analysis require the use of an offline training phase in which the system is trained with known segments of speech. The state-of-the-art method for text-independent speaker recognition is known as Gaussian Mixture Modeling (GMM), and it requires an iterative Expectation Maximization Procedure for training, which cannot be implemented in real time. In this paper, we discuss the details of such an online voice recognition system. For this purpose, we use our micro-clustering algorithms to design concise signatures of the target speakers. One of the surprising and insightful observations from our experiences with such a system is that while it was originally designed only for efficiency, we later discovered that it was also more accurate than the widely used Gaussian Mixture Model (GMM). This was because of the conciseness of the micro-cluster model, which made it less prone to over training. This is evidence of the fact that it is often possible to get the best of both worlds and do better than complex models both from an efficiency and accuracy perspective.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1013–1017},
numpages = {5},
keywords = {speaker recognition},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281303,
author = {Curry, Chris and Grossman, Robert L. and Locke, David and Vejcik, Steve and Bugajski, Joseph},
title = {Detecting Changes in Large Data Sets of Payment Card Data: A Case Study},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281303},
doi = {10.1145/1281192.1281303},
abstract = {An important problem in data mining is detecting changes in large datasets. Although there are a variety of change detection algorithms that have been developed, in practice it can be a problem to scale these algorithms to large data sets due to the heterogeneity of the data. In this paper, we describe a case study involving payment card data in which we built and monitored a separate change detection model for each cell in a multi-dimensional data cube. We describe a system that has been in operation for the past two years that builds and monitors over 15,000 separate baseline models and the process that isused for generating and investigating alerts using these baselines.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1018–1022},
numpages = {5},
keywords = {cubes of models, change detection, data quality, baselines},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281304,
author = {Pan, Rong and Zhao, Junhui and Zheng, Vincent Wenchen and Pan, Jeffrey Junfeng and Shen, Dou and Pan, Sinno Jialin and Yang, Qiang},
title = {Domain-Constrained Semi-Supervised Mining of Tracking Models in Sensor Networks},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281304},
doi = {10.1145/1281192.1281304},
abstract = {Accurate localization of mobile objects is a major research problem in sensor networks and an important data mining application. Specifically, the localization problem is to determine the location of a client device accurately given the radio signal strength values received at the client device from multiple beacon sensors or access points. Conventional data mining and machine learning methods can be applied to solve this problem. However, all of them require large amounts of labeled training data, which can be quite expensive. In this paper, we propose a probabilistic semi supervised learning approach to reduce the calibration effort and increase the tracking accuracy. Our method is based on semi-supervised conditional random fields which can enhance the learned model from a small set of training data with abundant unlabeled data effectively. To make our method more efficient, we exploit a Generalized EM algorithm coupled with domain constraints. We validate our method through extensive experiments in a real sensor network using Crossbow MICA2 sensors. The results demonstrate the advantages of methods compared to other state-of-the-art object-tracking algorithms.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1023–1027},
numpages = {5},
keywords = {localization, calibration, sensor networks, EM, CRF, tracking},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281305,
author = {Peng, Wei and Perng, Charles and Li, Tao and Wang, Haixun},
title = {Event Summarization for System Management},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281305},
doi = {10.1145/1281192.1281305},
abstract = {In system management applications, an overwhelming amount of data are generated and collected in the form of temporal events. While mining temporal event data to discover interesting and frequent patterns has obtained rapidly increasing research efforts, users of the applications are overwhelmed by the mining results. The extracted patterns are generally of large volume and hard to interpret, they may be of no emphasis, intricate and meaningless to non-experts, even to domain experts. While traditional research efforts focus on finding interesting patterns, in this paper, we take a novel approach called event summarization towards the understanding of the seemingly chaotic temporal data. Event summarization aims at providing a concise interpretation of the seemingly chaotic data, so that domain experts may take actions upon the summarized models. Event summarization decomposes the temporal information into many independent subsets and finds well fitted models to describe each subset.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1028–1032},
numpages = {5},
keywords = {event summarization, ERN, temporal dependency, log},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281306,
author = {Rao, R. Bharat and Bi, Jinbo and Fung, Glenn and Salganicoff, Marcos and Obuchowski, Nancy and Naidich, David},
title = {LungCAD: A Clinically Approved, Machine Learning System for Lung Cancer Detection},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281306},
doi = {10.1145/1281192.1281306},
abstract = {We present LungCAD, a computer aided diagnosis (CAD) system that employs a classification algorithm for detecting solid pulmonary nodules from CT thorax studies. We briefly describe some of the machine learning techniques developed to overcome the real world challenges in this medical domain. The most significant hurdle in transitioning from a machine learning research prototype that performs well on an in-house dataset into a clinically deployable system, is the requirement that the CAD system be tested in a clinical trial. We describe the clinical trial in which LungCAD was tested: a large scale multi-reader, multi-case (MRMC) retrospective observational study to evaluate the effect of CAD in clinical practice for detecting solid pulmonary nodules from CT thorax studies. The clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with LungCAD, both for detecting nodules and identifying potentially actionable nodules; this, along with other findings from the trial, has resulted in FDA approval for LungCAD in late 2006.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1033–1037},
numpages = {5},
keywords = {clinical trial, classification, lung cancer prognosis, computer aided detection},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281307,
author = {Yan, Robert J. and Ling, Charles X.},
title = {Machine Learning for Stock Selection},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281307},
doi = {10.1145/1281192.1281307},
abstract = {In this paper, we propose a new method called Prototype Ranking (PR) designed for the stock selection problem. PR takes into account the huge size of real-world stock data and applies a modified competitive learning technique to predict the ranks of stocks. The primary target of PR is to select the top performing stocks among many ordinary stocks. PR is designed to perform the learning and testing in a noisy stocks sample set where the top performing stocks are usually the minority. The performance of PR is evaluated by a trading simulation of the real stock data. Each week the stocks with the highest predicted ranks are chosen to construct a portfolio. In the period of 1978-2004, PR's portfolio earns a much higher average return as well as a higher risk-adjusted return than Cooper's method, which shows that the PR method leads to a clear profit improvement.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1038–1042},
numpages = {5},
keywords = {stock selection},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281308,
author = {Ye, Yanfang and Wang, Dingding and Li, Tao and Ye, Dongyi},
title = {IMDS: Intelligent Malware Detection System},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281308},
doi = {10.1145/1281192.1281308},
abstract = {The proliferation of malware has presented a serious threat to the security of computer systems. Traditional signature-based anti-virus systems fail to detect polymorphic and new, previously unseen malicious executables. In this paper, resting on the analysis of Windows API execution sequences called by PE files, we develop the Intelligent Malware Detection System (IMDS) using Objective-Oriented Association (OOA) mining based classification. IMDS is an integrated system consisting of three major modules: PE parser, OOA rule generator, and rule based classifier. An OOA_Fast_FP-Growth algorithm is adapted to efficiently generate OOA rules for classification. A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of King-Soft Corporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system out perform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan, as well as previous data mining based detection systems which employed Naive Bayes, Support Vector Machine (SVM) and Decision Tree techniques.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1043–1047},
numpages = {5},
keywords = {OOA mining, PE file, malware, windows API sequence},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281309,
author = {Yin, Xiaoxin and Han, Jiawei and Yu, Philip S.},
title = {Truth Discovery with Multiple Conflicting Information Providers on the Web},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281309},
doi = {10.1145/1281192.1281309},
abstract = {The world-wide web has become the most important information source for most of us. Unfortunately, there is no guarantee for the correctness of information on the web. Moreover, different web sites often provide conflicting information on a subject, such as different specifications for the same product. In this paper we propose a new problem called Veracity, i.e., conformity to truth, which studies how to find true facts from a large amount of conflicting information on many subjects that is provided by various web sites. We design a general framework for the Veracity problem, and invent an algorithm called TruthFinder, which utilizes the relationships between web sites and their information, i.e., a web site is trustworthy if it provides many pieces of true information, and a piece of information is likely to be true if it is provided by many trustworthy web sites. Our experiments show that TruthFinder successfully finds true facts among conflicting information, and identifies trustworthy web sites better than the popular search engines.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1048–1052},
numpages = {5},
keywords = {data quality, link analysis, web mining},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1281311,
author = {Parthasarathy, Srinivasan},
title = {Data Mining at the Crossroads: Successes, Failures and Learning from Them},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281311},
doi = {10.1145/1281192.1281311},
abstract = {Since the 1989 workshop on knowledge discovery in databases, the field has seen sustained growth and interest and has attained significant maturity. The main objectives of this panel will be to reflect on the successes and failures in the field of data mining over the last eighteen years and to examine what insights we can take with us as we move forward.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1053–1055},
numpages = {3},
keywords = {failures and mistakes, future outlook, applications and implementations, success stories},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327944,
author = {Li, Ying and Shen, Dou and Surendran, Arun C.},
title = {First International Workshop on Data Mining and Audience Intelligence for Advertising},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327944},
doi = {10.1145/1281192.1327944},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {1},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327945,
author = {Liu, Bing and Bennett, James and Elkan, Charles and Smyth, Padhraic and Tikk, Domonkos},
title = {KDD Cup and Workshop 2007},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327945},
doi = {10.1145/1281192.1327945},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {2},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327946,
author = {Yu, Philips and Zhang, Chengqi and Williams, Graham and Cao, Longbing},
title = {2007 International Workshop on Domain Driven Data Mining},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327946},
doi = {10.1145/1281192.1327946},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {3},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327947,
author = {Ganguly, Auroop R. and Gama, Joao and Omitaomu, Olufemi A. and Gaber, Mohamed Medhat and Vatsavai, Ranga Raju},
title = {First International Workshop on Knowledge Discovery from Sensor Data},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327947},
doi = {10.1145/1281192.1327947},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {4},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327948,
author = {Giles, Lee and McCallum, Andrew and Mobasher, Bamshad and Nasraoui, Olfa and Spiliopoulou, Myra and Srivastava, Jaideep and Yen, John and Zhang, Haizheng},
title = {The Joint Ninth WEBKDD and 1st SNA-KDD Workshop on Web Mining and Social Network Analysis},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327948},
doi = {10.1145/1281192.1327948},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {5},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327949,
author = {Bonchi, Francesco and Ferrari, Elena and Malin, Bradley and Saygin, Yucel},
title = {First ACM SIGKDD International Workshop on Privacy, Security, and Trust in KDD},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327949},
doi = {10.1145/1281192.1327949},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {6},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327950,
author = {Candan, K. Selcuk and Zhang, Zhongfei},
title = {Eighth International Workshop on Multimedia Data Mining},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327950},
doi = {10.1145/1281192.1327950},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {7},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327951,
author = {Grossman, Robert and Connelly, Shirley},
title = {Fifth International Workshop on Data Mining Standards, Services, and Platforms},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327951},
doi = {10.1145/1281192.1327951},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {8},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327952,
author = {Chen, Jake Y. and Lonardi, Stefano and Zaki, Mohammed},
title = {Seventh International Workshop on Data Mining in Bioinformatics},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327952},
doi = {10.1145/1281192.1327952},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {9},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327953,
author = {Zhu, Xingquan and Jin, Ruoming and Agrawal, Gagan},
title = {First International Workshop on Mining Multiple Information Sources},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327953},
doi = {10.1145/1281192.1327953},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {10},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327954,
author = {Keogh, Eamonn and Shelton, Christian and Moerchen, Fabian},
title = {First International Workshop and Challenge on Time Series Classification},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327954},
doi = {10.1145/1281192.1327954},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {11},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327955,
author = {Kitts, Brendan and Melli, Gabor and Piatetsky-Shapiro, Gregory and Courbois, Pip and Simoff, Simeon J. and Zhang, Jing Ying and Rexer, Karl and Wu, Gang and Elder, John and Osborn, Tom and Freeman, Ed and Li, Ying},
title = {Second Workshop on Data Mining Case Studies and Practice Prize},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327955},
doi = {10.1145/1281192.1327955},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {12},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327957,
author = {Faloutsos, Christos and Kolda, Tamara G. and Sun, Jimeng},
title = {Mining Large Time-Evolving Data Using Matrix and Tensor Tools},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327957},
doi = {10.1145/1281192.1327957},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {1},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327958,
author = {Dasu, Tamraparni and Urbanek, Simon},
title = {A Statistical Framework for Mining Data Streams},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327958},
doi = {10.1145/1281192.1327958},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {2},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327959,
author = {Domingos, Pedro},
title = {Statistical Modeling of Relational Data},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327959},
doi = {10.1145/1281192.1327959},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {3},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327960,
author = {Grobelnik, Marko and Mladenic, Dunja and Blaz, Fortuna},
title = {Text Mining and Link Analysis for Web and Semantic Web},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327960},
doi = {10.1145/1281192.1327960},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {4},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327961,
author = {Neapolitan, Richard E.},
title = {Learning Bayesian Networks},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327961},
doi = {10.1145/1281192.1327961},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {5},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327962,
author = {Seni, Giovanni and Elder, John},
title = {From Trees to Forests and Rule Sets: A Unified Overview of Ensemble Methods},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327962},
doi = {10.1145/1281192.1327962},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {6},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

@inproceedings{10.1145/1281192.1327963,
author = {Keogh, Eamonn},
title = {Mining Shape and Time Series Databases with Symbolic Representations},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1327963},
doi = {10.1145/1281192.1327963},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
articleno = {7},
numpages = {1},
location = {San Jose, California, USA},
series = {KDD '07}
}

