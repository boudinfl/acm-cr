@inproceedings{10.1145/502512.502513,
author = {Altman, Russ},
title = {Challenges for Knowledge Discovery in Biology},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502513},
doi = {10.1145/502512.502513},
abstract = {Bioinformatics is the study of information flow in biology. Interest in the field has exploded in the last 10 years with the emergence of techniques for large scale experimental data collection-including genome sequencing, gene expression analysis, protein interaction detection, high-throughput structure determination and others. These techniques, in the context of a large online published literature, have created relatively large data sets (at least by biological standards) that are not possible to analyze manually. There is therefore a critical need for methods to analyze these data and reduce them to new knowledge. The principle challenges to the field include the great diversity of data types and questions that are asked of the data, and the communication difficulties that can exist between experts in biology and experts in machine learning. In this talk, I will provide an introduction to the major biological questions that are being addressed, why they are important, and how the field is trying to address them with technical approaches.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2},
numpages = {1},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502514,
author = {Mitchell, Tom},
title = {Extracting Targeted Data from the Web},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502514},
doi = {10.1145/502512.502514},
abstract = {Tom M. Mitchell is author of the textbook "Machine Learning" (McGraw Hill, 1997), President of the American Association for Artificial Intelligence and a member of the National Research Council's Computer Science and Telecommunications Board. He is Vice President and Chief Scientist at WhizBang Labs and is currently on a two-year leave of absence from Carnegie Mellon University where he is the Fredkin Professor of Learning and AI in the School of Computer Science and founding Director of CMU's Center for Automated Learning and Discovery. Mitchell's research interests span many areas of Machine Learning theory and practice. His current work at WhizBang Labs involves developing machine learning methods for extracting information from text. For example, WhizBang has developed the world's largest database of job openings by training its software to automatically locate and extract detailed information from job postings on corporate web sites (see www.flipdog.com).},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {3},
numpages = {1},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502515,
author = {Ramakrishnan, Raghu},
title = {Mass Collaboration and Data Mining},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502515},
doi = {10.1145/502512.502515},
abstract = {Mass Collaboration is a new "P2P"-style approach to large-scale knowledge sharing, with applications in customer support, focused community development, and capturing knowledge distributed within large organizations. Effectively supporting this paradigm raises many technical challenges, and offers intriguing opportunities for mining massive amounts of data captured continually from user interactions. Data mining offers the promise of increased business intelligence, and also improved user experiences, leading to increased participation and greater quality in the knowledge that is captured, both of which are central objectives in Mass Collaboration. In this talk, I will introduce Mass Collaboration and discuss some important data mining related issues.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {4},
numpages = {1},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502516,
author = {Agrarwal, Nitin},
title = {Applications of Generalized Support Vector Machines to Predictive Modeling},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502516},
doi = {10.1145/502512.502516},
abstract = {The work of the Russian mathematician Vladimir Vapnik (AT&amp;T Labs) enables us to go back to the roots of theoretical statistics, leaving behind Fisher's parameters in favor of the general approaches started in the 1930s by Glivenko-Cantelli-Kolmogorov. Nowadays, it has become possible to model millions of events described by thousands of variables, within a reasonable time for a specific application. The SRM approach works with a family of models and calibrates the family of models to a point which is the best compromise between accuracy and robustness. It also measures the complexity of the model using VC dimension which is not plagued by number of parameters. Hence models for large events described by several parameters can be generalized. This opens up great prospects in numerous fields like Customer Relationship Management, Network Optimization, Risk Management, Manufacturing Yield Management, and a number of other data-rich problems.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {6},
numpages = {1},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502517,
author = {Edelstein, Herb},
title = {Data Mining: Are We There Yet?},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502517},
doi = {10.1145/502512.502517},
abstract = {Data mining started its move out of the statistics and machine learning ghettos and into the mainstream almost 10 years ago. With great fanfare and a large influx of venture capital, data mining was going to change the very nature of business. Yet data mining products have had relatively modest success in the marketplace. The reasons include limitations and misplaced emphasis in the products' features and functions, unrealistic expectations set by messages from the data mining community, and a lack of readiness by many prospective users. This session will look at where vendors have succeeded and failed with their products, what expectations users should have, and suggestions for achieving the potential of this exciting and valuable technology.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {7},
numpages = {1},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502518,
author = {Kohavi, Ron},
title = {Mining E-Commerce Data: The Good, the Bad, and the Ugly},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502518},
doi = {10.1145/502512.502518},
abstract = {Organizations conducting Electronic Commerce (e-commerce) can greatly benefit from the insight that data mining of transactional and clickstream data provides. Such insight helps not only to improve the electronic channel (e.g., a web site), but it is also a learning vehicle for the bigger organization conducting business at brick-and-mortar stores. The e-commerce site serves as an early alert system for emerging patterns and a laboratory for experimentation. For successful data mining, several ingredients are needed and e-commerce provides all the right ones (the Good). Web server logs, which are commonly used as the source of data for mining e-commerce data, were designed to debug web servers, and the data they provide is insufficient, requiring the use of heuristics to reconstruct events. Moreover, many events are never logged in web server logs, limiting the source of data for mining (the Bad). Many of the problems of dealing with web server log data can be resolved by properly architecting the e-commerce sites to generate data needed for mining. Even with a good architecture, however, there are challenging problems that remain hard to solve (the Ugly). Lessons and metrics based on mining real e-commerce data are presented.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {8–13},
numpages = {6},
keywords = {web site architecture, E-commerce, web server, application server, data mining},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502519,
author = {Netz, Amir},
title = {Data Mining Platform for Database Developers},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502519},
doi = {10.1145/502512.502519},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {14},
numpages = {1},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502520,
author = {Riedl, John},
title = {Recommender Systems in Commerce and Community},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502520},
doi = {10.1145/502512.502520},
abstract = {Recommender systems have been revolutionizing the way shoppers and information seekers find what they want. We will study some of the tremendous successes and spectacular failures of recommenders in E-commerce to understand the causes of the success or failure. We will leverage that understanding into a set of principles for successfully applying recommenders to business problems. Finally, we will study the economic and social forces that are shaping the evolution of recommenders, and peer into the crystal ball to glimpse the directions the technology will be going in the future.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {15},
numpages = {1},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502521,
author = {Bi, Zhiqiang and Faloutsos, Christos and Korn, Flip},
title = {The "DGX" Distribution for Mining Massive, Skewed Data},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502521},
doi = {10.1145/502512.502521},
abstract = {Skewed distributions appear very often in practice. Unfortunately, the traditional Zipf distribution often fails to model them well. In this paper, we propose a new probability distribution, the Discrete Gaussian Exponential (DGX), to achieve excellent fits in a wide variety of settings; our new distribution includes the Zipf distribution as a special case. We present a statistically sound method for estimating the DGX parameters based on maximum likelihood estimation (MLE). We applied DGX to a wide variety of real world data sets, such as sales data from a large retailer chain, us-age data from AT&amp;T, and Internet clickstream data; in all cases, DGX fits these distributions very well, with almost a 99% correlation coefficient in quantile-quantile plots. Our algorithm also scales very well because it requires only a single pass over the data. Finally, we illustrate the power of DGX as a new tool for data mining tasks, such as outlier detection.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {17–26},
numpages = {10},
keywords = {Zipf's law, DGX, lognormal distribution, frequency-count plot, rank-frequency plot, outlier detection, maximum likelihood estimation},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502522,
author = {Buja, Andreas and Lee, Yung-Seop},
title = {Data Mining Criteria for Tree-Based Regression and Classification},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502522},
doi = {10.1145/502512.502522},
abstract = {This paper is concerned with the construction of regression and classification trees that are more adapted to data mining applications than conventional trees. To this end, we propose new splitting criteria for growing trees. Conventional splitting criteria attempt to perform well on both sides of a split by attempting a compromise in the quality of fit between the left and the right side. By contrast, we adopt a data mining point of view by proposing criteria that search for interesting subsets of the data, as opposed to modeling all of the data equally well. The new criteria do not split based on a compromise between the left and the right bucket; they effectively pick the more interesting bucket and ignore the other.As expected, the result is often a simpler characterization of interesting subsets of the data. Less expected is that the new criteria often yield whole trees that provide more interpretable data descriptions. Surprisingly, it is a "flaw" that works to their advantage: The new criteria have an increased tendency to accept splits near the boundaries of the predictor ranges. This so-called "end-cut problem" leads to the repeated peeling of small layers of data and results in very unbalanced but highly expressive and interpretable trees.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {27–36},
numpages = {10},
keywords = {CART, Pima Indians Diabetes data, Boston Housing data, splitting criteria},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502523,
author = {Cadez, Igor V. and Smyth, Padhraic and Mannila, Heikki},
title = {Probabilistic Modeling of Transaction Data with Applications to Profiling, Visualization, and Prediction},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502523},
doi = {10.1145/502512.502523},
abstract = {Transaction data is ubiquitous in data mining applications. Examples include market basket data in retail commerce, telephone call records in telecommunications, and Web logs of individual page-requests at Web sites. Profiling consists of using historical transaction data on individuals to construct a model of each individual's behavior. Simple profiling techniques such as histograms do not generalize well from sparse transaction data. In this paper we investigate the application of probabilistic mixture models to automatically generate profiles from large volumes of transaction data. In effect, the mixture model represents each individual's behavior as a linear combination of "basis transactions." We evaluate several variations of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram-based techniques, as well as being relatively scalable, interpretable, and flexible. In addition we point to applications in outlier detection, customer ranking, interactive visualization, and so forth. The paper concludes by comparing and relating the proposed framework to other transaction-data modeling techniques such as association rules.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {37–46},
numpages = {10},
keywords = {profiles, mixture models, EM algorithm, transaction data},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502524,
author = {Dittrich, Jens-Peter and Seeger, Bernhard},
title = {GESS: A Scalable Similarity-Join Algorithm for Mining Large Data Sets in High Dimensional Spaces},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502524},
doi = {10.1145/502512.502524},
abstract = {The similarity join is an important operation for mining high-dimensional feature spaces. Given two data sets, the similarity join computes all tuples (x, y) that are within a distance ε.One of the most efficient algorithms for processing similarity-joins is the Multidimensional-Spatial Join (MSJ) by Koudas and Sevcik. In our previous work --- pursued for the two-dimensional case --- we found however that MSJ has several performance shortcomings in terms of CPU and I/O cost as well as memory-requirements. Therefore, MSJ is not generally applicable to high-dimensional data.In this paper, we propose a new algorithm named Generic External Space Sweep (GESS). GESS introduces a modest rate of data replication to reduce the number of expensive distance computations. We present a new cost-model for replication, an I/O model, and an inexpensive method for duplicate removal. The principal component of our algorithm is a highly flexible replication engine.Our analytical model predicts a tremendous reduction of the number of expensive distance computations by several orders of magnitude in comparison to MSJ (factor 107). In addition, the memory requirements of GESS are shown to be lower by several orders of magnitude. Furthermore, the I/O cost of our algorithm is by factor 2 better (independent from the fact whether replication occurs or not). Our analytical results are confirmed by a large series of simulations and experiments with synthetic and real high-dimensional data sets.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {47–56},
numpages = {10},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502525,
author = {Domingos, Pedro and Richardson, Matt},
title = {Mining the Network Value of Customers},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502525},
doi = {10.1145/502512.502525},
abstract = {One of the major applications of data mining is in helping companies determine which potential customers to market to. If the expected profit from a customer is greater than the cost of marketing to her, the marketing action for that customer is executed. So far, work in this area has considered only the intrinsic value of the customer (i.e, the expected profit from sales to her). We propose to model also the customer's network value: the expected profit from sales to other customers she may influence to buy, the customers those may influence, and so on recursively. Instead of viewing a market as a set of independent entities, we view it as a social network and model it as a Markov random field. We show the advantages of this approach using a social network mined from a collaborative filtering database. Marketing that exploits the network value of customers---also known as viral marketing---can be extremely effective, but is still a black art. Our work can be viewed as a step towards providing a more solid foundation for it, taking advantage of the availability of large relevant databases.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {57–66},
numpages = {10},
keywords = {social networks, Markov random fields, dependency networks, direct marketing, viral marketing, collaborative filtering},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502526,
author = {DuMouchel, William and Pregibon, Daryl},
title = {Empirical Bayes Screening for Multi-Item Associations},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502526},
doi = {10.1145/502512.502526},
abstract = {This paper considers the framework of the so-called "market basket problem", in which a database of transactions is mined for the occurrence of unusually frequent item sets. In our case, "unusually frequent" involves estimates of the frequency of each item set divided by a baseline frequency computed as if items occurred independently. The focus is on obtaining reliable estimates of this measure of interestingness for all item sets, even item sets with relatively low frequencies. For example, in a medical database of patient histories, unusual item sets including the item "patient death" (or other serious adverse event) might hopefully be flagged with as few as 5 or 10 occurrences of the item set, it being unacceptable to require that item sets occur in as many as 0.1% of millions of patient reports before the data mining algorithm detects a signal. Similar considerations apply in fraud detection applications. Thus we abandon the requirement that interesting item sets must contain a relatively large fixed minimal support, and adopt a criterion based on the results of fitting an empirical Bayes model to the item set counts. The model allows us to define a 95% Bayesian lower confidence limit for the "interestingness" measure of every item set, whereupon the item sets can be ranked according to their empirical Bayes confidence limits. For item sets of size J &gt; 2, we also distinguish between multi-item associations that can be explained by the observed J(J-1)/2 pairwise associations, and item sets that are significantly more frequent than their pairwise associations would suggest. Such item sets can uncover complex or synergistic mechanisms generating multi-item associations. This methodology has been applied within the U.S. Food and Drug Administration (FDA) to databases of adverse drug reaction reports and within AT&amp;T to customer international calling histories. We also present graphical techniques for exploring and understanding the modeling results.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {67–76},
numpages = {10},
keywords = {gamma-Poisson model, Knowledge Discovery, shrinkage estimation, Data Mining, empirical Bayes methods, market basket problem, Association rules, Statistical Models},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502527,
author = {Fung, Glenn and Mangasarian, Olvi L.},
title = {Proximal Support Vector Machine Classifiers},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502527},
doi = {10.1145/502512.502527},
abstract = {Instead of a standard support vector machine (SVM) that classifies points by assigning them to one of two disjoint half-spaces, points are classified by assigning them to the closest of two parallel planes (in input or feature space) that are pushed apart as far as possible. This formulation, which can also be interpreted as regularized least squares and considered in the much more general context of regularized networks [8, 9], leads to an extremely fast and simple algorithm for generating a linear or nonlinear classifier that merely requires the solution of a single system of linear equations. In contrast, standard SVMs solve a quadratic or a linear program that require considerably longer computational time. Computational results on publicly available datasets indicate that the proposed proximal SVM classifier has comparable test set correctness to that of standard SVM classifiers, but with considerably faster computational time that can be an order of magnitude faster. The linear proximal SVM can easily handle large datasets as indicated by the classification of a 2 million point 10-attribute set in 20.8 seconds. All computational results are based on 6 lines of MATLAB code.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {77–86},
numpages = {10},
keywords = {support vector machines, data classification, linear equations},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502528,
author = {Garcke, Jochen and Griebel, Michael},
title = {Data Mining with Sparse Grids Using Simplicial Basis Functions},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502528},
doi = {10.1145/502512.502528},
abstract = {Recently we presented a new approach [18] to the classification problem arising in data mining. It is based on the regularization network approach but, in contrast to other methods which employ ansatz functions associated to data points, we use a grid in the usually high-dimensional feature space for the minimization process. To cope with the curse of dimensionality, we employ sparse grids [49]. Thus, only O(hn-1nd-1) instead of O(hn-d) grid points and unknowns are involved. Here d denotes the dimension of the feature space and hn = 2-n gives the mesh size. We use the sparse grid combination technique [28] where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension. The sparse grid solution is then obtained by linear combination. In contrast to our former work, where d-linear functions were used, we now apply linear basis functions based on a simplicial discretization. This allows to handle more dimensions and the algorithm needs less operations per data point.We describe the sparse grid combination technique for the classification problem, give implementational details and discuss the complexity of the algorithm. It turns out that the method scales linearly with the number of given data points. Finally we report on the quality of the classifier built by our new method on data sets with up to 10 dimensions. It turns out that our new method achieves correctness rates which are competitive to that of the best existing methods.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {87–96},
numpages = {10},
keywords = {approximation, sparse grids, simplicial discretization, data mining, combination technique, classification},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502529,
author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
title = {Mining Time-Changing Data Streams},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502529},
doi = {10.1145/502512.502529},
abstract = {Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {97–106},
numpages = {10},
keywords = {concept drift, incremental learning, Hoeffding bounds, Decision trees, data streams, subsampling},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502530,
author = {Kandogan, Eser},
title = {Visualizing Multi-Dimensional Clusters, Trends, and Outliers Using Star Coordinates},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502530},
doi = {10.1145/502512.502530},
abstract = {Interactive visualizations are effective tools in mining scientific, engineering, and business data to support decision-making activities. Star Coordinates is proposed as a new multi-dimensional visualization technique, which supports various interactions to stimulate visual thinking in early stages of knowledge discovery process. In Star Coordinates, coordinate axes are arranged on a two-dimensional surface, where each axis shares the same origin point. Each multi-dimensional data element is represented by a point, where each attribute of the data contributes to its location through uniform encoding. Interaction features of Star Coordinates provide users the ability to apply various transformations dynamically, integrate and separate dimensions, analyze correlations of multiple dimensions, view clusters, trends, and outliers in the distribution of data, and query points based on data ranges. Our experience with Star Coordinates shows that it is particularly useful for the discovery of hierarchical clusters, and analysis of multiple factors providing insight in various real datasets including telecommunications churn.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {107–116},
numpages = {10},
keywords = {knowledge discovery, Multi-dimensional visualization},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502531,
author = {Keogh, Eamonn and Chu, Selina and Pazzani, Michael},
title = {Ensemble-Index: A New Approach to Indexing Large Databases},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502531},
doi = {10.1145/502512.502531},
abstract = {The problem of similarity search (query-by-content) has attracted much research interest. It is a difficult problem because of the inherently high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier Transform (DFT), the Discrete Wavelet Transform (DWT) and Piecewise Polynomial Approximation. In this work, we introduce a novel framework for using ensembles of two or more representations for more efficient indexing. The basic idea is that instead of committing to a single representation for an entire dataset, different representations are chosen for indexing different parts of the database. The representations are chosen based upon a local view of the database. For example, sections of the data that can achieve a high fidelity representation with wavelets are indexed as wavelets, but highly spectral sections of the data are indexed using the Fourier transform. At query time, it is necessary to search several small heterogeneous indices, rather than one large homogeneous index. As we will theoretically and empirically demonstrate this results in much faster query response times.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {117–125},
numpages = {9},
keywords = {dimensionality reduction, indexing and retrieval, data mining, similarity search, Time series},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502532,
author = {Knorr, Edwin M. and Ng, Raymond T. and Zamar, Ruben H.},
title = {Robust Space Transformations for Distance-Based Operations},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502532},
doi = {10.1145/502512.502532},
abstract = {For many KDD operations, such as nearest neighbor search, distance-based clustering, and outlier detection, there is an underlying κ-D data space in which each tuple/object is represented as a point in the space. In the presence of differing scales, variability, correlation, and/or outliers, we may get unintuitive results if an inappropriate space is used.The fundamental question that this paper addresses is: "What then is an appropriate space?" We propose using a robust space transformation called the Donoho-Stahel estimator. In the first half of the paper, we show the key properties of the estimator. Of particular importance to KDD applications involving databases is the stability property, which says that in spite of frequent updates, the estimator does not: (a) change much, (b) lose its usefulness, or (c) require re-computation. In the second half, we focus on the computation of the estimator for high-dimensional databases. We develop randomized algorithms and evaluate how well they perform empirically. The novel algorithm we develop called the Hybrid-random algorithm is, in most cases, at least an order of magnitude faster than the Fixed-angle and Subsampling algorithms.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {126–135},
numpages = {10},
keywords = {Data Mining, Robust Estimators, Distance-based Operations, Robust Statistics, Space Transformations, Outliers},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502533,
author = {Kramer, Stefan and De Raedt, Luc and Helma, Christoph},
title = {Molecular Feature Mining in HIV Data},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502533},
doi = {10.1145/502512.502533},
abstract = {We present the application of Feature Mining techniques to the Developmental Therapeutics Program's AIDS antiviral screen database. The database consists of 43576 compounds, which were measured for their capability to protect human cells from HIV-1 infection. According to these measurements, the compounds were classified as either active, moderately active or inactive. The distribution of classes is extremely skewed: Only 1.3 % of the molecules is known to be active, and 2.7 % is known to be moderately active.Given this database, we were interested in molecular substructures (i.e., features) that are frequent in the active molecules, and infrequent in the inactives. In data mining terms, we focused on features with a minimum support in active compounds and a maximum support in inactive compounds. We analyzed the database using the levelwise version space algorithm that forms the basis of the inductive query and database system MOLFEA (Molecular Feature Miner). Within this framework, it is possible to declaratively specify the features of interest, such as the frequency of features on (possibly different) datasets as well as on the generality and syntax of them. Assuming that the detected substructures are causally related to biochemical mechanisms, it should be possible to facilitate the development of new pharmaceuticals with improved activities.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {136–143},
numpages = {8},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502534,
author = {Liu, Bing and Ma, Yiming and Yu, Philip S.},
title = {Discovering Unexpected Information from Your Competitors' Web Sites},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502534},
doi = {10.1145/502512.502534},
abstract = {Ever since the beginning of the Web, finding useful information from the Web has been an important problem. Existing approaches include keyword-based search, wrapper-based information extraction, Web query and user preferences. These approaches essentially find information that matches the user's explicit specifications. This paper argues that this is insufficient. There is another type of information that is also of great interest, i.e., unexpected information, which is unanticipated by the user. Finding unexpected information is useful in many applications. For example, it is useful for a company to find unexpected information bout its competitors, e.g., unexpected services and products that its competitors offer. With this information, the company can learn from its competitors and/or design counter measures to improve its competitiveness. Since the number of pages of a typical commercial site is very large and there are also many relevant sites (competitors), it is very difficult for a human user to view each page to discover the unexpected information. Automated assistance is needed. In this paper, we propose a number of methods to help the user find various types of unexpected information from his/her competitors' Web sites. Experiment results show that these techniques are very useful in practice and also efficient.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {144–153},
numpages = {10},
keywords = {Web comparison, Information interestingness, Web mining},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502535,
author = {Padmanabhan, Balaji and Zheng, Zhiqiang and Kimbrough, Steven O.},
title = {Personalization from Incomplete Data: What You Don't Know Can Hurt},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502535},
doi = {10.1145/502512.502535},
abstract = {Clickstream data collected at any web site (site-centric data) is inherently incomplete, since it does not capture users' browsing behavior across sites (user-centric data). Hence, models learned from such data may be subject to limitations, the nature of which has not been well studied. Understanding the limitations is particularly important since most current personalization techniques are based on site-centric data only. In this paper, we empirically examine the implications of learning from incomplete data in the context of two specific problems: (a) predicting if the remainder of any given session will result in a purchase and (b) predicting if a given user will make a purchase at any future session. For each of these problems we present new algorithms for fast and accurate data preprocessing of clickstream data. Based on a comprehensive experiment on user-level clickstream data gathered from 20,000 users' browsing behavior, we demonstrate that models built on user-centric data outperform models built on site-centric data for both prediction tasks.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {154–163},
numpages = {10},
keywords = {data preprocessing, learning, Incomplete data, personalization, probabilistic clipping, clickstream data},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502536,
author = {Pavlov, Dmitry and Smyth, Padhraic},
title = {Probabilistic Query Models for Transaction Data},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502536},
doi = {10.1145/502512.502536},
abstract = {We investigate the application of Bayesian networks, Markov random fields, and mixture models to the problem of query answering for transaction data sets. We formulate two versions of the querying problem: the query selectivity estimation (i.e., finding exact counts for tuples in a data set) and the query generalization problem (i.e., computing the probability that a tuple will occur in new data). We show that frequent itemsets are useful for reducing the original data to a compressed representation and introduce a method to store them using an ADTree data structure. In an extension of our earlier work on this topic we propose several new schemes for query answering based on the compressed representation that avoid direct scans of the data at query time. Experimental results on real-world transaction data sets provide insights into various tradeoffs involving the offline time for model-building, the online time for query-answering, the memory footprint of the compressed data, and the accuracy of the estimate provided to the query.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {164–173},
numpages = {10},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502537,
author = {Pennock, David M. and Lawrence, Steve and Nielsen, Finn \r{A}rup and Giles, C. Lee},
title = {Extracting Collective Probabilistic Forecasts from Web Games},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502537},
doi = {10.1145/502512.502537},
abstract = {Game sites on the World Wide Web draw people from around the world with specialized interests, skills, and knowledge. Data from the games often reflects the players' expertise and will to win. We extract probabilistic forecasts from data obtained from three online games: the Hollywood Stock Exchange (HSX), the Foresight Exchange (FX), and the Formula One Pick Six (F1P6) competition. We find that all three yield accurate forecasts of uncertain future events. In particular, prices of so-called "movie stocks" on HSX are good indicators of actual box office returns. Prices of HSX securities in Oscar, Emmy, and Grammy awards correlate well with observed frequencies of winning. FX prices are reliable indicators of future developments in science and technology. Collective predictions from players in the F1 competition serve as good forecasts of true race outcomes. In some cases, forecasts induced from game data are more reliable than expert opinions. We argue that web games naturally attract well-informed and well-motivated players, and thus offer a valuable and oft-overlooked source of high-quality data with significant predictive value.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {174–183},
numpages = {10},
keywords = {Formula One Pick Six Competition, knowledge discovery, data mining, artificial markets, World Wide Web games, Collective probabilistic forecasts, Foresight Exchange, Hollywood Stock Exchange},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502538,
author = {Traina, Agma and Traina, Caetano and Papadimitriou, Spiros and Faloutsos, Christos},
title = {Tri-Plots: Scalable Tools for Multidimensional Data Mining},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502538},
doi = {10.1145/502512.502538},
abstract = {We focus on the problem of finding patterns across two large, multidimensional datasets. For example, given feature vectors of healthy and of non-healthy patients, we want to answer the following questions: Are the two clouds of points separable? What is the smallest/largest pair-wise distance across the two datasets? Which of the two clouds does a new point (feature vector) come from?We propose a new tool, the tri-plot, and its generalization, the pq-plot, which help us answer the above questions. We provide a set of rules on how to interpret a tri-plot, and we apply these rules on synthetic and real datasets. We also show how to use our tool for classification, when traditional methods (nearest neighbor, classification trees) may fail.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {184–193},
numpages = {10},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502539,
author = {Yang, Cheng and Fayyad, Usama and Bradley, Paul S.},
title = {Efficient Discovery of Error-Tolerant Frequent Itemsets in High Dimensions},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502539},
doi = {10.1145/502512.502539},
abstract = {We present a generalization of frequent itemsets allowing for the notion of errors in the itemset definition. We motivate the problem and present an efficient algorithm that identifies error-tolerant frequent clusters of items in transactional data (customer-purchase data, web browsing data, text, etc.). The algorithm exploits sparseness of the underlying data to find large groups of items that are correlated over database records (rows). The notion of transaction coverage allows us to extend the algorithm and view it as a fast clustering algorithm for discovering segments of similar transactions in binary sparse data. We evaluate the new algorithm on three real-world applications: clustering high-dimensional data, query selectivity estimation and collaborative filtering. Results show that the algorithm consistently uncovers structure in large sparse databases that other traditional clustering algorithms fail to find.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {194–203},
numpages = {10},
keywords = {collaborative filtering, Error-tolerant frequent itemset, query selectivity estimation, clustering, high dimensions},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502540,
author = {Zadrozny, Bianca and Elkan, Charles},
title = {Learning and Making Decisions When Costs and Probabilities Are Both Unknown},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502540},
doi = {10.1145/502512.502540},
abstract = {In many data mining domains, misclassification costs are different for different examples, in the same way that class membership probabilities are example-dependent. In these domains, both costs and probabilities are unknown for test examples, so both cost estimators and probability estimators must be learned. After discussing how to make optimal decisions given cost and probability estimates, we present decision tree and naive Bayesian learning methods for obtaining well-calibrated probability estimates. We then explain how to obtain unbiased estimators for example-dependent costs, taking into account the difficulty that in general, probabilities and costs are not independent random variables, and the training examples for which costs are known are not representative of all examples. The latter problem is called sample selection bias in econometrics. Our solution to it is based on Nobel prize-winning work due to the economist James Heckman. We show that the methods we propose perform better than MetaCost and all other known methods, in a comprehensive experimental comparison that uses the well-known, large, and challenging dataset from the KDD'98 data mining contest.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {204–213},
numpages = {10},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502541,
author = {Adderley, Richard and Musgrove, Peter B.},
title = {Data Mining Case Study: Modeling the Behavior of Offenders Who Commit Serious Sexual Assaults},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502541},
doi = {10.1145/502512.502541},
abstract = {This paper looks at the use of a Self Organizing Map (SOM), to link of records of crimes of serious sexual attacks. Once linked a profile can be derived of the offender(s) responsible.The data was drawn from the major crimes database at the National Crime Faculty of the National Police Staff College Bramshill UK. The data was encoded from text by a small team of specialists working to a well-defined protocol. The encoded data was analyzed using SOMs. Two exercises were conducted. These resulted in the linking of several offences in to clusters each of which were sufficiently similar to have possibly been committed by the same offender(s). A number of clusters were used to form profiles of offenders. Some of these profiles were confirmed by independent analysts as either belonging to known offenders or appeared sufficiently interesting to warrant further investigation.The prototype was developed over 10 weeks. This contrasts with an in-house study using a conventional approach, which took 2 years to reach similar results. As a consequence of this study the NCF intends to pursue an in-depth follow up study.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {215–220},
numpages = {6},
keywords = {Self Organizing Map, Crime Pattern Analysis, Offender Behavior, Data Mining, Knowledge Discovery},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502542,
author = {Aggarwal, Charu C.},
title = {A Human-Computer Cooperative System for Effective High Dimensional Clustering},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502542},
doi = {10.1145/502512.502542},
abstract = {High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Therefore, techniques have recently been proposed to find clusters in hidden subspaces of the data. However, since the behavior of the data may vary considerably in different subspaces, it is often difficult to define the notion of a cluster with the use of simple mathematical formalizations. In fact, the meaningfulness and definition of a cluster is best characterized with the use of human intuition. In this paper, we propose a system which performs high dimensional clustering by effective cooperation between the human and the computer. The complex task of cluster creation is accomplished by a combination of human intuition and the computational support provided by the computer. The result is a system which leverages the best abilities of both the human and the computer in order to create very meaningful sets of clusters in high dimensionality.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {221–226},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502543,
author = {Aggarwal, Charu C. and Parthasarathy, Srinivasan},
title = {Mining Massively Incomplete Data Sets by Conceptual Reconstruction},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502543},
doi = {10.1145/502512.502543},
abstract = {Incomplete data sets have become almost ubiquitous in a wide variety of application domains. Common examples can be found in climate and image data sets, sensor data sets and medical data sets. The incompleteness in these data sets may arise from a number of factors: in some cases it may simply be a reflection of certain measurements not being available at the time; in others the information may be lost due to partial system failure; or it may simply be a result of users being unwilling to specify attributes due to privacy concerns. When a significant fraction of the entries are missing in all of the attributes, it becomes very difficult to perform any kind of reasonable extrapolation on the original data. For such cases, we introduce the novel idea of conceptual reconstruction, in which we create effective conceptual representations on which the data mining algorithms can be directly applied. The attraction behind the idea of conceptual reconstruction is to use the correlation structure of the data in order to express it in terms of concepts rather the original dimensions. As a result, the reconstruction procedure estimates only those conceptual aspects of the data which can be mined from the incomplete data set, rather than force errors created by extrapolation. We demonstrate the effectiveness of the approach on a variety of real data sets.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {227–232},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502544,
author = {Basu, Sugato and Mooney, Raymond J. and Pasupuleti, Krupakar V. and Ghosh, Joydeep},
title = {Evaluating the Novelty of Text-Mined Rules Using Lexical Knowledge},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502544},
doi = {10.1145/502512.502544},
abstract = {In this paper, we present a new method of estimating the novelty of rules discovered by data-mining methods using WordNet, a lexical knowledge-base of English words. We assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule - the more the average distance, more is the novelty of the rule. The novelty of rules extracted by the DiscoTEX text-mining system on Amazon.com book descriptions were evaluated by both human subjects and by our algorithm. By computing correlation coefficients between pairs of human ratings and between human and automatic ratings, we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another. @Text mining},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {233–238},
numpages = {6},
keywords = {semantic distance, interesting rules, knowledge hierarchy, WordNet, novelty},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502545,
author = {Beygelzimer, Alina and Perng, Chang-Shing and Ma, Sheng},
title = {Fast Ordering of Large Categorical Datasets for Better Visualization},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502545},
doi = {10.1145/502512.502545},
abstract = {An important issue in visualizing categorical data is how to order categorical values. The focus of this paper is on constructing such orderings efficiently without compromising their visual quality.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {239–244},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502546,
author = {Bingham, Ella and Mannila, Heikki},
title = {Random Projection in Dimensionality Reduction: Applications to Image and Text Data},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502546},
doi = {10.1145/502512.502546},
abstract = {Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {245–250},
numpages = {6},
keywords = {random projection, image data, text document data, high-dimensional data, dimensionality reduction},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502547,
author = {Caragea, Doina and Cook, Dianne and Honavar, Vasant G.},
title = {Gaining Insights into Support Vector Machine Pattern Classifiers Using Projection-Based Tour Methods},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502547},
doi = {10.1145/502512.502547},
abstract = {This paper discusses visual methods that can be used to understand and interpret the results of classification using support vector machines (SVM) on data with continuous real-valued variables. SVM induction algorithms build pattern classifiers by identifying a maximal margin separating hyperplane from training examples in high dimensional pattern spaces or spaces induced by suitable nonlinear kernel transformations over pattern spaces. SVM have been demonstrated to be quite effective in a number of practical pattern classification tasks. Since the separating hyperplane is defined in terms of more than two variables it is necessary to use visual techniques that can navigate the viewer through high-dimensional spaces. We demonstrate the use of projection-based tour methods to gain useful insights into SVM classifiers with linear kernels on 8-dimensional data.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {251–256},
numpages = {6},
keywords = {machine leaning, support vector machines, classification, visualization, multivariate data, tours, Dynamic graphics},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502548,
author = {Chen, Chien Chin and Chen, Meng Chang and Sun, Yeali},
title = {PVA: A Self-Adaptive Personal View Agent System},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502548},
doi = {10.1145/502512.502548},
abstract = {In this paper, we present PVA, an adaptive personal view information agent system to track, learn and manage, user's interests in Internet documents. When user's interests change, PVA, in not only the contents, but also in the structure of user profile, is modified to adapt to the changes. Experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {257–262},
numpages = {6},
keywords = {Machine learning, Personalization, Personal view, WWW},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502549,
author = {Chiu, Tom and Fang, DongPing and Chen, John and Wang, Yao and Jeris, Christopher},
title = {A Robust and Scalable Clustering Algorithm for Mixed Type Attributes in Large Database Environment},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502549},
doi = {10.1145/502512.502549},
abstract = {Clustering is a widely used technique in data mining applications to discover patterns in the underlying data. Most traditional clustering algorithms are limited to handling datasets that contain either continuous or categorical attributes. However, datasets with mixed types of attributes are common in real life data mining problems. In this paper, we propose a distance measure that enables clustering data with both continuous and categorical attributes. This distance measure is derived from a probabilistic model that the distance between two clusters is equivalent to the decrease in log-likelihood function as a result of merging. Calculation of this measure is memory efficient as it depends only on the merging cluster pair and not on all the other clusters. Zhang et al [8] proposed a clustering method named BIRCH that is especially suitable for very large datasets. We develop a clustering algorithm using our distance measure based on the framework of BIRCH. Similar to BIRCH, our algorithm first performs a pre-clustering step by scanning the entire dataset and storing the dense regions of data records in terms of summary statistics. A hierarchical clustering algorithm is then applied to cluster the dense regions. Apart from the ability of handling mixed type of attributes, our algorithm differs from BIRCH in that we add a procedure that enables the algorithm to automatically determine the appropriate number of clusters and a new strategy of assigning cluster membership to noisy data. For data with mixed type of attributes, our experimental results confirm that the algorithm not only generates better quality clusters than the traditional k-means algorithms, but also exhibits good scalability properties and is able to identify the underlying number of clusters in the data correctly. The algorithm is implemented in the commercial data mining tool Clementine 6.0 which supports the PMML standard of data mining model deployment.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {263–268},
numpages = {6},
keywords = {log-likelihood, number of clusters, noisy data, clustering, Mixed type of attributes},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502550,
author = {Dhillon, Inderjit S.},
title = {Co-Clustering Documents and Words Using Bipartite Spectral Graph Partitioning},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502550},
doi = {10.1145/502512.502550},
abstract = {Both document clustering and word clustering are well studied problems. Most existing algorithms cluster documents and words separately but not simultaneously. In this paper we present the novel idea of modeling the document collection as a bipartite graph between documents and words, using which the simultaneous clustering problem can be posed as a bipartite graph partitioning problem. To solve the partitioning problem, we use a new spectral co-clustering algorithm that uses the second left and right singular vectors of an appropriately scaled word-document matrix to yield good bipartitionings. The spectral algorithm enjoys some optimality properties; it can be shown that the singular vectors solve a real relaxation to the NP-complete graph bipartitioning problem. We present experimental results to verify that the resulting co-clustering algorithm works well in practice.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {269–274},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502551,
author = {Ding, Chris H. Q. and He, Xiaofeng and Zha, Hongyuan},
title = {A Spectral Method to Separate Disconnected and Nearly-Disconnected Web Graph Components},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502551},
doi = {10.1145/502512.502551},
abstract = {Separation of connected components from a graph with disconnected graph components mostly use breadth-first search (BFS) or depth-first search (DFS) graph algorithms. Here we propose a new algebraic method to separate disconnected and nearly-disconnected components. This method is based on spectral graph partitioning, following a key observation that disconnected components will show up, after properly sorted, as step-function like curve in the lowest eigenvectors of the Laplacian matrix of the graph. Following an perturbative analysis framework, we systematically analyzed the graph structures, first on the disconnected subgraph case, and second on the effects of adding edges sparsely connecting different subgraphs as a perturbation. Several new results are derived, providing insights to spectral methods and related clustering objective function. Examples are given illustrating the concepts and results our methods. Comparing to the standard graph algorithms, this method has the same O(‖E ‖ + ‖V‖log(‖V‖)) complexity, but is easier to implement (using readily available eigensolvers). Further more the method can easily identify articulation points and bridges on nearly-disconnected graphs. Segmentation of a real example of Web graph for query amazon is given. We found that each disconnected or nearly-disconnected components forms a cluster on a clear topic.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {275–280},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502552,
author = {Harel, David and Koren, Yehuda},
title = {Clustering Spatial Data Using Random Walks},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502552},
doi = {10.1145/502512.502552},
abstract = {Discovering significant patterns that exist implicitly in huge spatial databases is an important computational task. A common approach to this problem is to use cluster analysis. We propose a novel approach to clustering, based on the deterministic analysis of random walks on a weighted graph generated from the data. Our approach can decompose the data into arbitrarily shaped clusters of different sizes and densities, overcoming noise and outliers that may blur the natural decomposition of the data. The method requires only O(n log n) time, and one of its variants needs only constant space.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {281–286},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502553,
author = {Indurkhya, Nitin and Weiss, Sholom M.},
title = {Solving Regression Problems with Rule-Based Ensemble Classifiers},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502553},
doi = {10.1145/502512.502553},
abstract = {We describe a lightweight learning method that induces an ensemble of decision-rule solutions for regression problems. Instead of direct prediction of a continuous output variable, the method discretizes the variable by k-means clustering and solves the resultant classification problem. Predictions on new examples are made by averaging the mean values of classes with votes that are close in number to the most likely class. We provide experimental evidence that this indirect approach can often yield strong results for many applications, generally outperforming direct approaches such as regression trees and rivaling bagged regression trees.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {287–292},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502554,
author = {Jin, Wen and Tung, Anthony K. H. and Han, Jiawei},
title = {Mining Top-n Local Outliers in Large Databases},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502554},
doi = {10.1145/502512.502554},
abstract = {Outlier detection is an important task in data mining with numerous applications, including credit card fraud detection, video surveillance, etc. A recent work on outlier detection has introduced a novel notion of local outlier in which the degree to which an object is outlying is dependent on the density of its local neighborhood, and each object can be assigned a Local Outlier Factor (LOF) which represents the likelihood of that object being an outlier. Although the concept of local outliers is a useful one, the computation of LOF values for every data objects requires a large number of κ-nearest neighbors searches and can be computationally expensive. Since most objects are usually not outliers, it is useful to provide users with the option of finding only n most outstanding local outliers, i.e., the top-n data objects which are most likely to be local outliers according to their LOFs. However, if the pruning is not done carefully, finding top-n outliers could result in the same amount of computation as finding LOF for all objects. In this paper, we propose a novel method to efficiently find the top-n local outliers in large databases. The concept of "micro-cluster" is introduced to compress the data. An efficient micro-cluster-based local outlier mining algorithm is designed based on this concept. As our algorithm can be adversely affected by the overlapping in the micro-clusters, we proposed a meaningful cut-plane solution for overlapping data. The formal analysis and experiments show that this method can achieve good performance in finding the most outstanding local outliers.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {293–298},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502555,
author = {Kalton, Annaka and Langley, Pat and Wagstaff, Kiri and Yoo, Jungsoon},
title = {Generalized Clustering, Supervised Learning, and Data Assignment},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502555},
doi = {10.1145/502512.502555},
abstract = {Clustering algorithms have become increasingly important in handling and analyzing data. Considerable work has been done in devising effective but increasingly specific clustering algorithms. In contrast, we have developed a generalized framework that accommodates diverse clustering algorithms in a systematic way. This framework views clustering as a general process of iterative optimization that includes modules for supervised learning and instance assignment. The framework has also suggested several novel clustering methods. In this paper, we investigate experimentally the efficacy of these algorithms and test some hypotheses about the relation between such unsupervised techniques and the supervised methods embedded in them.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {299–304},
numpages = {6},
keywords = {supervised learning, iterative optimization, Clustering},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502556,
author = {Lambert, Diane and Pinheiro, Jos\'{e} C.},
title = {Mining a Stream of Transactions for Customer Patterns},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502556},
doi = {10.1145/502512.502556},
abstract = {Transaction data can arrive at a ferocious rate in the order that transactions are completed. The data contain an enormous amount of information about customers, not just transactions, but extracting up-to-date customer information from an ever changing stream of data and mining it in real-time is a challenge. This paper describes a statistically principled approach to designing short, accurate summaries or signatures of high dimensional customer behavior that can be kept current with a stream of transactions. A signature database can then be used for data mining and to provide approximate answers to many kinds of queries about current customers quickly and accurately, as an empirical study of the calling patterns of 96,000 wireless customers who made about 18 million wireless calls over a three month period shows.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {305–310},
numpages = {6},
keywords = {signatures, histograms, massive data, incremental updates, customer profiles, Approximate queries, dynamic database},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502557,
author = {Lazarevic, Aleksandar and Obradovic, Zoran},
title = {The Distributed Boosting Algorithm},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502557},
doi = {10.1145/502512.502557},
abstract = {In this paper, we propose a general framework for distributed boosting intended for efficient integrating specialized classifiers learned over very large and distributed homogeneous databases that cannot be merged at a single location. Our distributed boosting algorithm can also be used as a parallel classification technique, where a massive database that cannot fit into main computer memory is partitioned into disjoint subsets for a more efficient analysis. In the proposed method, at each boosting round the classifiers are first learned from disjoint datasets and then exchanged amongst the sites. Finally the classifiers are combined into a weighted voting ensemble on each disjoint data set. The ensemble that is applied to an unseen test set represents an ensemble of ensembles built on all distributed sites. In experiments performed on four large data sets the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring less memory and less computational time. In addition, the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large-scale databases.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {311–316},
numpages = {6},
keywords = {Boosting, distributed learning, classifier ensembles},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502558,
author = {Lin, Dekang and Pantel, Patrick},
title = {Induction of Semantic Classes from Natural Language Text},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502558},
doi = {10.1145/502512.502558},
abstract = {Many applications dealing with textual information require classification of words into semantic classes (or concepts). However, manually constructing semantic classes is a tedious task. In this paper, we present an algorithm, UNICON, for UNsupervised Induction of CONcepts. Some advantages of UNICON over previous approaches include the ability to classify words with low frequency counts, the ability to cluster a large number of elements in a high-dimensional space, and the ability to classify previously unknown words into existing clusters. Furthermore, since the algorithm is unsupervised, a set of concepts may be constructed for any corpus.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {317–322},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502559,
author = {Lin, Dekang and Pantel, Patrick},
title = {DIRT @SBT@discovery of Inference Rules from Text},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502559},
doi = {10.1145/502512.502559},
abstract = {In this paper, we propose an unsupervised method for discovering inference rules from text, such as "X is author of Y ≈ X wrote Y", "X solved Y ≈ X found a solution to Y", and "X caused Y ≈ Y is triggered by X". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {323–328},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502560,
author = {Liu, Bing and Hsu, Wynne and Ma, Yiming},
title = {Identifying Non-Actionable Association Rules},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502560},
doi = {10.1145/502512.502560},
abstract = {Building predictive models and finding useful rules are two important tasks of data mining. While building predictive models has been well studied, finding useful rules for action still presents a major problem. A main obstacle is that many data mining algorithms often produce too many rules. Existing research has shown that most of the discovered rules are actually redundant or insignificant. Pruning techniques have been developed to remove those spurious and/or insignificant rules. In this paper, we argue that being a significant rule (or a non-redundant rule), however, does not mean that it is a potentially useful rule for action. Many significant rules (unpruned rules) are in fact not actionable. This paper studies this issue and presents an efficient algorithm to identify these non-actionable rules. Experiment results on many real-life datasets show that the number of non-actionable rules is typically quite large. The proposed technique thus enables the user to focus on fewer rules and to be assured that the remaining rules are non-redundant and potentially useful for action.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {329–334},
numpages = {6},
keywords = {rule interestingness, Non-actionable rules},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502561,
author = {Liu, Bing and Hsu, Wynne and Ma, Yiming},
title = {Discovering the Set of Fundamental Rule Changes},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502561},
doi = {10.1145/502512.502561},
abstract = {The world around us changes constantly. Knowing what has changed is an important part of our lives. For businesses, recognizing changes is also crucial. It allows businesses to adapt themselves to the changing market needs. In this paper, we study changes of association rules from one time period to another. One approach is to compare the supports and/or confidences of each rule in the two time periods and report the differences. This technique, however, is too simplistic as it tends to report a huge number of rule changes, and many of them are, in fact, simply the snowball effect of a small subset of fundamental changes. Here, we present a technique to highlight the small subset of fundamental changes. A change is fundamental if it cannot be explained by some other changes. The proposed technique has been applied to a number of real-life datasets. Experiments results show that the number of rules whose changes are unexplainable is quite small (about 20% of the total number of changes discovered), and many of these unexplainable changes reflect some fundamental shifts in the application domain.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {335–340},
numpages = {6},
keywords = {Change mining, data mining},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502562,
author = {Mannila, Heikki and Salmenkivi, Marko},
title = {Finding Simple Intensity Descriptions from Event Sequence Data},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502562},
doi = {10.1145/502512.502562},
abstract = {Sequences of events are an important type of data arising in various applications, including telecommunications, bio-statistics, web access analysis, etc. A basic approach to modeling such sequences is to find the underlying intensity functions describing the expected number of events per time unit. Typically, the intensity functions are assumed to be piecewise constant. We therefore consider different ways of fitting intensity models to event sequence data. We start by considering a Bayesian approach using Markov chain Monte Carlo (MCMC) methods with varying number of pieces. These methods can be used to produce posterior distributions on the intensity functions and they can also accomodate covariates. The drawback is that they are computationally intensive and thus are not very suitable for data mining applications in which large numbers of intensity functions have to be estimated. We consider dynamic programming approaches to finding the change points in the intensity functions. These methods can find the maximum likelihood intensity function in O(n2k) time for a sequence of n events and k different pieces of intensity. We show that simple heuristics can be used to prune the number of potential change points, yielding speedups of several orders of magnitude. The results of the improved dynamic programming method correspond very closely with the posterior averages produced by the MCMC methods.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {341–346},
numpages = {6},
keywords = {event sequence, intensity modeling, MCMC},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502563,
author = {Moody, Jonathan and Silva, Ricardo and Vanderwaart, Joseph},
title = {Data Filtering for Automatic Classification of Rocks from Reflectance Spectra},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502563},
doi = {10.1145/502512.502563},
abstract = {The ability to identify the mineral composition of rocks and soils is an important tool for the exploration of geological sites. For instance, NASA intends to design robots that are sufficiently autonomous to perform this task on planetary missions. Spectrometer readings provide one important source of data for identifying sites with minerals of interest. Reflectance spectrometers measure intensities of light reflected from surfaces over a range of wavelengths. Spectral intensity patterns may in some cases be sufficiently distinctive for proper identification of minerals or classes of minerals. For some mineral classes, carbonates for example, specific short spectral intervals are known to carry a distinctive signature. Finding similar distinctive spectral ranges for other mineral classes is not an easy problem. We propose and evaluate data-driven techniques that automatically search for spectral ranges optimized for specific minerals. In one set of studies, we partition the whole interval of wavelengths available in our data into sub-intervals, or bins, and use a genetic algorithm to evaluate a candidate selection of subintervals. As alternatives to this computationally expensive search technique, we present an entropy-based heuristic that gives higher scores for wavelengths more likely to distinguish between classes, as well as other greedy search procedures. Results are presented for four different classes, showing reasonable improvements in identifying some, but not all, of the mineral classes tested.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {347–352},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502564,
author = {Morimoto, Yasuhiko},
title = {Mining Frequent Neighboring Class Sets in Spatial Databases},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502564},
doi = {10.1145/502512.502564},
abstract = {We consider the problem of finding neighboring class sets. Objects of each instance of a neighboring class set are grouped using their Euclidean distances from each other. Recently, location-based services are growing along with mobile computing infrastructure such as cellular phones and PDAs. Therefore, we expect to see the development of spatial databases that contains very large number of access records including location information. The most typical type would be a database of point objects. Records of the objects may consist of "requested service name," "number of packet transmitted" in addition to x and y coordinate values indicating where the request came from. The algorithm presented here efficiently finds sets of "service names" that were frequently close to each other in the spatial database. For example, it may find a frequent neighboring class set, where "ticket" and "timetable" are frequently requested close to each other. By recognizing this, location-based service providers can promote a "ticket" service for customers who access the "timetable."},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {353–358},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502565,
author = {Oza, Nikunj C. and Russell, Stuart},
title = {Experimental Comparisons of Online and Batch Versions of Bagging and Boosting},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502565},
doi = {10.1145/502512.502565},
abstract = {Bagging and boosting are well-known ensemble learning methods. They combine multiple learned base models with the aim of improving generalization performance. To date, they have been used primarily in batch mode, i.e., they require multiple passes through the training data. In previous work, we presented online bagging and boosting algorithms that only require one pass through the training data and presented experimental results on some relatively small datasets. Through additional experiments on a variety of larger synthetic and real datasets, this paper demonstrates that our online versions perform comparably to their batch counterparts in terms of classification accuracy. We also demonstrate the substantial reduction in running time we obtain with our online algorithms because they require fewer passes through the training data.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {359–364},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502566,
author = {Sevon, Petteri and Toivonen, Hannu T. T. and Ollikainen, Vesa},
title = {TreeDT: Gene Mapping by Tree Disequilibrium Test},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502566},
doi = {10.1145/502512.502566},
abstract = {We introduce and evaluate TreeDT, a novel gene mapping method which is based on discovering and assessing tree-like patterns in genetic marker data. Gene mapping aims at discovering a statistical connection from a particular disease or trait to a narrow region in the genome. In a typical case-control setting, data consists of genetic markers typed for a set of disease-associated chromosomes and a set of control chromosomes. A computer scientist would view this data as a set of strings.TreeDT extracts, essentially in the form of substrings and prefix trees, information about the historical recombinations in the population. This information is used to locate fragments potentially inherited from a common diseased founder, and to map the disease gene into the most likely such fragment. The method measures for each chromosomal location the disequilibrium of the prefix tree of marker strings starting from the location, to assess the distribution of disease-associated chromosomes.We evaluate experimentally the performance of TreeDT on realistic, simulated data sets, and comparisons to state of the art methods (TDT, HPM) show that TreeDT is very competitive.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {365–370},
numpages = {6},
keywords = {prefix trees, permutation tests, algorithms, Gene mapping},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502567,
author = {Shekhar, Shashi and Lu, Chang-Tien and Zhang, Pusheng},
title = {Detecting Graph-Based Spatial Outliers: Algorithms and Applications (a Summary of Results)},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502567},
doi = {10.1145/502512.502567},
abstract = {Identification of outliers can lead to the discovery of unexpected, interesting, and useful knowledge. Existing methods are designed for detecting spatial outliers in multidimensional geometric data sets, where a distance metric is available. In this paper, we focus on detecting spatial outliers in graph structured data sets. We define statistical tests, analyze the statistical foundation underlying our approach, design several fast algorithms to detect spatial outliers, and provide a cost model for outlier detection procedures. In addition, we provide experimental results from the application of our algorithms on a Minneapolis-St.Paul(Twin Cities) traffic dataset to show their effectiveness and usefulness.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {371–376},
numpages = {6},
keywords = {Spatial Graphs, Outlier Detection, Spatial Data Mining},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502568,
author = {Street, W. Nick and Kim, YongSeog},
title = {A Streaming Ensemble Algorithm (SEA) for Large-Scale Classification},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502568},
doi = {10.1145/502512.502568},
abstract = {Ensemble methods have recently garnered a great deal of attention in the machine learning community. Techniques such as Boosting and Bagging have proven to be highly effective but require repeated resampling of the training data, making them inappropriate in a data mining context. The methods presented in this paper take advantage of plentiful data, building separate classifiers on sequential chunks of training points. These classifiers are combined into a fixed-size ensemble using a heuristic replacement strategy. The result is a fast algorithm for large-scale or streaming data that classifies as well as a single decision tree built on all the data, requires approximately constant memory, and adjusts quickly to concept drift.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {377–382},
numpages = {6},
keywords = {streaming data, ensemble classification},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502569,
author = {Webb, Geoffrey I.},
title = {Discovering Associations with Numeric Variables},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502569},
doi = {10.1145/502512.502569},
abstract = {This paper further develops Aumann and Lindell's [3] proposal for a variant of association rules for which the consequent is a numeric variable. It is argued that these rules can discover useful interactions with numeric data that cannot be discovered directly using traditional association rules with discretization. Alternative measures for identifying interesting rules are proposed. Efficient algorithms are presented that enable these rules to be discovered for dense data sets for which application of Auman and Lindell's algorithm is infeasible.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {383–388},
numpages = {6},
keywords = {Numeric Data, Search, Association Rule, Impact Rule},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502570,
author = {Yamanishi, Kenji and Takeuchi, Jun-ichi},
title = {Discovering Outlier Filtering Rules from Unlabeled Data: Combining a Supervised Learner with an Unsupervised Learner},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502570},
doi = {10.1145/502512.502570},
abstract = {This paper is concerned with the problem of detecting outliers from unlabeled data. In prior work we have developed SmartSifter, which is an on-line outlier detection algorithm based on unsupervised learning from data. On the basis of SmartSifter this paper yields a new framework for outlier filtering using both supervised and unsupervised learning techniques iteratively in order to make the detection process more effective and more understandable. The outline of the framework is as follows: In the first round, for an initial dataset, we run SmartSifter to give each data a score, with a high score indicating a high possibility of being an outlier. Next, giving positive labels to a number of higher scored data and negative labels to a number of lower scored data, we create labeled examples. Then we construct an outlier filtering rule by supervised learning from them. Here the rule is generated based on the principle of minimizing extended stochastic complexity. In the second round, for a new dataset, we filter the data using the constructed rule, then among the filtered data, we run SmartSifter again to evaluate the data in order to update the filtering rule. Applying of our framework to the network intrusion detection, we demonstrate that 1) it can significantly improve the accuracy of SmartSifter, and 2) outlier filtering rules can help the user to discover a general pattern of an outlier group.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {389–394},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502571,
author = {Yang, Jiong and Wang, Wei and Yu, Philip S.},
title = {Infominer: Mining Surprising Periodic Patterns},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502571},
doi = {10.1145/502512.502571},
abstract = {In this paper, we focus on mining surprising periodic patterns in a sequence of events. In many applications, e.g., computational biology, an infrequent pattern is still considered very significant if its actual occurrence frequency exceeds the prior expectation by a large margin. The traditional metric, such as support, is not necessarily the ideal model to measure this kind of surprising patterns because it treats all patterns equally in the sense that every occurrence carries the same weight towards the assessment of the significance of a pattern regardless of the probability of occurrence. A more suitable measurement, information, is introduced to naturally value the degree of surprise of each occurrence of a pattern as a continuous and monotonically decreasing function of its probability of occurrence. This would allow patterns with vastly different occurrence probabilities to be handled seamlessly. As the accumulated degree of surprise of all repetitions of a pattern, the concept of information gain is proposed to measure the overall degree of surprise of the pattern within a data sequence. The bounded information gain property is identified to tackle the predicament caused by the violation of the downward closure property by the information gain measure and in turn provides an efficient solution to this problem. Empirical tests demonstrate the efficiency and the usefulness of the proposed model.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {395–400},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502572,
author = {Zheng, Zijian and Kohavi, Ron and Mason, Llew},
title = {Real World Performance of Association Rule Algorithms},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502572},
doi = {10.1145/502512.502572},
abstract = {This study compares five well-known association rule algorithms using three real-world datasets and an artificial dataset. The experimental results confirm the performance improvements previously claimed by the authors on the artificial data, but some of these gains do not carry over to the real datasets, indicating overfitting of the algorithms to the IBM artificial dataset. More importantly, we found that the choice of algorithm only matters at support levels that generate more rules than would be useful in practice. For support levels that generate less than 1,000,000 rules, which is much more than humans can handle and is sufficient for prediction purposes where data is loaded into RAM, Apriori finishes processing in less than 10 minutes. On our datasets, we observed super-exponential growth in the number of rules. On one of our datasets, a 0.02% change in the support increased the number of rules from less than a million to over a billion, implying that outside a very narrow range of support values, the choice of algorithm is irrelevant.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {401–406},
numpages = {6},
keywords = {Comparisons, Association Rules, Benchmark, Frequent Itemsets, Affinity Analysis, Market Basket Analysis, Data Mining},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502573,
author = {Apte, C. and Bibelnieks, E. and Natarajan, R. and Pednault, E. and Tipu, F. and Campbell, D. and Nelson, B.},
title = {Segmentation-Based Modeling for Advanced Targeted Marketing},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502573},
doi = {10.1145/502512.502573},
abstract = {Fingerhut Business Intelligence (BI) has a long and successful history of building statistical models to predict consumer behavior. The models constructed are typically segmentation-based models in which the target audience is split into subpopulations (i.e., customer segments) and individually tailored statistical models are then developed for each segment. Such models are commonly employed in the direct-mail industry; however, segmentation is often performed on an ad-hoc basis without directly considering how segmentation affects the accuracy of the resulting segment models. Fingerhut BI approached IBM Research with the problem of how to build segmentation-based models more effectively so as to maximize predictive accuracy. The IBM Advanced Targeted Marketing-Single EventsTM (IBM ATM-SETM) solution is the result of IBM Research and Fingerhut BI directing their efforts jointly towards solving this problem. This paper presents an evaluation of ATM-SE's modeling capabilities using data from Fingerhut's catalog mailings.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {408–413},
numpages = {6},
keywords = {logistic regression, Segmentation-based models, feature selection, decision trees, targeted marketing, linear regression},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502574,
author = {Berkhin, Pavel and Beche, Jonathan D. and Randall, Dee Jay},
title = {Interactive Path Analysis of Web Site Traffic},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502574},
doi = {10.1145/502512.502574},
abstract = {The goal of Path Analysis is to understand visitors' navigation of a Web site. The fundamental analysis component is a path. A path is a finite sequence of elements, typically representing URLs or groups of URLs. A full path is an abstraction of a visit or a session, which can contain attributes described below. Subpaths represent interesting subsequences of the full paths.Path Analysis provides user-configurable extraction, filtering, preprocessing, noise reduction, descriptive statistics and detailed analysis of three basic specific objects: elements, (sub)paths, and couples of elements. In each case, lists of frequent objects --- subject to particular filtering and sorting --- are available. We call the corresponding interactive tools Element, Path, and Couple Analyzers.We also allow in-depth exploration of individual elements, paths, and couples: Element Explorer investigates composition and convergence of traffic through an element and allows conditioning based on the number of preceding/succeeding steps. Path Explorer visualizes in and out flows of a path and attrition rate along the path. Couple Explorer presents distinct paths connecting couple elements, along with measures of their association and some additional statistics.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {414–419},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502575,
author = {Datta, Piew and Drew, James H. and Betz, Andrew and Mani, D. R. and Howard, Jeffery},
title = {Estimating Business Targets},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502575},
doi = {10.1145/502512.502575},
abstract = {Determining and setting maximal revenue expectations or other business performance targets---whether it is for regional company divisions or individual customers---can have profound financial implications. Operational techniques are changed, staffing levels are altered and management attention is re-focused---all in the name of expectations. In practice these expectations are often derived in an ad hoc manner. To address this unsupervised task, we combine nearest neighbor methods and classical statistical methods and derive a new solution to the classical econometric task of frontier analysis. We apply our methodology to two real world business problems in Verizon, a major telecommunications provider in the United States, more specifically in the print yellow page division Verizon Information Services: (1) identifying under marketed customers for targeted upselling campaigns and focused sales attention, and (2) benchmarking regional directory divisions to incent performance improvements. Our analysis uncovers some commercially useful aspects of these domains and by conservative estimates can increase revenue by several million dollars in each domain.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {420–425},
numpages = {6},
keywords = {maximal value estimation, Nearest neighbor, frontier analysis},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502576,
author = {Elkan, Charles},
title = {Magical Thinking in Data Mining: Lessons from CoIL Challenge 2000},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502576},
doi = {10.1145/502512.502576},
abstract = {CoIL challenge 2000 was a supervised learning contest that attracted 43 entries. The authors of 29 entries later wrote explanations of their work. This paper discusses these reports and reaches three main conclusions. First, naive Bayesian classifiers remain competitive in practice: they were used by both the winning entry and the next best entry. Second, identifying feature interactions correctly is important for maximizing predictive accuracy: this was the difference between the winning classifier and all others. Third and most important, too many researchers and practitioners in data mining do not appreciate properly the issue of statistical significance and the danger of overfitting. Given a dataset such as the one for the CoIL contest, it is pointless to apply a very complicated learning algorithm, or to perform a very time-consuming model search. In either ease, one is likely to overfit the training data and to fool oneself in estimating predictive accuracy and in discovering useful correlations.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {426–431},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502577,
author = {Hotz, E. and Grimmer, U. and Heuser, W. and Nakhaeizadeh, G. and Wieczorek, M.},
title = {REVI-MINER, a KDD-Environment for Deviation Detection and Analysis of Warranty and Goodwill Cost Statements in Automotive Industry},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502577},
doi = {10.1145/502512.502577},
abstract = {REVI-MINER is a KDD-environment which supports the detection and analysis of deviations in warranty and goodwill cost statements. The system was developed within the framework of a cooperation between DaimlerChrysler Research &amp; Technology and Global Service and Parts (GSP) and is based upon the CRISP-DM methodology as a widely accepted process model for the solution of Data Mining problems. Also, we have implemented different approaches based on Machine learning and statistics which can be utilized for data cleaning in the preprocessing phase. The Data Mining models applied have been developed by using a statistical deviation detection approach. The tool supports controllers in their task of auditing the authorized repair shops. In this paper we describe the development phases which have led to REVI-MINER.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {432–437},
numpages = {6},
keywords = {Data Mining, deviation detection, data cleaning},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502578,
author = {Hueglin, Christoph and Vannotti, Francesco},
title = {Data Mining Techniques to Improve Forecast Accuracy in Airline Business},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502578},
doi = {10.1145/502512.502578},
abstract = {Predictive models developed by applying Data Mining techniques are used to improve forecasting accuracy in the airline business. In order to maximize the revenue on a flight, the number of seats available for sale is typically higher than the physical seat capacity (overbooking). To optimize the overbooking rate, an accurate estimation of the number of no-show passengers (passengers who hold a valid booking but do not appear at the gate to board for the flight) is essential. Currently, no-shows on future flights are estimated from the number of no-shows on historical flights averaged on booking class level. In this work, classification trees and logistic regression models are applied to estimate the probability that an individual passenger turns out to be a no-show. Passenger information stored in the reservation system of the airline is either directly used as explanatory variable or used to create attributes that have an impact on the probability of a passenger to be a no-show. The total number of no-shows in each booking class or on the total flight is then obtained by accumulating the individual no-show probabilities over the entity of interest. We show that this forecasting approach is more accurate than the currently used method. In addition, the selected models lead to a deepened insight into passenger behavior.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {438–442},
numpages = {5},
keywords = {CART, Decision tree, logistic regression, predictive model},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502579,
author = {Li, Hang and Yamanishi, Kenji},
title = {Mining from Open Answers in Questionnaire Data},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502579},
doi = {10.1145/502512.502579},
abstract = {Surveys are an important part of marketing and customer relationship management, and open answers (i.e., answers to open questions) in particular may contain valuable information and provide an important basis for making business decisions. We have developed a text mining system that provides a new way for analyzing open answers in questionnaire data. The product is able to perform the following two functions: (A) accurate extraction of characteristics for individual analysis targets, (B) accurate extraction of the relationships among characteristics of analysis targets. In this paper, we describe the working of our text mining system. It employs two statistical learning techniques: rule analysis and Correspondence Analysis for performing the two functions. Our text mining system has already been put into use by a number of large corporations in Japan in the performance of text mining on various types of survey data, including open answers about brand images, open answers about company images, complaints about products, comments written on home pages, business reports, and help desk records. In this it has been found to be useful in forming a basis for effective business decisions.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {443–449},
numpages = {7},
keywords = {Open Question, Text Mining, Survey, Correspondence Analysis, Association Rules, Questionnaire Data, Classification Rules},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502580,
author = {Mah, Teresa and Hoek, Hank and Li, Ying},
title = {Funnel Report Mining for the MSN Network},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502580},
doi = {10.1145/502512.502580},
abstract = {Data mining research has long concentrated on the five main areas: clustering, association discovery, classification, forecasting and sequential patterns. Web data mining projects are concerned mainly with text mining, user segmentation, forecasting web usage and analyzing users' clickstream patterns. We present a new type of web usage mining called funnel analysis or funnel report mining. A funnel report is a study of the retention behavior among a series of pages or sites. For example, of all hits on the home page of www.msn.com, what percentages of those are followed by hits to moneycentral.msn.com? What percentage of www.msn.com hits are followed by moneycentral.msn.com, and then www.msnbc.com? What are the most interesting funnels starting with www.msn.com? Where does the greatest drop off rate occur after a user has hit MSNBC? Funnel reports are extremely useful in e-business because they give product planners an idea of how usable and well-structured their site is. From our experience performing web usage mining for the MSN network of sites, funnel reports are requested even more than user segmentation analyses, site affiliation studies and classification exercises. In this paper, we define a framework for funnel analysis and provide a tree-based solution we have been using successfully to extract all relevant funnels using only one scan of the data file.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {450–455},
numpages = {6},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502581,
author = {Rosset, Saharon and Neumann, Einat and Eick, Uri and Vatnik, Nurit and Idan, Izhak},
title = {Evaluation of Prediction Models for Marketing Campaigns},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502581},
doi = {10.1145/502512.502581},
abstract = {We consider prediction-model evaluation in the context of marketing-campaign planning. In order to evaluate and compare models with specific campaign objectives in mind, we need to concentrate our attention on the appropriate evaluation-criteria. These should portray the model's ability to score accurately and to identify the relevant target population. In this paper we discuss some applicable model-evaluation and selection criteria, their relevance for campaign planning, their robustness under changing population distributions, and their employment when constructing confidence intervals. We illustrate our results with a case study based on our experience from several projects.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {456–461},
numpages = {6},
keywords = {Model Evaluation, Marketing Campaigns, Confidence Intervals, Performance Measures},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502582,
author = {Spangler, Scott and Kreulen, Jeffrey},
title = {Knowledge Base Maintenance Using Knowledge Gap Analysis},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502582},
doi = {10.1145/502512.502582},
abstract = {As the web and e-business have proliferated, the practice of using customer facing knowledge bases to augment customer service and support operations has increased. This can be a very efficient, scalable and cost effective way to share knowledge. The effectiveness and cost savings are proportional to the utility of the information within the knowledge base and inversely proportional to the amount of labor required in maintaining the knowledge. To address this issue, we have developed an algorithm and methodology to increase the utility of the information within a knowledge base while greatly reducing the labor required.In this paper, we describe an implementation of an algorithm and methodology for comparing a knowledge base to a set of problem tickets to determine which categories and subcategories are not well addressed within the knowledge base. We utilize text clustering on problem ticket text to determine a set of problem categories. We then compare each knowledge base solution document to each problem category centroid using a cosine distance metric. The distance between the "closest" solution document and the corresponding centroid becomes the basis of that problem category's "knowledge gap". Our claim is that this gap metric serves as a useful method for quickly and automatically determining which problem categories have no relevant solutions in a knowledge base. We have implemented our approach, and we present the results of performing a knowledge gap analysis on a set of support center problem tickets.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {462–466},
numpages = {5},
keywords = {Text Mining, Gap Analysis, Clustering, Knowledge Management},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502583,
author = {Warner, Doug and Richter, J. Neal and Durbin, Stephen D. and Banerjee, Bikramjit},
title = {Mining User Session Data to Facilitate User Interaction with a Customer Service Knowledge Base in RightNow Web},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502583},
doi = {10.1145/502512.502583},
abstract = {RightNow Web is an integrated software package for web-based customer service that has, at its core, a database of answers to frequently asked questions (FAQs). One major design goal is to facilitate end-user interaction with this dynamic document collection, i.e. make it as easy and efficient as possible for users to browse the collection and locate desired information. To this end, we perform several types of analysis on the session tracking database that records user navigation histories. First, using both explicit and implicit measures of user satisfaction, we infer a "solved count" representing the average utility of an FAQ. Second, using the user navigation patterns we construct a link matrix representing connections between FAQs. The technique of building up the link matrix and using it to advise users on related information amounts to a form of the "swarm intelligence" method of finding optimal paths. Both solved count and the link matrix are continuously updated as users interact with the site; furthermore, they are periodically "aged" to emphasize recent activity. The synergistic combination of these techniques allows users to learn from the database in a more effective manner, as evidenced by usage statistics.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {467–472},
numpages = {6},
keywords = {User session, clustering, collaborative filtering, customer service, multi-agent system, self-help, user data},
location = {San Francisco, California},
series = {KDD '01}
}

@inproceedings{10.1145/502512.502584,
author = {Yang, Qiang and Zhang, Haining Henry and Li, Tianyi},
title = {Mining Web Logs for Prediction Models in WWW Caching and Prefetching},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502584},
doi = {10.1145/502512.502584},
abstract = {Web caching and prefetching are well known strategies for improving the performance of Internet systems. When combined with web log mining, these strategies can decide to cache and prefetch web documents with higher accuracy. In this paper, we present an application of web log mining to obtain web-document access patterns and use these patterns to extend the well-known GDSF caching policies and prefetching policies. Using real web logs, we show that this application of data mining can achieve dramatic improvement to web-access performance.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {473–478},
numpages = {6},
keywords = {Web Log Mining, Application to Caching and Prefetching on the WWW},
location = {San Francisco, California},
series = {KDD '01}
}

