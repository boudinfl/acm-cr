@inproceedings{10.1145/1081870.1081875,
author = {Chen, Rong and Herskovits, Edward H.},
title = {A Bayesian Network Classifier with Inverse Tree Structure for Voxelwise Magnetic Resonance Image Analysis},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081875},
doi = {10.1145/1081870.1081875},
abstract = {We propose a Bayesian-network classifier with inverse-tree structure (BNCIT) for joint classification and variable selection. The problem domain of voxelwise magnetic-resonance image analysis often involves millions of variables but only dozens of samples. Judicious variable selection may render classification tractable, avoid over-fitting, and improve classifier performance. BNCIT embeds the variable-selection process within the classifier-training process, which makes this algorithm scalable. BNCIT is based on a Bayesian-network model with inverse-tree structure, i.e., the class variable C is a leaf node, and predictive variables are parents of C; thus, the classifier-training process returns a parent set for C, which is a subset of the Markov blanket of C. BNCIT uses voxels in the parent set, and voxels that are probabilistically equivalent to them, as variables for classification of new image data. Since the data set has a limited number of samples, we use the jackknife method to determine whether the classifier generated by BNCIT is a statistical artifact. In order to enhance stability and improve classification accuracy, we model the state of the probabilistically equivalent voxels with a latent variable. We employ an efficient method for determining states of hidden variables, thus reducing dramatically the computational cost of model generation. Experimental results confirm the accuracy and efficiency of BNCIT.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {4–12},
numpages = {9},
keywords = {Bayesian network, classifier, magnetic resonance image, Markov blanket},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081876,
author = {Dasgupta, Anirban and Kumar, Ravi and Raghavan, Prabhakar and Tomkins, Andrew},
title = {Variable Latent Semantic Indexing},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081876},
doi = {10.1145/1081870.1081876},
abstract = {Latent Semantic Indexing is a classical method to produce optimal low-rank approximations of a term-document matrix. However, in the context of a particular query distribution, the approximation thus produced need not be optimal. We propose VLSI, a new query-dependent (or "variable") low-rank approximation that minimizes approximation error for any specified query distribution. With this tool, it is possible to tailor the LSI technique to particular settings, often resulting in vastly improved approximations at much lower dimensionality. We validate this method via a series of experiments on classical corpora, showing that VLSI typically performs similarly to LSI with an order of magnitude fewer dimensions.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {13–21},
numpages = {9},
keywords = {LSI, matrix approximation, VLSI, SVD, linear algebra},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081877,
author = {Fan, Jianping and Luo, Hangzai and Hacid, Mohand-Said},
title = {Mining Images on Semantics via Statistical Learning},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081877},
doi = {10.1145/1081870.1081877},
abstract = {In this paper, we have proposed a novel framework to enable hierarchical image classification via statistical learning. By integrating the concept hierarchy for semantic image concept organization, a hierarchical mixture model is proposed to enable multi-level modeling of semantic image concepts and hierarchical classifier combination. Thus, learning the classifiers for the semantic image concepts at the high level of the concept hierarchy can be effectively achieved by detecting the presences of the relevant base-level atomic image concepts. To effectively learn the base-level classifiers for the atomic image concepts at the first level of the concept hierarchy, we have proposed a novel adaptive EM algorithm to achieve more effective model selection and parameter estimation. In addition, a novel penalty term is proposed to effectively eliminate the misleading effects of the outlying unlabeled images on semi-supervised classifier training. Our experimental results in a specific image domain of outdoor photos are very attractive.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {22–31},
numpages = {10},
keywords = {adaptive EM algorithm, image classification, hierarchical mixture model},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081878,
author = {Fung, Glenn and Sandilya, Sathyakama and Rao, R. Bharat},
title = {Rule Extraction from Linear Support Vector Machines},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081878},
doi = {10.1145/1081870.1081878},
abstract = {We describe an algorithm for converting linear support vector machines and any other arbitrary hyperplane-based linear classifiers into a set of non-overlapping rules that, unlike the original classifier, can be easily interpreted by humans. Each iteration of the rule extraction algorithm is formulated as a constrained optimization problem that is computationally inexpensive to solve. We discuss various properties of the algorithm and provide proof of convergence for two different optimization criteria We demonstrate the performance and the speed of the algorithm on linear classifiers learned from real-world datasets, including a medical dataset on detection of lung cancer from medical images. The ability to convert SVM's and other "black-box" classifiers into a set of human-understandable rules, is critical not only for physician acceptance, but also to reducing the regulatory barrier for medical-decision support systems based on such classifiers.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {32–40},
numpages = {9},
keywords = {medical decision-support, rule extraction, linear classifiers, mathematical programming},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081879,
author = {Gao, Bin and Liu, Tie-Yan and Zheng, Xin and Cheng, Qian-Sheng and Ma, Wei-Ying},
title = {Consistent Bipartite Graph Co-Partitioning for Star-Structured High-Order Heterogeneous Data Co-Clustering},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081879},
doi = {10.1145/1081870.1081879},
abstract = {Heterogeneous data co-clustering has attracted more and more attention in recent years due to its high impact on various applications. While the co-clustering algorithms for two types of heterogeneous data (denoted by pair-wise co-clustering), such as documents and terms, have been well studied in the literature, the work on more types of heterogeneous data (denoted by high-order co-clustering) is still very limited. As an attempt in this direction, in this paper, we worked on a specific case of high-order co-clustering in which there is a central type of objects that connects the other types so as to form a star structure of the inter-relationships. Actually, this case could be a very good abstract for many real-world applications, such as the co-clustering of categories, documents and terms in text mining. In our philosophy, we treated such kind of problems as the fusion of multiple pair-wise co-clustering sub-problems with the constraint of the star structure. Accordingly, we proposed the concept of consistent bipartite graph co-partitioning, and developed an algorithm based on semi-definite programming (SDP) for efficient computation of the clustering results. Experiments on toy problems and real data both verified the effectiveness of our proposed method.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {41–50},
numpages = {10},
keywords = {co-clustering, consistency, spectral graph, high-order heterogeneous data},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081880,
author = {Gionis, Aristides and Hinneburg, Alexander and Papadimitriou, Spiros and Tsaparas, Panayiotis},
title = {Dimension Induced Clustering},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081880},
doi = {10.1145/1081870.1081880},
abstract = {It is commonly assumed that high-dimensional datasets contain points most of which are located in low-dimensional manifolds. Detection of low-dimensional clusters is an extremely useful task for performing operations such as clustering and classification, however, it is a challenging computational problem. In this paper we study the problem of finding subsets of points with low intrinsic dimensionality. Our main contribution is to extend the definition of fractal correlation dimension, which measures average volume growth rate, in order to estimate the intrinsic dimensionality of the data in local neighborhoods. We provide a careful analysis of several key examples in order to demonstrate the properties of our measure. Based on our proposed measure, we introduce a novel approach to discover clusters with low dimensionality. The resulting algorithms extend previous density based measures, which have been successfully used for clustering. We demonstrate the effectiveness of our algorithms for discovering low-dimensional m-flats embedded in high dimensional spaces, and for detecting low-rank sub-matrices.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {51–60},
numpages = {10},
keywords = {fractal dimension, clustering},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081881,
author = {Goethals, Bart and Hoekx, Eveline and Van den Bussche, Jan},
title = {Mining Tree Queries in a Graph},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081881},
doi = {10.1145/1081870.1081881},
abstract = {We present an algorithm for mining tree-shaped patterns in a large graph. Novel about our class of patterns is that they can contain constants, and can contain existential nodes which are not counted when determining the number of occurrences of the pattern in the graph. Our algorithm has a number of provable optimality properties, which are based on the theory of conjunctive database queries. We propose a database-oriented implementation in SQL, and report upon some initial experimental results obtained with our implementation on graph data about food webs, about protein interactions, and about citation analysis.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {61–69},
numpages = {9},
keywords = {conjunctive query, redundancy checking, canonical form, tree query, graph, equivalence checking, levelwise, SQL},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081882,
author = {Gondek, David and Hofmann, Thomas},
title = {Non-Redundant Clustering with Conditional Ensembles},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081882},
doi = {10.1145/1081870.1081882},
abstract = {Data may often contain multiple plausible clusterings. In order to discover a clustering which is useful to the user, constrained clustering techniques have been proposed to guide the search. Typically, these techniques assume background knowledge in the form of explicit information about the desired clustering. In contrast, we consider the setting in which the background knowledge is instead about an undesired clustering. Such knowledge may be obtained from an existing classification or precedent algorithm. The problem is then to find a novel, "orthogonal" clustering in the data. We present a general algorithmic framework which makes use of cluster ensemble methods to solve this problem. One key advantage of this approach is that it takes a base clustering method which is used as a black box, allowing the practitioner to select the most appropriate clustering method for the domain. We present experimental results on synthetic and text data which establish the competitiveness of this framework.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {70–77},
numpages = {8},
keywords = {non-redundant clustering, cluster ensembles},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081883,
author = {Gruhl, Daniel and Guha, R. and Kumar, Ravi and Novak, Jasmine and Tomkins, Andrew},
title = {The Predictive Power of Online Chatter},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081883},
doi = {10.1145/1081870.1081883},
abstract = {An increasing fraction of the global discourse is migrating online in the form of blogs, bulletin boards, web pages, wikis, editorials, and a dizzying array of new collaborative technologies. The migration has now proceeded to the point that topics reflecting certain individual products are sufficiently popular to allow targeted online tracking of the ebb and flow of chatter around these topics. Based on an analysis of around half a million sales rank values for 2,340 books over a period of four months, and correlating postings in blogs, media, and web pages, we are able to draw several interesting conclusions.First, carefully hand-crafted queries produce matching postings whose volume predicts sales ranks. Second, these queries can be automatically generated in many cases. And third, even though sales rank motion might be difficult to predict in general, algorithmic predictors can use online postings to successfully predict spikes in sales rank.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {78–87},
numpages = {10},
keywords = {prediction, time-series analysis, sales rank, blogs},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081884,
author = {Guha, Sudipto and Harb, Boulos},
title = {Wavelet Synopsis for Data Streams: Minimizing Non-Euclidean Error},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081884},
doi = {10.1145/1081870.1081884},
abstract = {We consider the wavelet synopsis construction problem for data streams where given n numbers we wish to estimate the data by constructing a synopsis, whose size, say B is much smaller than n. The B numbers are chosen to minimize a suitable error between the original data and the estimate derived from the synopsis.Several good one-pass wavelet construction streaming algorithms minimizing the l2 error exist. For other error measures, the problem is less understood. We provide the first one-pass small space streaming algorithms with provable error guarantees (additive approximation) for minimizing a variety of non-Euclidean error measures including all weighted lp (including l∞) and relative error lp metrics.In several previous works solutions (for weighted l2, l∞ and maximum relative error) where the B synopsis coefficients are restricted to be wavelet coefficients of the data were proposed. This restriction yields suboptimal solutions on even fairly simple examples. Other lines of research, such as probabilistic synopsis, imposed restrictions on how the synopsis was arrived at. To the best of our knowledge this paper is the first paper to address the general problem, without any restriction on how the synopsis is arrived at, as well as provide the first streaming algorithms with guaranteed performance for these classes of error measures.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {88–97},
numpages = {10},
keywords = {wavelet synopses, streaming algorithm},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081885,
author = {Hershkop, Shlomo and Stolfo, Salvatore J.},
title = {Combining Email Models for False Positive Reduction},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081885},
doi = {10.1145/1081870.1081885},
abstract = {Machine learning and data mining can be effectively used to model, classify and discover interesting information for a wide variety of data including email. The Email Mining Toolkit, EMT, has been designed to provide a wide range of analyses for arbitrary email sources. Depending upon the task, one can usually achieve very high accuracy, but with some amount of false positive tradeoff. Generally false positives are prohibitively expensive in the real world. In the case of spam detection, for example, even if one email is misclassified, this may be unacceptable if it is a very important email. Much work has been done to improve specific algorithms for the task of detecting unwanted messages, but less work has been report on leveraging multiple algorithms and correlating models in this particular domain of email analysis.EMT has been updated with new correlation functions allowing the analyst to integrate a number of EMT's user behavior models available in the core technology. We present results of combining classifier outputs for improving both accuracy and reducing false positives for the problem of spam detection. We apply these methods to a very large email data set and show results of different combination methods on these corpora. We introduce a new method to compare multiple and combined classifiers, and show how it differs from past work. The method analyzes the relative gain and maximum possible accuracy that can be achieved for certain combinations of classifiers to automatically choose the best combination.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {98–107},
numpages = {10},
keywords = {multiple classifiers, spam, aggregators, model combination, false positive reduction, data mining, email mining},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081886,
author = {Jakulin, Aleks and Mo\v{z}ina, Martin and Dem\v{s}ar, Janez and Bratko, Ivan and Zupan, Bla\v{z}},
title = {Nomograms for Visualizing Support Vector Machines},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081886},
doi = {10.1145/1081870.1081886},
abstract = {We propose a simple yet potentially very effective way of visualizing trained support vector machines. Nomograms are an established model visualization technique that can graphically encode the complete model on a single page. The dimensionality of the visualization does not depend on the number of attributes, but merely on the properties of the kernel. To represent the effect of each predictive feature on the log odds ratio scale as required for the nomograms, we employ logistic regression to convert the distance from the separating hyperplane into a probability. Case studies on selected data sets show that for a technique thought to be a black-box, nomograms can clearly expose its internal structure. By providing an easy-to-interpret visualization the analysts can gain insight and study the effects of predictive factors.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {108–117},
numpages = {10},
keywords = {support vector machines, visualization, machine learning, nomogram},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081887,
author = {Jaroszewicz, Szymon and Scheffer, Tobias},
title = {Fast Discovery of Unexpected Patterns in Data, Relative to a Bayesian Network},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081887},
doi = {10.1145/1081870.1081887},
abstract = {We consider a model in which background knowledge on a given domain of interest is available in terms of a Bayesian network, in addition to a large database. The mining problem is to discover unexpected patterns: our goal is to find the strongest discrepancies between network and database. This problem is intrinsically difficult because it requires inference in a Bayesian network and processing the entire, potentially very large, database. A sampling-based method that we introduce is efficient and yet provably finds the approximately most interesting unexpected patterns. We give a rigorous proof of the method's correctness. Experiments shed light on its efficiency and practicality for large-scale Bayesian networks and databases.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {118–127},
numpages = {10},
keywords = {sampling, association rules, Bayesian networks},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081888,
author = {Kolcz, Aleksander},
title = {Local Sparsity Control for Naive Bayes with Extreme Misclassification Costs},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081888},
doi = {10.1145/1081870.1081888},
abstract = {In applications of data mining characterized by highly skewed misclassification costs certain types of errors become virtually unacceptable. This limits the utility of a classifier to a range in which such constraints can be met. Naive Bayes, which has proven to be very useful in text mining applications due to high scalability, can be particularly affected. Although its 0/1 loss tends to be small, its misclassifications are often made with apparently high confidence. Aside from efforts to better calibrate Naive Bayes scores, it has been shown that its accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance. Traditionally, sparsity is controlled globally, and the result for any particular document may vary. In this work we examine the merits of local sparsity control for Naive Bayes in the context of highly asymmetric misclassification costs. In experiments with three benchmark document collections we demonstrate clear advantages of document-level feature selection. In the extreme cost setting, multinomial Naive Bayes with local sparsity control is able to outperform even some of the recently proposed effective improvements to the Naive Bayes classifier. There are also indications that local feature selection may be preferable in different cost settings.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {128–137},
numpages = {10},
keywords = {high recall classification, text categorization, feature selection, naive Bayes},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081889,
author = {Kubica, Jeremy and Moore, Andrew and Connolly, Andrew and Jedicke, Robert},
title = {A Multiple Tree Algorithm for the Efficient Association of Asteroid Observations},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081889},
doi = {10.1145/1081870.1081889},
abstract = {In this paper we examine the problem of efficiently finding sets of observations that conform to a given underlying motion model. While this problem is often phrased as a tracking problem, where it is called track initiation, it is useful in a variety of tasks where we want to find correspondences or patterns in spatial-temporal data. Unfortunately, this problem often suffers from a combinatorial explosion in the number of potential sets that must be evaluated. We consider the problem with respect to large-scale asteroid observation data, where the goal is to find associations among the observations that correspond to the same underlying asteroid. In this domain, it is vital that we can efficiently extract the underlying associations.We introduce a new methodology for track initiation that exhaustively considers all possible linkages. We then introduce an exact tree-based algorithm for tractably finding all compatible sets of points. Further, we extend this approach to use multiple trees, exploiting structure from several time steps at once. We compare this approach to a standard sequential approach and show how the use of multiple trees can provide a significant benefit.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {138–146},
numpages = {9},
keywords = {multiple tree algorithms, track initiation},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081890,
author = {Lange, Tilman and Buhmann, Joachim M.},
title = {Combining Partitions by Probabilistic Label Aggregation},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081890},
doi = {10.1145/1081870.1081890},
abstract = {Data clustering represents an important tool in exploratory data analysis. The lack of objective criteria render model selection as well as the identification of robust solutions particularly difficult. The use of a stability assessment and the combination of multiple clustering solutions represents an important ingredient to achieve the goal of finding useful partitions. In this work, we propose a novel way of combining multiple clustering solutions for both, hard and soft partitions: the approach is based on modeling the probability that two objects are grouped together. An efficient EM optimization strategy is employed in order to estimate the model parameters. Our proposal can also be extended in order to emphasize the signal more strongly by weighting individual base clustering solutions according to their consistency with the prediction for previously unseen objects. In addition to that, the probabilistic model supports an out-of-sample extension that (i) makes it possible to assign previously unseen objects to classes of the combined solution and (ii) renders the efficient aggregation of solutions possible. In this work, we also shed some light on the usefulness of such combination approaches. In the experimental result section, we demonstrate the competitive performance of our proposal in comparison with other recently proposed methods for combining multiple classifications of a finite data set.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {147–156},
numpages = {10},
keywords = {re-sampling, consensus partition, clustering},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081891,
author = {Lazarevic, Aleksandar and Kumar, Vipin},
title = {Feature Bagging for Outlier Detection},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081891},
doi = {10.1145/1081870.1081891},
abstract = {Outlier detection has recently become an important problem in many industrial and financial applications. In this paper, a novel feature bagging approach for detecting outliers in very large, high dimensional and noisy databases is proposed. It combines results from multiple outlier detection algorithms that are applied using different set of features. Every outlier detection algorithm uses a small subset of features that are randomly selected from the original feature set. As a result, each outlier detector identifies different outliers, and thus assigns to all data records outlier scores that correspond to their probability of being outliers. The outlier scores computed by the individual outlier detection algorithms are then combined in order to find the better quality outliers. Experiments performed on several synthetic and real life data sets show that the proposed methods for combining outputs from multiple outlier detection algorithms provide non-trivial improvements over the base algorithm.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {157–166},
numpages = {10},
keywords = {false alarm, detection rate, integration, bagging, feature subsets, outlier detection},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081892,
author = {Leban, Gregor and Mramor, Minca and Bratko, Ivan and Zupan, Blaz},
title = {Simple and Effective Visual Models for Gene Expression Cancer Diagnostics},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081892},
doi = {10.1145/1081870.1081892},
abstract = {In the paper we show that diagnostic classes in cancer gene expression data sets, which most often include thousands of features (genes), may be effectively separated with simple two-dimensional plots such as scatterplot and radviz graph. The principal innovation proposed in the paper is a method called VizRank, which is able to score and identify the best among possibly millions of candidate projections for visualizations. Compared to recently much applied techniques in the field of cancer genomics that include neural networks, support vector machines and various ensemble-based approaches, VizRank is fast and finds visualization models that can be easily examined and interpreted by domain experts. Our experiments on a number of gene expression data sets show that VizRank was always able to find data visualizations with a small number of (two to seven) genes and excellent class separation. In addition to providing grounds for gene expression cancer diagnosis, VizRank and its visualizations also identify small sets of relevant genes, uncover interesting gene interactions and point to outliers and potential misclassifications in cancer data sets.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {167–176},
numpages = {10},
keywords = {machine learning, gene expression analysis, data visualization, data mining, cancer diagnosis},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081893,
author = {Leskovec, Jure and Kleinberg, Jon and Faloutsos, Christos},
title = {Graphs over Time: Densification Laws, Shrinking Diameters and Possible Explanations},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081893},
doi = {10.1145/1081870.1081893},
abstract = {How do real graphs evolve over time? What are "normal" growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these findings into statements about trends over time.Here we study a wide range of real graphs, and we observe some surprising phenomena. First, most of these graphs densify over time, with the number of edges growing super-linearly in the number of nodes. Second, the average distance between nodes often shrinks over time, in contrast to the conventional wisdom that such distance parameters should increase slowly as a function of the number of nodes (like O(log n) or O(log(log n)).Existing graph generation models do not exhibit these types of behavior, even at a qualitative level. We provide a new graph generator, based on a "forest fire" spreading process, that has a simple, intuitive justification, requires very few parameters (like the "flammability" of nodes), and produces graphs exhibiting the full range of properties observed both in prior work and in the present study.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {177–187},
numpages = {11},
keywords = {graph mining, small-world phenomena, heavy-tailed distributions, graph generators, densification power laws},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081894,
author = {Li, Tao},
title = {A General Model for Clustering Binary Data},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081894},
doi = {10.1145/1081870.1081894},
abstract = {Clustering is the problem of identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning the data points into similarity classes. This paper studies the problem of clustering binary data. This is the case for market basket datasets where the transactions contain items and for document datasets where the documents contain "bag of words". The contribution of the paper is three-fold. First a general binary data clustering model is presented. The model treats the data and features equally, based on their symmetric association relations, and explicitly describes the data assignments as well as feature assignments. We characterize several variations with different optimization procedures for the general model. Second, we also establish the connections between our clustering model with other existing clustering methods. Third, we also discuss the problem for determining the number of clusters for binary clustering. Experimental results show the effectiveness of the proposed clustering model.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {188–197},
numpages = {10},
keywords = {binary data, clustering, general model, matrix approximation},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081895,
author = {Mei, Qiaozhu and Zhai, ChengXiang},
title = {Discovering Evolutionary Theme Patterns from Text: An Exploration of Temporal Text Mining},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081895},
doi = {10.1145/1081870.1081895},
abstract = {Temporal Text Mining (TTM) is concerned with discovering temporal patterns in text information collected over time. Since most text information bears some time stamps, TTM has many applications in multiple domains, such as summarizing events in news articles and revealing research trends in scientific literature. In this paper, we study a particular TTM task -- discovering and summarizing the evolutionary patterns of themes in a text stream. We define this new text mining problem and present general probabilistic methods for solving this problem through (1) discovering latent themes from text; (2) constructing an evolution graph of themes; and (3) analyzing life cycles of themes. Evaluation of the proposed methods on two different domains (i.e., news articles and literature) shows that the proposed methods can discover interesting evolutionary theme patterns effectively.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {198–207},
numpages = {10},
keywords = {theme threads, temporal text mining, evolutionary theme patterns, clustering},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081896,
author = {Merugu, Srujana and Ghosh, Joydeep},
title = {A Distributed Learning Framework for Heterogeneous Data Sources},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081896},
doi = {10.1145/1081870.1081896},
abstract = {We present a probabilistic model-based framework for distributed learning that takes into account privacy restrictions and is applicable to scenarios where the different sites have diverse, possibly overlapping subsets of features. Our framework decouples data privacy issues from knowledge integration issues by requiring the individual sites to share only privacy-safe probabilistic models of the local data, which are then integrated to obtain a global probabilistic model based on the union of the features available at all the sites. We provide a mathematical formulation of the model integration problem using the maximum likelihood and maximum entropy principles and describe iterative algorithms that are guaranteed to converge to the optimal solution. For certain commonly occurring special cases involving hierarchically ordered feature sets or conditional independence, we obtain closed form solutions and use these to propose an efficient alternative scheme by recursive decomposition of the model integration problem. To address interpretability concerns, we also present a modified formulation where the global model is assumed to belong to a specified parametric family. Finally, to highlight the generality of our framework, we provide empirical results for various learning tasks such as clustering and classification on different kinds of datasets consisting of continuous vector, categorical and directional attributes. The results show that high quality global models can be obtained without much loss of privacy.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {208–217},
numpages = {10},
keywords = {heterogeneous data sources, distributed learning, probabilistic models, privacy},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081897,
author = {Neill, Daniel B. and Moore, Andrew W. and Sabhnani, Maheshkumar and Daniel, Kenny},
title = {Detection of Emerging Space-Time Clusters},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081897},
doi = {10.1145/1081870.1081897},
abstract = {We propose a new class of spatio-temporal cluster detection methods designed for the rapid detection of emerging space-time clusters. We focus on the motivating application of prospective disease surveillance: detecting space-time clusters of disease cases resulting from an emerging disease outbreak. Automatic, real-time detection of outbreaks can enable rapid epidemiological response, potentially reducing rates of morbidity and mortality. Building on the prior work on spatial and space-time scan statistics, our methods combine time series analysis (to determine how many cases we expect to observe for a given spatial region in a given time interval) with new "emerging cluster" space-time scan statistics (to decide whether an observed increase in cases in a region is significant), enabling fast and accurate detection of emerging outbreaks. We evaluate these methods on two types of simulated outbreaks: aerosol release of inhalational anthrax (e.g. from a bioterrorist attack) and FLOO ("Fictional Linear Onset Outbreak"), injected into actual baseline data (Emergency Department records and over-the-counter drug sales data from Allegheny County). We demonstrate that our methods are successful in rapidly detecting both outbreak types while keeping the number of false positives low, and show that our new "emerging cluster" scan statistics consistently outperform the standard "persistent cluster" scan statistics approach.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {218–227},
numpages = {10},
keywords = {cluster detection, space-time scan statistics, biosurveillance},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081898,
author = {Pei, Jian and Jiang, Daxin and Zhang, Aidong},
title = {On Mining Cross-Graph Quasi-Cliques},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081898},
doi = {10.1145/1081870.1081898},
abstract = {Joint mining of multiple data sets can often discover interesting, novel, and reliable patterns which cannot be obtained solely from any single source. For example, in cross-market customer segmentation, a group of customers who behave similarly in multiple markets should be considered as a more coherent and more reliable cluster than clusters found in a single market. As another example, in bioinformatics, by joint mining of gene expression data and protein interaction data, we can find clusters of genes which show coherent expression patterns and also produce interacting proteins. Such clusters may be potential pathways.In this paper, we investigate a novel data mining problem, mining cross-graph quasi-cliques, which is generalized from several interesting applications such as cross-market customer segmentation and joint mining of gene expression data and protein interaction data. We build a general model for mining cross-graph quasi-cliques, show why the complete set of cross-graph quasi-cliques cannot be found by previous data mining methods, and study the complexity of the problem. While the problem is difficult, we develop an efficient algorithm, Crochet, which exploits several interesting and effective techniques and heuristics to efficaciously mine cross-graph quasi-cliques. A systematic performance study is reported on both synthetic and real data sets. We demonstrate some interesting and meaningful cross-graph quasi-cliques in bioinformatics. The experimental results also show that algorithm Crochet is efficient and scalable.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {228–238},
numpages = {11},
keywords = {graph mining, patterns, bioinformatics},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081899,
author = {Radlinski, Filip and Joachims, Thorsten},
title = {Query Chains: Learning to Rank from Implicit Feedback},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081899},
doi = {10.1145/1081870.1081899},
abstract = {This paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results. We observe that users searching the web often perform a sequence, or chain, of queries with a similar information need. Using query chains, we generate new types of preference judgments from search engine logs, thus taking advantage of user intelligence in reformulating queries. To validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments. We also implemented a real-world search engine to test our approach, using a modified ranking SVM to learn an improved ranking function from preference data. Our results demonstrate significant improvements in the ranking given by the search engine. The learned rankings outperform both a static ranking function, as well as one trained without considering query chains.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {239–248},
numpages = {10},
keywords = {support vector machines, implicit feedback, clickthrough data, search engines, machine learning},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081900,
author = {Rosset, Saharon},
title = {Robust Boosting and Its Relation to Bagging},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081900},
doi = {10.1145/1081870.1081900},
abstract = {Several authors have suggested viewing boosting as a gradient descent search for a good fit in function space. At each iteration observations are re-weighted using the gradient of the underlying loss function. We present an approach of weight decay for observation weights which is equivalent to "robustifying" the underlying loss function. At the extreme end of decay this approach converges to Bagging, which can be viewed as boosting with a linear underlying loss function. We illustrate the practical usefulness of weight decay for improving prediction performance and present an equivalence between one form of weight decay and "Huberizing" --- a statistical method for making loss functions more robust.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {249–255},
numpages = {7},
keywords = {bagging, boosting, robust fitting},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081901,
author = {Sandler, Mark},
title = {On the Use of Linear Programming for Unsupervised Text Classification},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081901},
doi = {10.1145/1081870.1081901},
abstract = {We propose a new algorithm for dimensionality reduction and unsupervised text classification. We use mixture models as underlying process of generating corpus and utilize a novel, L1-norm based approach introduced by Kleinberg and Sandler [19]. We show that our algorithm performs extremely well on large datasets, with peak accuracy approaching that of supervised learning based on Support Vector Machines (SVMs) with large training sets. The method is based on the same idea that underlies Latent Semantic Indexing (LSI). We find a good low-dimensional subspace of a feature space and project all documents into it. However our projection minimizes different error, and unlike LSI we build a basis, that in many cases corresponds to the actual topics. We present results of testing of our algorithm on the abstracts of arXiv - an electronic repository of scientific papers, and the 20 Newsgroup dataset - a small snapshot of 20 specific newsgroups.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {256–264},
numpages = {9},
keywords = {latent class models, L1 norm, text classification, linear programming, dimensionality reduction, unsupervised learning, LSI, mixture models, generative models, singular value decomposition},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081902,
author = {Scholz, Martin},
title = {Sampling-Based Sequential Subgroup Mining},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081902},
doi = {10.1145/1081870.1081902},
abstract = {Subgroup discovery is a learning task that aims at finding interesting rules from classified examples. The search is guided by a utility function, trading off the coverage of rules against their statistical unusualness. One shortcoming of existing approaches is that they do not incorporate prior knowledge. To this end a novel generic sampling strategy is proposed. It allows to turn pattern mining into an iterative process. In each iteration the focus of subgroup discovery lies on those patterns that are unexpected with respect to prior knowledge and previously discovered patterns. The result of this technique is a small diverse set of understandable rules that characterise a specified property of interest. As another contribution this article derives a simple connection between subgroup discovery and classifier induction. For a popular utility function this connection allows to apply any standard rule induction algorithm to the task of subgroup discovery after a step of stratified resampling. The proposed techniques are empirically compared to state of the art subgroup discovery algorithms.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {265–274},
numpages = {10},
keywords = {subgroup discovery, sampling, prior knowledge},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081903,
author = {Silva, Ricardo and Zhang, Jiji and Shanahan, James G.},
title = {Probabilistic Workflow Mining},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081903},
doi = {10.1145/1081870.1081903},
abstract = {In several organizations, it has become increasingly popular to document and log the steps that makeup a typical business process. In some situations, a normative workflow model of such processes is developed, and it becomes important to know if such a model is actually being followed by analyzing the available activity logs. In other scenarios, no model is available and, with the purpose of evaluating cases or creating new production policies, one is interested in learning a workflow representation of such activities. In either case, machine learning tools that can mine workflow models are of great interest and still relatively unexplored. We present here a probabilistic workflow model and a corresponding learning algorithm that runs in polynomial time. We illustrate the algorithm on example data derived from a real world workflow.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {275–284},
numpages = {10},
keywords = {workflow mining, causal models, graphical models},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081904,
author = {Ukkonen, Antti and Fortelius, Mikael and Mannila, Heikki},
title = {Finding Partial Orders from Unordered 0-1 Data},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081904},
doi = {10.1145/1081870.1081904},
abstract = {In applications such as paleontology and medical genetics the 0-1 data has an underlying unknown order (the ages of the fossil sites, the locations of markers in the genome). The order might be total or partial: for example, two sites in different parts of the globe might be ecologically incomparable, or the ordering of certain markers might be different in different subgroups of the data. We consider the following problem. Given a table over a set of 0-1 variables, find a partial order for the rows minimizing a score function and being as specific as possible. The score function can be, e.g., the number of changes from 1 to 0 in a column (for paleontology) or the likelihood of the marker sequence (for genomic data). Our solution for this task first constructs small totally ordered fragments of the partial order, then finds good orientations for the fragments, and finally uses a simple and efficient heuristic method for finding a partial order that corresponds well with the collection of fragments. We describe the method, discuss its properties, and give empirical results on paleontological data demonstrating the usefulness of the method. In the application the use of the method highlighted some previously unknown properties of the data and pointed out probable errors in the data.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {285–293},
numpages = {9},
keywords = {partial order, consecutive ones property, hidden ordering},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081905,
author = {Wang, Muyuan and Li, Zhiwei and Lu, Lie and Ma, Wei-Ying and Zhang, Naiyao},
title = {Web Object Indexing Using Domain Knowledge},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081905},
doi = {10.1145/1081870.1081905},
abstract = {A web object is defined to represent any meaningful object embedded in web pages (e.g. images, music) or pointed to by hyperlinks (e.g. downloadable files). In many cases, users would like to search for information of a certain 'object', rather than a web page containing the query terms. To facilitate web object searching and organizing, in this paper, we propose a novel approach to web object indexing, by discovering its inherent structure information with existed domain knowledge. In our approach, first, Layered LSI spaces are built for a better representation of the hierarchically structured domain knowledge, in order to emphasize the specific semantics and term space in each layer of the domain knowledge. Meanwhile, the web object representation is constructed by hyperlink analysis, and further pruned to remove the noises. Then an optimal matching between the web object and the domain knowledge is performed, in order to pick out the structure attributes of the web object from the knowledge. Finally, the obtained structure attributes are used to re-organize and index the web objects. Our approach also indicates a new promising way to use trust-worthy Deep Web knowledge to help organize dispersive information of Surface Web.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {294–303},
numpages = {10},
keywords = {domain knowledge, music indexing, link analysis, indexing, confidence propagation, information retrieval, web object, latent semantic indexing},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081906,
author = {Phan, Xuan-Hieu and Nguyen, Le-Minh and Ho, Tu-Bao and Horiguchi, Susumu},
title = {Improving Discriminative Sequential Learning with Rare--but--Important Associations},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081906},
doi = {10.1145/1081870.1081906},
abstract = {Discriminative sequential learning models like Conditional Random Fields (CRFs) have achieved significant success in several areas such as natural language processing or information extraction. Their key advantage is the ability to capture various non--independent and overlapping features of inputs. However, several unexpected pitfalls have a negative influence on the model's performance; these mainly come from an imbalance among classes/labels, irregular phenomena, and potential ambiguity in the training data. This paper presents a data--driven approach that can deal with such hard--to--predict data instances by discovering and emphasizing rare--but--important associations of statistics hidden in the training data. Mined associations are then incorporated into these models to deal with difficult examples. Experimental results of English phrase chunking and named entity recognition using CRFs show a significant improvement in accuracy. In addition to the technical perspective, our approach also highlights a potential connection between association mining and statistical learning by offering an alternative strategy to enhance learning performance with interesting and useful patterns discovered from large dataset.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {304–313},
numpages = {10},
keywords = {feature selection, text segmentation, discriminative sequential learning, information extraction, association rule},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081907,
author = {Yan, Xifeng and Cheng, Hong and Han, Jiawei and Xin, Dong},
title = {Summarizing Itemset Patterns: A Profile-Based Approach},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081907},
doi = {10.1145/1081870.1081907},
abstract = {Frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets, sequences, and graphs. However, the bottleneck of frequent-pattern mining is not at the efficiency but at the interpretability, due to the huge number of patterns generated by the mining process.In this paper, we examine how to summarize a collection of itemset patterns using only K representatives, a small number of patterns that a user can handle easily. The K representatives should not only cover most of the frequent patterns but also approximate their supports. A generative model is built to extract and profile these representatives, under which the supports of the patterns can be easily recovered without consulting the original dataset. Based on the restoration error, we propose a quality measure function to determine the optimal value of parameter K. Polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement. Empirical studies indicate that we can obtain compact summarization in real datasets.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {314–323},
numpages = {10},
keywords = {summarization, frequent pattern, probabilistic model},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081908,
author = {Yan, Xifeng and Zhou, X. Jasmine and Han, Jiawei},
title = {Mining Closed Relational Graphs with Connectivity Constraints},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081908},
doi = {10.1145/1081870.1081908},
abstract = {Relational graphs are widely used in modeling large scale networks such as biological networks and social networks. In this kind of graph, connectivity becomes critical in identifying highly associated groups and clusters. In this paper, we investigate the issues of mining closed frequent graphs with connectivity constraints in massive relational graphs where each graph has around 10K nodes and 1M edges. We adopt the concept of edge connectivity and apply the results from graph theory, to speed up the mining process. Two approaches are developed to handle different mining requests: CloseCut, a pattern-growth approach, and splat, a pattern-reduction approach. We have applied these methods in biological datasets and found the discovered patterns interesting.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {324–333},
numpages = {10},
keywords = {closed pattern, connectivity, graph},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081909,
author = {Yang, Zhiqiang and Zhong, Sheng and Wright, Rebecca N.},
title = {Anonymity-Preserving Data Collection},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081909},
doi = {10.1145/1081870.1081909},
abstract = {Protection of privacy has become an important problem in data mining. In particular, individuals have become increasingly unwilling to share their data, frequently resulting in individuals either refusing to share their data or providing incorrect data. In turn, such problems in data collection can affect the success of data mining, which relies on sufficient amounts of accurate data in order to produce meaningful results. Random perturbation and randomized response techniques can provide some level of privacy in data collection, but they have an associated cost in accuracy. Cryptographic privacy-preserving data mining methods provide good privacy and accuracy properties. However, in order to be efficient, those solutions must be tailored to specific mining tasks, thereby losing generality.In this paper, we propose efficient cryptographic techniques for online data collection in which data from a large number of respondents is collected anonymously, without the help of a trusted third party. That is, our solution allows the miner to collect the original data from each respondent, but in such a way that the miner cannot link a respondent's data to the respondent. An advantage of such a solution is that, because it does not change the actual data, its success does not depend on the underlying data mining problem. We provide proofs of the correctness and privacy of our solution, as well as experimental data that demonstrates its efficiency. We also extend our solution to tolerate certain kinds of malicious behavior of the participants.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {334–343},
numpages = {10},
keywords = {data mining, data collection, anonymity},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081910,
author = {Yin, Xiaoxin and Han, Jiawei and Yu, Philip S.},
title = {Cross-Relational Clustering with User's Guidance},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081910},
doi = {10.1145/1081870.1081910},
abstract = {Clustering is an essential data mining task with numerous applications. However, data in most real-life applications are high-dimensional in nature, and the related information often spreads across multiple relations. To ensure effective and efficient high-dimensional, cross-relational clustering, we propose a new approach, called CrossClus, which performs cross-relational clustering with user's guidance. We believe that user's guidance, even likely in very simple forms, could be essential for effective high-dimensional clustering since a user knows well the application requirements and data semantics. CrossClus is carried out as follows: A user specifies a clustering task and selects one or a small set of features pertinent to the task. CrossClus extracts the set of highly relevant features in multiple relations connected via linkages defined in the database schema, evaluates their effectiveness based on user's guidance, and identifies interesting clusters that fit user's needs. This method takes care of both quality in feature extraction and efficiency in clustering. Our comprehensive experiments demonstrate the effectiveness and scalability of this approach.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {344–353},
numpages = {10},
keywords = {data mining, clustering, relational databases},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081911,
author = {Yu, Hwanjo},
title = {SVM Selective Sampling for Ranking with Application to Data Retrieval},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081911},
doi = {10.1145/1081870.1081911},
abstract = {Learning ranking (or preference) functions has been a major issue in the machine learning community and has produced many applications in information retrieval. SVMs (Support Vector Machines) - a classification and regression methodology - have also shown excellent performance in learning ranking functions. They effectively learn ranking functions of high generalization based on the "large-margin" principle and also systematically support nonlinear ranking by the "kernel trick". In this paper, we propose an SVM selective sampling technique for learning ranking functions. SVM selective sampling (or active learning with SVM) has been studied in the context of classification. Such techniques reduce the labeling effort in learning classification functions by selecting only the most informative samples to be labeled. However, they are not extendable to learning ranking functions, as the labeled data in ranking is relative ordering, or partial orders of data. Our proposed sampling technique effectively learns an accurate SVM ranking function with fewer partial orders. We apply our sampling technique to the data retrieval application, which enables fuzzy search on relational databases by interacting with users for learning their preferences. Experimental results show a significant reduction of the labeling effort in inducing accurate ranking functions.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {354–363},
numpages = {10},
keywords = {ranking, support vector machine, active learning, selective sampling},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081912,
author = {Zaki, Mohammed J. and Ramakrishnan, Naren},
title = {Reasoning about Sets Using Redescription Mining},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081912},
doi = {10.1145/1081870.1081912},
abstract = {Redescription mining is a newly introduced data mining problem that seeks to find subsets of data that afford multiple definitions. It can be viewed as a generalization of association rule mining, from finding implications to equivalences; as a form of conceptual clustering, where the goal is to identify clusters that afford dual characterizations; and as a form of constructive induction, to build features based on given descriptors that mutually reinforce each other. In this paper, we present the use of redescription mining as an important tool to reason about a collection of sets, especially their overlaps, similarities, and differences. We outline algorithms to mine all minimal (non-redundant) redescriptions underlying a dataset using notions of minimal generators of closed itemsets. We also show the use of these algorithms in an interactive context, supporting constraint-based exploration and querying. Specifically, we showcase a bioinformatics application that empowers the biologist to define a vocabulary of sets underlying a domain of genes and to reason about these sets, yielding significant biological insight.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {364–373},
numpages = {10},
keywords = {minimal generators, closed itemsets, data mining, redescription},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081913,
author = {Zhang, Nan and Wang, Shengquan and Zhao, Wei},
title = {A New Scheme on Privacy-Preserving Data Classification},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081913},
doi = {10.1145/1081870.1081913},
abstract = {We address privacy-preserving classification problem in a distributed system. Randomization has been the approach proposed to preserve privacy in such scenario. However, this approach is now proven to be insecure as it has been discovered that some privacy intrusion techniques can be used to reconstruct private information from the randomized data tuples. We introduce an algebraic-technique-based scheme. Compared to the randomization approach, our new scheme can build classifiers more accurately but disclose less private information. Furthermore, our new scheme can be readily integrated as a middleware with existing systems.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {374–383},
numpages = {10},
keywords = {privacy-preserving data mining, privacy},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081914,
author = {Zhou, Jing and Foster, Dean and Stine, Robert and Ungar, Lyle},
title = {Streaming Feature Selection Using Alpha-Investing},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081914},
doi = {10.1145/1081870.1081914},
abstract = {In Streaming Feature Selection (SFS), new features are sequentially considered for addition to a predictive model. When the space of potential features is large, SFS offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model. We describe α-investing, an adaptive complexity penalty method for SFS which dynamically adjusts the threshold on the error reduction required for adding a new feature. α-investing gives false discovery rate-style guarantees against overfitting. It differs from standard penalty methods such as AIC, BIC or RIC, which always drastically over- or under-fit in the limit of infinite numbers of non-predictive features. Empirical results show that SFS is competitive with much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with over a million potential features.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {384–393},
numpages = {10},
keywords = {false discovery rate, multiple regression, classification, feature selection},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081916,
author = {Forman, George and Eshghi, Kave and Chiocchetti, Stephane},
title = {Finding Similar Files in Large Document Repositories},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081916},
doi = {10.1145/1081870.1081916},
abstract = {Hewlett-Packard has many millions of technical support documents in a variety of collections. As part of content management, such collections are periodically merged and groomed. In the process, it becomes important to identify and weed out support documents that are largely duplicates of newer versions. Doing so improves the quality of the collection, eliminates chaff from search results, and improves customer satisfaction.The technical challenge is that through workflow and human processes, the knowledge of which documents are related is often lost. We required a method that could identify similar documents based on their content alone, without relying on metadata, which may be corrupt or missing.We present an approach for finding similar files that scales up to large document repositories. It is based on chunking the byte stream to find unique signatures that may be shared in multiple files. An analysis of the file-chunk graph yields clusters of related files. An optional bipartite graph partitioning algorithm can be applied to greatly increase scalability.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {394–400},
numpages = {7},
keywords = {near duplicate detection, document management, content management, similarity, scalability},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081917,
author = {Fujimaki, Ryohei and Yairi, Takehisa and Machida, Kazuo},
title = {An Approach to Spacecraft Anomaly Detection Problem Using Kernel Feature Space},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081917},
doi = {10.1145/1081870.1081917},
abstract = {Development of advanced anomaly detection and failure diagnosis technologies for spacecraft is a quite significant issue in the space industry, because the space environment is harsh, distant and uncertain. While several modern approaches based on qualitative reasoning, expert systems, and probabilistic reasoning have been developed recently for this purpose, any of them has a common difficulty in obtaining accurate and complete a priori knowledge on the space systems from human experts. A reasonable alternative to this conventional anomaly detection method is to reuse a vast amount of telemetry data which is multi-dimensional time-series continuously produced from a number of system components in the spacecraft.This paper proposes a novel "knowledge-free" anomaly detection method for spacecraft based on Kernel Feature Space and directional distribution, which constructs a system behavior model from the past normal telemetry data from a set of telemetry data in normal operation and monitors the current system status by checking incoming data with the model.In this method, we regard anomaly phenomena as unexpected changes of causal associations in the spacecraft system, and hypothesize that the significant causal associations inside the system will appear in the form of principal component directions in a high-dimensional non-linear feature space which is constructed by a kernel function and a set of data.We have confirmed the effectiveness of the proposed anomaly detection method by applying it to the telemetry data obtained from a simulator of an orbital transfer vehicle designed to make a rendezvous maneuver with the International Space Station.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {401–410},
numpages = {10},
keywords = {von Mises Fisher distribution, principal component analysis, anomaly detection, spacecraft, time series data, kernel feature space},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081918,
author = {Ghani, Rayid},
title = {Price Prediction and Insurance for Online Auctions},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081918},
doi = {10.1145/1081870.1081918},
abstract = {Online auctions are generating a new class of fine-grained data about online transactions. This data lends itself to a variety of applications and services that can be provided to both buyers and sellers in online marketplaces. We collect data from online auctions and use several classification algorithms to predict the probable-end prices of online auction items. This paper describes the feature extraction and selection process, and several machine learning formulations of the price prediction problem. As a prototype application, we developed Auction Price Insurance that uses the predicted end-price to offer price insurance to sellers in online auctions. We define Price Insurance as a service that offers insurance to auction sellers that guarantees a price for their goods, for an appropriate premium. If the item sells for less than the insured price, the seller is reimbursed for the difference. We show that our price prediction techniques are accurate enough to offer price insurance as a profitable business. While this paper deals specifically with online auctions, we believe that this is an interesting case study that applies to dynamic markets where the price of the goods is variable and is affected by both internal and external factors that change over time.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {411–418},
numpages = {8},
keywords = {auctions, classification, ECommerce, data mining, price prediction, price insurance},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081919,
author = {Glance, Natalie and Hurst, Matthew and Nigam, Kamal and Siegler, Matthew and Stockton, Robert and Tomokiyo, Takashi},
title = {Deriving Marketing Intelligence from Online Discussion},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081919},
doi = {10.1145/1081870.1081919},
abstract = {Weblogs and message boards provide online forums for discussion that record the voice of the public. Woven into this mass of discussion is a wide range of opinion and commentary about consumer products. This presents an opportunity for companies to understand and respond to the consumer by analyzing this unsolicited feedback. Given the volume, format and content of the data, the appropriate approach to understand this data is to use large-scale web and text data mining technologies.This paper argues that applications for mining large volumes of textual data for marketing intelligence should provide two key elements: a suite of powerful mining and visualization technologies and an interactive analysis environment which allows for rapid generation and testing of hypotheses. This paper presents such a system that gathers and annotates online discussion relating to consumer products using a wide variety of state-of-the-art techniques, including crawling, wrapping, search, text classification and computational linguistics. Marketing intelligence is derived through an interactive analysis framework uniquely configured to leverage the connectivity and content of annotated online discussion.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {419–428},
numpages = {10},
keywords = {information retrieval, text mining, content systems, machine learning, computational linguistics},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081920,
author = {He, Bin and Chang, Kevin Chen-Chuan},
title = {Making Holistic Schema Matching Robust: An Ensemble Approach},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081920},
doi = {10.1145/1081870.1081920},
abstract = {The Web has been rapidly "deepened" by myriad searchable databases online, where data are hidden behind query interfaces. As an essential task toward integrating these massive "deep Web" sources, large scale schema matching (i.e., discovering semantic correspondences of attributes across many query interfaces) has been actively studied recently. In particular, many works have emerged to address this problem by "holistically" matching many schemas at the same time and thus pursuing "mining" approaches in nature. However, while holistic schema matching has built its promise upon the large quantity of input schemas, it also suffers the robustness problem caused by noisy data quality. Such noises often inevitably arise in the automatic extraction of schema data, which is mandatory in large scale integration. For holistic matching to be viable, it is thus essential to make it robust against noisy schemas. To tackle this challenge, we propose a data-ensemble framework with sampling and voting techniques, which is inspired by bagging predictors. Specifically, our approach creates an ensemble of matchers, by randomizing input schema data into many independently downsampled trials, executing the same matcher on each trial and then aggregating their ranked results by taking majority voting. As a principled basis, we provide analytic justification of the effectiveness of this data-ensemble framework. Further, empirically, our experiments on real Web data show that the "ensemblization" indeed significantly boosts the matching accuracy under noisy schema input, and thus maintains the desired robustness of a holistic matcher.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {429–438},
numpages = {10},
keywords = {deep web, bagging predictors, ensemble, schema matching, data integration},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081921,
author = {Nasraoui, Olfa and Cardona, Cesar and Rojas, Carlos},
title = {Using Retrieval Measures to Assess Similarity in Mining Dynamic Web Clickstreams},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081921},
doi = {10.1145/1081870.1081921},
abstract = {While scalable data mining methods are expected to cope with massive Web data, coping with evolving trends in noisy data in a continuous fashion, and without any unnecessary stoppages and reconfigurations is still an open challenge. This dynamic and single pass setting can be cast within the framework of mining evolving data streams. In this paper, we explore the task of mining mass user profiles by discovering evolving Web session clusters in a single pass with a recently proposed scalable immune based clustering approach (TECNO-STREAMS), and study the effect of the choice of different similarity measures on the mining process and on the interpretation of the mined patterns. We propose a simple similarity measure that has the advantage of explicitly coupling the precision and coverage criteria to the early learning stages, and furthermore requiring that the affinity of the data to the learned profiles or summaries be defined by the minimum of their coverage or precision, hence requiring that the learned profiles are simultaneously precise and complete, with no compromises.In our experiments, we study the task of mining evolving user profiles from Web clickstream data (web usage mining) in a single pass, and under different trend sequencing scenarios, showing that compared oto the cosine similarity measure, the proposed similarity measure explicitly based on precision and coverage allows the discovery of more correct profiles at the same precision or recall quality levels.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {439–448},
numpages = {10},
keywords = {clustering, web mining, artificial immune systems, stream data mining, personalization, mining evolving data},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081922,
author = {Neville, Jennifer and \c{S}im\c{s}ek, \"{O}zg\"{u}r and Jensen, David and Komoroske, John and Palmer, Kelly and Goldberg, Henry},
title = {Using Relational Knowledge Discovery to Prevent Securities Fraud},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081922},
doi = {10.1145/1081870.1081922},
abstract = {We describe an application of relational knowledge discovery to a key regulatory mission of the National Association of Securities Dealers (NASD). NASD is the world's largest private-sector securities regulator, with responsibility for preventing and discovering misconduct among securities brokers. Our goal was to help focus NASD's limited regulatory resources on the brokers who are most likely to engage in securities violations. Using statistical relational learning algorithms, we developed models that rank brokers with respect to the probability that they would commit a serious violation of securities regulations in the near future. Our models incorporate organizational relationships among brokers (e.g., past coworker), which domain experts consider important but have not been easily used before now. The learned models were subjected to an extensive evaluation using more than 18 months of data unseen by the model developers and comprising over two person weeks of effort by NASD staff. Model predictions were found to correlate highly with the subjective evaluations of experienced NASD examiners. Furthermore, in all performance measures, our models performed as well as or better than the handcrafted rules that are currently in use at NASD.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {449–458},
numpages = {10},
keywords = {fraud detection, statistical relational learning, relational probability trees},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081923,
author = {Nor\'{e}n, G. Niklas and Orre, Roland and Bate, Andrew},
title = {A Hit-Miss Model for Duplicate Detection in the WHO Drug Safety Database},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081923},
doi = {10.1145/1081870.1081923},
abstract = {The WHO Collaborating Centre for International Drug Monitoring in Uppsala, Sweden, maintains and analyses the world's largest database of reports on suspected adverse drug reaction incidents that occur after drugs are introduced on the market. As in other post-marketing drug safety data sets, the presence of duplicate records is an important data quality problem and the detection of duplicates in the WHO drug safety database remains a formidable challenge, especially since the reports are anonymised before submitted to the database. However, to our knowledge no work has been published on methods for duplicate detection in post-marketing drug safety data. In this paper, we propose a method for probabilistic duplicate detection based on the hit-miss model for statistical record linkage described by Copas &amp; Hilton. We present two new generalisations of the standard hit-miss model: a hit-miss mixture model for errors in numerical record fields and a new method to handle correlated record fields. We demonstrate the effectiveness of the hit-miss model for duplicate detection in the WHO drug safety database both at identifying the most likely duplicate for a given record (94.7% accuracy) and at discriminating duplicates from random matches (63% recall with 71% precision). The proposed method allows for more efficient data cleaning in post-marketing drug safety data sets, and perhaps other applications throughout the KDD community.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {459–468},
numpages = {10},
keywords = {duplicate detection, mixture models, hit-miss model},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@dataset{10.1145/review-1081870.1081923_R40669,
author = {Fulcher, John A.},
title = {Review ID:R40669 for DOI: 10.1145/1081870.1081923},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1081870.1081923_R40669}
}

@inproceedings{10.1145/1081870.1081924,
author = {Raskutti, Bhavani and Herschtal, Alan},
title = {Predicting the Product Purchase Patterns of Corporate Customers},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081924},
doi = {10.1145/1081870.1081924},
abstract = {This paper describes TIPPPS (Time Interleaved Product Purchase Prediction System), which analyses billing data of corporate customers in a large telecommunications company in order to predict high value upsell opportunities. The challenges presented by this prediction problem are significant. Firstly, the diversity of products used by corporate telecommunications customers is huge. This, coupled with low product take-up rates, makes this a problem of learning from a very high dimensional feature space with very few minority examples. Further, it is important to give priority specifically to the identification of those new customers who are of high value. These challenges are overcome by introducing a number of modifications to standard data pre-processing and machine learning algorithms, the most important of which are time-interleaving of data and value weighting. Time interleaving is the concatenation of examples from multiple time periods, thus increasing the number of training examples, and hence the number of minority examples. Value weighting assigns importance to minority examples in proportion to the dollar value of take-up, thus biasing the system to identify high value customers. These modifications create a novel algorithm that makes the prediction system practical and usable.Comparison with other techniques designed for similar problems shows that the expected average improvement in ranking accuracy achieved using these modifications is 3.7%. TIPPPS has been in operation for several months and has been successful in identifying many upsell opportunities that were not identified by using the previous manual system.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {469–478},
numpages = {10},
keywords = {area under ROC, upsell, SVM applications, learning from few positive examples},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081925,
author = {Song, Xiaodan and Lin, Ching-Yung and Tseng, Belle L. and Sun, Ming-Ting},
title = {Modeling and Predicting Personal Information Dissemination Behavior},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081925},
doi = {10.1145/1081870.1081925},
abstract = {In this paper, we propose a new way to automatically model and predict human behavior of receiving and disseminating information by analyzing the contact and content of personal communications. A personal profile, called CommunityNet, is established for each individual based on a novel algorithm incorporating contact, content, and time information simultaneously. It can be used for personal social capital management. Clusters of CommunityNets provide a view of informal networks for organization management. Our new algorithm is developed based on the combination of dynamic algorithms in the social network field and the semantic content classification methods in the natural language processing and machine learning literatures. We tested CommunityNets on the Enron Email corpus and report experimental results including filtering, prediction, and recommendation capabilities. We show that the personal behavior and intention are somewhat predictable based on these models. For instance, "to whom a person is going to send a specific email" can be predicted by one's personal social network and content analysis. Experimental results show the prediction accuracy of the proposed adaptive algorithm is 58% better than the social network-based predictions, and is 75% better than an aggregated model based on Latent Dirichlet Allocation with social network enhancement. Two online demo systems we developed that allow interactive exploration of CommunityNet are also discussed.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {479–488},
numpages = {10},
keywords = {personal information management, information dissemination, user behavior modeling},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081926,
author = {Tang, Jie and Li, Hang and Cao, Yunbo and Tang, Zhaohui},
title = {Email Data Cleaning},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081926},
doi = {10.1145/1081870.1081926},
abstract = {Addressed in this paper is the issue of 'email data cleaning' for text mining. Many text mining applications need take emails as input. Email data is usually noisy and thus it is necessary to clean it before mining. Several products offer email cleaning features, however, the types of noises that can be eliminated are restricted. Despite the importance of the problem, email cleaning has received little attention in the research community. A thorough and systematic investigation on the issue is thus needed. In this paper, email cleaning is formalized as a problem of non-text filtering and text normalization. In this way, email cleaning becomes independent from any specific text mining processing. A cascaded approach is proposed, which cleans up an email in four passes including non-text filtering, paragraph normalization, sentence normalization, and word normalization. As far as we know, non-text filtering and paragraph normalization have not been investigated previously. Methods for performing the tasks on the basis of Support Vector Machines (SVM) have also been proposed in this paper. Features in the models have been defined. Experimental results indicate that the proposed SVM based methods can significantly outperform the baseline methods for email cleaning. The proposed method has been applied to term extraction, a typical text mining processing. Experimental results show that the accuracy of term extraction can be significantly improved by using the data cleaning method.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {489–498},
numpages = {10},
keywords = {data cleaning, email processing, statistical learning, text mining},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081927,
author = {Yamanishi, Kenji and Maruyama, Yuko},
title = {Dynamic Syslog Mining for Network Failure Monitoring},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081927},
doi = {10.1145/1081870.1081927},
abstract = {Syslog monitoring technologies have recently received vast attentions in the areas of network management and network monitoring. They are used to address a wide range of important issues including network failure symptom detection and event correlation discovery. Syslogs are intrinsically dynamic in the sense that they form a time series and that their behavior may change over time. This paper proposes a new methodology of dynamic syslog mining in order to detect failure symptoms with higher confidence and to discover sequential alarm patterns among computer devices. The key ideas of dynamic syslog mining are 1) to represent syslog behavior using a mixture of Hidden Markov Models, 2) to adaptively learn the model using an on-line discounting learning algorithm in combination with dynamic selection of the optimal number of mixture components, and 3) to give anomaly scores using universal test statistics with a dynamically optimized threshold. Using real syslog data we demonstrate the validity of our methodology in the scenarios of failure symptom detection, emerging pattern identification, and correlation discovery.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {499–508},
numpages = {10},
keywords = {probabilistic modeling, model selection, failure detection, syslog mining, correlation analysis},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081928,
author = {Yan, Lian and Fassino, Michael and Baldasare, Patrick},
title = {Enhancing the Lift under Budget Constraints: An Application in the Mutual Fund Industry},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081928},
doi = {10.1145/1081870.1081928},
abstract = {A lift curve, with the true positive rate on the y-axis and the customer pull (or contact) rate on the x-axis, is often used to depict the model performance in many data mining applications, especially in the area of customer relationship management (CRM). Typically, these applications concern only the model accuracy at a relatively small pull or contact/intervention rate of the whole customer base, which is predetermined by a budget constraint for the project, e.g., how many customers can be contacted every month. In this paper, we address the important problem of enhancing the lift (true positive rate) at a specified pull rate. We propose two distinct algorithms, which are applicable to different scenarios. In particular, when the binary class label of the training set is extracted from a continuous variable, we can optimize a training objective which takes into account the specified pull rate rather than the class prior, based on the often ignored continuous variable. In those cases where only the binary class label is available during training, we propose a constrained optimization algorithm to maximize the true positive rate related to a specific decision threshold at which the specified pull rate is achieved. We applied both algorithms to our projects of predicting defection (decline in account value) of mutual fund accounts for two major U.S. mutual fund companies and achieved substantial enhancement of the lift at the specified pull rate.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {509–515},
numpages = {7},
keywords = {lift curve, true positive rate, mutual fund redemption, pull rate, ROC curve, neural networks},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081929,
author = {Yang, Chunsheng and L\'{e}tourneau, Sylvain},
title = {Learning to Predict Train Wheel Failures},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081929},
doi = {10.1145/1081870.1081929},
abstract = {This paper describes a successful but challenging application of data mining in the railway industry. The objective is to optimize maintenance and operation of trains through prognostics of wheel failures. In addition to reducing maintenance costs, the proposed technology will help improve railway safety and augment throughput. Building on established techniques from data mining and machine learning, we present a methodology to learn models to predict train wheel failures from readily available operational and maintenance data. This methodology addresses various data mining tasks such as automatic labeling, feature extraction, model building, model fusion, and evaluation. After a detailed description of the methodology, we report results from large-scale experiments. These results clearly show the great potential of this innovative application of data mining in the railway industry.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {516–525},
numpages = {10},
keywords = {methodology, data mining, model fusion, wheel failure prediction, model building, model evaluation, machine learning},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081931,
author = {Aggarwal, Charu C.},
title = {Towards Exploratory Test Instance Specific Algorithms for High Dimensional Classification},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081931},
doi = {10.1145/1081870.1081931},
abstract = {In an interactive classification application, a user may find it more valuable to develop a diagnostic decision support method which can reveal significant classification behavior of exemplar records. Such an approach has the additional advantage of being able to optimize the decision process for the individual record in order to design more effective classification methods. In this paper, we propose the Subspace Decision Path method which provides the user with the ability to interactively explore a small number of nodes of a hierarchical decision process so that the most significant classification characteristics for a given test instance are revealed. In addition, the SD-Path method can provide enormous interpretability by constructing views of the data in which the different classes are clearly separated out. Even in cases where the classification behavior of the test instance is ambiguous, the SD-Path method provides a diagnostic understanding of the characteristics which result in this ambiguity. Therefore, this method combines the abilities of the human and the computer in creating an effective diagnostic tool for instance-centered high dimensional classification.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {526–531},
numpages = {6},
keywords = {classification, visual data mining},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081932,
author = {Banerjee, Arindam and Krumpelman, Chase and Ghosh, Joydeep and Basu, Sugato and Mooney, Raymond J.},
title = {Model-Based Overlapping Clustering},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081932},
doi = {10.1145/1081870.1081932},
abstract = {While the vast majority of clustering algorithms are partitional, many real world datasets have inherently overlapping clusters. Several approaches to finding overlapping clusters have come from work on analysis of biological datasets. In this paper, we interpret an overlapping clustering model proposed by Segal et al. [23] as a generalization of Gaussian mixture models, and we extend it to an overlapping clustering model based on mixtures of any regular exponential family distribution and the corresponding Bregman divergence. We provide the necessary algorithm modifications for this extension, and present results on synthetic data as well as subsets of 20-Newsgroups and EachMovie datasets.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {532–537},
numpages = {6},
keywords = {high-dimensional clustering, graphical model, overlapping clustering, Bregman divergences, exponential model},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081933,
author = {Besemann, Christopher and Denton, Anne},
title = {Integration of Profile Hidden Markov Model Output into Association Rule Mining},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081933},
doi = {10.1145/1081870.1081933},
abstract = {Scientific models typically depend on parameters. Preserving the parameter dependence of models in the pattern mining context opens up several applications. Within association rule mining (ARM), the choice of parameters can be studied with more flexibly then in traditional model building. Studying support, confidence, and other rule metrics as a function of model parameters allows conclusions on assumptions underlying the models. We present efficient techniques to handle multiple model output data sets at little more than the cost of one. We integrate output from hidden Markov models into the association rule mining framework, demonstrating the potential for frequent pattern mining in the field of scientific modeling and experimentation.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {538–543},
numpages = {6},
keywords = {profile hidden Markov model, association rule mining, model mining},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081934,
author = {Carenini, Giuseppe and Ng, Raymond T. and Zhou, Xiaodong},
title = {Scalable Discovery of Hidden Emails from Large Folders},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081934},
doi = {10.1145/1081870.1081934},
abstract = {The popularity of email has triggered researchers to look for ways to help users better organize the enormous amount of information stored in their email folders. One challenge that has not been studied extensively in text mining is the identification and reconstruction of hidden emails. A hidden email is an original email that has been quoted in at least one email in a folder, but does not present itself in the same folder. It may have been (un)intentionally deleted or may never have been received. The discovery and reconstruction of hidden emails is critical for many applications including email classification, summarization and forensics. This paper proposes a framework for reconstructing hidden emails using the embedded quotations found in messages further down the thread hierarchy. We evaluate the robustness and scalability of our framework by using the Enron public email corpus. Our experiments show that hidden emails exist widely in that corpus and also that our optimization techniques are effective in processing large email folders.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {544–549},
numpages = {6},
keywords = {text mining, hidden email, forensics},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081935,
author = {Chen, Xin and Wu, Yi-fang Brook},
title = {Web Mining from Competitors' Websites},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081935},
doi = {10.1145/1081870.1081935},
abstract = {This paper presents a framework for user-oriented text mining. It is then illustrated with an example of discovering knowledge from competitors' websites. The knowledge to be discovered is in the form of association rules. A user's background knowledge is represented as a concept hierarchy developed from documents on his/her own website. The concept hierarchy captures the semantic usage of words and relationships among words in background documents. Association rules are identified among the noun phrases extracted from documents on competitors' websites. The interestingness measure, i.e. novelty, which measures the semantic distance between the antecedent and the consequent of a rule in the background knowledge, is computed from the co-occurrence frequency of words and the connection lengths among words in the concept hierarchy. A user evaluation of the novelty of discovered rules demonstrates that the correlation between the algorithm and the human judges is comparable to that between human judges.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {550–555},
numpages = {6},
keywords = {text mining, association rules mining, background knowledge, novelty},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081936,
author = {Chen, Chien Chin and Chen, Meng Chang and Chen, Ming-Syan},
title = {LIPED: HMM-Based Life Profiles for Adaptive Event Detection},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081936},
doi = {10.1145/1081870.1081936},
abstract = {In this paper, the proposed LIPED (LIfe Profile based Event Detection) employs the concept of life profiles to predict the activeness of event for effective event detection. A group of events with similar activeness patterns shares a life profile, modeled by a hidden Markov model. Considering the burst-and-diverse property of events, LIPED identifies the activeness status of event. As a result, LIPED balances the clustering precision and recall to achieve better F1 scores than other well known approaches evaluated on the official TDT1 corpus.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {556–561},
numpages = {6},
keywords = {hidden markov models, clustering, event detection, life profiles},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081937,
author = {Cong, Shengnan and Han, Jiawei and Padua, David},
title = {Parallel Mining of Closed Sequential Patterns},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081937},
doi = {10.1145/1081870.1081937},
abstract = {Discovery of sequential patterns is an essential data mining task with broad applications. Among several variations of sequential patterns, closed sequential pattern is the most useful one since it retains all the information of the complete pattern set but is often much more compact than it. Unfortunately, there is no parallel closed sequential pattern mining method proposed yet. In this paper we develop an algorithm, called Par-CSP (Parallel Closed Sequential Pattern mining), to conduct parallel mining of closed sequential patterns on a distributed memory system. Par-CSP partitions the work among the processors by exploiting the divide-and-conquer property so that the overhead of interprocessor communication is minimized. Par-CSP applies dynamic scheduling to avoid processor idling. Moreover, it employs a technique, called selective sampling to address the load imbalance problem. We implement Par-CSP using MPI on a 64-node Linux cluster. Our experimental results show that Par-CSP attains good parallelization efficiencies on various input datasets.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {562–567},
numpages = {6},
keywords = {sampling, parallel algorithms, load balancing},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081938,
author = {Fast, Andrew and Jensen, David and Levine, Brian Neil},
title = {Creating Social Networks to Improve Peer-to-Peer Networking},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081938},
doi = {10.1145/1081870.1081938},
abstract = {We use knowledge discovery techniques to guide the creation of efficient overlay networks for peer-to-peer file sharing. An overlay network specifies the logical connections among peers in a network and is distinct from the physical connections of the network. It determines the order in which peers will be queried when a user is searching for a specific file. To better understand the role of the network overlay structure in the performance of peer-to-peer file sharing protocols, we compare several methods for creating overlay networks. We analyze the networks using data from a campus network for peer-to-peer file sharing that recorded anonymized data on 6,528 users sharing 291,925 music files over an 81-day period. We propose a novel protocol for overlay creation based on a model of user preference identified by latent-variable clustering with hierarchical Dirichlet processes (HDPs). Our simulations and empirical studies show that the clusters of songs created by HDPs effectively model user behavior and can be used to create desirable network overlays that outperform alternative approaches.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {568–573},
numpages = {6},
keywords = {distributed hash tables, social networks, hierarchical dirichlet processes, peer-to-peer networks, overlay networks},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081939,
author = {Guha, R. and Kumar, Ravi and Sivakumar, D. and Sundaram, Ravi},
title = {Unweaving a Web of Documents},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081939},
doi = {10.1145/1081870.1081939},
abstract = {We develop an algorithmic framework to decompose a collection of time-stamped text documents into semantically coherent threads. Our formulation leads to a graph decomposition problem on directed acyclic graphs, for which we obtain three algorithms --- an exact algorithm that is based on minimum cost flow and two more efficient algorithms based on maximum matching and dynamic programming that solve specific versions of the graph decomposition problem. Applications of our algorithms include superior summarization of news search results, improved browsing paradigms for large collections of text-intensive corpora, and integration of time-stamped documents from a variety of sources. Experimental results based on over 250,000 news articles from a major newspaper over a period of four years demonstrate that our algorithms efficiently identify robust threads of varying lengths and time-spans.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {574–579},
numpages = {6},
keywords = {news threads, graph algorithms, graph decomposition},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081940,
author = {Heeren, Cinda and Pitt, Leonard},
title = {Maximal Boasting},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081940},
doi = {10.1145/1081870.1081940},
abstract = {We introduce the boasting problem, wherein useful trends in historical ordinal data (rankings) are discovered. Claims of the form "our object was ranked r or better in x of the last t time units," are formalized, and maximal claims (boasts) of this form are defined under two natural partial orders. For the first partial order, we give an efficient and optimal algorithm for finding all such maximal claims. For the second, we apply a classical result from computational geometry to achieve an algorithm whose running time is significantly more efficient than that of a na\"{\i}ve one. Finally, we connect this boasting problem to a novel variation of the problem of finding optimized confidence association rules as originally posed by Fukuda, et al. [2], and give an efficient algorithm for solving a simplification of the new problem.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {580–585},
numpages = {6},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081941,
author = {Ito, Takahiko and Shimbo, Masashi and Kudo, Taku and Matsumoto, Yuji},
title = {Application of Kernels to Link Analysis},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081941},
doi = {10.1145/1081870.1081941},
abstract = {The application of kernel methods to link analysis is explored. In particular, Kandola et al.'s Neumann kernels are shown to subsume not only the co-citation and bibliographic coupling relatedness but also Kleinberg's HITS importance. These popular measures of relatedness and importance correspond to the Neumann kernels at the extremes of their parameter range, and hence these kernels can be interpreted as defining a spectrum of link analysis measures intermediate between co-citation/bibliographic coupling and HITS. We also show that the kernels based on the graph Laplacian, including the regularized Laplacian and diffusion kernels, provide relatedness measures that overcome some limitations of co-citation relatedness. The property of these kernel-based link analysis measures is examined with a network of bibliographic citations. Practical issues in applying these methods to real data are discussed, and possible solutions are proposed.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {586–592},
numpages = {7},
keywords = {HITS, graph kernel, link analysis, co-citation coupling},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081942,
author = {Jagannathan, Geetha and Wright, Rebecca N.},
title = {Privacy-Preserving Distributed k-Means Clustering over Arbitrarily Partitioned Data},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081942},
doi = {10.1145/1081870.1081942},
abstract = {Advances in computer networking and database technologies have enabled the collection and storage of vast quantities of data. Data mining can extract valuable knowledge from this data, and organizations have realized that they can often obtain better results by pooling their data together. However, the collected data may contain sensitive or private information about the organizations or their customers, and privacy concerns are exacerbated if data is shared between multiple organizations.Distributed data mining is concerned with the computation of models from data that is distributed among multiple participants. Privacy-preserving distributed data mining seeks to allow for the cooperative computation of such models without the cooperating parties revealing any of their individual data items. Our paper makes two contributions in privacy-preserving data mining. First, we introduce the concept of arbitrarily partitioned data, which is a generalization of both horizontally and vertically partitioned data. Second, we provide an efficient privacy-preserving protocol for k-means clustering in the setting of arbitrarily partitioned data.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {593–599},
numpages = {7},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081943,
author = {Jin, Ruoming and Sinha, Kaushik and Agrawal, Gagan},
title = {Simultaneous Optimization of Complex Mining Tasks with a Knowledgeable Cache},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081943},
doi = {10.1145/1081870.1081943},
abstract = {With an increasing use of data mining tools and techniques, we envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for the following scenarios: 1) Sequence of Queries: A user may analyze one or more datasets by issuing a sequence of related complex mining queries, and 2) Multiple Simultaneous Queries: Several users may be analyzing a set of datasets concurrently, and may issue related complex queries.This paper presents a systematic mechanism to optimize for the above cases, targeting the class of mining queries involving frequent pattern mining on one or multiple datasets. We present a system architecture and propose new algorithms to simultaneously optimize multiple such queries and use a knowledgeable cache to store and utilize the past query results. We have implemented and evaluated our system with both real and synthetic datasets. Our experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {600–605},
numpages = {6},
keywords = {knowledgeable cache, multiple query optimization, frequent pattern mining},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081944,
author = {Jin, R. and Wang, C. and Polshakov, D. and Parthasarathy, S. and Agrawal, G.},
title = {Discovering Frequent Topological Structures from Graph Datasets},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081944},
doi = {10.1145/1081870.1081944},
abstract = {The problem of finding frequent patterns from graph-based datasets is an important one that finds applications in drug discovery, protein structure analysis, XML querying, and social network analysis among others. In this paper we propose a framework to mine frequent large-scale structures, formally defined as frequent topological structures, from graph datasets. Key elements of our framework include, fast algorithms for discovering frequent topological patterns based on the well known notion of a topological minor, algorithms for specifying and pushing constraints deep into the mining process for discovering constrained topological patterns, and mechanisms for specifying approximate matches when discovering frequent topological patterns in noisy datasets. We demonstrate the viability and scalability of the proposed algorithms on real and synthetic datasets and also discuss the use of the framework to discover meaningful topological structures from protein structure data.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {606–611},
numpages = {6},
keywords = {frequent graph pattern, graph mining, topological minor},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081945,
author = {Jin, Xin and Zhou, Yanzan and Mobasher, Bamshad},
title = {A Maximum Entropy Web Recommendation System: Combining Collaborative and Content Features},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081945},
doi = {10.1145/1081870.1081945},
abstract = {Web users display their preferences implicitly by navigating through a sequence of pages or by providing numeric ratings to some items. Web usage mining techniques are used to extract useful knowledge about user interests from such data. The discovered user models are then used for a variety of applications such as personalized recommendations. Web site content or semantic features of objects provide another source of knowledge for deciphering users' needs or interests. We propose a novel Web recommendation system in which collaborative features such as navigation or rating data as well as the content features accessed by the users are seamlessly integrated under the maximum entropy principle. Both the discovered user patterns and the semantic relationships among Web objects are represented as sets of constraints that are integrated to fit the model. In the case of content features, we use a new approach based on Latent Dirichlet Allocation (LDA) to discover the hidden semantic relationships among items and derive constraints used in the model. Experiments on real Web site usage data sets show that this approach can achieve better recommendation accuracy, when compared to systems using only usage information. The integration of semantic information also allows for better interpretation of the generated recommendations.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {612–617},
numpages = {6},
keywords = {user profiling, recommendation, maximum entropy, web usage mining},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081946,
author = {Kawamae, Noriaki and Takahashi, Katsumi},
title = {Information Retrieval Based on Collaborative Filtering with Latent Interest Semantic Map},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081946},
doi = {10.1145/1081870.1081946},
abstract = {In this paper, we propose an information retrieval model called Latent Interest Semantic Map (LISM), which features retrieval composed of both Collaborative Filtering(CF) and Probabilistic Latent Semantic Analysis (PLSA). The motivation behind this study is that the relation between users and documents can be explained by the two different latent classes, where users belong probabilistically in one or more classes with the same interest groups, while documents also belong probabilistically in one or more class with the same topic groups. The novel aspect of LISM is that it simultaneously provides a user model and latent semantic analysis in one map. This benefit of LISM is to enable collaborative filtering in terms of user interest and document topic and thus solve the cold start problem.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {618–623},
numpages = {6},
keywords = {relationship analysis, user behavior, query suggestion, search results, latent semantic indexing, document categorization, collaborative filtering, Bayesian statistics},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081947,
author = {Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir},
title = {Determining an Author's Native Language by Mining a Text for Errors},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081947},
doi = {10.1145/1081870.1081947},
abstract = {In this paper, we show that stylistic text features can be exploited to determine an anonymous author's native language with high accuracy. Specifically, we first use automatic tools to ascertain frequencies of various stylistic idiosyncrasies in a text. These frequencies then serve as features for support vector machines that learn to classify texts according to author native language.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {624–628},
numpages = {5},
keywords = {author profiling, text mining},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081948,
author = {Dhillon, Inderjit and Guan, Yuqiang and Kulis, Brian},
title = {A Fast Kernel-Based Multilevel Algorithm for Graph Clustering},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081948},
doi = {10.1145/1081870.1081948},
abstract = {Graph clustering (also called graph partitioning) --- clustering the nodes of a graph --- is an important problem in diverse data mining applications. Traditional approaches involve optimization of graph clustering objectives such as normalized cut or ratio association; spectral methods are widely used for these objectives, but they require eigenvector computation which can be slow. Recently, graph clustering with a general cut objective has been shown to be mathematically equivalent to an appropriate weighted kernel k-means objective function. In this paper, we exploit this equivalence to develop a very fast multilevel algorithm for graph clustering. Multilevel approaches involve coarsening, initial partitioning and refinement phases, all of which may be specialized to different graph clustering objectives. Unlike existing multilevel clustering approaches, such as METIS, our algorithm does not constrain the cluster sizes to be nearly equal. Our approach gives a theoretical guarantee that the refinement step decreases the graph cut objective under consideration. Experiments show that we achieve better final objective function values as compared to a state-of-the-art spectral clustering algorithm: on a series of benchmark test graphs with up to thirty thousand nodes and one million edges, our algorithm achieves lower normalized cut values in 67% of our experiments and higher ratio association values in 100% of our experiments. Furthermore, on large graphs, our algorithm is significantly faster than spectral methods. Finally, our algorithm requires far less memory than spectral methods; we cluster a 1.2 million node movie network into 5000 clusters, which due to memory requirements cannot be done directly with spectral methods.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {629–634},
numpages = {6},
keywords = {spectral clustering, multilevel methods, kernel methods, graph clustering},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081949,
author = {Long, Bo and Zhang, Zhongfei (Mark) and Yu, Philip S.},
title = {Co-Clustering by Block Value Decomposition},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081949},
doi = {10.1145/1081870.1081949},
abstract = {Dyadic data matrices, such as co-occurrence matrix, rating matrix, and proximity matrix, arise frequently in various important applications. A fundamental problem in dyadic data analysis is to find the hidden block structure of the data matrix. In this paper, we present a new co-clustering framework, block value decomposition(BVD), for dyadic data, which factorizes the dyadic data matrix into three components, the row-coefficient matrix R, the block value matrix B, and the column-coefficient matrix C. Under this framework, we focus on a special yet very popular case -- non-negative dyadic data, and propose a specific novel co-clustering algorithm that iteratively computes the three decomposition matrices based on the multiplicative updating rules. Extensive experimental evaluations also demonstrate the effectiveness and potential of this framework as well as the specific algorithms for co-clustering, and in particular, for discovering the hidden block structure in the dyadic data.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {635–640},
numpages = {6},
keywords = {block value decomposition (BVD), hidden block structure, dyadic data, non-negative block value decomposition (NBVD), co-clustering, matrix decomposition, clustering},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081950,
author = {Lowd, Daniel and Meek, Christopher},
title = {Adversarial Learning},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081950},
doi = {10.1145/1081870.1081950},
abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {641–647},
numpages = {7},
keywords = {spam, linear classifiers, adversarial classification},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081951,
author = {Mane, Sandeep and Srivastava, Jaideep and Hwang, San-Yih},
title = {Estimating Missed Actual Positives Using Independent Classifiers},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081951},
doi = {10.1145/1081870.1081951},
abstract = {Data mining is increasingly being applied in environments having very high rate of data generation like network intrusion detection [7], where routers generate about 300,000 -- 500,000 connections every minute. In such rare class data domains, the cost of missing a rare-class instance is much higher than that of other classes. However, the high cost for manual labeling of instances, the high rate at which data is collected as well as real-time response constraints do not always allow one to determine the actual classes for the collected unlabeled datasets. In our previous work [9], this problem of missed false negatives was explained in context of two different domains -- "network intrusion detection" and "business opportunity classification". In such cases, an estimate for the number of such missed high-cost, rare instances will aid in the evaluation of the performance of the modeling technique (e.g. classification) used. A capture-recapture method was used for estimating false negatives, using two or more learning methods (i.e. classifiers). This paper focuses on the dependence between the class labels assigned by such learners. We define the conditional independence for classifiers given a class label and show its relation to the conditional independence of the features sets (used by the classifiers) given a class label. The later is a computationally expensive problem and hence, a heuristic algorithm is proposed for obtaining conditionally independent (or less dependent) feature sets for the classifiers. Initial results of this algorithm on synthetic datasets are promising and further research is being pursued.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {648–653},
numpages = {6},
keywords = {conditional independence of classifiers given class label, conditional mutual information, conditional independence of features given class label, capture-recapture method, false negative},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081952,
author = {Momma, Michinari},
title = {Efficient Computations via Scalable Sparse Kernel Partial Least Squares and Boosted Latent Features},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081952},
doi = {10.1145/1081870.1081952},
abstract = {Kernel partial least squares (KPLS) has been known as a generic kernel regression method and proven to be competitive with other kernel regression methods such as support vector machines for regression (SVM) and kernel ridge regression. Kernel boosted latent features (KBLF) is a variant of KPLS for any differentiable convex loss functions. It provides a more flexible framework for various predictive modeling tasks such as classification with logistic loss and robust regression with L1 norm loss, etc. However, KPLS and KBLF solutions are dense and thus not suitable for large-scale computations. Sparsification of KPLS solutions has been studied for dual and primal forms. For dual sparsity, it requires solving a nonlinear optimization problem at every iteration step and its computational burden limits its applicability to general regression tasks.In this paper, we propose simple heuristics to approximate sparse solutions for KPLS and the framework is also applied for sparsifying KBLF solutions. The algorithm provides an interesting "path" from a maximum residual criterion based algorithm with orthogonality conditions to the dense KPLS/KBLF. With the orthogonality, it differentiates itself from many existing forward selection-type algorithms. The computational advantage is illustrated by benchmark datasets and comparison to SVM is done.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {654–659},
numpages = {6},
keywords = {partial least squares, boosted latent features, scalable and sparse kernel method},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081953,
author = {M\"{o}rchen, Fabian and Ultsch, Alfred},
title = {Optimizing Time Series Discretization for Knowledge Discovery},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081953},
doi = {10.1145/1081870.1081953},
abstract = {Knowledge Discovery in time series usually requires symbolic time series. Many discretization methods that convert numeric time series to symbolic time series ignore the temporal order of values. This often leads to symbols that do not correspond to states of the process generating the time series and cannot be interpreted meaningfully. We propose a new method for meaningful unsupervised discretization of numeric time series called Persist. The algorithm is based on the Kullback-Leibler divergence between the marginal and the self-transition probability distributions of the discretization symbols. Its performance is evaluated on both artificial and real life data in comparison to the most common discretization methods. Persist achieves significantly higher accuracy than existing static methods and is robust against noise. It also outperforms Hidden Markov Models for all but very simple cases.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {660–665},
numpages = {6},
keywords = {discretization, persistence, time series},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081954,
author = {Morinaga, Satoshi and Arimura, Hiroki and Ikeda, Takahiro and Sakao, Yosuke and Akamine, Susumu},
title = {Key Semantics Extraction by Dependency Tree Mining},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081954},
doi = {10.1145/1081870.1081954},
abstract = {We propose a new text mining system which extracts characteristic contents from given documents. We define Key semantics as characteristic sub-structures of syntactic dependencies in the given documents, and consider the following three tasks in this paper: 1)Key semantics extraction: extracting characteristic syntactic dependency structures not only as ordered trees but also as unordered trees and free trees, 2)Redundancy reduction: from the result of extraction, deleting redundant dependency structures such as sub-structures or equivalent structures of the others, and 3)Phrase/sentence reconstruction: generating a phrase or sentence in a natural language corresponding to the extracted structure.Our system is a combination of natural language processing techniques and tree mining techniques. The system consists of the following five units: 1) syntactic dependency analysis unit, 2) input filters, 3) characteristic ordered subtree extraction unit, 4) output filters, and 5) phrase/sentence reconstruction unit. Although ordered trees are extracted in the third unit, the overall behavior of the system can be switched into the extraction of ordered trees, unordered trees, or free trees depending on which of the input filters is/are applied in the second step. The output filters delete redundant trees from the extraction result for efficient knowledge discovery. Finally, phrases or sentences corresponding to the extracted subtrees are reconstructed by utilizing the input documents.We demonstrate the validity of our system by showing experimental results using real data collected at a help desk and TDT pilot corpus.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {666–671},
numpages = {6},
keywords = {phrase/sentence reconstruction, tree enumeration, redundancy reduction, text mining, syntactic dependency},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081955,
author = {Kriegel, Hans-Peter and Pfeifle, Martin},
title = {Density-Based Clustering of Uncertain Data},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081955},
doi = {10.1145/1081870.1081955},
abstract = {In many different application areas, e.g. sensor databases, location based services or face recognition systems, distances between odjects have to be computed based on vague and uncertain data. Commonly, the distances between these uncertain object descriptions are expressed by one numerical distance value. Based on such single-valued distance functions standard data mining algorithms can work without any changes. In this paper, we propose to express the similarity between two fuzzy objects by distance probability functions. These fuzzy distance functions assign a probability value to each possible distance value. By integrating these fuzzy distance functions directly into data mining algorithms, the full information provided by these functions is exploited. In order to demonstrate the benefits of this general approach, we enhance the density-based clustering algorithm DBSCAN so that it can work directly on these fuzzy distance functions. In a detailed experimental evaluation based on artificial and real-world data sets, we show the characteristics and benefits of our new approach.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {672–677},
numpages = {6},
keywords = {uncertain data, density-based clustering, fuzzy distance functions},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081956,
author = {Spertus, Ellen and Sahami, Mehran and Buyukkokten, Orkut},
title = {Evaluating Similarity Measures: A Large-Scale Study in the Orkut Social Network},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081956},
doi = {10.1145/1081870.1081956},
abstract = {Online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering, which makes recommendations to users based on their collective past behavior. While many similarity measures have been proposed and individually evaluated, they have not been evaluated relative to each other in a large real-world environment. We present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the Orkut social network. We determine the usefulness of the different recommendations by actually measuring users' propensity to visit and join recommended communities. We also examine how the ordering of recommendations influenced user selection, as well as interesting social issues that arise in recommending communities within a real social network.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {678–684},
numpages = {7},
keywords = {social networks, online communities, similarity measure, data mining, recommender system, collaborative filtering},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081957,
author = {Surdeanu, Mihai and Turmo, Jordi and Ageno, Alicia},
title = {A Hybrid Unsupervised Approach for Document Clustering},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081957},
doi = {10.1145/1081870.1081957},
abstract = {We propose a hybrid, unsupervised document clustering approach that combines a hierarchical clustering algorithm with Expectation Maximization. We developed several heuristics to automatically select a subset of the clusters generated by the first algorithm as the initial points of the second one. Furthermore, our initialization algorithm generates not only an initial model for the iterative refinement algorithm but also an estimate of the model dimension, thus eliminating another important element of human supervision. We have evaluated the proposed system on five real-world document collections. The results show that our approach generates clustering solutions of higher quality than both its individual components.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {685–690},
numpages = {6},
keywords = {EM initialization, unsupervised clustering},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081958,
author = {Tao, Tao and Zhai, ChengXiang},
title = {Mining Comparable Bilingual Text Corpora for Cross-Language Information Integration},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081958},
doi = {10.1145/1081870.1081958},
abstract = {Integrating information in multiple natural languages is a challenging task that often requires manually created linguistic resources such as a bilingual dictionary or examples of direct translations of text. In this paper, we propose a general cross-lingual text mining method that does not rely on any of these resources, but can exploit comparable bilingual text corpora to discover mappings between words and documents in different languages. Comparable text corpora are collections of text documents in different languages that are about similar topics; such text corpora are often naturally available (e.g., news articles in different languages published in the same time period). The main idea of our method is to exploit frequency correlations of words in different languages in the comparable corpora and discover mappings between words in different languages. Such mappings can then be used to further discover mappings between documents in different languages, achieving cross-lingual information integration. Evaluation of the proposed method on a 120MB Chinese-English comparable news collection shows that the proposed method is effective for mapping words and documents in English and Chinese. Since our method only relies on naturally available comparable corpora, it is generally applicable to any language pairs as long as we have comparable corpora.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {691–696},
numpages = {6},
keywords = {frequency correlation, document alignment, cross-lingual text mining, comparable corpora},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081959,
author = {Torgo, Lu\'{\i}s},
title = {Regression Error Characteristic Surfaces},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081959},
doi = {10.1145/1081870.1081959},
abstract = {This paper presents a generalization of Regression Error Characteristic (REC) curves. REC curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of ROC curves to regression problems. REC curves provide useful information for analyzing the performance of models, particularly when compared to error statistics like for instance the Mean Squared Error. In this paper we present Regression Error Characteristic (REC) surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable, i.e. the joint cumulative distribution function of the errors and the target variable. This provides a more detailed analysis of the performance of models when compared to REC curves. This extra detail is particularly relevant in applications with non-uniform error costs, where it is important to study the performance of models for specific ranges of the target variable. In this paper we present the notion of REC surfaces, describe how to use them to compare the performance of models, and illustrate their use with an important practical class of applications: the prediction of rare extreme values.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {697–702},
numpages = {6},
keywords = {regression problems, evaluation metrics, model comparisons},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081960,
author = {Wu, Gang and Chang, Edward Y. and Panda, Navneet},
title = {Formulating Distance Functions via the Kernel Trick},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081960},
doi = {10.1145/1081870.1081960},
abstract = {Tasks of data mining and information retrieval depend on a good distance function for measuring similarity between data instances. The most effective distance function must be formulated in a context-dependent (also application-, data-, and user-dependent) way. In this paper, we propose to learn a distance function by capturing the nonlinear relationships among contextual information provided by the application, data, or user. We show that through a process called the "kernel trick," such nonlinear relationships can be learned efficiently in a projected space. Theoretically, we substantiate that our method is both sound and optimal. Empirically, using several datasets and applications, we demonstrate that our method is effective and useful.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {703–709},
numpages = {7},
keywords = {distance function, kernel trick},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081961,
author = {Yang, Ying and Wu, Xindong and Zhu, Xingquan},
title = {Combining Proactive and Reactive Predictions for Data Streams},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081961},
doi = {10.1145/1081870.1081961},
abstract = {Mining data streams is important in both science and commerce. Two major challenges are (1) the data may grow without limit so that it is difficult to retain a long history; and (2) the underlying concept of the data may change over time. Different from common practice that keeps recent raw data, this paper uses a measure of conceptual equivalence to organize the data history into a history of concepts. Along the journey of concept change, it identifies new concepts as well as re-appearing ones, and learns transition patterns among concepts to help prediction. Different from conventional methodology that passively waits until the concept changes, this paper incorporates proactive and reactive predictions. In a proactive mode, it anticipates what the new concept will be if a future concept change takes place, and prepares prediction strategies in advance. If the anticipation turns out to be correct, a proper prediction model can be launched instantly upon the concept change. If not, it promptly resorts to a reactive mode: adapting a prediction model to the new data. A system RePro is proposed to implement these new ideas. Experiments compare the system with representative existing prediction methods on various benchmark data sets that represent diversified scenarios of concept change. Empirical evidence demonstrates that the proposed methodology is an effective and efficient solution to prediction for data streams.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {710–715},
numpages = {6},
keywords = {data stream, conceptual equivalence, proactive learning},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081962,
author = {Yang, Hui and Parthasarathy, Srinivasan and Mehta, Sameep},
title = {A Generalized Framework for Mining Spatio-Temporal Patterns in Scientific Data},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081962},
doi = {10.1145/1081870.1081962},
abstract = {In this paper, we present a general framework to discover spatial associations and spatio-temporal episodes for scientific datasets. In contrast to previous work in this area, features are modeled as geometric objects rather than points. We define multiple distance metrics that take into account objects' extent and thus are more robust in capturing the influence of an object on other objects in spatial neighborhood. We have developed algorithms to discover four different types of spatial object interaction (association) patterns. We also extend our approach to accommodate temporal information and propose a simple algorithm to derive spatio-temporal episodes. We show that such episodes can be used to reason about critical events. We evaluate our framework on real datasets to demonstrate its efficacy. The datasets originate from two different areas: Computational Molecular Dynamics and Computational Fluid Flow. We present results highlighting the importance of the identified patterns and episodes by using knowledge from the underlying domains. We also show that the proposed algorithms scale linearly with respect to the dataset size.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {716–721},
numpages = {6},
keywords = {spatio-temporal association/episode, scientific data, spatial object association},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081963,
author = {Yang, Li},
title = {Building Connected Neighborhood Graphs for Isometric Data Embedding},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081963},
doi = {10.1145/1081870.1081963},
abstract = {Neighborhood graph construction is usually the first step in algorithms for isometric data embedding and manifold learning that cope with the problem of projecting high dimensional data to a low space. This paper begins by explaining the algorithmic fundamentals of techniques for isometric data embedding and derives a general classification of these techniques. We will see that the nearest neighbor approaches commonly used to construct neighborhood graphs do not guarantee connectedness of the constructed neighborhood graphs and, consequently, may cause an algorithm fail to project data to a single low dimensional coordinate system. In this paper, we review three existing methods to construct k-edge-connected neighborhood graphs and propose a new method to construct k-connected neighborhood graphs. These methods are applicable to a wide range of data including data distributed among clusters. Their features are discussed and compared through experiments.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {722–728},
numpages = {7},
keywords = {manifold learning, graph connectivity, dimensionality reduction, data embedding},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081964,
author = {Za\"{\i}ane, Osmar R. and El-Hajj, Mohammad},
title = {Pattern Lattice Traversal by Selective Jumps},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081964},
doi = {10.1145/1081870.1081964},
abstract = {Regardless of the frequent patterns to discover, either the full frequent patterns or the condensed ones, either closed or maximal, the strategy always includes the traversal of the lattice of candidate patterns. We study the existing depth versus breadth traversal approaches for generating candidate patterns and propose in this paper a new traversal approach that jumps in the search space among only promising nodes. Our leaping approach avoids nodes that would not participate in the answer set and reduce drastically the number of candidate patterns. We use this approach to efficiently pinpoint maximal patterns at the border of the frequent patterns in the lattice and collect enough information in the process to generate all subsequent patterns.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {729–735},
numpages = {7},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081965,
author = {Zaki, Mohammed J. and Peters, Markus and Assent, Ira and Seidl, Thomas},
title = {CLICKS: An Effective Algorithm for Mining Subspace Clusters in Categorical Datasets},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081965},
doi = {10.1145/1081870.1081965},
abstract = {We present a novel algorithm called CLICKS, that finds clusters in categorical datasets based on a search for k-partite maximal cliques. Unlike previous methods, CLICKS mines subspace clusters. It uses a selective vertical method to guarantee complete search. CLICKS outperforms previous approaches by over an order of magnitude and scales better than any of the existing method for high-dimensional datasets. These results are demonstrated in a comprehensive performance study on real and synthetic datasets.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {736–742},
numpages = {7},
keywords = {data mining, k-partite graph, clustering, categorical data, maximal cliques},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081966,
author = {Cole, Richard and Shasha, Dennis and Zhao, Xiaojian},
title = {Fast Window Correlations over Uncooperative Time Series},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081966},
doi = {10.1145/1081870.1081966},
abstract = {Data arriving in time order (a data stream) arises in fields including physics, finance, medicine, and music, to name a few. Often the data comes from sensors (in physics and medicine for example) whose data rates continue to improve dramatically as sensor technology improves. Further, the number of sensors is increasing, so correlating data between sensors becomes ever more critical in order to distill knowlege from the data. In many applications such as finance, recent correlations are of far more interest than long-term correlation, so correlation over sliding windows (windowed correlation) is the desired operation. Fast response is desirable in many applications (e.g., to aim a telescope at an activity of interest or to perform a stock trade). These three factors -- data size, windowed correlation, and fast response -- motivate this work.Previous work [10, 14] showed how to compute Pearson correlation using Fast Fourier Transforms and Wavelet transforms, but such techniques don't work for time series in which the energy is spread over many frequency components, thus resembling white noise. For such "uncooperative" time series, this paper shows how to combine several simple techniques -- sketches (random projections), convolution, structured random vectors, grid structures, and combinatorial design -- to achieve high performance windowed Pearson correlation over a variety of data sets.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {743–749},
numpages = {7},
keywords = {correlation, randomized algorithms, time series},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081968,
author = {Chen, Haifeng and Jiang, Guofei and Ungureanu, Cristian and Yoshihira, Kenji},
title = {Failure Detection and Localization in Component Based Systems by Online Tracking},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081968},
doi = {10.1145/1081870.1081968},
abstract = {The increasing complexity of today's systems makes fast and accurate failure detection essential for their use in mission-critical applications. Various monitoring methods provide a large amount of data about system's behavior. Analyzing this data with advanced statistical methods holds the promise of not only detecting the errors faster, but also detecting errors which are difficult to catch with current monitoring tools. Two challenges to building such detection tools are: the high dimensionality of observation data, which makes the models expensive to apply, and frequent system changes, which make the models expensive to update. In this paper, we present algorithms to reduce the dimensionality of data in a way that makes it easy to adapt to system changes. We decompose the observation data into signal and noise subspaces. Two statistics, the Hotelling T2 score and squared prediction error (SPE) are calculated to represent the data characteristics in signal and noise subspaces respectively. Instead of tracking the original data, we use a sequentially discounting expectation maximization (SDEM) algorithm to learn the distribution of the two extracted statistics. A failure event can then be detected based on the abnormal change of the distribution. Applying our technique to component interaction data in a simple e-commerce application shows better accuracy than building independent profiles for each component. Additionally, experiments on synthetic data show that the detection accuracy is high even for changing systems.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {750–755},
numpages = {6},
keywords = {distributed computing, online tracking, internet services, failure detection, statistics, subspace decomposition},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081969,
author = {Jeske, Daniel R. and Samadi, Behrokh and Lin, Pengyue J. and Ye, Lan and Cox, Sean and Xiao, Rui and Younglove, Ted and Ly, Minh and Holt, Douglas and Rich, Ryan},
title = {Generation of Synthetic Data Sets for Evaluating the Accuracy of Knowledge Discovery Systems},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081969},
doi = {10.1145/1081870.1081969},
abstract = {Information Discovery and Analysis Systems (IDAS) are designed to correlate multiple sources of data and use data mining techniques to identify potential significant events. Application domains for IDAS are numerous and include the emerging area of homeland security.Developing test cases for an IDAS requires background data sets into which hypothetical future scenarios can be overlaid. The IDAS can then be measured in terms of false positive and false negative error rates. Obtaining the test data sets can be an obstacle due to both privacy issues and also the time and cost associated with collecting a diverse set of data sources.In this paper, we give an overview of the design and architecture of an IDAS Data Set Generator (IDSG) that enables a fast and comprehensive test of an IDAS. The IDSG generates data using statistical and rule-based algorithms and also semantic graphs that represent interdependencies between attributes. A credit card transaction application is used to illustrate the approach.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {756–762},
numpages = {7},
keywords = {data mining, information discovery, data generation},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081970,
author = {Kalos, Alex and Rey, Tim},
title = {Data Mining in the Chemical Industry},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081970},
doi = {10.1145/1081870.1081970},
abstract = {In this paper we describe the experience of introducing data mining to a large chemical manufacturing company. The multi-national nature of doing business with multiple business units, presents a unique opportunity for the deployment of data mining. While each business unit has its own objectives and challenges, which may be at odds with those of other units, they also share many common interests and resources. In this environment, data mining can be used to identify potential value-creating opportunities, through large site integration of multiple assets and synergies from the use of common assets, such as site-wide manufacturing facilities, and world-wide supply-chain, purchasing and other shared services. However, issues arise, on one hand from overly complex systems, and on the other hand, from the danger of reaching sub-optimal solutions, if a big enough picture is not considered when executing projects. The company-wide initiative and use of Six Sigma at all levels of the company provided a fertile ground for making the case for data mining and facilitating its acceptance. The Six Sigma mindset of measuring the performance of processes and analyzing data promotes data-based decision making, therefore making data mining a natural extension of this methodology. We will describe the approach for launching a data mining capability within this framework, the strategy for securing upper management support, drawing from internal modeling, statistical, and other communities, and from external consultants and universities. Lessons learned from industrial case studies, enterprise-wide tool evaluation and peer benchmarking will be discussed.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {763–769},
numpages = {7},
keywords = {manufacturing, chemical industry, data mining},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081971,
author = {Li, Jiuyong and Fu, Ada Wai-chee and He, Hongxing and Chen, Jie and Jin, Huidong and McAullay, Damien and Williams, Graham and Sparks, Ross and Kelman, Chris},
title = {Mining Risk Patterns in Medical Data},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081971},
doi = {10.1145/1081870.1081971},
abstract = {In this paper, we discuss a problem of finding risk patterns in medical data. We define risk patterns by a statistical metric, relative risk, which has been widely used in epidemiological research. We characterise the problem of mining risk patterns as an optimal rule discovery problem. We study an anti-monotone property for mining optimal risk pattern sets and present an algorithm to make use of the property in risk pattern discovery. The method has been applied to a real world data set to find patterns associated with an allergic event for ACE inhibitors. The algorithm has generated some useful results for medical researchers.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {770–775},
numpages = {6},
keywords = {optimal risk pattern set, relative risk, rule, medical application},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081972,
author = {Li, Tao and Liang, Feng and Ma, Sheng and Peng, Wei},
title = {An Integrated Framework on Mining Logs Files for Computing System Management},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081972},
doi = {10.1145/1081870.1081972},
abstract = {Traditional approaches to system management have been largely based on domain experts through a knowledge acquisition process that translates domain knowledge into operating rules and policies. This has been well known and experienced as a cumbersome, labor intensive, and error prone process. In addition, this process is difficult to keep up with the rapidly changing environments. In this paper, we will describe our research efforts on establishing an integrated framework for mining system log files for automatic management. In particular, we apply text mining techniques to categorize messages in log files into common situations, improve categorization accuracy by considering the temporal characteristics of log messages, develop temporal mining techniques to discover the relationships between different events, and utilize visualization tools to evaluate and validate the interesting temporal patterns for system management.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {776–781},
numpages = {6},
keywords = {system management, event relationship, log categorization, temporal pattern},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081973,
author = {Li, Xiang and Ramachandran, Rahul and Graves, Sara and Movva, Sunil and Akkiraju, Bilahari and Emmitt, David and Greco, Steven and Atlas, Robert and Terry, Joseph and Jusem, Juan-Carlos},
title = {Automated Detection of Frontal Systems from Numerical Model-Generated Data},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081973},
doi = {10.1145/1081870.1081973},
abstract = {Fronts are significant meteorological phenomena of interest. The extraction of frontal systems from observations and model data can greatly benefit many kinds of research and applications in atmospheric sciences. Due to the huge amount of observational and model data available nowadays, automated extraction of front systems is necessary. This paper presents an automated method to detect frontal systems from numerical model-generated data. In this method, a frontal system is characterized by a vector of features, comprised of parameters derived from the model wind field. K-means clustering is applied to the generated sample set of the feature vectors to partition the feature space and to identify clusters representing the fronts. The probability that a model grid belongs to a front is estimated based on its feature vector. The probability image is generated corresponding to the model grids. A hierarchical thresholding technique is applied to the probability image to identify the frontal systems and a Gaussian Bayes classifier is trained to determine the proper threshold value. This is followed by post processing to filter out false signatures. Experiment results from this method are in good agreement with the ones identified by the domain experts.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {782–787},
numpages = {6},
keywords = {data mining applications, spatial data mining},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@dataset{10.1145/review-1081870.1081973_R39855,
author = {Thacker, William I.},
title = {Review ID:R39855 for DOI: 10.1145/1081870.1081973},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1081870.1081973_R39855}
}

@inproceedings{10.1145/1081870.1081974,
author = {Pearson, Ronald K. and Kingan, Robert J. and Hochberg, Alan},
title = {Disease Progression Modeling from Historical Clinical Databases},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081974},
doi = {10.1145/1081870.1081974},
abstract = {This paper considers the problem of modeling disease progression from historical clinical databases, with the ultimate objective of stratifying patients into groups with clearly distinguishable prognoses or suitability for different treatment strategies. To meet this objective, we describe a procedure that first fits clinical variables measured over time to a disease progression model. The resulting parameter estimates are then used as the basis for a stepwise clustering procedure to stratify patients into groups with distinct survival characteristics. As a practical illustration, we apply this procedure to survival prediction, using a liver transplant database from the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK).},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {788–793},
numpages = {6},
keywords = {logistic model, censoring, NIDDK liver transplant database, cluster analysis, disease progression modeling},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081975,
author = {Petrushin, Valery A.},
title = {Mining Rare and Frequent Events in Multi-Camera Surveillance Video Using Self-Organizing Maps},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081975},
doi = {10.1145/1081870.1081975},
abstract = {This paper describes a method for unsupervised classification of events in multi-camera indoors surveillance video. This research is a part of the Multiple Sensor Indoor Surveillance (MSIS) project which uses 32 AXIS-2100 webcams that observe an office environment. The research was inspired by the following practical problem: how automatically classify and visualize a 24 hour long video captured by 32 cameras? Raw data are sequences of JPEG images captured by webcams at the rate 2-6 Hz. The following features are extracted from the image data: foreground pixels' spatial distribution and color histogram. The data are integrated by event by averaging motion and color features and creating a "summary" frame which accumulates all foreground pixels of frames of the event into one image. The self-organizing map (SOM) approach is applied to event data for clustering and visualization. One-level and two-level SOM clustering are used. A tool for browsing results allows exploring units of the SOM maps at different levels of hierarchy, clusters of units and distances between units in 3D space. A special technique has been developed to visualize rare events. The results are presented and discussed.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {794–800},
numpages = {7},
keywords = {unsupervised learning, self-organizing maps, rare event detection, indoor surveillance, visualization},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081976,
author = {Powers, Rob and Goldszmidt, Moises and Cohen, Ira},
title = {Short Term Performance Forecasting in Enterprise Systems},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081976},
doi = {10.1145/1081870.1081976},
abstract = {We use data mining and machine learning techniques to predict upcoming periods of high utilization or poor performance in enterprise systems. The abundant data available and complexity of these systems defies human characterization or static models and makes the task suitable for data mining techniques. We formulate the problem as one of classification: given current and past information about the system's behavior, can we forecast whether the system will meet its performance targets over the next hour? Using real data gathered from several enterprise systems in Hewlett-Packard, we compare several approaches ranging from time series to Bayesian networks. Besides establishing the predictive power of these approaches our study analyzes three dimensions that are important for their application as a stand alone tool. First, it quantifies the gain in accuracy of multivariate prediction methods over simple statistical univariate methods. Second, it quantifies the variations in accuracy when using different classes of system and workload features. Third, it establishes that models induced using combined data from various systems generalize well and are applicable to new systems, enabling accurate predictions on systems with insufficient historical data. Together this analysis offers a promising outlook on the development of tools to automate assignment of resources to stabilize performance, (e.g., adding servers to a cluster) and allow opportunistic job scheduling (e.g., backups or virus scans).},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {801–807},
numpages = {7},
keywords = {enterprise systems, performance forecasting},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081977,
author = {Sanghai, Kaushal and Su, Ting and Dy, Jennifer and Kaeli, David},
title = {A Multinomial Clustering Model for Fast Simulation of Computer Architecture Designs},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081977},
doi = {10.1145/1081870.1081977},
abstract = {Computer architects utilize simulation tools to evaluate the merits of a new design feature. The time needed to adequately evaluate the tradeoffs associated with adding any new feature has become a critical issue. Recent work has found that by identifying execution phases present in common workloads used in simulation studies, we can apply clustering algorithms to significantly reduce the amount of time needed to complete the simulation. Our goal in this paper is to demonstrate the value of this approach when applied to the set of industry-standard benchmarks most commonly used in computer architecture studies. We also look to improve upon prior work by applying more appropriate clustering algorithms to identify phases, and to further reduce simulation time.We find that the phase clustering in computer architecture simulation has many similarities to text clustering. In prior work on clustering techniques to reduce simulation time, K-means clustering was used to identify representative program phases. In this paper we apply a mixture of multinomials to the clustering problem and show its advantages over using K-means on simulation data. We have implemented these two clustering algorithms and evaluate how well they can characterize program behavior. By adopting a mixture of multinomials model, we find that we can maintain simulation result fidelity, while greatly reducing overall simulation time. We report results for a range of applications taken from the SPEC2000 benchmark suite.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {808–813},
numpages = {6},
keywords = {EM, mixture of multinomials, program phase, simulation, clustering, k-means},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

@inproceedings{10.1145/1081870.1081978,
author = {Wang, Haixun and Pei, Jian and Yu, Philip S.},
title = {Pattern-Based Similarity Search for Microarray Data},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081870.1081978},
doi = {10.1145/1081870.1081978},
abstract = {One fundamental task in near-neighbor search as well as other similarity matching efforts is to find a distance function that can efficiently quantify the similarity between two objects in a meaningful way. In DNA microarray analysis, the expression levels of two closely related genes may rise and fall synchronously in response to a set of experimental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very similar. Unfortunately, none of the conventional distance metrics such as the Lp norm can model this similarity effectively. In this paper, we study the near-neighbor search problem based on this new type of similarity. We propose to measure the distance between two genes by subspace pattern similarity, i.e., whether they exhibit a synchronous pattern of rise and fall on a subset of dimensions. We then present an efficient algorithm for subspace near-neighbor search based on pattern similarity distance, and we perform tests on various data sets to show its effectiveness.},
booktitle = {Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining},
pages = {814–819},
numpages = {6},
keywords = {pattern recognition, distance function, near neighbor},
location = {Chicago, Illinois, USA},
series = {KDD '05}
}

