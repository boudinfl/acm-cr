@inproceedings{10.1145/3246134,
author = {Liu, Bing},
title = {Session Details: Usage Analysis},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246134},
doi = {10.1145/3246134},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060752,
author = {Chien, Steve and Immorlica, Nicole},
title = {Semantic Similarity between Search Engine Queries Using Temporal Correlation},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060752},
doi = {10.1145/1060745.1060752},
abstract = {We investigate the idea of finding semantically related search engine queries based on their temporal correlation; in other words, we infer that two queries are related if their popularities behave similarly over time. To this end, we first define a new measure of the temporal correlation of two queries based on the correlation coefficient of their frequency functions. We then conduct extensive experiments using our measure on two massive query streams from the MSN search engine, revealing that this technique can discover a wide range of semantically similar queries. Finally, we develop a method of efficiently finding the highest correlated queries for a given input query using far less space and time than the naive approach, making real-time implementation possible.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {2–11},
numpages = {10},
keywords = {semantic similarity among queries, query stream analysis, search engines},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060753,
author = {Metwally, Ahmed and Agrawal, Divyakant and El Abbadi, Amr},
title = {Duplicate Detection in Click Streams},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060753},
doi = {10.1145/1060745.1060753},
abstract = {We consider the problem of finding duplicates in data streams. Duplicate detection in data streams is utilized in various applications including fraud detection. We develop a solution based on Bloom Filters [9], and discuss the space and time requirements for running the proposed algorithm in both the contexts of sliding, and landmark stream windows. We run a comprehensive set of experiments, using both real and synthetic click streams, to evaluate the performance of the proposed solution. The results demonstrate that the proposed solution yields extremely low error rates.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {12–21},
numpages = {10},
keywords = {data streams, advertising networks, approximate queries, duplicate detection},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060754,
author = {Ziegler, Cai-Nicolas and McNee, Sean M. and Konstan, Joseph A. and Lausen, Georg},
title = {Improving Recommendation Lists through Topic Diversification},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060754},
doi = {10.1145/1060745.1060754},
abstract = {In this work we present topic diversification, a novel method designed to balance and diversify personalized recommendation lists in order to reflect the user's complete spectrum of interests. Though being detrimental to average accuracy, we show that our method improves user satisfaction with recommendation lists, in particular for lists generated using the common item-based collaborative filtering algorithm.Our work builds upon prior research on recommender systems, looking at properties of recommendation lists as entities in their own right rather than specifically focusing on the accuracy of individual recommendations. We introduce the intra-list similarity metric to assess the topical diversity of recommendation lists and the topic diversification approach for decreasing the intra-list similarity. We evaluate our method using book recommendation data, including offline analysis on 361, !, 349 ratings and an online study involving more than 2, !, 100 subjects.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {22–32},
numpages = {11},
keywords = {diversification, metrics, accuracy, recommender systems, collaborative filtering},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246135,
author = {Rabinovich, Michael},
title = {Session Details: Wide-Area Architecture and Protocols},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246135},
doi = {10.1145/3246135},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060756,
author = {Sivasubramanian, Swaminathan and Alonso, Gustavo and Pierre, Guillaume and van Steen, Maarten},
title = {GlobeDB: Autonomic Data Replication for Web Applications},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060756},
doi = {10.1145/1060745.1060756},
abstract = {We present GlobeDB, a system for hosting Web applications that performs autonomic replication of application data. GlobeDB offers data-intensive Web applications the benefits of low access latencies and reduced update traffic. The major distinction in our system compared to existing edge computing infrastructures is that the process of distribution and replication of application data is handled by the system automatically with very little manual administration. We show that significant performance gains can be obtained this way. Performance evaluations with the TPC-W benchmark over an emulated wide-area network show that GlobeDB reduces latencies by a factor of 4 compared to non-replicated systems and reduces update traffic by a factor of 6 compared to fully replicated systems.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {33–42},
numpages = {10},
keywords = {autonomic replication, data replication, edge services, rerformance},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060757,
author = {Irmak, Utku and Suel, Torsten},
title = {Hierarchical Substring Caching for Efficient Content Distribution to Low-Bandwidth Clients},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060757},
doi = {10.1145/1060745.1060757},
abstract = {While overall bandwidth in the internet has grown rapidly over the last few years, and an increasing number of clients enjoy broadband connectivity, many others still access the internet over much slower dialup or wireless links. To address this issue, a number of techniques for optimized delivery of web and multimedia content over slow links have been proposed, including protocol optimizations, caching, compression, and multimedia transcoding, and several large ISPs have recently begun to widely promote dialup acceleration services based on such techniques. A recent paper by Rhea, Liang, and Brewer proposed an elegant technique called value-based caching that caches substrings of files, rather than entire files, and thus avoids repeated transmission of substrings common to several pages or page versions.We propose and study a hierarchical substring caching technique that provides significant savings over this basic approach. We describe several additional techniques for minimizing overheads and perform an evaluation on a large set of real web access traces that we collected. In the second part of our work, we compare our approach to a widely studied alternative approach based on delta compression, and show how to integrate the two for best overall performance. The studied techniques are typically employed in a client-proxy environment, with each proxy serving a large number of clients, and an important aspect is how to conserve resources on the proxy while exploiting the significant memory and CPU power available on current clients.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {43–53},
numpages = {11},
keywords = {HTTP, compression, web proxies, WWW, web caching},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060758,
author = {Gupta, Rajeev and Puri, Ashish and Ramamritham, Krithi},
title = {Executing Incoherency Bounded Continuous Queries at Web Data Aggregators},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060758},
doi = {10.1145/1060745.1060758},
abstract = {Continuous queries are used to monitor changes to time varying data and to provide results useful for online decision making. Typically a user desires to obtain the value of some function over distributed data items, for example, to determine when and whether (a) the traffic entering a highway from multiple feed roads will result in congestion in a thoroughfare or (b) the value of a stock portfolio exceeds a threshold. Using the standard Web infrastructure for these applications will increase the reach of the underlying information. But, since these queries involve data from multiple sources, with sources supporting standard HTTP (pull-based) interfaces, special query processing techniques are needed. Also, these applications often have the flexibility to tolerate some incoherency, i.e., some differences between the results reported to the user and that produced from the virtual database made up of the distributed data sources.In this paper, we develop and evaluate client-pull-based techniques for refreshing data so that the results of the queries over distributed data can be correctly reported, conforming to the limited incoherency acceptable to the users.We model as well as estimate the dynamics of the data items using a probabilistic approach based on Markov Chains. Depending on the dynamics of data we adapt the data refresh times to deliver query results with the desired coherency. The commonality of data needs of multiple queries is exploited to further reduce refresh overheads. Effectiveness of our approach is demonstrated using live sources of dynamic data: the number of refreshes it requires is (a) an order of magnitude less than what we would need if every potential update is pulled from the sources, and (b) comparable to the number of messages needed by an ideal algorithm, one that knows how to optimally refresh the data from distributed data sources. Our evaluations also bring out a very practical and attractive tradeoff property of pull based approaches, e.g., a small increase in tolerable incoherency leads to a large decrease in message overheads.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {54–65},
numpages = {12},
keywords = {online decision making, fidelity, coherency, Markov model, continuous queries},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246136,
author = {Morishita, Shinichi},
title = {Session Details: Data Extraction},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246136},
doi = {10.1145/3246136},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060760,
author = {Zhao, Hongkun and Meng, Weiyi and Wu, Zonghuan and Raghavan, Vijay and Yu, Clement},
title = {Fully Automatic Wrapper Generation for Search Engines},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060760},
doi = {10.1145/1060745.1060760},
abstract = {When a query is submitted to a search engine, the search engine returns a dynamically generated result page containing the result records, each of which usually consists of a link to and/or snippet of a retrieved Web page. In addition, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine and advertisements. In this paper, we present a technique for automatically producing wrappers that can be used to extract search result records from dynamically generated result pages returned by search engines. Automatic search result record extraction is very important for many applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling. The novel aspect of the proposed technique is that it utilizes both the visual content features on the result page as displayed on a browser and the HTML tag structures of the HTML source file of the result page. Experimental results indicate that this technique can achieve very high extraction accuracy.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {66–75},
numpages = {10},
keywords = {information extraction, wrapper generation, search engine},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060761,
author = {Zhai, Yanhong and Liu, Bing},
title = {Web Data Extraction Based on Partial Tree Alignment},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060761},
doi = {10.1145/1060745.1060761},
abstract = {This paper studies the problem of extracting data from a Web page that contains several structured data records. The objective is to segment these data records, extract data items/fields from them and put the data in a database table. This problem has been studied by several researchers. However, existing methods still have some serious limitations. The first class of methods is based on machine learning, which requires human labeling of many examples from each Web site that one is interested in extracting data from. The process is time consuming due to the large number of sites and pages on the Web. The second class of algorithms is based on automatic pattern discovery. These methods are either inaccurate or make many assumptions. This paper proposes a new method to perform the task automatically. It consists of two steps, (1) identifying individual data records in a page, and (2) aligning and extracting data items from the identified data records. For step 1, we propose a method based on visual information to segment data records, which is more accurate than existing methods. For step 2, we propose a novel partial alignment technique based on tree matching. Partial alignment means that we align only those data fields in a pair of data records that can be aligned (or matched) with certainty, and make no commitment on the rest of the data fields. This approach enables very accurate alignment of multiple data records. Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {76–85},
numpages = {10},
keywords = {data record extraction, data extraction, wrapper},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060762,
author = {Hogue, Andrew and Karger, David},
title = {Thresher: Automating the Unwrapping of Semantic Content from the World Wide Web},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060762},
doi = {10.1145/1060745.1060762},
abstract = {We describe Thresher, a system that lets non-technical users teach their browsers how to extract semantic web content from HTML documents on the World Wide Web. Users specify examples of semantic content by highlighting them in a web browser and describing their meaning. We then use the tree edit distance between the DOM subtrees of these examples to create a general pattern, or wrapper, for the content, and allow the user to bind RDF classes and predicates to the nodes of these wrappers. By overlaying matches to these patterns on standard documents inside the Haystack semantic web browser, we enable a rich semantic interaction with existing web pages, "unwrapping" semantic data buried in the pages' HTML. By allowing end-users to create, modify, and utilize their own patterns, we hope to speed adoption and use of the Semantic Web and its applications.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {86–95},
numpages = {10},
keywords = {semantic Web, haystack, RDF, wrapper induction, tree edit distance},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246137,
author = {Tomkins, Andrew},
title = {Session Details: Semantic Querying},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246137},
doi = {10.1145/3246137},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060764,
author = {Del Corso, Gianna M. and Gull\'{\i}, Antonio and Romani, Francesco},
title = {Ranking a Stream of News},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060764},
doi = {10.1145/1060745.1060764},
abstract = {According to a recent survey made by Nielsen NetRatings, searching on news articles is one of the most important activity online. Indeed, Google, Yahoo, MSN and many others have proposed commercial search engines for indexing news feeds. Despite this commercial interest, no academic research has focused on ranking a stream of news articles and a set of news sources. In this paper, we introduce this problem by proposing a ranking framework which models: (1) the process of generation of a stream of news articles, (2) the news articles clustering by topics, and (3) the evolution of news story over the time. The ranking algorithm proposed ranks news information, finding the most authoritative news sources and identifying the most interesting events in the different categories to which news article belongs. All these ranking measures take in account the time and can be obtained without a predefined sliding window of observation over the stream. The complexity of our algorithm is linear in the number of pieces of news still under consideration at the time of a new posting. This allow a continuous on-line process of ranking. Our ranking framework is validated on a collection of more than 300,000 pieces of news, produced in two months by more then 2000 news sources belonging to 13 different categories (World, U.S, Europe, Sports, Business, etc). This collection is extracted from the index of comeToMyHead, an academic news search engine available online.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {97–106},
numpages = {10},
keywords = {news ranking, information extraction, news engines},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060765,
author = {Maguitman, Ana G. and Menczer, Filippo and Roinestad, Heather and Vespignani, Alessandro},
title = {Algorithmic Detection of Semantic Similarity},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060765},
doi = {10.1145/1060745.1060765},
abstract = {Automatic extraction of semantic information from text and links in Web pages is key to improving the quality of search results. However, the assessment of automatic semantic measures is limited by the coverage of user studies, which do not scale with the size, heterogeneity, and growth of the Web. Here we propose to leverage human-generated metadata --- namely topical directories --- to measure semantic relationships among massive numbers of pairs of Web pages or topics. The Open Directory Project classifies millions of URLs in a topical ontology, providing a rich source from which semantic relationships between Web pages can be derived. While semantic similarity measures based on taxonomies (trees) are well studied, the design of well-founded similarity measures for objects stored in the nodes of arbitrary ontologies (graphs) is an open problem. This paper defines an information-theoretic measure of semantic similarity that exploits both the hierarchical and non-hierarchical structure of an ontology. An experimental study shows that this measure improves significantly on the traditional taxonomy-based approach. This novel measure allows us to address the general question of how text and link analyses can be combined to derive measures of relevance that are in good agreement with semantic similarity. Surprisingly, the traditional use of text similarity turns out to be ineffective for relevance ranking.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {107–116},
numpages = {10},
keywords = {semantic similarity, Web search, Web mining, content and link similarity, ranking evaluation},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060766,
author = {Anyanwu, Kemafor and Maduko, Angela and Sheth, Amit},
title = {SemRank: Ranking Complex Relationship Search Results on the Semantic Web},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060766},
doi = {10.1145/1060745.1060766},
abstract = {While the idea that querying mechanisms for complex relationships (otherwise known as Semantic Associations) should be integral to Semantic Web search technologies has recently gained some ground, the issue of how search results will be ranked remains largely unaddressed. Since it is expected that the number of relationships between entities in a knowledge base will be much larger than the number of entities themselves, the likelihood that Semantic Association searches would result in an overwhelming number of results for users is increased, therefore elevating the need for appropriate ranking schemes. Furthermore, it is unlikely that ranking schemes for ranking entities (documents, resources, etc.) may be applied to complex structures such as Semantic Associations.In this paper, we present an approach that ranks results based on how predictable a result might be for users. It is based on a relevance model SemRank, which is a rich blend of semantic and information-theoretic techniques with heuristics that supports the novel idea of modulative searches, where users may vary their search modes to effect changes in the ordering of results depending on their need. We also present the infrastructure used in the SSARK system to support the computation of SemRank values for resulting Semantic Associations and their ordering.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {117–127},
numpages = {11},
keywords = {path expression tree, ranking complex relationships, semantic relationship search, semantic match, semantic summary, SemRank, semantic Web, semantic similarity, discovery query, semantic associations search, semantic ranking},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060768,
author = {Agarwal, Vikas and Dasgupta, Koustuv and Karnik, Neeran and Kumar, Arun and Kundu, Ashish and Mittal, Sumit and Srivastava, Biplav},
title = {A Service Creation Environment Based on End to End Composition of Web Services},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060768},
doi = {10.1145/1060745.1060768},
abstract = {The demand for quickly delivering new applications is increasingly becoming a business imperative today. Application development is often done in an ad hoc manner, without standard frameworks or libraries, thus resulting in poor reuse of software assets. Web services have received much interest in industry due to their potential in facilitating seamless business-to-business or enterprise application integration. A web services composition tool can help automate the process, from creating business process functionality, to developing executable workflows, to deploying them on an execution environment. However, we find that the main approaches taken thus far to standardize and compose web services are piecemeal and insufficient. The business world has adopted a (distributed) programming approach in which web service instances are described using WSDL, composed into flows with a language like BPEL and invoked with the SOAP protocol. Academia has propounded the AI approach of formally representing web service capabilities in ontologies, and reasoning about their composition using goal-oriented inferencing techniques from planning. We present the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the above approaches. We describe a prototype service creation environment along with a use-case scenario, and demonstrate how it can significantly speed up the time-to-market for new services.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {128–137},
numpages = {10},
keywords = {Web services composition, semantic Web, planning},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060769,
author = {Bhiri, Sami and Perrin, Olivier and Godart, Claude},
title = {Ensuring Required Failure Atomicity of Composite Web Services},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060769},
doi = {10.1145/1060745.1060769},
abstract = {The recent evolution of Internet, driven by the Web services technology, is extending the role of the Web from a support of information interaction to a middleware for B2B interactions.Indeed, the Web services technology allows enterprises to outsource parts of their business processes using Web services. And it also provides the opportunity to dynamically offer new value-added services through the composition of pre-existing Web services.In spite of the growing interest in Web services, current technologies are found lacking efficient transactional support for composite Web services (CSs).In this paper, we propose a transactional approach to ensure the failure atomicity, of a CS, required by partners. We use the Accepted Termination States (ATS) property as a mean to express the required failure atomicity.Partners specify their CS, mainly its control flow, and the required ATS. Then, we use a set of transactional rules to assist designers to compose a valid CS with regards to the specified ATS.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {138–147},
numpages = {10},
keywords = {reliable Web services compositions, failure atomicity, transactional models},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060770,
author = {Beyer, Dirk and Chakrabarti, Arindam and Henzinger, Thomas A.},
title = {Web Service Interfaces},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060770},
doi = {10.1145/1060745.1060770},
abstract = {We present a language for specifying web service interfaces. A web service interface puts three kinds of constraints on the users of the service. First, the interface specifies the methods that can be called by a client, together with types of input and output parameters; these are called signature constraints. Second, the interface may specify propositional constraints on method calls and output values that may occur in a web service conversation; these are called consistency constraints. Third, the interface may specify temporal constraints on the ordering of method calls; these are called protocol constraints. The interfaces can be used to check, first, if two or more web services are compatible, and second, if a web service A can be safely substituted for a web service B. The algorithm for compatibility checking verifies that two or more interfaces fulfill each others' constraints. The algorithm for substitutivity checking verifies that service A demands fewer and fulfills more constraints than service B.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {148–159},
numpages = {12},
keywords = {formal verification, formal specification, Web service substitutivity, Web services, Web service compatibility, Web service interfaces},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246138,
author = {Houben, Geert-Jan},
title = {Session Details: Web Application Design},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246138},
doi = {10.1145/3246138},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060772,
author = {Kurtev, Ivan and van den Berg, Klaas},
title = {Building Adaptable and Reusable XML Applications with Model Transformations},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060772},
doi = {10.1145/1060745.1060772},
abstract = {We present an approach in which the semantics of an XML language is defined by means of a transformation from an XML document model (an XML schema) to an application specific model. The application specific model implements the intended behavior of documents written in the language. A transformation is specified in a model transformation language used in the Model Driven Architecture (MDA) approach for software development. Our approach provides a better separation of three concerns found in XML applications: syntax, syntax processing logic and intended meaning of the syntax. It frees the developer of low-level syntactical details and improves the adaptability and reusability of XML applications. Declarative transformation rules and the explicit application model provide a finer control over the application parts affected by adaptations. Transformation rules and the application model for an XML language may be composed with the corresponding rules and application models defined for other XML languages. In that way we achieve reuse and composition of XML applications.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {160–169},
numpages = {10},
keywords = {model transformations, MDA, XML, transformation language, XML processing},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060773,
author = {Brambilla, Marco and Ceri, Stefano and Comai, Sara and Tziviskou, Christina},
title = {Exception Handling in Workflow-Driven Web Applications},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060773},
doi = {10.1145/1060745.1060773},
abstract = {As the Web becomes a platform for implementing B2B applications, the need arises of Web conceptual models for describing Web oriented workflow applications implementing business processes. In this context, new problems about process correctness arise, due to the loose control of Web applications upon the behavior of their Web clients. Indeed, incoherent user's behavior can lead to inconsistent processes.This paper presents a high level approach to the management of exceptions that occur during the execution of processes on the Web. We present a classification of exceptions that can be raised inside workflow-driven Web applications, and recovery policies to retrieve coherent status and data after an exception. We devise these concepts at high level and then we exploit them using a Web modeling language (WebML) that in turn provides development facilities like automatic code generation, validation of hypertext models, and so on. An industrial implementation experience is briefly presented too.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {170–179},
numpages = {10},
keywords = {Web applications, navigation behavior, workflow, exceptions, failure},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060774,
author = {Jeh\o{}j, Henning Qin and Bouvin, Niels Olof and Gr\o{}nb\ae{}k, Kaj},
title = {AwareDAV: A Generic WebDAV Notification Framework and Implementation},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060774},
doi = {10.1145/1060745.1060774},
abstract = {WebDAV needs awareness support in order to be a full-fledged collaboration system, This paper introduces AwareDAV, a new WebDAV extension framework enabling shared awareness through event notification. By extending the WebDAV protocol with seven new request-methods and an extensible XML based event subscription scheme, AwareDAV supports fine grained event subscriptions over a range of transport mechanisms and enables a wide range of collaboration scenarios. This paper describes the design of AwareDAV, its API, experiences with its initial implementation, as well as a comparison with Microsoft Exchange and WebDAV-notify.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {180–189},
numpages = {10},
keywords = {WebDAV, event notification, AwareDAV, CSCW},
location = {Chiba, Japan},
series = {WWW '05}
}

@dataset{10.1145/review-1060745.1060774_R39618,
author = {Hamza-Lup, Felix},
title = {Review ID:R39618 for DOI: 10.1145/1060745.1060774},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1060745.1060774_R39618}
}

@inproceedings{10.1145/3246139,
author = {Hendler, Jim},
title = {Session Details: Semantic Web},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246139},
doi = {10.1145/3246139},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060776,
author = {Sabou, Marta and Wroe, Chris and Goble, Carole and Mishne, Gilad},
title = {Learning Domain Ontologies for Web Service Descriptions: An Experiment in Bioinformatics},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060776},
doi = {10.1145/1060745.1060776},
abstract = {The reasoning tasks that can be performed with semantic web service descriptions depend on the quality of the domain ontologies used to create these descriptions. However, building such domain ontologies is a time consuming and difficult task.We describe an automatic extraction method that learns domain ontologies for web service descriptions from textual documentations attached to web services. We conducted our experiments in the field of bioinformatics by learning an ontology from the documentation of the web services used in myGrid, a project that supports biology experiments on the Grid. Based on the evaluation of the extracted ontology in the context of the project, we conclude that the proposed extraction method is a helpful tool to support the process of building domain ontologies for web service descriptions.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {190–198},
numpages = {9},
keywords = {OWL-S, ontology learning, Web services, domain ontology, bioinformatics, semantic Web, ontology evaluation},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060777,
author = {Rutledge, Lloyd and van Ossenbruggen, Jacco and Hardman, Lynda},
title = {Making RDF Presentable: Integrated Global and Local Semantic Web Browsing},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060777},
doi = {10.1145/1060745.1060777},
abstract = {This paper discusses generating document structure from annotated media repositories in a domain-independent manner. This approaches the vision of a universal RDF browser. We start by applying the search-and-browse paradigm established for the WWW to RDF presentation. Furthermore, this paper adds to this paradigm the clustering-based derivation of document structure from search returns, providing simple but domain-independent hypermedia generation from RDF stores. While such generated presentations hardly meet the standards of those written by humans, they provide quick access to media repositories when the required document has not yet been written. The resulting system allows a user to specify a topic for which it generates a hypermedia document providing guided navigation through virtually any RDF repository. The impact for content providers is that as soon as one adds new media items and their annotations to a repository, they become immediately available for automatic integration into subsequently requested presentations.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {199–206},
numpages = {8},
keywords = {browsing, hypermedia generation, clustering, media archives, search, semantic Web, RDF},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246140,
author = {Trevor, Jonathan},
title = {Session Details: Applications},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246140},
doi = {10.1145/3246140},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060779,
author = {Avesani, Paolo and Cova, Marco},
title = {Shared Lexicon for Distributed Annotations on the Web},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060779},
doi = {10.1145/1060745.1060779},
abstract = {The interoperability among distributed and autonomous systems is the ultimate challenge facing the semantic web. Heterogeneity of data representation is the main source of problems. This paper proposes an innovative solution that combines lexical approaches and language games. The benefits for distributed annotation systems on the web are twofold: firstly, it will reduce the complexity of the semantic problem by moving the focus from the full-featured ontology level to the simpler lexicon level; secondly, it will avoid the drawback of a centralized third party mediator that may become a single point of failure.The main contributions of this work are concerned with: providing a proof of concept that language games can be an effective solution to creating and managing a distributed process of agreement on a shared lexicon,describing a fully distributed service oriented architecture for language games,providing empirical evidence on a real world case study in the domain of ski mountaineering.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {207–214},
numpages = {8},
keywords = {distributed annotations, language games, interoperability, emergent semantics},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060780,
author = {Cardone, Richard and Soroker, Danny and Tiwari, Alpana},
title = {Using XForms to Simplify Web Programming},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060780},
doi = {10.1145/1060745.1060780},
abstract = {The difficulty of developing and deploying commercial web applications increases as the number of technologies they use increases and as the interactions between these technologies become more complex. This paper describes a way to avoid this increasing complexity by re-examining the basic requirements of web applications. Our approach is to first separate client concerns from server concerns, and then to reduce the interaction between client and server to its most elemental: parameter passing. We define a simplified programming model for form-based web applications and we use XForms and a subset of J2EE as enabling technologies. We describe our implementation of an MVC-based application builder for this model, which automatically generates the code needed to marshal input and output data between clients and servers. This marshalling uses type checking and other forms of validation on both clients and servers. We also show how our programming model and application builder support the customization of web applications for different execution targets, including, for example, different client devices.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {215–224},
numpages = {10},
keywords = {J2EE, eclipse, Web application, XMLBeans, visual builder, MVC, XForms},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060781,
author = {Dowman, Mike and Tablan, Valentin and Cunningham, Hamish and Popov, Borislav},
title = {Web-Assisted Annotation, Semantic Indexing and Search of Television and Radio News},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060781},
doi = {10.1145/1060745.1060781},
abstract = {The Rich News system, that can automatically annotate radio and television news with the aid of resources retrieved from the World Wide Web, is described. Automatic speech recognition gives a temporally precise but conceptually inaccurate annotation model. Information extraction from related web news sites gives the opposite: conceptual accuracy but no temporal data. Our approach combines the two for temporally accurate conceptual semantic annotation of broadcast news. First low quality transcripts of the broadcasts are produced using speech recognition, and these are then automatically divided into sections corresponding to individual news stories. A key phrases extraction component finds key phrases for each story and uses these to search for web pages reporting the same event. The text and meta-data of the web pages is then used to create index documents for the stories in the original broadcasts, which are semantically annotated using the KIM knowledge management platform. A web interface then allows conceptual search and browsing of news stories, and playing of the parts of the media files corresponding to each news story. The use of material from the World Wide Web allows much higher quality textual descriptions and semantic annotations to be produced than would have been possible using the ASR transcript directly. The semantic annotations can form a part of the Semantic Web, and an evaluation shows that the system operates with high precision, and with a moderate level of recall.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {225–234},
numpages = {10},
keywords = {natural language processing, key-phrase extraction, media archiving, topical segmentation, automatic speech recognition, Web search, semantic annotation, semantic Web, multimedia},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246141,
author = {McSherry, Frank},
title = {Session Details: Indexing and Querying},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246141},
doi = {10.1145/3246141},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060783,
author = {de Moura, Edleno S. and dos Santos, C\'{e}lia F. and Fernandes, Daniel R. and Silva, Altigran S. and Calado, Pavel and Nascimento, Mario A.},
title = {Improving Web Search Efficiency via a Locality Based Static Pruning Method},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060783},
doi = {10.1145/1060745.1060783},
abstract = {The unarguably fast, and continuous, growth of the volume of indexed (and indexable) documents on the Web poses a great challenge for search engines. This is true regarding not only search effectiveness but also time and space efficiency. In this paper we present an index pruning technique targeted for search engines that addresses the latter issue without disconsidering the former. To this effect, we adopt a new pruning strategy capable of greatly reducing the size of search engine indices. Experiments using a real search engine show that our technique can reduce the indices' storage costs by up to 60% over traditional lossless compression methods, while keeping the loss in retrieval precision to a minimum. When compared to the indices size with no compression at all, the compression rate is higher than 88%, i.e., less than one eighth of the original size. More importantly, our results indicate that, due to the reduction in storage overhead, query processing time can be reduced to nearly 65% of the original time, with no loss in average precision. The new method yields significative improvements when compared against the best known static pruning method for search engine indices. In addition, since our technique is orthogonal to the underlying search algorithms, it can be adopted by virtually any search engine.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {235–244},
numpages = {10},
keywords = {search engines, information retrieval, web search, pruning, indexing},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060784,
author = {Anagnostopoulos, Aris and Broder, Andrei Z. and Carmel, David},
title = {Sampling Search-Engine Results},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060784},
doi = {10.1145/1060745.1060784},
abstract = {We consider the problem of efficiently sampling Web search engine query results. In turn, using a small random sample instead of the full set of results leads to efficient approximate algorithms for several applications, such as: Determining the set of categories in a given taxonomy spanned by the search results;Finding the range of metadata values associated to the result set in order to enable "multi-faceted search;"Estimating the size of the result set;Data mining associations to the query terms.We present and analyze an efficient algorithm for obtaining uniform random samples applicable to any search engine based on posting lists and document-at-a-time evaluation. (To our knowledge, all popular Web search engines, e.g. Google, Inktomi, AltaVista, AllTheWeb, belong to this class.)Furthermore, our algorithm can be modified to follow the modern object-oriented approach whereby posting lists are viewed as streams equipped with a next method, and the next method for Boolean and other complex queries is built from the next method for primitive terms. In our case we show how to construct a basic next(p) method that samples term posting lists with probability p, and show how to construct next(p) methods for Boolean operators (AND, OR, WAND) from primitive methods.Finally, we test the efficiency and quality of our approach on both synthetic and real-world data.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {245–256},
numpages = {12},
keywords = {sampling, WAND, weighted AND, search engines},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060785,
author = {Long, Xiaohui and Suel, Torsten},
title = {Three-Level Caching for Efficient Query Processing in Large Web Search Engines},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060785},
doi = {10.1145/1060745.1060785},
abstract = {Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level.We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {257–266},
numpages = {10},
keywords = {caching, inverted index, Web search},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246142,
author = {Webber, Jim},
title = {Session Details: XML Query and Programming Languages},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246142},
doi = {10.1145/3246142},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060787,
author = {Sahuguet, Arnaud and Alexe, Bogdan},
title = {Sub-Document Queries over XML with XSQirrel},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060787},
doi = {10.1145/1060745.1060787},
abstract = {This paper describes XSQirrel, a new XML query language that transforms a document into a sub-document, i.e. a tree where the root-to-leaf paths are a subset of the root-to-leaf paths from the original document.We show that this type of queries is extremely useful for various applications (e.g. web services) and that the currently existing query languages are poorly equipped to express, reason and evaluate such queries. In particular, we emphasize the need to be able to compose such queries. We present the XSQirrel language with its syntax, semantics and two language specific operators, union and composition.For the evaluation of the language, we leverage well established query technologies by translating XSQirrel expressions into XPath programs, XQuery queries or XSLT stylesheets.We provide some experimental results that compare our various evaluation strategies. We also show the runtime benefits of query composition over sequential evaluation.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {268–277},
numpages = {10},
keywords = {sub-document, XML, XSQirrel},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060788,
author = {Harren, Matthew and Raghavachari, Mukund and Shmueli, Oded and Burke, Michael G. and Bordawekar, Rajesh and Pechtchanski, Igor and Sarkar, Vivek},
title = {XJ: Facilitating XML Processing in Java},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060788},
doi = {10.1145/1060745.1060788},
abstract = {The increased importance of XML as a data representation format has led to several proposals for facilitating the development of applications that operate on XML data. These proposals range from runtime API-based interfaces to XML-based programming languages. The subject of this paper is XJ, a research language that proposes novel mechanisms for the integration of XML as a first-class construct into Java™. The design goals of XJ distinguish it from past work on integrating XML support into programming languages --- specifically, the XJ design adheres to the XML Schema and XPath standards. Moreover, it supports in-place updates of XML data thereby keeping with the imperative nature of Java. We have built a prototype compiler for XJ, and our preliminary experiments demonstrate that the performance of XJ programs can approach that of traditional low-level API-based interfaces, while providing a higher level of abstraction.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {278–287},
numpages = {10},
keywords = {language design, XML, Java},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060789,
author = {Chen, Li and Rundensteiner, Elke A.},
title = {XQuery Containment in Presence of Variable Binding Dependencies},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060789},
doi = {10.1145/1060745.1060789},
abstract = {Semantic caching is an important technology for improving the response time of future user queries specified over remote servers. This paper deals with the fundamental query containment problem in an XQuery-based semantic caching system. To our best knowledge, the impact of subtle differences in XQuery semantics caused by different ways of specifying variables on query containment has not yet been studied. We introduce the concept of variable binding dependencies for representing the hierarchical element dependencies preserved by an XQuery. We analyze the problem of XQuery containment in the presence of such dependencies. We propose a containment mapping technique for nested XQuery in presence of variable binding dependencies. The implication of the nested block structure on XQuery containment is also considered. We mention the performance gains achieved by a semantic caching system we build based on the proposed technique.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {288–297},
numpages = {10},
keywords = {variable binding dependency, XQuery containment},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246143,
author = {Aroyo, Lora},
title = {Session Details: Web-Based Educational Applications},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246143},
doi = {10.1145/3246143},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060791,
author = {Brodersen, Christina and Christensen, Bent G. and Gr\o{}nb\ae{}k, Kaj and Dindler, Christian and Sundararajah, Balasuthas},
title = {EBag: A Ubiquitous Web Infrastructure for Nomadic Learning},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060791},
doi = {10.1145/1060745.1060791},
abstract = {This paper describes the eBag infrastructure, which is a generic infrastructure inspired from work with school children who could benefit from a electronic schoolbag for collaborative handling of their digital material. The eBag infrastructure is utilizing the Context-aware HyCon framework and collaborative web services based on WebDAV. A ubiquitous login and logout mechanism has been built based on BlueTooth sensor networks. The eBag infrastructure has been tried out in field tests with school kids. In this paper we discuss experiences and design issues for ubiquitous Web integration in interactive school environments with multiple interactive whiteboards and workstations. This includes proposals for specialized and adaptive XLink structures for organizing school materials as well as issues in login/logout based on proximity of different display surfaces.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {298–306},
numpages = {9},
keywords = {XLink, WebDAV, context-aware, HyCon, adaptive, eBag},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060792,
author = {Kotzinos, Dimitris and Pediaditaki, Sofia and Apostolidis, Apostolos and Athanasis, Nikolaos and Christophides, Vassilis},
title = {Online Curriculum on the Semantic Web: The CSD-UoC Portal for Peer-to-Peer e-Learning},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060792},
doi = {10.1145/1060745.1060792},
abstract = {Online Curriculum Portals aim to support networks of instructors and learners by providing a space of convergence for enhancing peer-to-peer learning interactions among individuals of an educational institution. To this end, effective, open and scalable e-learning systems are required to acquire, store, and share knowledge under the form of learning objects (LO). In this paper, we are interested in exploiting the semantic relationships that characterize these LOs (e.g., prerequisite, part-of or see-also) in order to capture and access individual and group knowledge in conjunction with the learning processes supported by educational institutions. To achieve this functionality, Semantic Web (e.g., RDF/s) and declarative query languages (e.g., RQL) are employed to represent LOs and their relationships (e.g., LOM), as well as, to support navigation at the conceptual e-learning Portal space. In this way, different LOs could be presented to the same learners, according to the traversed schema navigation paths (i.e., learning paths). Using the Apache Jetspeed framework we are able to generate and assemble at run-time portlets (i.e., pluggable web components) for visualizing personalized views as dynamic web pages. Last but not least, both learners and instructors can employ the same Portal GUI for updating semantically described LOs and thus support an open-ended continuum of learning. To the best of our knowledge, the work presented in this paper is the first Online Curriculum Portal platform supporting the aforementioned functionality.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {307–314},
numpages = {8},
keywords = {jetspeed portlets, semantic Web, e-learning portals, IEEE-LOM},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060793,
author = {Singley, Mark K. and Lam, Richard B.},
title = {The Classroom Sentinel: Supporting Data-Driven Decision-Making in the Classroom},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060793},
doi = {10.1145/1060745.1060793},
abstract = {Whereas schools typically record mounds of data regarding student performance, attendance, and other behaviors over the course of a school year, rarely is that data consulted and used to inform day-to-day instructional practice in the classroom. As teachers come under increasing pressure to ensure success for all of their students, we are attempting to provide tools to help teachers make sense of what is happening in their classrooms and take appropriate proactive and/or remedial action. One such tool is a Web service we've dubbed the Classroom Sentinel. The Classroom Sentinel mines electronic gradebook and other student information system data sources to detect critical teaching and learning patterns and bring those patterns to the attention of the teacher in the form of timely alerts. In this paper, we introduce the notion of classroom patterns, present some examples, and describe a framework for alert generation and delivery.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {315–321},
numpages = {7},
keywords = {data-driven decision making, data integration, classroom pattern detection, alert generation, teacher cognition},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060795,
author = {Kim, Jong Wook and Candan, K. Sel\c{c}uk and D\"{o}nderler, Mehmet E.},
title = {Topic Segmentation of Message Hierarchies for Indexing and Navigation Support},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060795},
doi = {10.1145/1060745.1060795},
abstract = {Message hierarchies in web discussion boards grow with new postings. Threads of messages evolve as new postings focus within or diverge from the original themes of the threads. Thus, just by investigating the subject headings or contents of earlier postings in a message thread, one may not be able to guess the contents of the later postings. The resulting navigation problem is further compounded for blind users who need the help of a screen reader program that can provide only a linear representation of the content. We see that, in order to overcome the navigation obstacle for blind as well as sighted users, it is essential to develop techniques that help identify how the content of a discussion board grows through generalizations and specializations of topics. This knowledge can be used in segmenting the content in coherent units and guiding the users through segments relevant to their navigational goals. Our experimental results showed that the segmentation algorithm described in this paper provides up to 80-85% success rate in labeling messages. The algorithm is being deployed in a software system to reduce the navigational load of blind students in accessing web-based electronic course materials; however, we note that the techniques are equally applicable for developing web indexing and summarization tools for users with sight.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {322–331},
numpages = {10},
keywords = {assistive technology for blind users, navigational aid, discussion boards, segmentation},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060796,
author = {Cimiano, Philipp and Ladwig, G\"{u}nter and Staab, Steffen},
title = {Gimme' the Context: Context-Driven Automatic Semantic Annotation with C-PANKOW},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060796},
doi = {10.1145/1060745.1060796},
abstract = {Without the proliferation of formal semantic annotations, the Semantic Web is certainly doomed to failure. In earlier work we presented a new paradigm to avoid this: the 'Self Annotating Web', in which globally available knowledge is used to annotate resources such as web pages. In particular, we presented a concrete method instantiating this paradigm, called PANKOW (Pattern-based ANnotation through Knowledge On the Web). In PANKOW, a named entity to be annotated is put into several linguistic patterns that convey competing semantic meanings. The patterns that are matched most often on the Web indicate the meaning of the named entity --- leading to automatic or semi-automatic annotation.In this paper we present C-PANKOW (Context-driven PANKOW), which alleviates several shortcomings of PANKOW. First, by downloading abstracts and processing them off-line, we avoid the generation of large number of linguistic patterns and correspondingly large number of Google queries.Second, by linguistically analyzing and normalizing the downloaded abstracts, we increase the coverage of our pattern matching mechanism and overcome several limitations of the earlier pattern generation process. Third, we use the annotation context in order to distinguish the significance of a pattern match for the given annotation task. Our experiments show that C-PANKOW inherits all the advantages of PANKOW (no training required etc.), but in addition it is far more efficient and effective.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {332–341},
numpages = {10},
keywords = {semantic Web, annotation, information extraction, metadata},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060797,
author = {Liu, Bing and Hu, Minqing and Cheng, Junsheng},
title = {Opinion Observer: Analyzing and Comparing Opinions on the Web},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060797},
doi = {10.1145/1060745.1060797},
abstract = {The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {342–351},
numpages = {10},
keywords = {visualization, opinion analysis, information extraction, sentiment analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246144,
author = {Fraternali, Piero},
title = {Session Details: Web Engineering with Semantic Annotation},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246144},
doi = {10.1145/3246144},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060799,
author = {Plessers, Peter and Casteleyn, Sven and Yesilada, Yeliz and De Troyer, Olga and Stevens, Robert and Harper, Simon and Goble, Carole},
title = {Accessibility: A Web Engineering Approach},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060799},
doi = {10.1145/1060745.1060799},
abstract = {Currently, the vast majority of web sites do not support accessibility for visually impaired users. Usually, these users have to rely on screen readers: applications that sequentially read the content of a web page in audio. Unfortunately, screen readers are not able to detect the meaning of the different page objects, and thus the implicit semantic knowledge conveyed in the presentation of the page is lost. One approach described in literature to tackle this problem, is the Dante approach, which allows semantic annotation of web pages to provide screen readers with extra (semantic) knowledge to better facilitate the audio presentation of a web page. Until now, such annotations were done manually, and failed for dynamic pages. In this paper, we combine the Dante approach with a web design method, WSDM, to fully automate the generation of the semantic annotation for visually impaired users. To do so, the semantic knowledge gathered during the design process is exploited, and the annotations are generated as a by-product of the design process, requiring no extra effort from the designer.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {353–362},
numpages = {10},
keywords = {Web engineering, accessibility, visual impairment, semantic Web},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060800,
author = {Tanaka-Ishii, Kumiko and Nakagawa, Hiroshi},
title = {A Multilingual Usage Consultation Tool Based on Internet Searching: More than a Search Engine, Less than QA},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060800},
doi = {10.1145/1060745.1060800},
abstract = {We present a usage consultation tool, based on Internet searching, for language learners. When a user enters a string of words for which he wants to find usages, the system sends this string as a query to a search engine and obtains search results about the string. The usages are extracted by performing statistical analysis on snippets and then fed back to the user.Unlike existing tools, this usage consultation tool is multi-lingual, so that usages can be obtained even in a language for which there are no well-established analytical methods. Our evaluation has revealed that usages can be obtained more effectively than by only using a search engine directly. Also, we have found that the resulting usage does not depend on the search engine for a prominent usage when the amount of data downloaded from the search engine is increased.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {363–371},
numpages = {9},
keywords = {usage consultation, text mining, question answering},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060801,
author = {D\'{\i}az, Oscar and Iturrioz, Jon and Irastorza, Arantza},
title = {Improving Portlet Interoperability through Deep Annotation},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060801},
doi = {10.1145/1060745.1060801},
abstract = {Portlets (i.e. multi-step, user-facing applications to be syndicated within a portal) are currently supported by most portal frameworks. However, there is not yet a definitive answer to portlet interoperation whereby data flows smoothly from one portlet to a neighbouring one. Both data-based and API-based approaches exhibit some drawbacks in either the limitation of the sharing scope or the standardization effort required. We argue that these limitations can be overcome by using deep annotation techniques. By providing additional markup about the background services, deep annotation strives to interact with these underlying services rather than with the HTML surface that conveys the markup. In this way, the portlet producer can extend a portlet markup, a fragment, with data about the processes whose rendering this fragment supports. Then, the portlet consumer (e.g. a portal) can use deep annotation to map an output process in fragment A to an input process in fragment B. This mapping results in fragment B having its input form (or other "input" widget) filled up. We consider deep annotation as particularly valid for portlet interoperation due to the controlled and cooperative environment that characterizes the portal setting.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {372–381},
numpages = {10},
keywords = {portal ontology, data-flow, event, deep-annotation, portlet interoperability},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246145,
author = {Davidson, Brian},
title = {Session Details: User-Focused Search and Crawling},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246145},
doi = {10.1145/3246145},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060803,
author = {Sun, Jian-Tao and Zeng, Hua-Jun and Liu, Huan and Lu, Yuchang and Chen, Zheng},
title = {CubeSVD: A Novel Approach to Personalized Web Search},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060803},
doi = {10.1145/1060745.1060803},
abstract = {As the competition of Web search market increases, there is a high demand for personalized Web search to conduct retrieval incorporating Web users' information needs. This paper focuses on utilizing clickthrough data to improve Web search. Since millions of searches are conducted everyday, a search engine accumulates a large volume of clickthrough data, which records who submits queries and which pages he/she clicks on. The clickthrough data is highly sparse and contains different types of objects (user, query and Web page), and the relationships among these objects are also very complicated. By performing analysis on these data, we attempt to discover Web users' interests and the patterns that users locate information. In this paper, a novel approach CubeSVD is proposed to improve Web search. The clickthrough data is represented by a 3-order tensor, on which we perform 3-mode analysis using the higher-order singular value decomposition technique to automatically capture the latent factors that govern the relations among these multi-type objects: users, queries and Web pages. A tensor reconstructed based on the CubeSVD analysis reflects both the observed interactions among these objects and the implicit associations among them. Therefore, Web search activities can be carried out based on CubeSVD analysis. Experimental evaluations using a real-world data set collected from an MSN search engine show that CubeSVD achieves encouraging search results in comparison with some standard methods.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {382–390},
numpages = {9},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060804,
author = {Lee, Uichin and Liu, Zhenyu and Cho, Junghoo},
title = {Automatic Identification of User Goals in Web Search},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060804},
doi = {10.1145/1060745.1060804},
abstract = {There has been recent interests in studying the "goal" behind a user's Web query, so that this goal can be used to improve the quality of a search engine's results. Previous studies have mainly focused on using manual query-log investigation to identify Web query goals. In this paper we study whether and how we can automate this goal-identification process. We first present our results from a human subject study that strongly indicate the feasibility of automatic query-goal identification. We then propose two types of features for the goal-identification task: user-click behavior and anchor-link distribution. Our experimental evaluation shows that by combining these features we can correctly identify the goals for 90% of the queries studied.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {391–400},
numpages = {10},
keywords = {user goals, Web search, query classification},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060805,
author = {Pandey, Sandeep and Olston, Christopher},
title = {User-Centric Web Crawling},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060805},
doi = {10.1145/1060745.1060805},
abstract = {Search engines are the primary gateways of information access on the Web today. Behind the scenes, search engines crawl the Web to populate a local indexed repository of Web pages, used to answer user search queries. In an aggregate sense, the Web is very dynamic, causing any repository of Web pages to become out of date over time, which in turn causes query answer quality to degrade. Given the considerable size, dynamicity, and degree of autonomy of the Web as a whole, it is not feasible for a search engine to maintain its repository exactly synchronized with the Web.In this paper we study how to schedule Web pages for selective (re)downloading into a search engine repository. The scheduling objective is to maximize the quality of the user experience for those who query the search engine. We begin with a quantitative characterization of the way in which the discrepancy between the content of the repository and the current content of the live Web impacts the quality of the user experience. This characterization leads to a user-centric metric of the quality of a search engine's local repository. We use this metric to derive a policy for scheduling Web page (re)downloading that is driven by search engine usage and free of exterior tuning parameters. We then focus on the important subproblem of scheduling refreshing of Web pages already present in the repository, and show how to compute the priorities efficiently. We provide extensive empirical comparisons of our user-centric method against prior Web page refresh strategies, using real Web data. Our results demonstrate that our method requires far fewer resources to maintain same search engine quality level for users, leaving substantially more resources available for incorporating new Web pages into the search repository.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {401–411},
numpages = {11},
keywords = {Web crawling, user-centric, Web page refreshing},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246146,
author = {Manasse, Mark},
title = {Session Details: Trustworthy Web Sites},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246146},
doi = {10.1145/3246146},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060807,
author = {Wang, Guilin},
title = {An Abuse-Free Fair Contract Signing Protocol Based on the RSA Signature},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060807},
doi = {10.1145/1060745.1060807},
abstract = {A fair contract signing protocol allows two potentially mistrusted parities to exchange their commitments (i.e., digital signatures) to an agreed contract over the Internet in a fair way, so that either each of them obtains the other's signature, or neither party does. Based on the RSA signature scheme, a new digital contract signing protocol is proposed in this paper. Like the existing RSA-based solutions for the same problem, our protocol is not only fair, but also optimistic, since the third trusted party is involved only in the situations where one party is cheating or the communication channel is interrupted. Furthermore, the proposed protocol satisfies a new property, i.e., it is abuse-free. That is, if the protocol is executed unsuccessfully, none of the two parties can show the validity of intermediate results to others. Technical details are provided to analyze the security and performance of the proposed protocol. In summary, we present the first abuse-free fair contract signing protocol based on the RSA signature, and show that it is both secure and efficient.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {412–421},
numpages = {10},
keywords = {fair-exchange, security, e-commerce, contract signing, RSA, digital signatures, cryptographic protocols},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060808,
author = {Srivatsa, Mudhakar and Xiong, Li and Liu, Ling},
title = {TrustGuard: Countering Vulnerabilities in Reputation Management for Decentralized Overlay Networks},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060808},
doi = {10.1145/1060745.1060808},
abstract = {Reputation systems have been popular in estimating the trustworthiness and predicting the future behavior of nodes in a large-scale distributed system where nodes may transact with one another without prior knowledge or experience. One of the fundamental challenges in distributed reputation management is to understand vulnerabilities and develop mechanisms that can minimize the potential damages to a system by malicious nodes. In this paper, we identify three vulnerabilities that are detrimental to decentralized reputation management and propose TrustGuard - a safeguard framework for providing a highly dependable and yet efficient reputation system. First, we provide a dependable trust model and a set of formal methods to handle strategic malicious nodes that continuously change their behavior to gain unfair advantages in the system. Second, a transaction based reputation system must cope with the vulnerability that malicious nodes may misuse the system by flooding feedbacks with fake transactions. Third, but not least, we identify the importance of filtering out dishonest feedbacks when computing reputation-based trust of a node, including the feedbacks filed by malicious nodes through collusion. Our experiments show that, comparing with existing reputation systems, our framework is highly dependable and effective in countering malicious nodes regarding strategic oscillating behavior, flooding malevolent feedbacks with fake transactions, and dishonest feedbacks.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {422–431},
numpages = {10},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060809,
author = {Minamide, Yasuhiko},
title = {Static Approximation of Dynamically Generated Web Pages},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060809},
doi = {10.1145/1060745.1060809},
abstract = {Server-side programming is one of the key technologies that support today's WWW environment. It makes it possible to generate Web pages dynamically according to a user's request and to customize pages for each user. However, the flexibility obtained by server-side programming makes it much harder to guarantee validity and security of dynamically generated pages.To check statically the properties of Web pages generated dynamically by a server-side program, we develop a static program analysis that approximates the string output of a program with a context-free grammar. The approximation obtained by the analyzer can be used to check various properties of a server-side program and the pages it generates.To demonstrate the effectiveness of the analysis, we have implemented a string analyzer for the server-side scripting language PHP. The analyzer is successfully applied to publicly available PHP programs to detect cross-site scripting vulnerabilities and to validate pages they generate dynamically.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {432–441},
numpages = {10},
keywords = {server-side scripting, HTML validation, cross-site scripting, static analysis, context-free grammars},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246147,
author = {Cho, Junghoo},
title = {Session Details: Semantic Search},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246147},
doi = {10.1145/3246147},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060811,
author = {Cafarella, Michael J. and Etzioni, Oren},
title = {A Search Engine for Natural Language Applications},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060811},
doi = {10.1145/1060745.1060811},
abstract = {Many modern natural language-processing applications utilize search engines to locate large numbers of Web documents or to compute statistics over the Web corpus. Yet Web search engines are designed and optimized for simple human queries---they are not well suited to support such applications. As a result, these applications are forced to issue millions of successive queries resulting in unnecessary search engine load and in slow applications with limited scalability.In response, this paper introduces the Bindings Engine (BE), which supports queries containing typed variables and  string-processing functions. For example, in response to the query  "powerful ‹noun›" BE will return all the nouns in its index that immediately follow the word "powerful", sorted by frequency. In response to the query  "Cities such as ProperNoun(Head(‹NounPhrase›))", BE will return a list of proper nouns likely to be city names.BE's novel  neighborhood index enables it to do so with O(k) random disk seeks and O(k) serial disk reads, where k is the number of non-variable terms in its query. As a result, BE can yield several orders of magnitude speedup for large-scale language-processing applications. The main cost is a modest increase in space to store the index. We report on experiments validating these claims, and analyze how BE's space-time tradeoff scales with the size of its index and the number of variable types. Finally, we describe how a BE-based application extracts thousands of facts from the Web at interactive speeds in response to simple user queries.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {442–452},
numpages = {11},
keywords = {query, variables, corpus, indexing, information extraction, language, search engine},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060812,
author = {Zhang, Lei and Yu, Yong and Zhou, Jian and Lin, ChenXi and Yang, Yin},
title = {An Enhanced Model for Searching in Semantic Portals},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060812},
doi = {10.1145/1060745.1060812},
abstract = {Semantic Portal is the next generation of web portals that are powered by Semantic Web technologies for improved information sharing and exchange for a community of users. Current methods of searching in Semantic Portals are limited to keyword-based search using information retrieval (IR) techniques, ontology-based formal query and reasoning, or a simple combination of the two. In this paper, we propose an enhanced model that tightly integrates IR with formal query and reasoning to fully utilize both textual and semantic information for searching in Semantic Portals. The model extends the search capabilities of existing methods and can answer more complex search requests. The ideas in a fuzzy description logic (DL) IR model and a formal DL query method are employed and combined in our model. Based on the model, a semantic search service is implemented and evaluated. The evaluation shows very large improvements over existing methods.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {453–462},
numpages = {10},
keywords = {fuzzy description logic, semantic search, semantic portal, information retrieval, fuzzy reasoning},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060813,
author = {Bekkerman, Ron and McCallum, Andrew},
title = {Disambiguating Web Appearances of People in a Social Network},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060813},
doi = {10.1145/1060745.1060813},
abstract = {Say you are looking for information about a particular person. A search engine returns many pages for that person's name but which pages are about the person you care about, and which are about other people who happen to have the same name? Furthermore, if we are looking for multiple people who are related in some way, how can we best leverage this social network? This paper presents two unsupervised frameworks for solving this problem: one based on link structure of the Web pages, another using Agglomerative/Conglomerative Double Clustering (A/CDC)---an application of a recently introduced multi-way distributional clustering method. To evaluate our methods, we collected and hand-labeled a dataset of over 1000 Web pages retrieved from Google queries on 12 personal names appearing together in someones in an email folder. On this dataset our methods outperform traditional agglomerative clustering by more than 20%, achieving over 80% F-measure.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {463–470},
numpages = {8},
keywords = {Web appearance, document clustering, name disambiguation, social network, information bottleneck, link structure},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060815,
author = {Halderman, J. Alex and Waters, Brent and Felten, Edward W.},
title = {A Convenient Method for Securely Managing Passwords},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060815},
doi = {10.1145/1060745.1060815},
abstract = {Computer users are asked to generate, keep secret, and recall an increasing number of passwords for uses including host accounts, email servers, e-commerce sites, and online financial services. Unfortunately, the password entropy that users can comfortably memorize seems insufficient to store unique, secure passwords for all these accounts, and it is likely to remain constant as the number of passwords (and the adversary's computational power) increases into the future. In this paper, we propose a technique that uses a strengthened cryptographic hash function to compute secure passwords for arbitrarily many accounts while requiring the user to memorize only a single short password. This mechanism functions entirely on the client; no server-side changes are needed. Unlike previous approaches, our design is both highly resistant to brute force attacks and nearly stateless, allowing users to retrieve their passwords from any location so long as they can execute our program and remember a short secret. This combination of security and convenience will, we believe, entice users to adopt our scheme. We discuss the construction of our algorithm in detail, compare its strengths and weaknesses to those of related approaches, and present Password Multiplier, an implementation in the form of an extension to the Mozilla Firefox web browser.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {471–479},
numpages = {9},
keywords = {password security, website user authentication},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060816,
author = {Levy, Stephen E. and Gutwin, Carl},
title = {Improving Understanding of Website Privacy Policies with Fine-Grained Policy Anchors},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060816},
doi = {10.1145/1060745.1060816},
abstract = {Website privacy policies state the ways that a site will use personal identifiable information (PII) that is collected from fields and forms in web-based transactions. Since these policies can be complex, machine-readable versions have been developed that allow automatic comparison of a site's privacy policy with a user's privacy preferences. However, it is still difficult for users to determine the cause and origin of conformance conflicts, because current standards operate at the page level - they can only say that there is a conflict on the page, not where the conflict occurs or what causes it. In this paper we describe fine-grained policy anchors, an extension to the way a website implements the Platform for Privacy Preferences (P3P), that solves this problem. Fine grained policy anchors enable field-level comparisons of policy and preference, field-specific conformance displays, and faster access to additional conformance information. We built a prototype user agent based on these extensions and tested it with representative users. We found that fine-grained anchors do help users understand how privacy policy relates to their privacy preferences, and where and why conformance conflicts occur.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {480–488},
numpages = {9},
keywords = {conformance conflicts, e-commerce, privacy policies, APPEL, privacy preferences, P3P, user agents},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060817,
author = {Xia, Haidong and Brustoloni, Jos\'{e} Carlos},
title = {Hardening Web Browsers against Man-in-the-Middle and Eavesdropping Attacks},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060817},
doi = {10.1145/1060745.1060817},
abstract = {Existing Web browsers handle security errors in a manner that often confuses users. In particular, when a user visits a secure site whose certificate the browser cannot verify, the browser typically allows the user to view and install the certificate and connect to the site despite the verification failure. However, few users understand the risk of man-in-the-middle attacks and the principles behind certificate-based authentication. We propose context-sensitive certificate verification (CSCV), whereby the browser interrogates the user about the context in which a certificate verification error occurs. Considering the context, the browser then guides the user in handling and possibly overcoming the security error. We also propose specific password warnings (SPW) when users are about to send passwords in a form vulnerable to eavesdropping. We performed user studies to evaluate CSCV and SPW. Our results suggest that CSCV and SPW can greatly improve Web browsing security and are easy to use even without training. Moreover, CSCV had greater impact than did staged security training.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {489–498},
numpages = {10},
keywords = {certificate, password, Web browser, eavesdropping attack, safe staging, HTTPS, just-in-time instruction, well-in-advance instruction, man-in-the-middle attack, SSL},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246148,
author = {Shenoy, Prashant},
title = {Session Details: Measurements and Analysis},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246148},
doi = {10.1145/3246148},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060819,
author = {Krishnamurthy, Balachander and Madhyastha, Harsha V. and Spatscheck, Oliver},
title = {ATMEN: A Triggered Network Measurement Infrastructure},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060819},
doi = {10.1145/1060745.1060819},
abstract = {Web performance measurements and availability tests have been carried out using a variety of infrastructures over the last several years. Disruptions in the Internet can lead to Web sites being unavailable or increase user-perceived latency. The unavailability could be due to DNS, failures in segments of the physical network cutting off thousands of users, or attacks. Prompt reactions to network-wide events can be facilitated by local or remote measurement and monitoring. Better yet, a distributed set of intercommunicating measurement and monitoring entities that react to events dynamically could go a long way to handle disruptions.We have designed and built ATMEN, a triggered measurement infrastructure to communicate and coordinate across various administrative entities. ATMEN nodes can trigger new measurements, query ongoing passive measurements or historical measurements stored on remote nodes, and coordinate the responses to make local decisions. ATMEN reduces wasted measurements by judiciously reusing measurements along three axes: spatial, temporal, and application.We describe the use of ATMEN for key Web applications such as performance based ranking of popular Web sites and availability of DNS servers on which most Web transactions are dependent. The evaluation of ATMEN is done using multiple network monitoring entities called Gigascopes installed across the USA, measurement data of a popular network application involving millions of users distributed across the Internet, and scores of clients to aid in gathering measurement information upon demand. Our results show that such a system can be built in a scalable fashion.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {499–509},
numpages = {11},
keywords = {triggered measurements, DNS availability, measurement, reuse, performance-based ranking, measurement infrastructure},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060820,
author = {Meiss, Mark and Menczer, Filippo and Vespignani, Alessandro},
title = {On the Lack of Typical Behavior in the Global Web Traffic Network},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060820},
doi = {10.1145/1060745.1060820},
abstract = {We offer the first large-scale analysis of Web traffic based on network flow data. Using data collected on the Internet2 network, we constructed a weighted bipartite client-server host graph containing more than 18 x 106 vertices and 68 x 106 edges valued by relative traffic flows. When considered as a traffic map of the World-Wide Web, the generated graph provides valuable information on the statistical patterns that characterize the global information flow on the Web. Statistical analysis shows that client-server connections and traffic flows exhibit heavy-tailed probability distributions lacking any typical scale. In particular, the absence of an intrinsic average in some of the distributions implies the absence of a prototypical scale appropriate for server design, Web-centric network design, or traffic modeling. The inspection of the amount of traffic handled by clients and servers and their number of connections highlights non-trivial correlations between information flow and patterns of connectivity as well as the presence of anomalous statistical patterns related to the behavior of users on the Web. The results presented here may impact considerably the modeling, scalability analysis, and behavioral study of Web applications.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {510–518},
numpages = {9},
keywords = {strength, traffic statistics, degree, network flows, Web usage, power laws, scale-free networks},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060821,
author = {Guo, Lei and Chen, Songqing and Xiao, Zhen and Zhang, Xiaodong},
title = {Analysis of Multimedia Workloads with Implications for Internet Streaming},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060821},
doi = {10.1145/1060745.1060821},
abstract = {In this paper, we study the media workload collected from a large number of commercial Web sites hosted by a major ISP and that collected from a large group of home users connected to the Internet via a well-known cable company. Some of our key findings are: (1) Surprisingly, the majority of media contents are still delivered via downloading from Web servers. (2) A substantial percentage of media downloading connections are aborted before completion due to the long waiting time. (3) A hybrid approach, pseudo streaming, is used by clients to imitate real streaming. (4) The mismatch between the downloading rate and the client playback speed in pseudo streaming is common, which either causes frequent playback delays to the clients, or unnecessary traffic to the Internet. (5) Compared with streaming, downloading and pseudo streaming are neither bandwidth efficient nor performance effective. To address this problem, we propose the design of AutoStream, an innovative system that can provide additional previewing and streaming services automatically for media objects hosted on standard Web sites in server farms at the client's will.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {519–528},
numpages = {10},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060823,
author = {Bonatti, P. A. and Festa, P.},
title = {On Optimal Service Selection},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060823},
doi = {10.1145/1060745.1060823},
abstract = {While many works have been devoted to service matchmaking and modeling nonfunctional properties, the problem of matching service requests to offers in an optimal way has not yet been extensively studied. In this paper we formalize three kinds of optimal service selection problems, based on different criteria. Then we study their complexity and implement solutions. We prove that one-time costs make the optimal selection problem computationally hard; in the absence of these costs the problem can be solved in polynomial time. We designed and implemented both exact and heuristic (suboptimal) algorithms for the hard case, and carried out a preliminary experimental evaluation with interesting results.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {530–538},
numpages = {9},
keywords = {automatic service composition, nonfunctional properties, service selection problem, service matchmaking},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060824,
author = {Petrovic, Milenko and Liu, Haifeng and Jacobsen, Hans-Arno},
title = {G-ToPSS: Fast Filtering of Graph-Based Metadata},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060824},
doi = {10.1145/1060745.1060824},
abstract = {RDF is increasingly being used to represent metadata. RDF Site Summary (RSS) is an application of RDF on the Web that has considerably grown in popularity. However, the way RSS systems operate today does not scale well. In this paper we introduce G-ToPSS, a scalable publish/subscribe system for selective information dissemination. G-ToPSS is particularly well suited for applications that deal with large-volume content distribution from diverse sources. RSS is an instance of the content distribution problem. G-ToPSS allows use of ontology as a way to provide additional information about the data. Furthermore, in this paper we show how G-ToPSS can support RDFS class taxonomies. We have implemented and experimentally evaluated G-ToPSS and we provide results in the paper demonstrating its scalability compared to alternatives.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {539–547},
numpages = {9},
keywords = {content-based routing, information dissemination, RDF, publish/subscribe, graph matching},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060825,
author = {Cardinaels, Kris and Meire, Michael and Duval, Erik},
title = {Automating Metadata Generation: The Simple Indexing Interface},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060825},
doi = {10.1145/1060745.1060825},
abstract = {In this paper, we focus on the development of a framework for automatic metadata generation. The first step towards this framework is the definition of an Application Programmer Interface (API), which we call the Simple Indexing Interface (SII). The second step is the definition of a framework for implementation of the SII. Both steps are presented in some detail in this paper. We also report on empirical evaluation of the metadata that the SII and supporting framework generated in a real-life context.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {548–556},
numpages = {9},
keywords = {learning objects, metadata generation},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246149,
author = {Suel, Torsten},
title = {Session Details: Link-Based Ranking},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246149},
doi = {10.1145/3246149},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060827,
author = {Boldi, Paolo and Santini, Massimo and Vigna, Sebastiano},
title = {PageRank as a Function of the Damping Factor},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060827},
doi = {10.1145/1060745.1060827},
abstract = {PageRank is defined as the stationary state of a Markov chain. The chain is obtained by perturbing the transition matrix induced by a web graph with a damping factor α that spreads uniformly part of the rank. The choice of α is eminently empirical, and in most cases the original suggestion α = 0.85 by Brin and Page is still used. Recently, however, the behaviour of PageRank with respect to changes in α was discovered to be useful in link-spam detection[21]. Moreover, an analytical justification of the value chosen for α is still missing. In this paper, we give the first mathematical analysis of PageRank when α changes. In particular, we show that, contrarily to popular belief, for real-world graphs values of α close to 1 do not give a more meaningful ranking. Then, we give closed-form formulae for PageRank derivatives of any order, and an extension of the Power Method that approximates them with convergence O (tk αt) for the k-th derivative. Finally, we show a tight connection between iterated computation and analytical behaviour by proving that the k-th iteration of the Power Method gives exactly the PageRank value obtained using a Maclaurin polynomial of degree k. The latter result paves the way towards the application of analytical methods to the study of PageRank.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {557–566},
numpages = {10},
keywords = {Web graph, approximation, PageRank},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060828,
author = {Nie, Zaiqing and Zhang, Yuanzhi and Wen, Ji-Rong and Ma, Wei-Ying},
title = {Object-Level Ranking: Bringing Order to Web Objects},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060828},
doi = {10.1145/1060745.1060828},
abstract = {In contrast with the current Web search methods that essentially do document-level ranking and retrieval, we are exploring a new paradigm to enable Web search at the object level. We collect Web information for objects relevant for a specific application domain and rank these objects in terms of their relevance and popularity to answer user queries. Traditional PageRank model is no longer valid for object popularity calculation because of the existence of heterogeneous relationships between objects. This paper introduces PopRank, a domain-independent object-level link analysis model to rank the objects within a specific domain. Specifically we assign a popularity propagation factor to each type of object relationship, study how different popularity propagation factors for these heterogeneous relationships could affect the popularity ranking, and propose efficient approaches to automatically decide these factors. Our experiments are done using 1 million CS papers, and the experimental results show that PopRank can achieve significantly better ranking results than naively applying PageRank on the object graph.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {567–574},
numpages = {8},
keywords = {PageRank, PopRank, Web information retrieval, Web objects, link analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060829,
author = {McSherry, Frank},
title = {A Uniform Approach to Accelerated PageRank Computation},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060829},
doi = {10.1145/1060745.1060829},
abstract = {In this note we consider a simple reformulation of the traditional power iteration algorithm for computing the stationary distribution of a Markov chain. Rather than communicate their current probability values to their neighbors at each step, nodes instead communicate only changes in probability value. This reformulation enables a large degree of flexibility in the manner in which nodes update their values, leading to an array of optimizations and features, including faster convergence, efficient incremental updating, and a robust distributed implementation.While the spirit of many of these optimizations appear in previous literature, we observe several cases where this unification simplifies previous work, removing technical complications and extending their range of applicability. We implement and measure the performance of several optimizations on a sizable (34M node) web subgraph, seeing significant composite performance gains, especially for the case of incremental recomputation after changes to the web graph.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {575–582},
numpages = {8},
keywords = {PageRank, link analysis, random walks, web graph},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246150,
author = {Maarek, Yoelle},
title = {Session Details: Improving the Browsing Experience},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246150},
doi = {10.1145/3246150},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060831,
author = {Aula, Anne and Jhaveri, Natalie and K\"{a}ki, Mika},
title = {Information Search and Re-Access Strategies of Experienced Web Users},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060831},
doi = {10.1145/1060745.1060831},
abstract = {Experienced web users have strategies for information search and re-access that are not directly supported by web browsers or search engines. We studied how prevalent these strategies are and whether even experienced users have problems with searching and re-accessing information. With this aim, we conducted a survey with 236 experienced web users. The results showed that this group has frequently used key strategies (e.g., using several browser windows in parallel) that they find important, whereas some of the strategies that have been suggested in previous studies are clearly less important for them (e.g., including URLs on a webpage). In some aspects, such as query formulation, this group resembles less experienced web users. For instance, we found that most of the respondents had misconceptions about how their search engine handles queries, as well as other problems with information search and re-access. In addition to presenting the prevalence of the strategies and rationales for their use, we present concrete designs solutions and ideas for making the key strategies also available to less experienced users.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {583–592},
numpages = {10},
keywords = {experienced web users, questionnaire study, information re-access, web search},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060832,
author = {Mukherjee, Saikat and Ramakrishnan, I. V.},
title = {Browsing Fatigue in Handhelds: Semantic Bookmarking Spells Relief},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060832},
doi = {10.1145/1060745.1060832},
abstract = {Focused Web browsing activities such as periodically looking up headline news, weather reports, etc., which require only selective fragments of particular Web pages, can be made more efficient for users of limited-display-size handheld mobile devices by delivering only the target fragments. Semantic bookmarks provide a robust conceptual framework for recording and retrieving such targeted content not only from the specific pages used in creating the bookmarks but also from any user-specified page with similar content semantics. This paper describes a technique for realizing semantic bookmarks by coupling machine learning with Web page segmentation to create a statistical model of the bookmarked content. These models are used to identify and retrieve the bookmarked content from Web pages that share a common content domain. In contrast to ontology-based approaches where semantic bookmarks are limited to available concepts in the ontology, the learning-based approach allows users to bookmark ad-hoc personalized semantic concepts to effectively target content that fits the limited display of handhelds. User evaluation measuring the effectiveness of a prototype implementation of learning-based semantic bookmarking at reducing browsing fatigue in handhelds is provided.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {593–602},
numpages = {10},
keywords = {Web page partitioning, semantic bookmarking, handheld device content adaptation},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060833,
author = {Potter, Shaya and Nieh, Jason},
title = {WebPod: Persistent Web Browsing Sessions with Pocketable Storage Devices},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060833},
doi = {10.1145/1060745.1060833},
abstract = {We present WebPod, a portable system that enables mobile users to use the same persistent, personalized web browsing session on any Internet-enabled device. No matter what computer is being used, WebPod provides a consistent browsing session, maintaining all of a user's plugins, bookmarks, browser web content, open browser windows, and browser configuration options and preferences. This is achieved by leveraging rapid improvements in capacity, cost, and size of portable storage devices. WebPod provides a virtualization and checkpoint/restart mechanism that decouples the browsing environment from the host, enabling web browsing sessions to be suspended to portable storage, carried around, and resumed from the storage device on another computer. WebPod virtualization also isolates web browsing sessions from the host, protecting the browsing privacy of the user and preventing malicious web content from damaging the host. We have implemented a Linux WebPod prototype and demonstrate its ability to quickly suspend and resume web browsing sessions, enabling a seamless web browsing experience for mobile users as they move among computers.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {603–612},
numpages = {10},
keywords = {process migration, web browsing, virtualization, portable storage, checkpoint/restart},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246151,
author = {Staab, Steffen},
title = {Session Details: Semantic Web Foundations},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246151},
doi = {10.1145/3246151},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060835,
author = {Carroll, Jeremy J. and Bizer, Christian and Hayes, Pat and Stickler, Patrick},
title = {Named Graphs, Provenance and Trust},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060835},
doi = {10.1145/1060745.1060835},
abstract = {The Semantic Web consists of many RDF graphs nameable by URIs. This paper extends the syntax and semantics of RDF to cover such Named Graphs. This enables RDF statements that describe graphs, which is beneficial in many Semantic Web application areas. As a case study, we explore the application area of Semantic Web publishing: Named Graphs allow publishers to communicate assertional intent, and to sign their graphs; information consumers can evaluate specific graphs using task-specific trust policies, and act on information from those Named Graphs that they accept. Graphs are trusted depending on: their content; information about the graph; and the task the user is performing. The extension of RDF to Named Graphs provides a formally defined framework to be a foundation for the Semantic Web trust layer.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {613–622},
numpages = {10},
keywords = {semantic Web, provenance, trust, RDF},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060836,
author = {de Bruijn, Jos and Lara, Rub\'{e}n and Polleres, Axel and Fensel, Dieter},
title = {OWL DL vs. OWL Flight: Conceptual Modeling and Reasoning for the Semantic Web},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060836},
doi = {10.1145/1060745.1060836},
abstract = {The Semantic Web languages RDFS and OWL have been around for some time now. However, the presence of these languages has not brought the breakthrough of the Semantic Web the creators of the languages had hoped for. OWL has a number of problems in the area of interoperability and usability in the context of many practical application scenarios which impede the connection to the Software Engineering and Database communities. In this paper we present OWL Flight, which is loosely based on OWL, but the semantics is grounded in Logic Programming rather than Description Logics, and it borrows the constraint-based modeling style common in databases. This results in different types of modeling primitives and enforces a different style of ontology modeling. We analyze the modeling paradigms of OWL DL and OWL Flight, as well as reasoning tasks supported by both languages. We argue that different applications on the Semantic Web require different styles of modeling and thus both types of languages are required for the Semantic Web.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {623–632},
numpages = {10},
keywords = {description logics, logic programming, ontologies, semantic Web},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060837,
author = {Parsia, Bijan and Sirin, Evren and Kalyanpur, Aditya},
title = {Debugging OWL Ontologies},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060837},
doi = {10.1145/1060745.1060837},
abstract = {As an increasingly large number of OWL ontologies become available on the Semantic Web and the descriptions in the ontologies become more complicated, finding the cause of errors becomes an extremely hard task even for experts. Existing ontology development environments provide some limited support, in conjunction with a reasoner, for detecting and diagnosing errors in OWL ontologies. Typically these are restricted to the mere detection of, for example, unsatisfiable concepts. We have integrated a number of simple debugging cues generated from our description logic reasoner, Pellet, in our hypertextual ontology development environment, Swoop. These cues, in conjunction with extensive undo/redo and Annotea based collaboration support in Swoop, significantly improve the OWL debugging experience, and point the way to more general improvements in the presentation of an ontology to new users.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {633–640},
numpages = {8},
keywords = {ontology engineering, explanation, OWL, semantic Web},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/3246152,
author = {Najork, Marc},
title = {Session Details: Link-Based Similarity},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246152},
doi = {10.1145/3246152},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060839,
author = {Fogaras, D\'{a}niel and R\'{a}cz, Bal\'{a}zs},
title = {Scaling Link-Based Similarity Search},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060839},
doi = {10.1145/1060745.1060839},
abstract = {To exploit the similarity information hidden in the hyperlink structure of the web, this paper introduces algorithms scalable to graphs with billions of vertices on a distributed architecture. The similarity of multi-step neighborhoods of vertices are numerically evaluated by similarity functions including SimRank [20], a recursive refinement of cocitation; PSimRank, a novel variant with better theoretical characteristics; and the Jaccard coefficient, extended to multi-step neighborhoods. Our methods are presented in a general framework of Monte Carlo similarity search algorithms that precompute an index database of random fingerprints, and at query time, similarities are estimated from the fingerprints. The performance and quality of the methods were tested on the Stanford Webbase [19] graph of 80M pages by comparing our scores to similarities extracted from the ODP directory [26]. Our experimental results suggest that the hyperlink structure of vertices within four to five steps provide more adequate information for similarity search than single-step neighborhoods.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {641–650},
numpages = {10},
keywords = {scalability, similarity search, fingerprint, link-analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060840,
author = {Bawa, Mayank and Condie, Tyson and Ganesan, Prasanna},
title = {LSH Forest: Self-Tuning Indexes for Similarity Search},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060840},
doi = {10.1145/1060745.1060840},
abstract = {We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings: Web search engines desire fast, parallel, main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data, including text and images; peer-to-peer systems desire distributed similarity indexes with low communication cost. We propose an indexing scheme called LSH Forest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing (LSH), but improves upon previous designs by (a) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned, and (b) improving on LSH's performance guarantees for skewed data distributions while retaining the same storage and query overhead. We show how to construct this index in main memory, on disk, in parallel systems, and in peer-to-peer systems. We evaluate the design with experiments on multiple text corpora and demonstrate both the self-tuning nature and the superior performance of LSH Forest.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {651–660},
numpages = {10},
keywords = {similarity indexes, peer-to-peer (P2P)},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060841,
author = {Ino, Hidehiko and Kudo, Mineichi and Nakamura, Atsuyoshi},
title = {Partitioning of Web Graphs by Community Topology},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060841},
doi = {10.1145/1060745.1060841},
abstract = {We introduce a stricter Web community definition to overcome boundary ambiguity of a Web community defined by Flake, Lawrence and Giles [2], and consider the problem of finding communities that satisfy our definition. We discuss how to find such communities and hardness of this problem.We also propose Web page partitioning by equivalence relation defined using the class of communities of our definition. Though the problem of efficiently finding all communities of our definition is NP-complete, we propose an efficient method of finding a subclass of communities among the sets partitioned by each of n-1 cuts represented by a Gomory-Hu tree [10], and partitioning a Web graph by equivalence relation defined using the subclass.According to our preliminary experiments, partitioning by our method divided the pages retrieved by keyword search into several different categories to some extent.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {661–669},
numpages = {9},
keywords = {Web community, maximum flow algorithm, graph partitioning},
location = {Chiba, Japan},
series = {WWW '05}
}

@dataset{10.1145/review-1060745.1060841_R39955,
author = {Angelis, Lefteris},
title = {Review ID:R39955 for DOI: 10.1145/1060745.1060841},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1060745.1060841_R39955}
}

@inproceedings{10.1145/3246153,
author = {Makoto, Murata},
title = {Session Details: XML Parsing and Stylesheets},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246153},
doi = {10.1145/3246153},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060843,
author = {Onizuka, Makoto and Chan, Fong Yee and Michigami, Ryusuke and Honishi, Takashi},
title = {Incremental Maintenance for Materialized XPath/XSLT Views},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060843},
doi = {10.1145/1060745.1060843},
abstract = {This paper proposes an incremental maintenance algorithm that efficiently updates the materialized XPath/XSLT views defined using XPath expressions in XP([],*,//,vars). The algorithm consists of two processes. 1) The dynamic execution flow of an XSLT program is stored as an XT (XML Transformation) tree during the full transformation. 2) In response to a source XML data update, the impacted portions of the XT-tree are identified and maintained by partially re-evaluating the XSLT program. This paper discusses the XPath/XSLT features of incremental view maintenance for subtree insertion/deletion and applies them to the maintenance algorithm. Experiments show that the incremental maintenance algorithm outperforms full XML transformation algorithms by factors of up to 500.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {671–681},
numpages = {11},
keywords = {view maintenance, materialized view, XSLT, XPath, XML},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060844,
author = {Fokoue, Achille and Rose, Kristoffer and Sim\'{e}on, J\'{e}r\^{o}me and Villard, Lionel},
title = {Compiling XSLT 2.0 into XQuery 1.0},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060844},
doi = {10.1145/1060745.1060844},
abstract = {As XQuery is gathering momentum as the standard query language for XML, there is a growing interest in using it as an integral part of the XML application development infrastructure. In that context, one question which is often raised is how well XQuery interoperates with other XML languages, and notably with XSLT. XQuery 1.0 [16] and XSLT 2.0 [7] share a lot in common: they share XPath 2.0 as a common sub-language and have the same expressiveness. However, they are based on fairly different programming paradigms. While XSLT has adopted a highly declarative template based approach, XQuery relies on a simpler, and more operational, functional approach.In this paper, we present an approach to compile XSLT 2.0 into XQuery 1.0, and a working implementation of that approach. The compilation rules explain how XSLT's template-based approach can be implemented using the functional approach of XQuery and underpins the tight connection between the two languages. The resulting compiler can be used to migrate a XSLT code base to XQuery, or to enable the use of XQuery runtimes (e.g., as will soon be provided by most relational database management systems) for XSLT users. We also identify a number of areas where compatibility between the two languages could be improved. Finally, we show experiments on actual XSLT stylesheets, demonstrating the applicability of the approach in practice.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {682–691},
numpages = {10},
keywords = {XQuery, XSLT, Web services, XML},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060845,
author = {Takase, Toshiro and MIYASHITA, Hisashi and Suzumura, Toyotaro and Tatsubori, Michiaki},
title = {An Adaptive, Fast, and Safe XML Parser Based on Byte Sequences Memorization},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060845},
doi = {10.1145/1060745.1060845},
abstract = {XML (Extensible Markup Language) processing can incur significant runtime overhead in XML-based infrastructural middleware such as Web service application servers. This paper proposes a novel mechanism for efficiently processing similar XML documents. Given a new XML document as a byte sequence, the XML parser proposed in this paper normally avoids syntactic analysis but simply matches the document with previously processed ones, reusing those results. Our parser is adaptive since it partially parses and then remembers XML document fragments that it has not met before. Moreover, it processes safely since its partial parsing correctly checks the well-formedness of documents. Our implementation of the proposed parser complies with the JSR 63 standard of the Java API for XML Processing (JAXP) 1.1 specification. We evaluated Deltarser performance with messages using Google Web services. Comparing to Piccolo (and Apache Xerces), it effectively parses 35% (106%) faster in a server-side use-case scenario, and 73% (126%) faster in a client-side use-case scenario.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {692–701},
numpages = {10},
keywords = {automata, XML parsers, SAX},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060847,
author = {Bry, Fran\c{c}ois and Ries, Frank-Andr\'{e} and Spranger, Stephanie},
title = {CaTTS: Calendar Types and Constraints for Web Applications},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060847},
doi = {10.1145/1060745.1060847},
abstract = {Data referring to cultural calendars such as the widespread Gregorian dates but also dates after the Chinese, Hebrew, or Islamic calendars as well as data referring to professional calendars like fiscal years or teaching terms are omnipresent on the Web. Formalisms such as XML Schema have acknowledged this by offering a rather extensive set of Gregorian dates and times as basic data types. This article introduces into CaTTS, the <u>C</u>alendar <u>a</u>nd <u>T</u>ime <u>T</u>ype <u>S</u>ystem. CaTTS goes far beyond predefined date and time types after the Gregorian calendar as supported by XML Schema. CaTTS first gives rise to  declaratively specify more or less complex cultural or professional calendars including specificities such as leap seconds, leap years, and time zones. CaTTS further offers a tool for the static type checking (of data typed after calendar(s) defined in CaTTS). CaTTS finally offers a language for declaratively expressing and a solver for efficiently solving temporal constraints (referring to calendar(s) expressed in CaTTS). CaTTS complements data modeling and reasoning methods designed for generic Semantic Web applications such as RDF or OWL with methods specific to the particular application domain of calendars and time.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {702–711},
numpages = {10},
keywords = {Web reasoning, types, time, calendars},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060848,
author = {Bex, Geert Jan and Martens, Wim and Neven, Frank and Schwentick, Thomas},
title = {Expressiveness of XSDs: From Practice to Theory, There and Back Again},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060848},
doi = {10.1145/1060745.1060848},
abstract = {On an abstract level, XML Schema increases the limited expressive power of Document Type Definitions (DTDs) by extending them with a recursive typing mechanism. However, an investigation of the XML Schema Definitions (XSDs) occurring in practice reveals that the vast majority of them are structurally equivalent to DTDs. This might be due to the complexity of the XML Schema specification and the difficulty to understand the effect of constraints on typing and validation of schemas. To shed some light on the actual expressive power of XSDs this paper studies the impact of the Element Declarations Consistent (EDC) and the Unique Particle Attribution (UPA) rule. An equivalent formalism based on contextual patterns rather than on recursive types is proposed which might serve as a light-weight front end for XML Schema. Finally, the effect of EDC and UPA on the way XML documents can be typed is discussed. It is argued that a cleaner, more robust, stronger but equally efficient class is obtained by replacing EDC and UPA with the notion of 1-pass preorder typing: schemas that allow to determine the type of an element of a streaming document when its opening tag is met. This notion can be defined in terms of restrained competition regular expressions and there is again an equivalent syntactical formalism based on contextual patterns.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {712–721},
numpages = {10},
keywords = {expressiveness, formal model, XML schema},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060849,
author = {Reif, Gerald and Gall, Harald and Jazayeri, Mehdi},
title = {WEESA: Web Engineering for Semantic Web Applications},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060849},
doi = {10.1145/1060745.1060849},
abstract = {The success of the Semantic Web crucially depends on the existence of Web pages that provide machine-understandable meta-data. This meta-data is typically added in the semantic annotation process which is currently not part of the Web engineering process. Web engineering, however, proposes methodologies to design, implement and maintain Web applications but lack the generation of meta-data. In this paper we introduce a technique to extend existing Web engineering methodologies to develop semantically annotated Web pages. The novelty of this approach is the definition of a mapping from XML Schema to ontologies, called WEESA, that can be used to automatically generate RDF meta-data from XML content documents. We further show how we integrated the WEESA mapping into an Apache Cocoon transformer to easily extend XML based Web applications to semantically annotated Web application.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {722–729},
numpages = {8},
keywords = {Web engineering, semantic annotation, semantic Web, ontology},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060851,
author = {Choi, Gyu Sang and Kim, Jin-Ha and Ersoz, Deniz and Das, Chita R.},
title = {A Multi-Threaded PIPELINED Web Server Architecture for SMP/SoC Machines},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060851},
doi = {10.1145/1060745.1060851},
abstract = {Design of high performance Web servers has become a recent research thrust to meet the increasing demand of network-based services. In this paper, we propose a new Web server architecture, called multi-threaded PIPELINED Web server, suitable for Symmetric Multi-Processor (SMP) or System-on-Chip (SoC) architectures. The proposed PIPELINED model consists of multiple thread pools, where each thread pool consists of five basic threads and two helper threads. The main advantages of the proposed model are global information sharing by the threads, minimal synchronization overhead due to less number of threads, and non-blocking I/O operations, possible with the helper threads.We have conducted an in-depth performance analysis of the proposed server model along with four prior Web server models (Multi-Process (MP), Multi-Thread (MT), Single-Process Event-Driven (SPED) and Asynchronous Multi-Process Event-Driven (AMPED)) via simulation using six Web server workloads. The experiments are conducted to investigate the impact of various factors such as the memory size, disk speed and numbers of clients. The simulation results indicate that the proposed PIPELINED Web server architecture shows the best performance across all system and workload parameters compared to the MP, MT, SPED and AMPED models. Although the MT and AMPED models show competitive performance with less number of processors, the advantage of the PIPELINED model becomes obvious as the number of processors or clients in an SMP/SoC machine increases. The MP model shows the worst performance in most of the cases. The results indicate that the proposed server architecture can be used in future large-scale SMP/SoC machines to boost system performance.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {730–739},
numpages = {10},
keywords = {multi-thread, symmetric multi-processor, single event-driven process, system-on-chip, asynchronous multi-process event-driven, multi-process},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060852,
author = {Urgaonkar, Bhuvan and Shenoy, Prashant},
title = {Cataclysm: Policing Extreme Overloads in Internet Applications},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060852},
doi = {10.1145/1060745.1060852},
abstract = {In this paper we present the Cataclysm server platform for handling extreme overloads in hosted Internet applications. The primary contribution of our work is to develop a low overhead, highly scalable admission control technique for Internet applications. Cataclysm provides several desirable features, such as guarantees on response time by conducting accurate size-based admission control, revenue maximization at multiple time-scales via preferential admission of important requests and dynamic capacity provisioning, and the ability to be operational even under extreme overloads. Cataclysm can transparently trade-off the accuracy of its decision making with the intensity of the workload allowing it to handle incoming rates of several tens of thousands of requests/second. We implement a prototype Cataclysm hosting platform on a Linux cluster and demonstrate the benefits of our integrated approach using a variety of workloads.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {740–749},
numpages = {10},
keywords = {internet application, sentry, overload},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1060745.1060853,
author = {Betin-Can, Aysu and Bultan, Tevfik and Fu, Xiang},
title = {Design for Verification for Asynchronously Communicating Web Services},
year = {2005},
isbn = {1595930469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1060745.1060853},
doi = {10.1145/1060745.1060853},
abstract = {We present a design for verification approach to developing reliable web services. We focus on composite web services which consist of asynchronously communicating peers. Our goal is to automatically verify properties of interactions among such peers. We propose a design pattern that eases the development of such web services and enables a modular, assume-guarantee style verification strategy. In the proposed design pattern, each peer is associated with a behavioral interface description which specifies how that peer will interact with other peers. Using these peer interfaces we automatically generate BPEL specifications to publish for interoperability. Assuming that the participating peers behave according to their interfaces, we verify safety and liveness properties about the global behavior of the composite web service during behavior verification. During interface verification, we check that each peer implementation conforms to its interface. Using the modularity in the proposed design pattern, we are able to perform the interface verification of each peer and the behavior verification as separate steps. Our experiments show that, using this modular approach, one can automatically and efficiently verify web service implementations.},
booktitle = {Proceedings of the 14th International Conference on World Wide Web},
pages = {750–759},
numpages = {10},
keywords = {composite web services, design patterns, asynchronous communication, BPEL},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062747,
author = {Roto, Virpi and Oulasvirta, Antti},
title = {Need for Non-Visual Feedback with Long Response Times in Mobile HCI},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062747},
doi = {10.1145/1062745.1062747},
abstract = {When browsing Web pages with a mobile device, the system response times are variable and much longer than on a PC. Users must repeatedly glance at the display to see when the page finally arrives, although mobility demands a Minimal Attention User Interface. We conducted a user study with 27 participants to discover the point at which visual feedback stops reaching the user in mobile context. In the study, we examined the deployment of attention during page loading to the phone vs. the environment in several different everyday mobility contexts, and compared these to the laboratory context. The first part of the page appeared on the screen typically in 11 seconds, but we found that the user's visual attention shifted away from the mobile browser usually between 4 and 8 seconds in the mobile context. In contrast, the continuous span of attention to the browser was more than 14 seconds in the laboratory condition. Based on our study results, we recommend mobile applications provide multimodal feedback for delays of more than four seconds.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {775–781},
numpages = {7},
keywords = {mobility, attention, multimodal feedback, usability, mobile web},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062748,
author = {Kidawara, Yutaka and Uchiyama, Tomoyuki and Tanaka, Katsumi},
title = {An Environment for Collaborative Content Acquisition and Editing by Coordinated Ubiquitous Devices},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062748},
doi = {10.1145/1062745.1062748},
abstract = {Digital content is not only stored by servers on the Internet, but also on various embedded devices belonging to ubiquitous networks. In this paper, we propose a content processing mechanism for use in an environment enabling collaborative acquisition of embedded digital content in real-world situations. We have developed a network management device that makes it possible to acquire embedded content using coordinated ubiquitous devices. The management device actively configures a network that includes content-providing devices and browsing devices to permit sharing of various items with digital content. We also developed a Functional web mechanism for processing embedded web content in the real-world without a keyboard. This mechanism adds various functions to conventional web content. These functions are activated by messages from a Field in a content processing device. We construct a practical prototype system, which is simple enough for children to use, that we called the "Virtual Insect Catcher". Through a test with 48 children, we demonstrated that this system can be used to acquire embedded web content, retrieve related content from the Internet, and then create new web content. We will also describe the proposed mechanism and the system testing.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {782–791},
numpages = {10},
keywords = {RFID, ubiquitous computing, multiple device operating, functional web, embedded content, ubiquitous network},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062750,
author = {Wood, David and Bjelogrlic, Zavisa and Hyland, Bernadette and Hendler, Jim and Masahide, Kanzaki},
title = {Can Semantic Web Be Made to Flourish?},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062750},
doi = {10.1145/1062745.1062750},
abstract = {This panel's objective will be to discuss whether the Semantic Web can be made to grow in a "viral" manner, like the World Wide Web did in the early 1990s. The scope of the discussion will include efforts by the World Wide Web Consortium's Semantic Web Best Practices &amp; Deployment Working Group to identify and publish best practices of Semantic Web practitioners, and the barriers to adoption of those practices by a wider community. The concept of "best practices" as it applies to a distributed, diverse and partially-defined Semantic Web will be discussed and its relevance debated. Specifically, panelists will discuss the capability of standards bodies, commercial companies and early adopters to create a viral technology.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {792},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062751,
author = {Broder, Andrei Z. and Maarek, Yoelle S. and Bharat, Krishna and Dumais, Susan and Papa, Steve and Pedersen, Jan and Raghavan, Prabhakar},
title = {Current Trends in the Integration of Searching and Browsing},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062751},
doi = {10.1145/1062745.1062751},
abstract = {Searching and browsing are the two basic information discovery paradigms, since the early days of the Web. After more than ten years down the road, three schools seem to have emerged: (1) The search-centric school argues that guided navigation is superfluous since free form search has become so good and the search UI so common, that users can satisfy all their needs via simple queries (2) The taxonomy navigation school claims that users have difficulties expressing informational needs and (3) The meta-data centric school advocates the use of meta-data for narrowing large sets of results, and is successful in e-commerce where it is known as "multi faceted search". This panel brings together experts and advocates for all three schools, who will discuss these approaches and share their experiences in the field. We will ask the audience to challenge our experts with real information architecture problems.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {793},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062752,
author = {Rabinovich, Michael and Pacifici, Giovanni and Colajanni, Michele and Ramamritham, Krithi and Maggs, Bruce},
title = {Do We Need More Web Performance Research?},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062752},
doi = {10.1145/1062745.1062752},
abstract = {This panel will discuss the future and purpose of Web performance research, concentrating on the reasons for modest success in the adoption of research results in practice. The panel will in particular examine factors that hinder technology transfer in the Web performance area, consider examples of past successes and failures in this arena, and stimulate the discussion on how to make Web performance research more relevant.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {794},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062753,
author = {Shahraray, Behzad and Ma, Wei-Ying and Zakhor, Avideh and Babaguchi, Noboru},
title = {Mobile Multimedia Services},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062753},
doi = {10.1145/1062745.1062753},
abstract = {This panel will mainly focus on the role that media processing can play in creating mobile communications, information, and entertainment services. A major premise of our discussion is that media processing techniques go beyond compression and can be employed to monitor, filter, convert, and repurpose information. Such automated techniques can serve to create personalized information and entertainment services in a cost-effective way, adapt existing content for consumption on mobile devices, and circumvent the inherent limitations of mobile devices. Some examples of the applications of media processing techniques for mobile service generation will be given.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {795},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062754,
author = {Ronchi, Alfredo M. and Thiesmeyer, Lynn and Quacchia, Antonella and Mihajes, Georges and Onoda, Katsuhiro and Makkuni, Ranjit},
title = {On Culture in a World-Wide Information Society: Toward the Knowledge Society - the Challenge},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062754},
doi = {10.1145/1062745.1062754},
abstract = {Starting from more then ten years of experience and achievements in online cultural content, the panel aims to provide a comprehensive view on controversial issues, or unsolved problems, both in the WWW and Cultural community to stimulate lively, thoughtful, and sometimes provocative discussions. Panelists will outline the relevance of digital collections of intangible heritage and endangered archives and discuss the following topics: the "global" Web vs. the preservation of "local" cultural identities, cultural diversities and their relevance in delivering web based services, preservation & future of digital memories, Web-based development and sustainability models. We expect the panelists to actively engage the audience and help them broaden their understanding of the issues. URL: <http://www.medicif.org/Events/MEDICI_events/WWW2005/default.htm.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {796},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062755,
author = {Sarukkai, Ramesh and Chakrabarthi, Soumen and Flake, Gary William and Shivakumar, Narayanan and Ansari, Asim M.},
title = {Exploiting the Dynamic Networking Effects of the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062755},
doi = {10.1145/1062745.1062755},
abstract = {This panel aims to explore the dynamic networking effects of the Web. Today, linkages on the Web are augmented with dynamic connectivities based on various monetization strategies: e.g. ads and sponsored links. Such linkages change the dynamics of user click/flow on the Web. The key focus of this panel is to debate whether/how such dynamic effects on the Web can be modeled and best exploited. How can we derive cooperative placement strategies that are optimal from a customer perspective? As the World Wide Web becomes more dynamic with fluid link placements guided by different factors, optimizing link placement in a cooperative fashion across the Web will be an integral and crucial component. URL: <http: research.yahoo.com="" workshops="" www2005="" networkingeffectsweb=""></http:>.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {797},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062756,
author = {Liu, Ling and Broder, Andrei Z. and Fensel, Dieter and Goble, Carole and Pu, Calton},
title = {Querying the Past, Present and Future: Where We Are and Where We Will Be},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062756},
doi = {10.1145/1062745.1062756},
abstract = {This panel will focus on exploring future enhancements of Web technology for active Internet-scale information delivery and dissemination. It will ask the questions of whether the current Web technology is sufficient, what can be leveraged in this endeavor, and how a combination of ideas from a variety of existing disciplines can help in meeting the new challenges of large scale information dissemination. Relevant existing technologies and research areas include: active databases, agent systems, continual queries, event Web, publish/subscribe technology, sensor and stream data management. We expect that some suggestions may be in conflict with current, well-accepted approaches.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {798},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062757,
author = {White, Bebo and Lowe, David and Gaedke, Martin and Schwabe, Daniel and Deshpande, Yogesh},
title = {Web Engineering: Technical Discipline or Social Process?},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062757},
doi = {10.1145/1062745.1062757},
abstract = {This panel aims to explore the nature of the emerging Web engineering discipline. It will attempt to strongly engage with the issue of whether Web Engineering is currently, and (more saliently) should be in the future, viewed primarily as a technical design discipline with its attention firmly on the way in which Web technologies can be leveraged in the design process, or whether it should be viewed primarily as a socio-positioned discipline which focuses on the nature of the way in which projects are managed, needs are understood and users interact.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {799},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062758,
author = {Khare, Rohit and Barr, Jeff and Baker, Mark and Bosworth, Adam and Bray, Tim and McManus, Jeffery},
title = {Web Services Considered Harmful?},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062758},
doi = {10.1145/1062745.1062758},
abstract = {It has been estimated that all of the Web Services specifications and proposals ("WS-*") weigh in at several thousand pages by now. At the same time, their predecessor technologies such as XML-RPC have developed alongside other "grassroots" technologies like RSS. This debate has arguably even risen to the architectural level, contrasting "service-oriented architectures" with REST-based architectural styles. Unfortunately, the multiple overlapping specifications, standards bodies, and vendor strategies tend to obscure the very real successes of providing machine-automatable services over the Web today. This panel asks: Are current community processes for developing, debating, and adopting Web Services are helping or hindering the adoption of Web Services technology? URL: <http://labs.commerce.net/wiki/images/1/19/CN-TR-04-05.pdf.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {800},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062760,
author = {Ferragina, Paolo and Gulli, Antonio},
title = {A Personalized Search Engine Based on Web-Snippet Hierarchical Clustering},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062760},
doi = {10.1145/1062745.1062760},
abstract = {In this paper we propose a hierarchical clustering engine, called snaket, that is able to organize on-the-fly the search results drawn from 16 commodity search engines into a hierarchy of labeled folders. The hierarchy offers a complementary view to the flat-ranked list of results returned by current search engines. Users can navigate through the hierarchy driven by their search needs. This is especially useful for informative, polysemous and poor queries.SnakeT is the first complete and open-source system in the literature that offers both hierarchical clustering and folder labeling with variable-length sentences. We extensively test SnakeT against all available web-snippet clustering engines, and show that it achieves efficiency and efficacy performance close to the best known engine Vivisimo.com.Recently, personalized search engines have been introduced with the aim of improving search results by focusing on the users, rather than on their submitted queries. We show how to plug SnakeT on top of any (un-personalized) search engine in order to obtain a form of personalization that is fully adaptive, privacy preserving, scalable, and non intrusive for underlying search engines.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {801–810},
numpages = {10},
keywords = {new search applications and interfaces, web snippets clustering, search engines, personalized web ranking, information extraction},
location = {Chiba, Japan},
series = {WWW '05}
}

@dataset{10.1145/review-1062745.1062760_R39645,
author = {Duben, Anthony Joseph},
title = {Review ID:R39645 for DOI: 10.1145/1062745.1062760},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1062745.1062760_R39645}
}

@inproceedings{10.1145/1062745.1062761,
author = {Xu, Jun and Cao, Yunbo and Li, Hang and Zhao, Min},
title = {Ranking Definitions with Supervised Learning Methods},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062761},
doi = {10.1145/1062745.1062761},
abstract = {This paper is concerned with the problem of definition search. Specifically, given a term, we are to retrieve definitional excerpts of the term and rank the extracted excerpts according to their likelihood of being good definitions. This is in contrast to the traditional approaches of either generating a single combined definition or simply outputting all retrieved definitions. Definition ranking is essential for the task. Methods for performing definition ranking are proposed in this paper, which formalize the problem as either classification or ordinal regression. A specification for judging the goodness of a definition is given. We employ SVM as the classification model and Ranking SVM as the ordinal regression model respectively, such that they rank definition candidates according to their likelihood of being good definitions. Features for constructing the SVM and Ranking SVM models are defined. An enterprise search system based on this method has been developed and has been put into practical use. Experimental results indicate that the use of SVM and Ranking SVM can significantly outperform the baseline methods of using heuristic rules or employing the conventional information retrieval method of Okapi. This is true both when the answers are paragraphs and when they are sentences. Experimental results also show that SVM or Ranking SVM models trained in one domain can be adapted to another domain, indicating that generic models for definition ranking can be constructed.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {811–819},
numpages = {9},
keywords = {search of definitions, web search, ordinal regression, text mining, classification, web mining},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062762,
author = {Wu, Baoning and Davison, Brian D.},
title = {Identifying Link Farm Spam Pages},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062762},
doi = {10.1145/1062745.1062762},
abstract = {With the increasing importance of search in guiding today's web traffic, more and more effort has been spent to create search engine spam. Since link analysis is one of the most important factors in current commercial search engines' ranking systems, new kinds of spam aiming at links have appeared. Building link farms is one technique that can deteriorate link-based ranking algorithms. In this paper, we present algorithms for detecting these link farms automatically by first generating a seed set based on the common link set between incoming and outgoing links of Web pages and then expanding it. Links between identified pages are re-weighted, providing a modified web graph to use in ranking page importance. Experimental results show that we can identify most link farm spam pages and the final ranking results are improved for almost all tested queries.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {820–829},
numpages = {10},
keywords = {HITS, link analysis, spam, web search engine, PageRank},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062763,
author = {Gibson, David and Punera, Kunal and Tomkins, Andrew},
title = {The Volume and Evolution of Web Page Templates},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062763},
doi = {10.1145/1062745.1062763},
abstract = {Web pages contain a combination of unique content and template material, which is present across multiple pages and used primarily for formatting, navigation, and branding. We study the nature, evolution, and prevalence of these templates on the web. As part of this work, we develop new randomized algorithms for template extraction that perform approximately twenty times faster than existing approaches with similar quality. Our results show that 40--50% of the content on the web is template content. Over the last eight years, the fraction of template content has doubled, and the growth shows no sign of abating. Text, links, and total HTML bytes within templates are all growing as a fraction of total content at a rate of between 6 and 8% per year. We discuss the deleterious implications of this growth for information retrieval and ranking, classification, and link analysis.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {830–839},
numpages = {10},
keywords = {web mining, templates, data mining, data cleaning, boilerplate, algorithms},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062765,
author = {Ntoulas, Alexandros and Chao, Gerald and Cho, Junghoo},
title = {The Infocious Web Search Engine: Improving Web Searching through Linguistic Analysis},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062765},
doi = {10.1145/1062745.1062765},
abstract = {In this paper we present the Infocious Web search engine [23]. Our goal in creating Infocious is to improve the way people find information on the Web by resolving ambiguities present in natural language text. This is achieved by performing linguistic analysis on the content of the Web pages we index, which is a departure from existing Web search engines that return results mainly based on keyword matching. This additional step of linguistic processing gives Infocious two main advantages. First, Infocious gains a deeper understanding of the content of Web pages so it can better match users' queries with indexed documents and therefore can improve relevancy of the returned results. Second, based on its linguistic processing, Infocious can organize and present the results to the user in more intuitive ways. In this paper we present the linguistic processing technologies that we incorporated in Infocious and how they are applied in helping users find information on the Web more efficiently. We discuss the various components in the architecture of Infocious and how each of them benefits from the added linguistic processing. Finally, we experimentally evaluate the performance of a component which leverages linguistic information in order to categorize Web pages.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {840–849},
numpages = {10},
keywords = {information retrieval, part-of-speech tagging, phrase identification, language analysis, web search engine, concept extraction, web searching, crawling, linguistic analysis of web text, word sense disambiguation, indexing, natural language processing},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062766,
author = {Huy, Hoang Pham and Kawamura, Takahiro and Hasegawa, Tetsuo},
title = {How to Make Web Sites Talk Together: Web Service Solution},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062766},
doi = {10.1145/1062745.1062766},
abstract = {Integrating web sites to provide more efficient services is a very promising way in the Internet. For example searching house for rent based on train system or preparing a holiday with several constrains such as hotel, air ticket, etc... From resource view point, current web sites in the Internet already provide quite enough information. However, the challenge is these web sites just provide information but do not support any mechanism to exchange them. As a consequence, it is very often that a human user has to take the role to "link" several web sites by browsing each one and get the concrete information. The reason comes from a historical objective. Web sites were developed for human users browsing and so, they do not support any machine-understandable mechanism.Current researches in WWW environment already propose several solutions to make newly web sites become understandable to other web sites so that they can be integrated. However, the question is how to integrate existing web sites to these new one. Evidently, redeveloping all of them is an unacceptable solution. In this paper, we propose a solution of Web Service Gateway to "wrap" existing web sites in Web services. Thus, without any efforts to duplicate the Web sites code, these services inherit all features from the sites while can be enriched with other Web service features like UDDI publishing, semantic describing, etc. This proposal was developed in Toshiba with Web Service Gateway and Wrapper Generator System. By using these systems, several integrated-applications were built and they are also presented and evaluated in this paper.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {850–855},
numpages = {6},
keywords = {service development, wrapper, web site, WSDL, web service},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062767,
author = {Hayashi, Koichi and Koguro, Naoki and Murakami, Reki},
title = {Diversified SCM Standard for the Japanese Retail Industry},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062767},
doi = {10.1145/1062745.1062767},
abstract = {In this paper, we present the concept of a diversified SCM (supply chain management) standard and distributed hub architecture which were used in B2B experiments for the Japanese retail industry. The conventional concept of B2B standards develops a single ideal set of business transactions to be supported. In contrast, our concept allows a wide range of diverse business transaction patterns necessary for industry supply chains. An industry develops a standard SCM model that partitions the whole supply chain into several transaction segments, each of which provides alternative business transaction patterns. For B2B collaboration, companies must agree on a collaboration configuration, which chooses the transaction alternatives from each segment. To support the development of a B2B system that executes an agreed collaboration, we introduce an SOA (service oriented architecture) based pattern called a distributed hub architecture. As a hub of B2B collaboration, it includes a complete set of services that can process every possible business transaction included in a standard SCM model. However, it does not function as a centralized service that coordinates participants. Instead, it is deployed on every participant and executes the assigned part of the supply chain collaboratively with other distributed hubs. Based on this concept, we analyzed actual business transactions in the Japanese retail industry and developed a standard SCM model, which represents more than a thousand possible transaction patterns. Based on the model, we developed an experimental system for the Japanese retail industry. The demonstration experiment involved major players in the industry including one of the largest general merchandise stores, one of the largest wholesalers, and major manufacturers in Japan.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {856–863},
numpages = {8},
keywords = {SOA (service oriented architecture), supply chain management, web services, business process management, ebXML, B2B collaboration, retail industry, standardization},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062768,
author = {Baeza-Yates, Ricardo and Castillo, Carlos and Marin, Mauricio and Rodriguez, Andrea},
title = {Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062768},
doi = {10.1145/1062745.1062768},
abstract = {This article compares several page ordering strategies for Web crawling under several metrics. The objective of these strategies is to download the most "important" pages "early" during the crawl. As the coverage of modern search engines is small compared to the size of the Web, and it is impossible to index all of the Web for both theoretical and practical reasons, it is relevant to index at least the most important pages.We use data from actual Web pages to build Web graphs and execute a crawler simulator on those graphs. As the Web is very dynamic, crawling simulation is the only way to ensure that all the strategies considered are compared under the same conditions. We propose several page ordering strategies that are more efficient than breadth- first search and strategies based on partial Pagerank calculations.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {864–872},
numpages = {9},
keywords = {web page importance, scheduling policy, web crawler},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062770,
author = {Pedersen, Jan O.},
title = {Internet Search Engines: Past and Future},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062770},
doi = {10.1145/1062745.1062770},
abstract = {I will review the short history of Internet Search Engines from early first generation systems to the current crop of stock market darlings. Many of the underlying technology problems remain the same, but the business has become significantly more sophisticated and high-powered. I will touch on some of the economics driving the remarkable success of these services and make some predictions about future trends.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {873},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062771,
author = {Bharat, Krishna},
title = {News in the Age of the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062771},
doi = {10.1145/1062745.1062771},
abstract = {One of the most exciting and successful examples of the Web impacting society is online news. The history of the news industry from print to the online medium is an interesting journey. Broadcast news transformed society by making news available instantly rather than once a day. While more channels became available, barriers to entry remained high and mainstream opinions continued to dominate. News on the net has brought in a number of valuable transformations, allowing news to be made (potentially) more accessible, diverse, democratic, personalized and interactive than before. Blogging has now made "citizen reporting" possible. As with any disruptive technology online news has both positive and negative implications, such as the threat of disinformation. Computer assisted news is a fun area of research that draws upon prior work in information retrieval, data mining and user interfaces. Given the volume of online news being generated today, the ability to find news and related facts quickly and with high relevance affects both readers and journalists. The talk will address the social implications as well as the technical challenges in the dissemination of online news, with a focus on Google News. Google News is an automated service that makes over 4,500 online, English sources searchable and browseable in real time, with an emphasis on breadth of coverage.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {874},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062772,
author = {Tomkins, Andrew},
title = {Technical Challenges in Exploiting the Web as a Business Resource},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062772},
doi = {10.1145/1062745.1062772},
abstract = {In this talk, I'll describe some recent indicators suggesting that businesses are on the cusp of operational exploitation of the web as a decision support resource. From consumer research and purchasing behavior to enterprise brand tracking, intelligence gathering, and advertising, the web is suddenly on everybody's mind - not as an exciting future possibility, but as an exploitable resource. I'll describe some technological approaches to employing this resource, talk about what's possible today, and describe some challenges for the future. As a running example, I'll cover IBM's WebFountain system: its architecture, analytical model, and applications.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {875},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062773,
author = {Tsujimura, Kiyoyuki},
title = {DoCoMo's Challenge towards New Mobile Services},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062773},
doi = {10.1145/1062745.1062773},
abstract = {NTT DoCoMo, the provider of "i-mode" mobile Internet service, which accommodates over 40 million subscribers in Japan, is now working to create new types of mobile communications services featuring visual content and contactless IC technology.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {876},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062774,
author = {Vandelle, Gilles},
title = {Automatic Text Processing to Enhance Product Search for On-Line Shopping},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062774},
doi = {10.1145/1062745.1062774},
abstract = {The growing eCommerce business requires an advanced way of searching for products. Buyers today are not only using the web to accomplish transactions but also to search for and select products that fit their needs. The products are now global but the users want a site that uses their language when shopping. This talk will describe how Kelkoo built a solution used across. The multiple European languages have been addressed with a simple linguistic approach combined with machine learning technologies. In this talk we will put the emphasis on the use of machine learning to address local diversity.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {877},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062776,
author = {Dom, Byron and Bharat, Krishna and Broder, Andrei and Najork, Marc and Pedersen, Jan and Tonomura, Yoshinobu},
title = {How Search Engines Shape the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062776},
doi = {10.1145/1062745.1062776},
abstract = {The state of the web today has been and continues to be greatly influenced by the existence of web-search engines. This panel will discuss the ways in which search engines have affected the web in the past and ways in which they may affect it in the future. Both positive and negative effects will be discussed as will potential measures to combat the latter. Besides the obvious ways in which search engines help people find content, other effects to be discussed include: the whole phenomenon of web-page spam, based on both text and link (e.g. link farms), the business of "Search Engine Optimization" (optimizing pages to rank highly in web-search results), the bided-terms business and the associated problem of click fraud, to name a few.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {879},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062778,
author = {Gulli, A.},
title = {The Anatomy of a News Search Engine},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062778},
doi = {10.1145/1062745.1062778},
abstract = {Today, news browsing and searching is one of the most important Internet activity. This paper introduces a general framework to build a News search engine by describing Velthune, an academic News search engine available on line.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {880–881},
numpages = {2},
keywords = {information, extraction, syndication, news search engines},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062779,
author = {Zhuge, Hai and Chen, Xue and Sun, Xiaoping},
title = {Preferential Walk: Towards Efficient and Scalable Search in Unstructured Peer-to-Peer Networks},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062779},
doi = {10.1145/1062745.1062779},
abstract = {To improve search efficiency and reduce unnecessary traffic in Peer-to-Peer (P2P) networks, this paper proposes a trust-based probabilistic search algorithm, called preferential walk (P-Walk). Every peer ranks its neighbors according to searching experience. The highly ranked neighbors have higher probabilities to be queried. Simulation results show that P-Walk is not only efficient, but also robust against malicious behaviors. Furthermore, we measure peers' rank distribution and draw implications.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {882–883},
numpages = {2},
keywords = {power-law, trust, search, P2P, probability},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062780,
author = {Cacciaguerra, Stefano and Ferretti, Stefano and Roccetti, Marco and Roffilli, Matteo},
title = {Car Racing through the Streets of the Web: A High-Speed 3D Game over a Fast Synchronization Service},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062780},
doi = {10.1145/1062745.1062780},
abstract = {The growth of the Internet brought a new age for game developers. New exciting, highly interactive Massively Multiplayer Online Games (MMOGs) may be now deployed on the Web, thanks to new scalable distributed solutions and amazing 3D graphics systems plugged directly into standard browsers. Along this line, taking advantage of a mirrored game server architecture, we developed a 3D car racing multiplayer game for use over the Web, freely inspired to Armagetron. Game servers are kept synchronized through the use of a fast synchronization scheme which is able to drop obsolete game events to uphold the playability degree while preserving the game state consistency. Preliminary results confirm that smart 3D spaces may be created over the Web where the magic of gaming is reproduced for the pleasure of a huge number of players. This result may be obtained only by converging highly accurate event synchronization technologies with 3D scene graph based rendering software.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {884–885},
numpages = {2},
keywords = {synchronization, scene graph, MMOG},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062781,
author = {Virmani, Ashish and Agarwal, Suchit and Thathoo, Rahul and Suman, Shekhar and Sanyal, Sudip},
title = {A Fast XPATH Evaluation Technique with the Facility of Updates},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062781},
doi = {10.1145/1062745.1062781},
abstract = {This paper addresses the problem of fast retrieval of data from XML documents by providing a labeling schema that can easily handle simple as well as complex XPATH queries and also provide for updates without the need for the entire document being re-indexed in the RDBMS. We introduce a new labeling schema called the "Z-Label" for efficiently processing XPATH queries involving child and descendant axes.The use of "Z-Label" coupled with the indexing schema provides for smooth updates in the XML document.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {886–887},
numpages = {2},
keywords = {XML, Dewey indexing, updates, biaxes path expression, XPath query optimization},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062782,
author = {Anand, Sai and Wilde, Erik},
title = {Mapping XML Instances},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062782},
doi = {10.1145/1062745.1062782},
abstract = {For XML-based applications in general and B2B applications in particular, mapping between differently structured XML documents, to enable exchange of data, is a basic problem. A generic solution to the problem is of interest and desirable both in an academic and practical sense. We present a case study of the problem that arises in an XML based project, which involves mapping of different XML schemas to each other. We describe our approach to solving the problem, its advantages and limitations. We also compare and contrast our approach with previously known approaches and commercially available software solutions.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {888–889},
numpages = {2},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062783,
author = {Sarukkai, Ramesh R.},
title = {How Much is a Keyword Worth?},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062783},
doi = {10.1145/1062745.1062783},
abstract = {How much is a keyword worth? At the crux of every search is a query that is composed of search keywords. Sponsors bid for placement on such keywords using a variety of factors, the key being the relative demand for the keyword, and its ability to drive customers to their site. In this paper, we explore the notion of "worth of a keyword". We determine the keyword's worth by tying it to the end criteria that needs to be maximized. As an illustrative example, keyword searches that drive e-commerce transactions are modeled and methods for estimating the Return On Investment/value of a keyword from the association data is discussed.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {890–891},
numpages = {2},
keywords = {e-commerce, sponsored listing, search keyword valuation, ROI, sponsored keyword recommendation, optimization},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062784,
author = {Gwizdka, Jacek and Spence, Ian},
title = {Predicting Outcomes of Web Navigation},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062784},
doi = {10.1145/1062745.1062784},
abstract = {Two exploratory studies examined the relationships among web navigation metrics, measures of lostness, and success on web navigation tasks. The web metrics were based on counts of visits to web pages, properties of the web usage graph, and similarity to an optimal path. Metrics based on similarity to an optimal path were good predictors of lostness and task success.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {892–893},
numpages = {2},
keywords = {web navigation, lostness, compactness, stratum, path similarity},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062785,
author = {Zhang, Sheng and Zhang, Ji and Liu, Han and Wang, Wei},
title = {XAR-Miner: Efficient Association Rules Mining for XML Data},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062785},
doi = {10.1145/1062745.1062785},
abstract = {In this paper, we propose a framework, called XAR-Miner, for mining ARs from XML documents efficiently. In XAR-Miner, raw data in the XML document are first preprocessed to transform to either an Indexed Content Tree (IX-tree) or Multi-relational databases (Multi-DB), depending on the size of XML document and memory constraint of the system, for efficient data selection and AR mining. Task-relevant concepts are generalized to produce generalized meta-patterns, based on which the large ARs that meet the support and confidence levels are generated.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {894–895},
numpages = {2},
keywords = {association rule mining, meta-patterns, XML data},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062786,
author = {Zhang, Ji and Wang, Wei and Liu, Han and Zhang, Sheng},
title = {X-Warehouse: Building Query Pattern-Driven Data},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062786},
doi = {10.1145/1062745.1062786},
abstract = {In this paper, we propose an approach to materialize XML data warehouses based on the frequent query patterns discovered from historical queries issued by users. The schemas of integrated XML documents in the warehouse are built using these frequent query patterns represented as Frequent Query Pattern Trees (FreqQPTs). Using hierarchical clustering technique, FreqQPTs are clustered and merged to produce a specified number of integrated XML documents for actual data feeding. Maintenance issue of the data warehouse is also treated in this paper.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {896–897},
numpages = {2},
keywords = {XML data, data integration, data warehouse, query patterns},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062787,
author = {Boldi, Paolo},
title = {TotalRank: Ranking without Damping},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062787},
doi = {10.1145/1062745.1062787},
abstract = {PageRank is defined as the stationary state of a Markov chain obtained by perturbing the transition matrix of a web graph with a damping factor α that spreads part of the rank. The choice of α is eminently empirical, but most applications use α = 0.85; nonetheless, the selection of α is critical, and some believe that link farms may use this choice adversarially. Recent results [1] prove that the PageRank of a page is a rational function of α, and that this function can be approximated quite efficiently: this fact can be used to define a new form of ranking, TotalRank, that averages PageRanks over all possible α's. We show how this rank can be computed efficiently, and provide some preliminary experimental results on its quality and comparisons with PageRank.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {898–899},
numpages = {2},
keywords = {ranking, Kendall's τ, pageRank, link farms},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062788,
author = {Waniek, Jacqueline and Langner, Holger and Schmidsberger, Falk},
title = {MemoSpace: A Visualization Tool for Web Navigation},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062788},
doi = {10.1145/1062745.1062788},
abstract = {A central aspect of reducing orientation problems in web navigation concerns the design of adequate navigation aids. Visualization of users' navigation path in form of a temporal-spatial template can function as external memory of users' search history, thereby supporting the user to find previously visited sites, getting an overview of the search process and moreover, provide structure for the complex WorldWideWeb (WWW) environment. This paper presents an application for dynamic 2 and 3 dimensional visualizations of users' navigation paths, called MemoSpace. In an explorative study, users behavior and subjective evaluation of a MemoSpace application was examined.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {900–901},
numpages = {2},
keywords = {MemoSpace, navigation},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062789,
author = {Gulli, A. and Signorini, A.},
title = {The Indexable Web is More than 11.5 Billion Pages},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062789},
doi = {10.1145/1062745.1062789},
abstract = {In this short paper we estimate the size of the public indexable web at 11.5 billion pages. We also estimate the overlap and the index size of Google, MSN, Ask/Teoma and Yahoo!},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {902–903},
numpages = {2},
keywords = {index sizes, size of the web, search engines},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062790,
author = {V\'{a}zquez, Juan Ignacio and de Ipi\'{n}a, Diego L\'{o}pez},
title = {A Language for Expressing User-Context Preferences in the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062790},
doi = {10.1145/1062745.1062790},
abstract = {In this paper, we introduce WPML (WebProfiles Markup Language) for expressing user-context preferences information in the Web. Using WPML a service provider can negotiate and obtain user-related information to personalise service experience without explicit manual configuration by the user, while preserving his privacy using P3P.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {904–905},
numpages = {2},
keywords = {cookies, state management, web, context-aware, profiles, ambient intelligence, HTTP},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062791,
author = {Yang, Christopher C. and Chan, K. Y.},
title = {Retrieving Multimedia Web Objects Based on PageRank Algorithm},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062791},
doi = {10.1145/1062745.1062791},
abstract = {Hyperlink analysis has been widely investigated to support the retrieval of Web documents in Internet search engines. It has been proven that the hyperlink analysis significantly improves the relevance of the search results and these techniques have been adopted in many commercial search engines, e.g. Google. However, hyperlink analysis is mostly utilized in the ranking mechanism of Web pages only but not including other multimedia objects, such as images and video. In this project, we propose a modified Multimedia PageRank algorithm to support the searching of multimedia objects in the Web.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {906–907},
numpages = {2},
keywords = {HITS, content based retrieval, PageRank, multimedia retrieval, hyperlink analysis, web search engines},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062792,
author = {Azzag, Hanene and Venturini, Gilles and Guinot, Christiane},
title = {Automatic Generation of Web Portals Using Artificial Ants},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062792},
doi = {10.1145/1062745.1062792},
abstract = {We present in this work a new model (named AntTree) based on artificial ants for document hierarchical clustering. This model is inspired from the self-assembly behavior of real ants. We have simulated this behavior to build a hierarchical tree-structured partitioning of a set of documents, according to the similarities between these documents. We have successfully compared our results to those obtained by ascending hierarchical clustering.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {908–909},
numpages = {2},
keywords = {portals sites, artificial ants, hierarchical clustering, web},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062793,
author = {Bryan-Kinns, N. and Healey, P. G. T. and Lee, J.},
title = {Persistence in Web Based Collaborations},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062793},
doi = {10.1145/1062745.1062793},
abstract = {We outline work on web based support for group creativity. We focus on a study of the effect persistence of participants' musical contributions has on their mutual engagement.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {910–911},
numpages = {2},
keywords = {collaboration, HCI, music, creativity, user interfaces},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062794,
author = {Avramouli, D. and Garofalakis, J. and Kavvadias, D. J. and Makris, C. and Panagis, Y. and Sakkopoulos, E.},
title = {Popular Web Hot Spots Identification and Visualization},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062794},
doi = {10.1145/1062745.1062794},
abstract = {This work aims a two-fold contribution: it presents a software to analyse logfiles and visualize popular web hot spots and, additionally, presents an algorithm to use this information in order to identify subsets of the website that display large access patterns. Such information is extremely valuable to the site maintainer, since it indicates points that may need content intervention or/and site graph restructuring. Experimental validation verified that the visualization tool, when coupled with algorithms that infer frequent traversal patterns, is both effective in indicating popular hot spots and efficient in doing so by using graph-based representations of popular traversals.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {912–913},
numpages = {2},
keywords = {access visualization, usage mining, maximal forward path},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062795,
author = {Salvetti, Franco and Srinivasan, Savitha},
title = {Information Flow Using Edge Stress Factor},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062795},
doi = {10.1145/1062745.1062795},
abstract = {This paper shows how a corpus of instant messages can be employed to detect de facto communities of practice automatically. A novel algorithm based on the concept of Edge Stress Factor is proposed and validated. Results show that this approach is fast and effective in studying collaborative behavior.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {914–915},
numpages = {2},
keywords = {social network analysis, graph clustering},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062796,
author = {Esfandiari, Babak and Nock, Richard},
title = {Adaptive Filtering of Advertisements on Web Pages},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062796},
doi = {10.1145/1062745.1062796},
abstract = {We present a browser extension to dynamically learn to filter unwanted images (such as advertisements or flashy graphics) based on minimal user feedback. To do so, we apply the weighted majority algorithm using pieces of the Uniform Resource Locators of such images as predictors. Experimental results tend to confirm that the accuracy of the predictions converges quickly to very high levels.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {916–917},
numpages = {2},
keywords = {weighted majority, interface agents, advertisement filtering},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062797,
author = {Habib, Sami and Safar, Maytham},
title = {WEBCAP: A Capacity Planning Tool for Web Resource Management},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062797},
doi = {10.1145/1062745.1062797},
abstract = {A staggering number of multimedia applications are being introduced every day. Yet, the inordinate delays encountered in retrieving multimedia documents make it difficult to use the Web for real-time applications such as educational broadcasting, video conferencing, and multimedia streaming. The problem of delivering multimedia documents in time while placing the least demands on the client, network and server resources is a challenging optimization problem. The WEBCAP is ongoing project that explores applying capacity planning techniques to manage or tune the Web resources (client, network, server) for optimal or near optimal performance, subject to minimizing the retrieval cost while satisfying the real-time constraints and available resources. The WEBCAP project consists of four software modules: object extractor, object representer, object scheduler, and system tuner. The four modules are connected serially with 3 feedback-loops. In this paper, we focus on how to extract objects from multimedia document and how to represent them as object and operation flow graphs while maintaining precedence relations among the objects.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {918–919},
numpages = {2},
keywords = {capacity-planning, multimedia, scheduling},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062798,
author = {Li, Kin F. and Yu, Wei and Nishio, Shojiro and Wang, Yali},
title = {Finding the Search Engine That Works for You},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062798},
doi = {10.1145/1062745.1062798},
abstract = {A search engine evaluation model that considers over seventy performance and feature parameters is presented. The design of a web-based system that allows the user to tailor the model to his/her own preference, and to evaluate search engines of interest, is introduced. The results presented to the user identify the most suitable search engine that suits his/her needs.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {920–921},
numpages = {2},
keywords = {personalization, search engines, performance evaluation},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062799,
author = {Wong, Wan Yeung and Lau, Tak Pang and King, Irwin},
title = {Information Retrieval in P2P Networks Using Genetic Algorithm},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062799},
doi = {10.1145/1062745.1062799},
abstract = {Hybrid Peer-to-Peer (P2P) networks based on the direct connection model have two shortcomings which are high bandwidth consumption and poor semi-parallel search. However, they can further be improved by the query propagation model. In this paper, we propose a novel query routing strategy called GAroute based on the query propagation model. By giving the current P2P network topology and relevance level of each peer, GAroute returns a list of query routing paths that cover as many relevant peers as possible. We model this as the Longest Path Problem in a directed graph which is NP-complete and we obtain high quality (0.95 in 100 peers) approximate solutions in polynomial time by using Genetic Algorithm (GA). We describe the problem modeling and proposed GA for finding long paths. Finally, we summarize the experimental results which measure the scalability and quality of different searching algorithms. According to these results, GAroute works well in some large scaled P2P networks.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {922–923},
numpages = {2},
keywords = {genetic algorithm, query routing, longest path problem, P2P},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062800,
author = {Rajapakse, Damith C. and Jarzabek, Stan},
title = {An Investigation of Cloning in Web Applications},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062800},
doi = {10.1145/1062745.1062800},
abstract = {Cloning (ad hoc reuse by duplication of design or code) speeds up development, but also hinders future maintenance. Cloning also hints at reuse opportunities that, if exploited systematically, might have positive impact on development and maintenance productivity. Unstable requirements and tight schedules pose unique challenges for Web Application engineering that encourage cloning. We are conducting a systematic study of cloning in Web Applications of different sizes, developed using a range of Web technologies, and serving diverse purposes. Our initial results show cloning rates up to 63% in both newly developed and already maintained Web Applications. Expected contribution of this work is two-fold: (1) to confirm potential benefits of reuse-based methods in addressing clone related problems of Web engineering, and (2) to create a framework of metrics and presentation views to be used in other similar studies.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {924–925},
numpages = {2},
keywords = {software reuse, clone metrics, web applications, clones, web engineering, software maintenance, clone analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062801,
author = {Yuan, Junli and Chi, Hung and Sun, Qibin},
title = {A More Precise Model for Web Retrieval},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062801},
doi = {10.1145/1062745.1062801},
abstract = {Most research works on web retrieval latency are object-level based, which we think is insufficient and sometimes inaccurate. In this paper, we propose a fine grained operation-level Web Retrieval Dependency Model (WRDM) to provide more precise capture of web retrieval process. Our model reveals some new factors in web retrieval which cannot be seen at object level but are very important to studies in the web retrieval area.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {926–927},
numpages = {2},
keywords = {model, performance, dependency, latency, web retrieval},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062802,
author = {Mehta, Rupesh R. and Mitra, Pabitra and Karnick, Harish},
title = {Extracting Semantic Structure of Web Documents Using Content and Visual Information},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062802},
doi = {10.1145/1062745.1062802},
abstract = {This work aims to provide a page segmentation algorithm which uses both visual and content information to extract the semantic structure of a web page. The visual information is utilized using the VIPS algorithm and the content information using a pre-trained Naive Bayes classifier. The output of the algorithm is a semantic structure tree whose leaves represent segments having unique topic. However contents of the leaf segments may possibly be physically distributed in the web page. This structure can be useful in many web applications like information retrieval, information extraction and automatic web page adaptation. This algorithm is expected to outperform other existing page segmentation algorithms since it utilizes both content and visual information.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {928–929},
numpages = {2},
keywords = {DOM, VIPS, naive bayes classifier, page segmentation, topic hierarchy},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062803,
author = {Kelly, Brian and Vidgen, Richard},
title = {A Quality Framework for Web Site Quality: User Satisfaction and Quality Assurance},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062803},
doi = {10.1145/1062745.1062803},
abstract = {Web site developers need to use of standards and best practices to ensure that Web sites are functional, accessible and interoperable. However many Web sites fail to achieve such goals. This short paper describes how a Web site quality assessment method (E-Qual) might be used in conjunction with a quality assurance framework (QA Focus) to provide a rounded view of Web site quality that takes account of end user and developer perspectives.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {930–931},
numpages = {2},
keywords = {best practices, web site quality, standards, quality assurance},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062804,
author = {Soro, Alessandro and Marcialis, Ivan and Carboni, Davide and Paddeu, Gavino},
title = {WebRogue: Virtual Presence in Web Sites},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062804},
doi = {10.1145/1062745.1062804},
abstract = {WebRogue is an application for virtual presence over the Web. It provides the Web Browser with a chat subwindow that allows users connected to the same Web site to meet, share opinions and cooperate in a totally free, non moderated and uncensored environment. Each time the user loads a Web page in the Web Browser, WebRogue opens a discussion channel in a centralized server application, that is completely decoupled from the Web server, using the URL of the Web site as a key. Thus whenever a new page is loaded the user can see who is connected, as if entering a physical site. Interactivity is supported by means of two type of commands: comunication commands allow synchronous interaction as with chat or instant messaging software; Social commands allow cooperation: group surfing, exchange of visit-cards and wait in line.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {932–933},
numpages = {2},
keywords = {virtual presence, web, chat, web communities},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062805,
author = {Kouroupas, George and Koutsoupias, Elias and Papadimitriou, Christos H. and Sideri, Martha},
title = {An Economic Model of the Worldwide Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062805},
doi = {10.1145/1062745.1062805},
abstract = {We believe that much novel insight into the worldwide web can be obtained from taking into account the important fact that it is created, used, and run by selfish optimizing agents: users, document authors, and search engines. On-going theoretical and experimental analysis of a simple abstract model of www creation and search based on user utilities illustrates this point: We find that efficiency is higher when the utilities are more clustered, and that power-law statistics of document degrees emerge very naturally in this context. More importantly, our work sets up many more elaborate questions, related, e.g., to www search algorithms seen as author incentives, to search engine spam, and to search engine quality and competition.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {934–935},
numpages = {2},
keywords = {power laws, utility function, market, game theory, price of anarchy, web search, economic model},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062806,
author = {Scarselli, Franco and Yong, Sweah Liang and Hagenbuchner, Markus and Tsoi, Ah Chung},
title = {Adaptive Page Ranking with Neural Networks},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062806},
doi = {10.1145/1062745.1062806},
abstract = {Recent developments in the area of neural networks provided new models which are capable of processing general types of graph structures. Neural networks are well-known for their generalization capabilities. This paper explores the idea of applying a novel neural network model to a web graph to compute an adaptive ranking of pages. Some early experimental results indicate that the new neural network models generalize exceptionally well when trained on a relatively small number of pages.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {936–937},
numpages = {2},
keywords = {adaptive page rank, graph processing, neural networks},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062807,
author = {Chiang, Wei-Tsen Milly and Hagenbuchner, Markus and Tsoi, Ah Chung},
title = {The WT10G Dataset and the Evolution of the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062807},
doi = {10.1145/1062745.1062807},
abstract = {The purpose of this paper is threefold. First, we study the evolution of the web based on data available from an earlier snapshot of the web and compare the results with those predicted in [2]. Secondly, we establish whether the WT10G dataset, a popular benchmark for the development and evaluation of internet based applications is appropriate for the tasks. Finally, is there a need for a collection of a new dataset for such purposes. The findings are that the appropriateness of using the popular WT10G dataset in recent Internet-based experiments is questionable and that there is a need for a new collection of dataset for development and evaluation purposes of algorithms related to Internet search engine developments.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {938–939},
numpages = {2},
keywords = {standard datasets, web evolution, rate of change},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062808,
author = {Liu, Jie and Zhuge, Hai},
title = {A Semantic-Link-Based Infrastructure for Web Service Discovery in P2P Networks},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062808},
doi = {10.1145/1062745.1062808},
abstract = {An important issue arising from P2P applications is how to accurately and efficiently retrieve the required Web services from large-scale repositories. This paper resolves this issue by organizing services in the overlay combining the Semantic Service Link Network and the Chord P2P network. A service request will first be routed in the Chord according to the given service operation names and keywords. Then, the same request will be routed in the Semantic Link Network according to the service link type and semantic matching. Compared with previous P2P service discovery approaches, the proposed approach has two advantages: (1) produce more accurate and meaning results when searching for particular services in a P2P network; and (2) enable users and peers to discover services in a more flexible way.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {940–941},
numpages = {2},
keywords = {semantic link, web service, peer-to-peer},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062809,
author = {Segawa, Osamu and Kawai, Jun and Sakauchi, Kazuyuki},
title = {Automatic Generation of Link Collections and Their Visualization},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062809},
doi = {10.1145/1062745.1062809},
abstract = {In this paper, we describe a method of generating link collections in a user-specified category by comprehensively collecting existing link collections and analyzing their hyperlink references. Moreover, we propose a visualization method for a bird's-eye view of the generated link collections. Our methods are effective in grasping intuitively the trend of significant sites and keywords in a category.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {942–943},
numpages = {2},
keywords = {link collection, hyperlink analysis, visualization},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062810,
author = {Yang, Haixuan and King, Irwin and Lyu, Michael R.},
title = {Predictive Ranking: A Novel Page Ranking Approach by Estimating the Web Structure},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062810},
doi = {10.1145/1062745.1062810},
abstract = {PageRank (PR) is one of the most popular ways to rank web pages. However, as the Web continues to grow in volume, it is becoming more and more difficult to crawl all the available pages. As a result, the page ranks computed by PR are only based on a subset of the whole Web. This produces inaccurate outcome because of the inherent incomplete information (dangling pages) that exist in the calculation. To overcome this incompleteness, we propose a new variant of the PageRank algorithm called, Predictive Ranking (PreR), in which different classes of dangling pages are analyzed individually so that the link structure can be predicted more accurately. We detail our proposed steps. Furthermore, experimental results show that this algorithm achieves encouraging results when compared with previous methods.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {944–945},
numpages = {2},
keywords = {predictive ranking, PageRank, link analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062811,
author = {Miyamori, Hisashi and Tanaka, Katsumi},
title = {Webified Video: Media Conversion from TV Program to Web Content and Their Integrated Viewing Method},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062811},
doi = {10.1145/1062745.1062811},
abstract = {A method is proposed for viewing broadcast content that converts TV programs into Web content and integrates the results with related information retrieved using local and/or Internet content.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {946–947},
numpages = {2},
keywords = {topic segmentation, metadata generation, scene search, next-generation storage TV, media conversion, fusion of broadcast and web content},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062812,
author = {Miyamori, Hisashi and Nakamura, Satoshi and Tanaka, Katsumi},
title = {Personal TV Viewing by Using Live Chat as Metadata},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062812},
doi = {10.1145/1062745.1062812},
abstract = {We propose a new TV viewing method by personalizing TV programs with live chat information on the Web. It enables a new way of viewing TV content from different perspectives reflecting viewers' viewpoints.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {948–949},
numpages = {2},
keywords = {metadata generation, semantic analysis, fusion of broadcast and web content, viewer, live chat, viewpoint, digest},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062813,
author = {Nakahira, Koji and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
title = {Accuracy Enhancement of Function-Oriented Web Image Classification},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062813},
doi = {10.1145/1062745.1062813},
abstract = {We propose a function-oriented classification of web images and show new applications using this categorization. We defined nine categories of images taking into account of their functions used in web pages, and classified web images by using Support Vector Machine (SVM) in tree structured way. In order to achieve high accuracy of classification, we employed two kinds of features, image-based features and text-based features, and the two kinds can be used together or separately for the stages of the classification. We also utilized DCT coefficients to classify photo images and illustrations. As a result, accurate classification has been achieved. Finally, we show the page summarization as a new application that is made feasible for the first time by our new categories of WWW images.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {950–951},
numpages = {2},
keywords = {web images, classification, support vector machine},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062814,
author = {Frasincar, Flavius and Houben, Geert-Jan and Barna, Peter},
title = {Hera Presentation Generator},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062814},
doi = {10.1145/1062745.1062814},
abstract = {Semantic Web Information Systems (SWIS) are Web Information Systems that use Semantic Web technologies. Hera is a model-driven design methodology for SWIS. In Hera, models are represented in RDFS and model instances in RDF. The Hera Presentation Generator (HPG) is an integrated development environment that supports the presentation generation layer of the Hera methodology. The HPG is based on a pipeline of data transformations driven by different Hera models.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {952–953},
numpages = {2},
keywords = {WIS, RDF(S), design environment, SWIS, semantic web},
location = {Chiba, Japan},
series = {WWW '05}
}

@dataset{10.1145/review-1062745.1062814_R39630,
author = {Hodges, Julia E.},
title = {Review ID:R39630 for DOI: 10.1145/1062745.1062814},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1062745.1062814_R39630}
}

@inproceedings{10.1145/1062745.1062815,
author = {Sydow, Marcin},
title = {Can Link Analysis Tell Us about Web Traffic?},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062815},
doi = {10.1145/1062745.1062815},
abstract = {In this paper we measure correlation between link analysis characteristics for Web pages such as in- and out-degree, PageRank and RBS with those obtained from real Web traffic analysis. Measurements made on real data from the Polish Web show that PageRank is observably but not strongly correlated with actual visits made by Web users to Web pages and that our RBS algorithm[2] is more correlated with traffic data than PageRank in some cases.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {954–955},
numpages = {2},
keywords = {web traffic analysis, RBS, link analysis, PageRank},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062816,
author = {Tatsumi, Yushin and Asahi, Toshiyuki},
title = {Analyzing Web Page Headings Considering Various Presentation},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062816},
doi = {10.1145/1062745.1062816},
abstract = {Exploiting document structure can solve the usability problem when browsing web pages designed for PCs with non-PC terminals. For example, by exploiting headings among document structure and showing them selectively within a display, users can easily grasp a page's overview. In this paper, as a basic part of document structure analysis, we propose a heading analysis method for web pages considering various presentation. Results of evaluation experiments confirmed that our proposed method could extract many headings that could not be extracted by using HTML element names.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {956–957},
numpages = {2},
keywords = {content adaptation, web document analysis, heading analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062817,
author = {Halvey, Martin and Keane, Mark T. and Smyth, Barry},
title = {Predicting Navigation Patterns on the Mobile-Internet Using Time of the Week},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062817},
doi = {10.1145/1062745.1062817},
abstract = {A predictive analysis of user navigation in the Internet is presented that exploits time-of-the-week data. Specifically, we investigate time as an environmental factor in making predictions about user navigation. An analysis is carried out of a large sample of user, navigation data (over 3.7 million sessions from 0.5 million users) in a mobile-Internet context to determine whether user surfing patterns vary depending on the time of the week on which they occur. We find that the use of time improves the predictive accuracy of navigation models.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {958–959},
numpages = {2},
keywords = {browsing, log file analysis, navigation, WWW, user modeling, mobile-web, WAP, prediction, mobile},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062818,
author = {Su, Xue-Feng and Zeng, Hua-Jun and Chen, Zheng},
title = {Finding Group Shilling in Recommendation System},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062818},
doi = {10.1145/1062745.1062818},
abstract = {In the age of information explosion, recommendation system has been proved effective to cope with information overload in e-commerce area. However, unscrupulous producers shill the systems in many ways to make profit, and it makes the system imprecise and unreliable in a long term. Among many shilling behaviors, a new form of attack, called group shilling, appears and does great harm to the system. Because group shilling users are now well organized and become more hidden among various normal users, it is hard to find them by traditional methods. However, these group shilling users are similar to some extent, for they both shill the target items. We bring out a similarity spreading algorithm to find these group shilling users and protect recommendation system from unfair ratings. In our algorithm, we try to find these cunning group shilling users through propagating similarities from items to users iteratively. The experiment shows our similarity spreading algorithm improves the precision of the system and provides the system a reliable protection.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {960–961},
numpages = {2},
keywords = {collaborative filtering, group shilling, recommendation system},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062819,
author = {Kossmann, Donald and Reichel, Christian},
title = {SLL: Running My Web Services on Your WS Platforms},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062819},
doi = {10.1145/1062745.1062819},
abstract = {Today, the choice for a particular programming language limits the alternative products that can be used to deploy the program. For instance, a Java program must be executed using a Java VM. This limitation is particularly harmful for the emergence of a new programming paradigm like SOA and Web Services because platforms for new innovative programming languages are typically not as stable and mature as the established platforms for traditional programming paradigms. The purpose of this work is to break the strong ties between programming languages and runtime environments and thus make it possible to innovate at both ends independently. Thereby, the specific focus is on Web Services and Service-Oriented Architectures; focusing on this domain makes it possible to achieve this goal with affordable efforts. The key idea is to introduce a Service Language Layer (SLL) which gives a high-level abstraction of a service-oriented program and which can easily and efficiently be executed on alternative Web Services platforms.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {962–963},
numpages = {2},
keywords = {decoupling, web services, XML-based service language, transformation, service language layer, XML},
location = {Chiba, Japan},
series = {WWW '05}
}

@dataset{10.1145/review-1062745.1062819_R39696,
author = {Waite, William M.},
title = {Review ID:R39696 for DOI: 10.1145/1062745.1062819},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1062745.1062819_R39696}
}

@inproceedings{10.1145/1062745.1062820,
author = {Nakayama, Kotaro and Hara, Takahiro and Nishio, Shojiro},
title = {An Agent System for Ontology Sharing on WWW},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062820},
doi = {10.1145/1062745.1062820},
abstract = {Semantic Web Services (SWS), a new generation WWW technology, will facilitate the automation of Web service tasks, including automated Web service discovery, execution, composition and mediation by using XML based metadata and ontology. There have been several efforts to build knowledge representation languages for Web Services. However, only few attempts have so far been made to develop applications based on SWS. Especially, front-end agent systems for users are one of the urgent research areas. The purpose of this paper is to introduce our new integrated front-end agent system for ontology management and SWS management.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {964–965},
numpages = {2},
keywords = {semantic web, web services, ontology, agent technologies},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062821,
author = {Ito, Kimihito},
title = {Introducing Multimodal Character Agents into Existing Web Applications},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062821},
doi = {10.1145/1062745.1062821},
abstract = {This paper proposes a framework in which end-users can instantaneously modify existing Web applications by introducing multimodal user-interface. The authors use the IntelligentPad architecture and MPML as the basis of the framework. Example applications include character agents that read the latest news on a news Web site. The framework does not require users to write any program codes or scripts to introduce multimodal user-interface to existing Web applications.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {966–967},
numpages = {2},
keywords = {MPML, web application, IntelligentPad, multimodal user interface},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062822,
author = {Sugibuchi, Tsuyoshi and Tanaka, Yuzuru},
title = {Interactive Web-Wrapper Construction for Extracting Relational Information from Web Documents},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062822},
doi = {10.1145/1062745.1062822},
abstract = {In this paper, we propose a new user interface to interactively specify Web wrappers to extract relational information from Web documents. In this study, we focused on improving user's trial-and-error repetitions for constructing a wrapper. Our approach is a combination of a light-weight wrapper construction method and the dynamic previewing interface which quickly previews how generated wrapper works. We adopted a simple algorithm which can construct a Web wrapper from given extraction examples in less than 100 milliseconds. By using the algorithm, our system dynamically generates a new wrapper from a stream of user's mouse events for specifying extraction examples, and immediately updates a preview result that shows how the generated wrapper extracts HTML nodes from a source Web document. Through this animated display, a user can make a lot of wrapper construction trials with various different combinations of extraction examples by only moving a mouse on the Web document, and reach a good set of examples to obtain an intended wrapper in a short time.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {968–969},
numpages = {2},
keywords = {user interfaces, web wrappers, information extraction},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062823,
author = {Itoh, Masahiko and Tanaka, Yuzuru},
title = {Multispace Information Visualization Framework for the Intercomparison of Data Sets Retrieved from Web Services},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062823},
doi = {10.1145/1062745.1062823},
abstract = {We introduce a new visualization framework for the intercomparison of more than one data set retrieved from Web services. In our framework, we use more than one visualization space simultaneously, each of which visualizes a single data set retrieved from the Web service. For this purpose, we provide a new 3D component for accessing Web services, and provide a 3D space component, in which data set retrieved from the Web service is visualized. Moreover, our framework provides users with various operations applicable to these space components, i.e., union, intersection, set-difference, cross-product, selection, projection, and joins.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {970–971},
numpages = {2},
keywords = {IntelligentBox, WorldBottle, visualization, web service},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062824,
author = {Bencz\'{u}r, Andr\'{a}s A. and Csalog\'{a}ny, K\'{a}roly and Sarl\'{o}s, Tam\'{a}s},
title = {On the Feasibility of Low-Rank Approximation for Personalized PageRank},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062824},
doi = {10.1145/1062745.1062824},
abstract = {Personalized PageRank expresses backlink-based page quality around user-selected pages in a similar way to PageRank over the entire Web. Algorithms for computing personalized PageRank on the fly are either limited to a restricted choice of page selection or believed to behave well only on sparser regions of the Web. In this paper we show the feasibility of computing personalized PageRank by a k &lt; 1000 low-rank approximation of the Page-Rank transition matrix; by our algorithm we may compute an approximate personalized Page-Rank by multiplying an n x k, a k x n matrix and the n-dimensional personalization vector. Since low-rank approximations are accurate on dense regions, we hope that our technique will combine well with known algorithms.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {972–973},
numpages = {2},
keywords = {low-rank approximation, personalized PageRank, link analysis, web information retrieval, singular value decomposition},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062825,
author = {Yu, Haibo and Mine, Tsunenori and Amamiya, Makoto},
title = {An Architecture for Personal Semantic Web Information Retrieval System},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062825},
doi = {10.1145/1062745.1062825},
abstract = {The semantic Web and Web service technologies have provided both new possibilities and challenges to automatic information processing. There are a lot of researches on applying these new technologies into current personal Web information retrieval systems, but no research addresses the semantic issues from the whole life cycle and architecture point of view. Web services provide a new way for accessing Web resources, but until now, they have been managed separately from traditional Web contents resources. In this poster, we propose a conceptual architecture for a personal semantic Web information retrieval system. It incorporates semantic Web, Web services and multi-agent technologies to enable not only precise location of Web resources but also the automatic or semi-automatic integration of hybrid Web contents and Web services.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {974–975},
numpages = {2},
keywords = {web portal, information retrieval system, web services, semantic web},
location = {Chiba, Japan},
series = {WWW '05}
}

@dataset{10.1145/review-1062745.1062825_R40330,
author = {Dutta, Satadip},
title = {Review ID:R40330 for DOI: 10.1145/1062745.1062825},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1062745.1062825_R40330}
}

@inproceedings{10.1145/1062745.1062826,
author = {Vigna, Sebastiano},
title = {TruRank: Taking PageRank to the Limit},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062826},
doi = {10.1145/1062745.1062826},
abstract = {PageRank is defined as the stationary state of a Markov chain depending on a damping factor α that spreads uniformly part of the rank. The choice of α is eminently empirical, and in most cases the original suggestion α=0.85 by Brin and Page is still used. It is common belief that values of α closer to 1 give a "truer to the web" PageRank, but a small α accelerates convergence. Recently, however, it has been shown that when α=1 all pages in the core component are very likely to have rank 0 [1]. This behaviour makes it difficult to understand PageRank when α≈1, as it converges to a meaningless value for most pages. We propose a simple and natural modification to the standard preprocessing performed on the adjacency matrix of the graph, resulting in a ranking scheme we call TruRank. TruRank ranks the web with principles almost identical to PageRank, but it gives meaningful values also when α☰ 1.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {976–977},
numpages = {2},
keywords = {web graph, PageRank},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062827,
author = {Limanto, Hanny Yulius and Giang, Nguyen Ngoc and Trung, Vo Tan and Zhang, Jun and He, Qi and Huy, Nguyen Quang},
title = {An Information Extraction Engine for Web Discussion Forums},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062827},
doi = {10.1145/1062745.1062827},
abstract = {In this poster, we present an information extraction engine for web-based forums. The engine analyzes the HTML files crawled from web forums, deduces the wrapper (template) of the pages and extracts the information about posts (e.g., author, title, content, number of replies and views, etc.). Extraction is an important module for forum search engine, since it helps to understand the content of a forum HTML page and facilitates ranking during retrieval. We discuss the system architecture of the extraction engine in the context of a forum search engine and present various components in the extraction engine. We also introduce briefly the extraction process and discuss some implementation issues.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {978–979},
numpages = {2},
keywords = {information retrieval, information extraction, discussion board, search engine, forums},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062828,
author = {Liu, Nan and Yang, C.},
title = {Mining Web Site's Topic Hierarchy},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062828},
doi = {10.1145/1062745.1062828},
abstract = {Searching and navigating a Web site is a tedious task and the hierarchical models, such as site maps, are frequently used for organizing the Web site's content. In this work, we propose to model a Web site's content structure using the topic hierarchy, a directed tree rooted at a Web site's homepage in which the vertices and edges correspond to Web pages and hyperlinks. Our algorithm for mining a Web site's topic hierarchy utilizes three types of information associated with a Web site: link structure, directory structure and Web pages' content.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {980–981},
numpages = {2},
keywords = {web mining, topic hierarchy, content structure},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062829,
author = {Kotb, Yasser and Katayama, Takuya},
title = {Consistency Checking of UML Model Diagrams Using the XML Semantics Approach},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062829},
doi = {10.1145/1062745.1062829},
abstract = {A software design is often modeled as a collection of unified Modeling Language (UML) diagrams. There are different aspects of the software system that are covered by many different UML diagrams. This leads for big risk that the overall specification of the system becomes inconsistent and incompleteness. This inherits the necessary to check the consistency between these related UML diagrams. In addition, as the software system gets evolution, those diagrams get modified that leads again to possible inconsistency and incompleteness between the different versions of these diagrams. In this paper, we plan to employ our previous novel XML semantics approach, which proposed for checking the semantic consistency of XML documents using attribute grammar techniques, to check the consistency of UML diagrams. The key idea here is translating the UML diagrams to its equivalent XMI documents. Then checking the consistency of these XMI documents, they are special forms of XML, by employing them to our previous XML semantics approach.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {982–983},
numpages = {2},
keywords = {UML, XMI, model checking, XML, attribute grammars},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062830,
author = {Sabucedo, Luis \'{A}lvarez and Rif\'{o}n, Luis Anido},
title = {Delivering New Web Content Reusing Remote and Heterogeneous Sites. A DOM-Based Approach},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062830},
doi = {10.1145/1062745.1062830},
abstract = {This contribution addresses the development of new web sites reusing already existing contents from external sources. Unlike common links to other resources, which retrieves the whole resource, we propose an approach where partial retrieval is possible: the unit for data reuse is a node in a DOM tree. This solution permits the partial reuse of external and heterogeneous web contents with no need for client (browser) modifications and just minor changes for web servers.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {984–985},
numpages = {2},
keywords = {content reuse, DOM, reusability, interoperability, HTTP, hypertext, web server, URL},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062831,
author = {Soetens, Peter and De Geyter, Matthias},
title = {Multi-Step Media Adaptation: Implementation of a Knowledge-Based Engine},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062831},
doi = {10.1145/1062745.1062831},
abstract = {Continuing changes in the domains of consumer devices and multimedia formats demand for a new approach to media adaptation. The publication of customized content on a device requires an automatic adaptation engine that takes into account the specifications of both the device and the material to be published. These specifications can be expressed using a single domain ontology that describes the concepts of the media adaptation domain. In this document, we provide insight into the implementation of an adaptation engine that exploits this domain knowledge. We explain how this engine, through the use of description matching and Semantic Web Services, composes a chain of adaptation services which will alter the original content to the needs of the target device.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {986–987},
numpages = {2},
keywords = {device independence, semantic web, content adaptation, multimedia, OWL, standards, services},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062832,
author = {Toda, Hiroyuki and Kataoka, Ryoji},
title = {A Clustering Method for News Articles Retrieval System},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062832},
doi = {10.1145/1062745.1062832},
abstract = {Organizing the results of a search facilitates the user in overviewing the information returned. We regard the clustering task as the tasks of making labels for a list of items and we focus on news articles and propose a clustering method that uses named entity extraction.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {988–989},
numpages = {2},
keywords = {document clustering, named entity, search result organization},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062833,
author = {Mikami, Yoshiki and Zavarsky, Pavol and Rozan, Mohd Zaidi Abd and Suzuki, Izumi and Takahashi, Masayuki and Maki, Tomohide and Ayob, Irwan Nizan and Boldi, Paolo and Santini, Massimo and Vigna, Sebastiano},
title = {The Language Observatory Project (LOP)},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062833},
doi = {10.1145/1062745.1062833},
abstract = {The first part of the paper provides a brief description of the Language Observatory Project (LOP) and highlights the major technical difficulties to be challenged. The latter part gives how we responded to these difficulties by adopting UbiCrawler as a data collecting engine for the project. An interactive collaboration between the two groups is producing quite satisfactory results.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {990–991},
numpages = {2},
keywords = {language digital divide, web crawler, language, scripts, language identification, character sets},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062834,
author = {Bangyong, Liang and Jie, Tang and Juanzi, Li},
title = {Association Search in Semantic Web: Search + Inference},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062834},
doi = {10.1145/1062745.1062834},
abstract = {Association search is to search for certain instances in semantic web and then make inferences from and about the instances we have found. In this paper, we propose the problem of association search and our preliminary solution for it using Bayesian network. We first minutely define the association search and its categorization. We then define tasks in association search. In terms of Bayesian network, we take ontology taxonomy as network structure in Bayesian network. We use the query log of instances to estimate the network parameters. After the Bayesian network is constructed, we give the solution for association search in the network.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {992–993},
numpages = {2},
keywords = {bayesian network, inference, knowledge management, ontology},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062835,
author = {\c{C}elik, Tantek and Meyer, Eric A. and Mullenweg, Matthew},
title = {XHTML Meta Data Profiles},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062835},
doi = {10.1145/1062745.1062835},
abstract = {In this paper, we describe XHTML Meta Data Profiles (XMDP) which use XHTML to define a simple profile format which is both human and machine readable. XMDP can be used to extend XHTML by defining new link relationships, meta data properties/values, and class name semantics. XMDP has already been used to extend semantic XHTML to represent social networks, document licensing, voting, and tagging.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {994–995},
numpages = {2},
keywords = {lowercase semantic web, schema, reuse, class names, HTML, link relationships, profiles, XMDP, XFN, XHTML, microformats, world wide web, WWW, meta data},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062836,
author = {Cheung, Ronnie},
title = {An Adaptive Middleware Infrastructure for Mobile Computing},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062836},
doi = {10.1145/1062745.1062836},
abstract = {In a mobile environment where mobile applications suffer from the limitation and variation of system resources availability, it is desirable for the applications to adapt their behaviors to resource limitations and variations. It is also necessary to exploit optimal application performance. However, adaptation mechanisms by mobile applications usually suffers from the problem of unfairness to other applications, in contrast, adaptation by the operation system focuses more on the overall system performance, while neglecting the needs of individual applications. Hence, the adaptation task is best coordinated by a middleware that is able to cater for individual application's need on a fair ground, while maintaining optimal system performance. This is achieved by a context-aware mobile middleware that sits in between the mobile application and the operating environment.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {996–997},
numpages = {2},
keywords = {mobile environments, middleware infrastructure, adaptation},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062837,
author = {Chatterjee, Ramkrishna and Arun, Gopalan},
title = {Data Versioning Techniques for Internet Transaction Management},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062837},
doi = {10.1145/1062745.1062837},
abstract = {An Internet transaction is a transaction that involves communication over the Internet using standard Internet protocols such as HTTPS. Such transactions are widely used in Internet-based applications such as e-commerce. With the growth of the Internet, the volume and complexity of Internet transactions are rapidly increasing. We present data versioning techniques that can reduce the complexity of managing Internet transactions and improve their scalability and reliability. These techniques have been implemented using standard database technology, without any change in database kernel. Our initial empirical results argue for the effectiveness of these techniques in practice.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {998–999},
numpages = {2},
keywords = {versioning, internet transaction, scalability},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062838,
author = {Kr\"{u}pl, Bernhard and Herzog, Marcus and Gatterbauer, Wolfgang},
title = {Using Visual Cues for Extraction of Tabular Data from Arbitrary HTML Documents},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062838},
doi = {10.1145/1062745.1062838},
abstract = {We describe a method to extract tabular data from web pages. Rather than just analyzing the DOM tree, we also exploit visual cues in the rendered version of the document to extract data from tables which are not explicitly marked with an HTML table element. To detect tables, we rely on a variant of the well-known X-Y cut algorithm as used in the OCR community. We implemented the system by directly accessing Mozilla's box model that contains the positional data for all HTML elements of a given web page.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1000–1001},
numpages = {2},
keywords = {table detection, web information extraction, visual analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062839,
author = {Wilde, Erik},
title = {Describing Namespaces with GRDDL},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062839},
doi = {10.1145/1062745.1062839},
abstract = {Describing XML Namespaces is an open issue for many users of XML technologies, and even though namespaces are one of the foundations of XML, there is no generally accepted and widely used format for namespace descriptions. We present a framework for describing namespaces based on GRDDL using a controlled vocabulary. Using this frame-work, namespace descriptions can be easily generated, har-vested and published in human- or machine-readable form.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1002–1003},
numpages = {2},
keywords = {management, languages},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062840,
author = {Gulli, A. and Signorini, A.},
title = {Building an Open Source Meta-Search Engine},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062840},
doi = {10.1145/1062745.1062840},
abstract = {In this short paper we introduce Helios, a flexible and efficient open source meta-search engine. Helios currently runs on the top of 18 search engines (in Web, Books, News, and Academic publication domains), but additional search engines can be easily plugged in. We also report some performance mesured during its development.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1004–1005},
numpages = {2},
keywords = {open source, meta search engines},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062841,
author = {Wei, Jianbin and Xu, Cheng-Zhong},
title = {Design and Implementation of a Feedback Controller for Slowdown Differentiation on Internet Servers},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062841},
doi = {10.1145/1062745.1062841},
abstract = {Proportional slowdown differentiation (PSD) aims to maintain slowdown ratios between different classes of clients according to their pre-specified differentiation parameters. In this paper, we design a feedback controller to allocate processing rate on Internet servers for PSD. In this approach, the processing rate of a class is adjusted by an integral feedback controller according to the difference between the target slowdown ratio and the achieved one. The initial rate class is estimated based on predicted workload using queueing theory. We implement the feedback controller in an Apache Web server. The experimental results under various environments demonstrate the controller's effectiveness and robustness.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1006–1007},
numpages = {2},
keywords = {quality of service, feedback control, slowdown},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062842,
author = {Fukagaya, Yujiro and Ozono, Tadachika and Ito, Takayuki and Shintani, Toramatsu},
title = {MiSpider: A Continuous Agent on Web Pages},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062842},
doi = {10.1145/1062745.1062842},
abstract = {In this paper, we propose a Web based agent system called MiSpider, which provides intelligent web services on web browsers. MiSpider enables users to use agents on existing browsers. Users can use MiSpider all over the world only to access the Internet. MiSpider Agent has persistency, and agents condition doesn't change if users change a browsing page. Moreover, agents have a message passing skill to communicate among the agents.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1008–1009},
numpages = {2},
keywords = {information system, browsing support, multiagent system},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062843,
author = {Punera, Kunal and Rajan, Suju and Ghosh, Joydeep},
title = {Automatically Learning Document Taxonomies for Hierarchical Classification},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062843},
doi = {10.1145/1062745.1062843},
abstract = {While several hierarchical classification methods have been applied to web content, such techniques invariably rely on a pre-defined taxonomy of documents. We propose a new technique that extracts a suitable hierarchical structure automatically from a corpus of labeled documents. We show that our technique groups similar classes closer together in the tree and discovers relationships among documents that are not encoded in the class labels. The learned taxonomy is then used along with binary SVMs for multi-class classification. We demonstrate the efficacy of our approach by testing it on the 20-Newsgroup dataset.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1010–1011},
numpages = {2},
keywords = {automatic taxonomy learning, hierarchical classification},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062844,
author = {Koga, Takahiro and Tashiro, Noriharu and Ozono, Tadachika and Ito, Takayuki and Shintani, Toramatsu},
title = {Web Page Marker: A Web Browsing Support System Based on Marking and Anchoring},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062844},
doi = {10.1145/1062745.1062844},
abstract = {In this paper, we propose a web browsing support system, called WPM, which provides marking and anchoring functions on ordinary web browsers. WPM users can mark words and phrases on web pages by using their browsers without any extra plug-ins like similar systems, and can anchor words to refer them later. WPM makes it possible to carry out marking to the existing Web page so that marking carried out to paper. By changing character decoration partially, the text is indicated by emphasis and improve readability. WPM is implemented using proxy agent. This system can be used in everyday browsing, without a user being conscious of a system by using a proxy.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1012–1013},
numpages = {2},
keywords = {marking, proxy agent, browsing support},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062845,
author = {Xu, Wei and Sekar, R. and Ramakrishnan, I. V. and Venkatakrishnan, V. N.},
title = {An Approach for Realizing Privacy-Preserving Web-Based Services},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062845},
doi = {10.1145/1062745.1062845},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1014–1015},
numpages = {2},
keywords = {web service, privacy, information flow},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062846,
author = {Bayardo, Roberto J. and Thomschke, Sebastian},
title = {Exploiting the Web for Point-in-Time File Sharing},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062846},
doi = {10.1145/1062745.1062846},
abstract = {We describe a simple approach to "point-in-time" file sharing based on time expiring web links and personal webservers. This approach to file sharing is useful in environments where instant messaging clients are varied and don't necessarily support (compatible) file transfer protocols. We discuss the features of such an approach along with a successfully deployed implementation now in wide use throughout the IBM corporation.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1016–1017},
numpages = {2},
keywords = {instant messaging, file sharing, personal web server},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062847,
author = {Tous, Rub\'{e}n and Delgado, Jaime},
title = {Using OWL for Querying an XML/RDF Syntax},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062847},
doi = {10.1145/1062745.1062847},
abstract = {Some recent initiatives try to take profit from RDF to make XML documents interoperate at the semantic level. Ontologies are used to establish semantic connections among XML languages, and some mechanisms have been defined to query them with natural XML query languages like XPath and XML Query. Generally structure-mapping approaches define a simple translation between trivial XPath expressions and some RDF query language like RDQL; however some XPath constructs cannot be covered in a structure-mapping strategy. In contrast, our work takes the model-mapping approach, respectful with node order, that allows mapping all XPath axis. The obtained XPath implementation has the properties of schema-awareness and IDREF-awareness, so it can be used to exploit inheritance hierarchies defined in one or more XML schemas.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1018–1019},
numpages = {2},
keywords = {ontologies, interoperability, XPath, RDF, idref-awareness, XML, semantic integration, schema-awareness},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062848,
author = {Tummarello, Giovanni and Morbidoni, Christian and Puliti, Paolo and Piazza, Francesco},
title = {Signing Individual Fragments of an RDF Graph},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062848},
doi = {10.1145/1062745.1062848},
abstract = {Being able to determine the provenience of statements is a fundamental step in any SW trust modeling. We propose a methodology that allows signing of small groups of RDF statements. Groups of statements signed with this methodology can be safely inserted into any existing triple store without the loss of provenance information since only standard RDF semantics and constructs are used. This methodology has been implemented and is both available as open source library and deployed in a SW P2P project.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1020–1021},
numpages = {2},
keywords = {digital signature, trust, semantic web, RDF},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062849,
author = {Feldman, Ronen and Rosenfeld, Benjamin and Fresko, Moshe and Davison, Brian D.},
title = {Hybrid Semantic Tagging for Information Extraction},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062849},
doi = {10.1145/1062745.1062849},
abstract = {The semantic web is expected to have an impact at least as big as that of the existing HTML based web, if not greater. However, the challenge lays in creating this semantic web and in converting existing web information into the semantic paradigm. One of the core technologies that can help in migration process is automatic markup, the semantic markup of content, providing the semantic tags to describe the raw content. This paper describes a hybrid statistical and knowledge-based information extraction model, able to extract entities and relations at the sentence level. The model attempts to retain and improve the high accuracy levels of knowledge-based systems while drastically reducing the amount of manual labor by relying on statistics drawn from a training corpus. The implementation of the model, called TEG (Trainable Extraction Grammar), can be adapted to any IE domain by writing a suitable set of rules in a SCFG (Stochastic Context Free Grammar) based extraction language, and training them using an annotated corpus. The experiments show that our hybrid approach outperforms both purely statistical and purely knowledge-based systems, while requiring orders of magnitude less manual rule writing and smaller amount of training data. We also demonstrate the robustness of our system under conditions of poor training data quality. This makes the system very suitable for converting legacy web pages to semantic web pages.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1022–1023},
numpages = {2},
keywords = {rules based systems, text mining, information extraction, HMM, semantic web},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062850,
author = {Curtmola, Emiran and Amer-Yahia, Sihem and Brown, Philip and Fern\'{a}ndez, Mary},
title = {GalaTex: A Conformant Implementation of the XQuery Full-Text Language},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062850},
doi = {10.1145/1062745.1062850},
abstract = {We describe GalaTex, the first complete implementation of XQuery Full-Text, a W3C specification that extends XPath 2.0 and XQuery 1.0 with full-text search. XQuery Full-Text provides composable full-text search primitives such as keyword search, Boolean queries, and keyword-distance predicates. GalaTex is intended to serve as a reference implementation for XQuery Full-Text and as a platform for addressing new research problems such as scoring full-text query results, optimizing XML queries over both structure and text, and evaluating top-k queries on scored results. GalaTex is an all-XQuery implementation initially focused on completeness and conformance rather than on efficiency. We describe its implementation on top of Galax, a complete XQuery implementation.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1024–1025},
numpages = {2},
keywords = {full-text, conformant prototype, XQuery},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062851,
author = {Sillence, E. and Briggs, P. and Fishwick, L. and Harris, P.},
title = {Guidelines for Developing Trust in Health Websites},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062851},
doi = {10.1145/1062745.1062851},
abstract = {How do people decide which health websites to trust and which to reject? Thirteen participants all diagnosed with hypertension were invited to search for information and advice relating to hypertension. Participants took part in a four-week study engaging in both free and directed web searches. A content analysis of the group discussions revealed support for a staged model of trust in which mistrust or rejection of websites is based on design factors and trust or selection of websites is based on content factors such as source credibility and personalization. A number of guidelines for developing trust in health websites are proposed.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1026–1027},
numpages = {2},
keywords = {trust, social identity, credibility, computer mediated communication, internet, health},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062852,
author = {Wu, Kun-Lung and Chen, Shyh-Kwei and Yu, Philip S.},
title = {Efficient Structural Joins with On-the-Fly Indexing},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062852},
doi = {10.1145/1062745.1062852},
abstract = {Previous work on structural joins mostly focuses on maintaining offline indexes on disks. Most of them also require the elements in both sets to be sorted. In this paper, we study an on-the-fly, in-memory indexing approach to structural joins. There is no need to sort the elements or maintain indexes on disks. We identify the similarity between the structural join problem and the stabbing query problem, and extend a main memory-based indexing technique for stabbing queries to structural joins.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1028–1029},
numpages = {2},
keywords = {XML, containment queries, structural joins},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062853,
author = {Bry, Fran\c{c}ois and Eckert, Michael},
title = {Processing Link Structures and Linkbases on the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062853},
doi = {10.1145/1062745.1062853},
abstract = {Hyperlinks are an essential feature of the World Wide Web, highly responsible for its success. XLink improves on HTML's linking capabilities in several ways. In particular, links after XLink can be "out-of-line" (i.e., not defined at a link source) and collected in (possibly several) linkbases, which considerably ease building complex link structures.Modeling of link structures and processing of linkbases under the Web's "open world linking" are aspects neglected by XLink. Adding a notion of "interface" to XLink, as suggested in this work, considerably improves modeling of link structures. When a link structure is traversed, the relevant linkbase(s) might become ambiguous. We suggest three linkbase management modes governing the binding of a linkbase to a document to resolve this ambiguity.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1030–1031},
numpages = {2},
keywords = {link modeling and processing, linkbase, hyperlink, XLink},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062854,
author = {Lan, Man and Tan, Chew-Lim and Low, Hwee-Boon and Sung, Sam-Yuan},
title = {A Comprehensive Comparative Study on Term Weighting Schemes for Text Categorization with Support Vector Machines},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062854},
doi = {10.1145/1062745.1062854},
abstract = {Term weighting scheme, which has been used to convert the documents as vectors in the term space, is a vital step in automatic text categorization. In this paper, we conducted comprehensive experiments to compare various term weighting schemes with SVM on two widely-used benchmark data sets. We also presented a new term weighting scheme tf-rf to improve the term's discriminating power. The controlled experimental results showed that this newly proposed tf-rf scheme is significantly better than other widely-used term weighting schemes. Compared with schemes related with tf factor alone, the idf factor does not improve or even decrease the term's discriminating power for text categorization.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1032–1033},
numpages = {2},
keywords = {text, SVM, categorization, term weighting schemes},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062855,
author = {Benini, Marco and Trombetta, Alberto and Acquaviva, Michela},
title = {A Model for Short-Term Content Adaptation},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062855},
doi = {10.1145/1062745.1062855},
abstract = {This paper proposes a model for short-term content adaptation whose aim is to satisfy the contingent needs of users by adjusting the information a web-application provides on the basis of a short-term user profile. The mathematical model results in the design of an adaptive filter that profiles users by observing their queries to the application and that adjusts the answers of the application according to the inferred user needs. Also, the mathematical model ensures the correctness of the filter, that is, the filter is guaranteed to exhibit a coherent short-term adaptive behaviour.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1034–1035},
numpages = {2},
keywords = {information filtering, user modelling},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062856,
author = {Otto, Karsten A.},
title = {Semantic Virtual Environments},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062856},
doi = {10.1145/1062745.1062856},
abstract = {Today's Virtual Environment (VE) systems share a number of issues with the HTML-based World Wide Web. Their content is usually designed for presentation to humans, and thus is not suitable for machine access. This is complicated by the large number of different data models and network protocols in use. Accordingly, it is difficult to develop VE software, such as agents, services, and tools.In this paper we adopt the Semantic Web idea to the field of virtual environments. Using the Resource Description Framework (RDF) we establish a machine-understandable abstraction of existing VE systems --- the Semantic Virtual Environments (SVE). On this basis it is possible to develop system-independent software, which could even operate across VE system boundaries.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1036–1037},
numpages = {2},
keywords = {virtual environments, semantic web, components, integration, framework},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062857,
author = {Wang, Hai and Li, Yuan Fang and Sun, Jing and Zhang, Hongyu},
title = {Verify Feature Models Using Protegeowl},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062857},
doi = {10.1145/1062745.1062857},
abstract = {Feature models are widely used in domain engineering to capture common and variant features among systems in a particular domain. However, the lack of a widely-adopted means of precisely representing and formally verifying feature models has hindered the development of this area. This paper presents an approach to modeling and verifying feature diagrams using Semantic Web ontologies.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1038–1039},
numpages = {2},
keywords = {OWL, semantic web, feature modeling, ontologies},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062858,
author = {Tang, Jie and Liang, Yong and Li, Zi},
title = {Multiple Strategies Detection in Ontology Mapping},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062858},
doi = {10.1145/1062745.1062858},
abstract = {Ontology mapping is the task of finding semantic relationships between entities (i.e. concept, attribute and relation) of two ontologies. In the existing literatures, many (semi-)automatic approaches have found considerable interest by combining several mapping strategies (namely multi-strategy mapping). However, experiments show that multi-strategy based mapping does not always outperform its single-strategy counterpart. We here mainly consider the following questions: For a new, unseen mapping task, should one use a multi-strategy or a single-strategy? And if the task is suitable for multi-strategy, then which strategies should be selected in the combined scenario? This paper proposes an approach of multiple strategies detection for ontology mapping. The results obtained so far show that multi-strategy detection improves both on precision and recall significantly.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1040–1041},
numpages = {2},
keywords = {semantic web, ontology mapping, multi-strategy detection},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062859,
author = {Huang, Shen and Yu, Yong and Li, Shengping and Xue, Gui-Rong and Zhang, Lei},
title = {A Study on Combination of Block Importance and Relevance to Estimate Page Relevance},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062859},
doi = {10.1145/1062745.1062859},
abstract = {Some work showed that segmenting web pages into "semantic independent" blocks could help to improve the whole page retrieval. One key and unexplored issue is how to combine the block importance and relevance to a given query. In this poster, we first propose an automatic way to measure block importance to improve retrieval. After that, user information need is also concerned to refine block importance for different users.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1042–1043},
numpages = {2},
keywords = {iterative combination, information need, block relevance, block importance},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062860,
author = {S, Pradeep and Ramachandran, Chitra and Srinivasa, Srinath},
title = {Towards Autonomic Web-Sites Based on Learning Automata},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062860},
doi = {10.1145/1062745.1062860},
abstract = {Autonomics or self-reorganization becomes pertinent for web-sites serving a large number of users with highly varying workloads. An important component of self-adaptation is to model the behaviour of users and adapt accordingly. This paper proposes a learning-automata based technique for model discovery. User access patterns are used to construct an FSM model of user behaviour that in turn is used for prediction and prefetching. The proposed technique uses a generalization algorithm to classify behaviour patterns into a small number of generalized classes. It has been tested on both synthetic and live data-sets and has shown a prediction hit-rate of up to 89% on a real web-site.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1044–1045},
numpages = {2},
keywords = {autonomic website, generalization, learning automata},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062861,
author = {Srinivasan, Savitha and Amir, Arnon and Deshpande, Prasad and Zbarsky, Vladimir},
title = {On Business Activity Modeling Using Grammars},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062861},
doi = {10.1145/1062745.1062861},
abstract = {Web based applications offer a mainstream channel for businesses to manage their activities. We model such business activity in a grammar-based framework. The Backus Naur form notation is used to represent the syntax of a regular grammar corresponding to Web log patterns of interest. Then, a deterministic finite state machine is used to parse Web logs against the grammar. Detected tasks are associated with metadata such as time taken to perform the activity, and aggregated along relevant corporate dimensions.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1046–1047},
numpages = {2},
keywords = {data mining, web log analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062862,
author = {Lucanu, Dorel and Li, Yuan Fang and Dong, Jin Song},
title = {Soundness Proof of Z Semantics of OWL Using Institutions},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062862},
doi = {10.1145/1062745.1062862},
abstract = {The correctness of the Z semantics of OWL is the theoretical foundation of using software engineering techniques to verify Web ontologies. As OWL and Z are based on different logical systems, we use institutions to represent their underlying logical systems and use institution morphisms to prove the correctness of the Z semantics for OWL DL.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1048–1049},
numpages = {2},
keywords = {Z, OWL, comorphism of institutions, institution},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062863,
author = {Juan, Yun-Fang and Chang, Chi-Chao},
title = {An Analysis of Search Engine Switching Behavior Using Click Streams},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062863},
doi = {10.1145/1062745.1062863},
abstract = {In this paper, we propose a simple framework to characterize the switching behavior between search engines based on click streams. We segment users into a number of categories based on their search engine usage during two adjacent time periods and construct the transition probability matrix across these usage categories. The principal eigenvector of the transposed transition probability matrix represents the limiting probabilities, which are proportions of users in each usage category at steady state. We experiment with this framework using click streams focusing on two search engines: one with a large market share and the other with a small market share. The results offer interesting insights into search engine switching. The limiting probabilities provide empirical evidence that small engines can still retain its fair share of users over time.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1050–1051},
numpages = {2},
keywords = {limiting probabilities, clustering, switching behavior, probability matrix, search engines, transition, session, principal eigenvectors, Markov chain, sequence},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062864,
author = {Vinay, Vishwa and Wood, Ken and Milic-Frayling, Natasa and Cox, Ingemar J.},
title = {Comparing Relevance Feedback Algorithms for Web Search},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062864},
doi = {10.1145/1062745.1062864},
abstract = {We evaluate three different relevance feedback (RF)algorithms, Rocchio, Robertson/Sparck-Jones (RSJ)and Bayesian, in the context of Web search. We use a target-testing experimental procedure whereby a user must locate a specific document. For user relevance feedback, we consider all possible user choices of indicating zero or more relevant documents from a set of 10 displayed documents. Examination of the effects of each user choice permits us to compute an upper-bound on the performance of each RF algorithm.We ind that there is a significant variation in the upper-bound performance o the three RF algorithms and that the Bayesian algorithm approaches the best possible.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1052–1053},
numpages = {2},
keywords = {web search, evaluation, relevance feedback},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062865,
author = {Feng, Jianlin and Liu, Huijun and Zou, Jing},
title = {SAT-MOD: Moderate Itemset Fittest for Text Classification},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062865},
doi = {10.1145/1062745.1062865},
abstract = {In this paper, we present a novel association-based method called SAT-MOD for text classification. SAT-MOD views a sentence rather than a document as a transaction, and uses a novel heuristic called MODFIT to select the most significant itemsets for constructing a category classifier. The effectiveness of SAT-MOD has been demonstrated comparable to well-known alternatives such as LinearSVM and much better than current document-level words association based methods on the Reuters corpus.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1054–1055},
numpages = {2},
keywords = {MODFIT (moderate itemset fittest) heuristic, text classification},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062866,
author = {Lowe, David and Kong, Xiaoying},
title = {Applying NavOptim to Minimise Navigational Effort},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062866},
doi = {10.1145/1062745.1062866},
abstract = {A major factor in the effectiveness of the interaction which users have with Web applications is the ease with which they can locate information and functionality which they are seeking. Effective design is however complicated by the multiple design purposes and diverse users which Web applications typically support. In this paper we describe a navigational design method aimed at optimising designs through minimizing navigational entropy. The approach uses a theoretical navigational depth for the various information and service components to moderate a nested hierarchical clustering of the content.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1056–1057},
numpages = {2},
keywords = {navigation architecture, efforts metrics, design},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062867,
author = {Facca, Federico M. and Ceri, Stefano and Armani, Jacopo and Demald\'{e}, Vera},
title = {Building Reactive Web Applications},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062867},
doi = {10.1145/1062745.1062867},
abstract = {The Adaptive Web is a new research area addressing the personalization of the Web experience for each user. In this paper we propose a new high-level model for the specification of Web applications that take into account the manner users interact with the application for supplying appropriate contents or gathering profile data. We therefore consider entire processes (rather than single properties) as smallest information units, allowing for automatic restructuring of application components. For this purpose, a high-level Event-Condition-Action (ECA) paradigm is proposed, which enables capturing arbitrary (and timed) clicking behaviors.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1058–1059},
numpages = {2},
keywords = {eca rule, user modeling, adaptive web, design method},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062868,
author = {Wenyin, Liu and Huang, Guanglin and Xiaoyue, Liu and Min, Zhang and Deng, Xiaotie},
title = {Detection of Phishing Webpages Based on Visual Similarity},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062868},
doi = {10.1145/1062745.1062868},
abstract = {An approach to detection of phishing webpages based on visual similarity is proposed, which can be utilized as a part of an enterprise solution for anti-phishing. A legitimate webpage owner can use this approach to search the Web for suspicious webpages which are visually similar to the true webpage. A webpage is reported as a phishing suspect if the visual similarity is higher than its corresponding preset threshold. Preliminary experiments show that the approach can successfully detect those phishing webpages for online use.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1060–1061},
numpages = {2},
keywords = {anti-phishing, visual similarity, information filtering, web document analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@dataset{10.1145/review-1062745.1062868_R39639,
author = {Jacobs, Shannon},
title = {Review ID:R39639 for DOI: 10.1145/1062745.1062868},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1062745.1062868_R39639}
}

@inproceedings{10.1145/1062745.1062869,
author = {Petricek, Vaclav and Cox, Ingemar J. and Han, Hui and Councill, Isaac G. and Giles, C. Lee},
title = {Modeling the Author Bias between Two On-Line Computer Science Citation Databases},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062869},
doi = {10.1145/1062745.1062869},
abstract = {We examine the difference and similarities between two on-line computer science citation databases DBLP and CiteSeer. The database entries in DBLP are inserted manually while the CiteSeer entries are obtained autonomously. We show that the CiteSeer database contains considerably fewer single author papers. This bias can be modeled by an exponential process with intuitive explanation. The model permits us to predict that the DBLP database covers approximately 30% of the entire literature of Computer Science.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1062–1063},
numpages = {2},
keywords = {acquisition bias, DBLP, citeSeer, bibliometrics},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062870,
author = {Li, Ning and Hui, Joshua and Hsiao, Hui-I and Beyer, Kevin},
title = {Hubble: An Advanced Dynamic Folder System for XML},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062870},
doi = {10.1145/1062745.1062870},
abstract = {Organizing large document collections for finding information easily and quickly has always been an important user requirement. This paper describes a flexible and powerful dynamic folder technology, called Hubble, which exploits XML semantics to precisely categorize XML documents into categories or folders.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1064–1065},
numpages = {2},
keywords = {categorization, content navigation, XML, dynamic folder},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062871,
author = {da Silva, Ana Cristina B. and de Oliveira, Joao B. S. and Mano, Fernando T. M. and Silva, Thiago B. and Meirelles, Leonardo L. and Meneguzzi, Felipe R. and Giannetti, Fabio},
title = {Support for Arbitrary Regions in XSL-FO: A Proposal for Extending XSL-FO Semantics and Processing Model},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062871},
doi = {10.1145/1062745.1062871},
abstract = {This paper proposes an extension of the XSL-FO standard which allows the specification of an unlimited number of arbitrarily shaped page regions. These extensions are built on top of XSL-FO 1.1 to enable flow content to be laid out into arbitrary shapes and allowing for page layouts currently available only to desktop publishing software. Such a proposal is expected to leverage XSL-FO towards usage as an enabling technology in the generation of content intended for personalized printing.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1066–1067},
numpages = {2},
keywords = {digital printing, XML, LaTeX, XSL-FO, SVG},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062872,
author = {Liu, Xue and Zheng, Rong and Heo, Jin and Sha, Lui},
title = {Improved Timing Control for Web Server Systems Using Internal State Information},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062872},
doi = {10.1145/1062745.1062872},
abstract = {How to effectively allocate system resource to meet the Service Level Agreement (SLA) of Web servers is a challenging problem. In this paper, we propose an improved scheme for autonomous timing performance control in Web servers under highly dynamic traffic loads. We devise a novel delay regulation technique called Queue Length Model Based Feedback Control utilizing server internal state information to reduce response time variance in presence of bursty traffic. Both simulation and experimental studies using synthesized workloads and real-world Web traces demonstrate the effectiveness of the proposed approach.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1068–1069},
numpages = {2},
keywords = {web server, feedback, queueing model, SLA, control theory},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062873,
author = {Zhou, Chen and Chia, Liang-Tien and Lee, Bu-Sung},
title = {Service Discovery and Measurement Based on DAML-QoS Ontology},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062873},
doi = {10.1145/1062745.1062873},
abstract = {As more and more Web services are deployed, Web service's discovery mechanisms become essential. Similar services can have quite different QoS behaviors. For service selection and management purpose, it is necessary to clearly specify QoS constraints and metrics definitions for Web services. We investigate on the semantic QoS specification and introduce our design principles on it. Based on the specification refinement and conformance, we introduce the QoS matchmaking algorithm with multiple matching degrees. The matchmaking prototype is designed to prove the feasibility. Well-defined Metrics can be further utilized by measurement organizations to monitor and evaluate the promised service level objectives.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1070–1071},
numpages = {2},
keywords = {matchmaking, QoS, semantic web, web service discovery},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062874,
author = {Dong, Yan-Shi and Han, Ke-Song},
title = {Boosting SVM Classifiers by Ensemble},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062874},
doi = {10.1145/1062745.1062874},
abstract = {By far, the support vector machines (SVM) achieve the state-of-the-art performance for the text classification (TC) tasks. Due to the complexity of the TC problems, it becomes a challenge to systematically develop classifiers with better performance. We try to attack this problem by ensemble methods, which are often used for boosting weak classifiers, such as decision tree, neural networks, etc., and whether they are effective for strong classifiers is not clear.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1072–1073},
numpages = {2},
keywords = {machine learning, text processing, classifier design and evaluation, neural nets, information filtering},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062875,
author = {Wu, Le-Shin and Akavipat, Ruj and Menczer, Filippo},
title = {Adaptive Query Routing in Peer Web Search},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062875},
doi = {10.1145/1062745.1062875},
abstract = {An unstructured peer network application was proposed to address the query forwarding problem of distributed search engines and scalability limitations of centralized search engines. Here we present novel techniques to improve local adaptive routing, showing they perform significantly better than a simple learning scheme driven by query response interactions among neighbors. We validate prototypes of our peer network application via simulations with 500 model users based on actual Web crawls. We finally compare the quality of the results with those obtained by centralized search engines, suggesting that our application can draw advantages from the context and coverage of the peer collective.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1074–1075},
numpages = {2},
keywords = {topical crawlers, adaptive query routing, peer collaborative search},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062876,
author = {Sumi, Kaoru and Tanaka, Katsumi},
title = {Transforming Web Contents into a Storybook with Dialogues and Animations},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062876},
doi = {10.1145/1062745.1062876},
abstract = {This paper describes a medium, called Interactive e-Hon, for helping children to understand contents from the Web. It works by transforming electronic contents into an easily understandable "storybook world." In this world, easy-to-understand contents are generated by creating 3D animations that include contents and metaphors, and by using a child-parent model with dialogue expression and a question-answering style comprehensible to children.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1076–1077},
numpages = {2},
keywords = {dialogue, information presentation, media conversion, agent, animation},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062877,
author = {Blanco, Yolanda and Pazos, Jos\'{e} J. and Gil, Alberto and Ramos, Manuel and Fern\'{a}ndez, Ana and D\'{\i}az, Rebeca P. and L\'{o}pez, Mart\'{\i}n and Barrag\'{a}ns, Bel\'{e}n},
title = {AVATAR: An Approach Based on Semantic Reasoning to Recommend Personalized TV Programs},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062877},
doi = {10.1145/1062745.1062877},
abstract = {In this paper a TV recommender system called AVATAR (AdVAnce Telematic search of Audiovisual contents by semantic Reasoning) is presented. This tool uses the experience gained in the field of the Semantic Web to personalize the TV programs shown to the end users. The main contribution of our system is a process of semantic reasoning carried out on the descriptions of the TV contents ---provided by means of metainformation--- and on the viewer preferences --- contained in personal profiles. Such process allows to diversify the offered suggestions maintaining the personalization, given that the aim is to find contents appealing for the users, which are related semantically to their programs of interest.Here the framework proposed for this reasoning is introduced, by including (i) the OWL ontology chosen to represent the knowledge of our application domain, (ii) the organization of the user profiles, (iii) the query language LIKO, which is intended to browse the ontology and (iv) the semantic relations inferred from the system knowledge base.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1078–1079},
numpages = {2},
keywords = {ontologies, inference of semantic relations, TV recommender system, semantic web},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062878,
author = {Bhatia, Anubhav and Mukherjee, Saikat and Mitra, Saugat and Srinivasa, Srinath},
title = {WAND: A Meta-Data Maintenance System over the Internet},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062878},
doi = {10.1145/1062745.1062878},
abstract = {WAND is a meta-data management system that provides a file-system tree for users of an internet based P2P network. The tree is robust and retains its structure even when nodes (peers) enter and leave the network. The robustness is based on a concept of virtual folders that are automatically created to retain paths to lower level folders whenever a node hosting a higher-level folder moves away. Other contributions of the WAND system include its novel approach towards managing root directory information and handling network partitions.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1080–1081},
numpages = {2},
keywords = {wide-area distributed file system, peer-to-peer, meta-data, maintenance},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062879,
author = {Bailey, James and Bry, Fran\c{c}ois and P\"{a}tr\^{a}njan, Paula-Lavinia},
title = {Composite Event Queries for Reactivity on the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062879},
doi = {10.1145/1062745.1062879},
abstract = {Reactivity on the Web is an emerging issue. The capability to automatically react to events (such as updates to Web resources) is essential for both Web services and Semantic Web systems. Such systems need to have the capability to detect and react to complex, real life situations. This presentation gives flavours of the high-level language XChange, for programming reactive behaviour on the Web.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1082–1083},
numpages = {2},
keywords = {reactive languages, web, event-condition-action rules, composite events},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062880,
author = {Kashihara, Akihiro and Hasegawa, Shinobu},
title = {Learning How to Learn with Web Contents},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062880},
doi = {10.1145/1062745.1062880},
abstract = {Learning Web contents requires learners not only to navigate the Web pages to construct their own knowledge from the contents learned at and between the pages, but also to control their own navigation and knowledge construction processes. However, it is not so easy to control the learning processes. The main issue addressed is how to help learners learn how to learn with Web contents. This paper discusses how to design a meta-learning tool.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1084–1085},
numpages = {2},
keywords = {navigational learning, web contents, hyperspace, learning affordance, meta-learning},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062881,
author = {Beauvisage, Thomas and Assadi, Houssem},
title = {From User-Centric Web Traffic Data to Usage Data},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062881},
doi = {10.1145/1062745.1062881},
abstract = {In this paper, we describe a user-centric Internet usage data processing platform. Raw usage data is collected using a software probe installed on a panel of Internet users' workstations. It is then processed by our platform. The transformation of raw usage data into qualified and usable information by Internet usage sociology researchers means setting up a series of relatively complex processes using quite a wide variety of resources. We use a combination of ad hoc rule-based systems and external resources to qualify the visited Web pages. We also implemented topological and temporal indicators in order to describe the dynamics of Web sessions.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1086–1087},
numpages = {2},
keywords = {internet uses, web usage mining, traffic analysis, usage data, user-centric traffic data},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062882,
author = {Beckers, Tom and Oorts, Nico and Hendrickx, Filip and Van De Walle, Rik},
title = {Multichannel Publication of Interactive Media Documents in a News Environment},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062882},
doi = {10.1145/1062745.1062882},
abstract = {Multichannel publication of multimedia presentations poses a significant challenge on the generic description of the presentation content and the system necessary to convert these descriptions into final-form presentations. We present a solution based on the XiMPF document model and a component based system architecture.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1088–1089},
numpages = {2},
keywords = {XML, standards, multichannel publication, multimedia, interactivity, device independence, framework},
location = {Chiba, Japan},
series = {WWW '05}
}

@dataset{10.1145/review-1062745.1062882_R39629,
author = {Jacobs, Shannon},
title = {Review ID:R39629 for DOI: 10.1145/1062745.1062882},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1062745.1062882_R39629}
}

@inproceedings{10.1145/1062745.1062883,
author = {Ardissono, L. and Console, L. and Goy, A. and Petrone, G. and Picardi, C. and Segnan, M. and Dupr\'{e}, D. Theseider},
title = {Advanced Fault Analysis in Web Service Composition},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062883},
doi = {10.1145/1062745.1062883},
abstract = {Currently, fault management in Web Services orchestrating multiple suppliers relies on a local analysis, that does not span across individual services, thus limiting the effectiveness of recovery strategies. We propose to address this limitation by employing Model-Based Diagnosis to enhance fault analysis. In our approach, a Diagnostic Web Service is added to the set of Web Services providing the overall service, and acts as a supervisor of their execution, by identifying anomalies and explaining them in terms of faults to be repaired.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1090–1091},
numpages = {2},
keywords = {diagnosis, fault management, web service composition},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062884,
author = {Matsumura, Naohiro and Goldberg, David E. and Llor\`{a}, Xavier},
title = {Mining Directed Social Network from Message Board},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062884},
doi = {10.1145/1062745.1062884},
abstract = {In the paper, we present an approach to mining a directed social network from a message board on the Internet where vertices denote individuals and directed links denote the flow of influence. The influence is measured based on propagating terms among individuals via messages. The distance with respect to contextual similarity between individuals is acquired since the influence indicates the degree of their shared interest represented as terms.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1092–1093},
numpages = {2},
keywords = {internet message board, directed social network},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062885,
author = {Desikan, Prasanna and Pathak, Nishith and Srivastava, Jaideep and Kumar, Vipin},
title = {Incremental Page Rank Computation on Evolving Graphs},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062885},
doi = {10.1145/1062745.1062885},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1094–1095},
numpages = {2},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062886,
author = {Korolova, Aleksandra and Farahat, Ayman and Golle, Philippe},
title = {Enhancing the Privacy of Web-Based Communication},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062886},
doi = {10.1145/1062745.1062886},
abstract = {A profiling adversary is an adversary whose goal is to classify a population of users into categories according to messages they exchange. This adversary models the most common privacy threat against web based communication.We propose a new encryption scheme, called stealth encryption, that protects users from profiling attacks by concealing the semantic content of plaintext while preserving its grammatical structure and other non-semantic linguistic features, such as word frequency distribution. Given English plaintext, stealth encryption produces ciphertext that cannot efficiently be distinguished from normal English text (our techniques apply to other languages as well).},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1096–1097},
numpages = {2},
keywords = {protection, profiling, privacy},
location = {Chiba, Japan},
series = {WWW '05}
}

@dataset{10.1145/review-1062745.1062886_R39706,
author = {Saleh, Zakaria},
title = {Review ID:R39706 for DOI: 10.1145/1062745.1062886},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1062745.1062886_R39706}
}

@inproceedings{10.1145/1062745.1062887,
author = {Shin, Dong-Hoon and Lee, Kyong-Ho},
title = {Generating XSLT Scripts for the Fast Transformation of XML Documents},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062887},
doi = {10.1145/1062745.1062887},
abstract = {This paper proposes a method of generating XSLT scripts, which support the fast transformation of XML documents, given one-to-one matching relationships between leaf nodes of XML schemas. The proposed method enhances the transformation speed of generated XSLT scripts through reducing template calls. Experimental results show that the proposed method has generated XSLT scripts that support the faster transformation of XML documents, compared with previous works.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1098–1099},
numpages = {2},
keywords = {XML, document transformation, XSLT},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062888,
author = {Rafiei, Davood and Curial, Stephen},
title = {ALVIN: A System for Visualizing Large Networks},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062888},
doi = {10.1145/1062745.1062888},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1100–1101},
numpages = {2},
keywords = {visualizing the web, network visualization, sampling},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062889,
author = {Shen, Xuehua and Dumais, Susan and Horvitz, Eric},
title = {Analysis of Topic Dynamics in Web Search},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062889},
doi = {10.1145/1062745.1062889},
abstract = {We report on a study of topic dynamics for pages visited by a sample of people using MSN Search. We examine the predictive accuracies of probabilistic models of topic transitions for individuals and groups of users. We explore temporal dynamics by comparing the accuracy of the models for predicting topic transitions at increasingly distant times in the future. Finally, we discuss directions for applying models of search topic dynamics.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1102–1103},
numpages = {2},
keywords = {user modeling, web search, topic transition, topic analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062890,
author = {Li, Qing and Kim, Byeong Man and Myaeng, Sung Hyon},
title = {Clustering for Probabilistic Model Estimation for CF},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062890},
doi = {10.1145/1062745.1062890},
abstract = {Based on the type of collaborative objects, a collaborative filtering (CF) system falls into one of two categories: item-based CF and user-based CF. Clustering is the basic idea in both cases, where users or items are classified into user groups where users share similar preference or item groups where items have similar attributes or characteristics. Observing the fact that in user-based CF each user community is characterized by a Gaussian distribution on the ratings for each item and the fact that in item-based CF the ratings of each user in item community satisfy a Gaussian distribution, we propose a method of probabilistic model estimation for CF, where objects (user or items) are classified into groups based on the content information and ratings at the same time and predictions are made considering the Gaussian distribution of ratings. Experiments on a real-world data set illustrate that our approach is favorable.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1104–1105},
numpages = {2},
keywords = {probabilistic model, collaborative filtering, information filtering},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062891,
author = {LIU, Tie-Yan and YANG, Yiming and WAN, Hao and ZHOU, Qian and GAO, Bin and ZENG, Hua-Jun and CHEN, Zheng and MA, Wei-Ying},
title = {An Experimental Study on Large-Scale Web Categorization},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062891},
doi = {10.1145/1062745.1062891},
abstract = {Taxonomies of the Web typically have hundreds of thousands of categories and skewed category distribution over documents. It is not clear whether existing text classification technologies can perform well on and scale up to such large-scale applications. To understand this, we conducted the evaluation of several representative methods (Support Vector Machines, k-Nearest Neighbor and Naive Bayes) with Yahoo! taxonomies. In particular, we evaluated the effectiveness/efficiency tradeoff in classifiers with hierarchical setting compared to conventional (flat) setting, and tested popular threshold tuning strategies for their scalability and accuracy in large-scale classification problems.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1106–1107},
numpages = {2},
keywords = {parameter tuning strategies, text categorization, very large web taxonomies, algorithm complexity},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062892,
author = {LIU, Tie-Yan and WAN, Hao and QIN, Tao and CHEN, Zheng and REN, Yong and MA, Wei-Ying},
title = {Site Abstraction for Rare Category Classification in Large-Scale Web Directory},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062892},
doi = {10.1145/1062745.1062892},
abstract = {Automatically classifying the Web directories is an effective way to manage Web information. However, our experiments showed that the state-of-the-art text classification technologies could not lead to acceptable performance in this task. Due to our analysis, the main problem is the lack of effective training data in rare categories of Web directories. To tackle this problem, we proposed a novel technology named Site Abstraction to synthesize new training examples from the website of the existing training document. The main idea is to propagate features through parent-child relationship in the sitemap tree. Experiments showed that our method significantly improved the classification performance.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1108–1109},
numpages = {2},
keywords = {support vector machines (SVM), site abstraction, hierarchical classification, web directory, text classification},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062893,
author = {Karampiperis, Pythagoras and Sampson, Demetrios},
title = {Designing Learning Services: From Content-Based to Activity-Based Learning Systems},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062893},
doi = {10.1145/1062745.1062893},
abstract = {The need for e-learning systems that support a diverse set of pedagogical requirements has been identified as an important issue in web-based education. Until now, significant R&amp;D effort has been devoted aiming towards web-based educational systems tailored to specific pedagogical approaches. The most advanced of them are based on the IEEE Learning Technology Systems Architecture and use standardized content structuring based on the ADL Sharable Content Object Reference Model in order to enable sharing and reusability of the learning content. However, sharing of learning activities among different web-based educational systems still remains an open issue. The open question is how web-based educational systems should be designed in order to enable reusing and repurposing of learning activities. In this paper we propose an authoring system, refered to as ASK-LDT that utilizes the Learning Design principles to provide the means for designing activity-based learning services and systems.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1110–1111},
numpages = {2},
keywords = {authoring tools, architectures, learning activities, reusability, learning design},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062894,
author = {Ciobanu, Gabriel and Rusu, D\u{a}nu\c{t}},
title = {Topological Spaces of the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062894},
doi = {10.1145/1062745.1062894},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1112–1113},
numpages = {2},
keywords = {separation, web metrics, topology density},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062895,
author = {Gupta, Suhit and Kaiser, Gail and Stolfo, Salvatore},
title = {Extracting Context to Improve Accuracy for HTML Content Extraction},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062895},
doi = {10.1145/1062745.1062895},
abstract = {Previous work on content extraction utilized various heuristics such as link to text ratio, prominence of tables, and identification of advertising. Many of these heuristics were associated with "settings", whereby some heuristics could be turned on or off and others parameterized by minimum or maximum threshold values. A given collection of settings - such as removing table cells with high linked to non-linked text ratios and removing all apparent advertising -- might work very well for a news website, but leave little or no content left for the reader of a shopping site or a web portal We present a new technique, based on incrementally clustering websites using search engine snippets, to associate a newly requested website with a particular "genre", and then employ settings previously determined to be appropriate for that genre, with dramatically improved content extraction results overall.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1114–1115},
numpages = {2},
keywords = {content extraction, reformatting, accessibility, HTML, context, speech rendering, DOM trees},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062896,
author = {Qian, Gang and Dong, Yisheng},
title = {Constructing Extensible XQuery Mappings},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062896},
doi = {10.1145/1062745.1062896},
abstract = {Constructing and maintaining semantic mappings are necessary but troublesome in data sharing systems. While most current work focuses on seeking automated techniques to solve this problem, this paper proposes a combination model for constructing exten-sible mappings between XML schemas. In our model, complex global mappings are constructed by first defining simple atomic mappings for each target schema element, and then combining them using a few basic operators. At the same time, we provide automated support for constructing such combined mappings.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1116–1117},
numpages = {2},
keywords = {extensibility, XQuery, automated support, mapping},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062897,
author = {Lu, Jiaheng and Chen, Ting and Ling, Tok Wang},
title = {TJFast: Effective Processing of XML Twig Pattern Matching},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062897},
doi = {10.1145/1062745.1062897},
abstract = {Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. A number of algorithms have been proposed to process a twig query based on region encoding. In this paper, based on a novel labeling scheme: extended Dewey, we propose a novel and efficient holistic twig join algorithm, namely TJFast. Compared to previous work, our algorithm only needs to access the labels of leaf query nodes. We report our experimental results to show that our algorithms are superior to previous approaches in terms of the number of elements scanned and query performance.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1118–1119},
numpages = {2},
keywords = {labeling scheme, holistic twig join},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062898,
author = {Imamura, Takeshi and Tatsubori, Michiaki and Nakamura, Yuichi and Giblin, Christopher},
title = {Web Services Security Configuration in a Service-Oriented Architecture},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062898},
doi = {10.1145/1062745.1062898},
abstract = {Security is one of the major concerns when developing mission-critical business applications, and this concern motivated the Web Services Security specifications. However, the existing tools to configure the security properties of Web Services give a technology-oriented view; only assisting in choosing data to encrypt and the encryption algorithms to use. A user must manually bridge the gap between the security requirements and the configuration, which could cause extra configuration costs and lead to potential misconfiguration hazards. To ease this situation, we came up with refining security requirements from business to technology, leveraging the concepts of Service-Oriented Architecture (SOA) and Model-Driven Architecture (MDA). Security requirements are gradually transformed to more detailed ones or countermeasures by bridging the gap between them by using best practice patterns.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1120–1121},
numpages = {2},
keywords = {best practice pattern, service-oriented architecture, model-driven architecture, web services security, security configuration},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062899,
author = {Bouklit, Mohamed and Mathieu, Fabien},
title = {BackRank: An Alternative for PageRank?},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062899},
doi = {10.1145/1062745.1062899},
abstract = {This paper proposes to extend a previous work, The Effect of the Back Button in a Random Walk: Application for PageRank [5]. We introduce an enhanced version of the PageRank algorithm using a realistic model for the Back button, thus improving the random surfer model. We show that in the special case where the history is bound to an unique page (you cannot use the Back button twice in a row), we can produce an algorithm that does not need much more resources than a standard PageRank. This algorithm, BackRank, can converge up to 30% faster than a standard PageRank and suppress most of the drawbacks induced by the existence of pages without links.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1122–1123},
numpages = {2},
keywords = {back button, random walk, PageRank, flow, web analysis},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062900,
author = {Dmitriev, Pavel and Lagoze, Carl and Suchkov, Boris},
title = {Finding the Boundaries of Information Resources on the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062900},
doi = {10.1145/1062745.1062900},
abstract = {In recent years, many algorithms for the Web have been developed that work with information units distinct from individual web pages. These include segments of web pages or aggregation of web pages into web communities. Using these logical information units has been shown to improve the performance of many web algorithms. In this paper, we focus on a type of logical information units called compound documents. We argue that the ability to identify compound documents can improve information retrieval, automatic metadata generation, and navigation on the Web. We propose a unified framework for identifying the boundaries of compound documents, which combines both structural and content features of constituent web pages. The framework is based on a combination of machine learning and clustering algorithms, with the former algorithm supervising the latter one. Experiments on a collection of educational web sites show that our approach can reliably identify most of the compound documents on these sites.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1124–1125},
numpages = {2},
keywords = {compound documents, WWW, clustering},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062901,
author = {Syeda-Mahmood, Tanveer and Shah, Gauri and Yan, Lingling and Urban, Willi},
title = {Semantic Search of Schema Repositories},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062901},
doi = {10.1145/1062745.1062901},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1126–1127},
numpages = {2},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062902,
author = {Hernandez, Thomas and Kambhampati, Subbarao},
title = {Improving Text Collection Selection with Coverage and Overlap Statistics},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062902},
doi = {10.1145/1062745.1062902},
abstract = {In an environment of distributed text collections, the first step in the information retrieval process is to identify which of all available collections are more relevant to a given query and which should thus be accessed to answer the query. We address the challenge of collection selection when there is full or partial overlap between the available text collections, a scenario which has not been examined previously despite its real-world applications. To that end, we present COSCO, a collection selection approach which uses collection-specific coverage and overlap statistics. We describe our experimental results which show that the presented approach displays the desired behavior of retrieving more new results early on in the collection order, and performs consistently and significantly better than CORI, previously considered to be one of the best collection selection systems.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1128–1129},
numpages = {2},
keywords = {collection overlap, collection selection, statistics gathering},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062903,
author = {Choi, Seunglak and Kim, Jungsook and Jang, Hyukjae and Kim, Su Myeon and Song, Junehwa and Kim, Hangkyu and Lee, Yunjoon},
title = {A Framework for Handling Dependencies among Web Services Transactions},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062903},
doi = {10.1145/1062745.1062903},
abstract = {This paper proposes an effective Web services (WS) transaction management framework to automatically manage inconsistencies occurred by relaxing isolation of WS transactions.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1130–1131},
numpages = {2},
keywords = {transaction model, web services, isolation relaxation, transaction management protocol},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062904,
author = {Charfi, Anis and Mezini, Mira},
title = {Middleware Services for Web Service Compositions},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062904},
doi = {10.1145/1062745.1062904},
abstract = {WS-* specifications cover a variety of issues ranging from security and reliability to transaction support in web services. However, these specifications do not address web service compositions. On the other hand, BPEL as the future standard web service composition language allows the specification of the functional part of the composition as a business process but it fails short in expressing non-functional properties such as security, reliability and persistence. In this paper, we propose an approach for the transparent integration of technical concerns in web service compositions. Our approach is driven by the analogy between web services and software components and is inspired from server-side component models such as Enterprise Java Beans. The main components of our framework are the process container, the middleware services and the deployment descriptor.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1132–1133},
numpages = {2},
keywords = {BPEL, middleware, web service composition},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062905,
author = {Su, Mu and Chi, Chi-Hung},
title = {Application Networking on Peer-to-Peer Networks},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062905},
doi = {10.1145/1062745.1062905},
abstract = {This paper proposes the AN.P2P architecture to facilitate efficient peer-to-peer content delivery with heterogeneous presentation requirements. In general, the AN.P2P enables a peer to deliver the original content objects and an associated workflow to other peers. The workflow is composed of content adaptation tasks. Hence, the recipient can reuse the original object to generate appropriate presentations for other peers.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1134–1135},
numpages = {2},
keywords = {application networking, peer-to-peer content distribution},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062906,
author = {Liu, Yiqun and Wang, Canhui and Zhang, Min and Ma, Shaoping},
title = {Web Data Cleansing for Information Retrieval Using Key Resource Page Selection},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062906},
doi = {10.1145/1062745.1062906},
abstract = {With the page explosion of WWW, how to cover more useful information with limited storage and computation resources becomes more and more important in web IR research. Using web page non-content feature analysis, we proposed a clustering-based method to select high quality pages from the whole page set. Although the result page set contains only 44.3% of the whole collection, it is related with more than 98% of links and covers about 90% of key information. Link property and retrieval affects are also observed and experiment results show that key resource selection method is more suitable for the job of data cleansing and the result page set outperforms the whole collection by smaller size and better retrieval performance.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1136–1137},
numpages = {2},
keywords = {non-content feature, web IR, web data cleansing},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062907,
author = {Wang, Chuang and Xie, Xing and Wang, Lee and Lu, Yansheng and Ma, Wei-Ying},
title = {Web Resource Geographic Location Classification and Detection},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062907},
doi = {10.1145/1062745.1062907},
abstract = {Rapid pervasion of the web into users' daily lives has put much importance on capturing location-specific information on the web, due to the fact that most human activities occur locally around where a user is located. This is especially true in the increasingly popular mobile and local search environments. Thus, how to correctly and effectively detect locations from web resources has become a key challenge to location-based web applications. In this paper, we first explicitly distinguish the locations of web resources into three types to cater to different application needs: 1) provider location; 2) content location; and 3) serving location. Then we describe a unified system that computes each of the three locations, employing a set of algorithms and different geographic sources.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1138–1139},
numpages = {2},
keywords = {geographic location, serving location, content location, web location, location-based web application, provider location},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062908,
author = {Verbert, Katrien and Ga\v{s}evi\'{c}, Dragan and Jovanovi\'{c}, Jelena and Duval, Erik},
title = {Ontology-Based Learning Content Repurposing},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062908},
doi = {10.1145/1062745.1062908},
abstract = {This paper investigates basic research issues that need to be addressed for developing an architecture that enables repurposing of learning objects in a flexible way. Currently, there are a number of Learning Object Content Models (e.g. the SCORM Content Aggregation Model) that define learning objects and their components in a more or less precise way. However, these models do not allow repurposing of fine-grained components (sentences, images). We developed an ontology-based solution for content repurposing. The ontology is a solid basis for an architecture that will enable on-the-fly access to learning object components and that will facilitate repurposing these components.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1140–1141},
numpages = {2},
keywords = {metadata, learning objects, ontologies, content models, repurposing},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062909,
author = {Hua, Zhigang and Liu, Hao and Xie, Xing and Lu, Hanqing and Ma, Wei-Ying},
title = {Representing Personal Web Information Using a Topic-Oriented Interface},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062909},
doi = {10.1145/1062745.1062909},
abstract = {Nowadays, Web activities have become daily practice for people. It is therefore essential to organize and present this continuously increasing Web information in a more usable manner. In this paper, we developed a novel approach to reorganize personal Web information as a topic-oriented interface. In our approach, we proposed to utilize anchor, title and URL information to represent content information for the browsed Web pages rather than the content body. Furthermore, we explored three methods to organize personal Web information: 1) top-down statistical clustering; 2) salience phrase based clustering; and 3) support vector machine (SVM) based classification. Finally, we conducted a usability study to verify the effectiveness of our proposed solution. The experimental results demonstrated that users could visit the pages that have been browsed previously more easily with our approach than existing solutions.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1142–1143},
numpages = {2},
keywords = {user interface, clustering, topic classfication, user information mining, personal web information},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062910,
author = {Nadamoto, Akiyo and Hayashi, Masaki and Tanaka, Katsumi},
title = {Web2Talkshow: Transforming Web Content into TV-Program-like Content Based on the Creation of Dialogue},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062910},
doi = {10.1145/1062745.1062910},
abstract = {We propose a new browsing system called "Web2Talkshow". It transforms declarative-based web content into humorous dialog-based TV-program-like content that is presented through cartoon animation and synthesized speech. The system does this based on keywords in the original web content. Web2Talkshow enable users to get desired web content easily, pleasantly, and in a user-friendly way while being able to continue working on other tasks. Thus, using it will be much like watching TV.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1144–1145},
numpages = {2},
keywords = {dialogue, TV-program-like content, humor, web content},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062911,
author = {Centeno, Vicente Luque and Kloos, Carlos Delgado and Gaedke, Martin and Nussbaumer, Martin},
title = {WCAG Formalization with W3C Standards},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062911},
doi = {10.1145/1062745.1062911},
abstract = {Web accessibility consists on a set of checkpoints which are rather expensive to evaluate or to spot. However, using W3C technologies, this cost can be clearly minimized. This article presents a W3C formalized rule-set version for automatable checkpoints from WCAG 1.0.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1146–1147},
numpages = {2},
keywords = {XPath, XPointer, XQuery, WAI, WCAG},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062912,
author = {Ehrig, Marc and Staab, Steffen and Sure, York},
title = {Bootstrapping Ontology Alignment Methods with APFEL},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062912},
doi = {10.1145/1062745.1062912},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1148–1149},
numpages = {2},
keywords = {mapping, ontology, matching, alignment, machine learning},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062913,
author = {Yin, Xinyi and Lee, Wee Sun},
title = {Understanding the Function of Web Elements for Mobile Content Delivery Using Random Walk Models},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062913},
doi = {10.1145/1062745.1062913},
abstract = {In this paper, we describe a method for understanding the function of web elements. It classifies web elements into five functional categories: Content (C), Related Links (R), Navigation and Support (N), Advertisement (A) and Form (F). We construct five graphs for a web page, and each graph is designed such that most of the probability mass of the stationary distribution is concentrated in nodes belong to its corresponding category. We perform random walks on these graphs until convergence and classify based on its rank value in different graphs. Our experiment shows that the new method performed very well comparing to basic machine learning methods.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1150–1151},
numpages = {2},
keywords = {classification, HTML, WWW (world wide web)},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062914,
author = {Quint, Julien and Apel, Ulrich},
title = {Does Learning How to Read Japanese Have to Be so Difficult: And Can the Web Help?},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062914},
doi = {10.1145/1062745.1062914},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1152–1153},
numpages = {2},
keywords = {SVG, reading help, Japanese, kanji, graphetic dictionary},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062915,
author = {Lee, Juhnyoung and Goodwin, Richard},
title = {The Semantic Webscape: A View of the Semantic Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062915},
doi = {10.1145/1062745.1062915},
abstract = {It has been a few years since the semantic Web was initiated by W3C, but its status has not been quantitatively measured. It is crucial to understand the status at this early stage, for researchers, developers and administrators to gain insight into what will come in this field. The objective of our work is to quantitatively measure and present the status of the semantic Web. We conduct a longitudinal study on the semantic Web pages to track trends in the use of semantic markup languages. This paper presents early results of this study with two historical data sets from October 2003 and October 2004. Our results show that while it is very early stage of semantic Web adoption, its growth outpaces that of the entire Web for the period. Also, RDF (Resource Description Framework) has dominated among semantic markup languages, taking about 98% of all semantic pages on the Web. It has been used in a variety of metadata annotation applications. This study shows that the most popular application is RSS (RDF Site Summary) for syndicating news and blogs, which takes more than 60% of all semantic Web pages. It also shows that the use of OWL (Web Ontology Language) which was recommended by W3C in early 2004 has been increased 900% for the period.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1154–1155},
numpages = {2},
keywords = {RSS, semantic web, markup languages, ontology},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062916,
author = {Gaedke, Martin and Meinecke, Johannes and Nussbaumer, Martin},
title = {A Modeling Approach to Federated Identity and Access Management},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062916},
doi = {10.1145/1062745.1062916},
abstract = {As the Web is increasingly used as a platform for heterogeneous applications, we are faced with new requirements to authentication, authorization and identity management. Modern architectures have to control access not only to single, isolated systems, but to whole business-spanning federations of applications and services. This task is complicated by the diversity of today's specifications concerning e.g. privacy, system integrity and distribution in the web. As an approach to such problems, in this paper, we introduce a solution catalogue of reusable building blocks for Identity and Access Management (IAM). The concepts of these blocks have been realized in a configurable system that supports IAM solutions for Web-based applications.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1156–1157},
numpages = {2},
keywords = {reuse, identity and access management, federation, security},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062917,
author = {Braga, Daniele and Campi, Alessandro and Cappa, Roberto and Salvi, Damiano},
title = {XSLT by Example},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062917},
doi = {10.1145/1062745.1062917},
abstract = {XQBE (XQuery By Example, [1]), a visual dialect of XQuery, uses hierarchical structures to express transformations between XML documents. XSLT, the standard transformation language for XML, is increasingly popular among programmers and Web developers for separating the application and presentation layers of Web applications. However, its syntax and its rule-based execution paradigm are rather intricate, and the number of XSLT experts is limited; the availability of easier "dialects" could be extremely valuable and may contribute to the adoption of XML for developing data-centered Web applications and services. With this motivation in mind, we adapted XQBE to serve as a visual interface for expressing XML-to-XML transformations and generate the XSLT code that performs such transformations.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1158–1159},
numpages = {2},
keywords = {XML, visual query languages, semi-structured data, XQuery},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062918,
author = {Di Noia, T. and Di Sciascio, E. and Donini, F. M. and Ragone, A. and Colucci, S.},
title = {Automated Semantic Web Services Orchestration via Concept Covering},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062918},
doi = {10.1145/1062745.1062918},
abstract = {We exploit the recently proposed Concept Abduction inference service in Description Logics to solve Concept Covering problems. We propose a framework and polynomial greedy algorithm for semantic based automated Web service orchestration, fully compliant with Semantic Web technologies. We show the proposed approach is able to deal with not exact solutions, computing an approximate orchestration with respect to an agent request modeled a subset of OWL-DL.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1160–1161},
numpages = {2},
keywords = {semantic web services, orchestration, description logics, semantic web},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062919,
author = {Vagena, Zografoula and Koudas, Nick and Srivastava, Divesh and Tsotras, Vassilis J.},
title = {Answering Order-Based Queries over XML Data},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062919},
doi = {10.1145/1062745.1062919},
abstract = {Order-based queries over XML data include XPath navigation axes such as following-sibling and following. In this paper, we present holistic algorithms that evaluate such order-based queries. An experimental comparison with previous approaches shows the performance benefits of our algorithms.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1162–1163},
numpages = {2},
keywords = {order-based queries, XML, holistic algorithms},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062920,
author = {Blake, M. Brian and Fado, David H. and Mack, Gregory A.},
title = {A Publish and Subscribe Collaboration Architecture for Web-Based Information},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062920},
doi = {10.1145/1062745.1062920},
abstract = {Markup languages, representations, schemas, and tools have significantly increased the ability for organizations to share their information. Languages, such as the Extensible Markup Language (XML), provide a vehicle for organizations to represent information in a common, machine-interpretable format. Although these approaches facilitate the collaboration and integration of inter-organizational information, the reality is that the schema representations behind these languages are reasonably difficult to learn, and automated schema integration (without semantics or ontology mappings) is currently an open problem. In this paper, we introduce an architecture and service-oriented infrastructure to facilitate organizational collaboration that combines the push features of the publish/subscribe protocol with storage of distributed registry capabilities.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1164–1165},
numpages = {2},
keywords = {management of semi-structured data, distributed and heterogeneous information management},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062921,
author = {Canfora, G. and Di Santo, G. and Venturi, G. and Zimeo, E. and Zito, M. V.},
title = {Migrating Web Application Sessions in Mobile Computing},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062921},
doi = {10.1145/1062745.1062921},
abstract = {The capability to change user agent while working is starting to appear in state of the art mobile computing due to the proliferation of different kinds of devices, ranging from personal wireless devices to desktop computers, and to the consequent necessity of migrating working sessions from a device to a more apt one. Research results related to the hand-off at low level are not sufficient to solve the problem at application level. The paper presents a scheme for session hand-off in Web applications which, by exploiting a proxy-based architecture, is able to work without interventions on existing code.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1166–1167},
numpages = {2},
keywords = {session hand-off, mobile computing, web applications},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062922,
author = {Reibman, Amy and Sen, Subhabrata and Van der Merwe, Jacobus},
title = {Video Quality Estimation for Internet Streaming},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062922},
doi = {10.1145/1062745.1062922},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1168–1169},
numpages = {2},
keywords = {network measurement, streaming, performance, video quality},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062923,
author = {Denaux, Ronald and Aroyo, Lora and Dimitrova, Vania},
title = {An Approach for Ontology-Based Elicitation of User Models to Enable Personalization on the Semantic Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062923},
doi = {10.1145/1062745.1062923},
abstract = {A novel framework for eliciting a user's conceptualization based on an ontology-driven dialog is presented here. It has been integrated in an RDF/OWL-based architecture of an adaptive learning content management system. The implemented framework is illustrated with an application scenario to deal with the cold start problem and to enable tailoring the system's behavior to the needs of each individual user.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1170–1171},
numpages = {2},
keywords = {application of semantic web technologies, personalization on the semantic web, adaptive content management, user modeling},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062924,
author = {Glance, Natalie and Hurst, Matthew and Nigam, Kamal and Siegler, Matthew and Stockton, Robert and Tomokiyo, Takashi},
title = {Analyzing Online Discussion for Marketing Intelligence},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062924},
doi = {10.1145/1062745.1062924},
abstract = {We present a system that gathers and analyzes online discussion as it relates to consumer products. Weblogs and online message boards provide forums that record the voice of the public. Woven into this discussion is a wide range of opinion and commentary about consumer products. Given its volume, format and content, the appropriate approach to understanding this data is large-scale web and text data mining. By using a wide variety of state-of-the-art techniques including crawling, wrapping, text classification and computational linguistics, online discussion is gathered and annotated within a framework that provides for interactive analysis that yields marketing intelligence for our customers.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1172–1173},
numpages = {2},
keywords = {content systems, text mining, machine learning, information retrieval, computational linguistics},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062925,
author = {Rocco, Daniel and Caverlee, James and Liu, Ling and Critchlow, Terence},
title = {Exploiting the Deep Web with DynaBot: Matching, Probing, and Ranking},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062925},
doi = {10.1145/1062745.1062925},
abstract = {We present the design of Dynabot, a guided Deep Web discovery system. Dynabot's modular architecture supports focused crawling of the Deep Web with an emphasis on matching, probing, and ranking discovered sources using two key components: service class descriptions and source-biased analysis. We describe the overall architecture of Dynabot and discuss how these components support effective exploitation of the massive Deep Web data available.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1174–1175},
numpages = {2},
keywords = {deep web, probing, service class, crawling},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062926,
author = {Jensen, Eric C. and Beitzel, Steven M. and Frieder, Ophir and Chowdhury, Abdur},
title = {A Framework for Determining Necessary Query Set Sizes to Evaluate Web Search Effectiveness},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062926},
doi = {10.1145/1062745.1062926},
abstract = {We describe a framework of bootstrapped hypothesis testing for estimating the confidence in one web search engine outperforming another over any randomly sampled query set of a given size. To validate this framework, we have constructed and made available a precision-oriented test collection consisting of manual binary relevance judgments for each of the top ten results of ten web search engines across 896 queries and the single best result for each of those queries. Results from this bootstrapping approach over typical query set sizes indicate that examining repeated statistical tests is imperative, as a single test is quite likely to find significant differences that do not necessarily generalize. We also find that the number of queries needed for a repeatable evaluation in a dynamic environment such as the web is much higher than previously studied.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1176–1177},
numpages = {2},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062927,
author = {Apte, Naresh and Deutsch, Keith and Jain, Ravi},
title = {Wireless SOAP: Optimizations for Mobile Wireless Web Services},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062927},
doi = {10.1145/1062745.1062927},
abstract = {We propose a set of optimization techniques, collectively called Wireless SOAP (WSOAP), to compress SOAP messages transmitted across a wireless link. The Name Space Equivalency technique rests on the observation that exact recovery of compressed messages is not required at the receiver; an equivalent form suffices. The WSDL Aware Encoding technique obtains further savings by utilizing knowledge of the underlying WSDL by means of an offline protocol we define. We summarize the design, implementation and performance of our Wireless SOAP prototype, and show that Wireless SOAP can reduce message sizes by 3x-12x compared to SOAP.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1178–1179},
numpages = {2},
keywords = {services, networks, WSDL, SOAP, web services, wireless, compression, applications},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062928,
author = {Davulcu, Hasan and Vadrevu, Srinivas and Nagarajan, Saravanakumar and Gelgi, Fatih},
title = {METEOR: Metadata and Instance Extraction from Object Referral Lists on the Web},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062928},
doi = {10.1145/1062745.1062928},
abstract = {The Web has established itself as the largest public data repository ever available. Even though the vast majority of information on the Web is formatted to be easily readable by the human eye, "meaningful information" is still largely inaccessible for the computer applications. In this paper we present the METEOR system which utilizes various presentation and linkage regularities from referral lists of various sorts to automatically separate and extract metadata and instance information. Experimental results for the university domain with 12 computer science department Web sites, comprising 361 individual faculty and course home pages indicate that the performance of the metadata and instance extraction averages 85%, 88% F-measure respectively. METEOR achieves this performance without any domain specific engineering requirement.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1180–1181},
numpages = {2},
keywords = {extraction, metadata, instance, semantic, object, web},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062929,
author = {Bayardo, Roberto J. and Sorensen, Jeffrey},
title = {Merkle Tree Authentication of HTTP Responses},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062929},
doi = {10.1145/1062745.1062929},
abstract = {We propose extensions to existing web protocols that allow proofs of authenticity of HTTP server responses, whether or not the HTTP server is under the control of the publisher. These extensions protect users from content that may be substituted by malicious servers, and therefore have immediate applications in improving the security of web caching, mirroring, and relaying systems that rely on untrusted machines [2,4]. Our proposal relies on Merkle trees to support 200 and 404 response authentication while requiring only a single cryptographic hash of trusted data per repository. While existing web protocols such as HTTPS can provide authenticity guarantees (in addition to confidentiality), HTTPS consumes significantly more computational resources, and requires that the hosting server act without malice in generating responses and in protecting the publisher's private key.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1182–1183},
numpages = {2},
keywords = {web content distribution, merkle hash tree, authenticity},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062930,
author = {Fujii, Atsushi and Itou, Katunobu and Ishikawa, Tetsuya},
title = {Cyclone: An Encyclopedic Web Search Site},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062930},
doi = {10.1145/1062745.1062930},
abstract = {We propose a Web search site called "Cyclone", in which a user can retrieve encyclopedic term descriptions on the Web. Cyclone searches the Web for headwords and page fragments describing the headwords. High-quality page fragments are selected as term descriptions and are classified into domains. The number of current headwords is over 700,000.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1184–1185},
numpages = {2},
keywords = {web search, organization, extraction, encyclopedias},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062931,
author = {Pistore, M. and Traverso, P. and Bertoli, P. and Marconi, A.},
title = {Automated Synthesis of Executable Web Service Compositions from BPEL4WS Processes},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062931},
doi = {10.1145/1062745.1062931},
abstract = {We propose a technique for the automated synthesis of new composite web services. Given a set of abstract bpel4ws descriptions of component services, and a composition requirement, we automatically generate a concrete bpel4ws process that, when executed, interacts with the components and satisfies the requirement.We implement the proposed approach exploiting efficient representation techniques, and we show its scalability over case studies taken from a real world application and over a parameterized domain.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1186–1187},
numpages = {2},
keywords = {web service composition, automated synthesis, business processes},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062932,
author = {Ou, Jian-Chih and Lee, Chang-Hung and Chen, Ming-Syan},
title = {Web Log Mining with Adaptive Support Thresholds},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062932},
doi = {10.1145/1062745.1062932},
abstract = {With the fast increase in Web activities, Web data mining has recently become an important research topic. However, most previous studies of mining path traversal patterns are based on the model of a uniform support threshold without taking into consideration such important factors as the length of a pattern, the positions of Web pages, and the importance of a particular pattern, etc. In view of this, we study and apply the Markov chain model to provide the determination of support threshold of Web documents. Furthermore, by properly employing some techniques devised for joining reference sequences, a new mining procedure of Web traversal patterns is proposed in this paper.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1188–1189},
numpages = {2},
keywords = {Markov model, path traversal pattern, web mining},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062933,
author = {Li, Jun and Furuse, Kazutaka and Yamaguchi, Kazunori},
title = {Focused Crawling by Exploiting Anchor Text Using Decision Tree},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062933},
doi = {10.1145/1062745.1062933},
abstract = {Focused crawlers are considered as a promising way to tackle the scalability problem of topic-oriented or personalized search engines. To design a focused crawler, the choice of strategy for prioritizing unvisited URLs is crucial. In this paper, we propose a method using a decision tree on anchor texts of hyperlinks. We conducted experiments on the real data sets of four Japanese universities and verified our approach.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1190–1191},
numpages = {2},
keywords = {anchor text, shortest path, focused crawling, decision tree learning},
location = {Chiba, Japan},
series = {WWW '05}
}

@inproceedings{10.1145/1062745.1062934,
author = {Murata, Makoto},
title = {One Project, Four Schema Languages: Medley or Melee?},
year = {2005},
isbn = {1595930515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062745.1062934},
doi = {10.1145/1062745.1062934},
abstract = {This talk first gives an overview of an XML project for e-Local Governments, which is under the auspices of MIAC (Ministry of Internal Affairs and Communications) of Japan. This talk then focuses on schema authoring and user interfaces. In particular, the use of four schema languages, namely RELAX NG, W3C XML Schema, DTD, and Schematron, is highlighted.},
booktitle = {Special Interest Tracks and Posters of the 14th International Conference on World Wide Web},
pages = {1192},
numpages = {1},
location = {Chiba, Japan},
series = {WWW '05}
}

