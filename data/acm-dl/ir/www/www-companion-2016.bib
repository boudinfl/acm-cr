@inproceedings{10.5555/2872518.3251207,
author = {Stringhini, Gianluca and Pan, Jeff Z.},
title = {Session Details: Posters},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the Poster Track, associated with WWW 2016.The poster track is a forum to foster interactions among researchers and practitioners, by allowing them to present their new and innovative work in-progress. By presenting their ideas to the WWW 2016, researchers will have a chance to collect feedback from the WWW community and start fruitful conversations. We have a wide variety of topics in the poster track, which cover many topics of interest to the WWW community. We hope that you will enjoy attending the track, and you will find it useful for your future research.The call for papers attracted submissions from United States and Europe. The program committee reviewed and accepted the following: Full Technical Papers Reviewed 182 Accepted 72.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {4},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889405,
author = {Abdollahi, Behnoush and Nasraoui, Olfa},
title = {Explainable Matrix Factorization for Collaborative Filtering},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889405},
doi = {10.1145/2872518.2889405},
abstract = {Explanations have been shown to increase the user's trust in recommendations in addition to providing other benefits such as scrutability, which is the ability to verify the validity of recommendations. Most explanation methods are designed for classical neighborhood-based Collaborative Filtering (CF) or rule-based methods. For the state of the art Matrix Factorization (MF) recommender systems, recent explanation methods, require an additional data source, such as item content data, in addition to rating data. In this paper, we address the case where no such additional data is available and propose a new Explainable Matrix Factorization (EMF) technique that computes an accurate top-$n$ recommendation list of items that are explainable. We also introduce new explanation quality metrics, that we call Mean Explainability Precision (MEP) and Mean Explainability Recall (MER).},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {5–6},
numpages = {2},
keywords = {recommender systems, explanations, matrix factorization (mf), collaborative filtering (cf)},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889390,
author = {Atre, Medha},
title = {For the DISTINCT Clause of SPARQL Queries},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889390},
doi = {10.1145/2872518.2889390},
abstract = {Evaluating SPARQL queries with the DISTINCT clause may become memory intensive due to the requirement of additional auxiliary data structures, like hash-maps, to discard the duplicates. DISTINCT queries make up to 16% of all the queries (e.g., DBPedia), and thus are non-negligible. In this poster we propose a novel method for such queries, by just manipulating the compressed bit-vector indexes called BitMats, for acyclic basic graph pattern (BGP) queries.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {7–8},
numpages = {2},
keywords = {boolean matrix multiplication, bit-vectors, sparql distinct, memory optimization},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889383,
author = {Bai, Xiaomei and Zhang, Jun and Cui, Hai and Ning, Zhaolong and Xia, Feng},
title = {PNCOIRank: Evaluating the Impact of Scholarly Articles with Positive and Negative Citations},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889383},
doi = {10.1145/2872518.2889383},
abstract = {Evaluating the impact of an article is a significant topic and has attracted extensive attention. Citation-based assessment methods currently face a limitation, i.e. the anomalous citations patterns still remain poorly understand. To remedy this drawback, we propose a Positive and Negative Conflict of Interest (COI)-based Rank algorithm, named PNCOIRank, to acquire positive COI, negative COI, positive suspected COI and negative suspected COI relationships. We investigate the citation relationships by the following scholarly factors: citing times, the interval of citing time, collaboration times, the interval of collaboration time, and team of citing authors with the purpose of weakening the COI relationships in citation network. A weighted PageRank is finally constructed and employed, with HITS algorithm to assess the impact of articles. Through experiments on American Physical Society (APS) dataset, we show that PNCOIRank significantly outperforms the existing methods in terms of recommendation intensity.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {9–10},
numpages = {2},
keywords = {citation analysis, team relationships, conflict of interest},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889414,
author = {Basak, Rajesh and Ganguly, Niloy and Sural, Shamik and Ghosh, Soumya K.},
title = {Look Before You Shame: A Study on Shaming Activities on Twitter},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889414},
doi = {10.1145/2872518.2889414},
abstract = {Online social networks (OSNs) are often flooded with scathing remarks against individuals or businesses on their perceived wrongdoing. This paper studies three such events to get insight into various aspects of shaming done through twitter. An important contribution of our work is categorization of shaming tweets, which helps in understanding the dynamics of spread of online shaming events. It also facilitates automated segregation of shaming tweets from non-shaming ones.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {11–12},
numpages = {2},
keywords = {twitter, public shaming, online social networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889409,
author = {Basharat, Amna and Arpinar, I. Budak and Rasheed, Khaled},
title = {Leveraging Crowdsourcing for the Thematic Annotation of the Qur'an},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889409},
doi = {10.1145/2872518.2889409},
abstract = {In this paper, we illustrate how we leverage crowdsourcing to create workflows for knowledge engineering in specialized and knowledge intensive domains. We undertake the special case of the Arabic script of the Qur'an, a widely studied manuscript, and attempt to employ crowdsourcing methods for its thematic annotation at the sub-verse level, for which, there is no standardized knowledge model available to date. We demonstrate that our proposed method presents feasibility to achieve reliable annotations in an efficient and scalable manner. The proposed methodology and framework is meant to be generalizable to other knowledge intensive and specialized domains.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {13–14},
numpages = {2},
keywords = {qur'an, disambiguation, semantic web, crowdsourcing, thematic annotation, ontology, knowledge engineering},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889351,
author = {Bayir, Murat Ali and Toroslu, Ismail Hakki},
title = {Finding All Maximal Paths In Web User Sessions},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889351},
doi = {10.1145/2872518.2889351},
abstract = {This paper introduces a new method for the session construction problem, which is the first main step of the web usage mining process. The proposed method is capable of extracting all possible maximal navigation sequences of web users. Through experiments, it is shown that when our new technique is used, it outperforms previous approaches in web usage mining applications such as next-page prediction.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {15–16},
numpages = {2},
keywords = {web mining, linked data, graph theory},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889380,
author = {Becker, Martin and Mewes, Hauke and Hotho, Andreas and Dimitrov, Dimitar and Lemmerich, Florian and Strohmaier, Markus},
title = {SparkTrails: A MapReduce Implementation of HypTrails for Comparing Hypotheses About Human Trails},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889380},
doi = {10.1145/2872518.2889380},
abstract = {HypTrails is a bayesian approach for comparing different hypotheses about human trails on the web. While a standard implementation exists, it exposes performance issues when working with large-scale data. In this paper, we propose a distributed implementation of HypTrails based on Apache Spark taking advantage of several structural properties inherent to HypTrails. The performance improves substantially. Our implementation is publicly available.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {17–18},
numpages = {2},
keywords = {paths, web, sequences, apache spark, distributed computing, bayesian statistics, human trails, markov chain, sequential human behavior, hypotheses, mapreduce},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889416,
author = {Chaudhury, Atef and Kim, Myunghwan and Tiwari, Mitul},
title = {Importance of First Steps in a Community for Growth, Diversity, and Engagement},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889416},
doi = {10.1145/2872518.2889416},
abstract = {In both the online and offline world, networks that people form within certain communities are critical for their engagement and growth in those communities. In this work, we analyze the growth of ego-networks on LinkedIn for new employees of companies, and study how the pattern of network formation in the new company affects one's growth and engagement in that company. We observe that the initial state of ego-network growth in a newly joined company shows strong correlations with the future status in the company -- such as network size, network diversity, and retention. We also present some key patterns that demonstrate the importance of the first few connections in the new company as well as how they lead to the phenomena we observed.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {19–20},
numpages = {2},
keywords = {social engagement, social network, network growth, organizational behavior},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889349,
author = {Chen, Xu and Wang, Pengfei and Qin, Zheng and Zhang, Yongfeng},
title = {HLBPR: A Hybrid Local Bayesian Personal Ranking Method},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889349},
doi = {10.1145/2872518.2889349},
abstract = {Bayesian Personal Ranking(BPR) method is a well-known model due to its high performance in the task of item recommendation. However, this method fail to distinguish user preference among the non-interacted items. In this paper, to enhance traditional BPR's performance, we introduce and analyse a hybrid method, namely Hybrid Local Bayesian Personal Ranking method(HLBPR for short). Our main idea is to construct additional item preference pairs among the products which haven't been purchased, and then utilize the extened pairs to optimize the ranking object. Experiments on two real-world transaction datasets demonstrated the effectiveness of our approach as compared with the state-of-the-art methods.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {21–22},
numpages = {2},
keywords = {item preference pairs, hybrid method, local similarity},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889358,
author = {de Ribaupierre, H\'{e}l\`{e}ne and Osborne, Francesco and Motta, Enrico},
title = {Combining NLP And Semantics For Mining Software Technologies From Research Publications},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889358},
doi = {10.1145/2872518.2889358},
abstract = {The natural language processing (NLP) community has developed a variety of methods for extracting and disambiguating information from research publications. However, they usually focus only on standard research entities such as authors, affiliations, venues, references and keywords. We propose a novel approach, which combines NLP and semantic technologies for generating from the text of research publications an OWL ontology describing software technologies used or introduced by researchers, such as applications, systems, frameworks, programming languages, and formats. The method was tested on a sample of 300 publications in the Semantic Web field, yielding promising results.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {23–24},
numpages = {2},
keywords = {data mining, semantic web, nlp, digital library, artificial intelligence, ontology, information extraction},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889408,
author = {Di Iorio, Angelo and Gonzalez-Beltran, Alejandra and Osborne, Francesco and Peroni, Silvio and Poggi, Francesco and Vitali, Fabio},
title = {It ROCS! The RASH Online Conversion Service},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889408},
doi = {10.1145/2872518.2889408},
abstract = {In this poster paper we introduce the RASH Online Conversion Service, i.e., a Web application that allows the conversion of ODT documents into RASH, a HTML-based markup language for writing scholarly articles, and from RASH into LaTeX according to Springer LNCS and ACM ICPS.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {25–26},
numpages = {2},
keywords = {rash, html-based format, scholarly html},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889388,
author = {Dimitrov, Dimitar and Singer, Philipp and Lemmerich, Florian and Strohmaier, Markus},
title = {Visual Positions of Links and Clicks on Wikipedia},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889388},
doi = {10.1145/2872518.2889388},
abstract = {In this work, we study the visual position of links and their clicks on Wikipedia, particularly where links are visually located, at which screen positions users click on links, and which areas on the screen exhibit more or less clicks per links. For that purpose, we introduce a novel dataset containing the on-screen coordinate position for all links between pages in the English Wikipedia and additionally resort to navigation logs of Wikipedia users. Using this data, we can observe a preference of certain link and click locations on Wikipedia including first evidence of positional click bias. For example, our results suggest that users have a tendency to prefer to click on the left side of the screen which exceeds what one would expect from the presence of links on pages. We believe that presented data and research can be useful for optimizing the process of link creation and link consumption on Wikipedia and other Web platforms.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {27–28},
numpages = {2},
keywords = {human click behavior, wikipedia, navigation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889403,
author = {Du, Hui and Xu, Xueke and Cheng, Xueqi and Wu, Dayong and Liu, Yue and Yu, Zhihua},
title = {Aspect-Specific Sentimental Word Embedding for Sentiment Analysis of Online Reviews},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889403},
doi = {10.1145/2872518.2889403},
abstract = {Recently, Deep Convolutional Neural Networks (CNNs) have been widely applied to sentiment analysis of short texts. Naturally, word embedding techniques are used to learn continuous word representations for constructing sentence matrix as input to CNN. As for sentiment analysis of customer reviews, we argue that it is problematic to learn a single representation for a word while ignoring sentiment information and the discussed aspects. In this poster, we propose a novel word embedding model to learn sentimental word embedding given specific aspects by modeling both sentiment and syntactic context under the specific aspects. We apply our method as input to CNN for sentiment analysis in multiple domains. Experiments show that the CNN based on the proposed model can consistently achieve superior performance compared to CNN based on traditional word embedding method.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {29–30},
numpages = {2},
keywords = {sentiment analysis, online reviews, word embedding, cnn},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889410,
author = {Duric, Nemanja and Grbovic, Mihajlo and Radosavljevic, Vladan and Savla, Jaikit and Bhagwan, Varun and Sharp, Doug},
title = {Travel the World: Analyzing and Predicting Booking Behavior Using E-Mail Travel Receipts},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889410},
doi = {10.1145/2872518.2889410},
abstract = {Tourism industry has grown tremendously in the previous several decades. Despite its global impact, there still remain a number of open questions related to better understanding of tourists and their habits. In this work we analyze the largest data set of travel receipts considered thus far, and focus on exploring and modeling booking behavior of online customers. We extract useful, actionable insights into the booking behavior, and tackle the task of predicting the booking time. The presented results can be directly used to improve booking experience of customers and optimize targeting campaigns of travel operators.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {31–32},
numpages = {2},
keywords = {travel analysis, e-mail receipts, travel receipts},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889389,
author = {Gao, Jinhua and Shen, Huawei and Liu, Shenghua and Cheng, Xueqi},
title = {Modeling and Predicting Retweeting Dynamics via a Mixture Process},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889389},
doi = {10.1145/2872518.2889389},
abstract = {Modeling and predicting retweeting dynamics in social media has important implications to an array of applications. Existing models either fail to model the triggering effect of retweeting dynamics, e.g., the model based on reinforced Poisson process, or are hard to be trained using only the retweeting dynamics of individual tweet, e.g., the model based on self-exciting Hawkes process. In this paper, motivated by the observation that each retweeting dynamics is generally dominated by a handful of key nodes that separately trigger a high number of retweets, we propose a mixture process to model and predict retweeting dynamics, with each subprocess capturing the retweeting dynamics initiated by a key node. Experiments demonstrate that the proposed model outperforms the state-of-the-art model.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {33–34},
numpages = {2},
keywords = {retweeting dynamics, popularity prediction, mixture process},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2893188,
author = {Ghosh, Shreya and Ghosh, Soumya K.},
title = {THUMP: Semantic Analysis on Trajectory Traces to Explore Human Movement Pattern},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2893188},
doi = {10.1145/2872518.2893188},
abstract = {Exploring human movement pattern from raw GPS traces is an interesting and challenging task. This paper aims at analysing a large volume of GPS data in spatio-temporal context, clustering trajectories using geographic and semantic location information and identifying different categories of people. It tries to exploit the fact that human moves with an intent. The proposed framework yields encouraging results using a large scale GPS dataset of Microsoft GeoLife.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {35–36},
numpages = {2},
keywords = {trajectory, categorization, gps data, geo-tagging, geocoding, clustering},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889360,
author = {Gilani, Zafar and Wang, Liang and Crowcroft, Jon and Almeida, Mario and Farahbakhsh, Reza},
title = {Stweeler: A Framework for Twitter Bot Analysis},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889360},
doi = {10.1145/2872518.2889360},
abstract = {The WWW has seen a massive growth in variety and usage of OSNs. The rising population of users on Twitter and its open nature has made it an ideal platform for various kinds of opportunistic pursuits, such as news and emergency communication, business promotion, political campaigning, spamming and spreading malicious content. Most of these opportunistic pursuits are exploited through automated programs, known as bots. In this study we propose a framework (Stweeler) to study bot impact and influence on Twitter from systems and social media perspectives.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {37–38},
numpages = {2},
keywords = {information dissemination, bot analyser, content analysis},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889377,
author = {H\'{e}on, Michel and Nkambou, Roger and Langheit, Christian},
title = {Toward G-OWL: A Graphical, Polymorphic And Typed Syntax For Building Formal OWL2 Ontologies},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889377},
doi = {10.1145/2872518.2889377},
abstract = {The Web Ontology Language (OWL-2) aims at offering a family of syntax such as RDF/XML, Manchester Turtle and others, for building ontologies. Ontology engineering is a complex task that requires skills that are rarely accessible to content experts. On the other hand, to model contents pertaining to a specific domain, graphical modeling is a technique that is often used to offer a knowledge representation tool to content experts that are not well acquainted with the process of formal ontology design. In this paper, we present the way in which the usage of polymorphism and symbol typing of graphical vocabulary have allowed us to design the G-OWL syntax, a graphical syntax that aims to graphically represent domain-specific knowledge using the OWL-2.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {39–40},
numpages = {2},
keywords = {graphical ontological syntax, graphical syntax, owl-2, graphical ontology, ontology, visual modeling},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889369,
author = {Herbelot, Aurelie},
title = {PeARS: A Peer-to-Peer Agent for Reciprocated Search},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889369},
doi = {10.1145/2872518.2889369},
abstract = {This paper presents PeARS (Peer-to-peer Agent for Reciprocated Search), an algorithm for distributed Web search that emulates the offline behaviour of a human with an information need. Specifically, the algorithm models the process of 'calling a friend', i.e. directing one's query to the knowledge holder most likely to answer it. The system allows network users to index and share part of their browsing history in a way that makes them 'experts' on some topics. A layer of distributional semantics agents then performs targeted information retrieval in response to user queries, in a fully automated fashion.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {41–42},
numpages = {2},
keywords = {distributed systems, web search, distributional semantics},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889415,
author = {Hmimida, Manel and Kanawati, Rushed},
title = {A Graph-Coarsening Approach for Tag Recommendation},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889415},
doi = {10.1145/2872518.2889415},
abstract = {In this paper we propose a new graph-based tag recommendation approach. The approach is structured into an offline step and an online one. Offline, the hypergraph depicting the history of tags assignment by users to resources is abstracted. On online, for a given target user and a resource, we first compute the set of recommended abstract tags (i.e tag clusters) applying a basic graph-based approach to the abstract graph. A new reduced graph is computed by unfolding the abstract subgraph composed of the set of recommended abstract tags and nodes representing the cluster of users (resp. resources) to which the target user (resp. resource) belongs to. Again the same basic graph-based tag recommendation approach is applied to this new reduced graph in order to compute the final set of tags to recommend. Experiments on real dataset show the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {43–44},
numpages = {2},
keywords = {multiplex network, tag recommendation, community detection},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889392,
author = {Ho, Tin Kam and Lastras, Luis A. and Shmueli, Oded},
title = {Concept Evolution Modeling Using Semantic Vectors},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889392},
doi = {10.1145/2872518.2889392},
abstract = {We propose a method for a concept-centric semantic analysis of an evolving corpus, highlighting the persistent concepts, emergence of new concepts, and the changes in the semantic associations between concepts. We report our findings on a corpus of computer science literature that spans six decades, revealing interesting patterns about the progress of the discipline.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {45–46},
numpages = {2},
keywords = {natural language understanding, word embedding, distributional semantics, concept analytics, text mining},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889370,
author = {Hui, Kai and Berberich, Klaus},
title = {Cluster Hypothesis in Low-Cost IR Evaluation with Different Document Representations},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889370},
doi = {10.1145/2872518.2889370},
abstract = {Offline evaluation for information retrieval aims to compare the performance of retrieval systems based on relevance judgments for a set of test queries. Since manual judgments are expensive, selective labeling has been developed to semi-automatically label documents, in the wake of the similarity relationship among retrieved documents. Intuitively, the agreement w.r.t the cluster hypothesis can directly determine the amount of manual judgments that can be saved by creating labels with a semi-automatic method. Meanwhile, in representing documents, certain information is lost. We argue that better document representation can lead to better agreement with the cluster hypothesis. To this end, we investigate different document representations on established benchmarks in the context of low-cost evaluation, showing that different document representations vary in how well they capture document similarity relative to a query.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {47–48},
numpages = {2},
keywords = {low-cost evaluation, cluster hypothesis, document representation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889382,
author = {J, Ganesh and Ganguly, Soumyajit and Gupta, Manish and Varma, Vasudeva and Pudi, Vikram},
title = {Author2Vec: Learning Author Representations by Combining Content and Link Information},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889382},
doi = {10.1145/2872518.2889382},
abstract = {In this paper, we consider the problem of learning representations for authors from bibliographic co-authorship networks. Existing methods for deep learning on graphs, such as DeepWalk, suffer from link sparsity problem as they focus on modeling the link information only. We hypothesize that capturing both the content and link information in a unified way will help mitigate the sparsity problem. To this end, we present a novel model 'Author2Vec', which learns low-dimensional author representations such that authors who write similar content and share similar network structure are closer in vector space. Such embeddings are useful in a variety of applications such as link prediction, node classification, recommendation and visualization. The author embeddings we learn are empirically shown to outperform DeepWalk by 2.35% and 0.83% for link prediction and clustering task respectively.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {49–50},
numpages = {2},
keywords = {network mining, author modeling, deep learning, author embedding, representation learning, distributed representation, machine learning},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889418,
author = {Kaneko, Takamu and Yanai, Keiji},
title = {Visual Event Mining from the Twitter Stream},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889418},
doi = {10.1145/2872518.2889418},
abstract = {Twitter is a unique microblog, which is different from conventional social media in terms of its quickness and on-the-spot-ness. Many Twitter's users send messages, which is commonly called "tweets", to Twitter on the spot with mobile phones or smart phones, and some of them send photos and geotags as well as tweets. Most of the photos are sent to Twitter soon after taken. In case of photos related to some events, most of them are taken during the events. We think that Twitter event photo mining is more useful to under- stand what happens currently over the world than only text-based Twitter event mining.In this paper, we propose a system to mine events visually from the Twitter stream. To do that, we use not only tweets having both geotags and photos but also tweets having geotags or photos for textual analysis or visual analysis.Although there exist many works related to Twitter mining using only text analysis such as typhoon and earthquake detection by Sakaki et al. [1], only a limited number of works exist on Twitter mining using image analysis. Nakaji et al. [2] proposed a system to mine representative photos related to the given keyword or term from a large number of geo-tweet photos. They extracted representative photos related to events such as "typhoon" and "New Year's Day". They used only geotagged photo tweets the number of which are limited compared to all the photo tweets. Gao et al. [3] proposed a method to mine brand product photos fromWeibo which employs supervised image recognition, which is different from event detection. They integrated visual features and social factors (users, relations, and locations) as well as textual features for brand product photo mining.In this paper, we detect visual events using geotagged non-photo tweets and non-geotagged photo tweets as well as geotagged photo tweets. In the experiments, we show some examples of detected events and their photos such as "rainbow", "fireworks" and "festival".},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {51–52},
numpages = {2},
keywords = {web image mining, event detection, twitter mining},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889385,
author = {Karimi, Fariba and Wagner, Claudia and Lemmerich, Florian and Jadidi, Mohsen and Strohmaier, Markus},
title = {Inferring Gender from Names on the Web: A Comparative Evaluation of Gender Detection Methods},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889385},
doi = {10.1145/2872518.2889385},
abstract = {Computational social scientists often harness the Web as a "societal observatory" where data about human social behavior is collected. This data enables novel investigations of psychological, anthropological and sociological research questions. However, in the absence of demographic information, such as gender, many relevant research questions cannot be addressed. To tackle this problem, researchers often rely on automated methods to infer gender from name information provided on the web. However, little is known about the accuracy of existing gender-detection methods and how biased they are against certain sub-populations. In this paper, we address this question by systematically comparing several gender detection methods on a random sample of scientists for whom we know their full name, their gender and the country of their workplace.We further suggest a novel method that employs web-based image retrieval and gender recognition in facial images in order to augment name-based approaches. Our findings show that the performance of name-based gender detection approaches can be biased towards countries of origin and such biases can be reduced by combining name-based an image-based gender detection methods.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {53–54},
numpages = {2},
keywords = {gender detection, computational social science, discrimination on web},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889396,
author = {Kawamae, Noriaki},
title = {Time Series Analysis Using NOC},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889396},
doi = {10.1145/2872518.2889396},
abstract = {We present a time series analysis employing natural language processing (NLP) techniques, and show the effect of N-gram over Context (NOC), that is a one of topic models that enjoy success in NLP, in this analysis.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {55–56},
numpages = {2},
keywords = {nonparametric models, time series analysis, latent variable models, topic models, n-gram topic model},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889364,
author = {Kim, Sungchul and Yeo, Jinyoung and Koh, Eunyee and Lipka, Nedim},
title = {Purchase Influence Mining: Identifying Top-k Items Attracting Purchase of Target Item},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889364},
doi = {10.1145/2872518.2889364},
abstract = {Web logs in e-commerce sites consist of user actions on items such as visiting an item description page, adding an item to a wishlist, and purchasing an item. Those items could be represented as nodes in a graph while viewing their relationships as edges according to the user actions. Based on the item graph, identifying items that attract users to purchase the target item could be practically used for supporting business decisions. To do this, we introduce a new task, called `Purchase Influence Mining', that finds the top-k items (PIM-items) maximizing the estimated purchase influence from them to a target item. We solve this problem by modeling the purchase influence as the shortest path between item pair. According to the result, our approach more consistently finds the k PIM-items than the baseline.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {57–58},
numpages = {2},
keywords = {e-commerce, purchase influence mining, data mining},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889407,
author = {Ko, Yun-Yong and Chae, Dong-Kyu and Kim, Sang-Wook},
title = {Accurate Path-Based Methods for Influence Maximization in Social Networks},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889407},
doi = {10.1145/2872518.2889407},
abstract = {This paper proposes a novel approach to target-oriented influence estimation, which remedies the drawback of state-of-the-art, thereby understanding information diffusion more accurately in a social network.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {59–60},
numpages = {2},
keywords = {social networks, information diffusion, influence maximization},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889384,
author = {Kratky, Peter and Chuda, Daniela},
title = {Estimating Gender and Age of Web Page Visitors from the Way They Use Their Mouse},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889384},
doi = {10.1145/2872518.2889384},
abstract = {Biometric data are affected by physiological properties of people, including gender or age. The paper describes an experiment of discovering gender and age in computer mouse movement data that might be notably beneficial for profiling anonymous visitors browsing the Web. The proposed method extracts features, such as velocity, path straightness or pauses duration, that are used by a multiclassifier system to make an estimate. Age category estimation shows encouraging results of the early method, especially for statistical analysis of a website audience.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {61–62},
numpages = {2},
keywords = {user modeling, gender estimation, mouse movement features, soft biometrics, age estimation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889391,
author = {Kumar, Srijan},
title = {Structure and Dynamics of Signed Citation Networks},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889391},
doi = {10.1145/2872518.2889391},
abstract = {Citations are important to track and understand the evolution of human knowledge. At the same time, it is widely accepted that all the citations made in a paper are not equal. However, there is no thorough understanding of how citations are created that explicitly criticize or endorse others. In this paper, we do a detailed study of such citations made within the NLP community by differentiating citations into endorsement (positive), criticism (negative) and neutral categories. We analyse this signed network created between papers and between authors for the first time from a social networks perspective. We make many observations - we find that the citations follow a heavy-tailed distribution and they are created in a way that follows weak balance theory and status theories. Moreover, we find that authors do not change their opinion towards others over time and rarely reciprocate the opinion that they receive. Overall, the paper builds the understanding of the structure and dynamics of positive, negative and neutral citations.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {63–64},
numpages = {2},
keywords = {negative citations, citation networks, bibliometrics, signed citations},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889371,
author = {Lamba, Hemank and Nagarajan, Vaishnavh and Shin, Kijung and Shajarisales, Naji},
title = {Incorporating Side Information in Tensor Completion},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889371},
doi = {10.1145/2872518.2889371},
abstract = {Matrix and tensor completion techniques have proven useful in many applications such as recommender systems, image/video restoration, and web search. We explore the idea of using external information in completing missing values in tensors. In this work, we present a framework that employs side information as kernel matrices for tensor factorization. We apply our framework to problems of recommender systems and video restoration and show that our framework effectively deals with the cold-start problem.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {65–66},
numpages = {2},
keywords = {tensor factorization, kernels, recommender systems},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889412,
author = {Lamba, Hemank and Pfeffer, J\"{u}rgen},
title = {Maximizing the Spread of Positive Influence by Deadline},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889412},
doi = {10.1145/2872518.2889412},
abstract = {Influence maximization has found applications in various fields such as sensor placement, viral marketing, controlling rumor outbreak, etc. In this paper, we propose a targeted approach to influence maximization in polarized networks i.e. networks where we already know or can predict node's opinion about a product or topic. The goal is to find a set of individuals to target, such that positive opinion about a specific topic or the product to be launched is maximized. Another key aspect that is present in most of the existing viral marketing algorithms is that they do not take into account the timeliness of the product adoption. In this paper, we present a framework where we infer the polarity, activity levels of the users, and then select seeds to launch viral marketing campaigns such that positive influence about the product is maximized by the given deadline.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {67–68},
numpages = {2},
keywords = {influence maximization, information diffusion, viral marketing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889397,
author = {Lange Di Cesare, Kevin and Gagnon, Michel and Zouaq, Amal and Jean-Louis, Ludovic},
title = {A Machine Learning Filter for Relation Extraction},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889397},
doi = {10.1145/2872518.2889397},
abstract = {The TAC KBP English slot filling track is an evaluation campaign that targets the extraction of 41 pre-identified relations related to specific named entities. In this work, we present a machine learning filter whose aim is to enhance the precision of relation extractors while minimizing the impact on recall. Our approach aims at filtering relation extractors' output using a binary classifier based on a wide array of features including syntactic, lexical and statistical features. We experimented the classifier on 14 of the 18 participating systems in the TAC KBP English slot filling track 2013. The results show that our filter is able to improve the precision of the best 2013 system by nearly 20% and improve the F1-score for 17 relations out of 33 considered.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {69–70},
numpages = {2},
keywords = {information extraction, slot filling, relation extraction},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889381,
author = {Lee, Yang-Yin and Ke, Hao and Huang, Hen-Hsen and Chen, Hsin-Hsi},
title = {Less is More: Filtering Abnormal Dimensions in GloVe},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889381},
doi = {10.1145/2872518.2889381},
abstract = {GloVe, global vectors for word representation, performs well in some word analogy and semantic relatedness tasks. However, we find that some dimensions of the trained word embedding are abnormal. We verify our conjecture via removing these abnormal dimensions using Kolmogorov-Smimov test and experiment on several benchmark datasets for semantic relatedness measurement. The experimental results confirm our finding. Interestingly, some of the tasks outperform the state-of-the-art model SensEmbed by simply removing these abnormal dimensions. The novel rule of thumb technique which leads to better performance is expected to be useful in practice.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {71–72},
numpages = {2},
keywords = {semantic relatedness, glove, word embedding},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889395,
author = {Lee, Yang-Yin and Ke, Hao and Huang, Hen-Hsen and Chen, Hsin-Hsi},
title = {Combining Word Embedding and Lexical Database for Semantic Relatedness Measurement},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889395},
doi = {10.1145/2872518.2889395},
abstract = {While many traditional studies on semantic relatedness utilize the lexical databases, such as WordNet or Wikitionary, the recent word embedding learning approaches demonstrate their abilities to capture syntactic and semantic information, and outperform the lexicon-based methods. However, word senses are not disambiguated in the training phase of both Word2Vec and GloVe, two famous word embedding algorithms, and the path length between any two senses of words in lexical databases cannot reflect their true semantic relatedness. In this paper, a novel approach that linearly combines Word2Vec and GloVe with the lexical database WordNet is proposed for measuring semantic relatedness. The experiments show that the simple method outperforms the state-of-the-art model SensEmbed.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {73–74},
numpages = {2},
keywords = {word2vec, word embedding, semantic relatedness, glove, wordnet},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889386,
author = {Lehmberg, Oliver and Ritze, Dominique and Meusel, Robert and Bizer, Christian},
title = {A Large Public Corpus of Web Tables Containing Time and Context Metadata},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889386},
doi = {10.1145/2872518.2889386},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {75–76},
numpages = {2},
keywords = {web tables, dataset, table corpus},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889387,
author = {Li, Manling and Jia, Yantao and Wang, Yuanzhuo and Li, Jingyuan and Cheng, Xueqi},
title = {Hierarchy-Based Link Prediction in Knowledge Graphs},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889387},
doi = {10.1145/2872518.2889387},
abstract = {Link prediction over a knowledge graph aims to predict the missing entity h or t for a triple (h,r,t). Existing knowledge graph embedding based predictive methods represent entities and relations in knowledge graphs as elements of a vector space, and employ the structural information for link prediction. However, knowledge graphs contain many hierarchical relations, which existing methods have pay little attention to. In this paper, we propose a hierarchy-constrained locally adaptive knowledge graph embedding based link prediction method, called hTransA, by integrating hierarchical structures into the predictive work. Experiments over two benchmark data sets demonstrate the superiority of hTransA.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {77–78},
numpages = {2},
keywords = {link prediction, hierarchy, knowledge graph embedding},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889350,
author = {Liu, Yuchen and Chechik, Dmitry and Cho, Junghoo},
title = {Power of Human Curation in Recommendation System},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889350},
doi = {10.1145/2872518.2889350},
abstract = {This paper introduces human curation signals and demonstrates incorporating human curation signals improves the relevance of state-of-art recommendation system models by up to 30% by experiments on a large-scale Pinterest dataset.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {79–80},
numpages = {2},
keywords = {human curation, collaborative filtering, recommendation system},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889372,
author = {Morstatter, Fred and Dani, Harsh and Sampson, Justin and Liu, Huan},
title = {Can One Tamper with the Sample API? Toward Neutralizing Bias from Spam and Bot Content},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889372},
doi = {10.1145/2872518.2889372},
abstract = {While social media mining continues to be an active area of research, obtaining data for research is a perennial problem. Even more, obtaining unbiased data is a challenge for researchers who wish to study human behavior, and not technical artifacts induced by the sampling algorithm of a social media site. In this work, we evaluate one social media data outlet that gives data to its users in the form of a stream: Twitter's Sample API. We show that in its current form, this API can be poisoned by bots or spammers who wish to promote their content, jeopardizing the credibility of the data collected through this API. We design a proof-of-concept algorithm that shows how malicious users could increase the probability of their content appearing in the Sample API, thus biasing the content towards spam and bot content and harming the representativity of this data outlet.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {81–82},
numpages = {2},
keywords = {data mining, data sampling, sample bias},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889361,
author = {Nalisnick, Eric and Mitra, Bhaskar and Craswell, Nick and Caruana, Rich},
title = {Improving Document Ranking with Dual Word Embeddings},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889361},
doi = {10.1145/2872518.2889361},
abstract = {This paper investigates the popular neural word embedding method Word2vec as a source of evidence in document ranking. In contrast to NLP applications of word2vec, which tend to use only the input embeddings, we retain both the input and the output embeddings, allowing us to calculate a different word similarity that may be more suitable for document ranking. We map the query words into the input space and the document words into the output space, and compute a relevance score by aggregating the cosine similarities across all the query-document word pairs. We postulate that the proposed Dual Embedding Space Model (DESM) provides evidence that a document is about a query term, in addition to and complementing the traditional term frequency based approach.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {83–84},
numpages = {2},
keywords = {document ranking, word2vec, word embeddings},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889347,
author = {Nguyen, Luan Minh},
title = {Context-Aware Text Representation for Social Relation Aided Sentiment Analysis},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889347},
doi = {10.1145/2872518.2889347},
abstract = {In this paper, we propose CaTER, which learns a novel context-aware joint representation of text and user by incorporating semantic text embedding of unlabeled tweets as well as social relation information. CaTER leverages the wealth of user contextual information available apart from user's utterances for sentiment analysis. Our approach is inspired by social science about emotional behaviors of connected users, who perhaps more likely to consensus on similar opinions. Our method outperforms numerous baselines on two real-world Twitter datasets.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {85–86},
numpages = {2},
keywords = {social relations, deep learning, text embedding, sentiment analysis},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889355,
author = {Okura, Shumpei and Tagami, Yukihiro and Tajima, Akira},
title = {Article De-Duplication Using Distributed Representations},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889355},
doi = {10.1145/2872518.2889355},
abstract = {In news recommendation systems, eliminating redundant information is important as well as providing interesting articles for users. We propose a method that quantifies the similarity of articles based on their distributed representation, learned with the category information as weak supervision. This method is useful for evaluation under tight time constraints, since it only requires low-dimensional inner product calculation for estimating similarities. The experimental results from human evaluation and online performance in A/B testing suggest the effectiveness of our proposed method, especially for quantifying middle-level similarities. Currently, this method is used on Yahoo! JAPAN's front page, which has millions of users per day and billions of page views per month.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {87–88},
numpages = {2},
keywords = {news recommendation, de-duplication, neural network},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889362,
author = {Park, Chanyoung and Kim, Donghyun and Oh, Jinoh and Yu, Hwanjo},
title = {TRecSo: Enhancing Top-k Recommendation With Social Information},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889362},
doi = {10.1145/2872518.2889362},
abstract = {Due to the data sparsity problem, social network information is often additionally used to improve the performance of recommender system. While most existing works exploit social information to reduce the rating prediction error, e.g., RMSE, a few had aimed to improve the top-k ranking prediction accuracy. This paper proposes a novel top-k oriented recommendation method, TRecSo, which incorporates social information into recommendation by modeling two different roles of users as trusters and trustees while considering the structural information of the network. Empirical studies on real-world datasets demonstrate that TRecSo leads to remarkable improvement compared to previous methods in top-k recommendation.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {89–90},
numpages = {2},
keywords = {social network, recommender system, learning-to-rank},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889375,
author = {Pradhan, Dinesh and Chakraborty, Tanmoy and Pandit, Saswata and Nandi, Subrata},
title = {On the Discovery of Success Trajectories of Authors},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889375},
doi = {10.1145/2872518.2889375},
abstract = {Understanding the qualitative patterns of research endeavor of scientific authors in terms of publication count and their impact (citation) is important in order to quantify success trajectories. Here, we examine the career profile of authors in computer science and physics domains and discover at least six different success trajectories in terms of normalized citation count in longitudinal scale. Initial observations of individual trajectories lead us to characterize the authors in each category. We further leverage this trajectory information to build a two-stage stratification model to predict future success of an author at the early stage of her career. Our model outperforms the baseline with an average improvement of 15.68% for both the datasets.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {91–92},
numpages = {2},
keywords = {citation networks, prediction, success trajectories},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889411,
author = {Radosavljevic, Vladan and Grbovic, Mihajlo and Djuric, Nemanja and Bhamidipati, Narayan and Zhang, Daneo and Wang, Jack and Dang, Jiankai and Huang, Haiying and Nagarajan, Ananth and Chen, Peiji},
title = {Smartphone App Categorization for Interest Targeting in Advertising Marketplace},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889411},
doi = {10.1145/2872518.2889411},
abstract = {Last decade has witnessed a tremendous expansion of mobile devices, which brought an unprecedented opportunity to reach a large number of mobile users at any point in time. This resulted in a surge of interest of mobile operators and ad publishers to understand usage patterns of mobile apps and allow more relevant content recommendations. Due to a large input space, a critical step in understanding app usage patterns is reducing sparseness by classifying apps into predefined interest taxonomies. However, besides short name and noisy description majority of apps have very limited information available, which makes classification a challenging task. We address this issue and present a novel method to classify apps into interest categories by: 1) embedding apps into low-dimensional space using a neural language model applied on smartphone logs; and 2) applying k-nearest-neighbors classification in the embedding space. To validate the method we run experiments on more than one billion device logs covering hundreds of thousands of apps. To the best of our knowledge this is the first app categorization study at this scale. Empirical results show that the proposed method outperforms the current state-of-the-art.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {93–94},
numpages = {2},
keywords = {app categorization, interest targeting, app representation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889402,
author = {Ren, Jing and Shen, Jialie and Kauffman, Robert J.},
title = {What Makes a Music Track Popular in Online Social Networks?},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889402},
doi = {10.1145/2872518.2889402},
abstract = {Tens of thousands of music tracks are uploaded to the Internet every day through social networks that focus on music and videos, as well as portal websites. While some of the content has been popular for decades, some tracks that have just been released have been completely ignored. So what makes a music track popular? Can we predict the popularity of a music track before it is released? In this research, we will focus on an online music social network, Last.fm, and investigate three key factors of a music track that may have impact on its popularity. They include: the music content, the artist reputation and the social context of the music. The results suggest that we can predict the future popularity of music with around 80% accuracy using just these three factors. We also found out that in the social networks scenario, the content of the music seems to be an surprisingly important factor that determines the popularity of a track online.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {95–96},
numpages = {2},
keywords = {acoustic content, topic, ugc, music online popularity},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889404,
author = {Sedhai, Surendra and Sun, Aixin},
title = {Effect of Spam on Hashtag Recommendation for Tweets},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889404},
doi = {10.1145/2872518.2889404},
abstract = {Presence of spam tweets in a dataset may affect the choices of feature selection, algorithm formulation, and system evaluation for many applications. However, most existing studies have not considered the impact of spam tweets. In this paper, we study the impact of spam tweets on hashtag recommendation for hyperlinked tweets (i.e., tweets containing URLs) in HSpam14 dataset. HSpam14 is a collection of 14 million tweets with annotations of being spam and ham (i.e., non-spam). In our experiments, we observe that it is much easier to recommend "correct" hashtags for spam tweets than ham tweets, because of the near duplicates in spam tweets. Simple approaches like recommending most popular hashtags achieves very good accuracy on spam tweets. On the other hand, features that are highly effective on ham tweets may not be effective on spam tweets. Our findings suggest that without removing spam tweets from the data collection (as in most studies), the results obtained could be misleading for hashtag recommendation tasks.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {97–98},
numpages = {2},
keywords = {microblog, tweets, spam, hashtag recommendation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889356,
author = {Shafqat, Wafa and Lee, Seunghun and Malik, Sehrish and Kim, Hyun-chul},
title = {The Language of Deceivers: Linguistic Features of Crowdfunding Scams},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889356},
doi = {10.1145/2872518.2889356},
abstract = {Crowdfunding sites with recent explosive growth are equally attractive platforms for swindlers or scammers. Though the growing number of articles on crowdfunding scams indicate that the fraud threats are accelerating, there has been little knowledge on the scamming practices and patterns. The key contribution of this research is to discover the hidden clues in the text by exploring linguistic features to distinguish scam campaigns from non-scams. Our results indicate that by providing less information and writing more carefully (and less informally), scammers deliberately try to deceive people; (i) they use less number of words, verbs, and sentences in their campaign pages. (ii) scammers make less typographical errors, 4.5-4.7 times lower than non-scammers.(iii) Expressivity of scams is 2.6-8.5 times lower as well.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {99–100},
numpages = {2},
keywords = {linguistic analysis, crowdfunding scam, kickstarter},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889354,
author = {Shi, Baoxu and Weninger, Tim},
title = {Fact Checking in Heterogeneous Information Networks},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889354},
doi = {10.1145/2872518.2889354},
abstract = {Traditional fact checking by experts and analysts cannot keep pace with the volume of newly created information. It is important and necessary, therefore, to enhance our ability to computationally determine whether some statement of fact is true or false. We view this problem as a link-prediction task in a knowledge graph, and show that a new model of the top discriminative meta paths is able to understand the meaning of some statement and accurately determine its veracity. We evaluate our approach by examining thousands of claims related to history, geography, biology, and politics using public, million node knowledge graphs extracted from Wikipedia and SemMedDB. Not only does our approach significantly outperform related models, we also find that the discriminative path model is easily interpretable and provides sensible reasons for the final determination.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {101–102},
numpages = {2},
keywords = {path mining, information networks, knowledge representation, fact checking},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889376,
author = {Shishkin, Alexander and Gladkikh, Ekaterina and Vorobev, Aleksandr},
title = {Selective Exploration of Commercial Documents in Web Search},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889376},
doi = {10.1145/2872518.2889376},
abstract = {Implicit user feedback is known to be a strong signal of user preferences in web search. Hence, solving the exploration-exploitation dilemma [5] became an important direction of improvement of ranking algorithms in the last years. In this poster, in the case of commercial queries, we consider a new negative effect of exploration on the user utility -- distracting and confusing users by shifting well-known documents from their common positions -- and propose an approach to take it into account within Multi-Armed Bandit algorithms, usually applied to solve the dilemma.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {103–104},
numpages = {2},
keywords = {exploration-exploitation dilemma, multi-armed bandit, web search ranking, implicit user feedback},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889374,
author = {Singh, Rina and Graves, Jeffrey A. and Talbert, Douglas A.},
title = {Complex Patterns in Dynamic Attributed Graphs},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889374},
doi = {10.1145/2872518.2889374},
abstract = {In recent years, there has been huge growth in the amount of graph data generated from various sources. These types of data are often represented by vertices and edges in a graph with real-valued attributes, topological properties, and temporal information associated with the vertices. Until recently, most pattern mining techniques focus solely on vertex attributes, topological properties, or a combination of these in a static sense; mining attribute and topological changes simultaneously over time has largely been overlooked. In this work-in-progress paper, we propose to extend an existing state-of-the-art technique to mine for patterns in dynamic attributed graphs which appear to trigger changes in attribute values.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {105–106},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889368,
author = {Skowron, Marcin and Tkal\v{c}i\v{c}, Marko and Ferwerda, Bruce and Schedl, Markus},
title = {Fusing Social Media Cues: Personality Prediction from Twitter and Instagram},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889368},
doi = {10.1145/2872518.2889368},
abstract = {Incorporating users' personality traits has shown to be instrumental in many personalized retrieval and recommender systems. Analysis of users' digital traces has become an important resource for inferring personality traits. To date, the analysis of users' explicit and latent characteristics is typically restricted to a single social networking site (SNS). In this work, we propose a novel method that integrates text, image, and users' meta features from two different SNSs: Twitter and Instagram. Our preliminary results indicate that the joint analysis of users' simultaneous activities in two popular SNSs seems to lead to a consistent decrease of the prediction errors for each personality trait.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {107–108},
numpages = {2},
keywords = {personality computing, social media mining, user modelling},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889398,
author = {Soundarajan, Sucheta and Tamersoy, Acar and Khalil, Elias B. and Eliassi-Rad, Tina and Chau, Duen Horng and Gallagher, Brian and Roundy, Kevin},
title = {Generating Graph Snapshots from Streaming Edge Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889398},
doi = {10.1145/2872518.2889398},
abstract = {We study the problem of determining the proper aggregation granularity for a stream of time-stamped edges. Such streams are used to build time-evolving networks, which are subsequently used to study topics such as network growth. Currently, aggregation lengths are chosen arbitrarily, based on intuition or convenience. We describe ADAGE, which detects the appropriate aggregation intervals from streaming edges and outputs a sequence of structurally mature graphs. We demonstrate the value of ADAGE in automatically finding the appropriate aggregation intervals on edge streams for belief propagation to detect malicious files and machines.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {109–110},
numpages = {2},
keywords = {aggregating edge streams, time-evolving networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889359,
author = {Srivastava, Ajitesh and Chelmis, Charalampos and Prasanna, Viktor K.},
title = {Mining Large Dense Subgraphs},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889359},
doi = {10.1145/2872518.2889359},
abstract = {Several applications including community detection in social networks and discovering correlated genes involve finding large subgraphs of high density. We propose the problem of finding the largest subgraph of a given density. The problem is a generalization of the Max-Clique problem which seeks the largest subgraph that has an edge density of 1. We define an objective function and prove that its optimization results in the largest graph of given density. We propose an algorithm that finds the subgraph by running multiple local search heuristics with random restarts. For massive graphs, where running the algorithm directly may be intractable, we use a sampling technique that reduces the graph to a smaller one which is likely to contain large dense subgraphs. We evaluate our algorithm on multiple real life and synthetic datasets. Our experiments show that our algorithm performs as well as the state-of-the-art for finding large subgraphs of high density, while providing density guarantees.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {111–112},
numpages = {2},
keywords = {discrete otimization, dense subgraphs, social networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889393,
author = {Taxidou, Io and Fischer, Peter M. and De Nies, Tom and Mannens, Erik and Van de Walle, Rik},
title = {Information Diffusion and Provenance of Interactions in Twitter: Is It Only about Retweets?},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889393},
doi = {10.1145/2872518.2889393},
abstract = {This paper sheds light on the different interaction types among social media users that benefit information diffusion and provenance analysis. In particular, we identify explicit and implicit interactions in Twitter, including informal conventions applied by users. In our empirical evaluation considering only retweets, the most common means of information propagation in Twitter, we can infer 50% of message provenance. However, if we consider other types of interactions, we can explain another 13%. Accordingly, we enrich the PROV-SAID model for information diffusion, which extends the W3C PROV standard for provenance.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {113–114},
numpages = {2},
keywords = {user interactions, social media, information provenance, information diffusion},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889401,
author = {Tinati, Ramine and Luczak-Roesch, Markus and Hall, Wendy and Shadbolt, Nigel},
title = {More than an Edit: Using Transcendental Information Cascades to Capture Hidden Structure in Wikipedia},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889401},
doi = {10.1145/2872518.2889401},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {115–116},
numpages = {2},
keywords = {information cascades, content analysis, wikipedia, information theory, network analysis},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889365,
author = {Toriumi, Fujio and Baba, Seigo},
title = {Real-Time Tweet Classification in Disaster Situation},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889365},
doi = {10.1145/2872518.2889365},
abstract = {During a disaster, appropriate information must be collected quickly. For example, residents along the coast require information about tsunamis and those who have lost their houses need information about shelters. Twitter can attract more attention than other forms of mass media under these circumstances because it can quickly provide such information. Since Twitter has an enormous amount of tweets, they must be classified to provide users with the information they need. Previous works on extracting information from Twitter focused on the text data of tweets. However, in some cases, text mining has difficulty extracting information. For example, it might be difficult for text mining to group tweets with URLs. On the other hand, by assuming that users who retweet the same tweet are interested in the same topic, we can classify tweets that are required by users with similar interests based on retweets. Thus, we employ the tweet classification method that focuses on retweets. In this paper, we demonstrated that our method works quickly in disaster situations and that it can quickly classify the required information based on the needs in disaster situations and is helpful for collecting information under them.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {117–118},
numpages = {2},
keywords = {disaster management, real-time classification, twitter},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889352,
author = {Van de Sompel, Herbert and Klein, Martin and Jones, Shawn M.},
title = {Persistent URIs Must Be Used To Be Persistent},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889352},
doi = {10.1145/2872518.2889352},
abstract = {We quantify the extent to which references to papers in scholarly literature use persistent HTTP URIs that leverage the Digital Object Identifier infrastructure. We find a significant number of references that do not, speculate why authors would use brittle URIs when persistent ones are available, and propose an approach to alleviate the problem.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {119–120},
numpages = {2},
keywords = {uri references, scholarly communication, digital object identifier},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889379,
author = {Wang, Bo and Yu, Yanshu and Zhang, Peng},
title = {Investigation of the Subjective Asymmetry of Social Interrelationship with Interactive Language},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889379},
doi = {10.1145/2872518.2889379},
abstract = {Instead of studying the properties of social relationship from an objective view, in this paper, we focus on the two individuals' subjective and asymmetric opinions on their interrelationships. The sociolinguistics theories propose to characterize the individuals' opinions of their interrelationship with interactive language features. With this inspiration, we investigate the subjective asymmetry of the interrelationship with the asymmetry of the interactive language features including the frequency, quantity, quality and emotion. Experimental results with Enron email corpus provide suggestive evidences and thus reveal that the pair-wise language styles on an interrelationship are asymmetric, and this asymmetry can be a joint effect of the individuals' opinions of the interrelationship and their personal language habits. The results also indicate that the degree of the asymmetry could be related to the individuals' personality traits.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {121–122},
numpages = {2},
keywords = {social relationship, interactive language, subjective asymmetry},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889348,
author = {Wang, Chengyu and Zhang, Rong and He, Xiaofeng and Zhou, Aoying},
title = {NERank: Ranking Named Entities in Document Collections},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889348},
doi = {10.1145/2872518.2889348},
abstract = {While most of the entity ranking research focuses on Web corpora with user queries as input, little has been done to rank entities directly from documents. We propose a ranking algorithm NERank to address this issue. NERank employs a random walk process on a weighted tripartite graph mined from the document collection. We evaluate NERank over real-life document datasets and compare it with baselines. Experimental results show the effectiveness of our method.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {123–124},
numpages = {2},
keywords = {entity ranking, random walk, tripartite graph},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889399,
author = {Wang, Dongjing and Deng, Shuiguang and Liu, Songguo and Xu, Guandong},
title = {Improving Music Recommendation Using Distributed Representation},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889399},
doi = {10.1145/2872518.2889399},
abstract = {In this paper, a music recommendation approach based on distributed representation is presented. The proposed approach firstly learns the distributed representations of music pieces and acquires users' preferences from listening records. Then, it recommends appropriate music pieces whose distributed representations are in accordance with target users' preferences. Experiments on a real world dataset demonstrate that the proposed approach outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {125–126},
numpages = {2},
keywords = {distributed representation, music recommendation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889400,
author = {Wu, Chao-Yuan and Beutel, Alex and Ahmed, Amr and Smola, Alexander J.},
title = {Explaining Reviews and Ratings with PACO: Poisson Additive Co-Clustering},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889400},
doi = {10.1145/2872518.2889400},
abstract = {Understanding a user's motivations provides valuable information beyond the ability to recommend items. Quite often this can be accomplished by perusing both ratings and review texts. Unfortunately matrix factorization approaches to recommendation result in large, complex models that are difficult to interpret. In this paper, we attack this problem through succinct additive co-clustering on both ratings and reviews. Our model yields accurate and interpretable recommendations.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {127–128},
numpages = {2},
keywords = {recommendation systems, co-clustering, joint modeling},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889413,
author = {Yamamoto, Kohei and Kobayashi, Hayato and Tagami, Yukihiro and Nakayama, Hideki},
title = {Multimodal Content-Aware Image Thumbnailing},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889413},
doi = {10.1145/2872518.2889413},
abstract = {News article recommendation has the key problem of needing to eliminate the redundant information in a ranked list in order to provide more relevant information within a limited time and space. In this study, we tackle this problem by using image thumbnailing, which can be regarded as the summarization of news images. We propose a multimodal image thumbnailing method considering news text as well as images themselves. We evaluate this approach on a real data set based on news articles that appeared on Yahoo! JAPAN. Experimental results demonstrate the effectiveness of our proposed method.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {129–130},
numpages = {2},
keywords = {convolutional neural networks, image thumbnaililing, multimodal learning},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889406,
author = {Yamazaki, Tomoya and Shimizu, Nobuyuki and Kobayashi, Hayato and Yamauchi, Satoshi},
title = {Weighted Micro-Clustering: Application to Community Detection in Large-Scale Co-Purchasing Networks with User Attributes},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889406},
doi = {10.1145/2872518.2889406},
abstract = {We propose a simple and scalable method for soft community detection that makes use of both graph structures and vertex attributes. Our method is based on micro-clustering, which is a scalable and efficient clique-based method for detecting overlapping communities in unweighted graphs. We extend this method to graphs with vertex attributes so that we can make use of information supplied by vertex attributes. Our method still requires the same time complexity as micro-clustering. We confirm the validity and efficiency of our method by applying it to a large-scale co-purchasing network of real online auction data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {131–132},
numpages = {2},
keywords = {online auction, micro-clustering, community detection},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889394,
author = {Yeo, Jinyoung and Kim, Sungchul and Koh, Eunyee and Hwang, Seung-won and Lipka, Nedim},
title = {Browsing2purchase: Online Customer Model for Sales Forecasting in an E-Commerce Site},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889394},
doi = {10.1145/2872518.2889394},
abstract = {This paper covers a sales forecasting problem on e-commerce sites. To predict product sales, we need to understand customers' browsing behavior and identify whether it is for purchase purpose or not. For this goal, we propose a new customer model, B2P, of aggregating predictive features extracted from customers' browsing history. We perform experiments on a real world e-commerce site and show that sales predictions by our model are consistently more accurate than those by existing state-of-the-art baselines.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {133–134},
numpages = {2},
keywords = {customer model, sales forecasting, e-commerce, sales prediction},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889353,
author = {Yin, Li'ang and Han, Jianhua and Yu, Yong},
title = {Label Aggregation with Instance Grouping Model},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889353},
doi = {10.1145/2872518.2889353},
abstract = {Label aggregation is one of the key topics in crowdsourcing research. Most researchers make their efforts in modeling ability of users and difficulty of instances. In this paper, we consider label aggregation from the view of grouping instances. We assume instances are sampled from latent groups and they share the same true label with their corresponding groups. We construct a graphical model named InGroup(Instance Grouping model) to infer latent group assignment as well as true labels. The experimental results show the advantages of our model compared with baselines.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {135–136},
numpages = {2},
keywords = {crowdsourcing, label aggregation, graphical model, instance grouping},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889378,
author = {Ying, Haochao and Chen, Liang and Xiong, Yuwen and Wu, Jian},
title = {PGRank: Personalized Geographical Ranking for Point-of-Interest Recommendation},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889378},
doi = {10.1145/2872518.2889378},
abstract = {Point-of-interest (POI) recommendation has become more and more important, since it could discover user behavior pattern and find interesting venues for them. To address this problem, we propose a rank-based method, PGRank, which integrates user geographical preference and latent preference into Bayesian personalized ranking framework. The experimental results on a real dataset show its effective.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {137–138},
numpages = {2},
keywords = {geographical preference, rank, poi recommendation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889363,
author = {Yokoi, Takeru and Fukuchi, Masato and Kobayakawa, Michihiro and Ibrahim, Roliana and Selamat, Ali},
title = {Analytical Framework of Relations among Nations Using News Articles},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889363},
doi = {10.1145/2872518.2889363},
abstract = {News articles are a type of instantaneous and regional media, and provide the daily concerns of its publication area. This work proposes an analytical framework of the relations among nations using the similarities in the content of news articles published in different nations. Our key idea is that those relations exist in the news articles miss-classified by different nations from their original publication area. In order to clearly illustrate those relations, the classification results are visualized as bar graphs. We also carried out some experiments for the proposed framework using a small collection of news articles.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {139–140},
numpages = {2},
keywords = {relation analysis, miss-classified items, text mining, spatiotemporal mining},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889373,
author = {Zhai, Shuangfei and Chang, Keng-hao and Zhang, Ruofei and Zhang, Zhongfei},
title = {Attention Based Recurrent Neural Networks for Online Advertising},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889373},
doi = {10.1145/2872518.2889373},
abstract = {We investigate the use of recurrent neural networks (RNNs) in the context of online advertising, where we use RNNs to map both query and ads to real valued vectors. In addition, we propose an attention network that assigns scores to different word locations according to their intent importance. The vector output is computed by a weighted sum of the vectors at each word. We perform end-to-end training of both the RNN and attention network under the guidance of user click logs. We show that the attention network improves the quality of learned vector representations evaluated by AUC on a manually labeled dataset. Moreover, we show that keywords extracted according to the attention scores are easy to interpret and significantly outperform the state-of-the-art query intent extraction methods.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {141–142},
numpages = {2},
keywords = {attention, rnn, online advertising, deep learning},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889366,
author = {Zhai, Zhongyi and Cheng, Bo and Wang, Zhaoning and Liu, Xuan and Liu, Meng and Chen, Junliang},
title = {Design and Implementation: The End User Development Ecosystem for Cross-Platform Mobile Applications},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889366},
doi = {10.1145/2872518.2889366},
abstract = {In this paper, we present an ecosystem for mobile application development by end-users. An advantage of this ecosystem is that the graphical user interface (GUI), as well as the application logic, can both be developed in a rapid and simple way. This ecosystem is mainly implemented through development and integration of two sub-systems, namely EasyApp and LSCE. EasyApp is responsible for developing the mobile app with the compatibility of multiple mobile platforms, while LSCE is in charge of creating the service process that can be invoked by mobile app directly. A case study is presented to illustrate the development process using this ecosystem.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {143–144},
numpages = {2},
keywords = {service mashup, end-user development, mobile application},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889357,
author = {Zhang, Shanshan and Vucetic, Slobodan},
title = {Sampling Bias in LinkedIn: A Case Study},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889357},
doi = {10.1145/2872518.2889357},
abstract = {This paper describes a case study of sampling bias in LinkedIn, a major professional social network. The study collected a sample of 1,989 STEM students who graduated from a major public university between 2002 and 2014. Overall, 40% of the graduates had a LinkedIn profile in summer of 2015. It was observed that LinkedIn participation significantly fluctuated among different majors, and ranged from 30% for biochemistry majors to 51% for information science majors. Year of graduation, gender, and grade point average surprisingly did not seem to create a large difference in LinkedIn participation. These results should be useful for design and interpretation of empirical studies which use LinkedIn data or select participants from LinkedIn social network.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {145–146},
numpages = {2},
keywords = {major, gender, gpa, linkedin, sampling bias, year},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889367,
author = {Zignani, Matteo and Gaito, Sabrina and Rossi, Gian Paolo},
title = {Predicting the Link Strength of "Newborn" Links},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889367},
doi = {10.1145/2872518.2889367},
abstract = {Measurements of online social networks (OSNs) support the common fact that not all links carry the same social value, and that the strength of each link is strictly related to the frequency of interactions between the connected users. In this paper, we investigate the predictability of the interactions on OSN links by wondering if it is possible to categorize interactive or non-interactive links at their creation time. We turn the problem into a binary classification task and introduce a set of features which leverage the temporal and topological properties of the social and interaction networks, without requiring the knowledge of the interaction history of the link. The best classifier trained on a Facebook dataset obtained 0.72 as AUC. The above performance suggests that we can distinguish between interactive/non-interactive links at the time of link creation.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {147–148},
numpages = {2},
keywords = {interaction prediction, interaction graphs, online social networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251208,
author = {Cuzzocrea, Alfredo and El Saddik, Abdulmotaleb},
title = {Session Details: Demonstrations},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the Demo Track of WWW 2016, The 25th International World Wide Web Conference, held in Montreal, Canada, during April 11-15, 2016.The WWW 2016 Demo Track, like in the tradition of WWW Demo conference series, allows researchers and practitioners to demonstrate new systems in a dedicated session. Demo contributions are based on an implemented and tested system that pursues one or more innovative ideas in the interest areas of Web data and information management, Web search, Web intelligence tools, Web mining, social network applications and so forth. Topics of interest for the 2016 edition's conference include (but are not limited to) the following ones: Behavioral Analysis and PersonalizationBig Data on the WebCrowdsourcing Systems and Social MediaContent AnalysisGraph Data Management and MiningHigh-Performance Infrastructures for Data- Intensive Web TasksInternet Economics and MonetizationPervasive Web and MobilitySecurity and PrivacySemantic WebSocial Networks and Graph AnalysisWeb Information RetrievalWeb Infrastructure: Datacenters, Content Delivery Networks, and Cloud ComputingWeb MiningWeb ScienceWeb Search Systems and ApplicationsDemo contributions come from academic researchers, industrial practitioners with prototypes or inproduction deployments, as well as from any W3C-related activities. All have in common to show innovative use of Web-based techniques.The WWW 2016 Demo Track call for papers attracted 65 submissions from all over the world (USA, North America, South America, Europe, Australia, Asia, Africa). The program committee reviewed and accepted a very selected collection of 29 papers, and the final statistics is the following: WWW 2016 Demo Track Statistics Number of Submitted Papers 65 Number of Accepted Papers 29.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890550,
author = {Al-Ghossein, Marie and Abdessalem, Talel},
title = {SoMap: Dynamic Clustering and Ranking of Geotagged Posts},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890550},
doi = {10.1145/2872518.2890550},
abstract = {This demo presents SoMap, a web-based platform that provides new scalable methods to aggregate, analyse and valorise large collections of heterogeneous social data in urban contexts. The platform relies on geotagged data extracted from social networks and microblogging applications such as Instagram, Flickr and Twitter and on Points Of Interest gathered from OpenStreetMap. It could be very insightful and interesting for data scientists and decision-makers. SoMap enables dynamic clustering of filtered social data in order to display it on a map in a combined form. The key components of this platform are the clustering module, which relies on a scalable algorithm described in this paper, and the ranking algorithm that combines the popularity of the posts, their location and their link to the points of interest found in the neighbourhood. The system further detects mobility patterns by identifying and aggregating trajectories for all the users. SoMap will be demonstrated through several examples that highlight all of its functionalities and reveal its effectiveness and usefulness.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {151–154},
numpages = {4},
keywords = {urban analytics, ranking, social data, clustering, geotagged posts},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890531,
author = {Appel, Ana Paula and Candello, Heloisa and de Souza, Beatriz S.R. and Andrade, Bruna D.},
title = {Destiny: A Cognitive Mobile Guide for the Olympics},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890531},
doi = {10.1145/2872518.2890531},
abstract = {In this work we present Destiny a cognitive mobile guide for Olympics games in Brazil that identifies user characteristics to deliver content. It will help visitors, athletes, and athletes' parents to localize themselves and receive tailored historical, cultural and entertainment information at a particular point of interest (POI). The application will recommend places to go and show POI contents according to user context based on several attributes such as personality (closeness, curiosity, adventurous), nationality, favorite places already visited, trip length, price range and favorite sports},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {155–158},
numpages = {4},
keywords = {scognitive, social recommendation, tourism, travel, mobile},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890536,
author = {Ba, Mouhamadou Lamine and Berti-Equille, Laure and Shah, Kushal and Hammady, Hossam M.},
title = {VERA: A Platform for Veracity Estimation over Web Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890536},
doi = {10.1145/2872518.2890536},
abstract = {Social networks and the Web in general are characterized by multiple information sources often claiming conflicting data values. Data veracity is hard to estimate, especially when there is no prior knowledge about the sources or the claims and in time-dependent scenarios where initially very few observers can report first information. Despite the wide set of recently proposed truth discovery approaches, "no-one-fits-all" solution emerges for estimating the veracity of on-line information in open contexts. However, analyzing the space of conflicting information and disagreeing sources might be relevant, as well as ensembling multiple truth discovery methods. This demonstration presents VERA, a Web-based platform that supports information extraction from Web textual data and micro-texts from Twitter and estimates data veracity. Given a user query, VERA systematically extracts entities and relations from Web content, structures them as claims relevant to the query and gathers more conflicting/corroborating information. VERA combines multiple truth discovery algorithms through ensembling returns the veracity label and score of each data value and the trustworthiness scores of the sources. VERA will be demonstrated through several real-world scenarios to show its potential value for fact-checking from Web data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {159–162},
numpages = {4},
keywords = {data veracity, truth discovery, source trustworthiness, fact-checking},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890545,
author = {Baron Neto, Ciro and M\"{u}ller, Kay and Br\"{u}mmer, Martin and Kontokostas, Dimitris and Hellmann, Sebastian},
title = {LODVader: An Interface to LOD Visualization, Analyticsand DiscovERy in Real-Time},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890545},
doi = {10.1145/2872518.2890545},
abstract = {The Linked Open Data (LOD) cloud is in danger of becoming a black box. Simple questions such as "What kind of datasets are in the LOD cloud?", "In what way(s) are these datasets connected?" -- albeit frequently asked -- are at the moment still difficult to answer due to the lack of proper tooling support. The infrequent update of the static LOD cloud diagram adds to the current dilemma, since there is neither reliable nor timely-updated information to perform an interactive search, analysis or in particular visualization in order to gain insight into the current state of Linked Open Data. In this paper, we propose a new hybrid system which combines LOD Visualisation, Analytics and DiscovERy (LODVader) to aid in answering the above questions. LODVader is equipped with (1) a multi-layer LOD cloud visualization component comprising datasets, subsets and vocabularies, (2) dataset analysis components that extend the state of the art with new similarity measures and efficient link extracting techniques and (3) a fast search index that is an entry point for dataset discovery. At its core, LODVader employs a timely-updated index using a complex cluster of Bloom filters as a fast search index with low memory footprint. This BF cluster is able to efficiently perform analysis on link and dataset similarities based on stored predicate and object information, which -- once inverted -- can be employed to discover invalid links by displaying the Dark LOD Cloud. By combining all these features, we allow for an up-to-date, multi-dimensional LOD cloud analysis, which -- to the best of our knowledge -- was not possible before.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {163–166},
numpages = {4},
keywords = {bloom filter, rdf diagram, linksets, linked open data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2901922,
author = {\c{C}elikten, Emre and Le Falher, G\'{e}raud and Mathioudakis, Michael},
title = {"What Is the City but the People?": Exploring Urban Activity Using Social Web Traces},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2901922},
doi = {10.1145/2872518.2901922},
abstract = {We demonstrate Geotopics, a system to explore geographical patterns of urban activity. The system collects publicly shared check-ins generated by Foursquare users, that reveal who spends time where, when, and on what type of activity. It then employs sparse probabilistic modeling techniques to learn associations between different regions of a city and multi-feature descriptions of urban activity. Through a web interface, users of the system can select a city of interest and explore visualizations that highlight how different types of activity are spatially and temporally distributed in the city.We discuss the opportunities that web data offer to understand urban activity and the challenges one faces in that task. We then describe our approach and the architecture of Geotopics. Finally, we lay out the demonstration scenario.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {167–170},
numpages = {4},
keywords = {location-based social networks, urban computing, probabilistic models},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890535,
author = {Collarana, Diego and Lange, Christoph and Auer, S\"{o}ren},
title = {FuhSen: A Platform for Federated, RDF-Based Hybrid Search},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890535},
doi = {10.1145/2872518.2890535},
abstract = {The increasing amount of structured and semi-structured information available on the Web and in distributed information systems, as well as the Web's diversification into different segments such as the Social Web, the Deep Web, or the Dark Web, requires new methods for horizontal search. FuhSen is a federated, RDF-based, hybrid search platform that searches, integrates and summarizes information about entities from distributed heterogeneous information sources using Linked Data. As a use case, we present scenarios where law enforcement institutions search and integrate data spread across these different Web segments to identify cases of organized crime. We present the architecture and implementation of FuhSen and explain the queries that can be addressed with this new approach.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {171–174},
numpages = {4},
keywords = {federated search, rdf, linked data, social web, deep web},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890532,
author = {Fokoue, Achille and Hassanzadeh, Oktie and Sadoghi, Mohammad and Zhang, Ping},
title = {Predicting Drug-Drug Interactions Through Similarity-Based Link Prediction Over Web Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890532},
doi = {10.1145/2872518.2890532},
abstract = {Drug-Drug Interactions (DDIs) are a major cause of preventable adverse drug reactions and a huge burden on public health and the healthcare system. On the other hand, there is a large amount of drug-related (open) data published on the Web, describing various properties of drugs and their relationships to other drugs, genes, diseases, and related concepts and entities. In this demonstration, we describe an end-to-end system we have designed to take in various Web data sources as input and provide as output a prediction of DDIs along with an explanation of why two drugs may interact. The system first creates a knowledge graph out of input data sources through large-scale semantic integration, and then performs link prediction among drug entities in the graph through large-scale similarity analysis and machine learning. The link prediction is performed using a logistic regression model over several similarity matrices built using different drug similarity measures. We present both the efficient link prediction framework implemented in Apache Spark, and our APIs and Web interface for predicting DDIs and exploring their potential causes and nature.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {175–178},
numpages = {4},
keywords = {drug-drug interactions, link prediction, semantic web, big data on the web},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890548,
author = {Franceschini, Michele M. and Soares, Livio B. and Lastras Monta\~{n}o, Luis A.},
title = {Watson Concept Insights: A Conceptual Association Framework},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890548},
doi = {10.1145/2872518.2890548},
abstract = {Watson Concept Insights (WCI) is a service that was recently made publicly available by IBM. WCI provides an information retrieval framework that is designed to facilitate search and exploration of text documents, and is particularly effective on sparse data sets. Its methodology consists of first defining a dictionary of concepts which are interconnected in a concept graph and then modeling a document by predicting its relevance to any given concept in the concept graph using the concepts that are directly mentioned in the document itself. This technique in effect increases the document recall for any given query, even for very sparse data sets, exposing the user to a variety of connections between their query and a data set of interest.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {179–182},
numpages = {4},
keywords = {information retrieval, cognitive computing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890538,
author = {Gallidabino, Andrea and Pautasso, Cesare},
title = {The Liquid.Js Framework for Migrating and Cloning Stateful Web Components across Multiple Devices},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890538},
doi = {10.1145/2872518.2890538},
abstract = {We are heading toward an era in which users own more than one single Web-enabled device. These devices range from smart phones, tablets and personal computers to smart Web-enabled devices found in houses and cars. The access mechanisms and usage patterns of Web applications are changing accordingly, as users interact more and more with Web applications through all their devices, even if the majority of Web applications are not ready to offer a good user experience taking full advantage of multiple devices. In this demonstration we introduce Liquid.js, a framework whose goal is to enable Web developers to take advantage of multiple heterogeneous devices and offer to their users a liquid user experience, whereby any device can be used sequentially or concurrently with Web applications that can effortlessly roam from one device to another. This way, as highlighted in the demonstration users do not need to stop and resume their work on their Web application as they migrate and clone them across different devices. The demo will also show how developers can easily add such liquid behavior to any Polymer Web component.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {183–186},
numpages = {4},
keywords = {stateful web components, web components, liquid web applications, multi-device environments, liquid software},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890530,
author = {Hagedorn, Stefan and Sattler, Kai-Uwe},
title = {Piglet: Interactive and Platform Transparent Analytics for RDF &amp; Dynamic Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890530},
doi = {10.1145/2872518.2890530},
abstract = {Data analytics has gained more and more focus during recent years and many data processing platforms have been developed. They all provide a powerful but often complex API that users have to learn. Furthermore, results can only be stored or printed, without any possibility for visualization.In this paper we present Piglet, a compiler for the high-level Pig Latin script language that generates code for various platforms like Spark, Flink, Storm, and PipeFabric. Piglet lets users write elegant code with extensions for SPARQL and RDF, as well as support for streaming data. An integration into the notebook-based frontend Zeppelin provides a homogeneous and interactive user interface for exploring, analyzing, and visualizing data from different sources and lets users share their scripts and results.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {187–190},
numpages = {4},
keywords = {code generation, sparql, pig latin, flink, data analyses tool, spark, pig, mapreduce, storm, rdf, pipefabric},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2901923,
author = {Hasan, Aisha and Hammoud, Mohammad and Nouri, Reza and Sakr, Sherif},
title = {DREAM in Action: A Distributed and Adaptive RDF System on the Cloud},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2901923},
doi = {10.1145/2872518.2901923},
abstract = {RDF and SPARQL query language are gaining wide popularity and acceptance. This demonstration paper presents DREAM, a hybrid RDF system, which combines the advantages and averts the disadvantages of the centralized and distributed RDF schemes. In particular, DREAM avoids partitioning RDF datasets and reversely partitions SPARQL queries. By not partitioning datasets, DREAM offers a general paradigm for different types of pattern matching queries and entirely precludes intermediate data shuffling (only auxiliary data are shuffled). By partitioning only queries, DREAM suggests an adaptive scheme, which runs queries on different numbers of machines depending on their complexities. DREAM achieves these goals and significantly outperforms related systems via employing a novel graph-based, rule-oriented query planner and a new cost model. This paper proposes demonstrating DREAM live over the cloud using a friendly graphical user interface (GUI). The GUI allows participants to execute and visualize pre-defined and user-defined (which can be written by participants on-the-fly) SPARQL queries over various real-world and synthetic RDF datasets. Furthermore, participants can empirically compare and contrast DREAM against three state-of-the-art RDF systems.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {191–194},
numpages = {4},
keywords = {query planning, sparql, distributed rdf systems, rdf},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890549,
author = {Ha-Thuc, Viet and Xu, Ye and Kanduri, Satya Pradeep and Wu, Xianren and Dialani, Vijay and Yan, Yan and Gupta, Abhishek and Sinha, Shakti},
title = {Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890549},
doi = {10.1145/2872518.2890549},
abstract = {One key challenge in talent search is how to translate complex criteria of a hiring position into a search query. This typically requires deep knowledge on which skills are typically needed for the position, what are their alternatives, which companies are likely to have such candidates, etc. However, listing examples of suitable candidates for a given position is a relatively easy job. Therefore, in order to help searchers overcome this challenge, we design a next generation of talent search paradigm at LinkedIn: Search by Ideal Candidates. This new system only needs the searcher to input one or several examples of suitable candidates for the position. The system will generate a query based on the input candidates and then retrieve and rank results based on the query as well as the input candidates. The query is also shown to the searcher to make the system transparent and to allow the searcher to interact with it. As the searcher modifies the initial query and makes it deviate from the ideal candidates, the search ranking function dynamically adjusts an refreshes the ranking results balancing between the roles of query and ideal candidates. As of writing this paper, the new system is being launched to our customers.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {195–198},
numpages = {4},
keywords = {query by example, ranking, query generation, talent search},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890534,
author = {He, Ruining and Lin, Chunbin and McAuley, Julian},
title = {Fashionista: A Fashion-Aware Graphical System for Exploring Visually Similar Items},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890534},
doi = {10.1145/2872518.2890534},
abstract = {To build a fashion recommendation system, we need to help users retrieve fashionable items that are visually similar to a particular query, for reasons ranging from searching alternatives (i.e., substitutes), to generating stylish outfits that are visually consistent, among other applications. In domains like clothing and accessories, such considerations are particularly paramount as the visual appearance of items is a critical feature that guides users' decisions. However, existing systems like Amazon and eBay still rely mainly on keyword search and recommending loosely consistent items (e.g. based on co-purchasing or browsing data), without an interface that makes use of visual information to serve the above needs. In this paper, we attempt to fill this gap by designing and implementing an image-based query system, called Fashionista, which provides a graphical interface to help users efficiently explore those items that are not only visually similar to a given query, but which are also fashionable, as determined by visually-aware recommendation approaches. Methodologically, Fashionista learns a low-dimensional visual space as well as the evolution of fashion trends from large corpora of binary feedback data such as purchase histories of Women's Clothing &amp; Accessories from Amazon, which we use for this demonstration.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {199–202},
numpages = {4},
keywords = {visualization, recommendation, fashion trends},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890537,
author = {Hoffart, Johannes and Milchevski, Dragan and Weikum, Gerhard and Anand, Avishek and Singh, Jaspreet},
title = {The Knowledge Awakens: Keeping Knowledge Bases Fresh with Emerging Entities},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890537},
doi = {10.1145/2872518.2890537},
abstract = {Entity search over news, social media and the Web allows users to precisely retrieve concise information about specific people, organizations, movies and their characters, and other kinds of entities. This expressive search mode builds on two major assets: 1) a knowledge base (KB) that contains the entities of interest and 2) entity markup in the documents of interest derived by automatic disambiguation of entity names (NED) and linking names to the KB. These prerequisites are not easily available, though, in the important case when a user is interested in a newly emerging entity (EE) such as new movies, new songs, etc. Automatic methods for detecting and canonicalizing EEs are not nearly at the same level as the NED methods for prominent entities that have rich descriptions in the KB.To overcome this major limitation, we have developed an approach and prototype system that allows searching for EEs in a user-friendly manner. The approach leverages the human in the loop by prompting for user feedback on candidate entities and on characteristic keyphrases for EEs. For convenience and low burden on users, this process is supported by the automatic harvesting oftentative keyphrases. Our demo system shows this interactive process and its high usability.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {203–206},
numpages = {4},
keywords = {entity disambiguation, emerging entities, knowledge base life cycle, knowledge base curation, human in the loop},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890555,
author = {Holzmann, Helge and Anand, Avishek},
title = {Tempas: Temporal Archive Search Based on Tags},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890555},
doi = {10.1145/2872518.2890555},
abstract = {Limited search and access patterns over Web archives have been well documented. One of the key reasons is the lack of understanding of the user access patterns over such collections, which in turn is attributed to the lack of effective search interfaces. Current search interfaces for Web archives are (a) either purely navigational or (b) have sub-optimal search experience due to ineffective retrieval models or query modeling. We identify that external longitudinal resources, such as social bookmarking data, are crucial sources to identify important and popular websites in the past. To this extent we present Tempas, a tag-based temporal search engine for Web archives.Websites are posted at specific times of interest on several external platforms, such as bookmarking sites like Delicious. Attached tags not only act as relevant descriptors useful for retrieval, but also encode the time of relevance. With Tempas we tackle the challenge of temporally searching a Web archive by indexing tags and time. We allow temporal selections for search terms, rank documents based on their popularity and also provide meaningful query recommendations by exploiting tag-tag and tag-document co-occurrence statistics in arbitrary time windows. Finally, Tempas operates as a fairly non-invasive indexing framework. By not dealing with contents from the actual Web archive it constitutes an attractive and low-overhead approach for quick access into Web archives.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {207–210},
numpages = {4},
keywords = {search, web archives, temporal},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890547,
author = {Hooshmand, Salman and Mahmud, Akib and Bochmann, Gregor V. and Faheem, Muhammad and Jourdan, Guy-Vincent and Couturier, Russ and Onut, Iosif-Viorel},
title = {D-ForenRIA: Distributed Reconstruction of User-Interactions for Rich Internet Applications},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890547},
doi = {10.1145/2872518.2890547},
abstract = {We present D-ForenRIA, a distributed forensic tool to automatically reconstruct user-sessions in Rich Internet Applications (RIAs), using solely the full HTTP traces of the sessions as input. D-ForenRIA recovers automatically each browser state, reconstructs the DOMs and re-creates screenshots of what was displayed to the user. The tool also recovers every action taken by the user on each state, including the user-input data. Our application domain is security forensics, where sometimes months-old sessions must be quickly reconstructed for immediate inspection. We will demonstrate our tool on a series of RIAs, including a vulnerable banking application created by IBM Security for testing purposes. In that case study, the attacker visits the vulnerable web site, and exploits several vulnerabilities (SQL-injections, XSS...) to gain access to private information and to perform unauthorized transactions. D-ForenRIA can reconstruct the session, including screenshots of all pages seen by the hacker, DOM of each page and the steps taken for unauthorized login and the inputs hacker exploited for the SQL-injection attack. D-ForenRIA is made efficient by applying advanced reconstruction techniques and by using several browsers concurrently to speed up the reconstruction process. Although we developed D-ForenRIA in the context of security forensics, the tool can also be useful in other contexts such as aided RIAs debugging and automated RIAs scanning.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {211–214},
numpages = {4},
keywords = {traffic replay, rich internet applications, http traces, user-interactions reconstruction, incident forensics},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890554,
author = {Kowark, Thomas and Richly, Keven and Uflacker, Matthias and Plattner, Hasso},
title = {Incremental, Per-Query Ontology Matching with RepMine},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890554},
doi = {10.1145/2872518.2890554},
abstract = {Ontology matching enables applications, such as automated data transformation or query rewriting. As it requires domain knowledge, it needs to be carried out by expert users, whose time is scarce and, therefore, should be used efficiently. To this end, the RepMine system presented in this paper does not treat ontology matching as a task of its own, but integrates it into a semi-automated query translation process. By that, users perform a task with immediate benefit for them and simultaneously contribute to alignments between ontologies. Furthermore, the overall task of matching two ontologies is split on a per-query basis and, thus, can be performed incrementally by all system users},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {215–218},
numpages = {4},
keywords = {ontology matching, query translation, user involvement},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890553,
author = {Lee, Jukyoung and Choi, Yonghwa and Kim, Suhkyung and Kim, Seongsoon and Kang, Jaewoo},
title = {SEMO: Searching Majority Opinions on Movies Using SNS QA Threads},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890553},
doi = {10.1145/2872518.2890553},
abstract = {Many people seek majority opinions by searching for question-answers that are uploaded by others or uploading their own questions on social media sites. However, people have to read through a large number of documents returned by search services to find the majority opinions. Moreover, even when users upload questions on social media sites, they cannot immediately obtain answers. To address these problems, we present Searching Majority Opinions System (SEMO), a novel majority opinion-based search system that uses QA threads uploaded on SNS and cQA websites. SEMO returns entities based on majority opinions for opinion-finding queries in real time. We also tackled a data sparsity problem using a novel query component expansion approach. To prove SEMO's usefulness in finding majority opinions, we implemented a prototype of SEMO for the movie domain. We believe that our method can cause a paradigm shift in opinion-finding query search and help people make decisions. SEMO is available at http://semo.korea.ac.kr/},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {219–222},
numpages = {4},
keywords = {entity search, social question answer, majority opinions search},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890529,
author = {Mansour, Essam and Sambra, Andrei Vlad and Hawke, Sandro and Zereba, Maged and Capadisli, Sarven and Ghanem, Abdurrahman and Aboulnaga, Ashraf and Berners-Lee, Tim},
title = {A Demonstration of the Solid Platform for Social Web Applications},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890529},
doi = {10.1145/2872518.2890529},
abstract = {Solid is a decentralized platform for social Web applications. In the Solid platform, users' data is managed independently of the applications that create and consume this data. Each user stores their data in a Web-accessible personal online datastore (or pod). Each user can have one or more pods from different pod providers, and can easily switch between providers. Applications access data in users' pods using well defined protocols, and a decentralized authentication and access control mechanism guarantees the privacy of the data. In this decentralized architecture, applications can operate on users' data wherever it is stored. Users control access to their data, and have the option to switch between applications at any time. We will demonstrate the utility of Solid and how it is experienced from the point of view of end users and application developers. For this, we will use a set of Solid servers and multiple Web applications that use these servers. We believe that experience with a concrete platform such as Solid is highly valuable in truly appreciating the power of a decentralized social Web.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {223–226},
numpages = {4},
keywords = {sparql, linked data platform (ldp), webid, decentralization, social web},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890544,
author = {Moyano, Luis G. and Appel, Ana Paula and de Santana, Vagner F. and Ito, Marcia and dos Santos, Thiago D.},
title = {GraPhys: Understanding Health Care Insurance Data through Graph Analytics.},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890544},
doi = {10.1145/2872518.2890544},
abstract = {Healthcare insurance data represent a rich source of information and has the potential to contribute significantly in guiding business decision making. In this work we present GraPhys, a Graph Analysis platform designed for exploration, visualization and analysis of healthcare insurance data and its corresponding metadata. By taking advantage of relationships contained in healthcare claims data, we are able to apply Graph Analytics methods and algorithms in order to devise useful business metrics to guide data analysis and exploration. Our tool focuses in better understanding physicians, patients and their practices. We illustrate our approach by demonstrating two use cases where we show how graph analytics metrics, combined with other data, may lead to useful insights not directly available to traditional Business Analytics.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {227–230},
numpages = {4},
keywords = {health, graph mining, visual mining},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890539,
author = {N\'{e}delec, Brice and Molli, Pascal and Mostefaoui, Achour},
title = {CRATE: Writing Stories Together with Our Browsers},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890539},
doi = {10.1145/2872518.2890539},
abstract = {Real-time collaborative editors are common tools for distributing work across space, time, and organizations. Unfortunately, mainstream editors such as Google Docs rely on central servers and raise privacy and scalability issues. CRATE is a real-time decentralized collaborative editor that runs directly in web browsers thanks to WebRTC. Compared to state-of-the-art, CRATE is the first real-time editor that only requires browsers in order to support collaborative editing and to transparently handle from small to large groups of users. Consequently, CRATE can also be used in massive online lectures, TV shows or large conferences to allow users to share their notes. CRATE's properties rely on two scientific results: (i) a replicated sequence structure with sub-linear upper bound on space complexity; this prevents the editor from running costly distributed garbage collectors, (ii) an adaptive peer sampling protocol; this prevent the editor from oversizing routing tables, hence from letting small networks pay the price of large networks. This paper describes CRATE, its properties and its usage.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {231–234},
numpages = {4},
keywords = {collaborative editor, decentralized, real-time},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890533,
author = {Pirr\`{o}, Giuseppe and Cuzzocrea, Alfredo},
title = {RECAP: Building Relatedness Explanations on the Web},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890533},
doi = {10.1145/2872518.2890533},
abstract = {We describe RECAP, a tool that, given a pair of entities defined in some Knowledge Graph (KG), builds an explanation, that is, a graph (of manageable size) reflecting their relatedness. Explanations enable to discover new knowledge and browse toward other entities of interest. We discuss different kinds of explanations based on information theory and diversity. The KG-agnostic approach adopted by RECAP, which retrieves the necessary information via SPARQL queries, makes it readily usable on a variety of KGs.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {235–238},
numpages = {4},
keywords = {relatedness explanation, rdf, sparql, path ranking},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890557,
author = {Prabhakaran, Vinodkumar and Saltzman, MIchael and Rambow, Owen},
title = {How Powerful Are You? GSPIN: Bringing Power Analysis to Your Finger Tips},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890557},
doi = {10.1145/2872518.2890557},
abstract = {We present the SPIN system, a computational tool to detect linguistic and dialog structure patterns in a social interaction that reveal the underlying power relations between its participants. The SPIN system labels sentences in an interaction with their dialog acts (i.e., communicative intents), detects instances of overt display of power, and predicts social power relations between its participants. We also describe a Google Chrome browser extension, namely gSPIN, to illustrate an exciting use-case of the SPIN system, which will be demonstrated at the demo session during the conference.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {239–242},
numpages = {4},
keywords = {email, content analysis, dialog, power relations},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890546,
author = {Riederer, Christopher and Echickson, Daniel and Huang, Stephanie and Chaintreau, Augustin},
title = {FindYou: A Personal Location Privacy Auditing Tool},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890546},
doi = {10.1145/2872518.2890546},
abstract = {The ubiquitous availability of location data to smartphone apps and online social networks has caused the collection of such information to grow at an unprecedented rate. However, the discriminative power and potential uses of this data collection is not always clear to the end user. In this work, we present FindYou, a web-based application that gives users the ability to perform a location data privacy audit. FindYou lets users import and visualize the location data collected by popular web services in order to understand what these companies know or can easily infer about them. Additionally, FindYou gives users the option to donate their data to the scientific community, creating new mobile datasets linked to user properties that will be open to use by academic institutions. We hope that FindYou will increase awareness of the privacy issues surrounding the collection and use of location data, the potential problem of "digital red-lining", and also create valuable new datasets with the full informed consent of interested users.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {243–246},
numpages = {4},
keywords = {mobility, location based social networks, algorithmic bias, personal information auditing, audit, social networks, location privacy, location},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890552,
author = {Saveski, Martin and Chu, Eric and Vosoughi, Soroush and Roy, Deb},
title = {Human Atlas: A Tool for Mapping Social Networks},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890552},
doi = {10.1145/2872518.2890552},
abstract = {Most social network analyses focus on online social networks. While these networks encode important aspects of our lives they fail to capture many real-world social connections. Most of these connections are, in fact, public and known to the members of the community. Mapping them is a task very suitable for crowdsourcing: it is easily broken down in many simple and independent subtasks. Due to the nature of social networks-presence of highly connected nodes and tightly knit groups-if we allow users to map their immediate connections and the connections between them, we will need few participants to map most connections within a community. To this end, we built the Human Atlas, a web-based tool for mapping social networks. To test it, we partially mapped the social network of the MIT Media Lab. We ran a user study and invited members of the community to use the tool. In 4.6 man-hours, 22 participants mapped 984 connections within the lab, demonstrating the potential of the tool.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {247–250},
numpages = {4},
keywords = {social network analysis, mapping social networks, crowdsourcing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890528,
author = {Seufert, Stephan and Ernst, Patrick and Bedathur, Srikanta J. and Kondreddi, Sarath Kumar and Berberich, Klaus and Weikum, Gerhard},
title = {Instant Espresso: Interactive Analysis of Relationships in Knowledge Graphs},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890528},
doi = {10.1145/2872518.2890528},
abstract = {We demonstrate InstantEspresso, a system to explain the relationship between two sets of entities in knowledge graphs. Instant-Espresso answers questions of the form. Which European politicians are related to politicians in the United States, and how? or How can one summarize the relationship between China and countries from the Middle East? Each question is specified by two sets of query entities. These sets (e.g. European politicians or United States politicians) can be determined by an initial graph query over a knowledge graph capturing relationships between real-world entities. Instant-Espresso analyzes the (indirect) relationships that connect entities from both sets and provides a user-friendly explanation of the answer in the form of concise subgraphs. These so-called relatedness cores correspond to important event complexes involving entities from the two sets. Our system provides a user interface for the specification of entity sets and displays a visually appealing visualization of the extracted subgraph to the user. The demonstrated system can be used to provide background information on the current state-of-affairs between real-world entities such as politicians, organizations, and the like, e.g. to a journalist preparing an article involving the entities of interest. InstantEspresso is available for an online demonstration at the URL http://espresso.mpi-inf.mpg.de/.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {251–254},
numpages = {4},
keywords = {knowledge graph, relatedness computation, relationship summarization},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890540,
author = {Song, Kaisong and Chen, Ling and Gao, Wei and Feng, Shi and Wang, Daling and Zhang, Chengqi},
title = {PerSentiment: A Personalized Sentiment Classification System for Microblog Users},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890540},
doi = {10.1145/2872518.2890540},
abstract = {Microblogging services are playing increasingly important roles in our daily life today. It is useful for microblog users to instantly understand the sentiment of a large number of microblogs posted by their friends and make appropriate response. Despite considerable progress on microblog sentiment classification, most of the existing works ignore the influence of personal distinctions of different microblog users on the sentiments they convey, and none of them has provided real-world personalized sentiment classification systems. Considering personal distinctions in sentiment analysis is natural and necessary as different people have different language habits, personal characters, opinion bias and so on. In this demonstration, we present a live system based on Twitter called PerSentiment, an individuality-dependent sentiment classification system which makes the first attempt to analyze the personalized sentiment of recent tweets and retweets posted by the authenticated user and the users he/she follows. Our system consists of four steps, i.e., requesting tweets via Twitter API, preprocessing collected tweets for extracting features, building personalized sentiment classifier based on a novel and extensible Latent Factor Model (LFM) trained on emoticon-tagged tweets, and finally visualizing the sentiment of friends' tweets to provide a guide for better sentiment understanding.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {255–258},
numpages = {4},
keywords = {latent factor model, microblogs, sentiment classification},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890542,
author = {Terdjimi, Mehdi and M\'{e}dini, Lionel and Mrissa, Michael},
title = {HyLAR+: Improving Hybrid Location-Agnostic Reasoning with Incremental Rule-Based Update},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890542},
doi = {10.1145/2872518.2890542},
abstract = {Web applications that rely on datasets of limited sizes to handle small but frequent updates and numerous queries have no simple way to define where data should be stored and processed. We propose a reasoning framework that can be integrated in Web applications and is able to perform the same reasoning tasks on both client or server sides. This framework embeds a rule-based reasoning engine that uses an algorithm relying on both incremental reasoning and named graphs. We evaluate the performance of our approach and compare the effects of incremental reasoning and named graphs in different experimental conditions. Results show that our reasoner can significantly reduce response times to INSERT and DELETE queries. During the demo we will exhibit how it can be used to perform reasoning tasks based on client-generated information and improve Web applications with location-agnostic reasoning.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {259–262},
numpages = {4},
keywords = {client-side reasoning, reasoning, rule-based reasoning, semantic web},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890551,
author = {Wakamiya, Shoko and Jatowt, Adam and Kawai, Yukiko and Akiyama, Toyokazu},
title = {Analyzing Global and Pairwise Collective Spatial Attention for Geo-Social Event Detection in Microblogs},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890551},
doi = {10.1145/2872518.2890551},
abstract = {Microblogging has been recently used for detecting common opinions of users at different geographic places. In this paper we propose a novel spatial visualization system for uncovering collective spatial attention and interest of users not at but rather towards different locations. In other words, we aim to answer questions of the type: what do users collectively talk about when they refer to certain geographical places? In addition, we analyze relations between geographical locations from where Twitter users issue messages and the locations they tweet about. This allows answering questions such as: what do users at a certain place commonly talk about when they refer to another geographical place? We demonstrate an online visualization system that supports the interactive analysis of collective spatial attention over time using 4 months' long collection of tweets in USA.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {263–266},
numpages = {4},
keywords = {visualization, spatial analysis, microblogs},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890556,
author = {Wang, Haofen and Fang, Zhijia and Ruan, Tong},
title = {KCF.Js: A Javascript Library for Knowledge Cards Fusion},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890556},
doi = {10.1145/2872518.2890556},
abstract = {Recently Web search engines have built knowledge graphs to support entity search and to provide structural summaries called emph{knowledge cards} for entities mentioned in queries. Different knowledge cards might be complementary or even have conflicts on values of the equivalent property. Thus, it is essential to achieve a more comprehensive fused card from those individual cards representing the same entity. In this paper, we present a system with technical details of card disambiguation, property alignment, value deduplication and card ranking to fuse knowledge cards from various search engines. We further develop a Javascript library called KCF.js based on the card fusion engine and demonstrates its usability via three possible applications.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {267–270},
numpages = {4},
keywords = {entity search, aggregated search, knowledge cards fusion},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251209,
author = {Seneviratne, Oshani and Kagal, Lalana and Sambra, Andrei},
title = {Session Details: Developers Track},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the rebooted Developers Day, at with WWW 2016.The success of the World Wide Web depends on its developers. From Hypertext and Web Browsers to the APIs that extend the capabilities of the Web, developers have played a very important role in making the Web as ubiquitous as it is today. To celebrate this great driving force behind the Web, we have rebooted the Developers Day at the World Wide Web Conference.The presenters at Developers Day are developers who have interesting open-source software to showcase to the Web developer community. The submissions that were accepted for presentation clearly showed an innovative use of technology, a potential to advance the state of the art of the Web, and a real-world application of the software. Some of the presentations include social bots, Web-based payment protocols, collaborative web environments, tracking changes in large knowledge organization systems, and testing frameworks suited for the Web. All of these topics are timely and are of interest to the general Web developer community.The program for the Developers Day consists of a round of lightning talks followed by Birds of a Feather (BoF) sessions where attendees are able to get an in-depth look at the software presented in the form of a hands-on tutorial as well as a discussion of open challenges, next steps, and application areas. We believe that such developer discourse is essential for the advancement of the Web.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889302,
author = {Davis, Clayton Allen and Varol, Onur and Ferrara, Emilio and Flammini, Alessandro and Menczer, Filippo},
title = {BotOrNot: A System to Evaluate Social Bots},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889302},
doi = {10.1145/2872518.2889302},
abstract = {While most online social media accounts are controlled by humans, these platforms also host automated agents called social bots or sybil accounts. Recent literature reported on cases of social bots imitating humans to manipulate discussions, alter the popularity of users, pollute content and spread misinformation, and even perform terrorist propaganda and recruitment actions. Here we present BotOrNot, a publicly-available service that leverages more than one thousand features to evaluate the extent to which a Twitter account exhibits similarity to the known characteristics of social bots. Since its release in May 2014, BotOrNot has served over one million requests via our website and APIs.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {273–274},
numpages = {2},
keywords = {social media, sybil account, social bot},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889304,
author = {Neubert, Joachim},
title = {Skos-History: Exploiting Web Standards for Change Tracking in Knowledge Organization Systems},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889304},
doi = {10.1145/2872518.2889304},
abstract = {"What's new?" and "What has changed?" are questions users of knowledge organizations systems, such as thesauri, classifications or taxonomies, ask, when new versions of such vocabularies are published. Until recently, it had been difficult for the publishers to provide this information (normally resorting to custom change logging within maintenance applications), and almost impossible for anybody else. With the widespread acceptance of SKOS as the standard publication and exchange format, the situation has changed fundamentally: Exact deltas between two sets of RDF triples can be computed, and the differences can be organized in a meaningful way through SPARQL queries, taking advantage of regular SKOS structures. skos-history combines a script for creating a "version store" with queries accessing this store to generate standard reports such as "added concepts" or "changed notations", or more subtle changes like concept splits, as well as aggregated statistics on certain change types, or a complete change history for a single concept across multiple versions. To allow for interactive sorting, filtering and downloading these reports, and for editing the queries in an IDE-like environment to adapt them to different vocabularies or user needs, other open source libraries are integrated.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {275–276},
numpages = {2},
keywords = {versioningl, concept evolution, kos, graph differences, rdf, sparql, named graphs, skos},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889306,
author = {Nolting, Michael and von Seggern, Jan Eike},
title = {Context-Based A/B Test Validation},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889306},
doi = {10.1145/2872518.2889306},
abstract = {Data-driven and continuous development and deployment of modern web applications depend critically on registering changes as fast aspossible, paving the way for short innovation cycles. A/B testing is a popular tool for comparing the performance of different variants. Despite the widespread use of A/B tests, there is little research on how to assert the validity of such tests. Even small changes in the application's user base, hard- or software stack not related to the variants under test can transform on possibly hidden paths into significant disturbances of the overall evaluation criterion (OEC) of an A/B test and, hence, invalidate such a test. Therefore, the highly dynamic server and client run-time environments of modern web applications make it difficult to assert correctly the validity of an A/B test.We propose the concept of test context to capture data relevant for the chosen OEC. We use pre-test data for dynamic base-lining of the target space of the system under test and to increase the statistical power. During an A/B experiment, the contexts of each variant are compared to the pre-test context to ensure the validity of the test. We have implemented this method using a generic parameter-free statistical test based on the bootstrap method focussing on frontend performance metrics.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {277–278},
numpages = {2},
keywords = {a/a testing, bootstrapping, frontend performance monitoring, a/b testing, dynamic base-lining},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889305,
author = {Schwartz, Evan},
title = {A Payment Protocol of the Web, for the Web: Or, Finally Enabling Web Micropayments with the Interledger Protocol},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889305},
doi = {10.1145/2872518.2889305},
abstract = {The history of the Web is full of attempts to enable micropayments for content and services. All have failed to achieve widespread adoption. This has fueled recurring debates about the merits or fundamental flaws of the concept of asking users to pay small amounts for what they use online. As a result, however, of the Web's lack of native payment infrastructure, the only viable business models concentrate earnings and power in a small group of content and advertising aggregators and increase demand for privacy-infringing technologies. We need to learn from the failures of previous micropayment schemes and we need to create a payment protocol that is of the Web, for the Web.We present a demo browser extension that uses the new Interledger Protocol (ILP) to demonstrate how payments and micropayments can be seamlessly built into the Web. ILP is an open payment protocol for payments across different payment networks that is being developed in the W3C Interledger Community Group. It enables new possibilities for developers and a better experience for users of the Open Web Platform.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {279–280},
numpages = {2},
keywords = {http 402, interledger protocol, micropayments, ilp},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889307,
author = {Hope-Bailie, Adrian and Thomas, Stefan},
title = {Interledger: Creating a Standard for Payments},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889307},
doi = {10.1145/2872518.2889307},
abstract = {The web has enabled free and open information exchange for a vast number of users around the world. However, it has so far failed to do the same for payments. Instead of finding the cheapest route for each payment from a competitive network of providers, we rely on a small number of proprietary operators with global reach.The work happening at the Web Payments Working Group at W3C is attempting to remove some of the friction in performing payments on the Web by defining a standard payment API and messaging in browsers. This will make payments on the Web easier but not entirely frictionless or integrated.As active participants in the W3C's Web Payments Working Group we present a browser polyfill of one of the prosed payment APIs and will walk the audience through the goals of the WG and vision of how payments will work on the Web in the future.Building on this, we will introduce the Interledger Protocol (ILP), a new neutral payments protocol being incubated in the Interledger Payments Community Group, also at the W3C. We will demonstrate how, in the future, the combination of the W3C's Web Payments APIs and the power of ILP payments will not only be frictionless but fully integrated into how we use the Web. Ubiquitous payments in an Internet of Value.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {281–282},
numpages = {2},
keywords = {internet of value, web payments, ilp, interledger protocol},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889303,
author = {Weller, Tobias and Maleshkova, Maria},
title = {Capturing and Annotating Processes Using a Collaborative Platform},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889303},
doi = {10.1145/2872518.2889303},
abstract = {Existing standards in capturing processes concentrate on client tools. Furthermore, semantic information are often available that cannot be captured in a structured way with the proposed standard formats. In addition, processes are usually used and maintained by multiple persons. Therefore, a collaborative platform to discuss and share information about processes is valuable.In order to address the challenge of maintaining and sharing knowledge about processes, we provide a tool to capture and annotate processes using Semantic MediaWiki as a collaborative platform. We demonstrate the practical applicability of our tool by presenting a demo available in the World Wide Web.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {283–284},
numpages = {2},
keywords = {business process model and notation, semantic mediawiki, semantic data management},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251210,
author = {Al-Masri, Eyhab and Liu, Tie-Yan},
title = {Session Details: PhD Symposium},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the PhD Symposium that is held in conjunction with the 25th International World Wide Web Conference, April 11 -- April 15, 2016, Montreal, Canada. The PhD Symposium of WWW2016 provides an excellent opportunity for PhD students at different stages in their research to present their ideas, and receive feedback on their work by experienced researchers and other PhD students working in research areas related to the World Wide Web.The call for papers attracted 16 submissions from Brazil, Canada, China, France, Germany, Greece, India, Ireland, United Kingdom, and the United States. The program committee reviewed and accepted 7 papers that cover a variety of topics including search and recommendation, web mining, social networks and graph analysis, crowdsourcing analysis, semantics and big data, among others. We hope that the program will serve as a valuable reference for researchers and developers in the field of World Wide Web.Putting together the WWW2016 PhD Symposium was a team effort. We first thank the authors for their contributions to the program. We must also thank the program committee members for their invaluable efforts in reviewing papers and providing constructive feedback to authors. We are also grateful to the General Chairs, James Hendler and Roger Nkambou, the Local Organization Committee Members and ACM SIGs for their guidance, support and great help in the preparation and organization of this program.We hope that you will find this program interesting and thought-provoking and that the PhD Symposium will continue its excellence and serve as an important forum for PhD candidates around the world to share their original research results in the field of World Wide Web.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {1},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888603,
author = {Abele, Andrejs},
title = {Linked Data Profiling: Identifying the Domain of Datasets Based on Data Content and Metadata},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888603},
doi = {10.1145/2872518.2888603},
abstract = {Since the beginning of the Linked Open Data initiative, the number of published open datasets has gradually increased, but the datasets often do not contain description about content such as the dataset domain (e.g., medicine, cancer), when this information is available, it is usually coarse-grained e.g. organic-edunet contains the metadata about a collection of learning objects exposed through the Organic.Edunet portal, but it is classified as Life science. In this work we propose approaches that will provide a detailed description of existing datasets as well as linking assistance when publishing new datasets by generating detailed descriptions of the publishers dataset.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {287–291},
numpages = {5},
keywords = {linked data, domain identification, linked data profiling},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888601,
author = {Aggarwal, Anupama},
title = {Detecting and Mitigating the Effect of Manipulated Reputation on Online Social Networks},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888601},
doi = {10.1145/2872518.2888601},
abstract = {In recent times, online social networks (OSNs) are being used not only to communicate but to also create a public/social image. Artists, celebrities and even common people are using social networks to build their brand value and gain more visibility either amongst a restricted set of people or public. In order to enable user to connect to other users in the OSN and gain following and appreciation from them, various OSNs provide different social metrics to the user such as Facebook likes, Twitter followers and Tumblr reblogs. Hence, these metrics give a sense of social reputation to the OSN user. As more users are trying to leverage social media to create a brand value and become more influential, spammers are luring such users to help manipulate their social reputation with the help of paid service (black markets) or collusion networks. In this work, we aim to build a robust alternate social reputation system and detect users with manipulated social reputation. In order to do so, we first start by understanding the underlying structure of various sources of crowdsourced social reputation manipulation like blackmarkets, supply-driven microtask websites and collusion networks. We then build a mechanism for an early detection of users with manipulated social reputation. Our initial results are encouraging and substantiate the possibility of a robust social reputation system.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {293–297},
numpages = {5},
keywords = {online social networks, social reputation, crowdsourced manipulation, social media, user behavior},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888602,
author = {Alrasheed, Hend},
title = {Structural Properties in δ-Hyperbolic Networks: Algorithmic Analysis and Implications},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888602},
doi = {10.1145/2872518.2888602},
abstract = {In graph theory, the δ-hyperbolicity is a global property that shows how close a given graph's structure is to the tree's structure metrically. It embeds multiple properties that facilitate solving several problems that found to be hard in the general graph form. Interestingly, not only that δ-hyperbolicity provides an idea about the structure of the graph, but also it explains how information navigates throughout the network. Therefore, δ-hyperbolicity has several applications in diverse applied fields. My PhD dissertation focuses on analyzing and exploiting structural properties of hyperbolic networks for different applications.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {299–303},
numpages = {5},
keywords = {vertex eccentricity, shortest paths, eccentricity centrality, core-periphery structure, δ-hyperbolic graph},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888600,
author = {Garc\'{\i}a-Recuero, \'{A}lvaro},
title = {Discouraging Abusive Behavior in Privacy-Preserving Online Social Networking Applications},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888600},
doi = {10.1145/2872518.2888600},
abstract = {In this position paper we present the challenge of detecting abuse in a modern Online Social Network (OSN) while balancing data utility and privacy, with the goal of limiting the amount of user sensitive information processed during data collection, extraction and analysis. While we are working with public domain data available in a contemporary OSN, our goal is to design a thorough method for future alternative OSN designs that both protect user's sensitive information and discourage abuse.In this summary, we present initial results for detecting abusive behavior on Twitter. We plan to further investigate the impact of reducing input metadata on the quality of the abuse detection. In addition, we will consider defeating Byzantine behavior by opponents in the system.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {305–309},
numpages = {5},
keywords = {privacy-preservation, online social networks, abuse detection},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888599,
author = {Jiang, Lu},
title = {Web-Scale Multimedia Search for Internet Video Content},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888599},
doi = {10.1145/2872518.2888599},
abstract = {The World Wide Web has been witnessing an explosion of video content. Video data are becoming one of the most valuable sources to assess insights and information. However, existing video search methods are still based on text matching (text-to-text search), and could fail for the huge volumes of videos that have little relevant metadata or no metadata at all. In this paper, we propose an accurate, efficient and scalable semantic search method for Internet videos that allows for intelligent and flexible search schemes over the video content (text-to-video search and text&amp;video-to-video search). To achieve this ambitious goal, we propose several novel methods to improve accuracy and efficiency. The extensive experiments demonstrate that the proposed methods are able to surpass state-of-the-art accuracy and efficiency on multiple datasets. Based on the proposed methods, we implement E-Lamp Lite, the first of its kind large-scale semantic search engine for Internet videos. According to National Institute of Standards and Technology (NIST), it achieved the best accuracy in the TRECVID Multimedia Event Detection (MED) 2013, 2014 and 2015, one of the most representative task for content-based video search. To the best of our knowledge, E-Lamp Lite is the first content-based semantic search system that is capable of indexing and searching a collection of 100 million videos.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {311–316},
numpages = {6},
keywords = {content-based retrieval, video content analysis, big data, web search, multimedia event detection},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888597,
author = {Plu, Julien},
title = {Knowledge Extraction in Web Media: At The Frontier of NLP, Machine Learning and Semantics},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888597},
doi = {10.1145/2872518.2888597},
abstract = {We identify two main factors that can cause numerous difficulties when developing a generic entity linking system: i) the amount of data currently available on the Web that do not stop to increase and where a large part comes in the form of natural language texts; ii) the velocity at which data is published that may impose to process streams of text in near real-time. Social media platforms such as Twitter, Facebook or LinkedIn become a reliable source of news and play a key role for being aware of events around the world. Encyclopedia and newspaper articles contain general knowledge of our world and they can be used to explain concepts and known entities. Videos can be associated with subtitles and images may have captions. Depending on where a text comes from, it can have different properties such as a specific language, style of writing or topic. In this research, we present a preliminary framework based on a novel hybrid architecture for an entity linking system, that combines methods from the Natural Language Processing (NLP), information retrieval and semantic fields. In particular, we propose a modular approach in order to be as independent as possible of the text to be processed. Our evaluation suggests that this framework can outperform the state-of-the-art systems or show encouraging results on three datasets: OKE2015, #Micropost 2014 and #Micropost 2015. We identify the current limitations and we provide promising future research directions.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {317–322},
numpages = {6},
keywords = {entity linking, adaptivity, entity extraction, entity pruning},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888598,
author = {Vega, Julio},
title = {Monitoring Parkinson's Disease Progression Using Behavioural Inferences, Mobile Devices and Web Technologies},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888598},
doi = {10.1145/2872518.2888598},
abstract = {Parkinson's Disease (PD) affects patients' motor and non-motor functionality. Traditional assessment techniques are inaccurate because PD symptoms vary throughout the day and are evaluated in sporadic and subjective sessions. Although recent works have utilised wearable devices to try to overcome these issues, most are unsuitable for following patients regularly for a long time. In contrast, my approach aims to monitor PD continuously in a longitudinal, naturalistic, non-disruptive and non-intrusive way. It uses smartphones to log and transmit over the Internet social, environmental, and interaction data about patients and their surroundings. This data is complemented with other web data sources (i.e., geographical and weather data) and then processed to infer a set of metrics (a latent behavioural variable or LBV) of people's activities and habits. Then, the LBV's trends are measured and mapped to the progression of the disease. As a part of the pilot study to test the proposed methodology, I have collected ~290 million records from 2 patients, making this dataset 34.5x bigger and 4x richer than state-of-the-art sets. I used the collected data to identify six possible PD-related LBVs. This project aims to get a more accurate disease picture and to reduce the physical and psychological burden of traditional assessment methods. Ultimately, the work has the potential to save patients' time and improve the efficiency and effectiveness of health services.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {323–327},
numpages = {5},
keywords = {smartphone, passive sensing, behaviour inferencing, parkinson's disease, health monitoring},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251211,
author = {Janssen, Marijn and Anthopoulos, Leonidas and Weerakkody, Vishanth},
title = {Session Details: AW4City'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 2016 ACM Workshop on WEB APPLICATIONS AND SMART CITIES -- AW4City'16, in conjunction with WWW 2016.This second event follows up last year's setup of a premier forum to address web-based application and Apps' design and development in the smart city context, which is a rapidly emerging domain and suggests a steadily evolving dominant market. Such applications are crucial, since they deliver smart services of all types to smart city habitants, visitors and businesses, while they create opportunities for new business installation and growth. Various exemplars are well known across the globe and they usually enable transactions between physical and virtual worlds.The mission of this year's workshop, is to emphasize on the contribution of web applications and Apps to current sustainability smart city challenges like urban efficiency against climate change, economic viability, adoption etc. Our short call for papers attracted submissions from Asia, Europe, and the United States. The program committee, with the contribution of additional scholars performed a blind peer-review process and accepted the following: Venue or Track - Reviewed - 8 Accepted - 6.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889308,
author = {Coleman, Beth},
title = {Generative Mappings of New Data Publics},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889308},
doi = {10.1145/2872518.2889308},
abstract = {I argue for a vital relationship between current understanding of Big Data and how we might move toward new modes of mobile application design that draw on affective and generative "data sets". Toward this end, I look at projects that use the city as a living laboratory, including mobile apps, social media mapping, and various civic uses of Internet of Things (IoT) and surveillance technology. I chose these case studies for their appropriation of psychogeographic "small data" to map a city as they engage affective and immersive aspects of interaction and spatial design.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {331},
numpages = {1},
keywords = {mobile locative design, big data, data publics},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888614,
author = {Paulin, Alois},
title = {Technological Ecosystems' Role in Preventing Neo-Feudalism in Smart-City Informatization},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888614},
doi = {10.1145/2872518.2888614},
abstract = {The paper argues that the Smart City idea lacks grounding in shared base technology and instead yields black-box artefacts. The reliance on black-box systems in public governance is considered a great hazard since it may result in sinecures, stifles democratic control of the public domain, and results in neo-feudal monopolies. Base technology (such as the WWW technology stack) on the other hand is use-neutral, implementation-neutral, open, and teach-/learn-able, thus enabling the emergence of cascading technological ecosystems, which can drive large-scale economic and societal progress. The concepts of a primary, secondary, and tertiary technological ecosystem are introduced to delineate the role and importance of base technology. The paper calls for stronger focus on Smart City foundational research and a change in culture from quick fixes to solutions that would survive generations.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {333–337},
numpages = {5},
keywords = {beyond bureaucracy, informating governance, bazaar-paradigm, democracy, system evolution, neo-feudalism},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888616,
author = {Argyriou, Iraklis},
title = {Planning the Smart City in China: Key Policy Issues and the Case of Dream Town in the City of Hangzhou},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888616},
doi = {10.1145/2872518.2888616},
abstract = {This paper offers a brief review of basic literature on smart cities to outline research issues in this field that require further attention from a governance perspective. It then maps the current urban planning landscape in China, a country which puts increasing emphasis on smart cities as a mechanism to promote sustainable development, in order to elicit key policy aspects that need to be considered in empirical analysis when "planning the Chinese smart city". The paper concludes by introducing a case of local innovation in the area of digital economy, the so-called Dream Town, undertaken in the city of Hangzhou as an illustrative example of urban China's current efforts on planning for smart city development.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {339–343},
numpages = {5},
keywords = {smart city development, planning, urban china},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888617,
author = {Gouveia, Jo\~{a}o Pedro and Seixas, J\'{u}lia and Giannakidis, George},
title = {Smart City Energy Planning: Integrating Data and Tools},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888617},
doi = {10.1145/2872518.2888617},
abstract = {This paper presents an innovative analytical framework to address incomplete interpretations and dispersed data of the energy system in cities, which usually generate multiple inefficiencies. Integrative city planning takes the city energy system from the supply to the demand while considering its spatial representativeness, and drives optimal cost-efficient assessment towards future sustainable energy targets. This holistic approach delivers more adequate policies and measures towards higher energy use efficiency. The proposed analytical framework has been developed within the INSMART EU funded project and focuses on data gathering procedures and data processing tools and models, covering a wide range of city's energy consumers, as residential buildings, transport and utilities. The results, mapped into a GIS, can be further exploited either for awareness increase of citizens and for decision support of city energy planners.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {345–350},
numpages = {6},
keywords = {buildings, integrative energy planning, transports and mobility, gis, smart meters},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888615,
author = {Anthopoulos, Leonidas G. and Reddick, Christopher G.},
title = {Smart City and Smart Government: Synonymous or Complementary?},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888615},
doi = {10.1145/2872518.2888615},
abstract = {Smart City is an emerging and multidisciplinary domain. It has been recently defined as innovation, not necessarily but mainly through information and communications technologies (ICT), which enhance urban life in terms of people, living, economy, mobility and governance. Smart government is also an emerging topic, which attracts increasing attention from scholars who work in public administration, political and information sciences. There is no widely accepted definition for smart government, but it appears to be the next step of e-government with the use of technology and innovation by governments for better performance. However, it is not clear whether these two terms co-exist or concern different domains. The aim of this paper is to investigate the term smart government and to clarify its meaning in relationship to the smart city. In this respect this paper performed a comprehensive literature review analysis and concluded that smart government is shown not to be synonymous with smart city. Our findings show that smart city has a dimension of smart government, and smart government uses smart city as an area of practice. The authors conclude that smart city is complimentary, part of larger smart government movement.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {351–355},
numpages = {5},
keywords = {smart governance, e-government, smart government, open government, smart city},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888618,
author = {Anthopoulos, Leonidas and Janssen, Marijn and Weerakkody, Vishanth},
title = {Smart Service Portfolios: Do the Cities Follow Standards?},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888618},
doi = {10.1145/2872518.2888618},
abstract = {Smart services concern the core element of a smart city, since they support the realization of urban "intelligence" in terms of people, economy, governance, environment, mobility and leaving. Smart services aim to enhance quality of life within a city and in this respect to improve "livability". The types and purposes of smart services cannot be easily pre-defined, since they are the outcome of innovation, which cannot be pre-defined either, but instead it is the product of citizens' and businesses' creativity. However, standard bodies that work on smart city definition have described smart city portfolios, which are suggested to city policy makers and potential entrepreneurs. The aim of this paper is to validate whether standardized smart service portfolios are being followed by smart cities in practice. In this regard, a set of more than 70 smart cities are examined and their smart services are matched to these portfolios. The outcomes are extremely important and leave space for future research in this regard.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {357–362},
numpages = {6},
keywords = {standardization, information cities, smart services, smart city},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888613,
author = {Ram, Sudha and Wang, Yun and Currim, Faiz and Dong, Fan and Dantas, Ezequiel and Sab\'{o}ia, Luiz Alberto},
title = {SMARTBUS: A Web Application for Smart Urban Mobility and Transportation},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888613},
doi = {10.1145/2872518.2888613},
abstract = {Systematic evaluation is crucial to the management and development of smart urban transportation, as it allows transportation planners to better understand the impact of their decisions and design targeted interventions to improve efficiency. Implementation of smart and adaptable public transportation is an important challenge in developing cities and newly industrialized economies where growth characteristics contribute to and can be impacted by factors like overcrowding and travel delays. In this paper, we focus on bus transportation, and present the design and implementation of a 3-layer web-based system for performance evaluation and decision support. This is part of a "Smart Cities" initiative, which is an international collaboration between academia and government. The first layer estimates fundamental indicators such as bus travel time and passenger demands by integrating heterogeneous data sources. A novel bus-stop network is then designed in the second layer, which enables the derivation of passenger patterns in public transit using network analysis. The third layer provides decision support by analyzing causal relationships between indicators. The proposed web-based system called SMARTBUS is being developed and validated with the city of Fortaleza in Brazil. We believe the use of generally available urban transportation data makes our methodology adaptable and customizable for other cities.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {363–368},
numpages = {6},
keywords = {web application, smart city, network analysis, urban transportation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251212,
author = {Xia, Feng and Liu, Huan and King, Irwin and Wang, Kuansan},
title = {Session Details: BigScholar'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to BigScholar 2016, The Third WWW Workshop on Big Scholarly Data: Towards the Web of Scholars. The workshop is held in Montreal, Canada, April 2016, as part of the 25th International World Wide Web Conference (WWW 2016).The BigScholar workshop aims at bringing together researchers and practitioners working on Big Scholarly Data to discuss what are emerging research issues and how to explore the Web of Scholars. Several core challenges, such as the tools and methods for analyzing and mining scholarly data will be the main center of discussions at the workshop. The goal is to contribute to the birth of a community having a shared interest around the Web of Scholars and exploring it using data mining, recommender systems, social network analysis and other appropriate technologies.In response to the call-for-papers, this third edition of the workshop received 22 submissions from Asia, Europe, South America, Canada, and the United States of America. Each paper was reviewed by at least two members of the program committee. As a result of the rigorous review process, 12 high-quality papers were accepted for presentation at the workshop and inclusion in the proceedings. In addition to paper presentations, the workshop also features two Invited Keynote Speeches delivered by Prof. C. Lee Giles from Pennsylvania State University and Prof. Jie Tang from Tsinghua University, respectively.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890512,
author = {Giles, C. Lee},
title = {Scholarly Big Data Knowledge and Semantics},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890512},
doi = {10.1145/2872518.2890512},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {371},
numpages = {1},
keywords = {semantic search, big data, digital libraries},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890513,
author = {Tang, Jie},
title = {AMiner: Mining Deep Knowledge from Big Scholar Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890513},
doi = {10.1145/2872518.2890513},
abstract = {AMiner is the second generation of the ArnetMiner system. We focus on developing author-centric analytic and mining tools for gaining a deep understanding of the large and heterogeneous networks formed by authors, papers, venues, and knowledge concepts. One fundamental goal is how to extract and integrate semantics from different sources. We have developed algorithms to automatically extract researchers' profiles from the Web and re- solve the name ambiguity problem, and connect different professional networks. We also developed methodologies to incorporate knowledge from the Wikipedia and other sources into the system to bridge the gap between network science and the web mining research. In this talk, I will focus on answering two fundamental questions for author-centric network analysis: who is who? and who are similar to each other? The system has been in operation since 2006 and has collected more than 100,000,000 author profiles, 100,000,000 publication papers, and 7,800,000 knowledge concepts. It has been widely used for collaboration recommendation, similarity analysis, and community evolution.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {373},
numpages = {1},
keywords = {big scholar data, knowledge graph, academic search, network analysis},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890516,
author = {Tsai, Chun-Hua and Lin, Yu-Ru},
title = {Tracing and Predicting Collaboration for Junior Scholars},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890516},
doi = {10.1145/2872518.2890516},
abstract = {Academic publication is a key indicator for measuring scholars' scientific productivity and has a crucial impact on their future career. Previous work has identified the positive association between the number of collaborators and academic productivity, which motivates the problem of tracing and predicting potential collaborators for junior scholars. Nevertheless, the insufficient publication record makes current approaches less effective for junior scholars. In this paper, we present an exploratory study of predicting junior scholars' future co-authorship in three different network density. By combining features based on affiliation, geographic and content information, the proposed model significantly outperforms the baseline methods by 12% in terms of sensitivity. Furthermore, the experiment result shows the association between network density and feature selection strategy. Our study sheds light on the re-evaluation of existing approaches to connect scholars in the emerging worldwide Web of Scholars.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {375–380},
numpages = {6},
keywords = {collaboration, link prediction, cold-start},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890521,
author = {Shi, Hui and Maly, Kurt and Chong, Dazhi and Yan, Gongjun and He, Wu},
title = {Backward Chaining Ontology Reasoning Systems with Custom Rules},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890521},
doi = {10.1145/2872518.2890521},
abstract = {In the semantic web, content is tagged with "meaning" or "semantics" to facilitate machine processing and web searching. In general, question answering systems that are built on top of reasoning and inference face a number of difficult issues. In this paper, we analyze scalability issues faced by a question answering system used by a knowledge base with science information that has been harvested from the web. Using this system, we will be able to answer questions that contain qualitative descriptors such as "groundbreaking", "top researcher", and "tenurable at university x". This question answering system has been built using ontologies, reasoning systems and custom based rules for the reasoning system. Furthermore, we evaluated the performance of our optimized backward chaining engine on supporting custom rules and designed the experimental environment including scalable datasets, rule sets, query sets and metrics and compared the experimental results with other in-memory ontology reasoning systems. The results show that our developed backward chaining ontology reasoning system has better scalability than in-memory reasoning systems.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {381–387},
numpages = {7},
keywords = {semantic web, custom rules, backward chaining reasoner, ontology reasoning system, ontology, benchmark},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890517,
author = {Wesley-Smith, Ian and West, Jevin D.},
title = {Babel: A Platform for Facilitating Research in Scholarly Article Discovery},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890517},
doi = {10.1145/2872518.2890517},
abstract = {The body of scientific literature is growing at an exponential rate. This expansion of scientific knowledge has increased the need for tools to help users find relevant articles. However, researchers developing new scholarly article recommendation algorithms face two substantial hurdles: acquiring high-quality, large-scale scholarly metadata and mechanisms for evaluating their recommendation algorithms. To address these problems we created Babel---an open-source platform uniting publisher, researchers, and users. Babel includes tens of millions of scholarly articles, several recommendation algorithms, and tools for integrating recommendations into publisher websites and other scholarly platforms.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {389–394},
numpages = {6},
keywords = {recommender systems, scientometrics, citation network, experimental platforms},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890524,
author = {Zhang, Jun and Xia, Feng and Wang, Wei and Bai, Xiaomei and Yu, Shuo and Bekele, Teshome Megersa and Peng, Zhong},
title = {CocaRank: A Collaboration Caliber-Based Method for Finding Academic Rising Stars},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890524},
doi = {10.1145/2872518.2890524},
abstract = {Evaluating the scientific impact of scholars has been studied by researchers from various disciplines for a long time. However, very few efforts have been devoted to evaluate the future potential of researchers based on their performance at the initial stage of scientific careers. Academic rising stars represent junior researchers who may not be very outstanding among the peers at the initial stage of their careers, but tend to become influential scholars in the future. In this paper, we propose a novel method named CocaRank, which integrates our proposed new indicator called the collaboration caliber, the typical indicator citation counts and hybrid calculation results on heterogeneous academic networks, to find academic rising stars. In addition, we investigate the appropriate time interval for the prediction of rising stars. The experimental results on real datasets demonstrate that our method can find more top ranked rising stars with higher average citation counts than other state-of-art methods.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {395–400},
numpages = {6},
keywords = {heterogeneous network, hits, pagerank, rising star},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890518,
author = {Totti, Luam C. and Mitra, Prasenjit and Ouzzani, Mourad and Zaki, Mohammed J.},
title = {A Query-Oriented Approach for Relevance in Citation Networks},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890518},
doi = {10.1145/2872518.2890518},
abstract = {Finding a relevant set of publications for a given topic of interest is a challenging problem. We propose a two-stage query-dependent approach for retrieving relevant papers given a keyword-based query. In the first stage, we utilize content similarity to select an initial seed set of publications; we then augment them by citation links weighted with information such as citation context relevance and age-based attenuation. In the second stage, we construct a multi-layer graph that expands the publications subgraph by including links to the authors, venues, and keywords. This allows us to return recommendations that are both highly authoritative, and also textually related to the query. We show that our staged approach gives superior results on three different benchmark query sets.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {401–406},
numpages = {6},
keywords = {paper recommendation, citation network analysis, relevance flow},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890520,
author = {Olensky, Marlies and Tsai, Tsung-Han and Chen, Kuan-Ta},
title = {H-Index Sequences across Fields: A Comparative Analysis},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890520},
doi = {10.1145/2872518.2890520},
abstract = {This study presents the first analysis of h-index sequences on a larger scale. Exemplarily, we investigated researchers from three different fields within Computer Science. We use Google Scholar citation profiles as data source to construct the h-index sequences of individual researchers. Our ultimate goal is to develop a self-evaluation tool, to assess one's own development of the h-index in comparison to other researchers in the same field, maybe identify career role models in the field and assess career development with future chances of success. The results of this study show that the average h-index sequences behave differently for the datasets, which is partly due to the different sample sizes. Hence, further research will be needed to confirm if every research field behaves differently. In addition, we applied the algorithm developed by Wu et al. to our data to classify the h-index sequences of individual authors according to five different shape categories. The majority of researchers has an S-shaped h-index sequence, followed by IS-shaped and linear sequences. Purely concave or convex sequences hardly ever occur. The researchers with the highest h-indices after 10 career years respectively belong to the S-shaped and IS-shaped categories with a few linear category occurrences. Hence, having a linear h-index is not only very hard to achieve, it is also not a guaranty to be the researcher with the highest h-index in a field.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {407–412},
numpages = {6},
keywords = {research evaluation, h-index sequences, large scale analysis, google scholar},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890523,
author = {Lee, Po-shen and West, Jevin D. and Howe, Bill},
title = {VizioMetrix: A Platform for Analyzing the Visual Information in Big Scholarly Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890523},
doi = {10.1145/2872518.2890523},
abstract = {We present VizioMetrix, a platform that extracts visual information from the scientific literature and makes it available for use in new information retrieval applications and for studies that look at patterns of visual information across millions of papers. New ideas are conveyed visually in the scientific literature through figures --- diagrams, photos, visualizations, tables --- but these visual elements remain ensconced in the surrounding paper and difficult to use directly to facilitate information discovery tasks or longitudinal analytics. Very few applications in information retrieval, academic search, or bibliometrics make direct use of the figures, and none attempt to recognize and exploit the type of figure, which can be used to augment interactions with a large corpus of scholarly literature.The VizioMetrix platform processes a corpus of documents, classifies the figures, organizes the results into a cloud-hosted databases, and drives three distinct applications to support bibliometric analysis and information retrieval. The first application supports information retrieval tasks by allowing rapid browsing of classified figures. The second application supports longitudinal analysis of visual patterns in the literature and facilitates data mining of these figures. The third application supports crowdsourced tagging of figures to improve classification, augment search, and facilitate new kinds of analyses. Our initial corpus is the entirety of PubMed Central (PMC), and will be released to the public alongside this paper; we welcome other researchers to make use of these resources.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {413–418},
numpages = {6},
keywords = {bibliometrics, open-data, crowdsourcing, viziometrics, information retrieval, scientometrics, figure retrieval},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890515,
author = {Whalen, Ryan and Huang, Yun and Tanis, Craig and Sawant, Anup and Uzzi, Brian and Contractor, Noshir},
title = {Citation Distance: Measuring Changes in Scientific Search Strategies},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890515},
doi = {10.1145/2872518.2890515},
abstract = {Using latent semantic analysis on the full text of scientific articles, we measure the distance between 36 million citing/cited article pairs and chart changes in citation proximity over time. The analysis shows that the mean distance between citing and cited articles has steadily increased since 1990. This demonstrates that current scholars are more likely to cite distantly related research than their peers of 20 years ago who tended to cite more proximate work. These changes coincide with the introduction of new information technologies like the Internet, and the increasing popularity of interdisciplinary and multidisciplinary research. The "citation distance" measure shows promise in improving our understanding of the evolution of knowledge. It also offers a method to add nuance to scholarly impact measures by assessing the extent to which an article influences proximate or distant future work.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {419–423},
numpages = {5},
keywords = {citation distance, scholarly impact, citation analysis},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890525,
author = {Effendy, Suhendry and Yap, Roland H.C.},
title = {Investigations on Rating Computer Sciences Conferences: An Experiment with the Microsoft Academic Graph Dataset},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890525},
doi = {10.1145/2872518.2890525},
abstract = {The rating of Computer Science (CS) conferences are important as it influences how papers published at the conferences and may also be used to evaluate research. In this paper, we proposed a method, rsit{}, based on a small given set of top conference ({em pivots}) and a relatedness measure based this set as well as basic baseline methods using citation count and field rating. We experimented with a snapshot dataset from Microsoft Academic Graph together with conference data from Microsoft Academic Search. We evaluated the conference ratings from our methods with the CCF conference rating list. We showed that rsit{} correlates well with CCF rating and correlates better than ratings from using a baseline ranking with citation count or field rating.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {425–430},
numpages = {6},
keywords = {conference rating, classification, relatedness measure},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890519,
author = {Wu, Yan and Venkat, Srini and Chiu, Dah Ming},
title = {Get To the Top and Stay There: A Study of Citation Rank Dynamics in Academia},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890519},
doi = {10.1145/2872518.2890519},
abstract = {Citations serve as an important metric for identifying experts and opinion leaders in academic communities. In this paper, we analyze the evolution of yearly rankings of top-cited (C-list) authors in the domain of Computer Science. In searching for factors that help authors become top-cited, we also gather authors in the top-collaboration list (A-list) and top-publication list (P-list) for each year, and analyze cross-correlation of the C-list with the corresponding A-list and P-list for each year. Results show that the A-list and P-list serve as (unreliable) indicators for appearance on the C-list, but their effect is quick and short-lived. Through further case studies we find other key factors, such as the seminal importance of an author's publication and the association on an author's work with hot topic trends, may significantly affect rank dynamics. Based on the study of citation rank dynamics in academia, we then discuss the modeling of rank dynamics, specifically a model based on item visibility and item strength, and the general applicability of such a model.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {431–436},
numpages = {6},
keywords = {author strength, rank dynamics, top-cited author, author visibility},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890514,
author = {Tan, Zhaowei and Liu, Changfeng and Mao, Yuning and Guo, Yunqi and Shen, Jiaming and Wang, Xinbing},
title = {AceMap: A Novel Approach towards Displaying Relationship among Academic Literatures},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890514},
doi = {10.1145/2872518.2890514},
abstract = {A large number of papers are being published every year, which makes it difficult for researchers to grasp the relationship among the scientific literatures and the big picture of academic fields. The new challenges have thus been raised, such as analyzing the complicated citation and author network, mining valuable scientific knowledge, and visualizing big scholarly data. The existing academic systems, such as Google Scholar and DBLP have mainly adopted text-based methods, while some other systems make attempts to better navigate the literatures, for example, AMiner and Science Navigation Map. Although these systems show improvements, they fail to present the academic data in a holistic way, and also have limited functions. Therefore, we need to develop new tools which can realize more modules and further explore the academic literatures.In this paper, we conceptualize and design a novel academic system, AceMap, to analyze the big scholarly data and present the results through a ``map'' approach. AceMap integrates several algorithms in the field of network analysis and data mining, and then displays the information in a clear and intuitive way, aiming to help the researchers facilitate their work. After describing the big picture, we present achieved results and our work in progress. By far, AceMap has implemented the following functions: dynamic citation network display, paper clustering, academic genealogy, author and conference homepage, etc. We have also designed and performed distributed network analysis algorithms in a cutting-edge Spark system and utilized modern visualization tools to present the results. Finally, we conclude our paper by proposing the future outlooks.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {437–442},
numpages = {6},
keywords = {visualization, big scholarly data, network analysis},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890522,
author = {Zuo, Zhiya and Wang, Xi and Eichmann, David and Zhao, Kang},
title = {Research Collaborations in Multidisciplinary Institutions: A Case Study of ISchools},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890522},
doi = {10.1145/2872518.2890522},
abstract = {Although closely related, multidisciplinarity and interdisciplinarity are different. The former indicates the co-existence of multiple disciplines while the latter is more about the integration among various areas. As collaboration between researchers from different areas is one of the major approaches for interdisciplinarity, this research investigated whether higher levels of multidisciplinarity in academic institutions are related to more collaborations, especially more interdisciplinary collaborations, among its faculty members. Using U.S. iSchools as a case study, we applied social network analysis and text mining techniques to faculty members' educational background and publication data, and proposed metrics for multidisciplinarity and collaboration interdisciplinarity. Our analysis results revealed that the multidisciplinarity of an iSchool is actually negatively correlated with the frequency and interdisciplinarity of research collaborations among its faculty members. This finding suggests that having a multidisciplinary environment alone is not sufficient to promote collaborations, nor interdisciplinary collaborations.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {443–448},
numpages = {6},
keywords = {interdisciplinarity, collaboration, network analysis, multidisciplinarity, text mining},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251227,
author = {Huth, Michael and ben Othmane, Lotfi and Jaatun, Martin Gilje and Weippl, Edgar},
title = {Session Details: ERMIS'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the Workshop on Empirical Research Methods in Information Security, associated with WWW 2016.This workshop seeks to engage empirical researchers in information security to think critically about the nusage of methods that support such empirical research, to evaluate current practice, and to offer new insights or proposals for establishing methods that enable a fairer and more robust evaluation of empirical research results, even when tensioned with particular constraints of information security such as the restricted access to meaningful data.The call for papers attracted submissions from United States, Asia and Europe. The program committee reviewed and accepted the following: Full Technical Papers Venue or Track Reviewed - 11 Accepted - 5.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {1},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888610,
author = {Alrawi, Omar and Mohaisen, Aziz},
title = {Chains of Distrust: Towards Understanding Certificates Used for Signing Malicious Applications},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888610},
doi = {10.1145/2872518.2888610},
abstract = {Digital certificates are key component of trust used by many operating systems. Modern operating systems implement a form of digital signature verification for various applications, including kernel driver installation, software execution, etc. Digital signatures rely on digital certificates that authenticate the signature, which then verify the validity of a given signature for a signed binary. Malware attempts to subvert the chain of trust through several techniques to achieve execution, evasion, and persistence. In this paper, we examine a large corpus of malware ($3.3$ million samples) to extract digital signatures and their corresponding certificates. We examine several characteristics of the digital certificates to study features in the process of malware authorship that will potentially be used for characterizing and classifying malware. We look at many features including the certificate's chain length, the issue and expiration year, the validity duration of a certificate, the issuing country, validity, top issuing certificate authorities (CAs), and others, highlighting potentially discriminatory features.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {451–456},
numpages = {6},
keywords = {malware, certificates, authenticode},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888605,
author = {Chen, Ping and Desmet, Lieven and Huygens, Christophe and Joosen, Wouter},
title = {Longitudinal Study of the Use of Client-Side Security Mechanisms on the European Web},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888605},
doi = {10.1145/2872518.2888605},
abstract = {As the web rapidly expands and gets integrated into all kinds of business, browsing the web has become an important part of people's daily lives. With the rising importance of various web applications sit in a browser, attackers also shifted their focus towards client-side attacks. To defend against these attacks, numerous client-side security mechanisms for the browser are proposed. The presence of these mechanisms on a website can be used as an indicator of the security awareness and practices of that website.In this paper, through a large-scale analysis of more than 18,000 European websites over two years, we analyze the longitudinal trends of the adoption of client-side security mechanisms. We validate that the most popular websites were adopting new security features quicker that less popular websites in the two year timeframe. By examining the websites based on their business vertical, we observe that the websites in the Finance and Education category are outperforming other verticals in the data set, with respect to the usage of client-side security mechanisms.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {457–462},
numpages = {6},
keywords = {empirical research, client-side security, european web},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888608,
author = {Kul, Gokhan and Luong, Duc and Xie, Ting and Coonan, Patrick and Chandola, Varun and Kennedy, Oliver and Upadhyaya, Shambhu},
title = {Ettu: Analyzing Query Intents in Corporate Databases},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888608},
doi = {10.1145/2872518.2888608},
abstract = {Insider threats to databases in the financial sector have become a very serious and pervasive security problem. This paper proposes a framework to analyze access patterns to databases by clustering SQL queries issued to the database. Our system Ettu works by grouping queries with other similarly structured queries. The small number of intent groups that result can then be efficiently labeled by human operators. We show how our system is designed and how the components of the system work. Our preliminary results show that our system accurately models user intent.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {463–466},
numpages = {4},
keywords = {insider threats, clustering, databases},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888609,
author = {Marschalek, Stefan and Kaiser, Manfred and Luh, Robert and Schrittwieser, Sebastian},
title = {Empirical Malware Research through Observation of System Behaviour},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888609},
doi = {10.1145/2872518.2888609},
abstract = {Behavioural analysis has become an important method of today's malware research. Malicious software is executed inside a controlled environment where its runtime behaviour can be studied. Recently, we proposed the concept of not only observing individual executables but a computer system as a whole. The basic idea is to identify malware by detecting anomalies in the way a system behaves. In this paper we discuss our methodology for empirical malware research and highlight its strengths and limitations. Furthermore, we explain the challenges we faced during our research and describe our lessons learned.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {467–469},
numpages = {3},
keywords = {security, system bahaviour, malware},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888606,
author = {Mayer, Wilfried and Schmiedecker, Martin},
title = {TLScompare: Crowdsourcing Rules for HTTPS Everywhere},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888606},
doi = {10.1145/2872518.2888606},
abstract = {For billions of users, today's Internet has become a critical infrastructure for information retrieval, social interaction and online commerce. However, in recent years research has shown that mechanisms to increase security and privacy like HTTPS are seldomly employed by default. With the exception of some notable key players like Google or Facebook, the transition to protecting not only sensitive information flows but all communication content using TLS is still in the early stages. While non-significant portion of the web can be reached securely using an open-source browser extension called HTTPS Everywhere by the EFF, the rules fueling it are so far manually created and maintained by a small set of people.In this paper we present our findings in creating and validating rules for HTTPS Everywhere using crowdsourcing approaches. We created a publicly reachable platform at tlscompare.org to validate new as well as existing rules at large scale. Over a period of approximately 5 months we obtained results for more than 7,500 websites, using multiple seeding approaches. In total, the users of TLScompare spent more than 28 hours of comparing time to validate existing and new rules for HTTPS Everywhere. One of our key findings is that users tend to disagree even regarding binary decisions like whether two websites are similar over port 80 and 443.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {471–476},
numpages = {6},
keywords = {crowdsourcing, https everywhere, security, tls, https, privacy},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2888607,
author = {Pirker, Martin and Nusser, Andreas},
title = {A Work-Flow for Empirical Exploration of Security Events},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888607},
doi = {10.1145/2872518.2888607},
abstract = {As the internet continuously expands, information security research is a never ending challenge. It is impossible to know all internet participants, protocols and their applications. Instead, security research focuses on empirically collected real-world data; stores, processes, transforms and analyses the data, in order to learn from it and its anomalies --- security issues --- as they happen. This paper presents one practical work-flow for collection and processing of security related data. It present a hard- and software setup, experiences made, and estimates future developments. This gives others the opportunity to learn and identify areas for improvement, especially those in the early stages of setting up a research project based on empirically gathered data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {477–480},
numpages = {4},
keywords = {security data mining, big data, virtualisation containers},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251213,
author = {Dietze, Stefan and d'Aquin, Mathieu and Herder, Eelco and Gasevic, Dragan and Sack, Harald},
title = {Session Details: LILE'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {Distance teaching and openly available educational resources on the Web are becoming common practices. Public higher education institutions as well as private training organisations increasingly realise the benefits of online resources. In addition, informal learning and knowledge exchange are inherent to the online interactions found on the Web in general. These interactions involve, for instance, learning and knowledgecentric social networks -- such as Bibsonomy, Slideshare or Videolectures -- but also general-purpose social environments such as LinkedIn, where matters related to skills, competence development or training are central concerns of involved stakeholders. These interactions generate a vast amount of informal knowledge resources of varying granularity, as well as indicators for learning and competences, which are currently under-investigated.On the other hand, the widespread adoption of Linked Data principles [2], as well as the more recent widespread adoption of embedded annotations through schema.org, Microformats and RDFa, has led to the availability of vast amounts of semi-structured data [1], which facilitates interpretation and reuse of Web content and data [4]. This includes schemas and vocabularies directly focused on learning (e.g., LRMI, AAISO, BIBO) and more knowledge-oriented datasets, such as the ones gathered by LinkedEducation.org1, LinkedUniversities.org2 and LinkedUp3. These repositories offer data from The Open University (UK), Learning Analytics datasets and resources [3], or the mEducator Linked Educational Resources [5] , as well as general purpose knowledge graphs, such as DBpedia, WordNet RDF.This has led to the creation of an embryonic "Web of Educational Data", which is largely focused on sharing semi-structured metadata about resources, but which still lacks sufficient recognition of learning-related activities and knowledge resources that are prevalent in less structured and informal online settings. On the other hand, progress in methods and tools for Entity-centric approaches for analysing and understanding the wealth of data on the Web -- such as entity extraction, linking and retrieval -- have paved the way for the exploration of Web data and knowledge relevant to learning and education. The widespread analysis of both informal and formal learning activities and resources has the potential to fundamentally aid and transform the production, recommendation and consumption of learning services and content. However, widespread take-up of such approaches is still hindered by issues that are both technical as well interdisciplinary.Building on the success of previous editions (LILE 2011-2015) 4, LILE20165 addresses such challenges by providing a forum for researchers and practitioners who make innovative use of Web Data for educational purposes. After extensive peer review (each submission was reviewed by at least three independent reviewers) we were able to select 7 papers for presentation in the program. The workshop would not have been possible without contributions of many people and institutions. We are very thankful to the organizers of the WWW 2016 conference for providing us with the opportunity to organize the workshop, for their excellent collaboration, and for looking after many important logistic issues. We are also very grateful to the members of the program committee for their commitment in reviewing the papers and assuring the good quality of the workshop program. We also thank all authors and invited speakers for their invaluable contributions to the workshop. Of course, great appreciation goes to our sponsors GNOSS6, AFEL and EATEL. We thank all supporters of LILE2016 for making this event possible.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890458,
author = {Absar, Rafa and Gruzd, Anatoliy and Haythornthwaite, Caroline and Paulin, Drew},
title = {Linking Online Identities and Content in Connectivist MOOCs across Multiple Social Media Platforms},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890458},
doi = {10.1145/2872518.2890458},
abstract = {In this paper, we examine how multiple social media platforms are being used for formal and informal learning by examining data from two connectivist MOOCs (or cMOOCs). Our overarching goal is to develop and evaluate methods for learning analytics to detect and study collaborative learning processes. For this paper, we focus on how to link multiple online identities of learners and their contributions across several social media platforms in order to study their learning behaviours in open online environments. Many challenges were found in collection, processing, and analyzing the data; results are presented here to provide others with insight into such issues for examining data across multiple open media platforms.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {483–488},
numpages = {6},
keywords = {social media, connectivism, moocs., learning, social networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890460,
author = {Durand, Guillaume and Belacel, Nabil and Goutte, Cyril},
title = {Competency Based Learning in the Web of Learning Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890460},
doi = {10.1145/2872518.2890460},
abstract = {In this paper, we present, discuss and summarize different research works we carried out toward the exploitation of the Web of data for learning and training purpose (Web of learning data). For several years now, we have conducted efforts to explore this main objective through two complementary directions. The first direction is the scalability and particularly the need to develop methods able to provide learners with adequate learning path in the world of big data. The second direction is related to the transition from Web data to Web of learning data and particularly the extraction of cognitive attributes from Web content. For this purpose, we proposed different text mining techniques as well as the development of competency framework engineering tools. Resulting evidence-based techniques allow us to properly evaluate and improve the relationships between learning materials, performance records and student competencies. Although some questions remain unanswered and challenging technology improvements are still required, promising results and developments are arising.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {489–494},
numpages = {6},
keywords = {web learning data recommendation, learning skills engineering., web data features extraction},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890461,
author = {Liu, Yang and Williamson, Carey},
title = {Workload Study of a Media-Rich Educational Web Site},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890461},
doi = {10.1145/2872518.2890461},
abstract = {Modern educational Web sites often feature a rich assortment of linked media content. In this paper, we present a workload study of such an educational Web site hosted at the University of Calgary. Three main insights emerge from our study. First, educational Web sites can generate large volumes of Internet traffic, even when the number of users is limited. Second, network usage is highly influenced by course-related events, such as midterms and finals. Third, the approach used by the site for displaying videos can have adverse impacts on user experience and network traffic. We demonstrate these effects with active measurement of different Web browser and video player implementations.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {495–500},
numpages = {6},
keywords = {http, video, workload characterization, educational web site, media, technology enhanced learning},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890462,
author = {Mouromtsev, Dmitry and Romanov, Aleksei and Volchek, Dmitry and Kozlov, Fedor},
title = {Metadata Extraction from Open EdX Online Courses Using Dynamic Mapping of NoSQL Queries},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890462},
doi = {10.1145/2872518.2890462},
abstract = {The paper describes use cases and architecture of the course extraction plugin for the Open edX platform build upon Linked Open Data. The issue of frequent repetitions of educational materials within the MOOC and relativity of recommendation tools for course developers is considered. Comprehensive review of the designed ontology and mapping, as well as evaluation using test courses are given. The last part of the paper discusses new possibilities and opportunities for the future work.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {501–506},
numpages = {6},
keywords = {metadata, edx, semantic web, education, linked learning, linked data in education, educational ontology population},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890459,
author = {Penteado, Bruno Elias},
title = {Correlational Analysis Between School Performance and Municipal Indicators in Brazil Supported by Linked Open Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890459},
doi = {10.1145/2872518.2890459},
abstract = {The advance in quality of public education is a challenge to public managers in contemporary society. In this sense, many studies point to the strong influence of socioeconomical factors in school performance but it is a challenge to select proper data to perform analyses on this matter. In tandem, it has happening a growth in provision of big quantities of educational indicators data, but in isolate cases, and by different agencies of Brazilian government. For this work, we use both education and economic indicators for analysis. The following socioeconomical indicators were selected: municipal human development index (MHDI), social vulnerability index (SVI), Gini coefficient and variables extracted from DBpedia, as part of the connection of this data to the Web of data: GDP per capita and municipal population. These data were used as independent variables to look into their correlations with Brazilian Basic Education Development Index (IDEB) performances at municipal level, supported by the application of linked open data principles. OpenRefine was used to extract the data from different sources, convert to RDF triples and then the mapping of the variables to existing ontologies and vocabularies in this domain, aiming at the reuse of existing semantics. The correlational analysis of the variables showed coherence with the literature about the theme, with significative magnitude between IDEB performances and the indicators related to income and parent education (SVI and HDI), besides moderate relations with the other varibles, except for the municipal population. Finally, the consolidated dataset, enriched by information extracted DBpedia was made available by a SPARQL endpoint for queries of humans and software agents, allowing other applications and researchers to explore the data from other platforms.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {507–512},
numpages = {6},
keywords = {ideb, school performance, lod, educational indicators, linked open data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890464,
author = {Taibi, Davide and Dietze, Stefan},
title = {Towards Embedded Markup of Learning Resources on the Web: An Initial Quantitative Analysis of LRMI Terms Usage},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890464},
doi = {10.1145/2872518.2890464},
abstract = {Embedded markup of Web pages have emerged as a significant source of structured data on the Web. In this context, the LRMI initiative has provided a set of vocabulary terms, now part of the schema.org vocabulary, to enable the markup of resources of educational value. In this paper we present a preliminary analysis of the use of LRMI terms on the Web by assessing LRMI-based statements extracted from the Web Data Commons dataset.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {513–517},
numpages = {5},
keywords = {lrmi, schema.org, web data commons, linked data for education},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890463,
author = {Wang, Shuting and Liu, Lei},
title = {Prerequisite Concept Maps Extraction for AutomaticAssessment},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890463},
doi = {10.1145/2872518.2890463},
abstract = {Traditional assessment modes usually give identical set of questions to each student, thus are inefficient for students to fix their problems. In order to perform an efficient assessment, we utilize prerequisite concept maps to find students' learning gaps and work on closing these gaps and proposed a two-phase model for concept map construction. Experiments on concept pairs with prerequisite relationships which are manually created show the promise of our proposed method. In order to meet the challenge of using concept maps in automatic assessments, we also derive a top-k concept selection algorithm which allows students to view different numbers of concepts.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {519–521},
numpages = {3},
keywords = {assessment, wikipedia, prerequiste concept map},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251214,
author = {Ahlers, Dirk and Wilde, Erik and Martins, Bruno},
title = {Session Details: LocWeb'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 6th International Workshop on Location and the Web, associated with WWW 2016. This is the sixth workshop in its series, having previously been held at WWW (LocWeb 2008), CHI (LocWeb 2009), IoT (LocWeb 2010), CIKM (LocWeb 2014), and WWW (LocWeb 2015).LocWeb continues following its main objective of bringing together a community of researchers at the intersection of location and the Web, serving as a unique venue to integrate different backgrounds and stimulating the exchange of ideas and fostering closer cooperation. LocWeb will provide a topic-specific venue where researchers from different fields, be it data mining, recommendation, search, systems, services, social media, applications, or standards, can discuss and develop the role of location. Its focus lies in Web-scale services and systems facilitating location-aware information access. We aim for a highly interactive, collaborative workshop with ample room for discussion that will explore and advance the geospatial topic.The location topic is understood as a crosscutting issue equally concerning information access, semantics and standards, and Web-scale systems and services. The workshop establishes an integrated venue where the location aspect can be discussed in depth within an interested community. LocWeb follows the main theme of Location-Aware Information Access, with subtopics related to Search, Analytics, Mobility, Apps, Services, and Systems. It is designed to reflect the multitude of fields that demand and utilize location features from an interdisciplinary perspective.The call for papers attracted submissions from Europe, the Americas, Asia, and the Middle East. The program committee reviewed and accepted the following: Venue or Track Reviewed - 5, Accepted - 3.We encourage WWW attendees to attend the three accepted paper presentations, a keynote talk, and a discussion session. The detailed programme will be available on the workshop website.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890469,
author = {Aiello, Luca Maria},
title = {The Sensorial Map of the City},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890469},
doi = {10.1145/2872518.2890469},
abstract = {Our daily urban experiences are the product of our perceptions and senses, yet the complete sensorial range is strikingly absent from urban studies. Sight has been historically privileged over the other senses and urban studies. However, smell and sound have also a huge influence over how we perceive places, they impact our behavior, attitudes and health. Yet, city planning is concerned only with a few bad smells and with limiting noise levels. We propose a new way of capturing nuanced sensorial perceptions of cities from data implicitly generated by social media users and of producing detailed sensorial maps of our cities.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {529},
numpages = {1},
keywords = {sound, urban informatics, smell},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890466,
author = {Bagci, Hakan and Karagoz, Pinar},
title = {Context-Aware Friend Recommendation for Location Based Social Networks Using Random Walk},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890466},
doi = {10.1145/2872518.2890466},
abstract = {The location-based social networks (LBSN) facilitate users to check-in their current location and share it with other users. The accumulated check-in data can be employed for the benefit of users by providing personalized recommendations. In this paper, we propose a random walk based context-aware friend recommendation algorithm (RWCFR). RWCFR considers the current context (i.e. current social relations, personal preferences and current location) of the user to provide personalized recommendations. Our LBSN model is an undirected unweighted graph model that represents users, locations, and their relationships. We build a graph according to the current context of the user depending on this LBSN model. In order to rank the recommendation scores of the users for friend recommendation, a random walk with restart approach is employed. We compare RWCFR with popularity-based, friend-based and expert-based baseline approaches. According to the results, our friend recommendation algorithm outperforms these approaches in all the tests.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {531–536},
numpages = {6},
keywords = {random walk, friend recommendation, location-based social networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890468,
author = {Esp\'{\i}n Noboa, Lisette and Lemmerich, Florian and Singer, Philipp and Strohmaier, Markus},
title = {Discovering and Characterizing Mobility Patterns in Urban Spaces: A Study of Manhattan Taxi Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890468},
doi = {10.1145/2872518.2890468},
abstract = {Nowadays, human movement in urban spaces can be traced digitally in many cases. It can be observed that movement patterns are not constant, but vary across time and space. In this work, we characterize such spatio-temporal patterns with an innovative combination of two separate approaches that have been utilized for studying human mobility in the past. First, by using non-negative tensor factorization (NTF), we are able to cluster human behavior based on spatio-temporal dimensions. Second, for characterizing these clusters, we propose to use HypTrails, a Bayesian approach for expressing and comparing hypotheses about human trails. To formalize hypotheses, we utilize publicly available Web data (i.e., Foursquare and census data). By studying taxi data in Manhattan, we can discover and characterize human mobility patterns that cannot be identified in a collective analysis. As one example, we find a group of taxi rides that end at locations with a high number of party venues on weekend nights. Our findings argue for a more fine-grained analysis of human mobility in order to make informed decisions for e.g., enhancing urban structures, tailored traffic control and location-based recommender systems.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {537–542},
numpages = {6},
keywords = {tensor factorization, hyptrails, human keywords},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890467,
author = {Rushforth, Peter},
title = {Maps for HTML: A New Media Type and Prototype Client for Web Mapping},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890467},
doi = {10.1145/2872518.2890467},
abstract = {The Web has a long history of Web mapping, being originally described at the first WWW Conference in 1994. Web maps have evolved significantly since then, and patterns have developed. The patterns of modern Web mapping underlie mapping programs across economic sectors. Key issues which remain are that Web standards do not directly address the needs of mapping, and Web mapping standards do not rely on Web architecture. As a result of this lack of collaboration, Web mapping and Web standards continue to evolve independently, with little coordination based on a broader interest. This has led to a situation where newcomers to Web mapping are faced with the problem as to what technologies to use for creating and publishing Web maps, of which the unintended consequence is increasing centralization of Web mapping. This paper documents the results of design and development done by Natural Resources Canada within the scope of the Maps For HTML Community Group. A declarative Web map extension to the HTML standard is proposed, together with a new supporting hypermedia type. Taken together, the proposed standards will progressively support simple to advanced Web map applications, including considerations of layers, projections and feature styling. If widely implemented, the proposed Web standards could help realize the value of the substantial investments in spatial data across all sectors of society.The paper concludes with several propositions drawn from the discussion, and proposed actions to be undertaken by the Maps for HTML Community Group, in which the reader is invited to participate.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {543–548},
numpages = {6},
keywords = {mapping, web maps, hypermedia, standards, cartography},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251215,
author = {Atzmueller, Martin and Chin, Alvin and Trattner, Christoph},
title = {Session Details: MSM'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {In our first workshop on Modeling Social Media (MSM 2010 in Toronto, Canada), we explored various different models of social media ranging from user modeling, hypertext models, software engineering models, sociological models and framework models. In our second workshop (MSM 2011 in Boston, USA), we addressed the user interface aspects of modeling social media. In our third workshop (MSM 2012 in Milwaukee, USA), we looked at the collective intelligence in social media, i.e. making sense of the content and context from social media websites such as Facebook, Twitter, Google+ and Foursquare by banalyzing tweets, tags, blog posts, likes, posts and check-ins, in order to create a new knowledge and semantic meaning. Our fourth workshop (MSM 2013 in Paris, France) then especially considered "recommender systems" for social media, also tackling the increasing information overload problem for recommending "things" in social media. The workshop in the last two years (MSM 2014 in Seoul, Korea and MSM 2015 in Florence, Italy) focused on mining Big Data on social media and the web.Behavioral analytics is an important topic, e.g., concerning web applications as well as mobile and ubiquitous applications, for understanding user behavior. Following the discussion at our workshop at WWW 2015 we aim to continue our focus on behavioral analytics on social media and the web, however, with a special focus: We aim to go beyond standard analytics approaches and try to answer the "why" question, which is often missing in analytical papers.The call for papers attracted 17 submissions, from which we were able to accept 8 submissions (five full papers and three short papers) based on a rigorous reviewing process. The accepted papers cover a variety of topics, including social media and dynamic behavioral analytics, usage analysis, recommendation, and behavior prediction. We hope that these proceedings will serve as a valuable reference for researchers and developers.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2896920,
author = {Gruzd, Anatoliy},
title = {Who Are We Modelling: Bots or Humans?},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2896920},
doi = {10.1145/2872518.2896920},
abstract = {Computational techniques such as Behavioural Analytics (BA) have been extremely effective at transforming social media data into useful insights for applications such as recommender systems [1] and customer relation management [2]. However, due to the rise of smarter, more sophisticated social bots [3] and the increasing reliance on algorithmic filtering (which nudges online users to make certain choices and take specific actions) [4], it begs the question, if we are using data from social media for modelling, are we modelling human behavior in social media or simply reverse engineering how bots and other algorithms operate?},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {551},
numpages = {1},
keywords = {social media data, social science research, bot detection, algorithmic filtering},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890082,
author = {Atzmueller, Martin and Schmidt, Andreas and Kibanov, Mark},
title = {DASHTrails: An Approach for Modeling and Analysis of Distribution-Adapted Sequential Hypotheses and Trails},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890082},
doi = {10.1145/2872518.2890082},
abstract = {The analysis of sequential trails and patterns is a prominent research topic. However, typically only explicitly observed trails are considered. In contrast, this paper proposes the DASHTrails approach that enables the modeling and analysis of distribution-adapted sequential trails and hypotheses. It presents a method for deriving transition matrices given a probability distribution over certain events. We demonstrate the applicability of the proposed approach using real-world data in the mobility domain, i.e., car trajectories and spatio-temporal distributions on car accidents.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {553–558},
numpages = {6},
keywords = {social network analysis, behavioral analytics, human trails, sequential hypothesis, mobility, social media},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890086,
author = {Eberhard, Lukas and Trattner, Christoph},
title = {Recommending Sellers to Buyers in Virtual Marketplaces Leveraging Social Information},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890086},
doi = {10.1145/2872518.2890086},
abstract = {Social information such as stated interests or geographic check-ins in social networks has shown to be useful in many recommender tasks recently. Although many successful examples exist, not much attention has been put on exploring the extent to which social impact is useful for the task of recommending sellers to buyers in virtual marketplaces. To contribute to this sparse field of research we collected data of a marketplace and a social network in the virtual world of Second Life and introduced several social features and similarity metrics that we used as input for a user-based $k$-nearest neighbor collaborative filtering method. As our results reveal, most of the types of social information and features which we used are useful to tackle the problem we defined. Social information such as joined groups or stated interests are more useful, while others such as places users have been checking in, do not help much for recommending sellers to buyers. Furthermore, we find that some of the features significantly vary in their predictive power over time, while others show more stable behaviors. This research is relevant for researchers interested in recommender systems and online marketplace research as well as for engineers interested in feature engineering.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {559–564},
numpages = {6},
keywords = {recommender systems, online marketplaces, online social networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890085,
author = {Ferwerda, Bruce and Schedl, Markus and Tkalcic, Marko},
title = {Personality Traits and the Relationship with (Non-) Disclosure Behavior on Facebook},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890085},
doi = {10.1145/2872518.2890085},
abstract = {Applications increasingly use personality traits to provide a personalized service to the user. To acquire personality, social media trails showed to be a reliable source. However, until now, analysis of social media trails have been focusing on textit{what} has been disclosed: content of disclosed items. These methods fail to acquire personality when there is a lack of content (non-disclosure). In this study we do not look at the disclosed content, but whether disclosure occurred or not. We extracted 40 items of different Facebook profile sections that users can disclose or not disclose. We asked participants to indicate to which extent they disclose the items in an online survey, and additionally asked them to fill in a personality questionnaire. Among 100 participants we found that users' personality can be predicted by solely looking at whether they disclose particular sections of their profiles. This allows for personality acquisition when content is missing.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {565–568},
numpages = {4},
keywords = {personality prediction, disclosure, personality, facebook, non-disclosure},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890084,
author = {Gallegos, Luciano and Lerman, Kristina and Huang, Arhur and Garcia, David},
title = {Geography of Emotion: Where in a City Are People Happier?},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890084},
doi = {10.1145/2872518.2890084},
abstract = {During the last years, researchers explored the geographic and environmental factors that affect happiness. More recently, location-sharing services provided by the social media has given an unprecedented access to geo-located data for studying the interplay between these factors on a much bigger scale. Do location-sharing services help in turn at distinguishing emotions in places within a city? Which aspects contribute better at understanding happier places? To answer these questions, we use data from Foursquare location-sharing service to identify areas within a major US metropolitan area with many check-ins, i.e., areas that people like to use. We then use data from the Twitter microblogging platform to analyze the properties of these areas. Specifically, we have extracted a large corpus of geo-tagged messages, called tweets, from a major metropolitan area and linked them US Census data through their locations. This allows us to measure the sentiment expressed in tweets that are posted from a specific area, and also use that area's demographic properties in analysis. Our results reveal that areas with many check-ins are different from other areas within the metropolitan region. In particular, these areas have happier tweets, which also encourage people living in it or from other areas to commute longer distances to these places. These findings shed light on the influence certain places play within a city regarding people's emotions and mobility, which in turn can be used for city planners for designing happier and more equitable cities.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {569–574},
numpages = {6},
keywords = {sentiment analysis, location-sharing services; mobility; sentiment analysis; social media, location-sharing services, social media, mobility},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890088,
author = {Khazaei, Taraneh and Xiao, Lu and Mercer, Robert and Khan, Atif},
title = {Privacy Behaviour and Profile Configuration in Twitter},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890088},
doi = {10.1145/2872518.2890088},
abstract = {The dichotomy between users' privacy behaviours and their privacy attitudes is a widely observed phenomenon in online social media. Such a disparity can be mainly attributed to the users' lack of awareness about the default privacy setting in social networking websites, which is often open and permissive. This problem has led to a large number of publicly available accounts that may belong to privacy-concerned users. As an initial step toward addressing this issue, we examined whether profile attributes of Twitter users with varying privacy settings are configured differently. As a result of the analysis, a set of features is identified and used to predict user privacy settings. For our best classifier, we obtained an F-score of 0.71, which outperforms the baselines considerably. Hence, profile attributes proved valuable for our task and suggest the possibility of the automatic detection of public accounts intended to be private based on online social footprints.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {575–580},
numpages = {6},
keywords = {profile attributes, social privacy, twitter analysis},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890083,
author = {Rudolph, Maja R. and Hoffman, Matthew and Hertzmann, Aaron},
title = {A Joint Model for Who-to-Follow and What-to-View Recommendations on Behance},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890083},
doi = {10.1145/2872518.2890083},
abstract = {Good recommendations are a key tool to increase user engagement and user satisfaction on many social networks. Here we focus on Behance, a social network for artist from various fields such as typography, street art, industrial design, and fashion. On Behance, the artists can connect by following each other, display their work in online portfolios, and brows each other's work. Each user has a personalized dashboard which is an integral part of the Behance experience.In this work we create a joint behavior model which jointly models the users' viewing behavior and the social network. The joint model which we fit with variational inference is capable of producing both who-to-follow and what-to-view recommendations. We show on real data from Behance that the joint behavior model outperforms a Poisson factorization approach which treats both data sources separately.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {581–584},
numpages = {4},
keywords = {recommendation systems, collaborative filtering, social networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890087,
author = {Schnitzer, Steffen and Neitzel, Svenja and Schmidt, Sebastian and Rensing, Christoph},
title = {Perceived Task Similarities for Task Recommendation in Crowdsourcing Systems},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890087},
doi = {10.1145/2872518.2890087},
abstract = {Crowdsourcing platforms support the assignment of jobs while relying on the workers' search capabilities. Recommenders can support the workers' decisions to improve quality and outcome for both worker and requester. A precedent study showed, that many workers expect to get tasks recommended, which are similar to previously finished ones. In order to create genuine task recommendation, similarities between tasks have to be identified and analyzed. Therefore, this work provides an empirical study about how workers perceive task similarities. The perceived task similarities may vary between workers with different cultural background and may depend e.g. on the complexity, required action or the requester of the task.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {585–590},
numpages = {6},
keywords = {user survey, recommender systems, crowdsourcing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890089,
author = {Wei, Pengyu and Wang, Ning},
title = {Wikipedia and Stock Return: Wikipedia Usage Pattern Helps to Predict the Individual Stock Movement},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890089},
doi = {10.1145/2872518.2890089},
abstract = {Vast volumes of online information related to news stories, blogs and online social media have an observable effect on investor's opinions towards financial markets. But do these particular information reflect or impact people's decision-making in investment? This paper investigates whether data generated from Internet usage can be used to predict the movements in the financial market. We provide evidence that data on how often a company's Wikipedia page is being viewed is linked to its subsequent performance in the stock market. We then develop a portfolio in line with the Wikipedia usages and demonstrate that our investment strategy based on Wikipedia views is profitable both financially and statistically. Our finding implies that online web data such as Wikipedia presents an alternative insights on collecting and quantifying investor's sentiments towards financial markets, which can be further employed as a timely approximation of investor's behaviours in decision-making.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {591–594},
numpages = {4},
keywords = {behavioural analytics, stock market prediction, wikipedia},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251228,
author = {Habib, Mena B. and Kunneman, Florian and van Keulen, Maurice},
title = {Session Details: NLPIT'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 2nd International Workshop on Natural Language Processing for Informal Text (NLPIT), associated with WWW 2016.The rapid growth of Internet usage in the last two decades adds new challenges to understand the informal user generated content (UGC) on the Internet. Textual UGC refers to textual posts on social media, blogs, emails, chat conversations, instant messages, forums, reviews, or advertisements that are created by endusers of an online system. A large portion of language used on textual UGC is informal. Informal text is the style of writing that disregards language grammars and uses a mixture of abbreviations and context dependent terms. The straightforward application of state-of-the-art Natural Language Processing approaches on informal text typically results in a significantly degraded performance due to the following reasons: the lack of sentence structure; the lack of enough context required; the seldom entities involved; the noisy sparse contents of users' contributions; and the untrusted facts contained.The NLPIT workshop hopes to bring opportunities and challenges involved in informal text processing under the attention of researchers. In particular, we are interested in discussing informal text modeling, normalization, mining, and understanding in addition to various application areas in which UGC is involved.The workshop is a follow-up of the first NLPIT workshop that was held in conjunction with ICWE: the International Conference on Web Engineering held in Rotterdam, The Netherlands, from 23rd to 26th of July 2015.The call for papers attracted submissions from 15 different countries. The program committee reviewed and accepted the following: Venue or Track: Reviewed - 16 Accepted - 6.The workshop started with a keynote presentation given by Rapha\"{e}l Troncy from EURECOM, France entitled "Linking Entities for Enriching and Structuring Social Media Content". The keynote is followed by 6 research presentations. The common theme of the research presentations is, analogous to the first edition of NLPIT, NLP for a multitude of languages. Among them the papers and presentations feature Arabic, Spanish, Russian, Chinese, Yoruba (West Africa), and various variations of English.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2892109,
author = {Troncy, Rapha\"{e}l},
title = {Linking Entities for Enriching and Structuring Social Media Content},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2892109},
doi = {10.1145/2872518.2892109},
abstract = {Social media platforms such as Twitter, Facebook or LinkedIn become a reliable source of news and play a key role for being aware of events around the world. Using social media to recognize, enrich or summarize events is however very challenging. In the first part of this talk, we will present ADEL, a novel hybrid architecture for an adaptive entity linking system, that combines methods from the natural language processing, information retrieval and semantic fields. The framework enables to link all the mentions occurring in a text to their entity counterparts in a knowledge base. It is modular and adaptive since it enables to process text written in different languages and of different kind (newswire, tweets, blog posts, etc.) while entities can be of common types (PERSON, LOCATION and ORGANIZATION) or specific ones (dates, numbers) and be disambiguated in generic or specialized knowledge bases. We will show how ADEL can outperform the state-of-the-art systems on the reference NEEL challenges that happens in the yearly #Micropost workshop (2014-2016).In the second part of this talk, we will present a framework that can collect microposts from more than 12 social platforms and that contain media items, as a result of a query -- for example a trending event. We will then show how we can automatically create different visual storyboards that reflect what users have shared about this particular event. The visualization emphasizes the different aspects of storyboards. A graph view shows the relationships between microposts and topics that we automatically extract, while the timeline view emphasizes the time dimension. The user can watch and interact with the summarized view of all the topics or select a particular one with the additional details. In addition, the states of different views are persistent through the URLs which makes easy sharing possible.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {597},
numpages = {1},
keywords = {named entity recognition, named entity disambiguation, adel, neel challenge, nerd, microposts},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890563,
author = {Adegbola, Tunde},
title = {Pattern-Based Unsupervised Induction Of Yor\`{u}B\'{a} Morphology},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890563},
doi = {10.1145/2872518.2890563},
abstract = {The Unsupervised induction of morphological rules from a simple list of words in a language of interest is a productive approach to Computational Morphology. The most popular algorithms used for this purpose in the literature are based on the assumption that the relatively high occurrence frequencies of certain word segments described as recurrent partials in a lexicon suggests the existence of morpheme boundaries around such high frequency word segments. Even though this word-segment-frequency approach works well for concatenative morphology, it does not cater for some of the most productive morphological processes in Yor\`{u}b\'{a} and some other African languages. In this paper, unsupervised induction of the morphological rules of Yor\`{u}b\'{a} was achieved based on a word-pattern-frequency rather than a word-segment-frequency approach. Words in a Yor\`{u}b\'{a} lexicon were clustered according to the morphological processes on which their formation are based, producing results that hitherto were achievable only by painstaking rule-based manual classification.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {599–604},
numpages = {6},
keywords = {frequent patterns, machine learning of morphology, frequent segments, morphological segmentation, probabilistic models, word labels},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890558,
author = {Cer\'{o}n-Guzm\'{a}n, Jhon Adri\'{a}n and Le\'{o}n-Guzm\'{a}n, Elizabeth},
title = {Lexical Normalization of Spanish Tweets},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890558},
doi = {10.1145/2872518.2890558},
abstract = {Twitter data have brought new opportunities to know what happens in the world in real-time, and conduct studies on the human subjectivity on a diversity of issues and topics at large scale, which would not be feasible using traditional methods. However, as well as these data represent a valuable source, a vast amount of noise can be found in them. Because of the brevity of texts and the widespread use of mobile devices, non-standard word forms abound in tweets, which degrade the performance of Natural Language Processing tools. In this paper, a lexical normalization system of tweets written in Spanish is presented. The system suggests normalization candidates for out-of-vocabulary (OOV) words based on similarity of graphemes or phonemes. Using contextual information, the best correction candidate for a word is selected. Experimental results show that the system correctly detects OOV words and the most of cases suggests the proper corrections. Together with this, results indicate a room for improvement in the correction candidate selection. Compared with other methods, the overall performance of the system is above-average and competitive to different approaches in the literature.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {605–610},
numpages = {6},
keywords = {finite-state transducers, language modeling, lexical normalization, out-of-vocabulary words, spanish tweets, twitter},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890560,
author = {Dias Cardoso, Pedro Miguel and Roy, Anindya},
title = {Language Identification for Social Media: Short Messages and Transliteration},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890560},
doi = {10.1145/2872518.2890560},
abstract = {Conversations on social media and microblogging websites such as Twitter typically consist of short and noisy texts. Due to the presence of slang, misspellings, and special elements such as hashtags, user mentions and URLs, such texts present a challenging case for the task of language identification. Furthermore, the extensive use of transliteration for languages such as Arabic and Russian that do not use Latin script raises yet another problem.This work studies the performance of language identification algorithms applied to tweets, i.e. short messages on Twitter. It uses a previously trained general purpose language identification model to semi-automatically label a large corpus of tweets - in order to train a tweet-specific language identification model. It gives special attention to text written in transliterated Arabic and Russian.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {611–614},
numpages = {4},
keywords = {document classification, transliteration, microblogging, language identification},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890559,
author = {J\o{}rgensen, Anna and S\o{}gaard, Anders},
title = {A Test Suite for Evaluating POS Taggers across Varieties of English},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890559},
doi = {10.1145/2872518.2890559},
abstract = {We present a suite of 12 datasets for evaluating POS taggers across varieties of English to enable researchers to evaluate the robustness of their models. The suite includes three new datasets, sampled from lyrics from black American hip-hop artists, southeastern American Twitter, and the subtitles from the TV series The Wire. We present an example eval- uation of an off-the-shelf POS tagger across these datasets.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {615–618},
numpages = {4},
keywords = {performance, evaluation, pos tagging, datasets},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890561,
author = {Yang, Ming and Hsu, William H.},
title = {HDPauthor: A New Hybrid Author-Topic Model Using Latent Dirichlet Allocation and Hierarchical Dirichlet Processes},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890561},
doi = {10.1145/2872518.2890561},
abstract = {We present a new approach towards capturing topic interests corresponding to all the observed latent topics generated by an author in documents to which he or she has contributed. Topic models based on Latent Dirichlet Allocation (LDA) have been built for this purpose but are brittle as to the number of topics allowed for a collection and for each author of documents within the collection. Meanwhile, topic models based upon Hierarchical Dirichlet Processes (HDPs) allow an arbitrary number of topics to be discovered and generative distributions of interest inferred from text corpora, but this approach is not directly extensible to generative models of authors as contributors to documents with variable topical expertise. Our approach combines an existing HDP framework for learning topics from free text with latent authorship learning within a generative model using author list information. This model adds another layer into the current hierarchy of HDPs to represent topic groups shared by authors, and the document topic distribution is represented as a mixture of topic distribution of its authors. Our model automatically learns author contribution partitions for documents in addition to topics.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {619–624},
numpages = {6},
keywords = {topic modeling, hierarchical dirichlet process},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890562,
author = {Zhang, Qian and Goncalves, Bruno},
title = {Topical Differences between Chinese Language Twitter and Sina Weibo},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890562},
doi = {10.1145/2872518.2890562},
abstract = {Sina Weibo, China's most popular microblogging platform, is considered to be a proxy of Chinese social life. In this study, we contrast the discussions occurring on Sina Weibo and on Chinese language Twitter in order to observe two different strands of Chinese culture: people within China who use Sina Weibo with its government imposed restrictions and those outside that are free to speak completely anonymously. We first propose a simple ad-hoc algorithm to identify topics of Tweets and Weibos. Different from previous works on micro-message topic detection, our algorithm considers topics of the same contents but with different #tags. Our algorithm can also detect topics for Tweets and Weibos without any #tags. Using a large corpus of Weibo and Chinese language tweets, covering the entire year of 2012, we obtain a list of topics using clustered #tags and compare them on two platforms. Surprisingly, we find that there are no common entries among the Top 100 most popular topics. Only 9.2% of tweets correspond to the Top 1000 topics of Weibo, and conversely only 4.4% of weibos were found to discuss the most popular Twitter topics. Our results reveal significant differences in social attention on the two platforms, with most popular topics on Weibo relating to entertainment while most tweets corresponded to cultural or political contents that is practically non existent in Weibo.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {625–628},
numpages = {4},
keywords = {online behavior, social attention, social media, topic detection on short-message, twitter, weibo},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251216,
author = {Charton, Eric and Ghoula, Nizar and Meurs, Marie-Jean},
title = {Session Details: OD4LS'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 1st Workshop on Open Data for Local Search, associated with WWW 2016.Local search engines are specialized information retrieval systems enabling users to discover amenities and services in their neighborhood (schools, businesses, hospitals, etc.). Developing a local search system still raises scientific questions, as well as very specific technical issues. One of the main problems encountered is the partial availability or even the absence of informative contents related to local actors, merchants or service providers.Introducing open data in the architecture of local search engines supports the identification and collection of structured content. Collaborative data such as those made available by the OpenStreetMap Foundation can be of help to identify new dealers, and improve their geolocation. Semantic Web resources such as DBpedia contain keywords or content for enriching ontologies associated with a local search service. Open data provided by cities or national organizations, such as descriptions of public institutions, opening hours, location of shopping centers are other usable resources.Available open data can be exploited to dramatically improve the design of local search engines and their contents. The aim of this workshop is to explore new fields of investigation both in terms of algorithmic approaches as well as originality of usable data. The workshop focuses on how open data can be used to enhance the capabilities of local search engines.Target audience will include researchers, and professionals interested in: semantic web and open data usage to improve local search, enhance ontologies and their alignment, and discover keywords and conceptsgeo-content improvement using open data to develop geo-search algorithms, build maps and content, improve and enrich geo-data.information extraction involving open data for knowledge management, named entity, business, and content discovery.The call for papers attracted submissions from the United States, Canada, Switzerland, India, and China. The program committee reviewed and accepted the following: Venue or Track Reviewed - 10 Accepted - 6.We encourage attendees to attend demonstrations, list of which will be available on the website http://od4ls.uqam.ca},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890481,
author = {AlObaidi, Mazen and Mahmood, Khalid and Sabra, Susan},
title = {Semantic Enrichment for Local Search Engine Using Linked Open Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890481},
doi = {10.1145/2872518.2890481},
abstract = {Local search engines are a vital part of presence of local businesses on the Internet. Local search engines improvement is an important element to ensure that local businesses can be found by millions of people whom are using web to find services. However, web presence can be disguised or not properly documented. Our approach will improve the effectiveness of local search, and increase the ranking of local businesses. We introduce an approach for enhancing local search engines efficiency in returning more accurate results. Our approach consists of semantically enriching the results of a query using Linked Open Data (LOD) web content. Our preliminary evaluation demonstrated evidence that our approach has a better search with more accurate results.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {631–634},
numpages = {4},
keywords = {knowledge discovery, linked open data, rdf, semantic enrichment, triple extraction},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890482,
author = {An, Chuankai and Rockmore, Dan},
title = {Improving Local Search with Open Geographic Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890482},
doi = {10.1145/2872518.2890482},
abstract = {Local search helps users find certain types of business units (restaurant, gas stations, hospitals, etc.) in the surrounding area. However, some merchants might not have much online content (e.g. customer reviews, business descriptions, opening hours, telephone numbers, etc.). This can pose a problem for traditional local search algorithms such as vector space based approaches. With this difficulty in mind, in this paper we present an approach to local search that incorporates geographic open data. Using the publicly available {em Yelp} dataset we are able to uncover patterns that link geographic features and user preferences. From this, we propose a model to infer user preferences that integrates geographic parameters. Through this model and estimation of user preference, we develop a new framework for ``local'' (in the sense of geography) search that offsets the absence of contexts regarding physical business units. Our initial analysis points to the meaningful integration of open geographic data in local search and points out several directions for further research.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {635–640},
numpages = {6},
keywords = {local search, preference estimation, geographic open data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890487,
author = {Charton, Eric and Ghoula, Nizar and Meurs, Marie-Jean},
title = {Open Data for Local Search: Challenges and Perspectives},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890487},
doi = {10.1145/2872518.2890487},
abstract = {Local search engines are specialized information retrieval systems enabling users to discover amenities and services in their neighbourhood. Developing a local search system still raises scientific questions, as well as very specific technical issues. Those issues come for example from the lack of information about local events and actors, or the specific form taken by the indexable data. Available open data can be exploited to dramatically improve the design of local search engines and their content. The purpose of this workshop is to explore new fields of investigation both in terms of algorithmic approaches as well as originality of usable data. The workshop focuses on how open data can be used to enhance the capabilities of local search engines.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {641–644},
numpages = {4},
keywords = {open data for local search, local search, semantic web, open data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890486,
author = {Lytle, Robert F.},
title = {Open Data Business Licence Usage to Improve Local Search Engine Content Ranking},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890486},
doi = {10.1145/2872518.2890486},
abstract = {This paper addresses the use of Open Data business licence records in the course of local web search. It assesses the feasibility of increasing the search ranking of authorised service providers and rank reduction or removal of providers with an invalid licence status. A case study was conducted to identify usable results returned with a local search across multiple providers. Result records were then analysed against an applicable Open Data set to determine the usability of the search results for end-users seeking service. A model for adjusting search result ranking is proposed for further analysis. Finally, findings based on the analysis lead to additional proposed research efforts in this space.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {645–649},
numpages = {5},
keywords = {search results, ranking algorithm, open data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890485,
author = {Milot, Jonathan and Munroe, Patrick and Beaudry, Eric and Grondin, Francois and Bourdeau, Guillaume},
title = {Lookupia: An Intelligent Real Estate Search Engine for Finding Houses Optimally Geolocated to Reach Points of Interest},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890485},
doi = {10.1145/2872518.2890485},
abstract = {Lookupia is an intelligent real estate search engine for finding houses optimally geolocated to reach points of interest. It uses data from OpenStreetMap and most of public transit corporations that publish transit schedules in the General Transit Feed Specification (GTFS) format.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {651–653},
numpages = {3},
keywords = {geographic location optimization, search engine, real estate, openstreetmap, public transit networks, web application},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890484,
author = {Peterman, Michael and Benomar, Omar and Mechedou, Hacene and Bachand, Felix-Herve},
title = {Address Geocoding Using Street Profiles for Local Search},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890484},
doi = {10.1145/2872518.2890484},
abstract = {Geocoding is the process of converting addresses to geocoordinates. It is widely used in several fields such as public health to monitor socioeconomic inequalities for example or in Geographical Information Systems (GIS) to be able to use with its provided features. In this work, we describe a method to create an address geocoder from a free and open government street lines data source. The address geocoder transforms a street address into a location typically measured in latitude-longitude coordinates. The address geocoder is used in a search engine to relate spatial data to search results and improve accuracy.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {655–656},
numpages = {2},
keywords = {local search, geocoding, street profiles},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890483,
author = {Tardy, Camille and Moccozet, Laurent and Falquet, Gilles},
title = {A Simple Tags Categorization Framework Using Spatial Coverage to Discover Geospatial Semantics},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890483},
doi = {10.1145/2872518.2890483},
abstract = {There exist many popular crowdsourcing and social services (Volunteered Geographic Information (VGI)) to share information and documents such as Flickr, Foursquare, Twitter , Facebook, etc. They all use metadata, folksonomy and more importantly a geographic axis with GPS coordinates and/or geographic tags. Using this available folksonomy in VGI services we propose a logical approach to highlight and possibly discover the characteristics of geographic places. The approach is based on the notion of spatial coverage and a model of tags categorization and on their semantic identification, using semantic services such as GeoNames, OpenStreetMap or WordNet. We illustrate our model with Flickr to retrieve the characteristics (function, usage?) of places even if those places have a small number of related photos. Those found characteristics allow tag disambiguation and can be use to complete the semantic gap on places and POIs such as the function of buildings, which can exist in geographic services.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {657–660},
numpages = {4},
keywords = {coverage, semantic, tag, geographic information, vgi, folksonomy},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251217,
author = {Cimiano, Philipp and Dalle, Jean-Michel and Gandon, Fabien},
title = {Session Details: Q4APS'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 1st Workshop on Question Answering And Activity Analysis in Participatory Sites (Q4APS) associated with WWW 2016.This workshop intends to bring together researchers and practitioners of Question Answering sites and services on the Web to present and discuss latest advances in analyzing, supporting and automating tasks of the life-cycle of such applications. The goal is to cover and bring together the different approaches existing in managing and answering natural language questions of users on the Web. This includes methods, models and algorithms from automated question-answering, for question-answering forums mining, as well as for monitoring and management automation.Topics included in the call for papers were the following: question routing, question answeringquestion and answer recommendationquestion and need analysis, question modelingexpert finding, expertise categorization, expertise labellingdebate analysis, argument mining, argument schemesmoderating support and automationanimation fostering, targeted solicitation and notificationanswer generation, multiple and/or heterogeneous answer sourcesanswer detection and ranking, best answer identificationanswer building and answer improvingquestions and expert topic labellingrole detection (questioner, answerer, editor, etc.), user modellingsocial aspects of question answeringfact checking, cross-validation, supporting evidencespam or abuse prevention in questions and answersanswer personalizationchallenges, datasets, benchmarks for question-answering evaluationWe received seven submissions for the workshop. Each submission received at least three reviews by members of the program committee. On the basis of these reviews, we decided to accept six papers to be presented at the workshop and to be included in the conference proceedings.The paper by Burel et al. "Structural Normalisation Methods for Improving Best Answer Identification in Question Answering Communities" investigates the problem of how to identify the best answers in community-based Q/A portals. They propose to apply structural normalization techniques to feature-based best answer identification models.The paper by Jenders et al. "Which Answer is Best? Predicting Accepted Answers in MOOC Forums" deals with a similar topic, and investigates the identification of best answers in the context of MOOCS. They present a method that exploits historical data to find best answers for a question.In their paper "Using Semantics to Search Answers for Unanswered Questions in Q&amp;A Forums", Singh et al. present an approach that tackles the problem that in many Q/A community sites there are many unanswered questions. To mitigate this problem, they propose a system that finds answered questions that are similar to unanswered questions.Sandor et al. present an approach to the detection of user issues and request types in technical forum question posts. They propose a system that categorizes posts into these types using techniques from discourse analysis.In their paper "Enriching Topic Modelling with Users' histories for Improving Tag Prediction in Q&amp;A Systems", Loeckx et al. describe an approach to predict the tags for questions in social Q/A sites such as StackExchange. They propose a model that factors in the user context and show that this extension improves upon a purely textual content-based baseline.Finally, Shekarpour et al., in their paper "Question Answering on Linked Data: Challenges and Future Directions", present an overview of open and future challenges in the field of question answering from Linked Data.Presenters are encouraged to bring demos to the workshop to enhance oral discussions and presentations.Acknowledgment. We thank the ANR for the ANR-12-CORD-0026 grant supporting the Ocktopus project and this workshop.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {3},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890572,
author = {Amatriain, Xavier},
title = {Machine Learning for Q&amp;A Sites: State of the Art and Research Directions},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890572},
doi = {10.1145/2872518.2890572},
abstract = {Q&amp;A sites like Quora aim at growing the world's knowledge. In order to do this, they need to get the right questions to the right people to answer them, but also the existing answers to people who are interested in them. In order to accomplish this, they need to build a complex ecosystem where issues such as content quality, engagement, demand, interests, or reputation are taken into account. It is not possible to build a system like this unless most of the process are highly automated and scalable. The good news is that using high-quality data you can build machine learning solutions that can help address all of the previous requirements.In this talk I will describe some interesting uses of machine learning for Q&amp;A that range from different recommendation approaches such as personalized ranking to classifiers built to detect duplicate questions or spam. I will describe some of the modeling and feature engineering approaches that go into building these systems. I will also share some of the challenges faced when building such a large-scale knowledge base of human-generated knowledge. Finally, I will describe some of the unresolved research challenges in the Q&amp;A Space.I will use my experience at Quora as the main driving example. Quora is a Q&amp;A site that despite having over 80 million unique visitors a month, it is known for keeping a high-quality of answers and content in general.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {665},
numpages = {1},
keywords = {machine learning, q&amp;a},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890573,
author = {Dalle, Jean-Michel and Faron-Zucker, Catherine and Gandon, Fabien and Lacage, Mathieu and Meng, Zide},
title = {Online Knowledge Triage: Searching, Detecting, Labelling and Orienting User Generated Content},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890573},
doi = {10.1145/2872518.2890573},
abstract = {This position paper provides an overview of the OCKTOPUS project whose goal is to increase the social and economic benefit of user-generated content, by transforming it into knowledge which can be shared and reused broadly.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {667–668},
numpages = {2},
keywords = {community detection, forums, question answering, communities of interest, expert finding, community labelling},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890566,
author = {Boudaer, Glenn and Loeckx, Johan},
title = {Enriching Topic Modelling with Users' Histories for Improving Tag Prediction in Q&amp;A Systems},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890566},
doi = {10.1145/2872518.2890566},
abstract = {The automatic attribution of tags in Question &amp; Answering (Q&amp;A) systems like Stack Exchange can significantly reduce the human effort in tagging as well as improve the consistency among users. Existing approaches typically either rely on Natural Language Processing solely or employ collaborative filtering techniques. In this paper, we attempt to combine the best of both worlds by investigating whether incorporating a personal profile, consisting of a user's history or its social network can significantly improve the predictions of state-of-the-art text-based methods. Our research has found that enriching content-based text features with this personal profile allows to trade-off the precision of predictions for recall and as such improve the "exact match" (predicting the number of tags and the tags themselves correctly) in a multi-label setting from a baseline of 18.2% text-only to 54.3%.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {669–672},
numpages = {4},
keywords = {user profiles, q&amp;a systems, collaborative tagging, topic modelling, personalisation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890570,
author = {Burel, Gregoire and Mulholland, Paul and Alani, Harith},
title = {Structural Normalisation Methods for Improving Best Answer Identification in Question Answering Communities},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890570},
doi = {10.1145/2872518.2890570},
abstract = {Nowadays, Question Answering (Q&amp;A) websites are popular source of information for finding answers to all kind of questions. Due to this popularity it is critical to help the identification of best answers to existing questions for simplifying the access to relevant information.Although it is possible to identify relatively accurately best answers by using binary classifiers coupled with user, content and thread features, existing works have generally ignored to incorporate the thread-like structure of Q&amp;A communities in the design of best answer identification predictors and algorithms.This paper investigates this particular issue by studying structural normalisation techniques for improving the accuracy of feature based best answer identification models.Thread-based normalisation methods are introduced for improving the accuracy of identification models by introducing a systematic normalisation approach that normalise predictors by taking into account relations between features and the thread-like structure of Q&amp;A communities.Compared to similar non normalised models, better results are obtained for each of the three communities studied. These results show that structural normalisation methods can improve the identification of best answers compared to non-normalised models.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {673–678},
numpages = {6},
keywords = {structural normalisation, social q&amp;a platforms, social media, online communities, best answers identification},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890567,
author = {Jenders, Maximilian and Krestel, Ralf and Naumann, Felix},
title = {Which Answer is Best? Predicting Accepted Answers in MOOC Forums},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890567},
doi = {10.1145/2872518.2890567},
abstract = {Massive Open Online Courses (MOOCs) have grown in reach and importance over the last few years, enabling a vast userbase to enroll in online courses. Besides watching videos, user participate in discussion forums to further their understanding of the course material. As in other community-based question-answering communities, in many MOOC forums a user posting a question can mark the answer they are most satisfied with. In this paper, we present a machine learning model that predicts this accepted answer to a forum question using historical forum data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {679–684},
numpages = {6},
keywords = {forum posts, reputation system, massive open online course, mooc, answer recommendation, forum},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890568,
author = {Sandor, Agnes and Lagos, Nikolaos and Vo, Ngoc-Phuoc-An and Brun, Caroline},
title = {Identifying User Issues and Request Types in Forum Question Posts Based on Discourse Analysis},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890568},
doi = {10.1145/2872518.2890568},
abstract = {In this paper we propose the detection of user issues and request types in technical forum question posts with a twofold purpose: supporting up-to-date knowledge generation in organizations that provide (semi-) automated customer-care services, and enriching forum metadata in order to enhance the effectiveness of search. We present a categorization system for detecting the proposed question post types based on discourse analysis, and show the advantage of using discourse patterns compared to a baseline relying on standard linguistic features. Besides the detailed description of our method, we also release our annotated corpus to the community.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {685–691},
numpages = {7},
keywords = {discourse analysis, question post analysis, on-line forum},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890571,
author = {Shekarpour, Saeedeh and Endris, Kemele M. and Jaya Kumar, Ashwini and Lukovnikov, Denis and Singh, Kuldeep and Thakkar, Harsh and Lange, Christoph},
title = {Question Answering on Linked Data: Challenges and Future Directions},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890571},
doi = {10.1145/2872518.2890571},
abstract = {Question Answering (QA) systems are becoming the inspiring model for the future of search engines. While, recently, datasets underlying QA systems have been promoted from unstructured datasets to structured datasets with semantically highly enriched metadata, question answering systems are still facing serious challenges and are therefore not meeting users' expectations. This paper provides an exhaustive insight of challenges known so far for building QA systems, with a special focus on employing structured data (i.e. knowledge graphs).It thus helps researchers to easily spot gaps to fill with their future research agendas.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {693–698},
numpages = {6},
keywords = {research challenge, distributed and heterogeneous datasets, question answering system, query understanding, data quality, speech interface, interoperability of components},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890569,
author = {Singh, Priyanka and Simperl, Elena},
title = {Using Semantics to Search Answers for Unanswered Questions in Q&amp;A Forums},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890569},
doi = {10.1145/2872518.2890569},
abstract = {The expert based question and answering forums are crowdsourced and rely on people to provide answers for questions. This paper focuses on technology based Q&amp;A systems like StackOverflow and Reddit. These websites are popular and yet many questions remain unanswered. The Suman system uses semantic keyword search in combination with traditional text search techniques to find similar questions with answers for unanswered questions. Furthermore, the Suman system also recommends experts who can answer those questions. This helps to narrow down the long tail of unanswered questions. The Suman system utilises Semantic Web and Linked Data technologies to integrate the datasets from two websites, structure them and link them to Linked Data Cloud. It uses available tools to solve name entity disambiguation problem and expands the query term with added semantics. The Suman system was evaluated and results were analysed to show its viability.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {699–706},
numpages = {8},
keywords = {reddit, linked data, stackoverflow, search applications, q&amp;a forums, keyword search, semantic search, semantic web},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251218,
author = {De Francisci Morales, Gianmarco and Aiello, Luca Maria and Papadopoulos, Symeon and Kwak, Haewoon},
title = {Session Details: SNOW'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the Third Workshop on Social News on the Web -- SNOW 2016 -- held in conjunction with the WWW 2016 conference on April 12th 2016 in Montreal, Canada.In recent years, the topics addressed by SNOW have become very popular among diverse scientific communities. Computer scientists have studied how information spreads and diffuses in social networks. Journalists, social scientists, and economists are typical professionals who can improve their respective fields and practice by adopting technologies of interest to SNOW. In addition, there are emerging regulatory and legal issues that are of interest to law researchers and practitioners. The workshop aims to provide a forum to foster communication between these communities. In particular, the workshop has attracted a number of high-quality submissions on the relationships between online news and social media.With this workshop we aim at continuing a tradition of interdisciplinary exchange and cross-domain fertilization among different research communities. The goal of the workshop is to share novel ideas and to discuss future directions in the emerging areas of news search, news mining, news verification, and news recommendation. It especially focuses on the interplay between news content, generated by professional journalists, and social media content, generated by millions of users in real time and subject to social media dynamics. We also welcome investigations on the topics of citizen journalism and computational journalism. SNOW aspires to give researchers and practitioners a unique opportunity to share their perspectives with others interested in news and social media.The call for papers attracted 11 submissions from Asia, America, and Europe. The program committee accepted 6 papers that cover a variety of topics, including news credibility and verification, robot journalism, news consumption and social sharing, and news production on social media. The major criterion for the selection of papers was their potential to generate discussion and influence future research directions. The program also includes four keynote talks from distinguished experts in the field, with themes ranging from event understanding and narration to tracking misinformation, from curation of online news comments to the role of journalism on social media.We hope you will find this program interesting and thought provoking, and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890099,
author = {Diakopoulos, Nicholas},
title = {CommentIQ: Enhancing Journalistic Curation of Online News Comments},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890099},
doi = {10.1145/2872518.2890099},
abstract = {National news outlets routinely publish articles that attract hundreds and even thousands of user comments. These comments often provide valuable feedback and critique, personal perspectives, new information and expertise, and opportunities for discussion (not to mention profanity and vitriol). The varying quality of comments demands a high level of moderation and curatorial attention in order to cultivate a successful online community around news. Amongst publishers there is a growing awareness that finding and publicly highlighting high quality comments can in turn promote the general quality of the discourse. Further journalistic value can be gleaned by identifying and developing new sources of information and expertise from comments. In this talk I will present an editorially-aware visual analytics system called CommentIQ that supports moderators in curating high quality news comments at scale. The possibilities and ramifications of algorithmically infused social media moderation will be discussed in terms of journalistic ideals and norms of free speech and inclusion.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {715–716},
numpages = {2},
keywords = {computational journalism, news commenting, comment quality, visual analytics, comment moderation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890092,
author = {Menczer, Filippo},
title = {The Spread of Misinformation in Social Media},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890092},
doi = {10.1145/2872518.2890092},
abstract = {As social media become major channels for the diffusion of news and information, they are also increasingly attractive and targeted for abuse and manipulation. This talk overviews ongoing network analytics, data mining, and modeling efforts to understand the spread of misinformation online and offline. I present machine learning methods to detect astroturf and social bots, and outline initial steps toward computational fact checking, as well as theoretical models to study how truthful and truthy facts compete for our collective attention. These efforts will be framed by a case study in which, ironically, our own research became the target of a coordinated disinformation campaign.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {717},
numpages = {1},
keywords = {diffusion networks, twitter, social media, misinformation, hoaxes, social bubbles, social bots, fact checking, echo chambers, astroturf},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890097,
author = {Bhattacharya, Devipsita and Ram, Sudha},
title = {Understanding the Competitive Landscape of News Providers on Social Media},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890097},
doi = {10.1145/2872518.2890097},
abstract = {Social media has emerged as a mechanism for online news propagation. This in turn has changed the competitive landscape of news providers, a landscape that was previously partitioned based on the traditional channels of news dispersion. The channels of news distribution refer to - television, newspaper, magazine, radio, news agency and online only. In this paper, we examine similarities and differences in news propagation patterns on social media based on the primary channel of a news provider. We collected news article propagation activity data from Twitter for 32 news providers over a three-week period and analyzed their propagation networks. Our analysis shows that the structural properties of the propagation networks are statistically different based on the type of primary channel. Our study has useful implications for understanding the competition between news providers in an online environment.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {719–724},
numpages = {6},
keywords = {article propagation, news propagation, twitter, micro-blogging},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890094,
author = {Meshulam, Ram and Sasson, Roy},
title = {For Your Eyes Only: Consuming vs. Sharing Content},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890094},
doi = {10.1145/2872518.2890094},
abstract = {This paper analyzes two types of user interactions with online content: (1) private engagement with content, measured by page-views and click-through rate; and (2) social engagement, measured by the number of shares on Facebook as well as share-rate. Based on more than a billion data points across hundreds of publishers worldwide and two time periods, it is shown that the correlation between these signals is generally low. Potential reasons for the low correlation are discussed, and the notion of private-social dissonance is defined. A more in-depth analysis shows that the dissonance between private engagement and social engagement consistently depends on content category. Categories such as Sex, Crime and Celebrities have higher private engagement than social engagement. On the other hand, categories such as Books, Careers and Music have higher social engagement than private engagement. In addition to the offline analysis, a model which utilizes the different signals was trained and deployed on a live recommendation system. The resulting weights ranked the social signal lower than click-through rate. The results are relevant for publishers, content marketers, architects of recommendation systems and researchers who wish to use social signals in order to measure and predict user engagement.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {725–730},
numpages = {6},
keywords = {social network, recommender system, bfacebook, behavioral modeling},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890093,
author = {Ombelet, Pieter-Jan and Kuczerawy, Aleksandra and Valcke, Peggy},
title = {Employing Robot Journalists: Legal Implications, Considerations and Recommendations},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890093},
doi = {10.1145/2872518.2890093},
abstract = {Algorithmic processes that convert data into narrative news texts allow news rooms to publish stories with limited to no human intervention (Carlson, 2015, p. 416). The new trend creates many opportunities, but also raises significant legal questions. Aside from financial benefits, further refinement could make the smart algorithms capable of writing less standard, maybe even opinion, pieces. The responsible human merely needs to define clear questions about what the algorithm needs to discuss in the article and in what manner. But how does it square with the traditional rules of publishing, editorial control and the privacy and data protection framework? This paper analyses the legal implications when employing robot journalists. More specifically, the question of authorship for algorithmic output and the liability issues that could arise when the algorithmic output includes unlawful personal data processing as well as inaccurate, harmful or even illegal content will be assessed. The analysis is performed analyzing European legislation on copyright and data protection and applying Belgian legislation on press liability as a consistent country example to support certain legal considerations and conclusions. Furthermore, the paper answers the question as to how publishers could prevent the creation of inaccurate content by the algorithms they use.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {731–736},
numpages = {6},
keywords = {robot journalism, legal liability, journalists' ethics},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890096,
author = {Rizos, Georgios and Papadopoulos, Symeon and Kompatsiaris, Yiannis},
title = {Predicting News Popularity by Mining Online Discussions},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890096},
doi = {10.1145/2872518.2890096},
abstract = {The paper presents a framework for the prediction of several news story popularity indicators, such as comment count, number of users, vote score and a measure of controversiality. The framework employs a feature engineering approach, focusing on features from two sources of social interactions inherent in online discussions: the comment tree and the user graph. We show that the proposed graph-based features capture the complexities of both these social interaction graphs and lead to improvements on the prediction of all popularity indicators in three online news post datasets and to significant improvement on the task of identifying controversial stories. Specifically, we noted a 5% relative improvement in mean square error for controversiality prediction on a news-focused Reddit dataset compared to a method employing only rudimentary comment tree features that were used by past studies.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {737–742},
numpages = {6},
keywords = {online discussion analysis, news popularity prediction},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890091,
author = {Schifferes, Stephen},
title = {Technological Transformations of News: A Long Term Perspective},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890091},
doi = {10.1145/2872518.2890091},
abstract = {The recent transformation of news is one of a series of major revolutions in news delivery. By studying previous episodes we may find some clues as to the future of social news on the web.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {743–744},
numpages = {2},
keywords = {mass media, news, social media, technological innovation, broadcast news, journalists},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890098,
author = {Shao, Chengcheng and Ciampaglia, Giovanni Luca and Flammini, Alessandro and Menczer, Filippo},
title = {Hoaxy: A Platform for Tracking Online Misinformation},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890098},
doi = {10.1145/2872518.2890098},
abstract = {Massive amounts of misinformation have been observed to spread in uncontrolled fashion across social media. Examples include rumors, hoaxes, fake news, and conspiracy theories. At the same time, several journalistic organizations devote significant efforts to high-quality fact checking of online claims. The resulting information cascades contain instances of both accurate and inaccurate information, unfold over multiple time scales, and often reach audiences of considerable size. All these factors pose challenges for the study of the social dynamics of online news sharing. Here we introduce Hoaxy, a platform for the collection, detection, and analysis of online misinformation and its related fact-checking efforts. We discuss the design of the platform and present a preliminary analysis of a sample of public tweets containing both fake news and fact checking. We find that, in the aggregate, the sharing of fact-checking content typically lags that of misinformation by 10-20 hours. Moreover, fake news are dominated by very active users, while fact checking is a more grass-roots activity. With the increasing risks connected to massive online misinformation, social news observatories have the potential to help researchers, journalists, and the general public understand the dynamics of real and fake news sharing.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {745–750},
numpages = {6},
keywords = {misinformation, fake news, twitter, hoaxes, rumor tracking, fact checking},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890095,
author = {Wiegand, Stefanie and Middleton, Stuart E.},
title = {Veracity and Velocity of Social Media Content during Breaking News: Analysis of November 2015 Paris Shootings},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890095},
doi = {10.1145/2872518.2890095},
abstract = {Social media sources are becoming increasingly important in journalism. Under breaking news deadlines semi-automated support for identification and verification of content is critical. We describe a large scale content-level analysis of over 6 million Twitter, You Tube and Instagram records covering the first 6 hours of the November 2015 Paris shootings. We ground our analysis by tracing how 5 ground truth images used in actual news reports went viral. We look at velocity of newsworthy content and its veracity with regards trusted source attribution. We also examine temporal segmentation combined with statistical frequency counters to identify likely eyewitness content for input to real-time breaking content feeds. Our results suggest attribution to trusted sources might be a good indicator of content veracity, and that temporal segmentation coupled with frequency statistical metrics could be used to highlight in real-time eyewitness content if applied with some additional text filters.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {751–756},
numpages = {6},
keywords = {credibility, semantics, natural language processing, news, social media, user generated content, trust, velocity, veracity},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251219,
author = {Shadbolt, Sir Nigel and Simperl, Elena and Tiropanis, Thanassis},
title = {Session Details: SOCM'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 4th International Workshop on the Theory and Practice of Social Machines, associated with WWW 2016. The workshop is a continuation of 3rd International Workshop on the Theory and Practice of Social Machines (SOCM2015), held at WWW2015.Continuing from previous years' "Theory and Practice of Social Machines" workshops at WWW2013, 2014, and 2015, the 2016 edition of the SOCM workshop will look deeply at social machines that have, or may yet soon have, a profound impact on the lives of individuals, businesses, governments, and the society as a whole. Our goal is to discuss issues pertinent to the observation of both extant and yet unrealized social machines building on work of the Web Observatory Workshops of the last two years (WOW2013 and WOW2014). SOCM2016 aims to identify factors that govern the growth or impede these systems to develop, and to identify unmet observation needs or the kinds of loosely-coordinated distributed social systems the Web enables. We also intend to discuss methods to analyze and explore social machines, as essential mechanisms for deriving the guidelines and best practices that will inform the design of social machine observatories.The workshop fosters multidisciplinary discussion on the following areas: Analyzing social machines: analytics and visualisations that provide insights about social machines and their impact.Designing social machine observatories: analyses of the design of effective (extant and future) social machines.Methodology and methods: papers describing approaches and methods for observing social machines.Philosophical Theories and Framework: describing and analysing the philosophical implications of social machines.The call for papers attracted submissions from United States and Europe. The program committee reviewed and accepted the following: Paper Submissions Reviewed - 8 Accepted - 8.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890594,
author = {Ahlers, Dirk and Driscoll, Patrick and L\"{o}fstr\"{o}m, Erica and Krogstie, John and Wyckmans, Annemie},
title = {Understanding Smart Cities as Social Machines},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890594},
doi = {10.1145/2872518.2890594},
abstract = {Smart Cities denote a stronger integration of information technology into the organisation of a city and the interaction and participation of its citizens. In developing the concept further, we propose to understand Smart Cities through the lens of Social Machines and thus stronger focus on the city as a socio-technical construct. We draw from an interdisciplinary background of computer science and urban planning to reexamine and combine existing theories and find a common understanding. We substantiate our claim to the validity of the concept of Smart-City-as-a-Social-Machine with a thorough literature study and comparison. We discuss the resulting system complexity issues and ways to address them. We further propose areas where this understanding can be useful in furthering research on both the Smart City and the Social Machine topics.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {759–764},
numpages = {6},
keywords = {participation, urban planning, smart city, cities, sustainability, social machines, urbanization, megamachine, citizen engagement, urban interactions},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890591,
author = {Applin, Sally A. and Fischer, Michael D.},
title = {Exploring Cooperation with Social Machines},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890591},
doi = {10.1145/2872518.2890591},
abstract = {As humans become more and more immersed in a networked world of connected and mobile devices, cooperation and sociability to achieve valued outcomes within geographic locales appears to be waning in favour of extended personal networks and interaction using semi-automated agents to support communications, transportation and other services.From a messaging structure that is complex, multiplexed and much of the time asynchronous, conditions emerge that disrupt symmetry of information exchange. People thus encounter circumstances that seem unpredictable given the information available to them, resulting in limited or failed cooperation and consequent quality of outcomes. We explore the role of Social Machines to support, change, and enhance human cooperation within a blended reality context.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {765–768},
numpages = {4},
keywords = {blended reality, anthropology, sociability, cooperation, automation, geography, social machines, community},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890596,
author = {Halcrow, Caroline A. and Carr, Leslie and Halford, Susan},
title = {Using the SPENCE Model of Online/Offline Community to Analyse Sociality of Social Machines},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890596},
doi = {10.1145/2872518.2890596},
abstract = {Online/offline community (O/OC), the integrated performance of community in a blend of online/offline activities is increasingly prevalent as online systems organise, mediate and broadcast forms of communal engagement. O/OCs are social machines where the focus is on the social achievement, rather than the computational outcomes, of the combined human-technical infrastructure. An O/OC model SPENCE is proposed as an analytical tool for describing social machines from the perspective of sociality. Twitter is a technical infrastructure and social network of shared online/offline community phenomena that is also a social machine combining social participation with conventional forms of machine-based computation. Drawing from the extensive Twitter research literature, a sample of papers are analysed against SPENCE, demonstrating the clarity of the organisation of inter-relating themes of a range of perspectives in current Twitter research. It is concluded that SPENCE provides a lens of synthesis for the sociality dimension of a social machine and can be used in taxonomic activities (such as the social machines observatory) to differentiate social machines.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {769–774},
numpages = {6},
keywords = {social machines, sociality, models, online/offline community, twitter, literature classification, sociam, web observatory},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890593,
author = {Madaan, Aastha and Tiropanis, Thanassis and Srinivasa, Srinath and Hall, Wendy},
title = {Observlets: Empowering Analytical Observations on Web Observatory},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890593},
doi = {10.1145/2872518.2890593},
abstract = {The Web observatory is proposed as a global catalogue for sharing data-sets and analytic applications to support researchers from a variety of disciplines for analysing huge amount of research data for Web Science research. However, often these users fail to understand various transformations and consequences of complex data processing involved in a data analytic application. Therefore, there is a need to enable these users develop and re-use analytic applications on web observatory. In this study, we propose formal design patterns called "Observlets" for analytic applications to "observe" various web phenomena. The observlets provide abstract definitions for intermediate analysis required for a data analytic application. The users can share observlets across distributed web observatory nodes. The observlets are aimed to enhance end-users' awareness and engagement on web observatory and support programmers for innovating various data analytic applications.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {775–780},
numpages = {6},
keywords = {web observatory, user-engagement, infrastructure support, analytic applications, design patterns},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890592,
author = {Matthews, Paul},
title = {Going Meta: Norm Formation and Enactment on the Stack Exchange Network},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890592},
doi = {10.1145/2872518.2890592},
abstract = {This paper presents research into the way norms are developed, expressed and enforced on sites that are part of the Stack Exchange (SE) social question-answering network. This network has a number of topical knowledge exchange communities using similar underlying software, enabling a focus on variation in social design. SE also separates community-related discussion from topic-specific content through the use of its "Meta" sub-sites. These were analysed together with their main sites for variation in the development and enforcement of norms. Norms expressed through explicit community policies seem rather less important than those embodied in busy discussion threads on the Meta sites. While Meta participation was fairly uniform across communities, different emphasis on scope and quality led to variation in Meta discussion and the way that norms were enacted through question closures. The social distribution of moderation work was also uneven between sites, with some sites having a few highly active moderators involved in question closure. The level of closures across the sites studied did not seem to significantly discourage participation. Indeed, modelling the effect of closures on quality and engagement indicated that low levels of closure enable "legitimate peripheral participation", the process by which newcomers can become inducted and make contributions of increasing quality over time.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {781–785},
numpages = {5},
keywords = {social question-answering, smqa, online communities, norms, cqa},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890597,
author = {Merchant, Arpit and Jha, Tushant and Singh, Navjyoti},
title = {The Use of Trust in Social Machines},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890597},
doi = {10.1145/2872518.2890597},
abstract = {Trust plays an important role in the effective working of Social Machines by allowing for cooperative behaviour amongst human and digital components of the system. A detailed study of trust helps in gaining insights into the working of social machines, and allows designers to create better systems which are able to engage more people and allow for efficient operations. In this paper, we undertake a discussion on the variety of ways in which trust can be observed in Social Machines by outlining a three class taxonomy (personal, social and functional). We build upon earlier observations in past literature while seeking a broader definition. We discuss the problem of trust, that of promoting trust amongst the trustworthy in social machines, and present the various insights, challenges and frontiers that arise in response. This includes the role of institutions, communication processes and value aligned technologies in social, personal and functional trust respectively.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {787–792},
numpages = {6},
keywords = {information systems, trust, social machines},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890595,
author = {Poddar, Shivani and Ernala, Sindhu Kiranmai and Singh, Navjyoti and Samvara, Ashin},
title = {Towards a Ubiquitous Model of an Individual in Social Machines},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890595},
doi = {10.1145/2872518.2890595},
abstract = {It is the activities of individuals that lead to formation and changes within any social system. Hence, the "social" component in a social machine (socio-technical machine) can be understood constructively from the conception of an individual as a machine. In this work, we present a stochastic finite state machine model of an individual based on Abhidhamma tradition of Buddhism. The machine models moment to moment states of consciousness of an individual in terms of the Buddhist formal ontology that constitutes an individual. Thus, the key contribution of our research is a ubiquitous framework of an individual which unifies the idea of a human agent across all possible social machines. It is shown that from web data of a particular individual this machine can be populated. We expound how our model solves issues pertaining to varied temporal granules and sparsity of data. We further illustrate through an example as to how our approach can unify the conceptualizations of an individual from the numerous ideologies and definitions of a social machine. As a part of our future work, we hope to align this proposed stochastic machine with social observatories on internet.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {793–798},
numpages = {6},
keywords = {social machine, automaton, individual, formal ontology},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890590,
author = {Zhao, Jun and Binns, Reuben and Van Kleek, Max and Shadbolt, Nigel},
title = {Privacy Languages: Are We There yet to Enable User Controls?},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890590},
doi = {10.1145/2872518.2890590},
abstract = {Privacy protection is one of the most prominent concerns for web users. Despite numerous efforts, users remain powerless in controlling how their personal information should be used and by whom, and find limited options to actually opt-out of dominating service providers, who often process users information with limited transparency or respect for their privacy preferences. Privacy languages are designed to express the privacy-related preferences of users and the practices of organisations, in order to establish a privacy-preserved data handling protocol. However, in practice there has been limited adoption of these languages, by either users or data controllers. This survey paper attempts to understand the strengths and limitations of existing policy languages, focusing on their capacity of enabling users to express their privacy preferences. Our preliminary results show a lack of focus on normal web users, in both language design and their tooling design. This systematic survey lays the ground work for future privacy protection designs that aim to be centred around web users for empowering their control of data privacy.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {799–806},
numpages = {8},
keywords = {data terms of use, privacy languages, user control},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251220,
author = {Gloria, Kristine Maria and Bazan, St\'{e}phane B. and White, Su},
title = {Session Details: TeachWeb'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 1st Workshop on "Web Education": Teaching Digital Literacies associated with WWW 2016.The dynamics of Web Education and Digital Literacies are among today's most important issues surrounding the development of the Web as an efficient, safe and universal information system.A wide range of disciplines including sociology, economics, political studies, health and management science have integrated courses and specializations to teach about the Web, its nature, its realities, its impact its evolution and its integration into every dimension of human activity.The workshop will gather a very broad community of participants: professors involved in digital literacy programs or courses, consultants empowering employees in a company, students or faculty in an interdisciplinary program or activists in an NGO teaching the Web to kids.The call for short papers attracted submissions from Asia, Europe and Canada. The program committee has reviewed and accepted 7 submissions.The workshop will also be preceded and followed by online activities on the Bookwitty.com platform. These activities aim to not only strengthen links within the Web Education Community, but also to gather and present the outcomes of the workshop.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890576,
author = {Coskun, Elisabeth and White, Su},
title = {Emerging a Web Science Curriculum},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890576},
doi = {10.1145/2872518.2890576},
abstract = {This text describes a project which aims to explore the scope of the discipline Web Science; an emerging subject which is fundamentally inter-disciplinary. There are very few definitive subject definitions currently available for Web Science. Additionally, the nature of the subject is constantly evolving as an increasing number of different disciplines begin to practice what might identifiably be called Web Science. This potentially provides educators and students with a problem; how do you teach or learn about Web Science when there is no clear definition? This text provides a brief overview of a PhD project, the final aim of which involves the emergence of a framework for a working definition of Web Science. This will be achieved by an examination and overview of current existing Web Science curricula, as well as available Web Science literature.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {811–813},
numpages = {3},
keywords = {education, web science curriculum, web science},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890578,
author = {Harris, Lisa J. and Fair, Nicholas S.R. and Hewitt, Sarah},
title = {Collaborative Social Learning: Rewards and Challenges in Mainstream Higher Education},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890578},
doi = {10.1145/2872518.2890578},
abstract = {This paper introduces the theoretical framework and design rationale for an innovative undergraduate module entitled "Living and Working on the Web" at the University of Southampton. The module design is based on the principles of collaborative social learning and the co-construction of knowledge. At the workshop a model of best practice will be presented, featuring a "blog-comment-reflect-feedback" cycle, which has derived from the synthesis of relevant literature and which will be reflected upon through an informal content analysis of the students' blogs.One of the problems facing this type of curriculum innovation is the difficulties faced when scaling up such modules to very large student groups, particularly in relation to feedback and assessment. To date, the single largest cohort has been 45 students. It is therefore also the intention of the presenters to engage the attendees in a discussion of how a module such as this could be feasibly extended into far larger cohort groups.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {815–820},
numpages = {6},
keywords = {social learning, collaborative learning, curriculum innovation, personal learning networks, digital literacies},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890575,
author = {Hoar, Ricardo and Connolly, Randy},
title = {The Garden of Earthly Delights: Constructing and Revising a Web Development Textbook},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890575},
doi = {10.1145/2872518.2890575},
abstract = {Web development is widely considered to be a difficult topic to teach successfully within post-secondary computing programs. One reason for this difficulty is the large number of shifting technologies that need to be taught along with the conceptual complexity that needs to be mastered by both student and professor. Another challenge is helping students see the scope of web development, and their role in an era where the web is a part of everyday human affairs. This paper describes our 2014 textbook [2], its reception, and our plans for a second edition revision (which will be published in early 2017). Our hope is that a discussion of current teaching materials will provide opportunities to spark a dialogue not only about current technological topics, but perspectives on web development from other disciplines.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {821–822},
numpages = {2},
keywords = {web development education, textbooks, pedagogy},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890580,
author = {Isteni\v{c} Star\v{c}i\v{c}, Andreja and Turk, \v{Z}iga},
title = {Ubiquitous Learning and Digital Literacy Practices Connecting Teacher and Learner},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890580},
doi = {10.1145/2872518.2890580},
abstract = {In this paper, we discuss digital literacy and preservice teacher education and reflect on the current state of web-based education and development of digital literacy in teacher education in Slovenia. The literacy context is discussed in the context of the Educational Technology course which is delivered in teacher education. The aim of this course for preservice preprimary and primary classroom teachers is to develop student teachers' digital literacy and prepare them for the efficient integration of ICT into their teaching. This will in turn influence learners' digital literacy and competence for active engagement in the emerging culture of participation. The paper discusses web-based teaching methodology with a focus on instructional design, learning resources and high-order learning outcomes. The affordance of mobile technology fosters ubiquitous learning which has been integrated into the teacher education curriculum. The notion of tools in learning and literacies is discussed in the context of the transition from traditional written culture to digital culture. In integrating mobile learning, three important dimensions converge, underlining the development of digital literacy: the technology dimension of wireless mobile providing instant access, the social dimension and the learning behaviours dimension. A survey was conducted to examine undergraduate student-teachers' attitudes on the application of ubiquitous education and the development of digital literacy through the integration of mobile learning. The results indicate that student-teachers have developed competences in a variety of mobile learning and teaching activities They believe that mobile technology increases connection between learner and teacher but are neutral about the integration of children's social practices from their free time to school environment.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {823–827},
numpages = {5},
keywords = {web literacy curriculum, web education, teacher, mobile technology},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@dataset{10.1145/review-2872518.2890580_R52077,
author = {Michaelson, Rosa},
title = {Review ID:R52077 for DOI: 10.1145/2872518.2890580},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2872518.2890580_R52077}
}

@inproceedings{10.1145/2872518.2890579,
author = {Jellinek, Brigitte},
title = {Experiences with Curricula for a BSc and MSc in Web Development: 2008-2016},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890579},
doi = {10.1145/2872518.2890579},
abstract = {In this position paper, we describe our experience in designing and delivering a bachelor and a master program in web development at Salzburg University of Applied Sciences, Austria. While aiming to archive similar learning objectives as other programs in web development or web engineering, our historical roots in an arts program and our decision to focus on dynamic programming languages have led us to a unique program.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {829–830},
numpages = {2},
keywords = {curriculum, computing education program, web development},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890577,
author = {Mori, Kate and Ractliffe, Lucy},
title = {Evaluating the Use of a MOOC within Higher Education Professional Development Training},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890577},
doi = {10.1145/2872518.2890577},
abstract = {This paper evaluates the effectiveness of a massive open online course (MOOC) as a professional development tool in higher education. The transition from the MOOC's initial intended use as a low cost way for students to access education and aid their studies has evolved to facilitate continuing professional development (CPD), particularly within the commercial sector [1]. Findings from this study indicate there is an increase in participation and satisfaction amongst higher education staff who undertook a MOOC compared to attending traditional staff development days. Recommendations from this study?s findings highlight that staff were keen to engage with the MOOC format, but felt they needed face-to-face meetings as well to reinforce, contextualize and discuss the key messages of the MOOC. In addition to this, time allocation within workloads should be considered for any future inclusion of MOOCs for staff development.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {831–833},
numpages = {3},
keywords = {professional development, mooc, higher education, staff development, cpd},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890574,
author = {Wang, Xiaoxuan and Gao, Jiale},
title = {The Practice of Web Product Design and Development Course Design},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890574},
doi = {10.1145/2872518.2890574},
abstract = {People who design and develop web product need interdisciplinary knowledge and skills from various fields. Thus in most Chinese vocational colleges, students are lack of a completed experience to fulfill employers' needs before they leave colleges. In this paper, we propose a project-based approach to design the course, in which students unify business, product design and development theories into one project practice to build the web product. Over different stages and increasingly complexity, students learn how to interact with customers, build project management skills, design product with business mind instead of focusing on design aesthetics and developing programming skills only.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {835–836},
numpages = {2},
keywords = {teaching methods, course design, web product},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251221,
author = {Spaniol, Marc and Baeza-Yates, Ricardo and Masan\`{a}s, Julien},
title = {Session Details: TempWeb'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension.For its sixth edition, TempWeb accepted six out of eleven submissions for oral presentation. We interpret the high quality of the submissions and the frequent contributors to TempWeb, as indicators of an evolving community. It shows a clear sign of a positive dynamic in the study of time in the scope of the Web and evidence of the relevance of this effort. The workshop proceedings are published by ACM DL as part of the WWW 2016 Companion Publication.We hope you will find in these papers as well as the keynotes of Wolfgang Nejdl (L3S Hanover, Germany) and Omar Alonso (Microsoft, USA), and the discussion and exchanges of this edition of TempWeb, some motivations to look more into this important aspect of the Web.TempWeb 2016 was jointly organized by Caen University (Caen, France), Yahoo Labs (Sunnyvale, USA) and Internet Memory Foundation (Paris, France).},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889296,
author = {Alonso, Omar},
title = {Time to Ship: Some Examples from the Real-World},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889296},
doi = {10.1145/2872518.2889296},
abstract = {There is new and exciting research work in analyzing and exploiting information from corpora with temporal information such as news, email, or social media. We review some of the current activities around extracting temporal information from social media and present a number of examples from real-world systems. We also outline a number of open problems and potential new research areas.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {839},
numpages = {1},
keywords = {social media, temporal information},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889289,
author = {Kuzey, Erdal and Str\"{o}tgen, Jannik and Setty, Vinay and Weikum, Gerhard},
title = {Temponym Tagging: Temporal Scopes for Textual Phrases},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889289},
doi = {10.1145/2872518.2889289},
abstract = {For many NLP and IR applications, anchored temporal information extracted from textual documents is of utmost importance. Thus, temporal tagging -- the extraction and normalization of temporal expressions -- has gained a lot of attention in recent years and several tools such as HeidelTime and SUTime are proposed. However, such tools do not address textual phrases with temporal scopes like "Clinton's time as First Lady". While such phrases (so-called temponyms) are not temporal expressions per se, information about their temporal scopes can be helpful in many scenarios, e.g., in the context of temporal information retrieval. In this paper, we describe the integration of a wide range of temponyms to the publicly available temporal tagger HeidelTime to include temponym tagging.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {841–842},
numpages = {2},
keywords = {temporal scopes, heideltime, temporal tagging, temponyms},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889290,
author = {Moffitt, Vera Zaychik and Stoyanovich, Julia},
title = {Towards a Distributed Infrastructure for Evolving Graph Analytics},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889290},
doi = {10.1145/2872518.2889290},
abstract = {Graphs are used to represent a plethora of phenomena, from the Web and social networks, to biological pathways, to semantic knowledge bases. Arguably the most interesting and important questions one can ask about graphs have to do with their evolution. Which Web pages are showing an increasing popularity trend? How does influence propagate in social networks? How does knowledge evolve?In this paper we present our ongoing work on the Portal system, an open-source distributed framework for evolving graphs. Portal streamlines exploratory analysis of evolving graphs, making it efficient and usable, and providing critical tools to computational and data scientists. Our system implements a declarative query language by the same name, which we briefly describe in this paper. Our basic abstraction is a TGraph, which logically represents a series of adjacent snapshots. We present different physical representations of TGraphs and show results of a preliminary experimental evaluation of these physical representations for an important class of evolving graph analytics.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {843–848},
numpages = {6},
keywords = {temporal databases, evolving graphs},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889291,
author = {Santos, A\'{e}cio and Pasini, Bruno and Freire, Juliana},
title = {A First Study on Temporal Dynamics of Topics on the Web},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889291},
doi = {10.1145/2872518.2889291},
abstract = {While much work has been devoted to understanding Web dynamics and using this knowledge to efficiently maintain the freshness of the indexes of generic search engines, the same is not true for domain-specific indexes constructed by focused crawlers. For the latter, the problem is compounded by the fact that it is important not only to maintain already-crawled pages fresh, but also to identify new relevant content and expand the collection. In this paper, we discuss the challenges involved in this problem and describe our preliminary efforts in building a testbed to better understand the dynamics of specific topics and characterize how they evolve over time. We propose a data collection methodology and a set of experiments to answer important questions about temporal dynamics and evolution of topics. We also present the results of the experimental analysis we carried out using data collected over a period of four weeks using two distinct topics. These results suggest that topic-specific refreshing strategies can be beneficial for focused crawlers.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {849–854},
numpages = {6},
keywords = {web crawling, web temporal dynamics, focused crawling, web analytics},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889292,
author = {Shahriari, Mohsen and Gunashekar, Stephen and Domarus, Marven von and Klamma, Ralf},
title = {Predictive Analysis of Temporal and Overlapping Community Structures in Social Media},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889292},
doi = {10.1145/2872518.2889292},
abstract = {Digital media has some observable traces named communities. Several events such as split, merge, dissolve and survive happen to communities in social media. But what are significant features to predict these events? And to which extent a feature is relevant in a social media? To answer these questions, we perform a study on community evolution analysis and prediction. We employ three overlapping community detection (OCD) algorithms from literature to the case of time-evolving networks including social, email communication and co-authorship networks. Group evolution discovery (GED) technique is applied to track the identified communities. We compare structural properties of OCD algorithms and investigate most persistent communities over time. Furthermore, static and temporal features of a community are applied to build a logistic classifier for community evolution prediction (CEP). Results reveal important features to predict events happening to a community.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {855–860},
numpages = {6},
keywords = {community evolution tracking, overlapping community detection, community evolution prediction},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889293,
author = {Steinbauer, Matthias and Anderst-Kotsis, Gabriele},
title = {DynamoGraph: A Distributed System for Large-Scale, Temporal Graph Processing, Its Implementation and First Observations},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889293},
doi = {10.1145/2872518.2889293},
abstract = {Graph models have a long standing history as models for real world structures and processes. In recent research two important dimensions of graphs are described of particular importance. (1) Temporal aspects of graphs cannot be neglected for many current application scenarios such as social network analysis or the analysis of the global web graph. (2) The mentioned graph structures have grown to very large sizes such that traditional methodologies no longer hold. In this work a distributed computing framework designed for storing and processing of large-scale temporal graphs is presented. For this system a reference implementation in Java was created. In this paper first insight on the implementation and observations in using the system are discussed.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {861–866},
numpages = {6},
keywords = {large-scale graph, pregel, graph processing, distributed computing, temporal graph},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2889294,
author = {Truv\'{e}, Staffan},
title = {Temporal Analytics for Predictive Cyber Threat Intelligence},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889294},
doi = {10.1145/2872518.2889294},
abstract = {Recorded Future has developed its Temporal Analytics Engine as a general purpose platform for harvesting and analyzing unstructured text from the open, deep, and dark web, and for transforming that content into a structured representation suitable for different analyses.In this paper we present some of the key components of our system, and show how it has been adapted to the increasingly important domain of cyber threat intelligence.We also describe how our data can be used for predictive analytics, e.g. to predict the likelihood of a product vulnerability being exploited or to assess the maliciousness of an IP address.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {867–868},
numpages = {2},
keywords = {web intelligence, cyber security, predictions},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251222,
author = {Berendt, Bettina and Hollink, Laura and Luczak-Roesch, Markus},
title = {Session Details: USEWOD'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 6th International Workshop on Usage Analysis and the Web of Data (USEWOD), associated with WWW 2016.The workshop is dedicated to the diverse ecosystem of Web of Data access mechanisms. From academic to government data, from complex SPARQL queries to Linked Data Fragments, from DBpedia to Wikidata: mthe data sources on the Web of Data and the ways in which these sources can be created and consumed vary greatly and raise fundamental questions.The call for papers attracted submissions from United States, Canada, Asia, and Europe. The program committee reviewed and accepted the following: Venue or Track Reviewed -4 Accepted - 2.Additionally, we decided to publish an invited paper that presents a particularly interesting crossdisciplinary perspective on the Web of Data and outline current research directions and challenges in an extended 'Message from the USEWOD Chairs'.The USEWOD 2016 Research Dataset As in previous years, a standard research dataset of usage data from well-recognized Web-of-Data datasets has been published to promote reproducible research on the workshop themes. A particular highlight of this year's dataset is overlapping usage data from the official DBpedia servers as well as the Linked Data Fragments interface to DBpedia and Wikidata. This dataset allows researchers to study alternative Web of Data usage mechanisms in an unprecedented way and could therefore become a unique resource of great importance for the field. For more information on the datasets released in previous years, please see http://usewod.org/data-sets.html. Special thanks got to Open Link Software for providing us with DBpedia logs as well as Ruben Verborgh for access to Linked Data Fragments usage data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891071,
author = {Casemajor, Nathalie},
title = {Embedded Metadata and the Digital Lifecycle of Images: Methodological Challenges},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891071},
doi = {10.1145/2872518.2891071},
abstract = {This paper analyses the specificities of metadata embedded in photographic images. It investigates how embedded metadata can help studying the usage patterns and conditions of circulation of images on digital networks.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {871–872},
numpages = {2},
keywords = {usage traces., embedded metadata, image, circulation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891069,
author = {Colpaert, Pieter and Chua, Alvin and Verborgh, Ruben and Mannens, Erik and Van de Walle, Rik and Vande Moere, Andrew},
title = {What Public Transit API Logs Tell Us about Travel Flows},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891069},
doi = {10.1145/2872518.2891069},
abstract = {In the field of smart cities, researchers need an indication of how people move in and between cities. Yet, getting statistics of travel flows within public transit systems has proven to be troublesome. In order to get an indication of public transit travel flows in Belgium, we analyzed the query logs of the iRail API, a highly expressive route planning API for the Belgian railways. We were able to study 100k to 500k requests for each month between October 2012 and November 2015, which is between 0.56% and 1.66% of the amount of monthly passengers. Using data visualizations, we illustrate the commuting patterns in Belgium and confirm that Brussels, the capital, acts as a central hub. The Flemish region appears to be polycentric, while in the Walloon region, everything converges on Brussels. The findings correspond to the real travel demand, according to experts of the passenger federation Trein Tram Bus. We conclude that query logs of route planners are of high importance in getting an indication of travel flows. However, better travel intentions would be acquirable using dedicated HTTP POST requests.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {873–878},
numpages = {6},
keywords = {linked open data, query logs, public transit, smart cities, open data, route planning},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891070,
author = {Lakshminarayan, Choudur and Kosuru, Ram and Hsu, Meichun},
title = {Modeling Complex Clickstream Data by Stochastic Models: Theory and Methods},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891070},
doi = {10.1145/2872518.2891070},
abstract = {As the website is a primary customer touch-point, millions are spent to gather web data about customer visits. Sadly, the trove of data and corresponding analytics have not lived up to the promise. Current marketing practice relies on ambiguous summary statistics or small-sample usability studies. Idiosyncratic browsing and low conversion (browser-to-buyer) make modeling hard. In this paper, we model browsing patterns (sequence of clicks) via Markov chain theory to predict users' propensity to buy within a session. We focus on model complexity, imputing missing values, data augmentation, and other attendant issues that impact performance. The paper addresses the following aspects; (1) Determine appropriate order of the Markov chain (assess the influence of prior history in prediction), (2) Impute missing transitions by exploiting the inherent link structure in the page sequences, (3) predict the likelihood of a purchase based on variable-length page sequences, and (4) Augment the training set of buyers (which is typically very small: 2% by viewing the page transitions as a graph and exploiting its link structure to improve performance. The cocktail of solutions address important issues in practical digital marketing. Extensive analysis of data applied to a large commercial web-site shows that Markov chain based classifiers are useful predictors of user intent.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {879–884},
numpages = {6},
keywords = {imputation, link analysis, prediction, markov chains, click streams},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891068,
author = {Luczak-Roesch, Markus and Hollink, Laura and Berendt, Bettina},
title = {Current Directions for Usage Analysis and the Web of Data: The Diverse Ecosystem of Web of Data Access Mechanisms},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891068},
doi = {10.1145/2872518.2891068},
abstract = {Usage mining always was and still is a key topic for research in the context of the Web [16]. This is evidenced by the series of papers that appear in the scientific tracks of the WWW conference year by year. Web usage is being studied to create economic value by placing targeted ads or delivering personalized content, but also in order to better understand how people behave online in mass movements and collective action.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {885–887},
numpages = {3},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251223,
author = {Bourdeau, Jacqueline and White, Bebo and King, Irwin},
title = {Session Details: WEBED'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the ACM WWW2016 Workshop on Web Science and Technology for Education (WebED2016), co-located with the 2016 International WWW Conference. This workshop series began as The Workshop on Web-based Education Technologies (WebET) at WWW2014 in Seoul, Korea. However, this year's workshop has expanded its scope to explore the influence the growing field of Web Science. By doing so it is our goal to bring together educational technologists, Web researchers, and members of social science communities seeking to investigate the impact of Web technology on teaching and learning. The mission of the workshop is for attendees to share novel solutions that fulfill the needs of heterogeneous applications and environments and identify new directions for future research and development. It is also our hope that WebED2016 attendees might identify others with similar interests possibly leading to new collaborations and joint efforts.We encourage workshop attendees to attend the keynote speaker presentation, the accepted paper presentations, and the expert panel discussion. Keynote: "Web Science, Social Media and Education," Dame Wendy Hall, University of Southampton,Panel: "Evaluating Educational Software in the Web Era," Jutta Treviranus (Ontario College of Art and Design University), Jean-Philippe Bradette (Ellicom), Irwin King (The Chinese University of Hong Kong), Beverly Woolf (University of Massachusetts Amherst), and Irina Muhina (iecarus, moderator)},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891081,
author = {Hall, Wendy},
title = {Web Science, Social Media and Education},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891081},
doi = {10.1145/2872518.2891081},
abstract = {Over the last 25 years the Web has evolved into a critical global infrastructure. Since its emergence in the 1990s, it has exploded into hundreds of billions of pages that touch almost all aspects of modern life. Little appreciated, however, is the fact that the Web is more than the sum of its pages and it is more than its technical protocols. Vast emergent properties have arisen that are transforming society. Web Science is the study of the Web as a socio-technical system. As the Web becomes increasingly significant in all our lives, studying it from an interdisciplinary perspective becomes even more important.We are now rapidly moving into a world of data on and about the Web, which gives rise to even more opportunities and challenges. In this talk we will explore the role of observatories and data analytics for the development of new methodologies for longitudinal research in Web Science that could help us understand more about how the Web evolves as a social-technical network. After many years of speculation about the potential of on-line learning, web technology has finally developed to a point where on-line learning is today a reality -- from MOOCS to complete on-line degree courses. All rely on the use of social media to enhance the learning environment for the students. We will discuss the application of the web observatory approach to the study of on-line learning and the insights such an approach can reveal that would not be possible in a traditional learning environment -- potentially giving rise to world-wide studies in this area.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {891},
numpages = {1},
keywords = {keynote talk},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891076,
author = {Carrillo, Rubiela and Lavou\'{e}, Elise and Pri\'{e}, Yannick},
title = {Towards Qualitative Insights for Visualizing Student Engagement in Web-Based Learning Environments},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891076},
doi = {10.1145/2872518.2891076},
abstract = {Learning Sciences argue that student engagement is composed of behavioral, motivational and cognitive dimensions. Many proposals in Learning Analytics have provided teachers with quantitative indicators focusing only on students' behaviors, such as the number and the duration of their actions with the learning environment. In this paper, we propose visual representations of cognitive indicators to add explanatory elements to behavioral indicators. We describe our general architecture for collecting and aggregating data used to build the proposed visualizations. We illustrate the use of these indicators in various pedagogical scenarios oriented towards supporting teachers in students' actions and performances understanding.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {893–898},
numpages = {6},
keywords = {indicators, web-based learning environment, learning analytics, visual analytics, student engagement, qualitative clues},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891080,
author = {Chan, Hou Pong and Zhao, Tong and King, Irwin},
title = {Trust-Aware Peer Assessment Using Multi-Armed Bandit Algorithms},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891080},
doi = {10.1145/2872518.2891080},
abstract = {Massive Open Online Coursers (MOOCs) offer a convenient way for people to access quality courses via the internet. However, the problem of grading open-ended assignments at such a large scale still remains challenging. Although peer assessment have been proposed to handle the large-scale grading problem in MOOCs, existing methods still suffer several limitations: (1) most current peer assessment research ignore the importance of how to allocate the assessment tasks among peers, (2) existing approaches for peer grading learn the complete ranking in an offline manner, (3) theoretical analysis for trust-aware peer grading is missing. In this work, we consider the case that we have prior knowledge about all students' reliability. We formulate the problem of peer assessment as a sequential noisy ranking aggregation problem. We derive a trust-aware allocation scheme for peer assessment to maximize the probability of constructing a correct ranking of assignments with a budget constraint.Moreover, we also derive an upper bound for the probability of prediction error on the inferred ranking of assignments. Furthermore, we propose the Trust-aware Ranking-based Multi-armed Bandit Algorithms to sequentially allocate the assessment tasks to the students based on the derived allocation scheme and learn an accurate peer grading result by taking students' reliability into consideration.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {899–903},
numpages = {5},
keywords = {multi-armed bandit, peer assessment, rank aggregation, moocs, peer grading, dueling bandit, ordinal feedback},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891073,
author = {Cordova-Sanchez, Mariheida and Yanardag, Pinar},
title = {Turning Down the Noise in Classrooms},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891073},
doi = {10.1145/2872518.2891073},
abstract = {The use of micro-blogging in classrooms is a recently trending concept in computer-aided education. Micro-blogs offer an effective way of communication in large classrooms, and engage students in meaningful discussions. However, existing micro-blogging systems in education setting suffer from a few drawbacks. First, relevant content might be overwhelmed by irrelevant posts to the lecture which could jeopardize effective learning. Second, students might generate redundant content by posting similar questions to each other and create substantial information overload. Third, posts covering different aspects of the class might be left undiscovered due to real-time characteristics of micro-blogs. To address these issues, we present a principled approach for picking a set of posts that promotes relevant and diverse content while effectively turning down the noise created by redundant posts. We formulate this task as a submodular optimization problem for which we provide an efficient and near-optimal solution. We evaluate our framework on real micro-blog based classroom datasets and our empirical results demonstrate that our framework is effectively able to cover the most important and diverse content that is being discussed in classrooms.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {905–910},
numpages = {6},
keywords = {education, micro-blogging, submodularity, query expansion, recommendation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891074,
author = {de Santana, Sivaldo J. and Souza, Hugo A. and Florentin, Victor A.F. and Paiva, Ranilson and Bittencourt, Ig Ibert and Isotani, Seiji},
title = {A Quantitative Analysis of the Most Relevant Gamification Elements in an Online Learning Environment},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891074},
doi = {10.1145/2872518.2891074},
abstract = {In the last decade, many researchers have studied the use of game elements in education. The term "gamification" refers to the application of elements used in the development of video games, such as mechanics and dynamics in other contexts unrelated to games, in order to generate more enjoyable and positive attitudes from the students. The gamication process involves using several elements present in video games, like: points, levels, rankings, rewards (badges/achievements) and missions. In this study, we assess whether or not, gamification elements can help and motivate students enrolled in a gamified ontology-based adaptive online learning environment called MeuTutor. In this context, we followed the Pedagogical Recommendation Process to discover which gamification elements were relevant to promote learning, in order to recommend improvements to the environment. To do that, this study shows a quantitative analysis(correlation analysis) of the gamification elements from MeuTutor.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {911–916},
numpages = {6},
keywords = {education, gamification, ontology-based adaptative learning environment, learning analytics},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891075,
author = {Gutierrez-Santos, Sergio and Capuzzi, Stefano and Kahn, Ken and Karkalas, Sokratis and Poulovassilis, Alexandra},
title = {Scalable Monitoring of Student Interaction Indicators in Exploratory Learning Environments},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891075},
doi = {10.1145/2872518.2891075},
abstract = {We present and evaluate a web-based architecture for monitoring student-system interaction indicators in Exploratory Learning Environments (ELEs),using as our case study a microworld for secondary school algebra. We discuss the challenging role of teachers in exploratory learning settings and motivate the need for visualisation and notification tools that can assist teachers in focusing their attention across the class and inform teachers' interventions. We present an architecture that can support such Teacher Assistance tools and demonstrate its scalability to allow concurrent usage by thousands of users (students and teachers).},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {917–922},
numpages = {6},
keywords = {visualisation, monitoring, exploratory learning environments},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891079,
author = {Luccioni, Alexandra and Nkambou, Roger and Massardi, Jean and Bourdeau, Jacqueline and Coulombe, Claude},
title = {STI-DICO: A Web-Based System for Intelligent Tutoring of Dictionary Skills},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891079},
doi = {10.1145/2872518.2891079},
abstract = {In this paper, we describe an innovative project where Web technologies are exploited to develop an Intelligent Tutoring System (ITS) that uses a Learning Management System (LMS) as its learning interface. The resulting ITS has been instantiated into a specific system called STI-DICO which aims at helping future French primary school teachers to acquire the knowledge and skills needed to use the French dictionary. The learning process in the ITS takes place via a number of authentic learning scenarios that represent situations that the future teachers will face in the classroom. By using a LMS as the learning interface component of the system, we enable it to be directly deployable on the Web to a large population of students, all the while retaining the adaptive components of an ITS to deliver a personalized learning experience to its users.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {923–928},
numpages = {6},
keywords = {web-based learning, service oriented architecture, learning management system, intelligent tutoring system},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inbook{10.1145/2872518.2891077,
author = {Pursel, Bart and Liang, Chen and Wang, Shuting and Wu, Zhaohui and Williams, Kyle and Brautigam, Benjamin and Saul, Sherwyn and Williams, Hannah and Bowen, Kyle and Giles, C. Lee},
title = {BBookX: Design of an Automated Web-Based Recommender System for the Creation of Open Learning Content},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891077},
abstract = {We describe BBookX, a web-based tool that uses a human-computing approach to facilitate the creation of open source textbooks. The goal of BBookX is to create a system that can search various Open Educational Resource (OER) repositories such as Wikipedia, based on a set of user-generated criteria, and return various resources that can be combined, remixed, and re-used to support specific learning goals. As BBookX is a work-in-progress, we are in the midst of a design-based research study, where user testing guided multiple rounds of iteration in the design of the user interface (UI) as well as the query engine. From an interface perspective, the challenges we present are the matching of the UI to users' mental models from similar systems, as well as educating users how to best work with the algorithms in an iterative manner to find and refine content for inclusion into open textbooks.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {929–933},
numpages = {5}
}

@inproceedings{10.5555/2872518.3251224,
author = {Akerkar, Rajendra and Maret, Pierre and Vercouter, Laurent},
title = {Session Details: WI&amp;C'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is with great pleasure, and on behalf of the organizing committee, we would like to welcome you to the 8th International Workshop on Web Intelligence &amp; Communities (WI&amp;C 2016) taking place on April 11th in Montreal, Canada and collocated with the WWW 2016 conference.This workshop, the eighth in a series of workshops, is intended to stimulate discussions on the forefront of research concerned with web intelligence applied to collaborative networks. Web Intelligence consists of a multidisciplinary area dealing with exploiting data and services over the Web, to create new data and services using both Information and Communication Technologies (ICT) and Artificial Intelligence (AI) techniques. Communities appear as a first-class object in the areas of web intelligence and agent technologies, as well as a crucial crossroads of several sub-domains (i.e. user modelling, protocols, data management, data mining, content modelling, etc.). These sub-domains impact the nature of the communities and the applications which are related to them. These applications are numerous, and the success of well-known Social Network Sites for entertainment should not be allowed to over-shadow the other application domains, for instance in education, health, design, knowledge management, and so forth.The workshop will provide presentation and discussion opportunities for researchers working on web intelligence applied to collaborative networks, such as virtual communities. The possibilities and consequences of the web usage for collaborative networks are tremendous and new tools are required to satisfy users and service providers.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {2},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890584,
author = {Esfandiari, Babak and Davoust, Alan},
title = {Distributed Wikis and Social Networks: A Good Fit},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890584},
doi = {10.1145/2872518.2890584},
abstract = {Social networks can play an important role in the process of decentralizing authority in distributed systems. We will focus on distributed wiki systems, and we show how, in the special case of a peer-to-peer wiki, there is a rational incentive for users to self-organize and form a meaningful social network. We discuss to that effect the basic metrics that can be derived from the topology of the social network to help assess the subjective quality of wiki entries. Demos and experimental results will illustrate and support our discussion. We finally speculate as to how these results may also translate to discussion forums or recommender systems.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {937–938},
numpages = {2},
keywords = {distributed wikis, social networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890585,
author = {Chernov, Alexandr and Lagos, Nikolaos and Gall\'{e}, Matthias and S\'{a}ndor, \'{A}gnes},
title = {Enriching How-to Guides by Linking Actionable Phrases},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890585},
doi = {10.1145/2872518.2890585},
abstract = {The World Wide Web contains a large number of community created knowledge of instructional nature. Similarly, in a commercial setting, databases of instructions are used by customer-care providers to guide clients in the resolution of issues. Most of these instructions are expressed in natural language. Knowledge Bases including such information are valuable through the sum of their single entries. However, as each entry is created mostly independently, users (e.g. other community members) cannot take advantage of the accumulated knowledge that can be developed via the aggregation of related entries. In this paper we consider the problem of inter-linking Knowledge Base entries, in order to get relevant information from other parts of the Knowledge Base.To achieve this, we propose to detect textit{actionable phrases} -- text fragments that describe how to perform a certain action -- and link them to other entries. The extraction method that we implement achieves an F-score of 67.35%. We also show that using actionable phrases results in better linking quality than using coarser-grained spans of text, as proposed in the literature. Besides the evaluation of both steps, we also include a detailed error analysis and release our annotation to the community.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {939–944},
numpages = {6},
keywords = {procedural knowledge, blog analysis, forum mining},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890583,
author = {Maret, Pierre and Akerkar, Rajendra and Vercouter, Laurent},
title = {Web Communities in Big Data Era. Editorial},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890583},
doi = {10.1145/2872518.2890583},
abstract = {Web-based community is a self-defined web-based network of interactive communication organized around a shared interest or purpose. It provides the means of interactions among people in which they create, share, and exchange information and ideas in virtual space and networks. Working with big data often requires querying and reasoning that data to isolate information of interest and manipulate it in various ways. This editorial paper explores recent big data research topics -- stream querying and reasoning -- over data from web based communities. It combines aspects from some well-studied research domains, such as, social network analysis, graph databases, and data streams. We provide a brief synopsis of some research issues in supporting reasoning and querying tasks. This editorial also presents the WI&amp;C-16 workshop's goal and programme.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {945–947},
numpages = {3},
keywords = {and reasoning, web communities, querying, datastream},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890587,
author = {Navarro, Lucas Fonseca and Hruschka, Estevam Rafael and Appel, Ana Paula},
title = {Ontological Networks: Mapping Ontological Knowledge Bases into Graphs},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890587},
doi = {10.1145/2872518.2890587},
abstract = {With the exponentially growing amount of available data on the Web over the last years, several projects have been created to automatically extract knowledge from this information set. As the data domains on the Web are too wide, most of these projects store the acquired knowledge in ontological knowledge bases (OKBs). Mapping it into graph-based representation makes possible to apply graph-mining techniques to extract implicit information. However most of these projects treat the mapping process using different adjustments in several ways, thus, there is not a standard mapping process or a formal way defifined to do this task. In this paper we formally describe a graph structure called Ontological Network and how it can be used to map an Ontological Knowledge Base. We also show some graph-mining based algorithms to add new facts and to extend the ontology of an OKB while mapped into an Ontological Network as example.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {949–954},
numpages = {6},
keywords = {graph mining, ontological knwoledge bases, machine learning, ontology engineering},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890586,
author = {Yamak, Zaher and Saunier, Julien and Vercouter, Laurent},
title = {Detection of Multiple Identity Manipulation in Collaborative Projects},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890586},
doi = {10.1145/2872518.2890586},
abstract = {Various techniques are used to manipulate users in OSN environments such as social spam, identity theft, spear phishing and Sybil attacks... In this article, we are interested in analyzing the behavior of multiple fake accounts that try to bypass the OSN regulation. In the context of social media manipulation detection, we focus on the special case of multiple Identity accounts (Sockpuppet) created on English Wikipedia (EnWiki). We set up a complete methodology spanning from the data extraction from EnWiki to the training and testing of our selected data using several machine learning algorithms.In our methodology we propose a set of features that grows on previous literature to use in automatic data analysis in order to detect the Sockpuppets accounts created on EnWiki. We apply them on a database of 10.000 user accounts. The results compare several machine learning algorithms to show that our new features and training data enable to detect 99% of fake accounts, improving previous results from the literature.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {955–960},
numpages = {6},
keywords = {deception, social media, collaborative project, manipulation, machine learning application, identity, sockpuppet, wikipedia},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890588,
author = {Ye, Qi and Wang, Feng and Li, Bo},
title = {StarrySky: A Practical System to Track Millions of High-Precision Query Intents},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890588},
doi = {10.1145/2872518.2890588},
abstract = {Query intent mining is a critical problem in various real-world search applications. In the past few years we have witnessed dramatic advances in the field of query intent mining area. In this paper, we present a practical system---StarrySky for identifying and inferring millions of query intents in daily sponsored search with high precision and acceptable coverage. We have already achieved great advantages by deploying this system in Sogou sponsored search enginefootnote {http://www.sogou.com}. The general architecture of StarrySky consists of three stages. First, we detect millions of fine-grained query clusters from two years of click logs which can represent different query intents. Second, we refine the qualities of query clusters with a series of well-designed operations, and call the final refined clusters as concepts. Third and foremost, we build a flexible real-time inference algorithm for assigning query intents to the detected concepts with high precision. Beyond the description of the system, we employ several experiments to evaluate its performance and flexibility. Our inference algorithm achieves up to 96% precision and 68% coverage on daily search requests. We believe StarrySky is a practical and valuable system for tracking query intents.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {961–966},
numpages = {6},
keywords = {query intent, large-scale multi-class query classification, community detection, search log mining},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251225,
author = {West, Robert and Zia, Leila and Taraborelli, Dario and Leskovec, Jure},
title = {Session Details: Wiki Workshop'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to Wiki Workshop, a forum for bringing together researchers exploring all aspects of Wikipedia and other Wikimedia sites.Like the editing aspect of Wikimedia sites, research on Wikimedia projects relies heavily on the community of researchers who explore the projects to improve our understanding of the current state and future directions of such projects. This workshop aims to bring this community together on an annual basis and to welcome new community members. As part of the workshop, we will share the latest research on Wikimedia projects, explore and share new directions for research, learn about the new data-sets that have been released publicly, and initiate or continue on new research initiatives.The target audience will include researchers in: natural language processingweb mining and data miningartificial intelligence and machine learninggraph and network theorysocial computinguser generated content industrysocial scienceslinguisticsThe workshop is the second workshop in these series, proceeding the workshop held in ICWSM 2015. The list of speakers and program can be found at the workshop's website.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {1},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891111,
author = {Boldi, Paolo and Monti, Corrado},
title = {Cleansing Wikipedia Categories Using Centrality},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891111},
doi = {10.1145/2872518.2891111},
abstract = {We propose a novel general technique aimed at pruning and cleansing the Wikipedia category hierarchy, with a tunable level of aggregation. Our approach is endogenous, since it does not use any information coming from Wikipedia articles, but it is based solely on the user-generated (noisy) Wikipedia category folksonomy itself. We show how the proposed techniques can help reduce the level of noise in the hierarchy and discuss how alternative centrality measures can differently impact on the result.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {969–974},
numpages = {6},
keywords = {categorization, networks},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891117,
author = {Brasileiro, Freddy and Almeida, Jo\~{a}o Paulo A. and Carvalho, Victorio A. and Guizzardi, Giancarlo},
title = {Applying a Multi-Level Modeling Theory to Assess Taxonomic Hierarchies in Wikidata},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891117},
doi = {10.1145/2872518.2891117},
abstract = {Wikidata captures structured data on a number of subject domains, managing, among others, the information underlying Wikipedia and other Wikimedia projects. Wikidata serves as a repository of structured data, whose purpose is to support the consistent sharing and linking of data on the Web. To support these purposes, it is key that Wikidata is built on consistent data models and representation schemas, which are constructed and managed in a collaborative platform. In this paper, we address the quality of taxonomic hierarchies in Wikidata. We focus on taxonomic hierarchies with entities at different classification levels (particular individuals, types of individuals, types of types of individuals, etc.). We use an axiomatic theory for multi-level modeling to analyze current Wikidata content, and identify a significant number of problematic classification and taxonomic statements. The problems seem to arise from an inadequate use of instantiation and subclassing in certain Wikidata hierarchies.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {975–980},
numpages = {6},
keywords = {taxonomies, metamodeling, wikidata, multi-level modeling},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891116,
author = {Chattopadhyay, T. and Maiti, Santa and Pal, Arindam and Ghose, Avik and Pal, Arpan and Viswanathan, Shanky and Sivakumar, Narendran},
title = {Automatic Discovery of Emerging Trends Using Cluster Name Synthesis on User Consumption Data: Extended Abstract},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891116},
doi = {10.1145/2872518.2891116},
abstract = {A business problem for the telecommunication companies is to provide an appropriate promotional coupon to suitable customers. This problem leads to the challenge of identifying behavioral patterns of customers and deliver the right customer engagement at the right time. So there is a need for a system that can enable the telecommunication companies to go for the best marketing strategy by leveraging customer intelligence to drive offer acceptance based on personas. Technically it is possible for the telecommunication companies to recommend suitable advertisements if they can classify the web sites browsed by their customers into classes like sports, e-commerce, social networking, streaming media etc. Another problem is to classify a new website when it doesn't belong to any of the existing clusters. In this paper, the authors are going to propose a method to automatically classify the websites and synthesize the cluster names in case it doesn't belong to any of the predefined clusters. We have experimented on a small set of data set and the classification results are quite convincing. Moreover, the phrases used to describe a website if it doesn't belong to existing classes are compliant to the phrases obtained from manual annotation. This proposed system uses the Wikipedia data to construct the document for the websites browsed by the customers.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {981–983},
numpages = {3},
keywords = {website classification, telecommunication use cases, wikipedia, cluster name synthesis},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891109,
author = {Gei\ss{}, Johanna and Gertz, Michael},
title = {With a Little Help from My Neighbors: Person Name Linking Using the Wikipedia Social Network},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891109},
doi = {10.1145/2872518.2891109},
abstract = {Driven by the popularity of social networks, there has been an increasing interest in employing such networks in the context of named entity linking. In this paper, we present a novel approach to person name disambiguation and linking that uses a large-scale social network extracted from the English Wikipedia. First, possible candidate matches for an ambiguous person name are determined. With each candidate match, a network substructure is associated. Based on the similarity between these network substructures and the latent network of an ambiguous person name in a document, we propose an efficient ranking method to resolve the ambiguity. We demonstrate the effectiveness of our approach, resulting in an overall precision of over 96% for disambiguating person names and linking them to real world entities.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {985–990},
numpages = {6},
keywords = {social network, nel, wikipedia, wikidata},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891115,
author = {Roitman, Haggai and Hummel, Shay and Rabinovich, Ella and Sznajder, Benjamin and Slonim, Noam and Aharoni, Ehud},
title = {On the Retrieval of Wikipedia Articles Containing Claims on Controversial Topics},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891115},
doi = {10.1145/2872518.2891115},
abstract = {This work presents a novel claim-oriented document retrieval task. For a given controversial topic, relevant articles containing claims that support or contest the topic are retrieved from a Wikipedia corpus. For that, a two-step retrieval approach is proposed. At the first step, an initial pool of articles that are relevant to the topic are retrieved using state-of-the-art retrieval methods. At the second step, articles in the initial pool are re-ranked according to their potential to contain as many relevant claims as possible using several claim discovery features. Hence, the second step aims at maximizing the overall claim recall of the retrieval system. Using a recently published claims benchmark, the proposed retrieval approach is demonstrated to provide more relevant claims compared to several other retrieval alternatives.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {991–996},
numpages = {6},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891112,
author = {Steiner, Thomas},
title = {Wikipedia Tools for Google Spreadsheets},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891112},
doi = {10.1145/2872518.2891112},
abstract = {In this paper, we introduce the Wikipedia Tools for Google Spreadsheets. Google Spreadsheets is part of a free, Web-based software office suite offered by Google within its Google Docs service. It allows users to create and edit spreadsheets online, while collaborating with other users in realtime. Wikipedia is a free-access, free-content Internet encyclopedia, whose content and data is available, among other means, through an API. With the Wikipedia Tools for Google Spreadsheets, we have created a toolkit that facilitates working with Wikipedia data from within a spreadsheet context. We make these tools available as open-source on GitHub [https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released under the permissive Apache 2.0 license.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {997–1000},
numpages = {4},
keywords = {google spreadsheets, wikidata, google sheets, wikipedia},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891113,
author = {Suzuki, Yu and Nakamura, Satoshi},
title = {Assessing the Quality of Wikipedia Editors through Crowdsourcing},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891113},
doi = {10.1145/2872518.2891113},
abstract = {In this paper, we propose a method for assessing the quality of Wikipedia editors. By effectively determining whether the text meaning persists over time, we can determine the actual contribution by editors. This is used in this paper to detect vandal. However, the meaning of text does not always change if a term in the text is added or removed. Therefore, we cannot capture the changes of text meaning automatically, so we cannot detect whether the meaning of text survives or not. To solve this problem, we use crowdsourcing to manually detect changes of text meaning. In our experiment, we confirmed that our proposed method improves the accuracy of detecting vandals by about 5%.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1001–1006},
numpages = {6},
keywords = {vandalism, crowdsourcing, wikipedia, quality},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891110,
author = {Tinati, Ramine and Luczak-Roesch, Markus and Hall, Wendy},
title = {Finding Structure in Wikipedia Edit Activity: An Information Cascade Approach},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891110},
doi = {10.1145/2872518.2891110},
abstract = {This paper documents a study of the real-time Wikipedia edit stream containing over 6 million edits on 1.5 million English Wikipedia articles, during 2015. We focus on answering questions related to identification and use of information cascades between Wikipedia articles, based on author editing activity. Our findings show that by constructing information cascades between Wikipedia articles using editing activity, we are able to construct an alternative linking structure in comparison to the embedded links within a Wikipedia page. This alternative article hyperlink structure was found to be relevant in topic, and timely in relation to external global events (e.g., political activity). Based on our analysis, we contextualise the findings against areas of interest such as events detection, vandalism, edit wars, and editing behaviour.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1007–1012},
numpages = {6},
keywords = {wikipedia, network structure, information cascades},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891114,
author = {Yadav, Vikrant and Kumar, Sandeep},
title = {Learning Web Queries for Retrieval of Relevant Information about an Entity in a Wikipedia Category},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891114},
doi = {10.1145/2872518.2891114},
abstract = {In this paper, we present a novel method to obtain a set of most appropriate queries for retrieval of relevant information about an entity from the Web. Using the body text of existing articles in a Wikipedia category, we generate a set of queries capable of fetching the most relevant content for any entity belonging to that category. We find the common topics discussed in the articles of a category using Latent Semantic Analysis (LSA) and use them to formulate the queries. Using Long Short-Term Memory (LSTM) neural network, we reduce the number of queries by removing the less sensible ones and then select the best ones out of them. The experimental results show that the proposed method outperforms the baselines. Existing approaches are performing better in generation of the relevant section title queries by extraction from the headings of the Wikipedia articles as compared to the generation of queries by extraction from the body text of the articles. Whereas, the experimental results show that the proposed approach can perform equally well and even better in extraction of the relevant queries from the body text of the Wikipedia articles.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1013–1014},
numpages = {2},
keywords = {long short-term memory neural networks (lstm), information retrieval, wikipedia article generation, query generation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2872518.3251226,
author = {Tiropanis, Thanassis and Weber, Matthew},
title = {Session Details: Tutorials},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the WWW 2016 Tutorials. We received 21 proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty, selecting 5 half-day tutorials and 2 full-day tutorials. We also took in account the coverage of the different areas related to WWW as well as the potential audience, to schedule them in two consecutive days with the minimal audience interest overlap.The morning of the first day includes the following four tutorials: Computational Social Science for the World Wide WebCentrality Measures on Big GraphsThe afternoon of the first day includes the following four tutorials: Computational Social Science for the World Wide Web (continued)Cryptographic Currencies Crash CourseThe second day starts with three tutorials: Building Decentralized Applications for the Social WebAutomatic Entity Recognition and Typing in Massive Text CorporaMining Big Time-series Data on the WebThe final afternoon includes the last three tutorials: Building Decentralized Applications for the Social Web (continued)Analyzing sequential User Behavior on the WebThe call for tutorials attracted submissions from United States, Europe, Asia, Africa and South America. Review and acceptance statistics are as follows: WWW 2016 Tutorials Reviewed -21 Accepted - 7.We believe that the program provides a good balance between several trending topics such as deep learning, social media analysis, graph mining, crowdsourcing, knowledge databases, mobile data, etc. Hence we hope that you will find the tutorial program interesting, providing you with a valuable opportunity to learn and share ideas with other researchers and practitioners from institutions around the world.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
numpages = {1},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891063,
author = {Bonchi, Francesco and De Francisci Morales, Gianmarco and Riondato, Matteo},
title = {Centrality Measures on Big Graphs: Exact, Approximated, and Distributed Algorithms},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891063},
doi = {10.1145/2872518.2891063},
abstract = {Centrality measures allow to measure the relative importance of a node or an edge in a graph w.r.t.~other nodes or edges. Several measures of centrality have been developed in the literature to capture different aspects of the informal concept of importance, and algorithms for these different measures have been proposed. In this tutorial, we survey the different definitions of centrality measures and the algorithms to compute them. We start from the most common measures, such as closeness centrality and betweenness centrality, and move to more complex ones such as spanning-edge centrality. In our presentation, we begin from exact algorithms and then progress to approximation algorithms, including sampling-based ones, and to highly-scalable MapReduce algorithms for huge graphs, both for exact computation and for keeping the measures up-to-date on dynamic graphs where edges are inserted or removed over time. Our goal is to show how advanced algorithmic techniques and scalable systems can be used to obtain efficient algorithms for an important graph mining task, and to encourage research in the area by highlighting open problems and possible directions.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1017–1020},
numpages = {4},
keywords = {centrality, streams, closeness, tutorial, mapreduce, sampling, betweenness},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891066,
author = {Judmayer, Aljosha and Weippl, Edgar},
title = {Cryptographic Currencies Crash Course (C4): Tutorial},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891066},
doi = {10.1145/2872518.2891066},
abstract = {"Bitcoin is a rare case where practice seems to be ahead of theory." Joseph Bonneau et al.[15]This tutorial aims to further close the gap between IT security research and the area of cryptographic currencies and block chains. We will describe and refer to Bitcoin as an example throughout the tutorial, as it is the most prominent representative of a such a system. It also is a good reference to discuss the underlying block chain mechanics which are the foundation of various altcoins (e.g. Namecoin) and other derived systems. In this tutorial, the topic of cryptographic currencies is solely addressed from a technical IT security point-of-view. Therefore we do not cover any legal, sociological, financial and economical aspects.The tutorial is designed for participants with a solid IT security background but will not assume any prior knowledge on cryptographic currencies. Thus, we will quickly advance our discussion into core aspects of this field.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1021–1024},
numpages = {4},
keywords = {bitcoin, block chain, cryptographic currencies},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891065,
author = {Ren, Xiang and El-Kishky, Ahmed and Wang, Chi and Han, Jiawei},
title = {Automatic Entity Recognition and Typing in Massive Text Corpora},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891065},
doi = {10.1145/2872518.2891065},
abstract = {In today's computerized and information-based society, we are soaked with vast amounts of natural language text data, ranging from news articles, product reviews, advertisements, to a wide range of user-generated content from social media. To turn such massive unstructured text data into actionable knowledge, one of the grand challenges is to gain an understanding of entities and the relationships between them. In this tutorial, we introduce data-driven methods to recognize typed entities of interest in different kinds of text corpora (especially in massive, domain-specific text corpora). These methods can automatically identify token spans as entity mentions in text and label their types (e.g., people, product, food) in a scalable way. We demonstrate on real datasets including news articles and yelp reviews how these typed entities aid in knowledge discovery and management.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1025–1028},
numpages = {4},
keywords = {massive text corpora, entity recognition and typing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891061,
author = {Sakurai, Yasushi and Matsubara, Yasuko and Faloutsos, Christos},
title = {Mining Big Time-Series Data on the Web},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891061},
doi = {10.1145/2872518.2891061},
abstract = {Online news, blogs, SNS and many other Web-based services has been attracting considerable interest for business and marketing purposes. Given a large collection of time series, such as web-click logs, online search queries, blog and review entries, how can we efficiently and effectively find typical time-series patterns? What are the major tools for mining, forecasting and outlier detection? Time-series data analysis is becoming of increasingly high importance, thanks to the decreasing cost of hardware and the increasing on-line processing capability.The objective of this tutorial is to provide a concise and intuitive overview of the most important tools that can help us find meaningful patterns in large-scale time-series data. Specifically we review the state of the art in three related fields: (1) similarity search, pattern discovery and summarization, (2) non-linear modeling and forecasting, and (3) the extension of time-series mining and tensor analysis. We also introduce case studies that illustrate their practical use for social media and Web-based services.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1029–1032},
numpages = {4},
keywords = {time-series, tensors, forecasting, pattern discovery},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891060,
author = {Sambra, Andrei and Guy, Amy and Capadisli, Sarven and Greco, Nicola},
title = {Building Decentralized Applications for the Social Web},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891060},
doi = {10.1145/2872518.2891060},
abstract = {Recent advancements in technologies and protocols mean that it is easier than ever to integrate social features into diverse web applications, and increased awareness of privacy concerns means that it is pertinent to consider empowerment of application users when doing so. Many developers are already familiar with the notion of personal data stores; this tutorial will demonstrate how to access or provide such stores for users, and build simple web applications which read and write to the storage whilst remaining completely decoupled from it. This advantages developers in two ways: by removing the burden of storing and maintaining a canonical copy of user data; and by enabling access to and ease of integration with data created through other applications, creating richer, seamless experiences. From the application users' perspective, they need no longer commit and become bound to particular services, but can mix, match and move between those that best meet their needs.We will introduce Solid, a set of protocols based on existing W3C recommendations, for reading, writing and access control of the contents of a personal data store, which can be layered up in order to integrate various social features into new or existing web applications. Attendees will leave with an understanding of Solid and how different parts of the protocols can work together, and having written some code to implement the parts that interest them most. They will also have hands on experience with existing libraries and tooling to facilitate working with the Solid protocols. Those who stay for the full day will have an opportunity to build a small but complete web application with decentralized social features, and to collaborate with others to see the advantages of sharing data between multiple applications.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1033–1034},
numpages = {2},
keywords = {social web, read-write web, decentralization, personal data stores, linked data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891064,
author = {Singer, Philipp and Lemmerich, Florian},
title = {Analyzing Sequential User Behavior on the Web},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891064},
doi = {10.1145/2872518.2891064},
abstract = {This tutorial aims at outlining fundamental methods for studying categorical sequences on the Web. Categorical sequences can refer to any kind of transitional data between a set of states, for example human navigation (transitions) between Web sites (states). Presented methods focus on sequential pattern mining, modeling and inference aiming at better understanding the production of sequences. A core model utilized in this tutorial is the Markov chain model. We hope that this tutorial raises interest and awareness of the field at hand and provides participants with basic tools for analyzing sequential user behavior on the Web.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1035–1036},
numpages = {2},
keywords = {markov chain, transitions, hypotheses, pattern mining, trails, modeling, sequential data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2891062,
author = {Weber, Ingmar and Wagner, Claudia and Strohmaier, Markus and Aiello, Luca Maria},
title = {Computational Social Science for the World Wide Web (CSSW3)},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891062},
doi = {10.1145/2872518.2891062},
abstract = {This tutorial aims at outlining fundamental methods for studying typical social science research questions with organic data (i.e., data that has not been designed for a specific research purpose but can be found on the Web). Further, social theories, statistical methods and models that help to understand the processes that generated the data will be discussed. Participants will learn (1) how to turn theoretical assumptions into models and test them, (2) how to validate measurements and (3) how to approximate causality when working with organic data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1037–1038},
numpages = {2},
keywords = {social theories, causality, networks, computational social science, research methodologies, measurements},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890599,
author = {Auer, S\"{o}ren and Heath, Tom and Bizer, Christian and Berners-Lee, Tim},
title = {LDOW2016: 9th Workshop on Linked Data on the Web},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890599},
doi = {10.1145/2872518.2890599},
abstract = {The ninth workshop on Linked Data (LDOW2016) on the Web is held in Montreal, Quebec, Canada on April 12, 2016 and co-located with the 25rd International World Wide Web Conference (WWW2016). The Web is developing from a medium for publishing textual documents into a medium for sharing structured data. This trend is fueled on the one hand by the adoption of the Linked Data principles by a growing number of data providers. On the other hand, large numbers of websites have started to semantically mark up the content of their HTML pages and thus also contribute to the wealth of structured data available on the Web. The 9th Workshop on Linked Data on the Web aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data from the Web as well as mining knowledge from the global Web of Data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1039–1040},
numpages = {2},
keywords = {rdf, semantic web, linked data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2893528,
author = {Cano, Amparo E. and Preotiuc-Pietro, Daniel and Radovanovi\'{c}, Danica and Weller, Katrin and Dadzie, Aba-Sah},
title = {#Microposts2016: 6th Workshop on Making Sense of Microposts: Big Things Come in Small Packages},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2893528},
doi = {10.1145/2872518.2893528},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1041–1042},
numpages = {2},
keywords = {microposts, user and data analysis, microblogs, computational social science, social media},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2872518.2890600,
author = {Gonzalez-Beltran, Alejandra and Osborne, Francesco and Peroni, Silvio},
title = {SAVE-SD 2016: Second Workshop on Semantics, Analytics and Visualisation: Enhancing Scholarly Data},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890600},
doi = {10.1145/2872518.2890600},
abstract = {The second edition of the Workshop on Semantics, Analytics and Visualisation: Enhancing Scholarly Data (SAVE-SD 2016) is held in Montreal, Canada on April 11, 2016 and co-located with the 25th International World Wide Web Conference. Its main goal is bringing together publishers, companies and researchers working on semantics, analytics and visualisation of scholarly data in order to bridge the gap between the theoretical/academic and practical/industrial aspects in this field.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1043–1044},
numpages = {2},
keywords = {semantic web, scholarly data, visualization, digital library, semantics, human computer interaction, analytics},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

