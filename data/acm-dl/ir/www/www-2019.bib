@inproceedings{10.1145/3308558.3313785,
author = {Dean, Jeff},
title = {Deep Learning for Solving Important Problems},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313785},
doi = {10.1145/3308558.3313785},
abstract = {In this keynote we describe progress in work that our research teams have been doing over the past years, including advances in difficult problems in artificial intelligence, on building large-scale computer systems for machine learning research, and, in collaboration with many teams at Google, on applying our research and systems to dozens of Google products. Our group has open-sourced the TensorFlow system [2], a widely popular system designed to easily express machine learning ideas, and to quickly train, evaluate and deploy machine learning systems. We then highlight some of our research accomplishments, and relate them to the National Academy of Engineering's Grand Engineering Challenges for the 21st Century.This is joint work with many people at Google.},
booktitle = {The World Wide Web Conference},
pages = {1},
numpages = {1},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313786,
author = {Lessig, Lawrence},
title = {The Law of the Horse at 20: Phases of the Net},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313786},
doi = {10.1145/3308558.3313786},
abstract = {The Internet moves in phases, and we are entering the third in 20 years. In this keynote, using a framework drawn from the Law of the Horse [1], I describe the phase we are entering - the surveillance phase - and the threat it presents to society generally, and democracy in particular. Along the way, I offer an understanding of the Net circa 1999, and the phase that followed it, circa 2009. At each stage, our inability to govern has been a significant liability. In the phase we are entering, it will be devastating. },
booktitle = {The World Wide Web Conference},
pages = {2},
numpages = {1},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313787,
author = {Wardle, Claire},
title = {Enlisting the Public to Build a Healthier Web Information Commons},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313787},
doi = {10.1145/3308558.3313787},
abstract = {Over the past three years, platforms, governments and a plethora of nonprofit initiatives have prioritized fighting online misinformation through a variety of different means. Yet the current framework is too fragmented to deliver global results. The big tech platforms have data, but no public accountability. Governments (mostly) have democratic legitimacy, but little information on what is actually going on in the platforms they're itching to regulate. And nonprofit initiatives too often lack the scale to affect change at the level needed. What if we came up with a dramatically new deliberative process that involves a global community of concerned citizens ready to share information and participate in consultations to improve collective decision-making? What if a more accountable, diverse and verifiable Web were still possible?},
booktitle = {The World Wide Web Conference},
pages = {3},
numpages = {1},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313697,
author = {Agarwal, Aman and Wang, Xuanhui and Li, Cheng and Bendersky, Michael and Najork, Marc},
title = {Addressing Trust Bias for Unbiased Learning-to-Rank},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313697},
doi = {10.1145/3308558.3313697},
abstract = {Existing unbiased learning-to-rank models use counterfactual inference, notably Inverse Propensity Scoring (IPS), to learn a ranking function from biased click data. They handle the click incompleteness bias, but usually assume that the clicks are noise-free, i.e., a clicked document is always assumed to be relevant. In this paper, we relax this unrealistic assumption and study click noise explicitly in the unbiased learning-to-rank setting. Specifically, we model the noise as the position-dependent trust bias and propose a noise-aware Position-Based Model, named TrustPBM, to better capture user click behavior. We propose an Expectation-Maximization algorithm to estimate both examination and trust bias from click data in TrustPBM. Furthermore, we show that it is difficult to use a pure IPS method to incorporate click noise and thus propose a novel method that combines a Bayes rule application with IPS for unbiased learning-to-rank. We evaluate our proposed methods on three personal search data sets and demonstrate that our proposed model can significantly outperform the existing unbiased learning-to-rank methods.},
booktitle = {The World Wide Web Conference},
pages = {4–14},
numpages = {11},
keywords = {trust bias, click noise, inverse propensity scoring, Unbiased learning-to-rank},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313695,
author = {Agrawal, Rakshit and de Alfaro, Luca},
title = {Learning Edge Properties in Graphs from Path Aggregations},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313695},
doi = {10.1145/3308558.3313695},
abstract = {Graph edges, along with their labels, can represent information of fundamental importance, such as links between web pages, friendship between users, the rating given by users to other users or items, and much more. We introduce LEAP, a trainable, general framework for predicting the presence and properties of edges on the basis of the local structure, topology, and labels of the graph. The LEAP framework is based on the exploration and machine-learning aggregation of the paths connecting nodes in a graph. We provide several methods for performing the aggregation phase by training path aggregators, and we demonstrate the flexibility and generality of the framework by applying it to the prediction of links and user ratings in social networks. We validate the LEAP framework on two problems: link prediction, and user rating prediction. On eight large datasets, among which the arXiv collaboration network, the Yeast protein-protein interaction, and the US airlines routes network, we show that the link prediction performance of LEAP is at least as good as the current state of the art methods, such as SEAL and WLNM. Next, we consider the problem of predicting user ratings on other users: this problem is known as the edge-weight prediction problem in weighted signed networks (WSN). On Bitcoin networks, and Wikipedia RfA, we show that LEAP performs consistently better than the Fairness &amp; Goodness based regression models, varying the amount of training edges between 10 to 90%. These examples demonstrate that LEAP, in spite of its generality, can match or best the performance of approaches that have been especially crafted to solve very specific edge prediction problems.},
booktitle = {The World Wide Web Conference},
pages = {15–25},
numpages = {11},
keywords = {Neural Networks, Edge Learning, Path Aggregation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313624,
author = {Alrashed, Tarfah and Lee, Chia-Jung and Bailey, Peter and Lin, Christopher and Shokouhi, Milad and Dumais, Susan},
title = {Evaluating User Actions as a Proxy for Email Significance},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313624},
doi = {10.1145/3308558.3313624},
abstract = {Email remains a critical channel for communicating information in both personal and work accounts. The number of emails people receive every day can be overwhelming, which in turn creates challenges for efficient information management and consumption. Having a good estimate of the significance of emails forms the foundation for many downstream tasks (e.g. email prioritization); but determining significance at scale is expensive and challenging. In this work, we hypothesize that the cumulative set of actions on any individual email can be considered as a proxy for the perceived significance of that email. We propose two approaches to summarize observed actions on emails, which we then evaluate against the perceived significance. The first approach is a fixed-form utility function parameterized on a set of weights, and we study the impact of different weight assignment strategies. In the second approach, we build machine learning models to capture users' significance directly based on the observed actions. For evaluation, we collect human judgments on email significance for both personal and work emails. Our analysis suggests that there is a positive correlation between actions and significance of emails and that actions performed on personal and work emails are different. We also find that the degree of correlation varies across people, which may reflect the individualized nature of email activity patterns or significance. Subsequently, we develop an example of real-time email significance prediction by using action summaries as implicit feedback at scale. Evaluation results suggest that the resulting significance predictions have positive agreement with human assessments, albeit not at statistically strong levels. We speculate that we may require personalized significance prediction to improve agreement levels. },
booktitle = {The World Wide Web Conference},
pages = {26–36},
numpages = {11},
keywords = {log data, user activity modelling, Email communication significance},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313668,
author = {Al-Rfou, Rami and Perozzi, Bryan and Zelle, Dustin},
title = {DDGK: Learning Graph Representations for Deep Divergence Graph Kernels},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313668},
doi = {10.1145/3308558.3313668},
abstract = {Can neural networks learn to compare graphs without feature engineering? In this paper, we show that it is possible to learn representations for graph similarity with neither domain knowledge nor supervision (i.e. feature engineering or labeled graphs). We propose Deep Divergence Graph Kernels, an unsupervised method for learning representations over graphs that encodes a relaxed notion of graph isomorphism. Our method consists of three parts. First, we learn an encoder for each anchor graph to capture its structure. Second, for each pair of graphs, we train a cross-graph attention network which uses the node representations of an anchor graph to reconstruct another graph. This approach, which we call isomorphism attention, captures how well the representations of one graph can encode another. We use the attention-augmented encoder's predictions to define a divergence score for each pair of graphs. Finally, we construct an embedding space for all graphs using these pair-wise divergence scores. Unlike previous work, much of which relies on 1) supervision, 2) domain specific knowledge (e.g. a reliance on Weisfeiler-Lehman kernels), and 3) known node alignment, our unsupervised method jointly learns node representations, graph representations, and an attention-based alignment between graphs. Our experimental results show that Deep Divergence Graph Kernels can learn an unsupervised alignment between graphs, and that the learned representations achieve competitive results when used as features on a number of challenging graph classification tasks. Furthermore, we illustrate how the learned attention allows insight into the the alignment of sub-structures across graphs.},
booktitle = {The World Wide Web Conference},
pages = {37–48},
numpages = {12},
keywords = {Graph Neural Networks, Graph Kernels, Similarity and Search, Representation Learning},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313504,
author = {Badjatiya, Pinkesh and Gupta, Manish and Varma, Vasudeva},
title = {Stereotypical Bias Removal for Hate Speech Detection Task Using Knowledge-Based Generalizations},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313504},
doi = {10.1145/3308558.3313504},
abstract = {With the ever-increasing cases of hate spread on social media platforms, it is critical to design abuse detection mechanisms to pro-actively avoid and control such incidents. While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. Bias removal has been traditionally studied for structured datasets, but we aim at bias mitigation from unstructured text data. In this paper, we make two important contributions. First, we systematically design methods to quantify the bias for any model and propose algorithms for identifying the set of words which the model stereotypes. Second, we propose novel methods leveraging knowledge-based generalizations for bias-free learning. Knowledge-based generalization provides an effective way to encode knowledge because the abstraction they provide not only generalizes content but also facilitates retraction of information from the hate speech detection classifier, thereby reducing the imbalance. We experiment with multiple knowledge generalization policies and analyze their effect on general performance and in mitigating bias. Our experiments with two real-world datasets, a Wikipedia Talk Pages dataset (WikiDetox) of size ~ 96k and a Twitter dataset of size ~ 24k, show that the use of knowledge-based generalizations results in better performance by forcing the classifier to learn from generalized content. Our methods utilize existing knowledge-bases and can easily be extended to other tasks. },
booktitle = {The World Wide Web Conference},
pages = {49–59},
numpages = {11},
keywords = {hate speech, stereotypical bias, knowledge-based generalization, bias detection, natural language processing, bias removal},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313568,
author = {Bai, Jinze and Zhou, Chang and Song, Junshuai and Qu, Xiaoru and An, Weiting and Li, Zhao and Gao, Jun},
title = {Personalized Bundle List Recommendation},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313568},
doi = {10.1145/3308558.3313568},
abstract = {Product bundling, offering a combination of items to customers, is one of the marketing strategies commonly used in online e-commerce and offline retailers. A high-quality bundle generalizes frequent items of interest, and diversity across bundles boosts the user-experience and eventually increases transaction volume. In this paper, we formalize the personalized bundle list recommendation as a structured prediction problem and propose a bundle generation network (BGN), which decomposes the problem into quality/diversity parts by the determinantal point processes (DPPs). BGN uses a typical encoder-decoder framework with a proposed feature-aware softmax to alleviate the inadequate representation of traditional softmax, and integrates the masked beam search and DPP selection to produce high-quality and diversified bundle list with an appropriate bundle size. We conduct extensive experiments on three public datasets and one industrial dataset, including two generated from co-purchase records and the other two extracted from real-world online bundle services. BGN significantly outperforms the state-of-the-art methods in terms of quality, diversity and response time over all datasets. In particular, BGN improves the precision of the best competitors by 16% on average while maintaining the highest diversity on four datasets, and yields a 3.85x improvement of response time over the best competitors in the bundle list recommendation problem.},
booktitle = {The World Wide Web Conference},
pages = {60–71},
numpages = {12},
keywords = {Bundle Recommendation, Diversity, Bundle Generation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313485,
author = {Bai, Tian and Vucetic, Slobodan},
title = {Improving Medical Code Prediction from Clinical Text via Incorporating Online Knowledge Sources},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313485},
doi = {10.1145/3308558.3313485},
abstract = {Clinical notes contain detailed information about health status of patients for each of their encounters with a health system. Developing effective models to automatically assign medical codes to clinical notes has been a long-standing active research area. Despite a great recent progress in medical informatics fueled by deep learning, it is still a challenge to find the specific piece of evidence in a clinical note which justifies a particular medical code out of all possible codes. Considering the large amount of online disease knowledge sources, which contain detailed information about signs and symptoms of different diseases, their risk factors, and epidemiology, there is an opportunity to exploit such sources. In this paper we consider Wikipedia as an external knowledge source and propose Knowledge Source Integration (KSI), a novel end-to-end code assignment framework, which can integrate external knowledge during training of any baseline deep learning model. The main idea of KSI is to calculate matching scores between a clinical note and disease related Wikipedia documents, and combine the scores with output of the baseline model. To evaluate KSI, we experimented with automatic assignment of ICD-9 diagnosis codes to the emergency department clinical notes from MIMIC-III data set, aided by Wikipedia documents corresponding to the ICD-9 codes. We evaluated several baseline models, ranging from logistic regression to recently proposed deep learning models known to achieve the state-of-the-art accuracy on clinical notes. The results show that KSI consistently improves the baseline models and that it is particularly successful in assignment of rare codes. In addition, by analyzing weights of KSI models, we can gain understanding about which words in Wikipedia documents provide useful information for predictions.},
booktitle = {The World Wide Web Conference},
pages = {72–82},
numpages = {11},
keywords = {document similarity learning, attention mechanism, Multi-label classification, healthcare},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313403,
author = {Yikun, Ban and Xin, Liu and Ling, Huang and Yitao, Duan and Xue, Liu and Wei, Xu},
title = {No Place to Hide: Catching Fraudulent Entities in Tensors},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313403},
doi = {10.1145/3308558.3313403},
abstract = {Many approaches focus on detecting dense blocks in the tensor of multimodal data to prevent fraudulent entities (e.g., accounts, links) from retweet boosting, hashtag hijacking, link advertising, etc. However, no existing method is effective to find the dense block if it only possesses high density on a subset of all dimensions in tensors. In this paper, we novelly identify dense-block detection with dense-subgraph mining, by modeling a tensor into a weighted graph without any density information lost. Based on the weighted graph, which we call information sharing graph (ISG), we propose an algorithm for finding multiple densest subgraphs, D-Spot, that is faster (up to 11x faster than the state-of-the-art algorithm) and can be computed in parallel. In an N-dimensional tensor, the entity group found by the ISG+D-Spot is at least 1/2 of the optimum with respect to density, compared with the 1/N guarantee ensured by competing methods. We use nine datasets to demonstrate that ISG+D-Spot becomes new state-of-the-art dense-block detection method in terms of accuracy specifically for fraud detection. },
booktitle = {The World Wide Web Conference},
pages = {83–93},
numpages = {11},
keywords = {Graph Algorithms, Dense-block Detection, Fraud Detection},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313626,
author = {Benson, Austin and Kleinberg, Jon},
title = {Link Prediction in Networks with Core-Fringe Data},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313626},
doi = {10.1145/3308558.3313626},
abstract = {Data collection often involves the partial measurement of a larger system. A common example arises in collecting network data: we often obtain network datasets by recording all of the interactions among a small set of core nodes, so that we end up with a measurement of the network consisting of these core nodes along with a potentially much larger set of fringe nodes that have links to the core. Given the ubiquity of this process for assembling network data, it is crucial to understand the role of such a “core-fringe” structure. Here we study how the inclusion of fringe nodes affects the standard task of network link prediction. One might initially think the inclusion of any additional data is useful, and hence that it should be beneficial to include all fringe nodes that are available. However, we find that this is not true; in fact, there is substantial variability in the value of the fringe nodes for prediction. Once an algorithm is selected, in some datasets, including any additional data from the fringe can actually hurt prediction performance; in other datasets, including some amount of fringe information is useful before prediction performance saturates or even declines; and in further cases, including the entire fringe leads to the best performance. While such variety might seem surprising, we show that these behaviors are exhibited by simple random graph models.},
booktitle = {The World Wide Web Conference},
pages = {94–104},
numpages = {11},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313687,
author = {Bernaschi, Massimo and Celestini, Alessandro and Guarino, Stefano and Lombardi, Flavio and Mastrostefano, Enrico},
title = {Spiders like Onions: On the Network of Tor Hidden Services},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313687},
doi = {10.1145/3308558.3313687},
abstract = {Tor hidden services allow offering and accessing various Internet resources while guaranteeing a high degree of provider and user anonymity. So far, most research work on the Tor network aimed at discovering protocol vulnerabilities to de-anonymize users and services. Other work aimed at estimating the number of available hidden services and classifying them. Something that still remains largely unknown is the structure of the graph defined by the network of Tor services. In this paper, we describe the topology of the Tor graph (aggregated at the hidden service level) measuring both global and local properties by means of well-known metrics. We consider three different snapshots obtained by extensively crawling Tor three times over a 5 months time frame. We separately study these three graphs and their shared “stable” core. In doing so, other than assessing the renowned volatility of Tor hidden services, we make it possible to distinguish time dependent and structural aspects of the Tor graph. Our findings show that, among other things, the graph of Tor hidden services presents some of the characteristics of social and surface web graphs, along with a few unique peculiarities, such as a very high percentage of nodes having no outbound links. },
booktitle = {The World Wide Web Conference},
pages = {105–115},
numpages = {11},
keywords = {Complex Networks, Dark Web, Web Graph, Tor},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313656,
author = {Bhowmik, Rajarshi and de Melo, Gerard},
title = {Be Concise and Precise: Synthesizing Open-Domain Entity Descriptions from Facts},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313656},
doi = {10.1145/3308558.3313656},
abstract = {Despite being vast repositories of factual information, cross-domain knowledge graphs, such as Wikidata and the Google Knowledge Graph, only sparsely provide short synoptic descriptions for entities. Such descriptions that briefly identify the most discernible features of an entity provide readers with a near-instantaneous understanding of what kind of entity they are being presented. They can also aid in tasks such as named entity disambiguation, ontological type determination, and answering entity queries. Given the rapidly increasing numbers of entities in knowledge graphs, a fully automated synthesis of succinct textual descriptions from underlying factual information is essential. To this end, we propose a novel fact-to-sequence encoder-decoder model with a suitable copy mechanism to generate concise and precise textual descriptions of entities. In an in-depth evaluation, we demonstrate that our method significantly outperforms state-of-the-art alternatives.},
booktitle = {The World Wide Web Conference},
pages = {116–126},
numpages = {11},
keywords = {open-domain factual knowledge, synoptic description generation, knowledge graphs},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313472,
author = {Bonifati, Angela and Martens, Wim and Timm, Thomas},
title = {Navigating the Maze of Wikidata Query Logs},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313472},
doi = {10.1145/3308558.3313472},
abstract = {This paper provides an in-depth and diversified analysis of the Wikidata query logs, recently made publicly available. Although the usage of Wikidata queries has been the object of recent studies, our analysis of the query traffic reveals interesting and unforeseen findings concerning the usage, types of recursion, and the shape classification of complex recursive queries. Wikidata specific features combined with recursion let us identify a significant subset of the entire corpus that can be used by the community for further assessment. We considered and analyzed the queries across many different dimensions, such as the robotic and organic queries, the presence/absence of constants along with the correctly executed and timed out queries. A further investigation that we pursue in this paper is to find, given a query, a number of queries structurally similar to the given query. We provide a thorough characterization of the queries in terms of their expressive power, their topological structure and shape, along with a deeper understanding of the usage of recursion in these logs. We make the code for the analysis available as open source.},
booktitle = {The World Wide Web Conference},
pages = {127–138},
numpages = {12},
keywords = {Query Similarity Search, Knowledge Graph, SPARQL endpoint, Query Shapes, Query Log Analysis},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313721,
author = {Budak, Ceren},
title = {What Happened? The Spread of Fake News Publisher Content During the 2016 U.S. Presidential Election},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313721},
doi = {10.1145/3308558.3313721},
abstract = {The spread of content produced by fake news publishers was one of the most discussed characteristics of the 2016 U.S. Presidential Election. Yet, little is known about the prevalence and focus of such content, how its prevalence changed over time, and how this prevalence related to important election dynamics. In this paper, we address these questions using tweets that mention the two presidential candidates sampled at the daily level, the news content mentioned in such tweets, and open-ended responses from nationally representative telephone interviews. The results of our analysis highlight various important lessons for news consumers and journalists. We find that (i.) traditional news producers outperformed fake news producers in aggregate, (ii.) the prevalence of content produced by fake news publishers increased over the course of the campaign-particularly among tweets that mentioned Clinton, and (iii.) changes in such prevalence were closely following changes in net Clinton favorability. Turning to content, we (iv.) identify similarities and differences in agenda setting by fake and traditional news media and show that (v.) information individuals most commonly reported to having read, seen or heard about the candidates was more closely aligned with content produced by fake news outlets than traditional news outlets, in particular for information Republican voters retained about Clinton. We also model fake-ness of retained information as a function of demographics characteristics. Implications for platform owners, news consumers, and journalists are discussed.},
booktitle = {The World Wide Web Conference},
pages = {139–150},
numpages = {12},
keywords = {topic modeling, news media, multi-level regression, fake news},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313705,
author = {Cao, Yixin and Wang, Xiang and He, Xiangnan and Hu, Zikun and Chua, Tat-Seng},
title = {Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313705},
doi = {10.1145/3308558.3313705},
abstract = {Incorporating knowledge graph (KG) into recommender system is promising in improving the recommendation accuracy and explainability. However, existing methods largely assume that a KG is complete and simply transfer the ”knowledge” in KG at the shallow level of entity raw data or embeddings. This may lead to suboptimal performance, since a practical KG can hardly be complete, and it is common that a KG has missing facts, relations, and entities. Thus, we argue that it is crucial to consider the incomplete nature of KG when incorporating it into recommender system. In this paper, we jointly learn the model of recommendation and knowledge graph completion. Distinct from previous KG-based recommendation methods, we transfer the relation information in KG, so as to understand the reasons that a user likes an item. As an example, if a user has watched several movies directed by (relation) the same person (entity), we can infer that the director relation plays a critical role when the user makes the decision, thus help to understand the user's preference at a finer granularity. Technically, we contribute a new translation-based recommendation model, which specially accounts for various preferences in translating a user to an item, and then jointly train it with a KG completion model by combining several transfer schemes. Extensive experiments on two benchmark datasets show that our method outperforms state-of-the-art KG-based recommendation methods. Further analysis verifies the positive effect of joint training on both tasks of recommendation and KG completion, and the advantage of our model in understanding user preference. We publish our project at https://github.com/TaoMiner/joint-kg-recommender.},
booktitle = {The World Wide Web Conference},
pages = {151–161},
numpages = {11},
keywords = {Knowledge Graph, Embedding, Joint Model, Item Recommendation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313588,
author = {Carmel, David and Fyodorov, Yaroslav and Kuzi, Saar and Mejer, Avihai and Raiber, Fiana and Rainshmidt, Elad},
title = {Enriching News Articles with Related Search Queries},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313588},
doi = {10.1145/3308558.3313588},
abstract = {Enriching the content of news articles with auxiliary resources is a technique often employed by online news services to keep articles up-to-date and thereby increase users' engagement. We address the task of enriching news articles with related search queries, which are extracted from a search engine's query log. Clicking on a recommended query invokes a search session that allows the user to further explore content related to the article. We present a three-phase retrieval framework for query recommendation that incorporates various article-dependent and article-independent relevance signals. Evaluation based on an offline experiment, performed using annotations by professional editors, and a large-scale online experiment, conducted with real users, demonstrates the merits of our approach. In addition, a comprehensive analysis of our online experiment reveals interesting characteristics of the type of queries users tend to click and the nature of their interaction with the resultant search engine results page.},
booktitle = {The World Wide Web Conference},
pages = {162–172},
numpages = {11},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313509,
author = {Chan, T-H. Hubert and Liang, Zhibin and Sozio, Mauro},
title = {Revisiting Opinion Dynamics with Varying Susceptibility to Persuasion via Non-Convex Local Search},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313509},
doi = {10.1145/3308558.3313509},
abstract = {We revisit the opinion susceptibility problem that was proposed by Abebe et al. [1], in which agents influence one another's opinions through an iterative process. Each agent has some fixed innate opinion. In each step, the opinion of an agent is updated to some convex combination between its innate opinion and the weighted average of its neighbors' opinions in the previous step. The resistance of an agent measures the importance it places on its innate opinion in the above convex combination. Under non-trivial conditions, this iterative process converges to some equilibrium opinion vector. For the unbudgeted variant of the problem, the goal is to select the resistance of each agent (from some given range) such that the sum of the equilibrium opinions is minimized. Contrary to the claim in the aforementioned KDD 2018 paper, the objective function is in general non-convex. Hence, formulating the problem as a convex program might have potential correctness issues. We instead analyze the structure of the objective function, and show that any local optimum is also a global optimum, which is somehow surprising as the objective function might not be convex. Furthermore, we combine the iterative process and the local search paradigm to design very efficient algorithms that can solve the unbudgeted variant of the problem optimally on large-scale graphs containing millions of nodes.},
booktitle = {The World Wide Web Conference},
pages = {173–183},
numpages = {11},
keywords = {non-convex local search, susceptibility to persuasion, opinion dynamics},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313638,
author = {Chang, Jonathan and Danescu-Niculescu-Mizil, Cristian},
title = {Trajectories of Blocked Community Members: Redemption, Recidivism and Departure},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313638},
doi = {10.1145/3308558.3313638},
abstract = {Community norm violations can impair constructive communication and collaboration online. As a defense mechanism, community moderators often address such transgressions by temporarily blocking the perpetrator. Such actions, however, come with the cost of potentially alienating community members. Given this tradeoff, it is essential to understand to what extent, and in which situations, this common moderation practice is effective in reinforcing community rules. In this work, we introduce a computational framework for studying the future behavior of blocked users on Wikipedia. After their block expires, they can take several distinct paths: they can reform and adhere to the rules, but they can also recidivate, or straight-out abandon the community. We reveal that these trajectories are tied to factors rooted both in the characteristics of the blocked individual and in whether they perceived the block to be fair and justified. Based on these insights, we formulate a series of prediction tasks aiming to determine which of these paths a user is likely to take after being blocked for their first offense, and demonstrate the feasibility of these new tasks. Overall, this work builds towards a more nuanced approach to moderation by highlighting the tradeoffs that are in play. },
booktitle = {The World Wide Web Conference},
pages = {184–195},
numpages = {12},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313692,
author = {Xavier Ferreira, Matheus and Weinberg, S. Matthew and Huang, Danny Yuxing and Feamster, Nick and Chattopadhyay, Tithi},
title = {Selling a Single Item with Negative Externalities},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313692},
doi = {10.1145/3308558.3313692},
abstract = {We consider the problem of regulating products with negative externalities to a third party that is neither the buyer nor the seller, but where both the buyer and seller can take steps to mitigate the externality. The motivating example to have in mind is the sale of Internet-of-Things (IoT) devices, many of which have historically been compromised for DDoS attacks that disrupted Internet-wide services such as Twitter [5, 26]. Neither the buyer (i.e., consumers) nor seller (i.e., IoT manufacturers) was known to suffer from the attack, but both have the power to expend effort to secure their devices. We consider a regulator who regulates payments (via fines if the device is compromised, or market prices directly), or the product directly via mandatory security requirements. Both regulations come at a cost-implementing security requirements increases production costs, and the existence of fines decreases consumers' values-thereby reducing the seller's profits. The focus of this paper is to understand the efficiency of various regulatory policies. That is, policy A is more efficient than policy B if A more successfully minimizes negatives externalities, while both A and B reduce seller's profits equally. We develop a simple model to capture the impact of regulatory policies on a buyer's behavior. In this model, we show that for homogeneous markets-where the buyer's ability to follow security practices is always high or always low-the optimal (externality-minimizing for a given profit constraint) regulatory policy need regulate only payments or production. In arbitrary markets, by contrast, we show that while the optimal policy may require regulating both aspects, there is always an approximately optimal policy which regulates just one.},
booktitle = {The World Wide Web Conference},
pages = {196–206},
numpages = {11},
keywords = {Auction Design, Tragedy of the Commons, Mechanism Design and Approximation, Negative Externalities, Policy and Regulation.},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313549,
author = {Chen, Gong and Meng, Wei and Copeland, John},
title = {Revisiting Mobile Advertising Threats with MAdLife},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313549},
doi = {10.1145/3308558.3313549},
abstract = {Online advertising is one of the primary funding sources for various of content, services, and applications on both web and mobile platforms. Mobile in-app advertising reuses many existing web technologies under the same ad-serving model (i.e., users - publishers - ad networks - advertisers). Nevertheless, mobile in-app advertising is different from the traditional web advertising in many aspects. For example, malicious app developers can generate fraudulent ad clicks in an automated fashion, but malicious web publishers have to launch click fraud with bots. In spite of using the same underlying web infrastructure, advertising threats behave differently on the two platforms.Existing works have studied separately click fraud and malvertising in the mobile setting. However, it is unknown if there exists a relationship between these two dominant threats. In this paper, we present an ad collection framework – MAdLife – on Android to capture all the in-app ad traffic generated during an ad's entire lifespan. MAdLife allows us to revisit both threats in a fine-grained manner and study the relationship between them. It further enables the exploration of other threats related to ad landing pages.We analyzed 5.7K Android apps crawled from the Google Play Store, and collected 83K ads and their landing pages using MAdLife. Similar to traditional web ads, 58K ads landed on web pages. We discovered 37 click-fraud apps, and found that 1.49% of the 58K ads were malicious. We also revealed a strong correlation between fraudulent apps and malicious ads. Specifically, 15.44% of malicious ads originated from the fraudulent apps. Conversely, 18.36% of the ads served in the fraudulent apps were malicious, while only 1.28% were malicious in the rest apps. This suggests that users of fraudulent apps are much more (14x) likely to encounter malicious ads. Additionally, we discovered that 243 popular JavaScript snippets embedded by over 10% of the landing pages were malicious. Finally, we conducted the first analysis on inappropriate mobile in-app ads. },
booktitle = {The World Wide Web Conference},
pages = {207–217},
numpages = {11},
keywords = {Online Advertising, Ad Fraud, Measurement, Malvertising, Mobile Apps},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313476,
author = {Chen, Huiyuan and Li, Jing},
title = {Modeling Relational Drug-Target-Disease Interactions via Tensor Factorization with Multiple Web Sources},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313476},
doi = {10.1145/3308558.3313476},
abstract = {Modeling the behaviors of drug-target-disease interactions is crucial in the early stage of drug discovery and holds great promise for precision medicine and personalized treatments. The growing availability of new types of data on the internet brings great opportunity of learning a more comprehensive relationship among drugs, targets, and diseases. However, existing methods often consider drug-target interactions or drug-disease interactions separately, which ignores the dependencies among these three entities. Also, many of them cannot directly incorporate rich heterogeneous information from diverse sources. In this work, we investigate the utility of tensor factorization to model the relationships of drug-target-disease, specifically leveraging different types of online data. Our motivation is two-fold. First, in human metabolic systems, many drugs interact with protein targets in cells to modulate target activities, which in turn alter biological pathways to promote healthy functions and to treat diseases. Instead of binary relationships of <drug, disease=""> or <drug, target="">, a tighter triple relationships <drug, target,="" disease=""> should be exploited to better understand drug mechanism of actions (MoAs). Second, medical data could be collected from different sources (i.e., drug's chemical structure, target's sequence, or expression measurements). Therefore, effectively exploiting the complementarity among multiple sources is of great importance. Our method elegantly explores a <drug, target,="" disease=""> tensor together with complementarity among different data sources, thus improves prediction accuracy. We achieve this goal by formulating the problem into a coupled tensor-matrix factorization problem and directly optimize it on the nonlinear manifold. Experimental results on real-world datasets show that the proposed model outperforms several competitive methods. Our model opens up opportunities to use large Web data to predict drugs' MoAs in pharmacological studies.},
booktitle = {The World Wide Web Conference},
pages = {218–227},
numpages = {10},
keywords = {Manifold optimization, Drug discovery, Disease analysis;, Multi-view learning, Tensor factorization, Grassmann manifold},
location = {San Francisco, CA, USA},
series = {WWW '19}
}</drug,></drug,></drug,></drug,>

@inproceedings{10.1145/3308558.3313582,
author = {Chen, Jiawei and Wang, Can and Zhou, Sheng and Shi, Qihao and Feng, Yan and Chen, Chun},
title = {SamWalker: Social Recommendation with Informative Sampling Strategy},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313582},
doi = {10.1145/3308558.3313582},
abstract = {Recommendation from implicit feedback is a highly challenging task due to the lack of reliable negative feedback data. Only positive feedback are observed and the unobserved feedback can be attributed to two reasons: unknow or dislike. Existing methods address this challenge by treating all the un-observed data as negative (dislike) but downweight the confidence of these data. However, this treatment causes two problems: (1) Confidence weights of the unobserved data are usually assigned manually, which lack flexible and may create empirical bias in evaluating user's preference. (2) To handle massive volume of the unobserved feedback data, most of the existing methods rely on stochastic inference and data sampling strategies. However, since users are only aware of a very small fraction of items in a large dataset, it is difficult for existing samplers to select informative training instances in which the user really dislikes the item rather than does not know it. To address the above two problems, we propose a new recommendation method SamWalker that leverages social information to infer data confidence and guide the sampling process. By modeling data confidence with a social context-aware function, SamWalker can adaptively specify different weights to different data based on users' social contexts. Further, a personalized random-walk-based sampling strategy is developed to adaptively draw informative training instances, which can speed up gradient estimation and reduce sampling variance. Extensive experiments on three real-world datasets demonstrate the superiority of the proposed SamWalker method and its sampling strategy. },
booktitle = {The World Wide Web Conference},
pages = {228–239},
numpages = {12},
keywords = {Social recommendation, Implicit feedback, Sampling},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313469,
author = {Chen, Li and Yang, Yonghua and Wang, Ningxia and Yang, Keping and Yuan, Quan},
title = {How Serendipity Improves User Satisfaction with Recommendations? A Large-Scale User Evaluation},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313469},
doi = {10.1145/3308558.3313469},
abstract = {Recommendation serendipity is being increasingly recognized as being equally important as the other beyond-accuracy objectives (such as novelty and diversity), in eliminating the “filter bubble” phenomenon of the traditional recommender systems. However, little work has empirically verified the effects of serendipity on increasing user satisfaction and behavioral intention. In this paper, we report the results of a large-scale user survey (involving over 3,000 users) conducted in an industrial mobile e-commerce setting. The study has identified the significant causal relationships from novelty, unexpectedness, relevance, and timeliness to serendipity, and from serendipity to user satisfaction and purchase intention. Moreover, our findings reveal that user curiosity plays a moderating role in strengthening the relationships from novelty to serendipity and from serendipity to satisfaction. Our third contribution lies in the comparison of several recommender algorithms, which demonstrates the significant improvements of the serendipity-oriented algorithm over the relevance- and novelty-oriented approaches in terms of user perceptions. We finally discuss the implications of this experiment, which include the feasibility of developing a more precise metric for measuring recommendation serendipity, and the potential benefit of a curiosity-based personalized serendipity strategy for recommender systems.},
booktitle = {The World Wide Web Conference},
pages = {240–250},
numpages = {11},
keywords = {curiosity, Recommender systems, user satisfaction, large-scale user evaluation, serendipity},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313600,
author = {Chen, Zhenpeng and Shen, Sheng and Hu, Ziniu and Lu, Xuan and Mei, Qiaozhu and Liu, Xuanzhe},
title = {Emoji-Powered Representation Learning for Cross-Lingual Sentiment Classification},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313600},
doi = {10.1145/3308558.3313600},
abstract = {Sentiment classification typically relies on a large amount of labeled data. In practice, the availability of labels is highly imbalanced among different languages, e.g., more English texts are labeled than texts in any other languages, which creates a considerable inequality in the quality of related information services received by users speaking different languages. To tackle this problem, cross-lingual sentiment classification approaches aim to transfer knowledge learned from one language that has abundant labeled examples (i.e., the source language, usually English) to another language with fewer labels (i.e., the target language). The source and the target languages are usually bridged through off-the-shelf machine translation tools. Through such a channel, cross-language sentiment patterns can be successfully learned from English and transferred into the target languages. This approach, however, often fails to capture sentiment knowledge specific to the target language, and thus compromises the accuracy of the downstream classification task. In this paper, we employ emojis, which are widely available in many languages, as a new channel to learn both the cross-language and the language-specific sentiment patterns. We propose a novel representation learning method that uses emoji prediction as an instrument to learn respective sentiment-aware representations for each language. The learned representations are then integrated to facilitate cross-lingual sentiment classification. The proposed method demonstrates state-of-the-art performance on benchmark datasets, which is sustained even when sentiment labels are scarce. },
booktitle = {The World Wide Web Conference},
pages = {251–262},
numpages = {12},
keywords = {sentiment classification, cross-lingual analysis, Emoji},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313748,
author = {Chin, Alex and Chen, Yatong and M. Altenburger, Kristen and Ugander, Johan},
title = {Decoupled Smoothing on Graphs},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313748},
doi = {10.1145/3308558.3313748},
abstract = {Graph smoothing methods are an extremely popular family of approaches for semi-supervised learning. The choice of graph used to represent relationships in these learning problems is often a more important decision than the particular algorithm or loss function used, yet this choice is less well-studied in the literature. In this work, we demonstrate that for social networks, the basic friendship graph itself may often not be the appropriate graph for predicting node attributes using graph smoothing. More specifically, standard graph smoothing is designed to harness the social phenomenon of homophily whereby individuals are similar to “the company they keep.” We present a decoupled approach to graph smoothing that decouples notions of “identity” and “preference,” resulting in an alternative social phenomenon of monophily whereby individuals are similar to “the company they're kept in,” as observed in recent empirical work. Our model results in a rigorous extension of the Gaussian Markov Random Field (GMRF) models that underlie graph smoothing, interpretable as smoothing on an appropriate auxiliary graph of weighted or unweighted two-hop relationships.},
booktitle = {The World Wide Web Conference},
pages = {263–272},
numpages = {10},
keywords = {graph smoothing, Semi-supervised learning, attribute prediction},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313499,
author = {Chu, Xiaokai and Fan, Xinxin and Yao, Di and Zhu, Zhihua and Huang, Jianhui and Bi, Jingping},
title = {Cross-Network Embedding for Multi-Network Alignment},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313499},
doi = {10.1145/3308558.3313499},
abstract = {Recently, data mining through analyzing the complex structure and diverse relationships on multi-network has attracted much attention in both academia and industry. One crucial prerequisite for this kind of multi-network mining is to map the nodes across different networks, i.e., so-called network alignment. In this paper, we propose a cross-network embedding method CrossMNA for multi-network alignment problem through investigating structural information only. Unlike previous methods focusing on pair-wise learning and holding the topology consistent assumption, our proposed CrossMNA considers the multi-network scenarios which involve at least two types of networks with diverse network structures. CrossMNA leverages the cross-network information to refine two types of node embedding vectors, i.e., inter-vector for network alignment and intra-vector for other downstream network analysis tasks. Finally, we verify the effectiveness and efficiency of our proposed method using several real-world datasets. The extensive experiments show that our CrossMNA can significantly outperform the existing baseline methods on multi-network alignment task, and also achieve better performance for link prediction task with less memory usage.},
booktitle = {The World Wide Web Conference},
pages = {273–284},
numpages = {12},
keywords = {network mining, multi-network alignment, node representation, network embedding},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313452,
author = {Coey, Dominic and Cunningham, Tom},
title = {Improving Treatment Effect Estimators Through Experiment Splitting},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313452},
doi = {10.1145/3308558.3313452},
abstract = {We present a method for implementing shrinkage of treatment effect estimators, and hence improving their precision, via experiment splitting. Experiment splitting reduces shrinkage to a standard prediction problem. The method makes minimal distributional assumptions, and allows for the degree of shrinkage in one metric to depend on other metrics. Using a dataset of 226 Facebook News Feed A/B tests, we show that a lasso estimator based on repeated experiment splitting has a 44% lower mean squared predictive error than the conventional, unshrunk treatment effect estimator, a 18% lower mean squared predictive error than the James-Stein shrinkage estimator, and would lead to substantially improved launch decisions over both.},
booktitle = {The World Wide Web Conference},
pages = {285–295},
numpages = {11},
keywords = {Empirical Bayes Shrinkage, Experiment Meta-Analysis., Sample Splitting, A/B Tests, Causal Inference},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313712,
author = {Cui, Hang and Abdelzaher, Tarek and Kaplan, Lance},
title = {A Semi-Supervised Active-Learning Truth Estimator for Social Networks},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313712},
doi = {10.1145/3308558.3313712},
abstract = {This paper introduces an active-learning-based truth estimator for social networks, such as Twitter, that enhances estimation accuracy significantly by requesting a well-selected (small) fraction of data to be labeled. Data assessment and truth discovery from arbitrary open online sources are a hard problem due to uncertainty regarding source reliability. Multiple truth finding systems were developed to solve this problem. Their accuracy is limited by the noisy nature of the data, where distortions, fabrications, omissions, and duplication are introduced. This paper presents a semi-supervised truth estimator for social networks, in which a portion of inputs are carefully selected to be reliably verified. The challenge is to find the subset of observations to verify that would maximally enhance the overall fact-finding accuracy. This work extends previous passive approaches to recursive truth estimation, as well as semi-supervised approaches where the estimator has no control over the choice of data to be labeled. Results show that by optimally selecting claims to be verified, we improve estimated accuracy by 12% over unsupervised baseline, and by 5% over previous semi-supervised approaches.},
booktitle = {The World Wide Web Conference},
pages = {296–306},
numpages = {11},
keywords = {Social Sensing, Semi Supervision, Maximum Likelihood Estimation, Active Learning, Truth Discovery},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

