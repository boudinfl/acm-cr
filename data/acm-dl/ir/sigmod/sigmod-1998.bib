@inproceedings{10.1145/276304.276306,
author = {Tsur, Dick and Ullman, Jeffrey D. and Abiteboul, Serge and Clifton, Chris and Motwani, Rajeev and Nestorov, Svetlozar and Rosenthal, Arnon},
title = {Query Flocks: A Generalization of Association-Rule Mining},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276306},
doi = {10.1145/276304.276306},
abstract = {Association-rule mining has proved a highly successful technique for extracting useful information from very large databases. This success is attributed not only to the appropriateness of the objectives, but to the fact that a number of new query-optimization ideas, such as the “a-priori” trick, make association-rule mining run much faster than might be expected. In this paper we see that the same tricks can be extended to a much more general context, allowing efficient mining of very large databases for many different kinds of patterns. The general idea, called “query flocks,” is a generate-and-test model for data-mining problems. We show how the idea can be used either in a general-purpose mining system or in a next generation of conventional query optimizers.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276306,
author = {Tsur, Dick and Ullman, Jeffrey D. and Abiteboul, Serge and Clifton, Chris and Motwani, Rajeev and Nestorov, Svetlozar and Rosenthal, Arnon},
title = {Query Flocks: A Generalization of Association-Rule Mining},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276306},
doi = {10.1145/276305.276306},
abstract = {Association-rule mining has proved a highly successful technique for extracting useful information from very large databases. This success is attributed not only to the appropriateness of the objectives, but to the fact that a number of new query-optimization ideas, such as the “a-priori” trick, make association-rule mining run much faster than might be expected. In this paper we see that the same tricks can be extended to a much more general context, allowing efficient mining of very large databases for many different kinds of patterns. The general idea, called “query flocks,” is a generate-and-test model for data-mining problems. We show how the idea can be used either in a general-purpose mining system or in a next generation of conventional query optimizers.},
journal = {SIGMOD Rec.},
month = jun,
pages = {1–12},
numpages = {12}
}

@inproceedings{10.1145/276304.276307,
author = {Ng, Raymond T. and Lakshmanan, Laks V. S. and Han, Jiawei and Pang, Alex},
title = {Exploratory Mining and Pruning Optimizations of Constrained Associations Rules},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276307},
doi = {10.1145/276304.276307},
abstract = {From the standpoint of supporting human-centered discovery of knowledge, the present-day model of mining association rules suffers from the following serious shortcomings: (i) lack of user exploration and control, (ii) lack of focus, and (iii) rigid notion of relationships. In effect, this model functions as a black-box, admitting little user interaction in between. We propose, in this paper, an architecture that opens up the black-box, and supports constraint-based, human-centered exploratory mining of associations. The foundation of this architecture is a rich set of constraint constructs, including domain, class, and SQL-style aggregate constraints, which enable users to clearly specify what associations are to be mined. We propose constrained association queries as a means of specifying the constraints to be satisfied by the antecedent and consequent of a mined association.In this paper, we mainly focus on the technical challenges in guaranteeing a level of performance that is commensurate with the selectivities of the constraints in an association query. To this end, we introduce and analyze two properties of constraints that are critical to pruning: anti-monotonicity and succinctness. We then develop characterizations of various constraints into four categories, according to these properties. Finally, we describe a mining algorithm called CAP, which achieves a maximized degree of pruning for all categories of constraints. Experimental results indicate that CAP can run much faster, in some cases as much as 80 times, than several basic algorithms. This demonstrates how important the succinctness and anti-monotonicity properties are, in delivering the performance guarantee.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {13–24},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276307,
author = {Ng, Raymond T. and Lakshmanan, Laks V. S. and Han, Jiawei and Pang, Alex},
title = {Exploratory Mining and Pruning Optimizations of Constrained Associations Rules},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276307},
doi = {10.1145/276305.276307},
abstract = {From the standpoint of supporting human-centered discovery of knowledge, the present-day model of mining association rules suffers from the following serious shortcomings: (i) lack of user exploration and control, (ii) lack of focus, and (iii) rigid notion of relationships. In effect, this model functions as a black-box, admitting little user interaction in between. We propose, in this paper, an architecture that opens up the black-box, and supports constraint-based, human-centered exploratory mining of associations. The foundation of this architecture is a rich set of constraint constructs, including domain, class, and SQL-style aggregate constraints, which enable users to clearly specify what associations are to be mined. We propose constrained association queries as a means of specifying the constraints to be satisfied by the antecedent and consequent of a mined association.In this paper, we mainly focus on the technical challenges in guaranteeing a level of performance that is commensurate with the selectivities of the constraints in an association query. To this end, we introduce and analyze two properties of constraints that are critical to pruning: anti-monotonicity and succinctness. We then develop characterizations of various constraints into four categories, according to these properties. Finally, we describe a mining algorithm called CAP, which achieves a maximized degree of pruning for all categories of constraints. Experimental results indicate that CAP can run much faster, in some cases as much as 80 times, than several basic algorithms. This demonstrates how important the succinctness and anti-monotonicity properties are, in delivering the performance guarantee.},
journal = {SIGMOD Rec.},
month = jun,
pages = {13–24},
numpages = {12}
}

@inproceedings{10.1145/276304.276308,
author = {Shintani, Takahiko and Kitsuregawa, Masaru},
title = {Parallel Mining Algorithms for Generalized Association Rules with Classification Hierarchy},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276308},
doi = {10.1145/276304.276308},
abstract = {Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.In this paper, we propose the new parallel algorithms for mining association rules with classification hierarchy on a shared-nothing parallel machine to improve its performance. Our algorithms partition the candidate itemsets over the processors, which exploits the aggregate memory of the system effectively. If the candidate itemsets are partitioned without considering classification hierarchy, both the items and its all the ancestor items have to be transmitted, that causes prohibitively large amount of communications. Our method minimizes interprocessor communication by considering the hierarchy. Moreover, in our algorithm, the available memory space is fully utilized by identifying the frequently occurring candidate itemsets and copying them over all the processors, through which frequent itemsets can be processed locally without any communication. Thus it can effectively reduce the load skew among the processors. Several experiments are done by changing the granule of copying itemsets, from the whole tree, to the small group of the frequent itemsets along the hierarchy. The coarser the grain, the easier the control but it is rather difficult to achieve the sufficient load balance. The finer the grain, the more complicated the control is required but it can balance the load quite well.We implemented proposed algorithms on IBM SP-2. Performance evaluations show that our algorithms are effective for handling skew and attain sufficient speedup ratio.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {25–36},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276308,
author = {Shintani, Takahiko and Kitsuregawa, Masaru},
title = {Parallel Mining Algorithms for Generalized Association Rules with Classification Hierarchy},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276308},
doi = {10.1145/276305.276308},
abstract = {Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.In this paper, we propose the new parallel algorithms for mining association rules with classification hierarchy on a shared-nothing parallel machine to improve its performance. Our algorithms partition the candidate itemsets over the processors, which exploits the aggregate memory of the system effectively. If the candidate itemsets are partitioned without considering classification hierarchy, both the items and its all the ancestor items have to be transmitted, that causes prohibitively large amount of communications. Our method minimizes interprocessor communication by considering the hierarchy. Moreover, in our algorithm, the available memory space is fully utilized by identifying the frequently occurring candidate itemsets and copying them over all the processors, through which frequent itemsets can be processed locally without any communication. Thus it can effectively reduce the load skew among the processors. Several experiments are done by changing the granule of copying itemsets, from the whole tree, to the small group of the frequent itemsets along the hierarchy. The coarser the grain, the easier the control but it is rather difficult to achieve the sufficient load balance. The finer the grain, the more complicated the control is required but it can balance the load quite well.We implemented proposed algorithms on IBM SP-2. Performance evaluations show that our algorithms are effective for handling skew and attain sufficient speedup ratio.},
journal = {SIGMOD Rec.},
month = jun,
pages = {25–36},
numpages = {12}
}

@inproceedings{10.1145/276304.276309,
author = {Rao, Jun and Ross, Kenneth A.},
title = {Reusing Invariants: A New Strategy for Correlated Queries},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276309},
doi = {10.1145/276304.276309},
abstract = {Correlated queries are very common and important in decision support systems. Traditional nested iteration evaluation methods for such queries can be very time consuming. When they apply, query rewriting techniques have been shown to be much more efficient. But query rewriting is not always possible. When query rewriting does not apply, can we do something better than the traditional nested iteration methods? In this paper, we propose a new invariant technique to evaluate correlated queries efficiently. The basic idea is to recognize the part of the subquery that is not related to the outer references and cache the result of that part after its first execution. Later, we can reuse the result and combine it with the result of the rest of the subquery that is changing for each iteration. Our technique applies to arbitrary correlated subqueries.This paper introduces algorithms to recognize the invariant part of a data flow tree, and to restructure the evaluation plan to reuse the stored intermediate result. We also propose an efficient method to teach an existing join optimizer to understand the invariant feature and thus allow it to be able to generate better join plans in the new context. Some other related optimization techniques are also discussed. The proposed techniques were implemented within three months on an existing real commercial database system.We also experimentally evaluate our proposed technique. Our evaluation indicates that, when query rewriting is not possible, the invariant technique is significantly better than the traditional nested iteration method. Even when query rewriting applies, the invariant technique is sometimes better than the query rewriting technique. Our conclusion is that the invariant technique should be considered as one of the alternatives in evaluating correlated queries since it fills the gap left by rewriting techniques.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {37–48},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276309,
author = {Rao, Jun and Ross, Kenneth A.},
title = {Reusing Invariants: A New Strategy for Correlated Queries},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276309},
doi = {10.1145/276305.276309},
abstract = {Correlated queries are very common and important in decision support systems. Traditional nested iteration evaluation methods for such queries can be very time consuming. When they apply, query rewriting techniques have been shown to be much more efficient. But query rewriting is not always possible. When query rewriting does not apply, can we do something better than the traditional nested iteration methods? In this paper, we propose a new invariant technique to evaluate correlated queries efficiently. The basic idea is to recognize the part of the subquery that is not related to the outer references and cache the result of that part after its first execution. Later, we can reuse the result and combine it with the result of the rest of the subquery that is changing for each iteration. Our technique applies to arbitrary correlated subqueries.This paper introduces algorithms to recognize the invariant part of a data flow tree, and to restructure the evaluation plan to reuse the stored intermediate result. We also propose an efficient method to teach an existing join optimizer to understand the invariant feature and thus allow it to be able to generate better join plans in the new context. Some other related optimization techniques are also discussed. The proposed techniques were implemented within three months on an existing real commercial database system.We also experimentally evaluate our proposed technique. Our evaluation indicates that, when query rewriting is not possible, the invariant technique is significantly better than the traditional nested iteration method. Even when query rewriting applies, the invariant technique is sometimes better than the query rewriting technique. Our conclusion is that the invariant technique should be considered as one of the alternatives in evaluating correlated queries since it fills the gap left by rewriting techniques.},
journal = {SIGMOD Rec.},
month = jun,
pages = {37–48},
numpages = {12}
}

@inproceedings{10.1145/276304.276310,
author = {Fegaras, Leonidas},
title = {Query Unnesting in Object-Oriented Databases},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276310},
doi = {10.1145/276304.276310},
abstract = {There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.This paper presents a new query unnesting algorithm that generalizes many unnesting techniques proposed recently in the literature. Our system is capable of removing any form of query nesting using a very simple and efficient algorithm. The simplicity of the system is due to the use of the monoid comprehension calculus as an intermediate form for OODB queries. The monoid comphrehension calculus treats operations over multiple collection types, aggregates, and quantifiers in a similar way, resulting in a uniform way of unnesting queries, regardless of their type of nesting.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {49–60},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276310,
author = {Fegaras, Leonidas},
title = {Query Unnesting in Object-Oriented Databases},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276310},
doi = {10.1145/276305.276310},
abstract = {There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.This paper presents a new query unnesting algorithm that generalizes many unnesting techniques proposed recently in the literature. Our system is capable of removing any form of query nesting using a very simple and efficient algorithm. The simplicity of the system is due to the use of the monoid comprehension calculus as an intermediate form for OODB queries. The monoid comphrehension calculus treats operations over multiple collection types, aggregates, and quantifiers in a similar way, resulting in a uniform way of unnesting queries, regardless of their type of nesting.},
journal = {SIGMOD Rec.},
month = jun,
pages = {49–60},
numpages = {12}
}

@inproceedings{10.1145/276304.276311,
author = {Cherniack, Mitch and Zdonik, Stan},
title = {Changing the Rules: Transformations for Rule-Based Optimizers},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276311},
doi = {10.1145/276304.276311},
abstract = {Rule-based optimizers are extensible because they consist of modifiable sets of rules. For modification to be straightforward, rules must be easily reasoned about (i.e., understood and verified). At the same time, rules must be expressive and efficient (to fire) for rule-based optimizers to be practical. Production-style rules (as in [15]) are expressed with code and are hard to reason about. Pure rewrite rules (as in [1]) lack code, but cannot atomically express complex transformations (e.g., normalizations). Some systems allow rules to be grouped, but sacrifice efficiency by providing limited control over their firing. Therefore, none of these approaches succeeds in making rules expressive, efficient and understandable.We propose a language (COKO) for expressing an alternative form of input to a rule-based optimizer. A COKO transformation consists of a set of declarative (KOLA) rewrite rules and a (firing) algorithm that specifies their firing. It is straightforward to reason about COKO transformations because all query modification is expressed with declarative rewrite rules. Firing is specified algorithmically with an expressive language that provides direct control over how query representations are traversed, and under what conditions rules are fired. Therefore, COKO achieves a delicate balance of understandability, efficiency and expressivity.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {61–72},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276311,
author = {Cherniack, Mitch and Zdonik, Stan},
title = {Changing the Rules: Transformations for Rule-Based Optimizers},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276311},
doi = {10.1145/276305.276311},
abstract = {Rule-based optimizers are extensible because they consist of modifiable sets of rules. For modification to be straightforward, rules must be easily reasoned about (i.e., understood and verified). At the same time, rules must be expressive and efficient (to fire) for rule-based optimizers to be practical. Production-style rules (as in [15]) are expressed with code and are hard to reason about. Pure rewrite rules (as in [1]) lack code, but cannot atomically express complex transformations (e.g., normalizations). Some systems allow rules to be grouped, but sacrifice efficiency by providing limited control over their firing. Therefore, none of these approaches succeeds in making rules expressive, efficient and understandable.We propose a language (COKO) for expressing an alternative form of input to a rule-based optimizer. A COKO transformation consists of a set of declarative (KOLA) rewrite rules and a (firing) algorithm that specifies their firing. It is straightforward to reason about COKO transformations because all query modification is expressed with declarative rewrite rules. Firing is specified algorithmically with an expressive language that provides direct control over how query representations are traversed, and under what conditions rules are fired. Therefore, COKO achieves a delicate balance of understandability, efficiency and expressivity.},
journal = {SIGMOD Rec.},
month = jun,
pages = {61–72},
numpages = {12}
}

@inproceedings{10.1145/276304.276312,
author = {Guha, Sudipto and Rastogi, Rajeev and Shim, Kyuseok},
title = {CURE: An Efficient Clustering Algorithm for Large Databases},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276312},
doi = {10.1145/276304.276312},
abstract = {Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers. To handle large databases, CURE employs a combination of random sampling and partitioning. A random sample drawn from the data set is first partitioned and each partition is partially clustered. The partial clusters are then clustered in a second pass to yield the desired clusters. Our experimental results confirm that the quality of clusters produced by CURE is much better than those found by existing algorithms. Furthermore, they demonstrate that random sampling and partitioning enable CURE to not only outperform existing algorithms but also to scale well for large databases without sacrificing clustering quality.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {73–84},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276312,
author = {Guha, Sudipto and Rastogi, Rajeev and Shim, Kyuseok},
title = {CURE: An Efficient Clustering Algorithm for Large Databases},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276312},
doi = {10.1145/276305.276312},
abstract = {Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers. To handle large databases, CURE employs a combination of random sampling and partitioning. A random sample drawn from the data set is first partitioned and each partition is partially clustered. The partial clusters are then clustered in a second pass to yield the desired clusters. Our experimental results confirm that the quality of clusters produced by CURE is much better than those found by existing algorithms. Furthermore, they demonstrate that random sampling and partitioning enable CURE to not only outperform existing algorithms but also to scale well for large databases without sacrificing clustering quality.},
journal = {SIGMOD Rec.},
month = jun,
pages = {73–84},
numpages = {12}
}

@inproceedings{10.1145/276304.276313,
author = {Bayardo, Roberto J.},
title = {Efficiently Mining Long Patterns from Databases},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276313},
doi = {10.1145/276304.276313},
abstract = {We present a pattern-mining algorithm that scales roughly linearly in the number of maximal patterns embedded in a database irrespective of the length of the longest pattern. In comparison, previous algorithms based on Apriori scale exponentially with longest pattern length. Experiments on real data show that when the patterns are long, our algorithm is more efficient by an order of magnitude or more.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {85–93},
numpages = {9},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276313,
author = {Bayardo, Roberto J.},
title = {Efficiently Mining Long Patterns from Databases},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276313},
doi = {10.1145/276305.276313},
abstract = {We present a pattern-mining algorithm that scales roughly linearly in the number of maximal patterns embedded in a database irrespective of the length of the longest pattern. In comparison, previous algorithms based on Apriori scale exponentially with longest pattern length. Experiments on real data show that when the patterns are long, our algorithm is more efficient by an order of magnitude or more.},
journal = {SIGMOD Rec.},
month = jun,
pages = {85–93},
numpages = {9}
}

@inproceedings{10.1145/276304.276314,
author = {Agrawal, Rakesh and Gehrke, Johannes and Gunopulos, Dimitrios and Raghavan, Prabhakar},
title = {Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276314},
doi = {10.1145/276304.276314},
abstract = {Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate cluster in large high dimensional datasets.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {94–105},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276314,
author = {Agrawal, Rakesh and Gehrke, Johannes and Gunopulos, Dimitrios and Raghavan, Prabhakar},
title = {Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276314},
doi = {10.1145/276305.276314},
abstract = {Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate cluster in large high dimensional datasets.},
journal = {SIGMOD Rec.},
month = jun,
pages = {94–105},
numpages = {12}
}

@inproceedings{10.1145/276304.276315,
author = {Kabra, Navin and DeWitt, David J.},
title = {Efficient Mid-Query Re-Optimization of Sub-Optimal Query Execution Plans},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276315},
doi = {10.1145/276304.276315},
abstract = {For a number of reasons, even the best query optimizers can very often produce sub-optimal query execution plans, leading to a significant degradation of performance. This is especially true in databases used for complex decision support queries and/or object-relational databases. In this paper, we describe an algorithm that detects sub-optimality of a query execution plan during query execution and attempts to correct the problem. The basic idea is to collect statistics at key points during the execution of a complex query. These statistics are then used to optimize the execution of the query, either by improving the resource allocation for that query, or by changing the execution plan for the remainder of the query. To ensure that this does not significantly slow down the normal execution of a query, the Query Optimizer carefully chooses what statistics to collect, when to collect them, and the circumstances under which to re-optimize the query. We describe an implementation of this algorithm in the Paradise Database System, and we report on performance studies, which indicate that this can result in significant improvements in the performance of complex queries.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {106–117},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276315,
author = {Kabra, Navin and DeWitt, David J.},
title = {Efficient Mid-Query Re-Optimization of Sub-Optimal Query Execution Plans},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276315},
doi = {10.1145/276305.276315},
abstract = {For a number of reasons, even the best query optimizers can very often produce sub-optimal query execution plans, leading to a significant degradation of performance. This is especially true in databases used for complex decision support queries and/or object-relational databases. In this paper, we describe an algorithm that detects sub-optimality of a query execution plan during query execution and attempts to correct the problem. The basic idea is to collect statistics at key points during the execution of a complex query. These statistics are then used to optimize the execution of the query, either by improving the resource allocation for that query, or by changing the execution plan for the remainder of the query. To ensure that this does not significantly slow down the normal execution of a query, the Query Optimizer carefully chooses what statistics to collect, when to collect them, and the circumstances under which to re-optimize the query. We describe an implementation of this algorithm in the Paradise Database System, and we report on performance studies, which indicate that this can result in significant improvements in the performance of complex queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {106–117},
numpages = {12}
}

@inproceedings{10.1145/276304.276316,
author = {J\'{o}nsson, Bj\"{o}rn T. and Franklin, Michael J. and Srivastava, Divesh},
title = {Interaction of Query Evaluation and Buffer Management for Information Retrieval},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276316},
doi = {10.1145/276304.276316},
abstract = {The proliferation of the World Wide Web has brought information retrieval (IR) techniques to the forefront of search technology. To the average computer user, “searching” now means using IR-based systems for finding information on the WWW or in other document collections. IR query evaluation methods and workloads differ significantly from those found in database systems. In this paper, we focus on three such differences. First, due to the inherent fuzziness of the natural language used in IR queries and documents, an additional degree of flexibility is permitted in evaluating queries. Second, IR query evaluation algorithms tend to have access patterns that cause problems for traditional buffer replacement policies. Third, IR search is often an iterative process, in which a query is repeatedly refined and resubmitted by the user. Based on these differences, we develop two complementary techniques to improve the efficiency of IR queries: 1) Buffer-aware query evaluation, which alters the query evaluation process based on the current contents of buffers; and 2) Ranking-aware buffer replacement, which incorporates knowledge of the query processing strategy into replacement decisions. In a detailed performance study we show that using either of these techniques yields significant performance benefits and that in many cases, combining them produces even further improvements.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {118–129},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276316,
author = {J\'{o}nsson, Bj\"{o}rn T. and Franklin, Michael J. and Srivastava, Divesh},
title = {Interaction of Query Evaluation and Buffer Management for Information Retrieval},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276316},
doi = {10.1145/276305.276316},
abstract = {The proliferation of the World Wide Web has brought information retrieval (IR) techniques to the forefront of search technology. To the average computer user, “searching” now means using IR-based systems for finding information on the WWW or in other document collections. IR query evaluation methods and workloads differ significantly from those found in database systems. In this paper, we focus on three such differences. First, due to the inherent fuzziness of the natural language used in IR queries and documents, an additional degree of flexibility is permitted in evaluating queries. Second, IR query evaluation algorithms tend to have access patterns that cause problems for traditional buffer replacement policies. Third, IR search is often an iterative process, in which a query is repeatedly refined and resubmitted by the user. Based on these differences, we develop two complementary techniques to improve the efficiency of IR queries: 1) Buffer-aware query evaluation, which alters the query evaluation process based on the current contents of buffers; and 2) Ranking-aware buffer replacement, which incorporates knowledge of the query processing strategy into replacement decisions. In a detailed performance study we show that using either of these techniques yields significant performance benefits and that in many cases, combining them produces even further improvements.},
journal = {SIGMOD Rec.},
month = jun,
pages = {118–129},
numpages = {12}
}

@inproceedings{10.1145/276304.276317,
author = {Urhan, Tolga and Franklin, Michael J. and Amsaleg, Laurent},
title = {Cost-Based Query Scrambling for Initial Delays},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276317},
doi = {10.1145/276304.276317},
abstract = {Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scrambling. These approaches vary, for example, in whether they optimize for total work or response-time, and whether they construct partial or complete alternative plans. Using a two-phase randomized query optimizer, a distributed query processing simulator, and a workload derived from queries of the TPCD benchmark, we evaluate these different approaches and compare their ability to cope with initial delays in accessing remote sources. The results show that cost-based scrambling can effectively hide initial delays, but that in the absence of good predictions of expected delay durations, there are fundamental tradeoffs between risk aversion and effectiveness.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {130–141},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276317,
author = {Urhan, Tolga and Franklin, Michael J. and Amsaleg, Laurent},
title = {Cost-Based Query Scrambling for Initial Delays},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276317},
doi = {10.1145/276305.276317},
abstract = {Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scrambling. These approaches vary, for example, in whether they optimize for total work or response-time, and whether they construct partial or complete alternative plans. Using a two-phase randomized query optimizer, a distributed query processing simulator, and a workload derived from queries of the TPCD benchmark, we evaluate these different approaches and compare their ability to cope with initial delays in accessing remote sources. The results show that cost-based scrambling can effectively hide initial delays, but that in the absence of good predictions of expected delay durations, there are fundamental tradeoffs between risk aversion and effectiveness.},
journal = {SIGMOD Rec.},
month = jun,
pages = {130–141},
numpages = {12}
}

@inproceedings{10.1145/276304.276318,
author = {Berchtold, Stefan and B\"{o}hm, Christian and Kriegal, Hans-Peter},
title = {The Pyramid-Technique: Towards Breaking the Curse of Dimensionality},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276318},
doi = {10.1145/276304.276318},
abstract = {In this paper, we propose the Pyramid-Technique, a new indexing method for high-dimensional data spaces. The Pyramid-Technique is highly adapted to range query processing using the maximum metric Lmax. In contrast to all other index structures, the performance of the Pyramid-Technique does not deteriorate when processing range queries on data of higher dimensionality. The Pyramid-Technique is based on a special partitioning strategy which is optimized for high-dimensional data. The basic idea is to divide the data space first into 2d pyramids sharing the center point of the space as a top. In a second step, the single pyramids are cut into slices parallel to the basis of the pyramid. These slices from the data pages. Furthermore, we show that this partition provides a mapping from the given d-dimensional space to a 1-dimensional space. Therefore, we are able to use a B+-tree to manage the transformed data. As an analytical evaluation of our technique for hypercube range queries and uniform data distribution shows, the Pyramid-Technique clearly outperforms index structures using other partitioning strategies. To demonstrate the practical relevance of our technique, we experimentally compared the Pyramid-Technique with the X-tree, the Hilbert R-tree, and the Linear Scan. The results of our experiments using both, synthetic and real data, demonstrate that the Pyramid-Technique outperforms the X-tree and the Hilbert R-tree by a factor of up to 14 (number of page accesses) and up to 2500 (total elapsed time) for range queries.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {142–153},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276318,
author = {Berchtold, Stefan and B\"{o}hm, Christian and Kriegal, Hans-Peter},
title = {The Pyramid-Technique: Towards Breaking the Curse of Dimensionality},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276318},
doi = {10.1145/276305.276318},
abstract = {In this paper, we propose the Pyramid-Technique, a new indexing method for high-dimensional data spaces. The Pyramid-Technique is highly adapted to range query processing using the maximum metric Lmax. In contrast to all other index structures, the performance of the Pyramid-Technique does not deteriorate when processing range queries on data of higher dimensionality. The Pyramid-Technique is based on a special partitioning strategy which is optimized for high-dimensional data. The basic idea is to divide the data space first into 2d pyramids sharing the center point of the space as a top. In a second step, the single pyramids are cut into slices parallel to the basis of the pyramid. These slices from the data pages. Furthermore, we show that this partition provides a mapping from the given d-dimensional space to a 1-dimensional space. Therefore, we are able to use a B+-tree to manage the transformed data. As an analytical evaluation of our technique for hypercube range queries and uniform data distribution shows, the Pyramid-Technique clearly outperforms index structures using other partitioning strategies. To demonstrate the practical relevance of our technique, we experimentally compared the Pyramid-Technique with the X-tree, the Hilbert R-tree, and the Linear Scan. The results of our experiments using both, synthetic and real data, demonstrate that the Pyramid-Technique outperforms the X-tree and the Hilbert R-tree by a factor of up to 14 (number of page accesses) and up to 2500 (total elapsed time) for range queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {142–153},
numpages = {12}
}

@inproceedings{10.1145/276304.276319,
author = {Seidl, Thomas and Kriegel, Hans-Peter},
title = {Optimal Multi-Step k-Nearest Neighbor Search},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276319},
doi = {10.1145/276304.276319},
abstract = {For an increasing number of modern database applications, efficient support of similarity search becomes an important task. Along with the complexity of the objects such as images, molecules and mechanical parts, also the complexity of the similarity models increases more and more. Whereas algorithms that are directly based on indexes work well for simple medium-dimensional similarity distance functions, they do not meet the efficiency requirements of complex high-dimensional and adaptable distance functions. The use of a multi-step query processing strategy is recommended in these cases, and our investigations substantiate that the number of candidates which are produced in the filter step and exactly evaluated in the refinement step is a fundamental efficiency parameter. After revealing the strong performance shortcomings of the state-of-the-art algorithm for k-nearest neighbor search [Korn et al. 1996], we present a novel multi-step algorithm which is guaranteed to produce the minimum number of candidates. Experimental evaluations demonstrate the significant performance gain over the previous solution, and we observed average improvement factors of up to 120 for the number of candidates and up to 48 for the total runtime.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {154–165},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276319,
author = {Seidl, Thomas and Kriegel, Hans-Peter},
title = {Optimal Multi-Step k-Nearest Neighbor Search},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276319},
doi = {10.1145/276305.276319},
abstract = {For an increasing number of modern database applications, efficient support of similarity search becomes an important task. Along with the complexity of the objects such as images, molecules and mechanical parts, also the complexity of the similarity models increases more and more. Whereas algorithms that are directly based on indexes work well for simple medium-dimensional similarity distance functions, they do not meet the efficiency requirements of complex high-dimensional and adaptable distance functions. The use of a multi-step query processing strategy is recommended in these cases, and our investigations substantiate that the number of candidates which are produced in the filter step and exactly evaluated in the refinement step is a fundamental efficiency parameter. After revealing the strong performance shortcomings of the state-of-the-art algorithm for k-nearest neighbor search [Korn et al. 1996], we present a novel multi-step algorithm which is guaranteed to produce the minimum number of candidates. Experimental evaluations demonstrate the significant performance gain over the previous solution, and we observed average improvement factors of up to 120 for the number of candidates and up to 48 for the total runtime.},
journal = {SIGMOD Rec.},
month = jun,
pages = {154–165},
numpages = {12}
}

@inproceedings{10.1145/276304.276320,
author = {Ravi Kanth, K. V. and Agrawal, Divyakant and Singh, Ambuj},
title = {Dimensionality Reduction for Similarity Searching in Dynamic Databases},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276320},
doi = {10.1145/276304.276320},
abstract = {Databases are increasingly being used to store multi-media objects such as maps, images, audio and video. Storage and retrieval of these objects is accomplished using multi-dimensional index structures such as R*-trees and SS-trees. As dimensionality increases, query performance in these index structures degrades. This phenomenon, generally referred to as the dimensionality curse, can be circumvented by reducing the dimensionality of the data. Such a reduction is however accompanied by a loss of precision of query results. Current techniques such as QBIC use SVD transform-based dimensionality reduction to ensure high query precision. The drawback of this approach is that SVD is expensive to compute, and therefore not readily applicable to dynamic databases. In this paper, we propose novel techniques for performing SVD-based dimensionality reduction in dynamic databases. When the data distribution changes considerably so as to degrade query precision, we recompute the SVD transform and incorporate it in the existing index structure. For recomputing the SVD-transform, we propose a novel technique that uses aggregate data from the existing index rather than the entire data. This technique reduces the SVD-computation time without compromising query precision. We then explore efficient ways to incorporate the recomputed SVD-transform in the existing index structure without degrading subsequent query response times. These techniques reduce the computation time by a factor of 20 in experiments on color and texture image vectors. The error due to approximate computation of SVD is less than 10%.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {166–176},
numpages = {11},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276320,
author = {Ravi Kanth, K. V. and Agrawal, Divyakant and Singh, Ambuj},
title = {Dimensionality Reduction for Similarity Searching in Dynamic Databases},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276320},
doi = {10.1145/276305.276320},
abstract = {Databases are increasingly being used to store multi-media objects such as maps, images, audio and video. Storage and retrieval of these objects is accomplished using multi-dimensional index structures such as R*-trees and SS-trees. As dimensionality increases, query performance in these index structures degrades. This phenomenon, generally referred to as the dimensionality curse, can be circumvented by reducing the dimensionality of the data. Such a reduction is however accompanied by a loss of precision of query results. Current techniques such as QBIC use SVD transform-based dimensionality reduction to ensure high query precision. The drawback of this approach is that SVD is expensive to compute, and therefore not readily applicable to dynamic databases. In this paper, we propose novel techniques for performing SVD-based dimensionality reduction in dynamic databases. When the data distribution changes considerably so as to degrade query precision, we recompute the SVD transform and incorporate it in the existing index structure. For recomputing the SVD-transform, we propose a novel technique that uses aggregate data from the existing index rather than the entire data. This technique reduces the SVD-computation time without compromising query precision. We then explore efficient ways to incorporate the recomputed SVD-transform in the existing index structure without degrading subsequent query response times. These techniques reduce the computation time by a factor of 20 in experiments on color and texture image vectors. The error due to approximate computation of SVD is less than 10%.},
journal = {SIGMOD Rec.},
month = jun,
pages = {166–176},
numpages = {11}
}

@inproceedings{10.1145/276304.276321,
author = {Cluet, Sophie and Delobel, Claude and Sim\'{e}on, J\'{e}rundefinedme and Smaga, Katarzyna},
title = {Your Mediators Need Data Conversion!},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276321},
doi = {10.1145/276304.276321},
abstract = {Due to the development of the World Wide Web, the integration of heterogeneous data sources has become a major concern of the database community. Appropriate architectures and query languages have been proposed. Yet, the problem of data conversion which is essential for the development of mediators/wrappers architectures has remained largely unexplored.In this paper, we present the YAT system for data conversion. This system provides tools for the specification and the implementation of data conversions among heterogeneous data sources. It relies on a middleware model, a declarative language, a customization mechanism and a graphical interface.The model is based on named trees with ordered and labeled nodes. Like semistructured data models, it is simple enough to facilitate the representation of any data. Its main originality is that it allows to reason at various levels of representation. The YAT conversion language (called YATL) is declarative, rule-based and features enhanced pattern matching facilities and powerful restructuring primitives. It allows to preserve or reconstruct the order of collections. The customization mechanism relies on program instantiations: an existing program may be instantiated into a more specific one, and then easily modified. We also present the architecture, implementation and practical use of the YAT prototype, currently under evaluation within the OPAL* project.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {177–188},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276321,
author = {Cluet, Sophie and Delobel, Claude and Sim\'{e}on, J\'{e}rundefinedme and Smaga, Katarzyna},
title = {Your Mediators Need Data Conversion!},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276321},
doi = {10.1145/276305.276321},
abstract = {Due to the development of the World Wide Web, the integration of heterogeneous data sources has become a major concern of the database community. Appropriate architectures and query languages have been proposed. Yet, the problem of data conversion which is essential for the development of mediators/wrappers architectures has remained largely unexplored.In this paper, we present the YAT system for data conversion. This system provides tools for the specification and the implementation of data conversions among heterogeneous data sources. It relies on a middleware model, a declarative language, a customization mechanism and a graphical interface.The model is based on named trees with ordered and labeled nodes. Like semistructured data models, it is simple enough to facilitate the representation of any data. Its main originality is that it allows to reason at various levels of representation. The YAT conversion language (called YATL) is declarative, rule-based and features enhanced pattern matching facilities and powerful restructuring primitives. It allows to preserve or reconstruct the order of collections. The customization mechanism relies on program instantiations: an existing program may be instantiated into a more specific one, and then easily modified. We also present the architecture, implementation and practical use of the YAT prototype, currently under evaluation within the OPAL* project.},
journal = {SIGMOD Rec.},
month = jun,
pages = {177–188},
numpages = {12}
}

@inproceedings{10.1145/276304.276322,
author = {Miller, Re\'{e}e J.},
title = {Using Schematically Heterogeneous Structures},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276322},
doi = {10.1145/276304.276322},
abstract = {Schematic heterogeneity arises when information that is represented as data under one schema, is represented within the schema (as metadata) in another. Schematic heterogeneity is an important class of heterogeneity that arises frequently in integrating legacy data in federated or data warehousing applications. Traditional query languages and view mechanisms are insufficient for reconciling and translating data between schematically heterogeneous schemas. Higher order query languages, that permit quantification over schema labels, have been proposed to permit querying and restructuring of data between schematically disparate schemas. We extend this work by considering how these languages can be used in practice. Specifically, we consider a restricted class of higher order views and show the power of these views in integrating legacy structures. Our results provide insights into the properties of restructuring transformations required to resolve schematic discrepancies. In addition, we show how the use of these views permits schema browsing and new forms of data independence that are important for global information systems. Furthermore, these views provide a framework for integrating semi-structured and unstructured queries, such as keyword searches, into a structured querying environment. We show how these views can be used with minimal extensions to existing query engines. We give conditions under which a higher order view is usable for answering a query and provide query translation algorithms.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {189–200},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276322,
author = {Miller, Re\'{e}e J.},
title = {Using Schematically Heterogeneous Structures},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276322},
doi = {10.1145/276305.276322},
abstract = {Schematic heterogeneity arises when information that is represented as data under one schema, is represented within the schema (as metadata) in another. Schematic heterogeneity is an important class of heterogeneity that arises frequently in integrating legacy data in federated or data warehousing applications. Traditional query languages and view mechanisms are insufficient for reconciling and translating data between schematically heterogeneous schemas. Higher order query languages, that permit quantification over schema labels, have been proposed to permit querying and restructuring of data between schematically disparate schemas. We extend this work by considering how these languages can be used in practice. Specifically, we consider a restricted class of higher order views and show the power of these views in integrating legacy structures. Our results provide insights into the properties of restructuring transformations required to resolve schematic discrepancies. In addition, we show how the use of these views permits schema browsing and new forms of data independence that are important for global information systems. Furthermore, these views provide a framework for integrating semi-structured and unstructured queries, such as keyword searches, into a structured querying environment. We show how these views can be used with minimal extensions to existing query engines. We give conditions under which a higher order view is usable for answering a query and provide query translation algorithms.},
journal = {SIGMOD Rec.},
month = jun,
pages = {189–200},
numpages = {12}
}

@inproceedings{10.1145/276304.276323,
author = {Cohen, William W.},
title = {Integration of Heterogeneous Databases without Common Domains Using Queries Based on Textual Similarity},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276323},
doi = {10.1145/276304.276323},
abstract = {Most databases contain “name constants” like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user's query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIRL is much faster than naive inference methods, even for short queries. We also show that inferences made by WHIRL are surprisingly accurate, equaling the accuracy of hand-coded normalization routines on one benchmark problem, and outperforming exact matching with a plausible global domain on a second.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {201–212},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276323,
author = {Cohen, William W.},
title = {Integration of Heterogeneous Databases without Common Domains Using Queries Based on Textual Similarity},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276323},
doi = {10.1145/276305.276323},
abstract = {Most databases contain “name constants” like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user's query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIRL is much faster than naive inference methods, even for short queries. We also show that inferences made by WHIRL are surprisingly accurate, equaling the accuracy of hand-coded normalization routines on one benchmark problem, and outperforming exact matching with a plausible global domain on a second.},
journal = {SIGMOD Rec.},
month = jun,
pages = {201–212},
numpages = {12}
}

@inproceedings{10.1145/276304.276324,
author = {Grumbach, St\'{e}phane and Rigaux, Philippe and Segoufin, Luc},
title = {The DEDALE System for Complex Spatial Queries},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276324},
doi = {10.1145/276304.276324},
abstract = {This paper presents DEDALE, a spatial database system intended to overcome some limitations of current systems by providing an abstract and non-specialized data model and query language for the representation and manipulation of spatial objects. DEDALE relies on a logical model based on linear constraints, which generalizes the constraint database model of [KKR90]. While in the classical constraint model, spatial data is always decomposed into its convex components, in DEDALE holes are allowed to fit the need of practical applications. The logical representation of spatial data although slightly more costly in memory, has the advantage of simplifying the algorithms. DEDALE relies on nested relations, in which all sorts of data (thematic, spatial, etc.) are stored in a uniform fashion. This new data model supports declarative query languages, which allow an intuitive and efficient manipulation of spatial objects. Their formal foundation constitutes a basis for practical query optimization. We describe several evaluation rules tailored for geometric data and give the specification of an optimizer module for spatial queries. Except for the latter module, the system has been fully implemented upon the O2 DBMS, thus proving the effectiveness of a constraint-based approach for the design of spatial database systems.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {213–224},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276324,
author = {Grumbach, St\'{e}phane and Rigaux, Philippe and Segoufin, Luc},
title = {The DEDALE System for Complex Spatial Queries},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276324},
doi = {10.1145/276305.276324},
abstract = {This paper presents DEDALE, a spatial database system intended to overcome some limitations of current systems by providing an abstract and non-specialized data model and query language for the representation and manipulation of spatial objects. DEDALE relies on a logical model based on linear constraints, which generalizes the constraint database model of [KKR90]. While in the classical constraint model, spatial data is always decomposed into its convex components, in DEDALE holes are allowed to fit the need of practical applications. The logical representation of spatial data although slightly more costly in memory, has the advantage of simplifying the algorithms. DEDALE relies on nested relations, in which all sorts of data (thematic, spatial, etc.) are stored in a uniform fashion. This new data model supports declarative query languages, which allow an intuitive and efficient manipulation of spatial objects. Their formal foundation constitutes a basis for practical query optimization. We describe several evaluation rules tailored for geometric data and give the specification of an optimizer module for spatial queries. Except for the latter module, the system has been fully implemented upon the O2 DBMS, thus proving the effectiveness of a constraint-based approach for the design of spatial database systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {213–224},
numpages = {12}
}

@inproceedings{10.1145/276304.276325,
author = {Papadopoulos, Apostolos N. and Manolopoulos, Yannis},
title = {Similarity Query Processing Using Disk Arrays},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276325},
doi = {10.1145/276304.276325},
abstract = {Similarity queries are fundamental operations that are used extensively in many modern applications, whereas disk arrays are powerful storage media of increasing importance. The basic trade-off in similarity query processing in such a system is that increased parallelism leads to higher resource consumptions and low throughput, whereas low parallelism leads to higher response times. Here, we propose a technique which is based on a careful investigation of the currently available data in order to exploit parallelism up to a point, retaining low response times during query processing. The underlying access method is a variation of the R*-tree, which is distributed among the components of a disk array, whereas the system is simulated using event-driven simulation. The performance results conducted, demonstrate that the proposed approach outperforms by factors a previous branch-and-bound algorithm and a greedy algorithm which maximizes parallelism as much as possible. Moreover, the comparison of the proposed algorithm to a hypothetical (non-existing) optimal one (with respect to the number of disk accesses) shows that the former is on average two times slower than the latter.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {225–236},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276325,
author = {Papadopoulos, Apostolos N. and Manolopoulos, Yannis},
title = {Similarity Query Processing Using Disk Arrays},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276325},
doi = {10.1145/276305.276325},
abstract = {Similarity queries are fundamental operations that are used extensively in many modern applications, whereas disk arrays are powerful storage media of increasing importance. The basic trade-off in similarity query processing in such a system is that increased parallelism leads to higher resource consumptions and low throughput, whereas low parallelism leads to higher response times. Here, we propose a technique which is based on a careful investigation of the currently available data in order to exploit parallelism up to a point, retaining low response times during query processing. The underlying access method is a variation of the R*-tree, which is distributed among the components of a disk array, whereas the system is simulated using event-driven simulation. The performance results conducted, demonstrate that the proposed approach outperforms by factors a previous branch-and-bound algorithm and a greedy algorithm which maximizes parallelism as much as possible. Moreover, the comparison of the proposed algorithm to a hypothetical (non-existing) optimal one (with respect to the number of disk accesses) shows that the former is on average two times slower than the latter.},
journal = {SIGMOD Rec.},
month = jun,
pages = {225–236},
numpages = {12}
}

@inproceedings{10.1145/276304.276326,
author = {Hjaltason, G\'{\i}sli R. and Samet, Hanan},
title = {Incremental Distance Join Algorithms for Spatial Databases},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276326},
doi = {10.1145/276304.276326},
abstract = {Two new spatial join operations, distance join and distance semi-join, are introduced where the join output is ordered by the distance between the spatial attribute values of the joined tuples. Incremental algorithms are presented for computing these operations, which can be used in a pipelined fashion, thereby obviating the need to wait for their completion when only a few tuples are needed. The algorithms can be used with a large class of hierarchical spatial data structures and arbitrary spatial data types in any dimensions. In addition, any distance metric may be employed. A performance study using R-trees shows that the incremental algorithms outperform non-incremental approaches by an order of magnitude if only a small part of the result is needed, while the penalty, if any, for the incremental processing is modest if the entire join result is required.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {237–248},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276326,
author = {Hjaltason, G\'{\i}sli R. and Samet, Hanan},
title = {Incremental Distance Join Algorithms for Spatial Databases},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276326},
doi = {10.1145/276305.276326},
abstract = {Two new spatial join operations, distance join and distance semi-join, are introduced where the join output is ordered by the distance between the spatial attribute values of the joined tuples. Incremental algorithms are presented for computing these operations, which can be used in a pipelined fashion, thereby obviating the need to wait for their completion when only a few tuples are needed. The algorithms can be used with a large class of hierarchical spatial data structures and arbitrary spatial data types in any dimensions. In addition, any distance metric may be employed. A performance study using R-trees shows that the incremental algorithms outperform non-incremental approaches by an order of magnitude if only a small part of the result is needed, while the penalty, if any, for the incremental processing is modest if the entire join result is required.},
journal = {SIGMOD Rec.},
month = jun,
pages = {237–248},
numpages = {12}
}

@inproceedings{10.1145/276304.276327,
author = {Kotidis, Yannis and Roussopoulos, Nick},
title = {An Alternative Storage Organization for ROLAP Aggregate Views Based on Cubetrees},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276327},
doi = {10.1145/276304.276327},
abstract = {The Relational On-Line Analytical Processing (ROLAP) is emerging as the dominant approach in data warehousing with decision support applications. In order to enhance query performance, the ROLAP approach relies on selecting and materializing in summary tables appropriate subsets of aggregate views which are then engaged in speeding up OLAP queries. However, a straight forward relational storage implementation of materialized ROLAP views is immensely wasteful on storage and incredibly inadequate on query performance and incremental update speed. In this paper we propose the use of Cubetrees, a collection of packed and compressed R-trees, as an alternative storage and index organization for ROLAP views and provide an efficient algorithm for mapping an arbitrary set of OLAP views to a collection of Cubetrees that achieve excellent performance. Compared to a conventional (relational) storage organization of materialized OLAP views, Cubetrees offer at least a 2-1 storage reduction, a 10-1 better OLAP query performance, and a 100-1 faster updates. We compare the two alternative approaches with data generated from the TPC-D benchmark and stored in the Informix Universal Server (IUS). The straight forward implementation materializes the ROLAP views using IUS tables and conventional B-tree indexing. The Cubetree implementation materializes the same ROLAP views using a Cubetree Datablade developed for IUS. The experiments demonstrate that the Cubetree storage organization is superior in storage, query performance and update speed.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {249–258},
numpages = {10},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276327,
author = {Kotidis, Yannis and Roussopoulos, Nick},
title = {An Alternative Storage Organization for ROLAP Aggregate Views Based on Cubetrees},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276327},
doi = {10.1145/276305.276327},
abstract = {The Relational On-Line Analytical Processing (ROLAP) is emerging as the dominant approach in data warehousing with decision support applications. In order to enhance query performance, the ROLAP approach relies on selecting and materializing in summary tables appropriate subsets of aggregate views which are then engaged in speeding up OLAP queries. However, a straight forward relational storage implementation of materialized ROLAP views is immensely wasteful on storage and incredibly inadequate on query performance and incremental update speed. In this paper we propose the use of Cubetrees, a collection of packed and compressed R-trees, as an alternative storage and index organization for ROLAP views and provide an efficient algorithm for mapping an arbitrary set of OLAP views to a collection of Cubetrees that achieve excellent performance. Compared to a conventional (relational) storage organization of materialized OLAP views, Cubetrees offer at least a 2-1 storage reduction, a 10-1 better OLAP query performance, and a 100-1 faster updates. We compare the two alternative approaches with data generated from the TPC-D benchmark and stored in the Informix Universal Server (IUS). The straight forward implementation materializes the ROLAP views using IUS tables and conventional B-tree indexing. The Cubetree implementation materializes the same ROLAP views using a Cubetree Datablade developed for IUS. The experiments demonstrate that the Cubetree storage organization is superior in storage, query performance and update speed.},
journal = {SIGMOD Rec.},
month = jun,
pages = {249–258},
numpages = {10}
}

@inproceedings{10.1145/276304.276328,
author = {Deshpande, Prasad M. and Ramasamy, Karthikeyan and Shukla, Amit and Naughton, Jeffrey F.},
title = {Caching Multidimensional Queries Using Chunks},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276328},
doi = {10.1145/276304.276328},
abstract = {Caching has been proposed (and implemented) by OLAP systems in order to reduce response times for multidimensional queries. Previous work on such caching has considered table level caching and query level caching. Table level caching is more suitable for static schemes. On the other hand, query level caching can be used in dynamic schemes, but is too coarse for “large” query results. Query level caching has the further drawback for small query results in that it is only effective when a new query is subsumed by a previously cached query. In this paper, we propose caching small regions of the multidimensional space called “chunks”. Chunk-based caching allows fine granularity caching, and allows queries to partially reuse the results of previous queries with which they overlap. To facilitate the computation of chunks required by a query but missing from the cache, we propose a new organization for relational tables, which we call a “chunked file.” Our experiments show that for workloads that exhibit query locality, chunked caching combined with the chunked file organization performs better than query level caching. An unexpected benefit of the chunked file organization is that, due to its multidimensional clustering properties, it can significantly improve the performance of queries that “miss” the cache entirely as compared to traditional file organizations.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {259–270},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276328,
author = {Deshpande, Prasad M. and Ramasamy, Karthikeyan and Shukla, Amit and Naughton, Jeffrey F.},
title = {Caching Multidimensional Queries Using Chunks},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276328},
doi = {10.1145/276305.276328},
abstract = {Caching has been proposed (and implemented) by OLAP systems in order to reduce response times for multidimensional queries. Previous work on such caching has considered table level caching and query level caching. Table level caching is more suitable for static schemes. On the other hand, query level caching can be used in dynamic schemes, but is too coarse for “large” query results. Query level caching has the further drawback for small query results in that it is only effective when a new query is subsumed by a previously cached query. In this paper, we propose caching small regions of the multidimensional space called “chunks”. Chunk-based caching allows fine granularity caching, and allows queries to partially reuse the results of previous queries with which they overlap. To facilitate the computation of chunks required by a query but missing from the cache, we propose a new organization for relational tables, which we call a “chunked file.” Our experiments show that for workloads that exhibit query locality, chunked caching combined with the chunked file organization performs better than query level caching. An unexpected benefit of the chunked file organization is that, due to its multidimensional clustering properties, it can significantly improve the performance of queries that “miss” the cache entirely as compared to traditional file organizations.},
journal = {SIGMOD Rec.},
month = jun,
pages = {259–270},
numpages = {12}
}

@inproceedings{10.1145/276304.276329,
author = {Zhao, Yihong and Deshpande, Prasad M. and Naughton, Jeffrey F. and Shukla, Amit},
title = {Simultaneous Optimization and Evaluation of Multiple Dimensional Queries},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276329},
doi = {10.1145/276304.276329},
abstract = {Database researchers have made significant progress on several research issues related to multidimensional data analysis, including the development of fast cubing algorithms, efficient schemes for creating and maintaining precomputed group-bys, and the design of efficient storage structures for multidimensional data. However, to date there has been little or no work on multidimensional query optimization. Recently, Microsoft has proposed “OLE DB for OLAP” as a standard multidimensional interface for databases. OLE DB for OLAP defines Multi-Dimensional Expressions (MDX), which have the interesting and challenging feature of allowing clients to ask several related dimensional queries in a single MDX expression. In this paper, we present three algorithms to optimize multiple related dimensional queries. Two of the algorithms focus on how to generate a global plan from several related local plans. The third algorithm focuses on generating a good global plan without first generating local plans. We also present three new query evaluation primitives that allow related query plans to share portions of their evaluation. Our initial performance results suggest that the exploitation of common subtask evaluation and global optimization can yield substantial performance improvements when relational database systems are used as data sources for multidimensional analysis.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {271–282},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276329,
author = {Zhao, Yihong and Deshpande, Prasad M. and Naughton, Jeffrey F. and Shukla, Amit},
title = {Simultaneous Optimization and Evaluation of Multiple Dimensional Queries},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276329},
doi = {10.1145/276305.276329},
abstract = {Database researchers have made significant progress on several research issues related to multidimensional data analysis, including the development of fast cubing algorithms, efficient schemes for creating and maintaining precomputed group-bys, and the design of efficient storage structures for multidimensional data. However, to date there has been little or no work on multidimensional query optimization. Recently, Microsoft has proposed “OLE DB for OLAP” as a standard multidimensional interface for databases. OLE DB for OLAP defines Multi-Dimensional Expressions (MDX), which have the interesting and challenging feature of allowing clients to ask several related dimensional queries in a single MDX expression. In this paper, we present three algorithms to optimize multiple related dimensional queries. Two of the algorithms focus on how to generate a global plan from several related local plans. The third algorithm focuses on generating a good global plan without first generating local plans. We also present three new query evaluation primitives that allow related query plans to share portions of their evaluation. Our initial performance results suggest that the exploitation of common subtask evaluation and global optimization can yield substantial performance improvements when relational database systems are used as data sources for multidimensional analysis.},
journal = {SIGMOD Rec.},
month = jun,
pages = {271–282},
numpages = {12}
}

@inproceedings{10.1145/276304.276330,
author = {Adelberg, Brad},
title = {NoDoSE—a Tool for Semi-Automatically Extracting Structured and Semistructured Data from Text Documents},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276330},
doi = {10.1145/276304.276330},
abstract = {Often interesting structured or semistructured data is not in database systems but in HTML pages, text files, or on paper. The data in these formats is not usable by standard query processing engines and hence users need a way of extracting data from these sources into a DBMS or of writing wrappers around the sources. This paper describes NoDoSE, the Northwestern Document Structure Extractor, which is an interactive tool for semi-automatically determining the structure of such documents and then extracting their data. Using a GUI, the user hierarchically decomposes the file, outlining its interesting regions and then describing their semantics. This task is expedited by a mining component that attempts to infer the grammar of the file from the information the user has input so far. Once the format of a document has been determined, its data can be extracted into a number of useful forms. This paper describes both the NoDoSE architecture, which can be used as a test bed for structure mining algorithms in general, and the mining algorithms that have been developed by the author. The prototype, which is written in Java, is described and experiences parsing a variety of documents are reported.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {283–294},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276330,
author = {Adelberg, Brad},
title = {NoDoSE—a Tool for Semi-Automatically Extracting Structured and Semistructured Data from Text Documents},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276330},
doi = {10.1145/276305.276330},
abstract = {Often interesting structured or semistructured data is not in database systems but in HTML pages, text files, or on paper. The data in these formats is not usable by standard query processing engines and hence users need a way of extracting data from these sources into a DBMS or of writing wrappers around the sources. This paper describes NoDoSE, the Northwestern Document Structure Extractor, which is an interactive tool for semi-automatically determining the structure of such documents and then extracting their data. Using a GUI, the user hierarchically decomposes the file, outlining its interesting regions and then describing their semantics. This task is expedited by a mining component that attempts to infer the grammar of the file from the information the user has input so far. Once the format of a document has been determined, its data can be extracted into a number of useful forms. This paper describes both the NoDoSE architecture, which can be used as a test bed for structure mining algorithms in general, and the mining algorithms that have been developed by the author. The prototype, which is written in Java, is described and experiences parsing a variety of documents are reported.},
journal = {SIGMOD Rec.},
month = jun,
pages = {283–294},
numpages = {12}
}

@inproceedings{10.1145/276304.276331,
author = {Nestorov, Svetlozar and Abiteboul, Serge and Motwani, Rajeev},
title = {Extracting Schema from Semistructured Data},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276331},
doi = {10.1145/276304.276331},
abstract = {Semistructured data is characterized by the lack of any fixed and rigid schema, although typically the data has some implicit structure. While the lack of fixed schema makes extracting semistructured data fairly easy and an attractive goal, presenting and querying such data is greatly impaired. Thus, a critical problem is the discovery of the structure implicit in semistructured data and, subsequently, the recasting of the raw data in terms of this structure. In this paper, we consider a very general form of semistructured data based on labeled, directed graphs. We show that such data can be typed using the greatest fixpoint semantics of monadic datalog programs. We present an algorithm for approximate typing of semistructured data. We establish that the general problem of finding an optimal such typing is NP-hard, but present some heuristics and techniques based on clustering that allow efficient and near-optimal treatment of the problem. We also present some preliminary experimental results.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {295–306},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276331,
author = {Nestorov, Svetlozar and Abiteboul, Serge and Motwani, Rajeev},
title = {Extracting Schema from Semistructured Data},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276331},
doi = {10.1145/276305.276331},
abstract = {Semistructured data is characterized by the lack of any fixed and rigid schema, although typically the data has some implicit structure. While the lack of fixed schema makes extracting semistructured data fairly easy and an attractive goal, presenting and querying such data is greatly impaired. Thus, a critical problem is the discovery of the structure implicit in semistructured data and, subsequently, the recasting of the raw data in terms of this structure. In this paper, we consider a very general form of semistructured data based on labeled, directed graphs. We show that such data can be typed using the greatest fixpoint semantics of monadic datalog programs. We present an algorithm for approximate typing of semistructured data. We establish that the general problem of finding an optimal such typing is NP-hard, but present some heuristics and techniques based on clustering that allow efficient and near-optimal treatment of the problem. We also present some preliminary experimental results.},
journal = {SIGMOD Rec.},
month = jun,
pages = {295–306},
numpages = {12}
}

@inproceedings{10.1145/276304.276332,
author = {Chakrabarti, Soumen and Dom, Byron and Indyk, Piotr},
title = {Enhanced Hypertext Categorization Using Hyperlinks},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276332},
doi = {10.1145/276304.276332},
abstract = {A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented with pre-classified samples from Yahoo!1 and the US Patent Database2. In previous work, we developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained. This classifier misclassified 36% of the patents, indicating that classifying hypertext can be more difficult than classifying text. Naively using terms in neighboring documents increased error to 38%; our hypertext classifier reduced it to 21%. Results with the Yahoo! sample were more dramatic: the text classifier showed 68% error, whereas our hypertext classifier reduced this to only 21%.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {307–318},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276332,
author = {Chakrabarti, Soumen and Dom, Byron and Indyk, Piotr},
title = {Enhanced Hypertext Categorization Using Hyperlinks},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276332},
doi = {10.1145/276305.276332},
abstract = {A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented with pre-classified samples from Yahoo!1 and the US Patent Database2. In previous work, we developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained. This classifier misclassified 36% of the patents, indicating that classifying hypertext can be more difficult than classifying text. Naively using terms in neighboring documents increased error to 38%; our hypertext classifier reduced it to 21%. Results with the Yahoo! sample were more dramatic: the text classifier showed 68% error, whereas our hypertext classifier reduced this to only 21%.},
journal = {SIGMOD Rec.},
month = jun,
pages = {307–318},
numpages = {12}
}

@inproceedings{10.1145/276304.276333,
author = {Subramanian, Subbu N. and Venkataraman, Shivakumar},
title = {Cost-Based Optimization of Decision Support Queries Using Transient-Views},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276333},
doi = {10.1145/276304.276333},
abstract = {Next generation decision support applications, besides being capable of processing huge amounts of data, require the ability to integrate and reason over data from multiple, heterogeneous data sources. Often, these data sources differ in a variety of aspects such as their data models, the query languages they support, and their network protocols. Also, typically they are spread over a wide geographical area. The cost of processing decision support queries in such a setting is quite high. However, processing these queries often involves redundancies such as repeated access of same data source and multiple execution of similar processing sequences. Minimizing these redundancies would significantly reduce the query processing cost. In this paper, we (1) propose an architecture for processing complex decision support queries involving multiple, heterogeneous data sources; (2) introduce the notion of transient-views — materialized views that exist only in the context of execution of a query — that is useful for minimizing the redundancies involved in the execution of these queries; (3) develop a cost-based algorithm that takes a query plan as input and generates an optimal “covering plan”, by minimizing redundancies in the original plan; (4) validate our approach by means of an implementation of the algorithms and a detailed performance study based on TPC-D benchmark queries on a commercial database system; and finally, (5) compare and contrast our approach with work in related areas, in particular, the areas of answering queries using views and optimization using common sub-expressions. Our experiments demonstrate the practicality and usefulness of transient-views in significantly improving the performance of decision support queries.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {319–330},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276333,
author = {Subramanian, Subbu N. and Venkataraman, Shivakumar},
title = {Cost-Based Optimization of Decision Support Queries Using Transient-Views},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276333},
doi = {10.1145/276305.276333},
abstract = {Next generation decision support applications, besides being capable of processing huge amounts of data, require the ability to integrate and reason over data from multiple, heterogeneous data sources. Often, these data sources differ in a variety of aspects such as their data models, the query languages they support, and their network protocols. Also, typically they are spread over a wide geographical area. The cost of processing decision support queries in such a setting is quite high. However, processing these queries often involves redundancies such as repeated access of same data source and multiple execution of similar processing sequences. Minimizing these redundancies would significantly reduce the query processing cost. In this paper, we (1) propose an architecture for processing complex decision support queries involving multiple, heterogeneous data sources; (2) introduce the notion of transient-views — materialized views that exist only in the context of execution of a query — that is useful for minimizing the redundancies involved in the execution of these queries; (3) develop a cost-based algorithm that takes a query plan as input and generates an optimal “covering plan”, by minimizing redundancies in the original plan; (4) validate our approach by means of an implementation of the algorithms and a detailed performance study based on TPC-D benchmark queries on a commercial database system; and finally, (5) compare and contrast our approach with work in related areas, in particular, the areas of answering queries using views and optimization using common sub-expressions. Our experiments demonstrate the practicality and usefulness of transient-views in significantly improving the performance of decision support queries.},
journal = {SIGMOD Rec.},
month = jun,
pages = {319–330},
numpages = {12}
}

@inproceedings{10.1145/276304.276334,
author = {Gibbons, Phillip B. and Matias, Yossi},
title = {New Sampling-Based Summary Statistics for Improving Approximate Query Answers},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276334},
doi = {10.1145/276304.276334},
abstract = {In large data recording and warehousing environments, it is often advantageous to provide fast, approximate answers to queries, whenever possible. Before DBMSs providing highly-accurate approximate answers can become a reality, many new techniques for summarizing data and for estimating answers from summarized data must be developed. This paper introduces two new sampling-based summary statistics, concise samples and counting samples, and presents new techniques for their fast incremental maintenance regardless of the data distribution. We quantify their advantages over standard sample views in terms of the number of additional sample points for the same view size, and hence in providing more accurate query answers. Finally, we consider their application to providing fast approximate answers to hot list queries. Our algorithms maintain their accuracy in the presence of ongoing insertions to the data warehouse.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {331–342},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276334,
author = {Gibbons, Phillip B. and Matias, Yossi},
title = {New Sampling-Based Summary Statistics for Improving Approximate Query Answers},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276334},
doi = {10.1145/276305.276334},
abstract = {In large data recording and warehousing environments, it is often advantageous to provide fast, approximate answers to queries, whenever possible. Before DBMSs providing highly-accurate approximate answers can become a reality, many new techniques for summarizing data and for estimating answers from summarized data must be developed. This paper introduces two new sampling-based summary statistics, concise samples and counting samples, and presents new techniques for their fast incremental maintenance regardless of the data distribution. We quantify their advantages over standard sample views in terms of the number of additional sample points for the same view size, and hence in providing more accurate query answers. Finally, we consider their application to providing fast approximate answers to hot list queries. Our algorithms maintain their accuracy in the presence of ongoing insertions to the data warehouse.},
journal = {SIGMOD Rec.},
month = jun,
pages = {331–342},
numpages = {12}
}

@inproceedings{10.1145/276304.276335,
author = {Sarawagi, Sunita and Thomas, Shiby and Agrawal, Rakesh},
title = {Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276335},
doi = {10.1145/276304.276335},
abstract = {Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {343–354},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276335,
author = {Sarawagi, Sunita and Thomas, Shiby and Agrawal, Rakesh},
title = {Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276335},
doi = {10.1145/276305.276335},
abstract = {Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.},
journal = {SIGMOD Rec.},
month = jun,
pages = {343–354},
numpages = {12}
}

@inproceedings{10.1145/276304.276336,
author = {Chan, Chee-Yong and Ioannidis, Yannis E.},
title = {Bitmap Index Design and Evaluation},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276336},
doi = {10.1145/276304.276336},
abstract = {Bitmap indexing has been touted as a promising approach for processing complex adhoc queries in read-mostly environments, like those of decision support systems. Nevertheless, only few possible bitmap schemes have been proposed in the past and very little is known about the space-time tradeoff that they offer. In this paper, we present a general framework to study the design space of bitmap indexes for selection queries and examine the disk-space and time characteristics that the various alternative index choices offer. In particular, we draw a parallel between bitmap indexing and number representation in different number systems, and define a space of two orthogonal dimensions that captures a wide array of bitmap indexes, both old and new. Within that space, we identify (analytically or experimentally) the following interesting points: (1) the time-optimal bitmap index; (2) the space-optimal bitmap index; (3) the bitmap index with the optimal space-time tradeoff (knee); and (4) the time-optimal bitmap index under a given disk-space constraint. Finally, we examine the impact of bitmap compression and bitmap buffering on the space-time tradeoffs among those indexes. As part of this work, we also describe a bitmap-index-based evaluation algorithm for selection queries that represents an improvement over earlier proposals. We believe that this study offers a useful first set of guidelines for physical database design using bitmap indexes.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {355–366},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276336,
author = {Chan, Chee-Yong and Ioannidis, Yannis E.},
title = {Bitmap Index Design and Evaluation},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276336},
doi = {10.1145/276305.276336},
abstract = {Bitmap indexing has been touted as a promising approach for processing complex adhoc queries in read-mostly environments, like those of decision support systems. Nevertheless, only few possible bitmap schemes have been proposed in the past and very little is known about the space-time tradeoff that they offer. In this paper, we present a general framework to study the design space of bitmap indexes for selection queries and examine the disk-space and time characteristics that the various alternative index choices offer. In particular, we draw a parallel between bitmap indexing and number representation in different number systems, and define a space of two orthogonal dimensions that captures a wide array of bitmap indexes, both old and new. Within that space, we identify (analytically or experimentally) the following interesting points: (1) the time-optimal bitmap index; (2) the space-optimal bitmap index; (3) the bitmap index with the optimal space-time tradeoff (knee); and (4) the time-optimal bitmap index under a given disk-space constraint. Finally, we examine the impact of bitmap compression and bitmap buffering on the space-time tradeoffs among those indexes. As part of this work, we also describe a bitmap-index-based evaluation algorithm for selection queries that represents an improvement over earlier proposals. We believe that this study offers a useful first set of guidelines for physical database design using bitmap indexes.},
journal = {SIGMOD Rec.},
month = jun,
pages = {355–366},
numpages = {12}
}

@inproceedings{10.1145/276304.276337,
author = {Chaudhuri, Surajit and Narasayya, Vivek},
title = {AutoAdmin “What-If” Index Analysis Utility},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276337},
doi = {10.1145/276304.276337},
abstract = {As databases get widely deployed, it becomes increasingly important to reduce the overhead of database administration. An important aspect of data administration that critically influences performance is the ability to select indexes for a database. In order to decide the right indexes for a database, it is crucial for the database administrator (DBA) to be able to perform a quantitative analysis of the existing indexes. Furthermore, the DBA should have the ability to propose hypothetical (“what-if”) indexes and quantitatively analyze their impact on performance of the system. Such impact analysis may consist of analyzing workloads over the database, estimating changes in the cost of a workload, and studying index usage while taking into account projected changes in the sizes of the database tables. In this paper we describe a novel index analysis utility that we have prototyped for Microsoft SQL Server 7.0. We describe the interfaces exposed by this utility that can be leveraged by a variety of front-end tools and sketch important aspects of the user interfaces enabled by the utility. We also discuss the implementation techniques for efficiently supporting “what-if” indexes. Our framework can be extended to incorporate analysis of other aspects of physical database design.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {367–378},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276337,
author = {Chaudhuri, Surajit and Narasayya, Vivek},
title = {AutoAdmin “What-If” Index Analysis Utility},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276337},
doi = {10.1145/276305.276337},
abstract = {As databases get widely deployed, it becomes increasingly important to reduce the overhead of database administration. An important aspect of data administration that critically influences performance is the ability to select indexes for a database. In order to decide the right indexes for a database, it is crucial for the database administrator (DBA) to be able to perform a quantitative analysis of the existing indexes. Furthermore, the DBA should have the ability to propose hypothetical (“what-if”) indexes and quantitatively analyze their impact on performance of the system. Such impact analysis may consist of analyzing workloads over the database, estimating changes in the cost of a workload, and studying index usage while taking into account projected changes in the sizes of the database tables. In this paper we describe a novel index analysis utility that we have prototyped for Microsoft SQL Server 7.0. We describe the interfaces exposed by this utility that can be leveraged by a variety of front-end tools and sketch important aspects of the user interfaces enabled by the utility. We also discuss the implementation techniques for efficiently supporting “what-if” indexes. Our framework can be extended to incorporate analysis of other aspects of physical database design.},
journal = {SIGMOD Rec.},
month = jun,
pages = {367–378},
numpages = {12}
}

@inproceedings{10.1145/276304.276338,
author = {Jaedicke, Michael and Mitschang, Bernhard},
title = {On Parallel Processing of Aggregate and Scalar Functions in Object-Relational DBMS},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276338},
doi = {10.1145/276304.276338},
abstract = {Nowadays parallel object-relational DBMS are envisioned as the next great wave, but there is still a lack of efficient implementation concepts for some parts of the proposed functionality. Thus one of the current goals for parallel object-relational DBMS is to move towards higher performance. In this paper we develop a framework that allows to process user-defined functions with data parallelism. We will describe the class of partitionable functions that can be processed parallelly. We will also propose an extension which allows to speed up the processing of another large class of functions by means of parallel sorting. Functions that can be processed by means of our techniques are often used in decision support queries on large data volumes, for example. Hence a parallel execution is indispensable.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {379–389},
numpages = {11},
keywords = {object-relational database systems, user-defined functions, parallel query processing, aggregates},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276338,
author = {Jaedicke, Michael and Mitschang, Bernhard},
title = {On Parallel Processing of Aggregate and Scalar Functions in Object-Relational DBMS},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276338},
doi = {10.1145/276305.276338},
abstract = {Nowadays parallel object-relational DBMS are envisioned as the next great wave, but there is still a lack of efficient implementation concepts for some parts of the proposed functionality. Thus one of the current goals for parallel object-relational DBMS is to move towards higher performance. In this paper we develop a framework that allows to process user-defined functions with data parallelism. We will describe the class of partitionable functions that can be processed parallelly. We will also propose an extension which allows to speed up the processing of another large class of functions by means of parallel sorting. Functions that can be processed by means of our techniques are often used in decision support queries on large data volumes, for example. Hence a parallel execution is indispensable.},
journal = {SIGMOD Rec.},
month = jun,
pages = {379–389},
numpages = {11},
keywords = {user-defined functions, object-relational database systems, parallel query processing, aggregates}
}

@inproceedings{10.1145/276304.276339,
author = {Godfrey, Michael and Mayr, Tobias and Seshadri, Praveen and von Eicken, Thorsten},
title = {Secure and Portable Database Extensibility},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276339},
doi = {10.1145/276304.276339},
abstract = {The functionality of extensible database servers can be augmented by user-defined functions (UDFs). However, the server's security and stability are concerns whenever new code is incorporated. Recently, there has been interest in the use of Java for database extensibility. This raises several questions: Does Java solve the security problems? How does it affect efficiency?We explore the tradeoffs involved in extending the PREDATOR object-relational database server using Java. We also describe some interesting details of our implementation. The issues examined in our study are security, efficiency, and portability. Our performance experiments compare Java-based extensibility with traditional alternatives in the native language of the server. We explore a variety of UDFs that differ in the amount of computation involved and in the quantity of data accessed. We also qualitatively compare the security and portability of the different alternatives. Our conclusion is that Java-based UDFs are a viable approach in terms of performance. However, there may be challenging design issues in integrating Java UDFs with existing database systems.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {390–401},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276339,
author = {Godfrey, Michael and Mayr, Tobias and Seshadri, Praveen and von Eicken, Thorsten},
title = {Secure and Portable Database Extensibility},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276339},
doi = {10.1145/276305.276339},
abstract = {The functionality of extensible database servers can be augmented by user-defined functions (UDFs). However, the server's security and stability are concerns whenever new code is incorporated. Recently, there has been interest in the use of Java for database extensibility. This raises several questions: Does Java solve the security problems? How does it affect efficiency?We explore the tradeoffs involved in extending the PREDATOR object-relational database server using Java. We also describe some interesting details of our implementation. The issues examined in our study are security, efficiency, and portability. Our performance experiments compare Java-based extensibility with traditional alternatives in the native language of the server. We explore a variety of UDFs that differ in the amount of computation involved and in the quantity of data accessed. We also qualitatively compare the security and portability of the different alternatives. Our conclusion is that Java-based UDFs are a viable approach in terms of performance. However, there may be challenging design issues in integrating Java UDFs with existing database systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {390–401},
numpages = {12}
}

@inproceedings{10.1145/276304.276340,
author = {Adali, S. and Bonatti, P. and Sapino, M. L. and Subrahmanian, V. S.},
title = {A Multi-Similarity Algebra},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276340},
doi = {10.1145/276304.276340},
abstract = {The need to automatically extract and classify the contents of multimedia data archives such as images, video, and text documents has led to significant work on similarity based retrieval of data. To date, most work in this area has focused on the creation of index structures for similarity based retrieval. There is very little work on developing formalisms for querying multimedia databases that support similarity based computations and optimizing such queries, even though it is well known that feature extraction and identification algorithms in media data are very expensive. We introduce a similarity algebra that brings together relational operators and results of multiple similarity implementations in a uniform language. The algebra can be used to specify complex queries that combine different interpretations of similarity values and multiple algorithms for computing these values. We prove equivalence and containment relationships between similarity algebra expressions and develop query rewriting methods based on these results. We then provide a generic cost model for evaluating cost of query plans in the similarity algebra and query optimization methods based on this model. We supplement the paper with experimental results that illustrate the use of the algebra and the effectiveness of query optimization methods using the Integrated Search Engine (I.SEE) as the testbed.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {402–413},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276340,
author = {Adali, S. and Bonatti, P. and Sapino, M. L. and Subrahmanian, V. S.},
title = {A Multi-Similarity Algebra},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276340},
doi = {10.1145/276305.276340},
abstract = {The need to automatically extract and classify the contents of multimedia data archives such as images, video, and text documents has led to significant work on similarity based retrieval of data. To date, most work in this area has focused on the creation of index structures for similarity based retrieval. There is very little work on developing formalisms for querying multimedia databases that support similarity based computations and optimizing such queries, even though it is well known that feature extraction and identification algorithms in media data are very expensive. We introduce a similarity algebra that brings together relational operators and results of multiple similarity implementations in a uniform language. The algebra can be used to specify complex queries that combine different interpretations of similarity values and multiple algorithms for computing these values. We prove equivalence and containment relationships between similarity algebra expressions and develop query rewriting methods based on these results. We then provide a generic cost model for evaluating cost of query plans in the similarity algebra and query optimization methods based on this model. We supplement the paper with experimental results that illustrate the use of the algebra and the effectiveness of query optimization methods using the Integrated Search Engine (I.SEE) as the testbed.},
journal = {SIGMOD Rec.},
month = jun,
pages = {402–413},
numpages = {12}
}

@inproceedings{10.1145/276304.276341,
author = {Fern\'{a}ndez, Mary and Florescu, Daniela and Kang, Jaewoo and Levy, Alon and Suciu, Dan},
title = {Catching the Boat with Strudel: Experiences with a Web-Site Management System},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276341},
doi = {10.1145/276304.276341},
abstract = {The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel's key idea is separating the management of the site's data, the creation and management of the site's structure, and the visual presentation of the site's pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site's structure by applying a “site-definition query” to the underlying data. The result of evaluating this query is a “site graph”, which represents both the site's content and structure. Third, the builder specifies the visual presentation of pages in Strudel's HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs.We describe Strudel's key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience. We describe our experience constructing several Web sites with Strudel and discuss the impact of potential users' requirements on Strudel's design. We address two main questions: (1) when does a declarative specification of site structure provide significant benefits, and (2) what are the main advantages provided by the semi-structured data model.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {414–425},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276341,
author = {Fern\'{a}ndez, Mary and Florescu, Daniela and Kang, Jaewoo and Levy, Alon and Suciu, Dan},
title = {Catching the Boat with Strudel: Experiences with a Web-Site Management System},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276341},
doi = {10.1145/276305.276341},
abstract = {The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel's key idea is separating the management of the site's data, the creation and management of the site's structure, and the visual presentation of the site's pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site's structure by applying a “site-definition query” to the underlying data. The result of evaluating this query is a “site graph”, which represents both the site's content and structure. Third, the builder specifies the visual presentation of pages in Strudel's HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs.We describe Strudel's key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience. We describe our experience constructing several Web sites with Strudel and discuss the impact of potential users' requirements on Strudel's design. We address two main questions: (1) when does a declarative specification of site structure provide significant benefits, and (2) what are the main advantages provided by the semi-structured data model.},
journal = {SIGMOD Rec.},
month = jun,
pages = {414–425},
numpages = {12}
}

@inproceedings{10.1145/276304.276342,
author = {Manku, Gurmeet Singh and Rajagopalan, Sridhar and Lindsay, Bruce G.},
title = {Approximate Medians and Other Quantiles in One Pass and with Limited Memory},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276342},
doi = {10.1145/276304.276342},
abstract = {We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply for arbitrary value distributions and arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e. they apply with respect to a (user controlled) confidence parameter.We present the algorithms, their theoretical analysis and simulation results on different datasets.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {426–435},
numpages = {10},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276342,
author = {Manku, Gurmeet Singh and Rajagopalan, Sridhar and Lindsay, Bruce G.},
title = {Approximate Medians and Other Quantiles in One Pass and with Limited Memory},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276342},
doi = {10.1145/276305.276342},
abstract = {We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply for arbitrary value distributions and arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e. they apply with respect to a (user controlled) confidence parameter.We present the algorithms, their theoretical analysis and simulation results on different datasets.},
journal = {SIGMOD Rec.},
month = jun,
pages = {426–435},
numpages = {10}
}

@inproceedings{10.1145/276304.276343,
author = {Chaudhuri, Surajit and Motwani, Rajeev and Narasayya, Vivek},
title = {Random Sampling for Histogram Construction: How Much is Enough?},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276343},
doi = {10.1145/276304.276343},
abstract = {Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining “How much sampling is enough?” We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose a new error metric which has a reliable estimator and can still be exploited by query optimizers to influence the choice of execution plans. The algorithm for histogram construction was prototyped on Microsoft SQL Server 7.0 and we present experimental results showing that the adaptive algorithm accurately approximates the true histogram over different data distributions.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {436–447},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276343,
author = {Chaudhuri, Surajit and Motwani, Rajeev and Narasayya, Vivek},
title = {Random Sampling for Histogram Construction: How Much is Enough?},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276343},
doi = {10.1145/276305.276343},
abstract = {Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining “How much sampling is enough?” We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose a new error metric which has a reliable estimator and can still be exploited by query optimizers to influence the choice of execution plans. The algorithm for histogram construction was prototyped on Microsoft SQL Server 7.0 and we present experimental results showing that the adaptive algorithm accurately approximates the true histogram over different data distributions.},
journal = {SIGMOD Rec.},
month = jun,
pages = {436–447},
numpages = {12}
}

@inproceedings{10.1145/276304.276344,
author = {Matias, Yossi and Vitter, Jeffrey Scott and Wang, Min},
title = {Wavelet-Based Histograms for Selectivity Estimation},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276344},
doi = {10.1145/276304.276344},
abstract = {Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations.In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data distributions give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using them in an on-line fashion for selectivity estimation. Our histograms also provide quick approximate answers to OLAP queries when the exact answers are not required. Our method captures the joint distribution of multiple attributes effectively, even when the attributes are correlated. Experiments confirm that our histograms offer substantial improvements in accuracy over random sampling and other previous approaches.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {448–459},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276344,
author = {Matias, Yossi and Vitter, Jeffrey Scott and Wang, Min},
title = {Wavelet-Based Histograms for Selectivity Estimation},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276344},
doi = {10.1145/276305.276344},
abstract = {Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations.In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data distributions give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using them in an on-line fashion for selectivity estimation. Our histograms also provide quick approximate answers to OLAP queries when the exact answers are not required. Our method captures the joint distribution of multiple attributes effectively, even when the attributes are correlated. Experiments confirm that our histograms offer substantial improvements in accuracy over random sampling and other previous approaches.},
journal = {SIGMOD Rec.},
month = jun,
pages = {448–459},
numpages = {12}
}

@inproceedings{10.1145/276304.276345,
author = {Lomet, David and Weikum, Gerhard},
title = {Efficient Transparent Application Recovery in Client-Server Information Systems},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276345},
doi = {10.1145/276304.276345},
abstract = {Database systems recover persistent data, providing high database availability. However, database applications, typically residing on client or “middle-tier” application-server machines, may lose work because of a server failure. This prevents the masking of server failures from the human user and substantially degrades application availability. This paper aims to enable high application availability with an integrated method for database server recovery and transparent application recovery in a client-server system. The approach, based on application message logging, is similar to earlier work on distributed system fault tolerance. However, we exploit advanced database logging and recovery techniques and request/reply messaging properties to significantly improve efficiency. Forced log I/Os, frequently required by other methods, are usually avoided. Restart time, for both failed server and failed client, is reduced by checkpointing and log truncation. Our method ensures that a server can recover independently of clients. A client may reduce logging overhead in return for dependency on server availability during client restart.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {460–471},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276345,
author = {Lomet, David and Weikum, Gerhard},
title = {Efficient Transparent Application Recovery in Client-Server Information Systems},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276345},
doi = {10.1145/276305.276345},
abstract = {Database systems recover persistent data, providing high database availability. However, database applications, typically residing on client or “middle-tier” application-server machines, may lose work because of a server failure. This prevents the masking of server failures from the human user and substantially degrades application availability. This paper aims to enable high application availability with an integrated method for database server recovery and transparent application recovery in a client-server system. The approach, based on application message logging, is similar to earlier work on distributed system fault tolerance. However, we exploit advanced database logging and recovery techniques and request/reply messaging properties to significantly improve efficiency. Forced log I/Os, frequently required by other methods, are usually avoided. Restart time, for both failed server and failed client, is reduced by checkpointing and log truncation. Our method ensures that a server can recover independently of clients. A client may reduce logging overhead in return for dependency on server availability during client restart.},
journal = {SIGMOD Rec.},
month = jun,
pages = {460–471},
numpages = {12}
}

@inproceedings{10.1145/276304.276346,
author = {Larson, Per-\r{A}ke and Graefe, Goetz},
title = {Memory Management during Run Generation in External Sorting},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276346},
doi = {10.1145/276304.276346},
abstract = {If replacement selection is used in an external mergesort to generate initial runs, individual records are deleted and inserted in the sort operation's workspace. Variable-length records introduce the need for possibly complex memory management and extra copying of records. As a result, few systems employ replacement selection, even though it produces longer runs than commonly used algorithms. We experimentally compared several algorithms and variants for managing this workspace. We found that the simple best fit algorithm achieves memory utilization of 90% or better and run lengths over 1.8 times workspace size, with no extra copying of records and very little other overhead, for widely varying record sizes and for a wide range of memory sizes. Thus, replacement selection is a viable algorithm for commercial database systems, even for variable-length records.Efficient memory management also enables an external sort algorithm that degrades gracefully when its input is only slightly larger than or a small multiple of the available memory size. This is not the case with the usual implementations of external sorting, which incur I/O for the entire input even if it is as little as one record larger than memory. Thus, in some cases, our techniques may reduce I/O volume by a factor 10 compared to traditional database sort algorithms. Moreover, the gradual rather than abrupt growth in I/O volume for increasing input sizes significantly eases design and implementation of intra-query memory management policies.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {472–483},
numpages = {12},
keywords = {merge sort, sorting, memory management, run formation, replacement selection, last-run optimization, variable length records},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276346,
author = {Larson, Per-\r{A}ke and Graefe, Goetz},
title = {Memory Management during Run Generation in External Sorting},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276346},
doi = {10.1145/276305.276346},
abstract = {If replacement selection is used in an external mergesort to generate initial runs, individual records are deleted and inserted in the sort operation's workspace. Variable-length records introduce the need for possibly complex memory management and extra copying of records. As a result, few systems employ replacement selection, even though it produces longer runs than commonly used algorithms. We experimentally compared several algorithms and variants for managing this workspace. We found that the simple best fit algorithm achieves memory utilization of 90% or better and run lengths over 1.8 times workspace size, with no extra copying of records and very little other overhead, for widely varying record sizes and for a wide range of memory sizes. Thus, replacement selection is a viable algorithm for commercial database systems, even for variable-length records.Efficient memory management also enables an external sort algorithm that degrades gracefully when its input is only slightly larger than or a small multiple of the available memory size. This is not the case with the usual implementations of external sorting, which incur I/O for the entire input even if it is as little as one record larger than memory. Thus, in some cases, our techniques may reduce I/O volume by a factor 10 compared to traditional database sort algorithms. Moreover, the gradual rather than abrupt growth in I/O volume for increasing input sizes significantly eases design and implementation of intra-query memory management policies.},
journal = {SIGMOD Rec.},
month = jun,
pages = {472–483},
numpages = {12},
keywords = {merge sort, variable length records, sorting, memory management, run formation, last-run optimization, replacement selection}
}

@inproceedings{10.1145/276304.276347,
author = {Anderson, Todd and Breitbart, Yuri and Korth, Henry F. and Wool, Avishai},
title = {Replication, Consistency, and Practicality: Are These Mutually Exclusive?},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276347},
doi = {10.1145/276304.276347},
abstract = {Previous papers have postulated that traditional schemes for the management of replicated data are doomed to failure in practice due to a quartic (or worse) explosion in the probability of deadlocks. In this paper, we present results of a simulation study for three recently introduced protocols that guarantee global serializability and transaction atomicity without resorting to the two-phase commit protocol. The protocols analyzed in this paper include a global locking protocol [10], a “pessimistic” protocol based on a replication graph [5], and an “optimistic” protocol based on a replication graph [7]. The results of the study show a wide range of practical applicability for the lazy replica-update approach employed in these protocols. We show that under reasonable contention conditions and sufficiently high transaction rate, both replication-graph-based protocols outperform the global locking protocol. The distinctions among the protocols in terms of performance are significant. For example, an offered load where 70% - 80% of transactions under the global locking protocol were aborted, only 10% of transactions were aborted under the protocols based on the replication graph. The results of the study suggest that protocols based on a replication graph offer practical techniques for replica management. However, it also shows that performance deteriorates rapidly and dramatically when transaction throughput reaches a saturation point.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {484–495},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276347,
author = {Anderson, Todd and Breitbart, Yuri and Korth, Henry F. and Wool, Avishai},
title = {Replication, Consistency, and Practicality: Are These Mutually Exclusive?},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276347},
doi = {10.1145/276305.276347},
abstract = {Previous papers have postulated that traditional schemes for the management of replicated data are doomed to failure in practice due to a quartic (or worse) explosion in the probability of deadlocks. In this paper, we present results of a simulation study for three recently introduced protocols that guarantee global serializability and transaction atomicity without resorting to the two-phase commit protocol. The protocols analyzed in this paper include a global locking protocol [10], a “pessimistic” protocol based on a replication graph [5], and an “optimistic” protocol based on a replication graph [7]. The results of the study show a wide range of practical applicability for the lazy replica-update approach employed in these protocols. We show that under reasonable contention conditions and sufficiently high transaction rate, both replication-graph-based protocols outperform the global locking protocol. The distinctions among the protocols in terms of performance are significant. For example, an offered load where 70% - 80% of transactions under the global locking protocol were aborted, only 10% of transactions were aborted under the protocols based on the replication graph. The results of the study suggest that protocols based on a replication graph offer practical techniques for replica management. However, it also shows that performance deteriorates rapidly and dramatically when transaction throughput reaches a saturation point.},
journal = {SIGMOD Rec.},
month = jun,
pages = {484–495},
numpages = {12}
}

@inproceedings{10.1145/276304.276348,
author = {Stonebraker, Michael},
title = {Are We Working on the Right Problems? (Panel)},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276348},
doi = {10.1145/276304.276348},
abstract = {There appears to be a discrepancy between the research topics being pursued by the database research community and the key problems facing information systems decisions makers such as Chief Information Officers (CIOs). Panelists will present their view of the key problems that would benefit from a research focus in the database research community and will discuss perceived discrepancies. Based on personal experience, the most commonly discussed information systems problems facing CIOs today include:},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {496},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276348,
author = {Stonebraker, Michael},
title = {Are We Working on the Right Problems? (Panel)},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276348},
doi = {10.1145/276305.276348},
abstract = {There appears to be a discrepancy between the research topics being pursued by the database research community and the key problems facing information systems decisions makers such as Chief Information Officers (CIOs). Panelists will present their view of the key problems that would benefit from a research focus in the database research community and will discuss perceived discrepancies. Based on personal experience, the most commonly discussed information systems problems facing CIOs today include:},
journal = {SIGMOD Rec.},
month = jun,
pages = {496},
numpages = {1}
}

@inproceedings{10.1145/276304.276349,
author = {Mylopoulos, John},
title = {Next Generation Database Systems Won't Work without Semantics! (Panel)},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276349},
doi = {10.1145/276304.276349},
abstract = {In the late '70s, while second generation DBMS products and technologies were entering the market, there was significant research activity whose aim was to make greater use of semantic information in database systems. The focus of that research was primarily on semantic data models and data modelling, including semantic query processing and integrity checking. However, few if any of the results of these efforts found their way in database technologies of the day, or database management practices. Instead, semantic issues were delegated to early phases of information system development (including requirements analysis and design), as well as application development.Today's database system technologies perform admirably well with semantically trivial operations and  representations. At the same time, these technologies are being challenged in virtually every area of data management, with new applications which demand ways of dealing more explicitly with the meaning of the data being managed. For example, interoperation between applications requires that the underlying databases interoperate meaningfully. This currently requires a mammoth manual reverse engineering effort that simply cannot be sustained or funded by any large organization. The same applies to data warehouses, since they too require the correct semantic merging of data from semantically diverse sources. Errors in merging these sources can lead to significant problems of interpretation and potentially of the functions that the warehouse is designed to deliver. The  effectiveness of database technologies to web-related information gathering and management applications is likewise limited by the degree to which they can accommodate semantics of the information being sought. Along the same lines, the emergence of organizational knowledge management as the next major application of computing in organizations clearly offers a tremendous opportunity for database technologies. But, again, this opportunity begs  the question whether such technologies can succeed if they continue to ignore semantic issues.In summary, semantic issues were put aside by database technologies of the past. However, the database application challenges of the '90s seem to demand solutions to precisely such issues today. This panel intends to examine these long standing research issues on database semantics and their failures to penetrate database technologies. The discussion will also review emerging  application areas and their need for mechanisms that deal with data semantics. Finally, the panelists will comment on relevant research tasks that need to be addressed in this long-ignored area of database modeling, management, access, and processing.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {497},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276349,
author = {Mylopoulos, John},
title = {Next Generation Database Systems Won't Work without Semantics! (Panel)},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276349},
doi = {10.1145/276305.276349},
abstract = {In the late '70s, while second generation DBMS products and technologies were entering the market, there was significant research activity whose aim was to make greater use of semantic information in database systems. The focus of that research was primarily on semantic data models and data modelling, including semantic query processing and integrity checking. However, few if any of the results of these efforts found their way in database technologies of the day, or database management practices. Instead, semantic issues were delegated to early phases of information system development (including requirements analysis and design), as well as application development.Today's database system technologies perform admirably well with semantically trivial operations and  representations. At the same time, these technologies are being challenged in virtually every area of data management, with new applications which demand ways of dealing more explicitly with the meaning of the data being managed. For example, interoperation between applications requires that the underlying databases interoperate meaningfully. This currently requires a mammoth manual reverse engineering effort that simply cannot be sustained or funded by any large organization. The same applies to data warehouses, since they too require the correct semantic merging of data from semantically diverse sources. Errors in merging these sources can lead to significant problems of interpretation and potentially of the functions that the warehouse is designed to deliver. The  effectiveness of database technologies to web-related information gathering and management applications is likewise limited by the degree to which they can accommodate semantics of the information being sought. Along the same lines, the emergence of organizational knowledge management as the next major application of computing in organizations clearly offers a tremendous opportunity for database technologies. But, again, this opportunity begs  the question whether such technologies can succeed if they continue to ignore semantic issues.In summary, semantic issues were put aside by database technologies of the past. However, the database application challenges of the '90s seem to demand solutions to precisely such issues today. This panel intends to examine these long standing research issues on database semantics and their failures to penetrate database technologies. The discussion will also review emerging  application areas and their need for mechanisms that deal with data semantics. Finally, the panelists will comment on relevant research tasks that need to be addressed in this long-ignored area of database modeling, management, access, and processing.},
journal = {SIGMOD Rec.},
month = jun,
pages = {497},
numpages = {1}
}

@inproceedings{10.1145/276304.276350,
author = {Adam, Nabil R. and Yesha, Yelena},
title = {Electronic Commerce: Tutorial},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276350},
doi = {10.1145/276304.276350},
abstract = {As we embark on the information age the use of electronic information is spreading through all sectors of society, both nationally and internationally. As a result, commercial organizations, educational institutions and government agencies are finding it essential to be linked by world wide networks, and commercial Internet usage is growing at an accelerating pace.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {498},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276350,
author = {Adam, Nabil R. and Yesha, Yelena},
title = {Electronic Commerce: Tutorial},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276350},
doi = {10.1145/276305.276350},
abstract = {As we embark on the information age the use of electronic information is spreading through all sectors of society, both nationally and internationally. As a result, commercial organizations, educational institutions and government agencies are finding it essential to be linked by world wide networks, and commercial Internet usage is growing at an accelerating pace.},
journal = {SIGMOD Rec.},
month = jun,
pages = {498},
numpages = {1}
}

@inproceedings{10.1145/276304.276351,
author = {Kemper, Alfons and Kossmann, Donald and Matthes, Florian},
title = {SAP R/3 (Tutorial): A Database Application System},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276351},
doi = {10.1145/276304.276351},
abstract = {Many database applications in the real world are no longer built on top of a stand-alone database system. Rather, generic (standard) application systems are employed in which the database system is one integrated component. SAP is the market leader for integrated business administration systems, and its SAP R/3 product is a comprehensive software system which integrates modules for finance, material management, sales and distribution, etc. From an architectural point of view, SAP R/3 is a client/server application system with a relational database system as back-end. SAP supports a choice between a variety of commercial relational database products.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {499},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276351,
author = {Kemper, Alfons and Kossmann, Donald and Matthes, Florian},
title = {SAP R/3 (Tutorial): A Database Application System},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276351},
doi = {10.1145/276305.276351},
abstract = {Many database applications in the real world are no longer built on top of a stand-alone database system. Rather, generic (standard) application systems are employed in which the database system is one integrated component. SAP is the market leader for integrated business administration systems, and its SAP R/3 product is a comprehensive software system which integrates modules for finance, material management, sales and distribution, etc. From an architectural point of view, SAP R/3 is a client/server application system with a relational database system as back-end. SAP supports a choice between a variety of commercial relational database products.},
journal = {SIGMOD Rec.},
month = jun,
pages = {499},
numpages = {1}
}

@inproceedings{10.1145/276304.276352,
author = {Clossman, Gray and Shaw, Phil and Hapner, Mark and Klein, Johannes and Pledereder, Richard and Becker, Brian},
title = {Java and Relational Databases (Tutorial): SQLJ},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276352},
doi = {10.1145/276304.276352},
abstract = {This Tutorial presents the latest developments in the area of Java and Relational Databases. The material is based on the SQLJ consortium effort whose goal is to leverage Java technology for SQL processing. The SQLJ effort is driven by major industry vendors such as Oracle, Sybase, Tandem, JavaSoft, IBM, Informix and others. The SQLJ specifications describe Embedded SQL in Java, Java Stored Procedures, Java UDFs and Java Data Types.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {500},
numpages = {1},
keywords = {JDBC, ORDBMS, SQLJ, Java, Java Relational, SQL},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276352,
author = {Clossman, Gray and Shaw, Phil and Hapner, Mark and Klein, Johannes and Pledereder, Richard and Becker, Brian},
title = {Java and Relational Databases (Tutorial): SQLJ},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276352},
doi = {10.1145/276305.276352},
abstract = {This Tutorial presents the latest developments in the area of Java and Relational Databases. The material is based on the SQLJ consortium effort whose goal is to leverage Java technology for SQL processing. The SQLJ effort is driven by major industry vendors such as Oracle, Sybase, Tandem, JavaSoft, IBM, Informix and others. The SQLJ specifications describe Embedded SQL in Java, Java Stored Procedures, Java UDFs and Java Data Types.},
journal = {SIGMOD Rec.},
month = jun,
pages = {500},
numpages = {1},
keywords = {Java, SQLJ, JDBC, SQL, ORDBMS, Java Relational}
}

@inproceedings{10.1145/276304.276353,
author = {Berchtold, Stefan and Keim, Daniel A.},
title = {High-Dimensional Index Structures Database Support for next Decade's Applications (Tutorial)},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276353},
doi = {10.1145/276304.276353},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {501},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276353,
author = {Berchtold, Stefan and Keim, Daniel A.},
title = {High-Dimensional Index Structures Database Support for next Decade's Applications (Tutorial)},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276353},
doi = {10.1145/276305.276353},
journal = {SIGMOD Rec.},
month = jun,
pages = {501},
numpages = {1}
}

@inproceedings{10.1145/276304.276354,
author = {Blakeley, Jos\'{e} A. and Pizzo, Michael J.},
title = {Microsoft Universal Data Access Platform},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276354},
doi = {10.1145/276304.276354},
abstract = {Microsoft Universal Data Access defines a platform for developing multi-tier enterprise applications that require efficient access to diverse relational or non-relational data sources across intranets or the Internet. Universal Data Access consists of a collection of software components that interact with each other using system-level interfaces defined by OLE DB and providing an application-level data access model called ActiveX Data Objects (ADO). This talk provides an overview of the platform.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {502–503},
numpages = {2},
keywords = {ODABC, OLE DB, database extensibility, component databases, ADO, spatial, data access and manipulation, OLAP},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276354,
author = {Blakeley, Jos\'{e} A. and Pizzo, Michael J.},
title = {Microsoft Universal Data Access Platform},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276354},
doi = {10.1145/276305.276354},
abstract = {Microsoft Universal Data Access defines a platform for developing multi-tier enterprise applications that require efficient access to diverse relational or non-relational data sources across intranets or the Internet. Universal Data Access consists of a collection of software components that interact with each other using system-level interfaces defined by OLE DB and providing an application-level data access model called ActiveX Data Objects (ADO). This talk provides an overview of the platform.},
journal = {SIGMOD Rec.},
month = jun,
pages = {502–503},
numpages = {2},
keywords = {data access and manipulation, component databases, OLAP, spatial, ODABC, OLE DB, ADO, database extensibility}
}

@inproceedings{10.1145/276304.276355,
author = {White, Seth and Cattell, Rick and Finkelstein, Shel},
title = {Enterprise Java Platform Data Access},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276355},
doi = {10.1145/276304.276355},
abstract = {This paper describes alternative methods for data access that are available to developers using the Java™ platform and related technologies to create a new generation of enterprise applications. The paper highlights industry trends and describes Java technologies that are responsible for a new paradigm in data access. Java technology represents a new level of portability, scalability, and ease-of-use for applications that require data access.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {504–505},
numpages = {2},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276355,
author = {White, Seth and Cattell, Rick and Finkelstein, Shel},
title = {Enterprise Java Platform Data Access},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276355},
doi = {10.1145/276305.276355},
abstract = {This paper describes alternative methods for data access that are available to developers using the Java™ platform and related technologies to create a new generation of enterprise applications. The paper highlights industry trends and describes Java technologies that are responsible for a new paradigm in data access. Java technology represents a new level of portability, scalability, and ease-of-use for applications that require data access.},
journal = {SIGMOD Rec.},
month = jun,
pages = {504–505},
numpages = {2}
}

@inproceedings{10.1145/276304.276356,
author = {Reinwald, Berthold and Pirahesh, Hamid},
title = {SQL Open Heterogeneous Data Access},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276356},
doi = {10.1145/276304.276356},
abstract = {We describe the open, extensible architecture of SQL for accessing data stored in external data sources not managed by the SQL engine. In this scenario, SQL engines act as middleware servers providing access to external data using SQL DML statements and joining external data with SQL tables in heterogeneous queries. We describe the state-of-the art in object-relational systems and their companion products, and provide an outlook on future directions.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {506–507},
numpages = {2},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276356,
author = {Reinwald, Berthold and Pirahesh, Hamid},
title = {SQL Open Heterogeneous Data Access},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276356},
doi = {10.1145/276305.276356},
abstract = {We describe the open, extensible architecture of SQL for accessing data stored in external data sources not managed by the SQL engine. In this scenario, SQL engines act as middleware servers providing access to external data using SQL DML statements and joining external data with SQL tables in heterogeneous queries. We describe the state-of-the art in object-relational systems and their companion products, and provide an outlook on future directions.},
journal = {SIGMOD Rec.},
month = jun,
pages = {506–507},
numpages = {2}
}

@inproceedings{10.1145/276304.276357,
author = {Eaton, Chris},
title = {Managing Large Systems with DB2 UDB},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276357},
doi = {10.1145/276304.276357},
abstract = {In this talk, we will describe the usability challenges facing large distributed corporations. As well, we will discuss what IBM's DB2 Universal Database is doing to address these complex issues.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {508–509},
numpages = {2},
keywords = {IBM DB2 UDB, usability},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276357,
author = {Eaton, Chris},
title = {Managing Large Systems with DB2 UDB},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276357},
doi = {10.1145/276305.276357},
abstract = {In this talk, we will describe the usability challenges facing large distributed corporations. As well, we will discuss what IBM's DB2 Universal Database is doing to address these complex issues.},
journal = {SIGMOD Rec.},
month = jun,
pages = {508–509},
numpages = {2},
keywords = {IBM DB2 UDB, usability}
}

@inproceedings{10.1145/276304.276358,
author = {Doherty, C. Gregory},
title = {Database Systems Management and Oracle8},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276358},
doi = {10.1145/276304.276358},
abstract = {Oracle's corporate mission is to enable the Information Age through network computing, a vision of broader access to information for all and the empowerment and increased productivity that can result. The technology implications of the network computing vision are ubiquitous access via low-cost appliances to smaller numbers of larger databases, accessed via professionally managed networks compliant with open internetworking protocols. The latest release of the Oracle data server, Oracle8, provides new technology for management of very large databases containing rich and user-defined data types, and is continuing to evolve to make it economically beneficial to store all forms of digital information in a database.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {510–511},
numpages = {2},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276358,
author = {Doherty, C. Gregory},
title = {Database Systems Management and Oracle8},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276358},
doi = {10.1145/276305.276358},
abstract = {Oracle's corporate mission is to enable the Information Age through network computing, a vision of broader access to information for all and the empowerment and increased productivity that can result. The technology implications of the network computing vision are ubiquitous access via low-cost appliances to smaller numbers of larger databases, accessed via professionally managed networks compliant with open internetworking protocols. The latest release of the Oracle data server, Oracle8, provides new technology for management of very large databases containing rich and user-defined data types, and is continuing to evolve to make it economically beneficial to store all forms of digital information in a database.},
journal = {SIGMOD Rec.},
month = jun,
pages = {510–511},
numpages = {2}
}

@inproceedings{10.1145/276304.276359,
author = {Spiro, Peter},
title = {Ubiquitous, Self-Tuning, Scalable Servers},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276359},
doi = {10.1145/276304.276359},
abstract = {Hardware developments allow wonderful reliability and essentially limitless capabilities in storage, networks, memory, and processing power. Costs have dropped dramatically. PCs are becoming ubiquitous.The features and scalability of DBMS software have advanced to the point where most commercial systems can solve virtually all OLTP and DSS requirements.The Internet and application software packages allow rapid deployment and facilitate a broad range of solutions.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {512–515},
numpages = {4},
keywords = {self-tuning database, managing data},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276359,
author = {Spiro, Peter},
title = {Ubiquitous, Self-Tuning, Scalable Servers},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276359},
doi = {10.1145/276305.276359},
abstract = {Hardware developments allow wonderful reliability and essentially limitless capabilities in storage, networks, memory, and processing power. Costs have dropped dramatically. PCs are becoming ubiquitous.The features and scalability of DBMS software have advanced to the point where most commercial systems can solve virtually all OLTP and DSS requirements.The Internet and application software packages allow rapid deployment and facilitate a broad range of solutions.},
journal = {SIGMOD Rec.},
month = jun,
pages = {512–515},
numpages = {4},
keywords = {self-tuning database, managing data}
}

@inproceedings{10.1145/276304.276360,
author = {Franklin, Michael and Zdonik, Stan},
title = {“Data in Your Face”: Push Technology in Perspective},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276360},
doi = {10.1145/276304.276360},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {516–519},
numpages = {4},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276360,
author = {Franklin, Michael and Zdonik, Stan},
title = {“Data in Your Face”: Push Technology in Perspective},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276360},
doi = {10.1145/276305.276360},
journal = {SIGMOD Rec.},
month = jun,
pages = {516–519},
numpages = {4}
}

@inproceedings{10.1145/276304.276361,
author = {Ramakrishnan, Satish and Dayal, Vibha},
title = {The PointCast Network (Abstract)},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276361},
doi = {10.1145/276304.276361},
abstract = {PointCast Inc, the inventor and leader in broadcast news via the Internet and corporate intranets was founded in 1992 to deliver news as it happens from leading sources such as CNN, the New York Times, Wall Street Journal Interactive Edition and more, directly to a viewer's computer screen.The PointCast Network is an integrated client/server system. The system give users control of selecting kinds of information the client retrieves and, within limits, the frequency of those retrievals.The system is divided into client and server segments, referred to as “PointCast Client” and “the DataCenter” respectively. PointCast client is a program that runs on the user's Internet-connected computer, and performs a number of functions in addition  to retrieving information from the DataCenter.The server side of the system, known as the PointCast DataCenter, supports the client by providing compelling content in a timely fashion. The Data Center is composed of multiple sites geographically distributed all over US, each carrying a number of industrial strength web servers called “PointServers”. The PointServers are highly customized and “infinitely” scalable to serve close to 200 million requests handled by the PointCast network in a day.The PointCast network receives content from over 100 different sources via satellite links or over the internet. A cluster of servers in the Data Center run customized processes round the clock which assimilate data from various sources to index, format  and store it in multiple content databases.This presentation will describe the basic plumbing of the PointCast Network and how some of the challenges of establishing one of the busiest data centers in the world were addressed and implemented. It will focus on following issues:
fault toleranceload balancingachieving scalability through pre-caching on serverspackaging information to optimize internet bandwidth usageminimizing data latency.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {520},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276361,
author = {Ramakrishnan, Satish and Dayal, Vibha},
title = {The PointCast Network (Abstract)},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276361},
doi = {10.1145/276305.276361},
abstract = {PointCast Inc, the inventor and leader in broadcast news via the Internet and corporate intranets was founded in 1992 to deliver news as it happens from leading sources such as CNN, the New York Times, Wall Street Journal Interactive Edition and more, directly to a viewer's computer screen.The PointCast Network is an integrated client/server system. The system give users control of selecting kinds of information the client retrieves and, within limits, the frequency of those retrievals.The system is divided into client and server segments, referred to as “PointCast Client” and “the DataCenter” respectively. PointCast client is a program that runs on the user's Internet-connected computer, and performs a number of functions in addition  to retrieving information from the DataCenter.The server side of the system, known as the PointCast DataCenter, supports the client by providing compelling content in a timely fashion. The Data Center is composed of multiple sites geographically distributed all over US, each carrying a number of industrial strength web servers called “PointServers”. The PointServers are highly customized and “infinitely” scalable to serve close to 200 million requests handled by the PointCast network in a day.The PointCast network receives content from over 100 different sources via satellite links or over the internet. A cluster of servers in the Data Center run customized processes round the clock which assimilate data from various sources to index, format  and store it in multiple content databases.This presentation will describe the basic plumbing of the PointCast Network and how some of the challenges of establishing one of the busiest data centers in the world were addressed and implemented. It will focus on following issues:
fault toleranceload balancingachieving scalability through pre-caching on serverspackaging information to optimize internet bandwidth usageminimizing data latency.},
journal = {SIGMOD Rec.},
month = jun,
pages = {520},
numpages = {1}
}

@inproceedings{10.1145/276304.276362,
author = {Chan, Arvola},
title = {Transactional Publish/Subscribe: The Proactive Multicast of Database Changes (Abstract)},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276362},
doi = {10.1145/276304.276362},
abstract = {For many years, TIBCO (the Information Bus Company) has pioneered the use of Publish/Subscribe—a form of push technology — to build flexible, real-time loosely-coupled distributed applications. Today, Publish/Subscribe is used by 300 of the world's largest financial institutions, deployed in 6 of the top 10 semiconductor manufacturer' factory floors, utilized in the implementation large-scale Internet services like Yahoo, Intuit, and ETrade, and chosen by many of the world's leading corporations as the enterprise infrastructure for integrating disparate applications. In this paper, we will:
Contrast the Publish/Subscribe event-driven interaction paradigm against the traditional demand-driven request-reply interaction paradigm;Explain the concepts of subject-based addressing and self-describing messages, the cornerstones of Publish/Subscribe;Describe the scalable implementation of Publish/Subscribe via multicast and broadcast, and the proposed Pragmatic General Multicast Internet standard; andCategorize the qualities of service needed by different kinds of event-driven applications.Today, TIBCO products support:
Reliable delivery for front-office applications which require update notifications only while they are online;Guaranteed delivery for back-office applications that cannot afford to lose messages despite network and application failures; andTransactionally guaranteed delivery for those applications that must update databases, consume messages on one set of subjects, and publish messages on another set of subjects, all within properly bracketed atomic transactions.Three different implementations of transactional Publish/Subscribe can be found in:
A generic, database independent implementation embodied in TIBCO's Enterprise Transaction Express (ETX) product. ETX optimizes two-phase commit for those applications that span a single database and the messaging system by using the last resource manager optimization. It also supports more complicated transactions by playing the role of an XA-compliant resource manager, leaving the transaction coordination to standard-based transaction monitors.An Informix Universal Server specific extension package called the TIBCO Message Blade. This extends the SQL language with TIBCO-provided User Defined Routines (UDRs) for synchronous Publish / Subscribe operations. In general, UDRs can be used inside stored procedures and triggers to publish and consume (potentially complex) structured messages. The need for two-phase commit is finessed by storing messages in the same database that houses application tables.A bidirectional bridge between Oracle 8's Advanced Queueing (AQ) facility and TIBCO's TIB/Rendezvous guaranteed message delivery implementation. Oracle AQ supports enqueue and dequeue operations to queues (actually implemented as Oracle tables) that can be performed as part of database transactions. The bridge dequeues from Oracle queues and republishes on the Information Bus. Conversely, the bridge subscribes to TIB/Rendezvous messages and enqueues them to Oracle queues for consumption by Oracle applications. Multiple bridges can be used to route AQ messages from one Oracle database to another.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {521},
numpages = {1},
keywords = {transactionally guaranteed delivery, event-driven, publish.subscribe, subject-based addressing, multicast, reliable delivery, guaranted delivery},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276362,
author = {Chan, Arvola},
title = {Transactional Publish/Subscribe: The Proactive Multicast of Database Changes (Abstract)},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276362},
doi = {10.1145/276305.276362},
abstract = {For many years, TIBCO (the Information Bus Company) has pioneered the use of Publish/Subscribe—a form of push technology — to build flexible, real-time loosely-coupled distributed applications. Today, Publish/Subscribe is used by 300 of the world's largest financial institutions, deployed in 6 of the top 10 semiconductor manufacturer' factory floors, utilized in the implementation large-scale Internet services like Yahoo, Intuit, and ETrade, and chosen by many of the world's leading corporations as the enterprise infrastructure for integrating disparate applications. In this paper, we will:
Contrast the Publish/Subscribe event-driven interaction paradigm against the traditional demand-driven request-reply interaction paradigm;Explain the concepts of subject-based addressing and self-describing messages, the cornerstones of Publish/Subscribe;Describe the scalable implementation of Publish/Subscribe via multicast and broadcast, and the proposed Pragmatic General Multicast Internet standard; andCategorize the qualities of service needed by different kinds of event-driven applications.Today, TIBCO products support:
Reliable delivery for front-office applications which require update notifications only while they are online;Guaranteed delivery for back-office applications that cannot afford to lose messages despite network and application failures; andTransactionally guaranteed delivery for those applications that must update databases, consume messages on one set of subjects, and publish messages on another set of subjects, all within properly bracketed atomic transactions.Three different implementations of transactional Publish/Subscribe can be found in:
A generic, database independent implementation embodied in TIBCO's Enterprise Transaction Express (ETX) product. ETX optimizes two-phase commit for those applications that span a single database and the messaging system by using the last resource manager optimization. It also supports more complicated transactions by playing the role of an XA-compliant resource manager, leaving the transaction coordination to standard-based transaction monitors.An Informix Universal Server specific extension package called the TIBCO Message Blade. This extends the SQL language with TIBCO-provided User Defined Routines (UDRs) for synchronous Publish / Subscribe operations. In general, UDRs can be used inside stored procedures and triggers to publish and consume (potentially complex) structured messages. The need for two-phase commit is finessed by storing messages in the same database that houses application tables.A bidirectional bridge between Oracle 8's Advanced Queueing (AQ) facility and TIBCO's TIB/Rendezvous guaranteed message delivery implementation. Oracle AQ supports enqueue and dequeue operations to queues (actually implemented as Oracle tables) that can be performed as part of database transactions. The bridge dequeues from Oracle queues and republishes on the Information Bus. Conversely, the bridge subscribes to TIB/Rendezvous messages and enqueues them to Oracle queues for consumption by Oracle applications. Multiple bridges can be used to route AQ messages from one Oracle database to another.},
journal = {SIGMOD Rec.},
month = jun,
pages = {521},
numpages = {1},
keywords = {transactionally guaranteed delivery, subject-based addressing, multicast, reliable delivery, guaranted delivery, event-driven, publish.subscribe}
}

@inproceedings{10.1145/276304.276364,
author = {Ng, KianSing and Liu, Huan and Kwah, HweeBong},
title = {A Data Mining Application: Customer Retention at the Port of Singapore Authority (PSA)},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276364},
doi = {10.1145/276304.276364},
abstract = {“Customer retention” is an important real-world problem in many sales and services related industries today. This work illustrates how we can integrate the various techniques of data-mining, such as decision-tree induction, deviation analysis and multiple concept-level association rules to form an intuitive and novel approach to gauging customer's loyalty and predicting their likelihood of defection. Immediate action taken against these “early-warnings” is often the key to the eventual retention or loss of the customers involved.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {522–525},
numpages = {4},
keywords = {customer retention, multiple concept-level association rules, decision-tree induction, deviation analysis},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276364,
author = {Ng, KianSing and Liu, Huan and Kwah, HweeBong},
title = {A Data Mining Application: Customer Retention at the Port of Singapore Authority (PSA)},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276364},
doi = {10.1145/276305.276364},
abstract = {“Customer retention” is an important real-world problem in many sales and services related industries today. This work illustrates how we can integrate the various techniques of data-mining, such as decision-tree induction, deviation analysis and multiple concept-level association rules to form an intuitive and novel approach to gauging customer's loyalty and predicting their likelihood of defection. Immediate action taken against these “early-warnings” is often the key to the eventual retention or loss of the customers involved.},
journal = {SIGMOD Rec.},
month = jun,
pages = {522–525},
numpages = {4},
keywords = {deviation analysis, multiple concept-level association rules, customer retention, decision-tree induction}
}

@inproceedings{10.1145/276304.276365,
author = {Anderson, Richard and Arun, Gopalan and Frank, Richard},
title = {Oracle Rdb's Record Caching Model},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276365},
doi = {10.1145/276304.276365},
abstract = {In this paper we present a more efficient record based caching model than the conventional page (disk block) based scheme. In a record caching model, individual records are stored together in a section of shared memory to form the cache. Traditional relational database systems have individual pages that are stored together in shared memory to form the cache and records are then extracted from these pages on demand. The record cache model has better memory utilization than the page model and also helps reduce overheads like page fetches/writes, page locks and code path.In May 1996, Oracle Rdb announced a record breaking number of 14227 tpmC on a Digital AlphaServer 8400. At the time, this was the best TPC-C performance achieved on a single SMP machine. A total of 15 record caches, caching 19.5 million records, consuming almost 7 GB of memory, formed the bulk of the shared memory.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {526–527},
numpages = {2},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276365,
author = {Anderson, Richard and Arun, Gopalan and Frank, Richard},
title = {Oracle Rdb's Record Caching Model},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276365},
doi = {10.1145/276305.276365},
abstract = {In this paper we present a more efficient record based caching model than the conventional page (disk block) based scheme. In a record caching model, individual records are stored together in a section of shared memory to form the cache. Traditional relational database systems have individual pages that are stored together in shared memory to form the cache and records are then extracted from these pages on demand. The record cache model has better memory utilization than the page model and also helps reduce overheads like page fetches/writes, page locks and code path.In May 1996, Oracle Rdb announced a record breaking number of 14227 tpmC on a Digital AlphaServer 8400. At the time, this was the best TPC-C performance achieved on a single SMP machine. A total of 15 record caches, caching 19.5 million records, consuming almost 7 GB of memory, formed the bulk of the shared memory.},
journal = {SIGMOD Rec.},
month = jun,
pages = {526–527},
numpages = {2}
}

@inproceedings{10.1145/276304.276366,
author = {Lahiri, Tirthankar and Joshi, Ashok and Jasuja, Amit and Chatterjee, Sumanta},
title = {50,000 Users on an Oracle8 Universal Server Database},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276366},
doi = {10.1145/276304.276366},
abstract = {In this paper, we describe the Oracle Large User Population Demonstration and highlight the scalability mechanisms in the Oracle8 Universal Data Server which make it possible to support as many as 50,000 concurrent users on a single Oracle8 database without any middle-tier TP-monitor software. Supporting such large user populations requires many mechanisms for high concurrency and throughput. Algorithms in all areas of the server ranging from process and buffer management to SQL compilation and execution must be designed to be highly scalable. Efficient resource sharing mechanisms are required to prevent server-side resource requirements from growing unboundedly with the number of users. Parallel execution across multiple systems is necessary to allow user-population and throughput to scale beyond the restrictions of a single system. In addition to scalability, mechanisms for high availability, ease-of-use, and rich functionality are necessary for supporting complex user applications typical of realistic workloads. All mechanisms must be portable to a wide variety of installations ranging from desk-top systems to large scale enterprise servers and to a wide variety of operating systems.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {528–530},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276366,
author = {Lahiri, Tirthankar and Joshi, Ashok and Jasuja, Amit and Chatterjee, Sumanta},
title = {50,000 Users on an Oracle8 Universal Server Database},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276366},
doi = {10.1145/276305.276366},
abstract = {In this paper, we describe the Oracle Large User Population Demonstration and highlight the scalability mechanisms in the Oracle8 Universal Data Server which make it possible to support as many as 50,000 concurrent users on a single Oracle8 database without any middle-tier TP-monitor software. Supporting such large user populations requires many mechanisms for high concurrency and throughput. Algorithms in all areas of the server ranging from process and buffer management to SQL compilation and execution must be designed to be highly scalable. Efficient resource sharing mechanisms are required to prevent server-side resource requirements from growing unboundedly with the number of users. Parallel execution across multiple systems is necessary to allow user-population and throughput to scale beyond the restrictions of a single system. In addition to scalability, mechanisms for high availability, ease-of-use, and rich functionality are necessary for supporting complex user applications typical of realistic workloads. All mechanisms must be portable to a wide variety of installations ranging from desk-top systems to large scale enterprise servers and to a wide variety of operating systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {528–530},
numpages = {3}
}

@inproceedings{10.1145/276304.276367,
author = {Aulakh, Kamar},
title = {About Quark Digital Media System},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276367},
doi = {10.1145/276304.276367},
abstract = {In this paper, we describe the Oracle Large User Population Demonstration and highlight the scalability mechanisms in the Oracle8 Universal Data Server which make it possible to support as many as 50,000 concurrent users on a single Oracle8 database without any middle-tier TP-monitor software. Supporting such large user populations requires many mechanisms for high concurrency and throughput. Algorithms in all areas of the server ranging from process and buffer management to SQL compilation and execution must be designed to be highly scalable. Efficient resource sharing mechanisms are required to prevent server-side resource requirements from growing unboundedly with the number of users. Parallel execution across multiple systems is necessary to allow user-population and throughput to scale beyond the restrictions of a single system. In addition to scalability, mechanisms for high availability, ease-of-use, and rich functionality are necessary for supporting complex user applications typical of realistic workloads. All mechanisms must be portable to a wide variety of installations ranging from desk-top systems to large scale enterprise servers and to a wide variety of operating systems.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {531–532},
numpages = {2},
keywords = {Quark Digital Media System, quark, QuarkDMS},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276367,
author = {Aulakh, Kamar},
title = {About Quark Digital Media System},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276367},
doi = {10.1145/276305.276367},
abstract = {In this paper, we describe the Oracle Large User Population Demonstration and highlight the scalability mechanisms in the Oracle8 Universal Data Server which make it possible to support as many as 50,000 concurrent users on a single Oracle8 database without any middle-tier TP-monitor software. Supporting such large user populations requires many mechanisms for high concurrency and throughput. Algorithms in all areas of the server ranging from process and buffer management to SQL compilation and execution must be designed to be highly scalable. Efficient resource sharing mechanisms are required to prevent server-side resource requirements from growing unboundedly with the number of users. Parallel execution across multiple systems is necessary to allow user-population and throughput to scale beyond the restrictions of a single system. In addition to scalability, mechanisms for high availability, ease-of-use, and rich functionality are necessary for supporting complex user applications typical of realistic workloads. All mechanisms must be portable to a wide variety of installations ranging from desk-top systems to large scale enterprise servers and to a wide variety of operating systems.},
journal = {SIGMOD Rec.},
month = jun,
pages = {531–532},
numpages = {2},
keywords = {Quark Digital Media System, QuarkDMS, quark}
}

@inproceedings{10.1145/276304.276368,
author = {Whelan, Daniel S.},
title = {FileNet Integrated Document Management Database Usage and Issues},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276368},
doi = {10.1145/276304.276368},
abstract = {The FileNet Integrated Document Management (IDM) products consists of a family of client applications and Imaging and Electronic Document Management (EDM) services. These services provide robust facilities for document creation, update, and deletion along with document search capabilities. Document properties are stored in an underlying relational database (RDBMS); document content is stored in files or in a specialized optical disk hierarchical storage manager. FileNet Corporation's Visual WorkFlo® and Ensemble® workflow products can be utilized in conjunction with FileNet's IDM technologies to automate production and ad hoc business processes respectively.This talk will discuss how Integrated Document Management requirements affect an IDM system's usage of a RDBMS. Some of the areas to be discussed include:},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {533},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276368,
author = {Whelan, Daniel S.},
title = {FileNet Integrated Document Management Database Usage and Issues},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276368},
doi = {10.1145/276305.276368},
abstract = {The FileNet Integrated Document Management (IDM) products consists of a family of client applications and Imaging and Electronic Document Management (EDM) services. These services provide robust facilities for document creation, update, and deletion along with document search capabilities. Document properties are stored in an underlying relational database (RDBMS); document content is stored in files or in a specialized optical disk hierarchical storage manager. FileNet Corporation's Visual WorkFlo® and Ensemble® workflow products can be utilized in conjunction with FileNet's IDM technologies to automate production and ad hoc business processes respectively.This talk will discuss how Integrated Document Management requirements affect an IDM system's usage of a RDBMS. Some of the areas to be discussed include:},
journal = {SIGMOD Rec.},
month = jun,
pages = {533},
numpages = {1}
}

@inproceedings{10.1145/276304.276369,
author = {Nauman, John and Suorsa, Ray},
title = {Developing a High Traffic, Read-Only Web Site},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276369},
doi = {10.1145/276304.276369},
abstract = {In this paper, we describe some of the considerations for designing highly trafficked web sites with read-only or read mostly characteristics.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {534–535},
numpages = {2},
keywords = {web site caching, highly trafficked web sites, stable sockets},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276369,
author = {Nauman, John and Suorsa, Ray},
title = {Developing a High Traffic, Read-Only Web Site},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276369},
doi = {10.1145/276305.276369},
abstract = {In this paper, we describe some of the considerations for designing highly trafficked web sites with read-only or read mostly characteristics.},
journal = {SIGMOD Rec.},
month = jun,
pages = {534–535},
numpages = {2},
keywords = {web site caching, stable sockets, highly trafficked web sites}
}

@inproceedings{10.1145/276304.276370,
author = {Chong, James},
title = {Real Business Processing Meets the Web},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276370},
doi = {10.1145/276304.276370},
abstract = {Charles Schwab and Co, Inc. is a major web trader generating a large proportion of its revenue from the Web. That revenue is based on both having a site with lots of useful facilities and also the speed of execution, ability to cope with peaks in demand volumes, and the reliability of the site and its underlying services. James Chong, VP Architecture and Planning at Schwab, will talk about the fundamental infrastructure that supports the Web trading, and his plans for its evolution.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {536},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276370,
author = {Chong, James},
title = {Real Business Processing Meets the Web},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276370},
doi = {10.1145/276305.276370},
abstract = {Charles Schwab and Co, Inc. is a major web trader generating a large proportion of its revenue from the Web. That revenue is based on both having a site with lots of useful facilities and also the speed of execution, ability to cope with peaks in demand volumes, and the reliability of the site and its underlying services. James Chong, VP Architecture and Planning at Schwab, will talk about the fundamental infrastructure that supports the Web trading, and his plans for its evolution.},
journal = {SIGMOD Rec.},
month = jun,
pages = {536},
numpages = {1}
}

@inproceedings{10.1145/276304.276371,
author = {Lassettre, Edwin R.},
title = {Olympic Records for Data at the 1998 Nagano Games},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276371},
doi = {10.1145/276304.276371},
abstract = {The 1998 Nagano Olympic games had more intensive demands on data management than any previous Olympics in history. This talk will take you behind the scenes to talk about the technical challenges and the architectures that made it possible to handle 4.5 Terabytes of data and sustain a total of almost 650 million web requests, reaching a peak of over 103K per minute. We will discuss the overall structure of the most comprehensive and heavily used Internet technology application in history. Many products were involved, both hardware and software, but this talk will focus in on the database and web challenges, the technology that made it possible to support this tremendous workload. High availability, data integrity, high performance, support of both SMPs and clustered architectures were among the features and functions that were critical. We will cover the Olympic Results System, the Commentator Information System, Info '98, Games Management, and the Olympic web site that made this information available to the Internet community. The speaker will be Ed Lassettre, IBM Fellow, and a key member of IBM's Olympic team.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {537},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276371,
author = {Lassettre, Edwin R.},
title = {Olympic Records for Data at the 1998 Nagano Games},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276371},
doi = {10.1145/276305.276371},
abstract = {The 1998 Nagano Olympic games had more intensive demands on data management than any previous Olympics in history. This talk will take you behind the scenes to talk about the technical challenges and the architectures that made it possible to handle 4.5 Terabytes of data and sustain a total of almost 650 million web requests, reaching a peak of over 103K per minute. We will discuss the overall structure of the most comprehensive and heavily used Internet technology application in history. Many products were involved, both hardware and software, but this talk will focus in on the database and web challenges, the technology that made it possible to support this tremendous workload. High availability, data integrity, high performance, support of both SMPs and clustered architectures were among the features and functions that were critical. We will cover the Olympic Results System, the Commentator Information System, Info '98, Games Management, and the Olympic web site that made this information available to the Internet community. The speaker will be Ed Lassettre, IBM Fellow, and a key member of IBM's Olympic team.},
journal = {SIGMOD Rec.},
month = jun,
pages = {537},
numpages = {1}
}

@inproceedings{10.1145/276304.276372,
author = {Brewer, Eric A.},
title = {Delivering High Availability for Inktomi Search Engines},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276372},
doi = {10.1145/276304.276372},
abstract = {Inktomi provides the back-end for several well-known search engines, including Wired's HotBot and Microsoft's MS Start page. The services are supported by a highly available cluster with more than 300 CPUs and several hundred disks.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {538},
numpages = {1},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276372,
author = {Brewer, Eric A.},
title = {Delivering High Availability for Inktomi Search Engines},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276372},
doi = {10.1145/276305.276372},
abstract = {Inktomi provides the back-end for several well-known search engines, including Wired's HotBot and Microsoft's MS Start page. The services are supported by a highly available cluster with more than 300 CPUs and several hundred disks.},
journal = {SIGMOD Rec.},
month = jun,
pages = {538},
numpages = {1}
}

@inproceedings{10.1145/276304.276373,
author = {Kennamer, Sherri},
title = {Microsoft.Com: A High-Scale Data Management and Transaction Processing Solution},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276373},
doi = {10.1145/276304.276373},
abstract = {Microsoft.com, is the world's largest corporate website both in terms of site visitors and pages served. Overall, it is the fourth-largest website in total visitors behind America Online, Yahoo and Netscape. We offer 250,000 pages of content, viewable in all major browser versions (yes, we aggressively support Netscape), supported by three server farms internationally and featuring content updated as often as every three hours, seven days a week.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {539–540},
numpages = {2},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276373,
author = {Kennamer, Sherri},
title = {Microsoft.Com: A High-Scale Data Management and Transaction Processing Solution},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276373},
doi = {10.1145/276305.276373},
abstract = {Microsoft.com, is the world's largest corporate website both in terms of site visitors and pages served. Overall, it is the fourth-largest website in total visitors behind America Online, Yahoo and Netscape. We offer 250,000 pages of content, viewable in all major browser versions (yes, we aggressively support Netscape), supported by three server farms internationally and featuring content updated as often as every three hours, seven days a week.},
journal = {SIGMOD Rec.},
month = jun,
pages = {539–540},
numpages = {2}
}

@inproceedings{10.1145/276304.276374,
author = {Li, Bin and Shasha, Dennis},
title = {Free Parallel Data Mining},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276374},
doi = {10.1145/276304.276374},
abstract = {Data mining is computationally expensive. Since the benefits of data mining results are unpredictable, organizations may not be willing to buy new hardware for that purpose. We will present a system that enables data mining applications to run in parallel on networks of workstations in a fault-tolerant manner. We will describe our parallelization of a combinatorial pattern discovery algorithm and a classification tree algorithm. We will demonstrate the effectiveness of our system with two real applications: discovering active motifs in protein sequences and predicting foreign exchange rate movement.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {541–543},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276374,
author = {Li, Bin and Shasha, Dennis},
title = {Free Parallel Data Mining},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276374},
doi = {10.1145/276305.276374},
abstract = {Data mining is computationally expensive. Since the benefits of data mining results are unpredictable, organizations may not be willing to buy new hardware for that purpose. We will present a system that enables data mining applications to run in parallel on networks of workstations in a fault-tolerant manner. We will describe our parallelization of a combinatorial pattern discovery algorithm and a classification tree algorithm. We will demonstrate the effectiveness of our system with two real applications: discovering active motifs in protein sequences and predicting foreign exchange rate movement.},
journal = {SIGMOD Rec.},
month = jun,
pages = {541–543},
numpages = {3}
}

@inproceedings{10.1145/276304.276375,
author = {Mecca, G. and Atzeni, P. and Masci, A. and Sindoni, G. and Merialdo, P.},
title = {The Araneus Web-Based Management System},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276375},
doi = {10.1145/276304.276375},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {544–546},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276375,
author = {Mecca, G. and Atzeni, P. and Masci, A. and Sindoni, G. and Merialdo, P.},
title = {The Araneus Web-Based Management System},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276375},
doi = {10.1145/276305.276375},
journal = {SIGMOD Rec.},
month = jun,
pages = {544–546},
numpages = {3}
}

@inproceedings{10.1145/276304.276376,
author = {Liu, Ling and Pu, Calton and Tang, Wei and Buttler, David and Biggs, John and Zhou, Tong and Benninghoff, Paul and Han, Wei and Yu, Fenghua},
title = {CQ: A Personalized Update Monitoring Toolkit},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276376},
doi = {10.1145/276304.276376},
abstract = {The CQ project at OGI, funded by DARPA, aims at developing a scalable toolkit and techniques for update monitoring and event-driven information delivery on the net. The main feature of the CQ project is a “personalized update monitoring” toolkit based on continual queries [3]. Comparing with the pure pull (such as DBMSs, various web search engines) and pure push (such as Pointcast, Marimba, Broadcast disks) technology, the CQ project can be seen as a hybrid approach that combines the pull and push technology by supporting personalized update monitoring through a combined client-pull and server-push paradigm.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {547–549},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276376,
author = {Liu, Ling and Pu, Calton and Tang, Wei and Buttler, David and Biggs, John and Zhou, Tong and Benninghoff, Paul and Han, Wei and Yu, Fenghua},
title = {CQ: A Personalized Update Monitoring Toolkit},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276376},
doi = {10.1145/276305.276376},
abstract = {The CQ project at OGI, funded by DARPA, aims at developing a scalable toolkit and techniques for update monitoring and event-driven information delivery on the net. The main feature of the CQ project is a “personalized update monitoring” toolkit based on continual queries [3]. Comparing with the pure pull (such as DBMSs, various web search engines) and pure push (such as Pointcast, Marimba, Broadcast disks) technology, the CQ project can be seen as a hybrid approach that combines the pull and push technology by supporting personalized update monitoring through a combined client-pull and server-push paradigm.},
journal = {SIGMOD Rec.},
month = jun,
pages = {547–549},
numpages = {3}
}

@inproceedings{10.1145/276304.276377,
author = {Olston, Chris and Woodruff, Allison and Aiken, Alexander and Chu, Michael and Ercegovac, Vuk and Lin, Mark and Spalding, Mybrid and Stonebraker, Michael},
title = {DataSplash},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276377},
doi = {10.1145/276304.276377},
abstract = {Database visualization is an area of growing importance as database systems become larger and more accessible. DataSplash is an easy-to-use, integrated environment for navigating, creating, and querying visual representations of data. We will demonstrate the three main components which make up the DataSplash environment: a navigation system, a direct-manipulation interface for creating and modifying visualizations, and a direct-manipulation visual query system.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {550–552},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276377,
author = {Olston, Chris and Woodruff, Allison and Aiken, Alexander and Chu, Michael and Ercegovac, Vuk and Lin, Mark and Spalding, Mybrid and Stonebraker, Michael},
title = {DataSplash},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276377},
doi = {10.1145/276305.276377},
abstract = {Database visualization is an area of growing importance as database systems become larger and more accessible. DataSplash is an easy-to-use, integrated environment for navigating, creating, and querying visual representations of data. We will demonstrate the three main components which make up the DataSplash environment: a navigation system, a direct-manipulation interface for creating and modifying visualizations, and a direct-manipulation visual query system.},
journal = {SIGMOD Rec.},
month = jun,
pages = {550–552},
numpages = {3}
}

@inproceedings{10.1145/276304.276378,
author = {Chaudhuri, Surajit and Narasayya, Vivek},
title = {Microsoft Index Turning Wizard for SQL Server 7.0},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276378},
doi = {10.1145/276304.276378},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {553–554},
numpages = {2},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276378,
author = {Chaudhuri, Surajit and Narasayya, Vivek},
title = {Microsoft Index Turning Wizard for SQL Server 7.0},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276378},
doi = {10.1145/276305.276378},
journal = {SIGMOD Rec.},
month = jun,
pages = {553–554},
numpages = {2}
}

@inproceedings{10.1145/276304.276379,
author = {Kiepuszewski, Bartek and Muhlberger, Ralf and Orlowska, Maria E.},
title = {FlowBack: Providing Backward Recovery for Workflow Management Systems},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276379},
doi = {10.1145/276304.276379},
abstract = {The Distributed Systems Technology Centre (DSTC) framework for workflow specification, verification and management captures workflows transaction-like behavior for long lasting processes. FlowBack is an advanced prototype functionally enhancing an existing workflow management system by providing process backward recovery. It is based on extensive theoretical research ([3],[4],[5],[6],[8],[9]), and its architecture and construction assumptions are product independent. FlowBack clearly demonstrates the extent to which generic backward recovery can be automated and system supported. The provision of a solution for handling exceptional business process behavior requiring backward recovery makes workflow solutions more suitable for a large class of applications, therefore opening up new dimensions within the market. For the demonstration purpose, FlowBack operates with IBM FlowMark, one of the leading workflow products.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {555–557},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276379,
author = {Kiepuszewski, Bartek and Muhlberger, Ralf and Orlowska, Maria E.},
title = {FlowBack: Providing Backward Recovery for Workflow Management Systems},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276379},
doi = {10.1145/276305.276379},
abstract = {The Distributed Systems Technology Centre (DSTC) framework for workflow specification, verification and management captures workflows transaction-like behavior for long lasting processes. FlowBack is an advanced prototype functionally enhancing an existing workflow management system by providing process backward recovery. It is based on extensive theoretical research ([3],[4],[5],[6],[8],[9]), and its architecture and construction assumptions are product independent. FlowBack clearly demonstrates the extent to which generic backward recovery can be automated and system supported. The provision of a solution for handling exceptional business process behavior requiring backward recovery makes workflow solutions more suitable for a large class of applications, therefore opening up new dimensions within the market. For the demonstration purpose, FlowBack operates with IBM FlowMark, one of the leading workflow products.},
journal = {SIGMOD Rec.},
month = jun,
pages = {555–557},
numpages = {3}
}

@inproceedings{10.1145/276304.276380,
author = {Cohen, William W.},
title = {Providing Database-like Access to the Web Using Queries Based on Textual Similarity},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276380},
doi = {10.1145/276304.276380},
abstract = {Most databases contain “name constants” like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. Here we assume instead that the names are given in natural language text. We then propose a logic for database integration called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. An implemented data integration system based on WHIRL has been used to successfully integrate information from several dozen Web sites in two domains.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {558–560},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276380,
author = {Cohen, William W.},
title = {Providing Database-like Access to the Web Using Queries Based on Textual Similarity},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276380},
doi = {10.1145/276305.276380},
abstract = {Most databases contain “name constants” like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. Here we assume instead that the names are given in natural language text. We then propose a logic for database integration called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. An implemented data integration system based on WHIRL has been used to successfully integrate information from several dozen Web sites in two domains.},
journal = {SIGMOD Rec.},
month = jun,
pages = {558–560},
numpages = {3}
}

@inproceedings{10.1145/276304.276381,
author = {Ambite, Jos\'{e} Luis and Ashish, Naveen and Barish, Greg and Knoblock, Craig A. and Minton, Steven and Modi, Pragnesh J. and Muslea, Ion and Philpot, Andrew and Tejada, Sheila},
title = {Ariadne: A System for Constructing Mediators for Internet Sources},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276381},
doi = {10.1145/276304.276381},
abstract = {The Web is based on a browsing paradigm that makes it difficult to retrieve and integrate data from multiple sites. Today, the only way to achieve this integration is by building specialized applications, which are time-consuming to develop and difficult to maintain. We are addressing this problem by creating the technology and tools for rapidly constructing information mediators that extract, query, and integrate data from web sources. The resulting system, called Ariadne, makes it feasible to rapidly build information mediators that access existing web sources.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {561–563},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276381,
author = {Ambite, Jos\'{e} Luis and Ashish, Naveen and Barish, Greg and Knoblock, Craig A. and Minton, Steven and Modi, Pragnesh J. and Muslea, Ion and Philpot, Andrew and Tejada, Sheila},
title = {Ariadne: A System for Constructing Mediators for Internet Sources},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276381},
doi = {10.1145/276305.276381},
abstract = {The Web is based on a browsing paradigm that makes it difficult to retrieve and integrate data from multiple sites. Today, the only way to achieve this integration is by building specialized applications, which are time-consuming to develop and difficult to maintain. We are addressing this problem by creating the technology and tools for rapidly constructing information mediators that extract, query, and integrate data from web sources. The resulting system, called Ariadne, makes it feasible to rapidly build information mediators that access existing web sources.},
journal = {SIGMOD Rec.},
month = jun,
pages = {561–563},
numpages = {3}
}

@inproceedings{10.1145/276304.276382,
author = {Li, Chen and Yerneni, Ramana and Vassalos, Vasilis and Garcia-Molina, Hector and Papakonstantinou, Yannis and Ullman, Jeffrey and Valiveti, Murty},
title = {Capability Based Mediation in TSIMMIS},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276382},
doi = {10.1145/276304.276382},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {564–566},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276382,
author = {Li, Chen and Yerneni, Ramana and Vassalos, Vasilis and Garcia-Molina, Hector and Papakonstantinou, Yannis and Ullman, Jeffrey and Valiveti, Murty},
title = {Capability Based Mediation in TSIMMIS},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276382},
doi = {10.1145/276305.276382},
journal = {SIGMOD Rec.},
month = jun,
pages = {564–566},
numpages = {3}
}

@inproceedings{10.1145/276304.276383,
author = {Avnur, Ron and Hellerstein, Joseph M. and Lo, Bruce and Olston, Chris and Raman, Bhaskaran and Raman, Vijayshankar and Roth, Tali and Wylie, Kirk},
title = {CONTROL: Continuous Output and Navigation Technology with Refinement on-Line},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276383},
doi = {10.1145/276304.276383},
abstract = {The CONTROL project at U.C. Berkeley has developed technologies to provide online behavior for data-intensive applications. Using new query processing algorithms, these technologies continuously improve estimates and confidence statistics. In addition, they react to user feedback, thereby giving the user control over the behavior of long-running operations. This demonstration displays the modifications to a database system and the resulting impact on aggregation queries, data visualization, and GUI widgets. We then compare this interactive behavior to batch-processing alternatives.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {567–569},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276383,
author = {Avnur, Ron and Hellerstein, Joseph M. and Lo, Bruce and Olston, Chris and Raman, Bhaskaran and Raman, Vijayshankar and Roth, Tali and Wylie, Kirk},
title = {CONTROL: Continuous Output and Navigation Technology with Refinement on-Line},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276383},
doi = {10.1145/276305.276383},
abstract = {The CONTROL project at U.C. Berkeley has developed technologies to provide online behavior for data-intensive applications. Using new query processing algorithms, these technologies continuously improve estimates and confidence statistics. In addition, they react to user feedback, thereby giving the user control over the behavior of long-running operations. This demonstration displays the modifications to a database system and the resulting impact on aggregation queries, data visualization, and GUI widgets. We then compare this interactive behavior to batch-processing alternatives.},
journal = {SIGMOD Rec.},
month = jun,
pages = {567–569},
numpages = {3}
}

@inproceedings{10.1145/276304.276384,
author = {Kornacker, Marcel and Shah, Mehul and Hellerstein, Joseph M.},
title = {AMDB: An Access Method Debugging Tool},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276384},
doi = {10.1145/276304.276384},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {570–571},
numpages = {2},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276384,
author = {Kornacker, Marcel and Shah, Mehul and Hellerstein, Joseph M.},
title = {AMDB: An Access Method Debugging Tool},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276384},
doi = {10.1145/276305.276384},
journal = {SIGMOD Rec.},
month = jun,
pages = {570–571},
numpages = {2}
}

@inproceedings{10.1145/276304.276385,
author = {Lacroix, Zo\'{e} and Sahuguet, Arnaud and Chandrasekar, Raman},
title = {User-Oriented Smart-Cache for the Web: What You Seek is What You Get!},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276385},
doi = {10.1145/276304.276385},
abstract = {Standard database approaches to querying information on the Web focus on the source(s) and provide a query language based on a given predefined organization (schema) of the data: this is the source-driven approach. However, can the Web be seen as a standard database? There is no super-user in charge of monitoring the source(s) (the data is constantly updated), there is no homogeneous structure (no common explicit structure thus), the Web itself never stops growing, etc. For these reasons, we believe that the source-driven standard approach is not suitable to the Web.As an alternative, we propose a user-oriented approach based on the idea that the schema is a posteriori expressed by the user's needs when asking a query. Given a user query, AKIRA (Agentive Knowledge-based Information Retrieval Architecture) [6] extracts a target structure (structure expressed in the query) and uses standard information retrieval and filtering techniques to access potentially relevant documents.The user-oriented paradigm means that the structure through which the data is viewed does not come from the source but is extracted from the user query. When a user asks a query, the relevant information is retrieved from the Web and stored as is in a cache. Then the information is extracted from the raw data using computational linguistic techniques. The AKIRA cache (smart-cache) represents these extracted layers of meta-information on top of the raw data.  The smart-cache is an object-oriented database whose schema is inferred from the user's target structure. It is designed on demand through a library of concepts that can be assembled together to match concepts and meta-concepts required in the user's query. The smart cache can be seen as a view of the Web.To the best of our knowledge, AKIRA is the only system that uses information retrieval and extraction integrated with database techniques to provide maximum flexibility to the user and offer transparent access to the content of Web documents.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {572–574},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276385,
author = {Lacroix, Zo\'{e} and Sahuguet, Arnaud and Chandrasekar, Raman},
title = {User-Oriented Smart-Cache for the Web: What You Seek is What You Get!},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276385},
doi = {10.1145/276305.276385},
abstract = {Standard database approaches to querying information on the Web focus on the source(s) and provide a query language based on a given predefined organization (schema) of the data: this is the source-driven approach. However, can the Web be seen as a standard database? There is no super-user in charge of monitoring the source(s) (the data is constantly updated), there is no homogeneous structure (no common explicit structure thus), the Web itself never stops growing, etc. For these reasons, we believe that the source-driven standard approach is not suitable to the Web.As an alternative, we propose a user-oriented approach based on the idea that the schema is a posteriori expressed by the user's needs when asking a query. Given a user query, AKIRA (Agentive Knowledge-based Information Retrieval Architecture) [6] extracts a target structure (structure expressed in the query) and uses standard information retrieval and filtering techniques to access potentially relevant documents.The user-oriented paradigm means that the structure through which the data is viewed does not come from the source but is extracted from the user query. When a user asks a query, the relevant information is retrieved from the Web and stored as is in a cache. Then the information is extracted from the raw data using computational linguistic techniques. The AKIRA cache (smart-cache) represents these extracted layers of meta-information on top of the raw data.  The smart-cache is an object-oriented database whose schema is inferred from the user's target structure. It is designed on demand through a library of concepts that can be assembled together to match concepts and meta-concepts required in the user's query. The smart cache can be seen as a view of the Web.To the best of our knowledge, AKIRA is the only system that uses information retrieval and extraction integrated with database techniques to provide maximum flexibility to the user and offer transparent access to the content of Web documents.},
journal = {SIGMOD Rec.},
month = jun,
pages = {572–574},
numpages = {3}
}

@inproceedings{10.1145/276304.276386,
author = {Baumann, P. and Dehmel, A. and Furtado, P. and Ritsch, R. and Widmann, N.},
title = {The Multidimensional Database System RasDaMan},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276386},
doi = {10.1145/276304.276386},
abstract = {RasDaMan is a universal — i.e., domain-independent — array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.RasDaMan is being used in several international projects for the management of geo and healthcare data of various dimensionality.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {575–577},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276386,
author = {Baumann, P. and Dehmel, A. and Furtado, P. and Ritsch, R. and Widmann, N.},
title = {The Multidimensional Database System RasDaMan},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276386},
doi = {10.1145/276305.276386},
abstract = {RasDaMan is a universal — i.e., domain-independent — array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.RasDaMan is being used in several international projects for the management of geo and healthcare data of various dimensionality.},
journal = {SIGMOD Rec.},
month = jun,
pages = {575–577},
numpages = {3}
}

@inproceedings{10.1145/276304.276387,
author = {Park, Jang Ho and Kwon, Yong Sik and Kim, Ki Hong and Lee, Sang Ho and Park, Byoung Dae and Cha, Sang Kyun},
title = {Xmas: An Extensible Main-Memory Storage System for High-Performance Applications},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276387},
doi = {10.1145/276304.276387},
abstract = {Xmas is an extensible main-memory storage system for high-performance embedded database applications. Xmas not only provides the core functionality of DBMS, such as data persistence, crash recovery, and concurrency control, but also pursues an extensible architecture to meet the requirements from various application areas. One crucial aspect of such extensibility is that an application developer can compose application-specific, high-level operations with a basic set of operations provided by the system. Called composite actions in Xmas, these operations are processed by a customized Xmas server with minimum interaction with application processes, thus improving the overall performance. This paper first presents the architecture and functionality of Xmas, and then demonstrates a simulation of mobile communication service.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {578–580},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276387,
author = {Park, Jang Ho and Kwon, Yong Sik and Kim, Ki Hong and Lee, Sang Ho and Park, Byoung Dae and Cha, Sang Kyun},
title = {Xmas: An Extensible Main-Memory Storage System for High-Performance Applications},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276387},
doi = {10.1145/276305.276387},
abstract = {Xmas is an extensible main-memory storage system for high-performance embedded database applications. Xmas not only provides the core functionality of DBMS, such as data persistence, crash recovery, and concurrency control, but also pursues an extensible architecture to meet the requirements from various application areas. One crucial aspect of such extensibility is that an application developer can compose application-specific, high-level operations with a basic set of operations provided by the system. Called composite actions in Xmas, these operations are processed by a customized Xmas server with minimum interaction with application processes, thus improving the overall performance. This paper first presents the architecture and functionality of Xmas, and then demonstrates a simulation of mobile communication service.},
journal = {SIGMOD Rec.},
month = jun,
pages = {578–580},
numpages = {3}
}

@inproceedings{10.1145/276304.276388,
author = {Za\"{\i}ane, Osmar R. and Han, Jiawei and Li, Ze-Nian and Chee, Sonny H. and Chiang, Jenny Y.},
title = {MultiMediaMiner: A System Prototype for Multimedia Data Mining},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276388},
doi = {10.1145/276304.276388},
abstract = {Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {581–583},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276388,
author = {Za\"{\i}ane, Osmar R. and Han, Jiawei and Li, Ze-Nian and Chee, Sonny H. and Chiang, Jenny Y.},
title = {MultiMediaMiner: A System Prototype for Multimedia Data Mining},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276388},
doi = {10.1145/276305.276388},
abstract = {Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.},
journal = {SIGMOD Rec.},
month = jun,
pages = {581–583},
numpages = {3}
}

@inproceedings{10.1145/276304.276389,
author = {Toyama, Motomichi},
title = {SuperSQL: An Extended SQL for Database Publishing and Presentation},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276389},
doi = {10.1145/276304.276389},
abstract = {SuperSQL is an extension of SQL that allows query results presented in various media for publishing and presentations with simple but sophisticated formatting capabilities. SuperSQL query can generate various kinds of materials, for example, a LaTeX source file to publish query results in a nested table, HTML or Java source files to present the result on WWW browsers, and other media including MS-Excel worksheet, Tcl/Tk, O2C, etc. O2C is a data manipulation language of O2 and thus useful to migrate data in a relational database to an object oriented database.SuperSQL is meant to provide a theoretical and practical foundation for 4GL-type applications such as report writers and DB/WWW coordinators.In this demonstration, we show how TFE reorganize the query results into various media in a universal way, first by grouping tuples according to an arbitrary tree structured schema, and by translating them with the constructors available in the target media.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {584–586},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276389,
author = {Toyama, Motomichi},
title = {SuperSQL: An Extended SQL for Database Publishing and Presentation},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276389},
doi = {10.1145/276305.276389},
abstract = {SuperSQL is an extension of SQL that allows query results presented in various media for publishing and presentations with simple but sophisticated formatting capabilities. SuperSQL query can generate various kinds of materials, for example, a LaTeX source file to publish query results in a nested table, HTML or Java source files to present the result on WWW browsers, and other media including MS-Excel worksheet, Tcl/Tk, O2C, etc. O2C is a data manipulation language of O2 and thus useful to migrate data in a relational database to an object oriented database.SuperSQL is meant to provide a theoretical and practical foundation for 4GL-type applications such as report writers and DB/WWW coordinators.In this demonstration, we show how TFE reorganize the query results into various media in a universal way, first by grouping tuples according to an arbitrary tree structured schema, and by translating them with the constructors available in the target media.},
journal = {SIGMOD Rec.},
month = jun,
pages = {584–586},
numpages = {3}
}

@inproceedings{10.1145/276304.276390,
author = {Ceri, Stefano and Fraternali, Piero and Paraboschi, Stefano},
title = {The IDEA Web Lab},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276390},
doi = {10.1145/276304.276390},
abstract = {With the spreading of the World Wide Web as a uniform and ubiquitous interface to computer applications and information, novel opportunities are offered for introducing significant changes in all organizations and their processes. This demo presents the IDEA Web Laboratory (Web Lab), a Web-based software design environment available on the Internet, which demonstrates a novel approach to the software production process on the Web.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {587–589},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276390,
author = {Ceri, Stefano and Fraternali, Piero and Paraboschi, Stefano},
title = {The IDEA Web Lab},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276390},
doi = {10.1145/276305.276390},
abstract = {With the spreading of the World Wide Web as a uniform and ubiquitous interface to computer applications and information, novel opportunities are offered for introducing significant changes in all organizations and their processes. This demo presents the IDEA Web Laboratory (Web Lab), a Web-based software design environment available on the Internet, which demonstrates a novel approach to the software production process on the Web.},
journal = {SIGMOD Rec.},
month = jun,
pages = {587–589},
numpages = {3}
}

@inproceedings{10.1145/276304.276391,
author = {Dar, Shaul and Entin, Gadi and Geva, Shai and Palmon, Eran},
title = {DTL's DataSpot: Database Exploration as Easy as Browsing the Web…},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276391},
doi = {10.1145/276304.276391},
abstract = {DTL's DataSpot is an advanced, programming-free tool that lets Web designers and database developers automatically publish their databases for Web browser access. DataSpot enables non-technical end users to explore a database using free-form plain language queries combined with hypertext navigation, in a fashion similar to using search engines such as Alta Vista to search text files on the Internet.DataSpot is based on a novel representation of data in the form of a schema-less semi-structured graph called a Web View. The DataSpot Publisher takes one or more possibly heterogeneous databases, predefined knowledge banks such as a thesaurus, and user-defined associations, and creates the Web View. The DataSpot Search Server, which connects to any standard HTTP server, performs searches and navigation against the Web View, generating dynamic HTML pages that are returned to the user. The presentation and navigation of answers are controlled by templates that can be modified by the data provider.The DataSpot product has been successfully deployed in diverse Internet and Intranet application areas, including electronic catalogs, yellow pages, classified ads, help desks and finance.},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {590–592},
numpages = {3},
keywords = {database publishing, navigation, search, Internet},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276391,
author = {Dar, Shaul and Entin, Gadi and Geva, Shai and Palmon, Eran},
title = {DTL's DataSpot: Database Exploration as Easy as Browsing the Web…},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276391},
doi = {10.1145/276305.276391},
abstract = {DTL's DataSpot is an advanced, programming-free tool that lets Web designers and database developers automatically publish their databases for Web browser access. DataSpot enables non-technical end users to explore a database using free-form plain language queries combined with hypertext navigation, in a fashion similar to using search engines such as Alta Vista to search text files on the Internet.DataSpot is based on a novel representation of data in the form of a schema-less semi-structured graph called a Web View. The DataSpot Publisher takes one or more possibly heterogeneous databases, predefined knowledge banks such as a thesaurus, and user-defined associations, and creates the Web View. The DataSpot Search Server, which connects to any standard HTTP server, performs searches and navigation against the Web View, generating dynamic HTML pages that are returned to the user. The presentation and navigation of answers are controlled by templates that can be modified by the data provider.The DataSpot product has been successfully deployed in diverse Internet and Intranet application areas, including electronic catalogs, yellow pages, classified ads, help desks and finance.},
journal = {SIGMOD Rec.},
month = jun,
pages = {590–592},
numpages = {3},
keywords = {navigation, search, database publishing, Internet}
}

@inproceedings{10.1145/276304.276392,
author = {Chen, Jing and Wong, Limsoon and Zhang, Louxin},
title = {A Protein Patent Query System Powered by Kleisli},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276392},
doi = {10.1145/276304.276392},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {593–595},
numpages = {3},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@article{10.1145/276305.276392,
author = {Chen, Jing and Wong, Limsoon and Zhang, Louxin},
title = {A Protein Patent Query System Powered by Kleisli},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276392},
doi = {10.1145/276305.276392},
journal = {SIGMOD Rec.},
month = jun,
pages = {593–595},
numpages = {3}
}

