@inproceedings{10.1145/50202.50203,
author = {Swami, Arun and Gupta, Anoop},
title = {Optimization of Large Join Queries},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50203},
doi = {10.1145/50202.50203},
abstract = {We investigate the problem of optimizing Select—Project—Join queries with large numbers of joins. Taking advantage of commonly used heuristics, the problem is reduced to that of determining the optimal join order. This is a hard combinatorial optimization problem. Some general techniques, such as iterative improvement and simulated annealing, have often proved effective in attacking a wide variety of combinatorial optimization problems. In this paper, we apply these general algorithms to the large join query optimization problem. We use the statistical techniques of factorial experiments and analysis of variance (ANOVA) to obtain reliable values for the parameters of these algorithms and to compare these algorithms. One interesting result of our experiments is that the relatively simple iterative improvement proves to be better than all the other algorithms (included the more complex simulated annealing). We also find that the general algorithms do quite well at the maximum time limit.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {8–17},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50203,
author = {Swami, Arun and Gupta, Anoop},
title = {Optimization of Large Join Queries},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50203},
doi = {10.1145/971701.50203},
abstract = {We investigate the problem of optimizing Select—Project—Join queries with large numbers of joins. Taking advantage of commonly used heuristics, the problem is reduced to that of determining the optimal join order. This is a hard combinatorial optimization problem. Some general techniques, such as iterative improvement and simulated annealing, have often proved effective in attacking a wide variety of combinatorial optimization problems. In this paper, we apply these general algorithms to the large join query optimization problem. We use the statistical techniques of factorial experiments and analysis of variance (ANOVA) to obtain reliable values for the parameters of these algorithms and to compare these algorithms. One interesting result of our experiments is that the relatively simple iterative improvement proves to be better than all the other algorithms (included the more complex simulated annealing). We also find that the general algorithms do quite well at the maximum time limit.},
journal = {SIGMOD Rec.},
month = jun,
pages = {8–17},
numpages = {10}
}

@inproceedings{10.1145/50202.50204,
author = {Lohman, Guy M.},
title = {Grammar-like Functional Rules for Representing Query Optimization Alternatives},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50204},
doi = {10.1145/50202.50204},
abstract = {Extensible query optimization requires that the “repertoire” of alternative strategies for executing queries be represented as data, not embedded in the optimizer code. Recognizing that query optimizers are essentially expert systems, several researchers have suggested using strategy rules to transform query execution plans into alternative or better plans. Though extremely flexible, these systems can be very inefficient at any step in the processing, many rules may be eligible for application and complicated conditions must be tested to determine that eligibility during unification. We present a constructive, “building blocks” approach to defining alternative plans, in which the rules defining alternatives are an extension of the productions of a grammar to resemble the definition of a function in mathematics. The extensions permit each token of the grammar to be parametrized and each of its alternative definitions to have a complex condition. The terminals of the grammar are base-level database operations on tables that are interpreted at run-time. The non-terminals are defined declaratively by production rules that combine those operations into meaningful plans for execution. Each production produces a set of alternative plans, each having a vector of properties, including the estimated cost of producing that plan. Productions can require certain properties of their inputs, such as tuple order and location, and we describe a “glue” mechanism for augmenting plans to achieve the required properties. We give detailed examples to illustrate the power and robustness of our rules and to contrast them with related ideas.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {18–27},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50204,
author = {Lohman, Guy M.},
title = {Grammar-like Functional Rules for Representing Query Optimization Alternatives},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50204},
doi = {10.1145/971701.50204},
abstract = {Extensible query optimization requires that the “repertoire” of alternative strategies for executing queries be represented as data, not embedded in the optimizer code. Recognizing that query optimizers are essentially expert systems, several researchers have suggested using strategy rules to transform query execution plans into alternative or better plans. Though extremely flexible, these systems can be very inefficient at any step in the processing, many rules may be eligible for application and complicated conditions must be tested to determine that eligibility during unification. We present a constructive, “building blocks” approach to defining alternative plans, in which the rules defining alternatives are an extension of the productions of a grammar to resemble the definition of a function in mathematics. The extensions permit each token of the grammar to be parametrized and each of its alternative definitions to have a complex condition. The terminals of the grammar are base-level database operations on tables that are interpreted at run-time. The non-terminals are defined declaratively by production rules that combine those operations into meaningful plans for execution. Each production produces a set of alternative plans, each having a vector of properties, including the estimated cost of producing that plan. Productions can require certain properties of their inputs, such as tuple order and location, and we describe a “glue” mechanism for augmenting plans to achieve the required properties. We give detailed examples to illustrate the power and robustness of our rules and to contrast them with related ideas.},
journal = {SIGMOD Rec.},
month = jun,
pages = {18–27},
numpages = {10}
}

@inproceedings{10.1145/50202.50205,
author = {Muralikrishna, M. and DeWitt, David J.},
title = {Equi-Depth Multidimensional Histograms},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50205},
doi = {10.1145/50202.50205},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {28–36},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50205,
author = {Muralikrishna, M. and DeWitt, David J.},
title = {Equi-Depth Multidimensional Histograms},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50205},
doi = {10.1145/971701.50205},
journal = {SIGMOD Rec.},
month = jun,
pages = {28–36},
numpages = {9}
}

@inproceedings{10.1145/50202.50206,
author = {Garza, Jorge F. and Kim, Won},
title = {Transaction Management in an Object-Oriented Database System},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50206},
doi = {10.1145/50202.50206},
abstract = {In this paper, we describe transaction management in ORION, an object-oriented database system. The application environments for which ORION is intended led us to implement the notions of sessions of transactions, and hypothetical transactions (transactions which always abort). The object-oriented data model which ORION implements complicates locking requirements. ORION supports a concurrency control mechanism based on extensions to the current theory of locking, and a transaction recovery mechanism based on conventional logging.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {37–45},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50206,
author = {Garza, Jorge F. and Kim, Won},
title = {Transaction Management in an Object-Oriented Database System},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50206},
doi = {10.1145/971701.50206},
abstract = {In this paper, we describe transaction management in ORION, an object-oriented database system. The application environments for which ORION is intended led us to implement the notions of sessions of transactions, and hypothetical transactions (transactions which always abort). The object-oriented data model which ORION implements complicates locking requirements. ORION supports a concurrency control mechanism based on extensions to the current theory of locking, and a transaction recovery mechanism based on conventional logging.},
journal = {SIGMOD Rec.},
month = jun,
pages = {37–45},
numpages = {9}
}

@inproceedings{10.1145/50202.50207,
author = {Jagannathan, D. and Fritchman, B. L. and Guck, R. L. and Thompson, J. P. and Tolbert, D. M.},
title = {SIM: A Database System Based on the Semantic Data Model},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50207},
doi = {10.1145/50202.50207},
abstract = {SIM is a fully featured, commercially available database management system based on a semantic data model similar to Hammer and McLeod's SDM SIM has two primary modeling goals. The first is to narrow the gap between a user's real-world perception of data and the conceptual view imposed by the database system because of modeling presuppositions or limitations. The second goal is to allow, as much as possible, the semantics of data to be defined in the schema and make the database system responsible for enforcing its integrity SIM provides a rich set of constructs for schema definition, including those for specifying generalization hierarchies modeled by directed acyclic graphs, interobject relationships and integrity constraints. It also features a novel, easy-to-use, English-like DML. This paper describes the key modeling features of SIM, the architecture of the system and its implementation considerations.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {46–55},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50207,
author = {Jagannathan, D. and Fritchman, B. L. and Guck, R. L. and Thompson, J. P. and Tolbert, D. M.},
title = {SIM: A Database System Based on the Semantic Data Model},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50207},
doi = {10.1145/971701.50207},
abstract = {SIM is a fully featured, commercially available database management system based on a semantic data model similar to Hammer and McLeod's SDM SIM has two primary modeling goals. The first is to narrow the gap between a user's real-world perception of data and the conceptual view imposed by the database system because of modeling presuppositions or limitations. The second goal is to allow, as much as possible, the semantics of data to be defined in the schema and make the database system responsible for enforcing its integrity SIM provides a rich set of constructs for schema definition, including those for specifying generalization hierarchies modeled by directed acyclic graphs, interobject relationships and integrity constraints. It also features a novel, easy-to-use, English-like DML. This paper describes the key modeling features of SIM, the architecture of the system and its implementation considerations.},
journal = {SIGMOD Rec.},
month = jun,
pages = {46–55},
numpages = {10}
}

@inproceedings{10.1145/50202.50208,
author = {Caruso, Michael and Sciore, Edward},
title = {Contexts and Metamessages in Object-Oriented Database Programming Language Design},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50208},
doi = {10.1145/50202.50208},
abstract = {VISION is an object-oriented database system currently used commercially to develop investment analysis and other large statistical applications. Characteristic of these applications, beside the standard issues of structural and computational richness, is the need to handle time, versions, and concurrency control in a manner that does not produce combinatoric complexity in object protocol. This paper describes the approach taken by VISION in addressing these issues.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {56–65},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50208,
author = {Caruso, Michael and Sciore, Edward},
title = {Contexts and Metamessages in Object-Oriented Database Programming Language Design},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50208},
doi = {10.1145/971701.50208},
abstract = {VISION is an object-oriented database system currently used commercially to develop investment analysis and other large statistical applications. Characteristic of these applications, beside the standard issues of structural and computational richness, is the need to handle time, versions, and concurrency control in a manner that does not produce combinatoric complexity in object protocol. This paper describes the approach taken by VISION in addressing these issues.},
journal = {SIGMOD Rec.},
month = jun,
pages = {56–65},
numpages = {10}
}

@inproceedings{10.1145/50202.50209,
author = {Laurent, D. and Spyratis, N.},
title = {Partition Semantics for Incomplete Information in Relational Databases},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50209},
doi = {10.1145/50202.50209},
abstract = {We define partition semantics for databases with incomplete information and we present an algorithm for query processing in the presence of incomplete information and functional dependencies. We show that Lipski's model for databases with incomplete information can be seen as a special case of our model.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {66–73},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50209,
author = {Laurent, D. and Spyratis, N.},
title = {Partition Semantics for Incomplete Information in Relational Databases},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50209},
doi = {10.1145/971701.50209},
abstract = {We define partition semantics for databases with incomplete information and we present an algorithm for query processing in the presence of incomplete information and functional dependencies. We show that Lipski's model for databases with incomplete information can be seen as a special case of our model.},
journal = {SIGMOD Rec.},
month = jun,
pages = {66–73},
numpages = {8}
}

@inproceedings{10.1145/50202.50210,
author = {Yuan, Li Yan and Chiang, Ding-An},
title = {A Sound and Complete Query Evaluation Algorithm for Relational Databases with Null Values},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50210},
doi = {10.1145/50202.50210},
abstract = {Reiter has proposed extended relational theory to formulate relational databases with null values and presented a query evaluation algorithm for such databases. However, due to indefinite information brought in by null values, Reiter's algorithm is sound but not complete. In this paper, we first propose an extended relation to represent indefinite information in relational databases. Then, we define an extended relational algebra for extended relations. Based on Reiter's extended relational theory, and our extended relations and the extended relational algebra, we present a sound and complete query evaluation algorithm for relational databases with null values},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {74–81},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50210,
author = {Yuan, Li Yan and Chiang, Ding-An},
title = {A Sound and Complete Query Evaluation Algorithm for Relational Databases with Null Values},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50210},
doi = {10.1145/971701.50210},
abstract = {Reiter has proposed extended relational theory to formulate relational databases with null values and presented a query evaluation algorithm for such databases. However, due to indefinite information brought in by null values, Reiter's algorithm is sound but not complete. In this paper, we first propose an extended relation to represent indefinite information in relational databases. Then, we define an extended relational algebra for extended relations. Based on Reiter's extended relational theory, and our extended relations and the extended relational algebra, we present a sound and complete query evaluation algorithm for relational databases with null values},
journal = {SIGMOD Rec.},
month = jun,
pages = {74–81},
numpages = {8}
}

@inproceedings{10.1145/50202.50211,
author = {Malvestuto, F. M.},
title = {The Derivation Problem of Summary Data},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50211},
doi = {10.1145/50202.50211},
abstract = {Given a statistical database consisting of two summary tables based on a common but not identical classification criterion (e.g., two geographical partitionings of a country) there are additional summary tables that are derivable in the sense that they are uniquely (i.e., with no uncertainty) determined by the tables given. Derivable tables encompass not only, of course, “less detailed” tables (that is, aggregated data) but also “more detailed” tables (that is, disaggregated data). Tables of the second type can be explicitly constructed by using a “procedure of data refinement” based on the graph representation of the correspondences between the categories of the two classification systems given in some cases, that is, when such a graph representation meets the acyclicity condition, the underlying database is “equivalent” to a single table (called representative table) and then a necessary and sufficient condition for a table to be derivable can be stated.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {82–89},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50211,
author = {Malvestuto, F. M.},
title = {The Derivation Problem of Summary Data},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50211},
doi = {10.1145/971701.50211},
abstract = {Given a statistical database consisting of two summary tables based on a common but not identical classification criterion (e.g., two geographical partitionings of a country) there are additional summary tables that are derivable in the sense that they are uniquely (i.e., with no uncertainty) determined by the tables given. Derivable tables encompass not only, of course, “less detailed” tables (that is, aggregated data) but also “more detailed” tables (that is, disaggregated data). Tables of the second type can be explicitly constructed by using a “procedure of data refinement” based on the graph representation of the correspondences between the categories of the two classification systems given in some cases, that is, when such a graph representation meets the acyclicity condition, the underlying database is “equivalent” to a single table (called representative table) and then a necessary and sufficient condition for a table to be derivable can be stated.},
journal = {SIGMOD Rec.},
month = jun,
pages = {82–89},
numpages = {8}
}

@inproceedings{10.1145/50202.50212,
author = {Alexander, W. and Copeland, G.},
title = {Process and Dataflow Control in Distributed Data-Intensive Systems},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50212},
doi = {10.1145/50202.50212},
abstract = {In dataflow architectures, each dataflow operation is typically executed on a single physical node. We are concerned with distributed data-intensive systems, in which each base (i.e., persistent) set of data has been declustered over many physical nodes to achieve load balancing. Because of large base set size, each operation is executed where the base set resides, and intermediate results are transferred between physical nodes. In such systems, each dataflow operation is typically executed on many physical nodes. Furthermore, because computations are data-dependent, we cannot know until run time which subset of the physical nodes containing a particular base set will be involved in a given dataflow operation. This uncertainty creates several problems.We examine the problems of efficient program loading, dataflow—operation activation and termination, control of data transfer among dataflow operations, and transaction commit and abort in a distributed data-intensive system. We show how these problems are interrelated, and we present a unified set of mechanisms for efficiently solving them. For some of the problems, we present several solutions and compare them quantitatively.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {90–98},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50212,
author = {Alexander, W. and Copeland, G.},
title = {Process and Dataflow Control in Distributed Data-Intensive Systems},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50212},
doi = {10.1145/971701.50212},
abstract = {In dataflow architectures, each dataflow operation is typically executed on a single physical node. We are concerned with distributed data-intensive systems, in which each base (i.e., persistent) set of data has been declustered over many physical nodes to achieve load balancing. Because of large base set size, each operation is executed where the base set resides, and intermediate results are transferred between physical nodes. In such systems, each dataflow operation is typically executed on many physical nodes. Furthermore, because computations are data-dependent, we cannot know until run time which subset of the physical nodes containing a particular base set will be involved in a given dataflow operation. This uncertainty creates several problems.We examine the problems of efficient program loading, dataflow—operation activation and termination, control of data transfer among dataflow operations, and transaction commit and abort in a distributed data-intensive system. We show how these problems are interrelated, and we present a unified set of mechanisms for efficiently solving them. For some of the problems, we present several solutions and compare them quantitatively.},
journal = {SIGMOD Rec.},
month = jun,
pages = {90–98},
numpages = {9}
}

@inproceedings{10.1145/50202.50213,
author = {Copeland, George and Alexander, William and Boughter, Ellen and Keller, Tom},
title = {Data Placement in Bubba},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50213},
doi = {10.1145/50202.50213},
abstract = {This paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications being developed at MCC. “Highly-parallel” implies that load balancing is a critical performance issue. “Data-intensive” means data is so large that operations should be executed where the data resides. As a result, data placement becomes a critical performance issue.In general, determining the optimal placement of data across processing nodes for performance is a difficult problem. We describe our heuristic approach to solving the data placement problem in Bubba. We then present experimental results using a specific workload to provide insight into the problem. Several researchers have argued the benefits of declustering (i e, spreading each base relation over many nodes). We show that as declustering is increased, load balancing continues to improve. However, for transactions involving complex joins, further declustering reduces throughput because of communications, startup and termination overhead.We argue that data placement, especially declustering, in a highly-parallel system must be considered early in the design, so that mechanisms can be included for supporting variable declustering, for minimizing the most significant overheads associated with large-scale declustering, and for gathering the required statistics.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {99–108},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50213,
author = {Copeland, George and Alexander, William and Boughter, Ellen and Keller, Tom},
title = {Data Placement in Bubba},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50213},
doi = {10.1145/971701.50213},
abstract = {This paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications being developed at MCC. “Highly-parallel” implies that load balancing is a critical performance issue. “Data-intensive” means data is so large that operations should be executed where the data resides. As a result, data placement becomes a critical performance issue.In general, determining the optimal placement of data across processing nodes for performance is a difficult problem. We describe our heuristic approach to solving the data placement problem in Bubba. We then present experimental results using a specific workload to provide insight into the problem. Several researchers have argued the benefits of declustering (i e, spreading each base relation over many nodes). We show that as declustering is increased, load balancing continues to improve. However, for transactions involving complex joins, further declustering reduces throughput because of communications, startup and termination overhead.We argue that data placement, especially declustering, in a highly-parallel system must be considered early in the design, so that mechanisms can be included for supporting variable declustering, for minimizing the most significant overheads associated with large-scale declustering, and for gathering the required statistics.},
journal = {SIGMOD Rec.},
month = jun,
pages = {99–108},
numpages = {10}
}

@inproceedings{10.1145/50202.50214,
author = {Patterson, David A. and Gibson, Garth and Katz, Randy H.},
title = {A Case for Redundant Arrays of Inexpensive Disks (RAID)},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50214},
doi = {10.1145/50202.50214},
abstract = {Increasing performance of CPUs and memories will be squandered if not matched by a similar performance increase in I/O. While the capacity of Single Large Expensive Disks (SLED) has grown rapidly, the performance improvement of SLED has been modest. Redundant Arrays of Inexpensive Disks (RAID), based on the magnetic disk technology developed for personal computers, offers an attractive alternative to SLED, promising improvements of an order of magnitude in performance, reliability, power consumption, and scalability. This paper introduces five levels of RAIDs, giving their relative cost/performance, and compares RAID to an IBM 3380 and a Fujitsu Super Eagle.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {109–116},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50214,
author = {Patterson, David A. and Gibson, Garth and Katz, Randy H.},
title = {A Case for Redundant Arrays of Inexpensive Disks (RAID)},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50214},
doi = {10.1145/971701.50214},
abstract = {Increasing performance of CPUs and memories will be squandered if not matched by a similar performance increase in I/O. While the capacity of Single Large Expensive Disks (SLED) has grown rapidly, the performance improvement of SLED has been modest. Redundant Arrays of Inexpensive Disks (RAID), based on the magnetic disk technology developed for personal computers, offers an attractive alternative to SLED, promising improvements of an order of magnitude in performance, reliability, power consumption, and scalability. This paper introduces five levels of RAIDs, giving their relative cost/performance, and compares RAID to an IBM 3380 and a Fujitsu Super Eagle.},
journal = {SIGMOD Rec.},
month = jun,
pages = {109–116},
numpages = {8}
}

@inproceedings{10.1145/50202.50215,
author = {Kumar, Akhil and Stonebraker, Michael},
title = {Semantics Based Transaction Management Techniques for Replicated Data},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50215},
doi = {10.1145/50202.50215},
abstract = {Data is often replicated in distributed database applications to improve availability and response time. Conventional multi-copy algorithms deliver fast response times and high availability for read-only transactions while sacrificing these goals for updates. In this paper, we propose a multi-copy algorithm that works well in both retrieval and update environments by exploiting special application semantics. By subdividing transactions into various categories, and utilizing a commutativity property, we demonstrate cheaper techniques and show that they guarantee correctness. A performance comparison between our techniques and conventional ones quantifies the extent of the savings.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {117–125},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50215,
author = {Kumar, Akhil and Stonebraker, Michael},
title = {Semantics Based Transaction Management Techniques for Replicated Data},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50215},
doi = {10.1145/971701.50215},
abstract = {Data is often replicated in distributed database applications to improve availability and response time. Conventional multi-copy algorithms deliver fast response times and high availability for read-only transactions while sacrificing these goals for updates. In this paper, we propose a multi-copy algorithm that works well in both retrieval and update environments by exploiting special application semantics. By subdividing transactions into various categories, and utilizing a commutativity property, we demonstrate cheaper techniques and show that they guarantee correctness. A performance comparison between our techniques and conventional ones quantifies the extent of the savings.},
journal = {SIGMOD Rec.},
month = jun,
pages = {117–125},
numpages = {9}
}

@inproceedings{10.1145/50202.50216,
author = {El Abbadi, Amr and Toueg, Sam},
title = {The Group Paradigm for Concurrency Control},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50216},
doi = {10.1145/50202.50216},
abstract = {We propose a paradigm for developing, describing and proving the correctness of concurrency control protocols for replicated databases in the presence of failures or communication restrictions. Our approach is to hierarchically divide the problem of achieving one-copy serializability by introducing the notion of a “group” that is a higher level of abstraction than transactions. Instead of dealing with the overall problem of serializing all transactions, our paradigm divides the problem into two simpler ones. (1) A local policy for each group that ensures a total order of all transactions in that group. (2) A global policy that ensures a correct serialization of all groups. We use the paradigm to demonstrate the similarities between several concurrency control protocols by comparing the way they achieve correctness.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {126–134},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50216,
author = {El Abbadi, Amr and Toueg, Sam},
title = {The Group Paradigm for Concurrency Control},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50216},
doi = {10.1145/971701.50216},
abstract = {We propose a paradigm for developing, describing and proving the correctness of concurrency control protocols for replicated databases in the presence of failures or communication restrictions. Our approach is to hierarchically divide the problem of achieving one-copy serializability by introducing the notion of a “group” that is a higher level of abstraction than transactions. Instead of dealing with the overall problem of serializing all transactions, our paradigm divides the problem into two simpler ones. (1) A local policy for each group that ensures a total order of all transactions in that group. (2) A global policy that ensures a correct serialization of all groups. We use the paradigm to demonstrate the similarities between several concurrency control protocols by comparing the way they achieve correctness.},
journal = {SIGMOD Rec.},
month = jun,
pages = {126–134},
numpages = {9}
}

@inproceedings{10.1145/50202.50217,
author = {Breitbart, Yuri and Silberschatz, Avi},
title = {Multidatabase Update Issues},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50217},
doi = {10.1145/50202.50217},
abstract = {A formal model of data updates in a multidatabase environment is developed, and a theory of concurrency control in such an environment is presented. We formulate a correctness condition for the concurrency control mechanism and propose a protocol that allows concurrent execution of a set of global transactions in presence of local ones. This protocol ensures the consistency of the multidatabase and deadlock freedom. We use the developed theory to prove the protocol's correctness and discuss complexity issues of implementing the proposed protocol.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {135–142},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50217,
author = {Breitbart, Yuri and Silberschatz, Avi},
title = {Multidatabase Update Issues},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50217},
doi = {10.1145/971701.50217},
abstract = {A formal model of data updates in a multidatabase environment is developed, and a theory of concurrency control in such an environment is presented. We formulate a correctness condition for the concurrency control mechanism and propose a protocol that allows concurrent execution of a set of global transactions in presence of local ones. This protocol ensures the consistency of the multidatabase and deadlock freedom. We use the developed theory to prove the protocol's correctness and discuss complexity issues of implementing the proposed protocol.},
journal = {SIGMOD Rec.},
month = jun,
pages = {135–142},
numpages = {8}
}

@inproceedings{10.1145/50202.50218,
author = {Abiteboul, Serge and Hull, Richard},
title = {Data Functions, Datalog and Negation},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50218},
doi = {10.1145/50202.50218},
abstract = {Datalog is extended to incorporate single-valued “data functions”, which correspond to attributes in semantic models, and which may be base (user-specified) or derived (computed). Both conventional and stratified datalog are considered. Under the extension, a datalog program may not be consistent, because a derived function symbol may evaluate to something which is not a function. Consistency is shown to be undecidable, and is decidable in a number of restricted cases. A syntactic restriction, panwise consistency, is shown to guarantee consistency. The framework developed here can also be used to incorporate single-valued data functions into the Complex Object Language (COL), which supports deductive capabilities, complex database objects, and set-valued data functions.There is a natural correspondence between the extended datalog introduced here, and the usual datalog with functional dependencies. For families Φ and Γ of dependencies and a family of datalog programs Λ, the Φ-Γ implication problem for Λ asks, given sets F ⊆ Φ and G ⊆ Γ and a program P in Λ, whether for all inputs I, I @@@@ F implies P(I) @@@@ G. The FD-FD implication problem is undecidable for datalog, and the TGD-EGD implication problem is decidable for stratified datalog. Also, the \O{}-MVD problem is undecidable (and hence also the MVD-preservation problem).},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {143–153},
numpages = {11},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50218,
author = {Abiteboul, Serge and Hull, Richard},
title = {Data Functions, Datalog and Negation},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50218},
doi = {10.1145/971701.50218},
abstract = {Datalog is extended to incorporate single-valued “data functions”, which correspond to attributes in semantic models, and which may be base (user-specified) or derived (computed). Both conventional and stratified datalog are considered. Under the extension, a datalog program may not be consistent, because a derived function symbol may evaluate to something which is not a function. Consistency is shown to be undecidable, and is decidable in a number of restricted cases. A syntactic restriction, panwise consistency, is shown to guarantee consistency. The framework developed here can also be used to incorporate single-valued data functions into the Complex Object Language (COL), which supports deductive capabilities, complex database objects, and set-valued data functions.There is a natural correspondence between the extended datalog introduced here, and the usual datalog with functional dependencies. For families Φ and Γ of dependencies and a family of datalog programs Λ, the Φ-Γ implication problem for Λ asks, given sets F ⊆ Φ and G ⊆ Γ and a program P in Λ, whether for all inputs I, I @@@@ F implies P(I) @@@@ G. The FD-FD implication problem is undecidable for datalog, and the TGD-EGD implication problem is decidable for stratified datalog. Also, the \O{}-MVD problem is undecidable (and hence also the MVD-preservation problem).},
journal = {SIGMOD Rec.},
month = jun,
pages = {143–153},
numpages = {11}
}

@inproceedings{10.1145/50202.50219,
author = {Krishnamurthy, Ravi and Ramakrishnan, Raghu and Shmueli, Oded},
title = {A Framework for Testing Safety and Effective Computability of Extended Datalog},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50219},
doi = {10.1145/50202.50219},
abstract = {This paper presents a methodology for testing a general logic program containing function symbols and built-in predicates for safety and effective computability. Safety is the property that the set of answers for a given query is finite. A related issues is whether the evaluation strategy can effectively compute all answers and terminate. We consider these problems under the assumption that queries are evaluated using a bottom-up fixpoint computation. We also approximate the use of function symbols by considering Datalog programs with infinite base relations over which finiteness constraints and monotonicity constraints are considered. One of the main results of this paper is a recursive algorithm, check_clique, to test the safety and effective computability of predicates in arbitrarily complex cliques. This algorithm takes certain procedures as parameters, and its applicability can be strengthened by making these procedures more sophisticated. We specify the properties required of these procedures precisely, and present a formal proof of correctness for algorithm check_clique. This work provides a framework for testing safety and effective computability of recursive programs, and is based on a clique by clique analysis. The results reported here form the basis of the safety testing for the LDL language, being implemented at MCC.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {154–163},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50219,
author = {Krishnamurthy, Ravi and Ramakrishnan, Raghu and Shmueli, Oded},
title = {A Framework for Testing Safety and Effective Computability of Extended Datalog},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50219},
doi = {10.1145/971701.50219},
abstract = {This paper presents a methodology for testing a general logic program containing function symbols and built-in predicates for safety and effective computability. Safety is the property that the set of answers for a given query is finite. A related issues is whether the evaluation strategy can effectively compute all answers and terminate. We consider these problems under the assumption that queries are evaluated using a bottom-up fixpoint computation. We also approximate the use of function symbols by considering Datalog programs with infinite base relations over which finiteness constraints and monotonicity constraints are considered. One of the main results of this paper is a recursive algorithm, check_clique, to test the safety and effective computability of predicates in arbitrarily complex cliques. This algorithm takes certain procedures as parameters, and its applicability can be strengthened by making these procedures more sophisticated. We specify the properties required of these procedures precisely, and present a formal proof of correctness for algorithm check_clique. This work provides a framework for testing safety and effective computability of recursive programs, and is based on a clique by clique analysis. The results reported here form the basis of the safety testing for the LDL language, being implemented at MCC.},
journal = {SIGMOD Rec.},
month = jun,
pages = {154–163},
numpages = {10}
}

@inproceedings{10.1145/50202.50220,
author = {Chen, Qiming and Gardarin, Georges},
title = {An Implementation Model for Reasoning with Complex Objects},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50220},
doi = {10.1145/50202.50220},
abstract = {In this paper, we first propose a natural syntactical extension of DATALOG called NESTED_DATALOG for dealing with complex objects represented as nested predicates. Then, we introduce the token object model which is a simple extension of the relational model with tokens to represent complex objects and support referential information sharing. An implementation model of a NESTED_DATALOG program is defined by mapping it to the token object model which remains a straightforward extension of classical logical databases. Through this work, we can accommodate two basic requirements. The availability of a rule language for reasoning with complex objects, and the mechanism for mapping a complex object rule program to a relational DBMS offering a pure DATALOG rule language. In summary, the main contributions of the paper are the definition of a rule language for complex objects and the development of a technique to compile this complex object rule language to classical DATALOG.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {164–172},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50220,
author = {Chen, Qiming and Gardarin, Georges},
title = {An Implementation Model for Reasoning with Complex Objects},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50220},
doi = {10.1145/971701.50220},
abstract = {In this paper, we first propose a natural syntactical extension of DATALOG called NESTED_DATALOG for dealing with complex objects represented as nested predicates. Then, we introduce the token object model which is a simple extension of the relational model with tokens to represent complex objects and support referential information sharing. An implementation model of a NESTED_DATALOG program is defined by mapping it to the token object model which remains a straightforward extension of classical logical databases. Through this work, we can accommodate two basic requirements. The availability of a rule language for reasoning with complex objects, and the mechanism for mapping a complex object rule program to a relational DBMS offering a pure DATALOG rule language. In summary, the main contributions of the paper are the definition of a rule language for complex objects and the development of a technique to compile this complex object rule language to classical DATALOG.},
journal = {SIGMOD Rec.},
month = jun,
pages = {164–172},
numpages = {9}
}

@inproceedings{10.1145/50202.50221,
author = {Kim, Myoung Ho and Pramanik, Sakti},
title = {Optimal File Distribution for Partial Match Retrieval},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50221},
doi = {10.1145/50202.50221},
abstract = {In this paper we present data distribution methods for parallel processing environment. The primary objective is to process partial match retrieval type queries for parallel devices.The main contribution of this paper is the development of a new approach called FX (Fieldwise eXclusive) distribution for maximizing data access concurrency. An algebraic property of exclusive-or operation, and field transformation techniques are fundamental to this data distribution techniques. We have shown through theorems and corollaries that this FX distribution approach performs better than other methods proposed earlier. We have also shown, by computing probability of optimal distribution and query response time, that FX distribution gives better performance than others over a large class of partial match queries. This approach presents a new basis in which optimal data distribution for more general type of queries can be formulated.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {173–182},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50221,
author = {Kim, Myoung Ho and Pramanik, Sakti},
title = {Optimal File Distribution for Partial Match Retrieval},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50221},
doi = {10.1145/971701.50221},
abstract = {In this paper we present data distribution methods for parallel processing environment. The primary objective is to process partial match retrieval type queries for parallel devices.The main contribution of this paper is the development of a new approach called FX (Fieldwise eXclusive) distribution for maximizing data access concurrency. An algebraic property of exclusive-or operation, and field transformation techniques are fundamental to this data distribution techniques. We have shown through theorems and corollaries that this FX distribution approach performs better than other methods proposed earlier. We have also shown, by computing probability of optimal distribution and query response time, that FX distribution gives better performance than others over a large class of partial match queries. This approach presents a new basis in which optimal data distribution for more general type of queries can be formulated.},
journal = {SIGMOD Rec.},
month = jun,
pages = {173–182},
numpages = {10}
}

@inproceedings{10.1145/50202.50222,
author = {Hutflesz, Andreas and Six, Hans-Werner and Widmayer, Peter},
title = {Twin Grid Files: Space Optimizing Access Schemes},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50222},
doi = {10.1145/50202.50222},
abstract = {Storage access schemes for points, supporting spatial searching, usually suffer from an undesirably low storage space utilization. We show how a given set of points can be distributed among two grid files in such a way that storage space utilization is optimal. The optimal twin grid file can be built practically as fast as a standard grid file, i.e., the storage space optimality is obtained at almost no extra cost. We compare the performances of the standard grid file, the optimal static twin grid file, and an efficient dynamic twin grid file, where insertions and deletions trigger the redistribution of points among the two grid files. Twin grid files utilize storage space at roughly 90%, as compared with the 69% of the standard grid file. Typical range queries - the most important spatial search operations - can be answered in twin grid files at least as fast as in the standard grid file.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {183–190},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50222,
author = {Hutflesz, Andreas and Six, Hans-Werner and Widmayer, Peter},
title = {Twin Grid Files: Space Optimizing Access Schemes},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50222},
doi = {10.1145/971701.50222},
abstract = {Storage access schemes for points, supporting spatial searching, usually suffer from an undesirably low storage space utilization. We show how a given set of points can be distributed among two grid files in such a way that storage space utilization is optimal. The optimal twin grid file can be built practically as fast as a standard grid file, i.e., the storage space optimality is obtained at almost no extra cost. We compare the performances of the standard grid file, the optimal static twin grid file, and an efficient dynamic twin grid file, where insertions and deletions trigger the redistribution of points among the two grid files. Twin grid files utilize storage space at roughly 90%, as compared with the 69% of the standard grid file. Typical range queries - the most important spatial search operations - can be answered in twin grid files at least as fast as in the standard grid file.},
journal = {SIGMOD Rec.},
month = jun,
pages = {183–190},
numpages = {8}
}

@inproceedings{10.1145/50202.50223,
author = {Ramakrishna, M. V.},
title = {Hashing Practice: Analysis of Hashing and Universal Hashing},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50223},
doi = {10.1145/50202.50223},
abstract = {Much of the literature on hashing deals with overflow handling (collision resolution) techniques and its analysis. What does all the analytical results mean in practice and how can they be achieved with practical files? This paper considers the problem of achieving analytical performance of hashing techniques in practice with reference to successful search lengths, unsuccessful search lengths and the expected worst case performance (expected length of the longest probe sequence). There has been no previous attempt to explicitly link the analytical results to performance of real life files. Also, the previously reported experimental results deal mostly with successful search lengths. We show why the well known division method performs “well” under a specific model of selecting the test file. We formulate and justify an hypothesis that by choosing functions from a particular class of hashing functions, the analytical performance can be obtained in practice on real life files. Experimental results presented strongly support our hypothesis. Several interesting problems arising are mentioned in conclusion.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {191–199},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50223,
author = {Ramakrishna, M. V.},
title = {Hashing Practice: Analysis of Hashing and Universal Hashing},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50223},
doi = {10.1145/971701.50223},
abstract = {Much of the literature on hashing deals with overflow handling (collision resolution) techniques and its analysis. What does all the analytical results mean in practice and how can they be achieved with practical files? This paper considers the problem of achieving analytical performance of hashing techniques in practice with reference to successful search lengths, unsuccessful search lengths and the expected worst case performance (expected length of the longest probe sequence). There has been no previous attempt to explicitly link the analytical results to performance of real life files. Also, the previously reported experimental results deal mostly with successful search lengths. We show why the well known division method performs “well” under a specific model of selecting the test file. We formulate and justify an hypothesis that by choosing functions from a particular class of hashing functions, the analytical performance can be obtained in practice on real life files. Experimental results presented strongly support our hypothesis. Several interesting problems arising are mentioned in conclusion.},
journal = {SIGMOD Rec.},
month = jun,
pages = {191–199},
numpages = {9}
}

@inproceedings{10.1145/50202.50224,
author = {Ioannidis, Yannis E. and Livny, Miron},
title = {Data Modeling in DELAB},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50224},
doi = {10.1145/50202.50224},
abstract = {As the size and complexity of processing and manufacturing systems increases, the need for Database Management Systems (DBMS) that meet the special needs of studies that experiment with such systems becomes more current. System analysts who study the performance of modern processing systems have to manipulate large amounts of data in order to profile the behavior of the system. They have to identify the relationship between the properties of a compound system and a wide spectrum of performance metrics. In a recent study in which we have analyzed a set of distributed concurrency control algorithms, we performed more than 1400 simulation experiments. Each experiment was characterized by more than 6000 input parameters and generated more than 400 output values. It is thus clear that powerful means for defining the structure and properties of complex systems are needed, as well as efficient tools to retrieve the data accumulated in the course of the study. We are currently engaged in an effort to develop and implement the DELAB simulation laboratory that aims to provide such means and tools for simulation studies.The goal of the first phase of this effort was to design and implement a simulation language. It ended in 1986 when the DENET (Discrete Event NETwork) simulation language became operational. The language is based on the concept of Discrete Event System Specifications (DEVS). It views the simulator as a collection of self contained objects that communicate via Discrete Event Connectors that provide a unified synchronization protocol In the past two years the language has been used in a number of real life studies. It was used to simulate distributed processing environments, communication protocols, and production lines Several tools have been developed around the language. All tools adhere to the same modeling methodology and thus create a cohesive simulation environment.In the second phase of the DELAB project we have been addressing the data management problem DENET has been interfaced to a special purpose relational DBMS that can store descriptions of simulation runs and provides access to the stored data Based on our experience with thus DBMS, we have reached the conclusion that system analysts need to be provided with a view of the data that differs from the way the DENET program views the data, and thus decided to develop a data model that meets their needs. The M@@@@SE data model, which is the result of this effort, has an object oriented flavor. It was developed with the guidance of potential users and was tested on a number of real life simulation studies.Although the conception of M@@@@SE was motivated by the specific needs of a simulation laboratory, we believe that it addresses the representational needs of many other environments We have decided to support the notion of an object. Every object is assigned a unique identifier. Depending on their properties (attributes), objects can simultaneously belong to several classes, inheriting properties from all of them. Among these classes, one is characterized as the primary class of the object. The notion of a primary class helps achieving a “conceptual” as well as a physical clustering among similar objects. Collections of objects are supported as regular objects in M@@@@SE in the form of sets, multisets (bags), and arrays. The extent of a class, i.e., the objects that are known members of the class, is explicitly stored in the database. Every M@@@@SE database schema has a straightforward directed graph representation. Each node represents a class of objects and is labeled by the class name. Relationships between the classes in the schema are captured by the arcs of the graph. Similarly to most object-oriented data models, M@@@@SE has two major types of arcs component arcs and inheritance arcs. The former capture structural relationships (part-of), whereas the latter capture semantic relationships (is-a) between classes of objects.Although a component arc (A → B) has a direction from the parent to the child class, it relates the two classes in both directions. An object in A has a part that is in B, an object in B is part of an object that is in A. The values of the attributes of an object may be explicitly assigned by the user or derived by the system through rules associated with the attribute. Although not restricted to those, aggregates of collections are the dominant kind of such derived attributes. The values assigned to derived attributes may be explicitly stored in the database (forward application of rules), or may be inferred on demand (backward application of rules). Null values are supported in M@@@@SE and are interpreted as “no related object.” Thus, null is distinct from any other value. Also, default values are supported in their full generality M@@@@SE supports four types of user-defined structural constraints that can be used to control sharing or existence dependence between objects. These are (a) sharing of objects among collections, (b) sharing of objects among their parents, (c) sharing of objects along different paths in the component graph, and (d) existence dependence of a child to its parents.An inheritance arc (A → B) implies that every object in B is in A also B(X) → A(X). This rule is called a generalization rule and is implicitly assumed for every inheritance arc. In addition to an implicit generalization rule, an inheritance arc may be explicitly associated with a specialization rule, which specifies which elements of the parent class belong to the child class A specialized class can be instantiated at all times, or its objects can be retrieved on demand, by applying the corresponding rule(s) on an as-needed basis. Although the semantics of simple specialization rules could be captured by typing information and inheritance, specialization rules can be arbitrarily complex, thus allowing the necessary generality to capture the level of abstraction needed for experiment management.We are currently designing a query language for M@@@@SE that will take advantage of its features and will give the system analyst the capability to express complex queries easily. Multiple inheritance will be a significant feature of the language, and one of the key sources of its power and ease of use. Work planned for the future includes building a graphics user-interface to the DBMS that will take advantage of the graphic representation of a M@@@@SE schema and will allow the user to express queries as paths on the graph. The ability to express all types of constraints in a graphic form will then become very important. Some preliminary work on the subject has shown that focusing on different parts of the database establishes different contexts, within which the same constraint can be expressed in different ways. We are currently studying this relationship between constraints and contexts and plan to incorporate it in the query language as well as in the graphics interface.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {200},
numpages = {1},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50224,
author = {Ioannidis, Yannis E. and Livny, Miron},
title = {Data Modeling in DELAB},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50224},
doi = {10.1145/971701.50224},
abstract = {As the size and complexity of processing and manufacturing systems increases, the need for Database Management Systems (DBMS) that meet the special needs of studies that experiment with such systems becomes more current. System analysts who study the performance of modern processing systems have to manipulate large amounts of data in order to profile the behavior of the system. They have to identify the relationship between the properties of a compound system and a wide spectrum of performance metrics. In a recent study in which we have analyzed a set of distributed concurrency control algorithms, we performed more than 1400 simulation experiments. Each experiment was characterized by more than 6000 input parameters and generated more than 400 output values. It is thus clear that powerful means for defining the structure and properties of complex systems are needed, as well as efficient tools to retrieve the data accumulated in the course of the study. We are currently engaged in an effort to develop and implement the DELAB simulation laboratory that aims to provide such means and tools for simulation studies.The goal of the first phase of this effort was to design and implement a simulation language. It ended in 1986 when the DENET (Discrete Event NETwork) simulation language became operational. The language is based on the concept of Discrete Event System Specifications (DEVS). It views the simulator as a collection of self contained objects that communicate via Discrete Event Connectors that provide a unified synchronization protocol In the past two years the language has been used in a number of real life studies. It was used to simulate distributed processing environments, communication protocols, and production lines Several tools have been developed around the language. All tools adhere to the same modeling methodology and thus create a cohesive simulation environment.In the second phase of the DELAB project we have been addressing the data management problem DENET has been interfaced to a special purpose relational DBMS that can store descriptions of simulation runs and provides access to the stored data Based on our experience with thus DBMS, we have reached the conclusion that system analysts need to be provided with a view of the data that differs from the way the DENET program views the data, and thus decided to develop a data model that meets their needs. The M@@@@SE data model, which is the result of this effort, has an object oriented flavor. It was developed with the guidance of potential users and was tested on a number of real life simulation studies.Although the conception of M@@@@SE was motivated by the specific needs of a simulation laboratory, we believe that it addresses the representational needs of many other environments We have decided to support the notion of an object. Every object is assigned a unique identifier. Depending on their properties (attributes), objects can simultaneously belong to several classes, inheriting properties from all of them. Among these classes, one is characterized as the primary class of the object. The notion of a primary class helps achieving a “conceptual” as well as a physical clustering among similar objects. Collections of objects are supported as regular objects in M@@@@SE in the form of sets, multisets (bags), and arrays. The extent of a class, i.e., the objects that are known members of the class, is explicitly stored in the database. Every M@@@@SE database schema has a straightforward directed graph representation. Each node represents a class of objects and is labeled by the class name. Relationships between the classes in the schema are captured by the arcs of the graph. Similarly to most object-oriented data models, M@@@@SE has two major types of arcs component arcs and inheritance arcs. The former capture structural relationships (part-of), whereas the latter capture semantic relationships (is-a) between classes of objects.Although a component arc (A → B) has a direction from the parent to the child class, it relates the two classes in both directions. An object in A has a part that is in B, an object in B is part of an object that is in A. The values of the attributes of an object may be explicitly assigned by the user or derived by the system through rules associated with the attribute. Although not restricted to those, aggregates of collections are the dominant kind of such derived attributes. The values assigned to derived attributes may be explicitly stored in the database (forward application of rules), or may be inferred on demand (backward application of rules). Null values are supported in M@@@@SE and are interpreted as “no related object.” Thus, null is distinct from any other value. Also, default values are supported in their full generality M@@@@SE supports four types of user-defined structural constraints that can be used to control sharing or existence dependence between objects. These are (a) sharing of objects among collections, (b) sharing of objects among their parents, (c) sharing of objects along different paths in the component graph, and (d) existence dependence of a child to its parents.An inheritance arc (A → B) implies that every object in B is in A also B(X) → A(X). This rule is called a generalization rule and is implicitly assumed for every inheritance arc. In addition to an implicit generalization rule, an inheritance arc may be explicitly associated with a specialization rule, which specifies which elements of the parent class belong to the child class A specialized class can be instantiated at all times, or its objects can be retrieved on demand, by applying the corresponding rule(s) on an as-needed basis. Although the semantics of simple specialization rules could be captured by typing information and inheritance, specialization rules can be arbitrarily complex, thus allowing the necessary generality to capture the level of abstraction needed for experiment management.We are currently designing a query language for M@@@@SE that will take advantage of its features and will give the system analyst the capability to express complex queries easily. Multiple inheritance will be a significant feature of the language, and one of the key sources of its power and ease of use. Work planned for the future includes building a graphics user-interface to the DBMS that will take advantage of the graphic representation of a M@@@@SE schema and will allow the user to express queries as paths on the graph. The ability to express all types of constraints in a graphic form will then become very important. Some preliminary work on the subject has shown that focusing on different parts of the database establishes different contexts, within which the same constraint can be expressed in different ways. We are currently studying this relationship between constraints and contexts and plan to incorporate it in the query language as well as in the graphics interface.},
journal = {SIGMOD Rec.},
month = jun,
pages = {200},
numpages = {1}
}

@inproceedings{10.1145/50202.50225,
author = {Ono, Kiyoshi and Aoyama, Mikio and Fujimoto, Hiroshi},
title = {Data Management of Telecommunications Networks},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50225},
doi = {10.1145/50202.50225},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {201},
numpages = {1},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50225,
author = {Ono, Kiyoshi and Aoyama, Mikio and Fujimoto, Hiroshi},
title = {Data Management of Telecommunications Networks},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50225},
doi = {10.1145/971701.50225},
journal = {SIGMOD Rec.},
month = jun,
pages = {201},
numpages = {1}
}

@inproceedings{10.1145/50202.50226,
author = {Alho, Kari and Peltonen, Hannu and M\"{a}ntyl\"{a}, Martti and Sulonen, Rejio},
title = {A Design Data Manager},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50226},
doi = {10.1145/50202.50226},
abstract = {HutBase is a visual design data manager that can be used to store and manipulate data objects created and processed by a variety of design applications. In particular, HutBase allows the user to manipulate the data and start applications, and provides a access mechanism for the applications.HutBase consists of three software layers. The lowest layer, the Object Management System (OMS), is based on the Entity-Relationship model and includes those basic operations related to the storage and access of design data objects that are common to all applications. The database is divided into workspaces, which are collections of OMS objects and relationships organized according to an application-dependent schema and forming a significant whole (e.g., a design project) from the user's point of view Workspace is also the unit for locking and access control.An object is a collection of attributes. Each attribute has a name and value. The name is a string and the value is an arbitrary sequence of bytes. The value of an attribute can be of any length, from a single integer to an external representation of a complicated geometric model. A relationship is a named directed connection between two objects. Relationships have attributes like objects.The OMS library contains functions for creating, opening and removing workspaces, objects, relationships and attributes. All operations are carried out within transactions. The functions do not change the permanent data on the disk until the user calls the save_changes function, which saves the current state of all workspaces opened in a given transaction.The next layer is a prototype data model built on top of OMS, which stores the objects in each workspace as a hierarchical tree by means of relationships. The leaves of the hierarchy are called representations and contain the actual data manipulated by the applications. Each representation is associated with a representation type, which in turn are linked to the application programs, or tools. The representation types and tools are stored as objects in a separate workspace.The top level contains a user interface and a procedural application interface. The user interface shows the available representation types, tools, and contents of one or more workspaces in iconic form. A representation can be opened by selecting its icon on the screen. The tool corresponding to the type of the representation is then started with a handle to the representation as argument. The interface also allows the user to create, remove and copy objects.The tool programs run as subprocesses of the HutBase process. Tools access the data base by remote procedure calls that send data base requests from the tool process to the HutBase process. The tools can also create relationships between representations and navigate in the workspace by following the relationship links.We are currently working on a interpreted definition language that can be used to describe the structure of a workspace. The definition language will be based on an object-oriented notation, where object and relation types form a class hierarchy. Class descriptions include (possibly inherited) methods for dealing with the various HutBase operations. With the contemplated description facility, new object and relationship types can be defined by declaring new subclasses of the existing ones.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {202},
numpages = {1},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50226,
author = {Alho, Kari and Peltonen, Hannu and M\"{a}ntyl\"{a}, Martti and Sulonen, Rejio},
title = {A Design Data Manager},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50226},
doi = {10.1145/971701.50226},
abstract = {HutBase is a visual design data manager that can be used to store and manipulate data objects created and processed by a variety of design applications. In particular, HutBase allows the user to manipulate the data and start applications, and provides a access mechanism for the applications.HutBase consists of three software layers. The lowest layer, the Object Management System (OMS), is based on the Entity-Relationship model and includes those basic operations related to the storage and access of design data objects that are common to all applications. The database is divided into workspaces, which are collections of OMS objects and relationships organized according to an application-dependent schema and forming a significant whole (e.g., a design project) from the user's point of view Workspace is also the unit for locking and access control.An object is a collection of attributes. Each attribute has a name and value. The name is a string and the value is an arbitrary sequence of bytes. The value of an attribute can be of any length, from a single integer to an external representation of a complicated geometric model. A relationship is a named directed connection between two objects. Relationships have attributes like objects.The OMS library contains functions for creating, opening and removing workspaces, objects, relationships and attributes. All operations are carried out within transactions. The functions do not change the permanent data on the disk until the user calls the save_changes function, which saves the current state of all workspaces opened in a given transaction.The next layer is a prototype data model built on top of OMS, which stores the objects in each workspace as a hierarchical tree by means of relationships. The leaves of the hierarchy are called representations and contain the actual data manipulated by the applications. Each representation is associated with a representation type, which in turn are linked to the application programs, or tools. The representation types and tools are stored as objects in a separate workspace.The top level contains a user interface and a procedural application interface. The user interface shows the available representation types, tools, and contents of one or more workspaces in iconic form. A representation can be opened by selecting its icon on the screen. The tool corresponding to the type of the representation is then started with a handle to the representation as argument. The interface also allows the user to create, remove and copy objects.The tool programs run as subprocesses of the HutBase process. Tools access the data base by remote procedure calls that send data base requests from the tool process to the HutBase process. The tools can also create relationships between representations and navigate in the workspace by following the relationship links.We are currently working on a interpreted definition language that can be used to describe the structure of a workspace. The definition language will be based on an object-oriented notation, where object and relation types form a class hierarchy. Class descriptions include (possibly inherited) methods for dealing with the various HutBase operations. With the contemplated description facility, new object and relationship types can be defined by declaring new subclasses of the existing ones.},
journal = {SIGMOD Rec.},
month = jun,
pages = {202},
numpages = {1}
}

@inproceedings{10.1145/50202.50227,
author = {Naeymi-Rad, Frank and Carmony, Lowell and Trace, David and Georgakis, Christine and Weil, Max Harry},
title = {A Relational Database Design in Support of Standard Medical Terminology in Multi-Domain Knowledge Bases},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50227},
doi = {10.1145/50202.50227},
abstract = {Relational database techniques have been used to create knowledge bases for a medical diagnostic consultant system. Known as MEDAS (Medical Emergency Decision Assistance System), this expert system, using disorder patterns consisting of features such as symptoms and laboratory results, is able to diagnose multiple disorders. Database technology has been used in MEDAS to develop knowledge engineering tools, called the TOOL BOX, which permit domain experts to create knowledge without the assistance of a knowledge engineer.In the process of knowledge development with the TOOL BOX a standardization of terms was needed. This led us to design a Feature Dictionary and a grammar to support a standardized format for features. A common dictionary of features will allow us to merge knowledge bases, translate between multi-domain bases, and compare competing expert systems. In addition, standard terminology will assist communication across domainsThe Feature Dictionary has the following attributes Long forms of the feature name (White Blood Count) and short forms (WBC) as well as a three line description of the feature. The type, binary (Abdominal Pain), continuous-valued (WBC), or derived (pulse pressure = systolic - diastolic) is also kept for each featureFor value features the appropriate unit (cc, kg, etc.) as well as range limits are stored so that these can be used as a form of quality control on input. The permanence (Y/N) of each feature is kept so it is possible to automatically include permanent features in future encounters. In addition, for each feature three separate “cost” parameters are kept. Risk measures the danger to the patient from no risk such as taking a blood pressure to highly invasive proceedings such as a liver biopsy. Time measures whether results can be expected in minutes, hours, or days. Money measures the actual cost to the patient FD-Equivalents stores the synonyms and antonyms of each feature. These are used to translate between knowledge bases using different terminology.Features were first classified in terms of a Problem Oriented Medical Record. We have added an anatomical reclassification in terms of body systems. Experts will be able to add new kinds of feature classifications.MEDAS, a multi-membership Bayesian model, needs binary representations for its inference. These Binary Features are created by the expert physician in the given disorder patterns. For example, “WBC &gt; 50,000”, or “Age &gt; 2 &amp; Female &amp; Hematocrit &gt; 42” are binary features that might appear in a disorder pattern. Laboratory results often lead to a multiplicity of binary features (such as “WBC ≤ 3,000”, or 3,000 ≤ WBC ≤ 10,000, etc.). Our design allows the user to enter the value of such a feature and have the system set of all the corresponding binary features. This intelligent user interface is controlled by a grammar that allows us to parse the binary features and generate rules for them.The knowledge base for a particular problem domain such as OB/GYN is organized as a collection of disorder patterns. Each of these is represented as a list of binary features and associated probabilities. The domain knowledge base contains only the features relevant to that domain.Experience with the Feature Dictionary has convinced us that there are many advantages in using a DBMS to store the knowledge base for an expert system. The TOOL BOX, originally in ACCENT-R, was rewritten in dBase III for the PC. The knowledge bases created on the PC were then ported to the mainframe. As the number of domains supported by MEDAS grew, it became evident that we needed a DBMS that could function in both environments so we are in the process of converting to ORACLE.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {203},
numpages = {1},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50227,
author = {Naeymi-Rad, Frank and Carmony, Lowell and Trace, David and Georgakis, Christine and Weil, Max Harry},
title = {A Relational Database Design in Support of Standard Medical Terminology in Multi-Domain Knowledge Bases},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50227},
doi = {10.1145/971701.50227},
abstract = {Relational database techniques have been used to create knowledge bases for a medical diagnostic consultant system. Known as MEDAS (Medical Emergency Decision Assistance System), this expert system, using disorder patterns consisting of features such as symptoms and laboratory results, is able to diagnose multiple disorders. Database technology has been used in MEDAS to develop knowledge engineering tools, called the TOOL BOX, which permit domain experts to create knowledge without the assistance of a knowledge engineer.In the process of knowledge development with the TOOL BOX a standardization of terms was needed. This led us to design a Feature Dictionary and a grammar to support a standardized format for features. A common dictionary of features will allow us to merge knowledge bases, translate between multi-domain bases, and compare competing expert systems. In addition, standard terminology will assist communication across domainsThe Feature Dictionary has the following attributes Long forms of the feature name (White Blood Count) and short forms (WBC) as well as a three line description of the feature. The type, binary (Abdominal Pain), continuous-valued (WBC), or derived (pulse pressure = systolic - diastolic) is also kept for each featureFor value features the appropriate unit (cc, kg, etc.) as well as range limits are stored so that these can be used as a form of quality control on input. The permanence (Y/N) of each feature is kept so it is possible to automatically include permanent features in future encounters. In addition, for each feature three separate “cost” parameters are kept. Risk measures the danger to the patient from no risk such as taking a blood pressure to highly invasive proceedings such as a liver biopsy. Time measures whether results can be expected in minutes, hours, or days. Money measures the actual cost to the patient FD-Equivalents stores the synonyms and antonyms of each feature. These are used to translate between knowledge bases using different terminology.Features were first classified in terms of a Problem Oriented Medical Record. We have added an anatomical reclassification in terms of body systems. Experts will be able to add new kinds of feature classifications.MEDAS, a multi-membership Bayesian model, needs binary representations for its inference. These Binary Features are created by the expert physician in the given disorder patterns. For example, “WBC &gt; 50,000”, or “Age &gt; 2 &amp; Female &amp; Hematocrit &gt; 42” are binary features that might appear in a disorder pattern. Laboratory results often lead to a multiplicity of binary features (such as “WBC ≤ 3,000”, or 3,000 ≤ WBC ≤ 10,000, etc.). Our design allows the user to enter the value of such a feature and have the system set of all the corresponding binary features. This intelligent user interface is controlled by a grammar that allows us to parse the binary features and generate rules for them.The knowledge base for a particular problem domain such as OB/GYN is organized as a collection of disorder patterns. Each of these is represented as a list of binary features and associated probabilities. The domain knowledge base contains only the features relevant to that domain.Experience with the Feature Dictionary has convinced us that there are many advantages in using a DBMS to store the knowledge base for an expert system. The TOOL BOX, originally in ACCENT-R, was rewritten in dBase III for the PC. The knowledge bases created on the PC were then ported to the mainframe. As the number of domains supported by MEDAS grew, it became evident that we needed a DBMS that could function in both environments so we are in the process of converting to ORACLE.},
journal = {SIGMOD Rec.},
month = jun,
pages = {203},
numpages = {1}
}

@inproceedings{10.1145/50202.50228,
author = {Hern\'{a}ndez, H\'{e}ctor J. and Chan, Edward P. F.},
title = {A Characterization of Constant-Time Maintainability for BCNF Database Schemes},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50228},
doi = {10.1145/50202.50228},
abstract = {The maintenance problem (for database states) of a database scheme R with respect to a set of functional dependencies F is the following decision problem. Let r be a consistent state of R with respect to F and assume we insert a tuple t into rp ε r. Is r ∪ {t} a consistent state of R with respect to F? R is said to be constant-time-maintainable with respect to F if there is an algorithm that solves the maintenance problem of R with respect to F in time independent of the state size.A characterization of constant-time-maintainability for the class of BCNF database schemes is given. An efficient algorithm that tests this characterization is shown, as well as an algorithm for solving the maintenance problem in time independent of the state size. It is also proven that constant-time-maintainable BCNF database schemes are bounded. In particular, it is shown that total projections of the representative instance can be computed via unions of projections of extension joins. Throughout we assume that database schemes are cover embedding and BCNF, and that functional dependencies are given in the form of key dependencies.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {209–217},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50228,
author = {Hern\'{a}ndez, H\'{e}ctor J. and Chan, Edward P. F.},
title = {A Characterization of Constant-Time Maintainability for BCNF Database Schemes},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50228},
doi = {10.1145/971701.50228},
abstract = {The maintenance problem (for database states) of a database scheme R with respect to a set of functional dependencies F is the following decision problem. Let r be a consistent state of R with respect to F and assume we insert a tuple t into rp ε r. Is r ∪ {t} a consistent state of R with respect to F? R is said to be constant-time-maintainable with respect to F if there is an algorithm that solves the maintenance problem of R with respect to F in time independent of the state size.A characterization of constant-time-maintainability for the class of BCNF database schemes is given. An efficient algorithm that tests this characterization is shown, as well as an algorithm for solving the maintenance problem in time independent of the state size. It is also proven that constant-time-maintainable BCNF database schemes are bounded. In particular, it is shown that total projections of the representative instance can be computed via unions of projections of extension joins. Throughout we assume that database schemes are cover embedding and BCNF, and that functional dependencies are given in the form of key dependencies.},
journal = {SIGMOD Rec.},
month = jun,
pages = {209–217},
numpages = {9}
}

@inproceedings{10.1145/50202.50229,
author = {Leuchner, J. and Miller, L. and Slutzki, G.},
title = {A Polynomial Time Algorithm for Testing Implications of a Join Dependency and Embodied Functional Dependencies},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50229},
doi = {10.1145/50202.50229},
abstract = {The problem of deciding whether a full join dependency (JD) ⋈ [R] and a set of functional dependencies (FDs) F imply an embedded join dependency (EJD) ⋈ [S] is known to be NP-complete. We show that the problem can be decided in polynomial time if S ⊆ R and F is embedded in R. Our work uses arguments based on an extension of complete intersection graphs rather than tableaus. This approach has facilitated our results and should prove useful for future research.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {218–224},
numpages = {7},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50229,
author = {Leuchner, J. and Miller, L. and Slutzki, G.},
title = {A Polynomial Time Algorithm for Testing Implications of a Join Dependency and Embodied Functional Dependencies},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50229},
doi = {10.1145/971701.50229},
abstract = {The problem of deciding whether a full join dependency (JD) ⋈ [R] and a set of functional dependencies (FDs) F imply an embedded join dependency (EJD) ⋈ [S] is known to be NP-complete. We show that the problem can be decided in polynomial time if S ⊆ R and F is embedded in R. Our work uses arguments based on an extension of complete intersection graphs rather than tableaus. This approach has facilitated our results and should prove useful for future research.},
journal = {SIGMOD Rec.},
month = jun,
pages = {218–224},
numpages = {7}
}

@inproceedings{10.1145/50202.50230,
author = {Gyssens, Marc and van Gucht, Dirk},
title = {The Powerset Algebra as a Result of Adding Programming Constructs to the Nested Relational Algebra},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50230},
doi = {10.1145/50202.50230},
abstract = {In this paper, we discuss augmentations of the nested relational algebra with programming constructs, such as while-loops and for-loops. We show that the algebras obtained in this way are equivalent to a slight extension of the powerset algebra, thus emphasizing both the strength and the naturalness of the powerset algebra as a tool to manipulate nested relations, and, at the same time, indicating more direct ways to implement this algebra.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {225–232},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50230,
author = {Gyssens, Marc and van Gucht, Dirk},
title = {The Powerset Algebra as a Result of Adding Programming Constructs to the Nested Relational Algebra},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50230},
doi = {10.1145/971701.50230},
abstract = {In this paper, we discuss augmentations of the nested relational algebra with programming constructs, such as while-loops and for-loops. We show that the algebras obtained in this way are equivalent to a slight extension of the powerset algebra, thus emphasizing both the strength and the naturalness of the powerset algebra as a tool to manipulate nested relations, and, at the same time, indicating more direct ways to implement this algebra.},
journal = {SIGMOD Rec.},
month = jun,
pages = {225–232},
numpages = {8}
}

@inproceedings{10.1145/50202.50231,
author = {Mazumdar, Subhasish and Stemple, David and Sheard, Tim},
title = {Resolving the Tension between Integrity and Security Using a Theorem Prover},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50231},
doi = {10.1145/50202.50231},
abstract = {Some information in databases and knowledge bases often needs to be protected from disclosure to certain users. Traditional solutions involving multi-level mechanisms are threatened by the user's ability to infer higher level information from the semantics of the application. We concentrate on the revelation of secrets through a user running transactions in the presence of database integrity constraints. We develop a method of specifying secrets formally that not only exposes a useful structure and equivalence among secrets but also allows a theorem prover to detect certain security lapses during transaction compilation time.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {233–242},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50231,
author = {Mazumdar, Subhasish and Stemple, David and Sheard, Tim},
title = {Resolving the Tension between Integrity and Security Using a Theorem Prover},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50231},
doi = {10.1145/971701.50231},
abstract = {Some information in databases and knowledge bases often needs to be protected from disclosure to certain users. Traditional solutions involving multi-level mechanisms are threatened by the user's ability to infer higher level information from the semantics of the application. We concentrate on the revelation of secrets through a user running transactions in the presence of database integrity constraints. We develop a method of specifying secrets formally that not only exposes a useful structure and equivalence among secrets but also allows a theorem prover to detect certain security lapses during transaction compilation time.},
journal = {SIGMOD Rec.},
month = jun,
pages = {233–242},
numpages = {10}
}

@inproceedings{10.1145/50202.50232,
author = {Qian, Xiaolei and Waldinger, Richard},
title = {A Transaction Logic for Database Specification},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50232},
doi = {10.1145/50202.50232},
abstract = {We introduce a logical formalism for the specification of the dynamic behavior of databases. The evolution of databases is characterized by both the dynamic integrity constraints which describe the properties of state transitions and the transactions whose executions lead to state transitions. Our formalism is based on a variant of first-order situational logic in which the states of computations are explicit objects. Integrity constraints and transactions are uniformly specifiable as expressions in our language. We also point out the application of the formalism to the verification and synthesis of transactions.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {243–250},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50232,
author = {Qian, Xiaolei and Waldinger, Richard},
title = {A Transaction Logic for Database Specification},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50232},
doi = {10.1145/971701.50232},
abstract = {We introduce a logical formalism for the specification of the dynamic behavior of databases. The evolution of databases is characterized by both the dynamic integrity constraints which describe the properties of state transitions and the transactions whose executions lead to state transitions. Our formalism is based on a variant of first-order situational logic in which the states of computations are explicit objects. Integrity constraints and transactions are uniformly specifiable as expressions in our language. We also point out the application of the formalism to the verification and synthesis of transactions.},
journal = {SIGMOD Rec.},
month = jun,
pages = {243–250},
numpages = {8}
}

@inproceedings{10.1145/50202.50233,
author = {Gadia, Shashi K. and Yeung, Chuen-Sing},
title = {A Generalized Model for a Relational Temporal Database},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50233},
doi = {10.1145/50202.50233},
abstract = {We propose a generalized relational model for a temporal database which allows time stamping with respect to a Boolean algebra of multidimensional time stamps. The interplay between the various temporal dimensions is symmetric. As an application, a two dimensional model which allows objects with real world and transaction oriented time stamps is discussed. The two dimensional model can be used to query the past states of the database. It can also be used to give a precise classification of the errors and updates in a database, and is a promising approach for querying these errors and updates.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {251–259},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50233,
author = {Gadia, Shashi K. and Yeung, Chuen-Sing},
title = {A Generalized Model for a Relational Temporal Database},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50233},
doi = {10.1145/971701.50233},
abstract = {We propose a generalized relational model for a temporal database which allows time stamping with respect to a Boolean algebra of multidimensional time stamps. The interplay between the various temporal dimensions is symmetric. As an application, a two dimensional model which allows objects with real world and transaction oriented time stamps is discussed. The two dimensional model can be used to query the past states of the database. It can also be used to give a precise classification of the errors and updates in a database, and is a promising approach for querying these errors and updates.},
journal = {SIGMOD Rec.},
month = jun,
pages = {251–259},
numpages = {9}
}

@inproceedings{10.1145/50202.50234,
author = {Peinl, Peter and Reuter, Andreas and Sammer, Harald},
title = {High Contention in a Stock Trading Database: A Case Study},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50234},
doi = {10.1145/50202.50234},
abstract = {Though in general, current database systems adequately support application development and operation for online transaction processing (OLTP), increasing complexity of applications and throughput requirements reveal a number of weaknesses with respect to the data model and implementation techniques used. By presenting the experiences gained from a case study of a large, high volume stock trading system, representative for a broad class of OLTP applications, it is shown, that this particularly holds for dealing with high frequency access to a small number of data elements (hot spots). As a result, we propose extended data types and several novel mechanisms, which are easy to use and highly increase the expressional power of transaction oriented programming, that effectively cope with hot spots. Moreover, their usefulness and their ability to increased parallelism is exemplified by the stock trading application.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {260–268},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50234,
author = {Peinl, Peter and Reuter, Andreas and Sammer, Harald},
title = {High Contention in a Stock Trading Database: A Case Study},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50234},
doi = {10.1145/971701.50234},
abstract = {Though in general, current database systems adequately support application development and operation for online transaction processing (OLTP), increasing complexity of applications and throughput requirements reveal a number of weaknesses with respect to the data model and implementation techniques used. By presenting the experiences gained from a case study of a large, high volume stock trading system, representative for a broad class of OLTP applications, it is shown, that this particularly holds for dealing with high frequency access to a small number of data elements (hot spots). As a result, we propose extended data types and several novel mechanisms, which are easy to use and highly increase the expressional power of transaction oriented programming, that effectively cope with hot spots. Moreover, their usefulness and their ability to increased parallelism is exemplified by the stock trading application.},
journal = {SIGMOD Rec.},
month = jun,
pages = {260–268},
numpages = {9}
}

@inproceedings{10.1145/50202.50235,
author = {Haynie, M.},
title = {A DBMS for Large Design Automation Databases},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50235},
doi = {10.1145/50202.50235},
abstract = {Large capacity Design Automation (CAD/CAM) database management systems require special capabilities over and above what commercial DBMSs or small workstation-based CAD/CAM systems provide. This paper describes one such system, Tacoma, used at Amdahl Corporation for the storage and retrieval of LSI and VLSI mainframe computer designs Tacoma is based on the relational model with additional object-oriented database features.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {269–276},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50235,
author = {Haynie, M.},
title = {A DBMS for Large Design Automation Databases},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50235},
doi = {10.1145/971701.50235},
abstract = {Large capacity Design Automation (CAD/CAM) database management systems require special capabilities over and above what commercial DBMSs or small workstation-based CAD/CAM systems provide. This paper describes one such system, Tacoma, used at Amdahl Corporation for the storage and retrieval of LSI and VLSI mainframe computer designs Tacoma is based on the relational model with additional object-oriented database features.},
journal = {SIGMOD Rec.},
month = jun,
pages = {269–276},
numpages = {8}
}

@inproceedings{10.1145/50202.50236,
author = {Bell, Jean L.},
title = {A Specialized Data Management System for Parallel Execution of Particle Physics Codes},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50236},
doi = {10.1145/50202.50236},
abstract = {The specialized data management system described in this paper was motivated by the need for much more efficient data management than a standard database management system could provide for particle physics codes in shared memory multiprocessor environments. The special characteristics of data and access patterns in particle physics codes need to be fully exploited in order to effect efficient data management. The data management system allows parameteric user control over system features not usually available to them, especially details of physical design and retrieval such as horizontal clustering, asynchronous I/O, and automatic distribution across processors. In the past, each physics code has constructed the equivalent of a primitive data management system from scratch. The system described in this paper is a generic system that can now be interfaced with a variety of physics codes.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {277–285},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50236,
author = {Bell, Jean L.},
title = {A Specialized Data Management System for Parallel Execution of Particle Physics Codes},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50236},
doi = {10.1145/971701.50236},
abstract = {The specialized data management system described in this paper was motivated by the need for much more efficient data management than a standard database management system could provide for particle physics codes in shared memory multiprocessor environments. The special characteristics of data and access patterns in particle physics codes need to be fully exploited in order to effect efficient data management. The data management system allows parameteric user control over system features not usually available to them, especially details of physical design and retrieval such as horizontal clustering, asynchronous I/O, and automatic distribution across processors. In the past, each physics code has constructed the equivalent of a primitive data management system from scratch. The system described in this paper is a generic system that can now be interfaced with a variety of physics codes.},
journal = {SIGMOD Rec.},
month = jun,
pages = {277–285},
numpages = {9}
}

@inproceedings{10.1145/50202.50237,
author = {Christodoulakis, Stavros and Ford, Daniel Alexander},
title = {Performance Analysis and Fundamental Performance Tradeoffs for CLV Optical Disks},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50237},
doi = {10.1145/50202.50237},
abstract = {CLV type optical disks is a very large and important class of optical disk technology, of which CD-ROM disks form a subclass.In this paper we present a model of retrieval from CLV optical disks. We then provide exact and approximate results analyzing the retrieval performance from them. Our analysis takes into account disks with and without a mirror in the read mechanism, small objects completely placed within block boundaries, placement that allows block boundary crossing, as well as very large objects (such as documents) placed within files.In the second part of the paper we describe some fundamental implications of physical data base design for data bases stored on CLV optical disks. We show that very significant performance gains may be realized by appropriate design.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {286–294},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50237,
author = {Christodoulakis, Stavros and Ford, Daniel Alexander},
title = {Performance Analysis and Fundamental Performance Tradeoffs for CLV Optical Disks},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50237},
doi = {10.1145/971701.50237},
abstract = {CLV type optical disks is a very large and important class of optical disk technology, of which CD-ROM disks form a subclass.In this paper we present a model of retrieval from CLV optical disks. We then provide exact and approximate results analyzing the retrieval performance from them. Our analysis takes into account disks with and without a mirror in the read mechanism, small objects completely placed within block boundaries, placement that allows block boundary crossing, as well as very large objects (such as documents) placed within files.In the second part of the paper we describe some fundamental implications of physical data base design for data bases stored on CLV optical disks. We show that very significant performance gains may be realized by appropriate design.},
journal = {SIGMOD Rec.},
month = jun,
pages = {286–294},
numpages = {9}
}

@inproceedings{10.1145/50202.50238,
author = {Hanson, Eric N.},
title = {Processing Queries Aganist Database Procedures: A Performance Analysis},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50238},
doi = {10.1145/50202.50238},
abstract = {A database procedure is a collection of queries stored in the database. Several methods are possible for processing queries that retrieve the value returned by a database procedure. The conventional algorithm is to execute the queries in a procedure whenever it is accessed. A second strategy requires caching the previous value returned by the database procedure. If the cached value is valid at the time of a query, the value is returned immediately. If the cached value has been invalidated by an update, the value is recomputed, stored back into the cache, and then returned. A third strategy uses a differential view maintenance algorithm to maintain an up-to-date copy of the value returned by the procedure. This paper compares the performance of these three alternatives. The results show that which algorithm is preferred depends heavily on the database environment, particularly, the frequency of updates and the size of objects retrieved by database procedures.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {295–302},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50238,
author = {Hanson, Eric N.},
title = {Processing Queries Aganist Database Procedures: A Performance Analysis},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50238},
doi = {10.1145/971701.50238},
abstract = {A database procedure is a collection of queries stored in the database. Several methods are possible for processing queries that retrieve the value returned by a database procedure. The conventional algorithm is to execute the queries in a procedure whenever it is accessed. A second strategy requires caching the previous value returned by the database procedure. If the cached value is valid at the time of a query, the value is returned immediately. If the cached value has been invalidated by an update, the value is recomputed, stored back into the cache, and then returned. A third strategy uses a differential view maintenance algorithm to maintain an up-to-date copy of the value returned by the procedure. This paper compares the performance of these three alternatives. The results show that which algorithm is preferred depends heavily on the database environment, particularly, the frequency of updates and the size of objects retrieved by database procedures.},
journal = {SIGMOD Rec.},
month = jun,
pages = {295–302},
numpages = {8}
}

@inproceedings{10.1145/50202.50239,
author = {Jarke, Matthias and Rose, Thomas},
title = {Managing Knowledge about Information System Evolution},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50239},
doi = {10.1145/50202.50239},
abstract = {This paper describes the design and initial prototype implementation of a knowledge base management system (KBMS) for controlling database software development and maintenance. The KBMS employs a version of the conceptual modelling language CML to represent knowledge about the tool-aided development process of an information system from requirements analysis to conceptual design to implementation, together with the relationship of these system components to the real-world environment in which the information system is intended to function. A decision-centered documentation methodology facilitates communication across time and among multiple developers (and possibly users), thus enabling improved maintenance support.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {303–311},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50239,
author = {Jarke, Matthias and Rose, Thomas},
title = {Managing Knowledge about Information System Evolution},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50239},
doi = {10.1145/971701.50239},
abstract = {This paper describes the design and initial prototype implementation of a knowledge base management system (KBMS) for controlling database software development and maintenance. The KBMS employs a version of the conceptual modelling language CML to represent knowledge about the tool-aided development process of an information system from requirements analysis to conceptual design to implementation, together with the relationship of these system components to the real-world environment in which the information system is intended to function. A decision-centered documentation methodology facilitates communication across time and among multiple developers (and possibly users), thus enabling improved maintenance support.},
journal = {SIGMOD Rec.},
month = jun,
pages = {303–311},
numpages = {9}
}

@inproceedings{10.1145/50202.50240,
author = {Naughton, Jeffrey F.},
title = {Compiling Separable Recursions},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50240},
doi = {10.1145/50202.50240},
abstract = {In this paper we consider evaluating queries on relations defined by a combination of recursive rules. We first define separable recursions. We then give a specialized algorithm for evaluating selections on separable recursions. Like the Generalized Magic Sets and Generalized Counting algorithms, thus algorithm uses selection constants to avoid examining irrelevant portions of the database, however, on some simple recursions this algorithm is Ο(n), whereas Generalized Magic Sets is Ω(n2) and Generalized Counting is Ω(2n)},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {312–319},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50240,
author = {Naughton, Jeffrey F.},
title = {Compiling Separable Recursions},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50240},
doi = {10.1145/971701.50240},
abstract = {In this paper we consider evaluating queries on relations defined by a combination of recursive rules. We first define separable recursions. We then give a specialized algorithm for evaluating selections on separable recursions. Like the Generalized Magic Sets and Generalized Counting algorithms, thus algorithm uses selection constants to avoid examining irrelevant portions of the database, however, on some simple recursions this algorithm is Ο(n), whereas Generalized Magic Sets is Ω(n2) and Generalized Counting is Ω(2n)},
journal = {SIGMOD Rec.},
month = jun,
pages = {312–319},
numpages = {8}
}

@inproceedings{10.1145/50202.50241,
author = {Youn, Cheong and Henschen, Lawrence J. and Han, Jiawei},
title = {Classification of Recursive Formulas in Deductive Databases},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50241},
doi = {10.1145/50202.50241},
abstract = {In this paper, we present results on the classification of linear recursive formulas in deductive databases and apply those results to the compilation and optimization of recursive queries. We also introduce compiled formulas and query evaluation plans for a representative query for each of these classes.To explain general recursive formulas, we use a graph model that shows the connectivity between variables. The connecticity between variables is the most critical part in processing recursive formulas. We demonstrate that based on such a graph model all the linear recursive formulas can be classified into several classes and each class shares some common characteristics in compilation and query processing. The compiled formulas and the corresponding query evaluation plans can be derived based on the study of the compilation of each class.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {320–328},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50241,
author = {Youn, Cheong and Henschen, Lawrence J. and Han, Jiawei},
title = {Classification of Recursive Formulas in Deductive Databases},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50241},
doi = {10.1145/971701.50241},
abstract = {In this paper, we present results on the classification of linear recursive formulas in deductive databases and apply those results to the compilation and optimization of recursive queries. We also introduce compiled formulas and query evaluation plans for a representative query for each of these classes.To explain general recursive formulas, we use a graph model that shows the connectivity between variables. The connecticity between variables is the most critical part in processing recursive formulas. We demonstrate that based on such a graph model all the linear recursive formulas can be classified into several classes and each class shares some common characteristics in compilation and query processing. The compiled formulas and the corresponding query evaluation plans can be derived based on the study of the compilation of each class.},
journal = {SIGMOD Rec.},
month = jun,
pages = {320–328},
numpages = {9}
}

@inproceedings{10.1145/50202.50242,
author = {Wolfson, Ouri and Silberschatz, Avi},
title = {Distributed Processing of Logic Programs},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50242},
doi = {10.1145/50202.50242},
abstract = {This paper is concerned with the issue of parallel evaluation of logic programs. To address this issue we define a new concept of predicate decomposability. If a predicate is decomposable, it means that the load of evaluating it can be divided among a number of processors, without a need for communication among them. This in turn results in a very significant speed-up of the evaluation process.We completely characterize three classes of single rule programs (sirups) with respect to decomposability nonrecursive, linear, and simple chain programs. All three classes were studied previously in various contexts. We establish that nonrecursive programs are decomposable, whereas for the other two classes we determine which ones are, and which ones are not decomposable. We also establish two sufficient conditions for sirup decomposability.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {329–336},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50242,
author = {Wolfson, Ouri and Silberschatz, Avi},
title = {Distributed Processing of Logic Programs},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50242},
doi = {10.1145/971701.50242},
abstract = {This paper is concerned with the issue of parallel evaluation of logic programs. To address this issue we define a new concept of predicate decomposability. If a predicate is decomposable, it means that the load of evaluating it can be divided among a number of processors, without a need for communication among them. This in turn results in a very significant speed-up of the evaluation process.We completely characterize three classes of single rule programs (sirups) with respect to decomposability nonrecursive, linear, and simple chain programs. All three classes were studied previously in various contexts. We establish that nonrecursive programs are decomposable, whereas for the other two classes we determine which ones are, and which ones are not decomposable. We also establish two sufficient conditions for sirup decomposability.},
journal = {SIGMOD Rec.},
month = jun,
pages = {329–336},
numpages = {8}
}

@inproceedings{10.1145/50202.50243,
author = {Tandem Performance Group},
title = {A Benchmark of NonStop SQL on the Debit Credit Transaction},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50243},
doi = {10.1145/50202.50243},
abstract = {NonStop SQL is an implementation of ANSI SQL on Tandem Computer Systems Debit Credit is a widely used industry-standard transaction. This paper summarizes a benchmark of NonStop SQL which demonstrated linear growth of throughout from 14 to 208 Debit Credit transactions per second as the hardware grew from 2 to 32 processors. The benchmark also compared the performance of NonStop SQL to the performance of a record-at a time file system interface},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {337–341},
numpages = {5},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50243,
author = {Tandem Performance Group},
title = {A Benchmark of NonStop SQL on the Debit Credit Transaction},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50243},
doi = {10.1145/971701.50243},
abstract = {NonStop SQL is an implementation of ANSI SQL on Tandem Computer Systems Debit Credit is a widely used industry-standard transaction. This paper summarizes a benchmark of NonStop SQL which demonstrated linear growth of throughout from 14 to 208 Debit Credit transactions per second as the hardware grew from 2 to 32 processors. The benchmark also compared the performance of NonStop SQL to the performance of a record-at a time file system interface},
journal = {SIGMOD Rec.},
month = jun,
pages = {337–341},
numpages = {5}
}

@inproceedings{10.1145/50202.50244,
author = {Borr, A.},
title = {High Performance SQL through Low-Level System Integration},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50244},
doi = {10.1145/50202.50244},
abstract = {NonStop SQL [TM] achieves high performance through an implementation which integrates SQL record access with the pre-existing disk I/O and transaction management subsystems, and moves SQL function downward from the client to the server level of these subsystems. System integration and movement of function to the server reduce message traffic and CPU consumption by putting SQL optimizations at the lower levels of the system. Examples of such optimizations are message traffic savings by filtering data and applying updates at the data source, I/O savings by SQL-optimized buffer pool management, and locking and transaction journaling techniques which take advantage of SQL semantics. Achieving message traffic reduction is particularly important in a distributed, non shared-memory architecture such as the Tandem NonStop System. The result of this implementation is an SQL system which matches the performance of the pre-existing DBMS, while inheriting such pre-existing architecturally-derived features as high availability, transaction-based data integrity, and distribution of both data and execution.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {342–349},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50244,
author = {Borr, A.},
title = {High Performance SQL through Low-Level System Integration},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50244},
doi = {10.1145/971701.50244},
abstract = {NonStop SQL [TM] achieves high performance through an implementation which integrates SQL record access with the pre-existing disk I/O and transaction management subsystems, and moves SQL function downward from the client to the server level of these subsystems. System integration and movement of function to the server reduce message traffic and CPU consumption by putting SQL optimizations at the lower levels of the system. Examples of such optimizations are message traffic savings by filtering data and applying updates at the data source, I/O savings by SQL-optimized buffer pool management, and locking and transaction journaling techniques which take advantage of SQL semantics. Achieving message traffic reduction is particularly important in a distributed, non shared-memory architecture such as the Tandem NonStop System. The result of this implementation is an SQL system which matches the performance of the pre-existing DBMS, while inheriting such pre-existing architecturally-derived features as high availability, transaction-based data integrity, and distribution of both data and execution.},
journal = {SIGMOD Rec.},
month = jun,
pages = {342–349},
numpages = {8}
}

@inproceedings{10.1145/50202.50245,
author = {DeWitt, D. J. and Ghanderaizadeh, S. and Schneider, D.},
title = {A Performance Analysis of the Gamma Database Machine},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50245},
doi = {10.1145/50202.50245},
abstract = {This paper presents the results of an initial performance evaluation of the Gamma database machine. In our experiments we measured the effect of relation size and indices on response time for selection, join, and aggregation queries, and single-tuple updates. A Teradata DBC/1012 database machine of similar size is used as a basis for interpreting the results obtained. We also analyze the performance of Gemma relative to the number of processors employed and study the impact of varying the memory size and disk page size on the execution time of a variety of selection and join queries. We analyze and interpret the results of these experiments based on our understanding of the system hardware and software, and conclude with an assessment of the strengths and weaknesses of Gamma.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {350–360},
numpages = {11},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50245,
author = {DeWitt, D. J. and Ghanderaizadeh, S. and Schneider, D.},
title = {A Performance Analysis of the Gamma Database Machine},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50245},
doi = {10.1145/971701.50245},
abstract = {This paper presents the results of an initial performance evaluation of the Gamma database machine. In our experiments we measured the effect of relation size and indices on response time for selection, join, and aggregation queries, and single-tuple updates. A Teradata DBC/1012 database machine of similar size is used as a basis for interpreting the results obtained. We also analyze the performance of Gemma relative to the number of processors employed and study the impact of varying the memory size and disk page size on the execution time of a variety of selection and join queries. We analyze and interpret the results of these experiments based on our understanding of the system hardware and software, and conclude with an assessment of the strengths and weaknesses of Gamma.},
journal = {SIGMOD Rec.},
month = jun,
pages = {350–360},
numpages = {11}
}

@inproceedings{10.1145/50202.50246,
author = {Roesler, M. and Burkhard, W. A.},
title = {Semantic Lock Models in Object-Oriented Distributed Systems and Deadlock Resolution},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50246},
doi = {10.1145/50202.50246},
abstract = {We propose a distributed algorithm for detection and resolution of resource deadlocks in object-oriented distributed systems. The algorithm proposed is shown to detect and resolve all O(n1) cycles present in the worst case waits-for-graph (WFG) with n vertices by transmitting O(n3) messages of small constant size. Its average time complexity has been shown to be O(ne), where e is the number of edges in the WFG After deadlock resolution, the algorithm leaves information in the system concerning dependence relations of running transactions. This information will preclude the wasteful retransmission of messages and reduce the delay in detecting future deadlocks.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {361–370},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50246,
author = {Roesler, M. and Burkhard, W. A.},
title = {Semantic Lock Models in Object-Oriented Distributed Systems and Deadlock Resolution},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50246},
doi = {10.1145/971701.50246},
abstract = {We propose a distributed algorithm for detection and resolution of resource deadlocks in object-oriented distributed systems. The algorithm proposed is shown to detect and resolve all O(n1) cycles present in the worst case waits-for-graph (WFG) with n vertices by transmitting O(n3) messages of small constant size. Its average time complexity has been shown to be O(ne), where e is the number of edges in the WFG After deadlock resolution, the algorithm leaves information in the system concerning dependence relations of running transactions. This information will preclude the wasteful retransmission of messages and reduce the delay in detecting future deadlocks.},
journal = {SIGMOD Rec.},
month = jun,
pages = {361–370},
numpages = {10}
}

@inproceedings{10.1145/50202.50247,
author = {Ramarao, K. V. S.},
title = {Commitment in a Partitioned Distributed Database},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50247},
doi = {10.1145/50202.50247},
abstract = {Network partition is among the hardest failure types in a distributed system even if all processors and links are of fail-stop type. We address the transaction commitment problem in a partitioned distributed database. It is assumed that partitions are detectable. The approach taken is conservative - that is, the same transaction cannot be committed by one site and aborted by another.A new and very general formal model of protocols operating in a partitioned system is introduced and protocols more efficient than the existing ones are constructed.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {371–378},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50247,
author = {Ramarao, K. V. S.},
title = {Commitment in a Partitioned Distributed Database},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50247},
doi = {10.1145/971701.50247},
abstract = {Network partition is among the hardest failure types in a distributed system even if all processors and links are of fail-stop type. We address the transaction commitment problem in a partitioned distributed database. It is assumed that partitions are detectable. The approach taken is conservative - that is, the same transaction cannot be committed by one site and aborted by another.A new and very general formal model of protocols operating in a partitioned system is introduced and protocols more efficient than the existing ones are constructed.},
journal = {SIGMOD Rec.},
month = jun,
pages = {371–378},
numpages = {8}
}

@inproceedings{10.1145/50202.50248,
author = {Korth, H. K. and Speegle, G.},
title = {Formal Model of Correctness without Serializabilty},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50248},
doi = {10.1145/50202.50248},
abstract = {In the classical approach to transaction processing, a concurrent execution is considered to be correct if it is equivalent to a non-concurrent schedule. This notion of correctness is called serializability. Serializability has proven to be a highly useful concept for transaction systems for data-processing style applications. Recent interest in applying database concepts to applications in computer-aided design, office information systems, etc. has resulted in transactions of relatively long duration. For such transactions, there are serious consequences to requiring serializability as the notion of correctness. Specifically, such systems either impose long-duration waits or require the abortion of long transactions. In this paper, we define a transaction model that allows for several alternative notions of correctness without the requirement of serializability. After introducing the model, we investigate classes of schedules for transactions. We show that these classes are richer than analogous classes under the classical model. Finally, we show the potential practicality of our model by describing protocols that permit a transaction manager to allow correct non-serializable executions},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {379–386},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50248,
author = {Korth, H. K. and Speegle, G.},
title = {Formal Model of Correctness without Serializabilty},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50248},
doi = {10.1145/971701.50248},
abstract = {In the classical approach to transaction processing, a concurrent execution is considered to be correct if it is equivalent to a non-concurrent schedule. This notion of correctness is called serializability. Serializability has proven to be a highly useful concept for transaction systems for data-processing style applications. Recent interest in applying database concepts to applications in computer-aided design, office information systems, etc. has resulted in transactions of relatively long duration. For such transactions, there are serious consequences to requiring serializability as the notion of correctness. Specifically, such systems either impose long-duration waits or require the abortion of long transactions. In this paper, we define a transaction model that allows for several alternative notions of correctness without the requirement of serializability. After introducing the model, we investigate classes of schedules for transactions. We show that these classes are richer than analogous classes under the classical model. Finally, we show the potential practicality of our model by describing protocols that permit a transaction manager to allow correct non-serializable executions},
journal = {SIGMOD Rec.},
month = jun,
pages = {379–386},
numpages = {8}
}

@inproceedings{10.1145/50202.50249,
author = {Ramnarayan, R. and Lu, H.},
title = {A Data/Knowledge Base Management Testbed and Experimental Results on Data/Knowledge Base Query and Update Processing},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50249},
doi = {10.1145/50202.50249},
abstract = {This paper presents our experience in designing and implementing a data/knowledge base management testbed. The testbed consists of two layers, the knowledge manager and the database management system, with the former at the top. The testbed is based on the logic programming paradigm, wherein data, knowledge, and queries are all expressed as Horn clauses. The knowledge manager compiles pure, function-free Horn clause queries into embedded-SQL programs, which are executed by the database management system to produce the query results. The database management system is a commercial relational database system and provides storage for both rules and facts. First, the testbed architecture and major data structures and algorithms are described. Then, several preliminary tests conducted using the current version of the testbed and the conclusions from the test results are presented. The principal contributions of this work have been to unify various concepts, both previously published and new ones we developed, into a real system and to present several insights into data/knowledge base management system design gleaned from the test results and our design and implementation experience.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {387–395},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50249,
author = {Ramnarayan, R. and Lu, H.},
title = {A Data/Knowledge Base Management Testbed and Experimental Results on Data/Knowledge Base Query and Update Processing},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50249},
doi = {10.1145/971701.50249},
abstract = {This paper presents our experience in designing and implementing a data/knowledge base management testbed. The testbed consists of two layers, the knowledge manager and the database management system, with the former at the top. The testbed is based on the logic programming paradigm, wherein data, knowledge, and queries are all expressed as Horn clauses. The knowledge manager compiles pure, function-free Horn clause queries into embedded-SQL programs, which are executed by the database management system to produce the query results. The database management system is a commercial relational database system and provides storage for both rules and facts. First, the testbed architecture and major data structures and algorithms are described. Then, several preliminary tests conducted using the current version of the testbed and the conclusions from the test results are presented. The principal contributions of this work have been to unify various concepts, both previously published and new ones we developed, into a real system and to present several insights into data/knowledge base management system design gleaned from the test results and our design and implementation experience.},
journal = {SIGMOD Rec.},
month = jun,
pages = {387–395},
numpages = {9}
}

@inproceedings{10.1145/50202.50250,
author = {Delcambre, L. M.L. and Etheredge, J. N.},
title = {A Self-Controlling Interpreter for the Relational Production Language},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50250},
doi = {10.1145/50202.50250},
abstract = {The Relational Production Language (RPL) solves the paradigm mismatch between expert systems and database systems by relying on the relational data model as the underlying formalism for an expert system. The result is a formally-defined production system language with immediate access to conventional databases. Working memory is modeled as a relational database and rules consist of a relational query on the left hand side (LHS) and database updates on the right hand side (RHS). This paper reports on the design of the RPL 1 0 prototype. The prototype directly executes RPL programs and capitalizes on the inherent advantages of the relational approach, particularly for intra-rule and inter-rule parallelism. By using a self-describing approach for representing the interpreter data structures, the interpreter is a self-controlling system that allows conflict resolution, error handling and a wide spectrum of software metrics to be explicitly specified using RPL meta-rules.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {396–403},
numpages = {8},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50250,
author = {Delcambre, L. M.L. and Etheredge, J. N.},
title = {A Self-Controlling Interpreter for the Relational Production Language},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50250},
doi = {10.1145/971701.50250},
abstract = {The Relational Production Language (RPL) solves the paradigm mismatch between expert systems and database systems by relying on the relational data model as the underlying formalism for an expert system. The result is a formally-defined production system language with immediate access to conventional databases. Working memory is modeled as a relational database and rules consist of a relational query on the left hand side (LHS) and database updates on the right hand side (RHS). This paper reports on the design of the RPL 1 0 prototype. The prototype directly executes RPL programs and capitalizes on the inherent advantages of the relational approach, particularly for intra-rule and inter-rule parallelism. By using a self-describing approach for representing the interpreter data structures, the interpreter is a self-controlling system that allows conflict resolution, error handling and a wide spectrum of software metrics to be explicitly specified using RPL meta-rules.},
journal = {SIGMOD Rec.},
month = jun,
pages = {396–403},
numpages = {8}
}

@inproceedings{10.1145/50202.50251,
author = {Sellis, T. and Lin, C. C. and Raschid, L.},
title = {Implementing Large Production Systems in a DBMS Environment: Concepts and Algorithms},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50251},
doi = {10.1145/50202.50251},
abstract = {It has been widely recognized that many future database applications, including engineering processes, manufacturing and communications, will require some kind of rule based reasoning. In this paper we study methods for storing and manipulating large rule bases using relational database management systems. First, we provide a matching algorithm which can be used to efficiently identify applicable rules. The second contribution of this paper, is our proposal for concurrent execution strategies which surpass, in terms of performance, the sequential OPS5 execution algorithm. The proposed method is fully parallelizable, which makes its use even more attractive, as it can be used in parallel computing environments.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {404–423},
numpages = {20},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50251,
author = {Sellis, T. and Lin, C. C. and Raschid, L.},
title = {Implementing Large Production Systems in a DBMS Environment: Concepts and Algorithms},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50251},
doi = {10.1145/971701.50251},
abstract = {It has been widely recognized that many future database applications, including engineering processes, manufacturing and communications, will require some kind of rule based reasoning. In this paper we study methods for storing and manipulating large rule bases using relational database management systems. First, we provide a matching algorithm which can be used to efficiently identify applicable rules. The second contribution of this paper, is our proposal for concurrent execution strategies which surpass, in terms of performance, the sequential OPS5 execution algorithm. The proposed method is fully parallelizable, which makes its use even more attractive, as it can be used in parallel computing environments.},
journal = {SIGMOD Rec.},
month = jun,
pages = {404–423},
numpages = {20}
}

@inproceedings{10.1145/50202.50252,
author = {Carey, Michael J. and DeWitt, David J. and Vandenberg, Scott L.},
title = {A Data Model and Query Language for EXODUS},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50252},
doi = {10.1145/50202.50252},
abstract = {In this paper, we present the design of the EXTRA data model and the EXCESS query language for the EXODUS extensible database system. The EXTRA data model includes support for complex objects with shared subobjects, a novel mix of object- and value-oriented semantics for data, support for persistent objects of any type in the EXTRA type lattice, and user-defined abstract data types (ADTs). The EXCESS query language provides facilities for querying and updating complex object structures, and it can be extended through the addition of ADT functions and operators, procedures and functions for manipulating EXTRA schema types, and generic set functions EXTRA and EXCESS are intended to serve as a test vehicle for tools developed under the EXODUS extensible database system project.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {413–423},
numpages = {11},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50252,
author = {Carey, Michael J. and DeWitt, David J. and Vandenberg, Scott L.},
title = {A Data Model and Query Language for EXODUS},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50252},
doi = {10.1145/971701.50252},
abstract = {In this paper, we present the design of the EXTRA data model and the EXCESS query language for the EXODUS extensible database system. The EXTRA data model includes support for complex objects with shared subobjects, a novel mix of object- and value-oriented semantics for data, support for persistent objects of any type in the EXTRA type lattice, and user-defined abstract data types (ADTs). The EXCESS query language provides facilities for querying and updating complex object structures, and it can be extended through the addition of ADT functions and operators, procedures and functions for manipulating EXTRA schema types, and generic set functions EXTRA and EXCESS are intended to serve as a test vehicle for tools developed under the EXODUS extensible database system project.},
journal = {SIGMOD Rec.},
month = jun,
pages = {413–423},
numpages = {11}
}

@inproceedings{10.1145/50202.50253,
author = {Lecluse, C. and Richard, P. and Velez, F.},
title = {O2, an Object-Oriented Data Model},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50253},
doi = {10.1145/50202.50253},
abstract = {The Altair group is currently designing an object-oriented data base system called O2. This paper presents a formal description of the object-oriented data model of this system. It proposes a type system defined in the framework of a set-and-tuple data model. It models the well known inheritance mechanism and enforces strong typing.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {424–433},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50253,
author = {Lecluse, C. and Richard, P. and Velez, F.},
title = {O2, an Object-Oriented Data Model},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50253},
doi = {10.1145/971701.50253},
abstract = {The Altair group is currently designing an object-oriented data base system called O2. This paper presents a formal description of the object-oriented data model of this system. It proposes a type system defined in the framework of a set-and-tuple data model. It models the well known inheritance mechanism and enforces strong typing.},
journal = {SIGMOD Rec.},
month = jun,
pages = {424–433},
numpages = {10}
}

@inproceedings{10.1145/50202.50254,
author = {Borgida, A.},
title = {Modeling Class Hierarchies with Contradictions},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50254},
doi = {10.1145/50202.50254},
abstract = {One characteristic feature of object-oriented systems and knowledge bases (semantic data models, conceptual modeling languages, Al frames) is that they offer as a basic paradigm the notion of objects grouped into classes, which are themselves organized in subclass hierarchies. Through ideas such as inheritance and bounded polymorphism, this feature supports the technique of “abstraction by generalization”, which has been argued to be of importance in designing Information Systems [11, 2].We provide in this paper examples demonstrating that in some applications over-generalization is likely to occur an occasional natural subclass may contradict in some way one if its superclass definitions, and thus turn out not to be a strict subtype of this superclass. A similar problem arises when an object is allowed to be a member of several classes which make incompatible predictions about its type. We argue that none of the previous approaches suggested to deal with such situations is entirely satisfactory.A language feature is therefore presented to permit class definitions which contradict aspects of other classes, such as superclasses, in an object-based language. In essence, the approach requires contradictions among class definitions to be explicitly acknowledged. We define a semantics of the resulting language, which restores the condition that subclasses are both subsets and subtypes, and deals correctly with the case when an object can belong to several classes. This is done by separating the notions of “class” and “type”, and it allows query compilers to detect type errors as well as eliminate some run-time checks in queries, even in the presence of “contradictory” class definitions.},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {434–443},
numpages = {10},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{10.1145/971701.50254,
author = {Borgida, A.},
title = {Modeling Class Hierarchies with Contradictions},
year = {1988},
issue_date = {June 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/971701.50254},
doi = {10.1145/971701.50254},
abstract = {One characteristic feature of object-oriented systems and knowledge bases (semantic data models, conceptual modeling languages, Al frames) is that they offer as a basic paradigm the notion of objects grouped into classes, which are themselves organized in subclass hierarchies. Through ideas such as inheritance and bounded polymorphism, this feature supports the technique of “abstraction by generalization”, which has been argued to be of importance in designing Information Systems [11, 2].We provide in this paper examples demonstrating that in some applications over-generalization is likely to occur an occasional natural subclass may contradict in some way one if its superclass definitions, and thus turn out not to be a strict subtype of this superclass. A similar problem arises when an object is allowed to be a member of several classes which make incompatible predictions about its type. We argue that none of the previous approaches suggested to deal with such situations is entirely satisfactory.A language feature is therefore presented to permit class definitions which contradict aspects of other classes, such as superclasses, in an object-based language. In essence, the approach requires contradictions among class definitions to be explicitly acknowledged. We define a semantics of the resulting language, which restores the condition that subclasses are both subsets and subtypes, and deals correctly with the case when an object can belong to several classes. This is done by separating the notions of “class” and “type”, and it allows query compilers to detect type errors as well as eliminate some run-time checks in queries, even in the presence of “contradictory” class definitions.},
journal = {SIGMOD Rec.},
month = jun,
pages = {434–443},
numpages = {10}
}

