@inproceedings{10.1145/3318464.3393817,
author = {Stoica, Ion},
title = {Systems and ML: When the Sum is Greater than Its Parts},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3393817},
doi = {10.1145/3318464.3393817},
abstract = {BIOGRAPHY: Ion Stoica is a Professor in the EECS Department at the University of California at Berkeley, and the Director of RISELab (https://rise.cs.berkeley.edu/). He is currently doing research on cloud computing and AI systems. Past work includes Apache Spark, Apache Mesos, Tachyon, Chord DHT, and Dynamic Packet State (DPS). He is an ACM Fellow and has received numerous awards, including the Mark Weiser Award (2019), SIGOPS Hall of Fame Award (2015), the SIGCOMM Test of Time Award (2011), and the ACM doctoral dissertation award (2001). He also co-founded three companies, Anyscale (2019), Databricks (2013) and Conviva (2006).},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1},
numpages = {1},
keywords = {AI, systems, ML, cloud computing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389719,
author = {Wei, Dong and Basu Roy, Senjuti and Amer-Yahia, Sihem},
title = {Recommending Deployment Strategies for Collaborative Tasks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389719},
doi = {10.1145/3318464.3389719},
abstract = {Our work contributes to aiding requesters in deploying collaborative tasks in crowdsourcing. We initiate the study of recommending deployment strategies for collaborative tasks to requesters that are consistent with deployment parameters they desire: a lower-bound on the quality of the crowd contribution, an upper-bound on the latency of task completion, and an upper-bound on the cost incurred by paying workers. A deployment strategy is a choice of value for three dimensions: Structure (whether to solicit the workforce sequentially or simultaneously), Organization (to organize it collaboratively or independently), and Style (to rely solely on the crowd or to combine it with machine algorithms). We propose StratRec, an optimization-driven middle layer that recommends deployment strategies and alternative deployment parameters to requesters by accounting for worker availability. Our solutions are grounded in discrete optimization and computational geometry techniques that produce results with theoretical guarantees. We present extensive experiments on Amazon Mechanical Turk, and conduct synthetic experiments to validate the qualitative and scalability aspects of StratRec.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {3–17},
numpages = {15},
keywords = {approximation algorithm, computational geometry, deployment strategy recommendation, crowdsourcing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389772,
author = {Chai, Chengliang and Cao, Lei and Li, Guoliang and Li, Jian and Luo, Yuyu and Madden, Samuel},
title = {Human-in-the-Loop Outlier Detection},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389772},
doi = {10.1145/3318464.3389772},
abstract = {Outlier detection is critical to a large number of applications from finance fraud detection to health care. Although numerous approaches have been proposed to automatically detect outliers, such outliers detected based on statistical rarity do not necessarily correspond to the true outliers to the interest of applications. In this work, we propose a human-in-the-loop outlier detection approach HOD that effectively leverages human intelligence to discover the true outliers. There are two main challenges in HOD. The first is to design human-friendly questions such that humans can easily understand the questions even if humans know nothing about the outlier detection techniques. The second is to minimize the number of questions. To address the first challenge, we design a clustering-based method to effectively discover a small number of objects that are unlikely to be outliers (aka, inliers) and yet effectively represent the typical characteristics of the given dataset. HOD then leverages this set of inliers (called context inliers) to help humans understand the context in which the outliers occur. This ensures humans are able to easily identify the true outliers from the outlier candidates produced by the machine-based outlier detection techniques. To address the second challenge, we propose a bipartite graph-based question selection strategy that is theoretically proven to be able to minimize the number of questions needed to cover all outlier candidates. Our experimental results on real data sets show that HOD significantly outperforms the state-of-the-art methods on both human efforts and the quality of the discovered outliers.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {19–33},
numpages = {15},
keywords = {outlier detection, question selection, human-in-the-loop},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380561,
author = {Chan, Tsz Nam and Cheng, Reynold and Yiu, Man Lung},
title = {QUAD: Quadratic-Bound-Based Kernel Density Visualization},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380561},
doi = {10.1145/3318464.3380561},
abstract = {Kernel density visualization, or KDV, is used to view and understand data points in various domains, including traffic or crime hotspot detection, ecological modeling, chemical geology, and physical modeling. Existing solutions, which are based on computing kernel density (KDE) functions, are computationally expensive. Our goal is to improve the performance of KDV, in order to support large datasets (e.g., one million points) and high screen resolutions (e.g., 1280 x 960 pixels). We examine two widely-used variants of KDV, namely approximate kernel density visualization (EKDV) and thresholded kernel density visualization (TKDV). For these two operations, we develop fast solution, called QUAD, by deriving quadratic bounds of KDE functions for different types of kernel functions, including Gaussian, triangular etc. We further adopt a progressive visualization framework for KDV, in order to stream partial visualization results to users continuously. Extensive experiment results show that our new KDV techniques can provide at least one-order-of-magnitude speedup over existing methods, without degrading visualization quality. We further show that QUAD can produce the reasonable visualization results in real-time (0.5 sec) by combining the progressive visualization framework in single machine setting without using GPU and parallel computation.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {35–50},
numpages = {16},
keywords = {quadratic bounds, KDV, kernel density visualization, QUAD},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389722,
author = {Siddiqui, Tarique and Luh, Paul and Wang, Zesheng and Karahalios, Karrie and Parameswaran, Aditya},
title = {ShapeSearch: A Flexible and Efficient System for Shape-Based Exploration of Trendlines},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389722},
doi = {10.1145/3318464.3389722},
abstract = {Identifying trendline visualizations with desired patterns is a common task during data exploration. Existing visual analytics tools offer limited flexibility, expressiveness, and scalability for such tasks, especially when the pattern of interest is under-specified and approximate. We propose ShapeSearch, an efficient and flexible pattern-searching tool, that enables the search for desired patterns via multiple mechanisms: sketch, natural-language, and visual regular expressions. We develop a novel shape querying algebra, with a minimal set of primitives and operators that can express a wide variety of shape search queries, and design a natural- language and regex-based parser to translate user queries to the algebraic representation. To execute these queries within interactive response times, ShapeSearch uses a fast shape algebra execution engine with query-aware optimizations, and perceptually-aware scoring methodologies. We present a thorough evaluation of the system, including a user study, a case study involving genomics data analysis, as well as performance experiments, comparing against state-of-the-art trendline shape matching approaches-that together demonstrate the usability and scalability of ShapeSearch.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {51–65},
numpages = {15},
keywords = {pattern querying, regular expression, time series, query processing, natural language, shape algebra},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389730,
author = {Dong, Liming and Bai, Qiushi and Kim, Taewoo and Chen, Taiji and Liu, Weidong and Li, Chen},
title = {Marviq: Quality-Aware Geospatial Visualization of Range-Selection Queries Using Materialization},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389730},
doi = {10.1145/3318464.3389730},
abstract = {We study the problem of efficient spatial visualization on a large data set stored in a database using SQL queries with ad-hoc range conditions on numerical attributes, for example, a spatial scatterplot of taxi pickup events in New York between 1/1/2015 and 3/10/2015. We present a novel middleware-based technique called Marviq. It divides the selection-attribute domain into intervals, and precomputes and stores a visualization for each interval. These results are called MVS and stored as tables in the database. We can compute an exact visualization for a request by accessing MVS and retrieving additional records from the base table. To further reduce the latter time, we present algorithms for using MVS to compute an approximate visualization that satisfies a user-specified similarity threshold. We show a family of functions with certain properties that can use this technique. We present an improvement by dividing the MVS intervals into smaller intervals and materializing low-resolution visualization for these intervals. We report the results of an extensive evaluation of Marviq, including a user study, and show its high performance in both space and time.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {67–82},
numpages = {16},
keywords = {Marviq, spatial data, quality guarantee, visualization},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389710,
author = {Wu, Chenggang and Sreekanti, Vikram and Hellerstein, Joseph M.},
title = {Transactional Causal Consistency for Serverless Computing},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389710},
doi = {10.1145/3318464.3389710},
abstract = {We consider the setting of serverless Function-as-a-Service (FaaS) platforms, where storage services are disaggregated from the machines that support function execution. FaaS applications consist of compositions of functions, each of which may run on a separate machine and access remote storage. The challenge we address is improving I/O latency in this setting while also providing application-wide consistency. Previous work has explored providing causal consistency for individual I/Os by carefully managing the versions stored in a client-side data cache. In our setting, a single application may execute multiple functions across different nodes, and therefore issue interrelated I/Os to multiple distinct caches. This raises the challenge of Multisite Transactional Causal Consistency (MTCC): the ability to provide causal consistency for all I/Os within a given transaction even if it runs across multiple physical sites. We present protocols for MTCC implemented in a system called HYDROCACHE. Our evaluation demonstrates orders-of-magnitude performance improvements due to caching, while also protecting against consistency anomalies that otherwise arise frequently.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {83–97},
numpages = {15},
keywords = {serverless computing, causal consistency, low-latency},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380584,
author = {Siddiqui, Tarique and Jindal, Alekh and Qiao, Shi and Patel, Hiren and Le, Wangchao},
title = {Cost Models for Big Data Query Processing: Learning, Retrofitting, and Our Findings},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380584},
doi = {10.1145/3318464.3380584},
abstract = {Query processing over big data is ubiquitous in modern clouds, where the system takes care of picking both the physical query execution plans and the resources needed to run those plans, using a cost-based query optimizer. A good cost model, therefore, is akin to better resource efficiency and lower operational costs. Unfortunately, the production workloads at Microsoft show that costs are very complex to model for big data systems. In this work, we investigate two key questions: (i) can we learn accurate cost models for big data systems, and (ii) can we integrate the learned models within the query optimizer. To answer these, we make three core contributions. First, we exploit workload patterns to learn a large number of individual cost models and combine them to achieve high accuracy and coverage over a long period. Second, we propose extensions to Cascades framework to pick optimal resources, i.e, number of containers, during query planning. And third, we integrate the learned cost models within the Cascade-style query optimizer of SCOPE at Microsoft. We evaluate the resulting system, Cleo, in a production environment using both production and TPC-H workloads. Our results show that the learned cost models are 2 to 3 orders of magnitude more accurate, and 20X more correlated with the actual runtimes, with a large majority (70%) of the plan changes leading to substantial improvements in latency as well as resource usage.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {99–113},
numpages = {15},
keywords = {resource optimization, query optimization, machine learning, cost models},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389758,
author = {M\"{u}ller, Ingo and Marroqu\'{\i}n, Renato and Alonso, Gustavo},
title = {Lambada: Interactive Data Analytics on Cold Data Using Serverless Cloud Infrastructure},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389758},
doi = {10.1145/3318464.3389758},
abstract = {Serverless computing has recently attracted a lot of attention from research and industry due to its promise of ultimate elasticity and operational simplicity. However, there is no consensus yet on whether or not the approach is suitable for data processing. In this paper, we present Lambada, a serverless distributed data processing framework designed to explore how to perform data analytics on serverless computing. In our analysis, supported with extensive experiments, we show in which scenarios serverless makes sense from an economic and performance perspective. We address several important technical questions that need to be solved to support data analytics and present examples from several domains where serverless offers a cost and performance advantage over existing solutions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {115–130},
numpages = {16},
keywords = {serverless functions, cloud computing, interactive analytics, serverless computing, data lake, elasticity},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380609,
author = {Perron, Matthew and Castro Fernandez, Raul and DeWitt, David and Madden, Samuel},
title = {Starling: A Scalable Query Engine on Cloud Functions},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380609},
doi = {10.1145/3318464.3380609},
abstract = {Much like on-premises systems, the natural choice for running database analytics workloads in the cloud is to provision a cluster of nodes to run a database instance. However, analytics workloads are often bursty or low volume, leaving clusters idle much of the time, meaning customers pay for compute resources even when underutilized. The ability of cloud function services, such as AWS Lambda or Azure Functions, to run small, fine granularity tasks make them appear to be a natural choice for query processing in such settings. But implementing an analytics system on cloud functions comes with its own set of challenges. These include managing hundreds of tiny stateless resource-constrained workers, handling stragglers, and shuffling data through opaque cloud services. In this paper we present Starling, a query execution engine built on cloud function services that employs a number of techniques to mitigate these challenges, providing interactive query latency at a lower total cost than provisioned systems with low-to-moderate utilization. In particular, on a 1TB TPC-H dataset in cloud storage, Starling is less expensive than the best provisioned systems for workloads when queries arrive 1 minute apart or more. Starling also has lower latency than competing systems reading from cloud object stores and can scale to larger datasets.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {131–141},
numpages = {11},
keywords = {FAAS, cloud, OLAP, serverless},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389704,
author = {Hilprecht, Benjamin and Binnig, Carsten and R\"{o}hm, Uwe},
title = {Learning a Partitioning Advisor for Cloud Databases},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389704},
doi = {10.1145/3318464.3389704},
abstract = {Cloud vendors provide ready-to-use distributed DBMS solutions as a service. While the provisioning of a DBMS is usually fully automated, customers typically still have to make important design decisions which were traditionally made by the database administrator such as finding an optimal partitioning scheme for a given database schema and workload. In this paper, we introduce a new learned partitioning advisor based on Deep Reinforcement Learning (DRL) for OLAP-style workloads. The main idea is that a DRL agent learns the cost tradeoffs of different partitioning schemes and can thus automate the partitioning decision. In the evaluation, we show that our advisor is able to find non-trivial partitionings for a wide range of workloads and outperforms more classical approaches for automated partitioning design.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {143–157},
numpages = {15},
keywords = {machine learning, database tuning, database management systems},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380575,
author = {Jasny, Matthias and Ziegler, Tobias and Kraska, Tim and Roehm, Uwe and Binnig, Carsten},
title = {DB4ML - An In-Memory Database Kernel with Machine Learning Support},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380575},
doi = {10.1145/3318464.3380575},
abstract = {In this paper, we revisit the question of how ML algorithms can be best integrated into existing DBMSs to not only avoid expensive data copies to external ML tools but also to comply with regulatory reasons. The key observation is that database transactions already provide an execution model that allows DBMSs to efficiently mimic the execution model of modern parallel ML algorithms. As a main contribution, this paper presents DB4ML, an in-memory database kernel that allows applications to implement user-defined ML algorithms and efficiently run them inside a DBMS. Thereby, the ML algorithms are implemented using a programming model based on the idea of so called iterative transactions. Our experimental evaluation shows that DB4ML can support user-defined ML algorithms inside a DBMS with the efficiency of modern specialized ML engines. In contrast to DB4ML, these engines not only need to transfer data out of the DBMS but also hardcode the ML algorithms and thus are not extensible.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {159–173},
numpages = {15},
keywords = {parallel and distributed dbmss, machine learning, execution engine},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389768,
author = {Ma, Lin and Ding, Bailu and Das, Sudipto and Swaminathan, Adith},
title = {Active Learning for ML Enhanced Database Systems},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389768},
doi = {10.1145/3318464.3389768},
abstract = {Recent research has shown promising results by using machine learning (ML) techniques to improve the performance of database systems, e.g., in query optimization or index recommendation. However, in many production deployments, the ML models' performance degrades significantly when the test data diverges from the data used to train these models. In this paper, we address this performance degradation by using B-instances to collect additional data during deployment. We propose an active data collection platform, ADCP, that employs active learning (AL) to gather relevant data cost-effectively. We develop a novel AL technique, Holistic Active Learner (HAL), that robustly combines multiple noisy signals for data gathering in the context of database applications. HAL applies to various ML tasks, budget sizes, cost types, and budgeting interfaces for database applications. We evaluate ADCP on both industry-standard benchmarks and real customer workloads. Our evaluation shows that, compared with other baselines, our technique improves ML models' prediction performance by up to 2x with the same cost budget. In particular, on production workloads, our technique reduces the prediction error of ML models by 75% using about 100 additionally collected queries.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {175–191},
numpages = {17},
keywords = {distribution mismatch, active data collection platform, machine learning for databases, active learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389770,
author = {Yang, Zongheng and Chandramouli, Badrish and Wang, Chi and Gehrke, Johannes and Li, Yinan and Minhas, Umar Farooq and Larson, Per-\r{A}ke and Kossmann, Donald and Acharya, Rajeev},
title = {Qd-Tree: Learning Data Layouts for Big Data Analytics},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389770},
doi = {10.1145/3318464.3389770},
abstract = {Corporations today collect data at an unprecedented and accelerating scale, making the need to run queries on large datasets increasingly important. Technologies such as columnar block-based data organization and compression have become standard practice in most commercial database systems. However, the problem of best assigning records to data blocks on storage is still open. For example, today's systems usually partition data by arrival time into row groups, or range/hash partition the data based on selected fields. For a given workload, however, such techniques are unable to optimize for the important metric of the number of blocks accessed by a query. This metric directly relates to the I/O cost, and therefore performance, of most analytical queries. Further, they are unable to exploit additional available storage to drive this metric down further. In this paper, we propose a new framework called a query-data routing tree, or qd-tree, to address this problem, and propose two algorithms for their construction based on greedy and deep reinforcement learning techniques. Experiments over benchmark and real workloads show that a qd-tree can provide physical speedups of more than an order of magnitude compared to current blocking schemes, and can reach within 2X of the lower bound for data skipping based on selectivity, while providing complete semantic descriptions of created blocks.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {193–208},
numpages = {16},
keywords = {deep learning, data layout, query processing, deep reinforcement learning, OLAP, storage, indexing, data partitioning, data analytics, big data},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380602,
author = {Zolaktaf, Zainab and Milani, Mostafa and Pottinger, Rachel},
title = {Facilitating SQL Query Composition and Analysis},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380602},
doi = {10.1145/3318464.3380602},
abstract = {Formulating efficient SQL queries requires several cycles of tuning and execution. We examine methods that can accelerate and improve this interaction by providing insights about SQL queries prior to execution. We achieve this by predicting properties such as the query answer size, its run-time, and error class. Unlike existing approaches, our approach does not rely on any statistics from the database instance or query execution plans. Our approach is based on using data-driven machine learning techniques that rely on large query workloads to model SQL queries and their properties. Empirical results show that the neural network models are more accurate in predicting several query properties.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {209–224},
numpages = {16},
keywords = {machine learning, query property prediction, workload},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389728,
author = {Sikdar, Sourav and Jermaine, Chris},
title = {MONSOON: Multi-Step Optimization and Execution of Queries with Partially Obscured Predicates},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389728},
doi = {10.1145/3318464.3389728},
abstract = {User-defined functions (UDFs) in modern SQL database systems and Big Data processing systems such as Spark---that offer API bindings in high-level languages such as Python or Scala---make automatic optimization challenging. The foundation of modern database query optimization is the collection of statistics describing the data to be processed, but when a database or Big Data computation is partially obscured by UDFs, good statistics are often unavailable. In this paper, we describe a query optimizer called the Monsoon optimizer. In the presence of UDFs, the Monsoon optimizer may choose to collect statistics on the UDFs, and then run the computation. Or, it may optimize and execute part of the plan, collecting statistics on the result of the partial plan, followed by a re optimization step, with the process repeated as needed. Monsoon decides how to interleave execution and statistics collection in a principled fashion by formalizing the problem as a Markov decision process.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {225–240},
numpages = {16},
keywords = {query processing and optimization, machine learning for data management},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389759,
author = {Salimi, Babak and Parikh, Harsh and Kayali, Moe and Getoor, Lise and Roy, Sudeepa and Suciu, Dan},
title = {Causal Relational Learning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389759},
doi = {10.1145/3318464.3389759},
abstract = {Causal inference is at the heart of empirical research in natural and social sciences and is critical for scientific discovery and informed decision making. The gold standard in causal inference is performing randomized controlled trials ; unfortunately these are not always feasible due to ethical, legal, or cost constraints. As an alternative, methodologies for causal inference from observational data have been developed in statistical studies and social sciences. However, existing methods critically rely on restrictive assumptions such as the study population consisting of homogeneous elements that can be represented in a single flat table, where each row is referred to as a unit. In contrast, in many real-world settings, the study domain naturally consists of heterogeneous elements with complex relational structure, where the data is naturally represented in multiple related tables. In this paper, we present a formal framework for causal inference from such relational data. We propose a declarative language called CARL for capturing causal background knowledge and assumptions, and specifying causal queries using simple Datalog-like rules. CARL provides a foundation for inferring causality and reasoning about the effect of complex interventions in relational domains. We present an extensive experimental evaluation on real relational data to illustrate the applicability of CARL in social sciences and healthcare.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {241–256},
numpages = {16},
keywords = {declarative language, algorithms, experiments, graphical causal models, causal inference, relational data},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380606,
author = {Orr, Laurel and Balazinska, Magdalena and Suciu, Dan},
title = {Sample Debiasing in the Themis Open World Database System},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380606},
doi = {10.1145/3318464.3380606},
abstract = {Open world database management systems assume tuples not in the database still exist and are becoming an increasingly important area of research. We present Themis, the first open world database that automatically rebalances arbitrarily biased samples to approximately answer queries as if they were issued over the entire population. We leverage apriori population aggregate information to develop and combine two different approaches for automatic debiasing: sample reweighting and Bayesian network probabilistic modeling. We build a prototype of Themis and demonstrate that Themis achieves higher query accuracy than the default AQP approach, an alternative sample reweighting technique, and a variety of Bayesian network models while maintaining interactive query response times. We also show that Themis is robust to differences in the support between the sample and population, a key use case when using social media samples.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {257–268},
numpages = {12},
keywords = {databases, open world database, data debiasing, data science, approximate query processing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389765,
author = {Brucato, Matteo and Yadav, Nishant and Abouzied, Azza and Haas, Peter J. and Meliou, Alexandra},
title = {Stochastic Package Queries in Probabilistic Databases},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389765},
doi = {10.1145/3318464.3389765},
abstract = {We provide methods for in-database support of decision making under uncertainty. Many important decision problems correspond to selecting a "package" (bag of tuples in a relational database) that jointly satisfy a set of constraints while minimizing some overall "cost" function; in most real-world problems, the data is uncertain. We provide methods for specifying---via a SQL extension---and processing stochastic package queries (SPQS), in order to solve optimization problems over uncertain data, right where the data resides. Prior work in stochastic programming uses Monte Carlo methods where the original stochastic optimization problem is approximated by a large deterministic optimization problem that incorporates many "scenarios", i.e., sample realizations of the uncertain data values. For large database tables, however, a huge number of scenarios is required, leading to poor performance and, often, failure of the solver software. We therefore provide a novel \ss{}s algorithm that, instead of trying to solve a large deterministic problem, seamlessly approximates it via a sequence of smaller problems defined over carefully crafted "summaries" of the scenarios that accelerate convergence to a feasible and near-optimal solution. Experimental results on our prototype system show that \ss{}s can be orders of magnitude faster than prior methods at finding feasible and high-quality packages.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {269–283},
numpages = {15},
keywords = {probabilistic databases, portfolio optimization, simulation, prescriptive analytics, optimization, package queries, data integration, stochastic programming, decision making},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389785,
author = {Liang, Xi and Shang, Zechao and Krishnan, Sanjay and Elmore, Aaron J. and Franklin, Michael J.},
title = {Fast and Reliable Missing Data Contingency Analysis with Predicate-Constraints},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389785},
doi = {10.1145/3318464.3389785},
abstract = {Today, data analysts largely rely on intuition to determine whether missing or withheld rows of a dataset significantly affect their analyses. We propose a framework that can produce automatic contingency analysis, i.e., the range of values an aggregate SQL query could take, under formal constraints describing the variation and frequency of missing data tuples. We describe how to process SUM, COUNT, AVG, MIN, and MAX queries in these conditions resulting in hard error bounds with testable constraints. We propose an optimization algorithm based on an integer program that reconciles a set of such constraints, even if they are overlapping, conflicting, or unsatisfiable, into such bounds. Our experiments on real-world datasets against several statistical imputation and inference baselines show that statistical techniques can have a deceptively high error rate that is often unpredictable. In contrast, our framework offers hard bounds that are guaranteed to hold if the constraints are not violated. In spite of these hard bounds, we show competitive accuracy to statistical baselines.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {285–295},
numpages = {11},
keywords = {data sketching, contingency analysis, missing data, aggregate estimation, approximate query processing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380573,
author = {Kenig, Batya and Mundra, Pranay and Prasaad, Guna and Salimi, Babak and Suciu, Dan},
title = {Mining Approximate Acyclic Schemes from Relations},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380573},
doi = {10.1145/3318464.3380573},
abstract = {Acyclic schemes have numerous applications in databases and in machine learning, such as improved design, more efficient storage, and increased performance for queries and machine learning algorithms. Multivalued dependencies (MVDs) are the building blocks of acyclic schemes. The discovery from data of both MVDs and acyclic schemes is more challenging than other forms of data dependencies, such as Functional Dependencies, because these dependencies do not hold on subsets of data, and because they are very sensitive to noise in the data; for example a single wrong or missing tuple may invalidate the schema. In this paper we present Maimon, a system for discovering approximate acyclic schemes and MVDs from data. We give a principled definition of approximation, by using notions from information theory, then describe the two components of Maimon: mining for approximate MVDs, then reconstructing acyclic schemes from approximate MVDs. We conduct an experimental evaluation of Maimon on 20 real-world datasets, and show that it can scale up to 1M rows, and up to 30 columns.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {297–312},
numpages = {16},
keywords = {database schema design, integrity constraints, data dependencies},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386132,
author = {Luo, Xusheng and Liu, Luxin and Yang, Yonghua and Bo, Le and Cao, Yuanpeng and Wu, Jinghang and Li, Qiang and Yang, Keping and Zhu, Kenny Q.},
title = {AliCoCo: Alibaba E-Commerce Cognitive Concept Net},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386132},
doi = {10.1145/3318464.3386132},
abstract = {One of the ultimate goals of e-commerce platforms is to satisfy various shopping needs for their customers. Much efforts are devoted to creating taxonomies or ontologies in e-commerce towards this goal. However, user needs in e-commerce are still not well defined, and none of the existing ontologies has the enough depth and breadth for universal user needs understanding. The semantic gap in-between prevents shopping experience from being more intelligent. In this paper, we propose to construct a large-scale E-commerce Cognitive Concept Net named "AliCoCo", which is practiced in Alibaba, the largest Chinese e-commerce platform in the world. We formally define user needs in e-commerce, then conceptualize them as nodes in the net. We present details on how AliCoCo is constructed semi-automatically and its successful, ongoing and potential applications in e-commerce.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {313–327},
numpages = {15},
keywords = {concept net, user needs, e-commerce},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386135,
author = {Buragohain, Chiranjeeb and Risvik, Knut Magne and Brett, Paul and Castro, Miguel and Cho, Wonhee and Cowhig, Joshua and Gloy, Nikolas and Kalyanaraman, Karthik and Khanna, Richendra and Pao, John and Renzelmann, Matthew and Shamis, Alex and Tan, Timothy and Zheng, Shuheng},
title = {A1: A Distributed In-Memory Graph Database},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386135},
doi = {10.1145/3318464.3386135},
abstract = {A1 is an in-memory distributed database used by the Bing search engine to support complex queries over structured data. The key enablers for A1 are availability of cheap DRAM and high speed RDMA (Remote Direct Memory Access) networking in commodity hardware. A1 uses FaRM [11,12] as its underlying storage layer and builds the graph abstraction and query engine on top. The combination of in-memory storage and RDMA access requires rethinking how data is allocated, organized and queried in a large distributed system. A single A1 cluster can store tens of billions of vertices and edges and support a throughput of 350+ million of vertex reads per second with end to end query latency in single digit milliseconds. In this paper we describe the A1 data model, RDMA optimized data structures and query execution.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {329–344},
numpages = {16},
keywords = {graph database, distributed database, in-memory database, graph query processing, RDMA},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386138,
author = {Tian, Yuanyuan and Xu, En Liang and Zhao, Wei and Pirahesh, Mir Hamid and Tong, Sui Jun and Sun, Wen and Kolanko, Thomas and Apu, Md. Shahidul Haque and Peng, Huijuan},
title = {IBM Db2 Graph: Supporting Synergistic and Retrofittable Graph Queries Inside IBM Db2},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386138},
doi = {10.1145/3318464.3386138},
abstract = {To meet the challenge of analyzing rapidly growing graph and network data created by modern applications, a large number of graph databases have emerged, such as Neo4j and JanusGraph. They mainly target low-latency graph queries, such as finding the neighbors of a vertex with certain properties, and retrieving the shortest path between two vertices. Although many of the graph databases handle the graph-only queries very well, they fall short for real life applications involving graph analysis. This is because graph queries are not all that one does in an analytics workload of a real life application. They are often only a part of an integrated heterogeneous analytics pipeline, which may include SQL, machine learning, graph, and other analytics. This means graph queries need to be synergistic with other analytics. Unfortunately, most existing graph databases are standalone and cannot easily integrate with other analytics systems. In addition, many graph data (data about relationships between objects or people) are already prevalent in existing non-graph databases, although they are not explicitly stored as graphs. None of existing graph databases can retrofit graph queries onto these existing data without transferring or transforming data. In this paper, we propose an in-DBMS graph query approach, IBM Db2 Graph, to support synergistic and retrofittable graph queries inside the IBM Db2 relational database. It is implemented as a layer inside Db2, and thus can support integrated graph and SQL analytics efficiently. Db2 Graph employs a novel graph overlay approach to expose a graph view of the relational data. This approach flexibly retrofits graph queries to existing graph data stored in relational tables, without expensive data transfer or transformation. In addition, it enables efficient execution of graph queries with the help of Db2 relational engine, through sophisticated compile-time and runtime optimization strategies. Our experimental study, as well as our experience with real customers using Db2 Graph, showed that Db2 Graph can provide very competitive and sometimes even better performance on graph-only queries, compared to existing graph databases. Moreover, it optimizes the overall performance of complex analytics workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {345–359},
numpages = {15},
keywords = {graph overlay, graph database, graph query},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386139,
author = {Quamar, Abdul and Lei, Chuan and Miller, Dorian and Ozcan, Fatma and Kreulen, Jeffrey and Moore, Robert J. and Efthymiou, Vasilis},
title = {An Ontology-Based Conversation System for Knowledge Bases},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386139},
doi = {10.1145/3318464.3386139},
abstract = {Domain-specific knowledge bases (KB), carefully curated from various data sources, provide an invaluable reference for professionals. Conversation systems make these KBs easily accessible to professionals and are gaining popularity due to recent advances in natural language understanding and AI. Despite the increasing use of various conversation systems in open-domain applications, the requirements of a domain-specific conversation system are quite different and challenging. In this paper, we propose an ontology-based conversation system for domain-specific KBs. In particular, we exploit the domain knowledge inherent in the domain ontology to identify user intents, and the corresponding entities to bootstrap the conversation space. We incorporate the feedback from domain experts to further refine these patterns, and use them to generate training samples for the conversation model, lifting the heavy burden from the conversation designers. We have incorporated our innovations into a conversation agent focused on healthcare as a feature of the IBM Micromedex product.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {361–376},
numpages = {16},
keywords = {knowledge bases, conversation systems, ontology-driven},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386144,
author = {Deutsch, Alin and Xu, Yu and Wu, Mingxi and Lee, Victor E.},
title = {Aggregation Support for Modern Graph Analytics in TigerGraph},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386144},
doi = {10.1145/3318464.3386144},
abstract = {We describe how GSQL, TigerGraph's graph query language, supports the specification of aggregation in graph analytics. GSQL makes several unique design decisions with respect to both the expressive power and the evaluation complexity of the specified aggregation. We detail our design showing how our ideas transcend GSQL and are eminently portable to the upcoming graph query language standards as well as the existing pattern-based declarative query languages.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {377–392},
numpages = {16},
keywords = {graph databases, aggregation, graph query languages},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386145,
author = {Liu, Bang and Guo, Weidong and Niu, Di and Luo, Jinwen and Wang, Chaoyue and Wen, Zhen and Xu, Yu},
title = {GIANT: Scalable Creation of a Web-Scale Ontology},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386145},
doi = {10.1145/3318464.3386145},
abstract = {Understanding what online users may pay attention to on the web is key to content recommendation and search services. These services will benefit from a highly structured and web-scale ontology of entities, concepts, events, topics and categories. While existing knowledge bases and taxonomies embody a large volume of entities and categories, we argue that they fail to discover properly grained concepts, events and topics in the language style of online users. Neither is a logically structured ontology maintained among these notions. In this paper, we present GIANT, a mechanism to construct a user-centered, web-scale, structured ontology, containing a large number of natural language phrases conforming to user attentions at various granularities, mined from the vast volume of web documents and search click logs. Various types of edges are also constructed to maintain a hierarchy in the ontology. We present our detailed techniques used in GIANT, and evaluate the proposed models and methods as compared to a variety of baselines, as well as deploy the resulted Attention Ontology in real-world applications, involving over a billion users, to observe its effect on content recommendation. The online performance of the ontology built by GIANT proves that it can significantly improve the click-through rate in news feeds recommendation.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {393–409},
numpages = {17},
keywords = {user interest modeling, ontology creation, event mining, document understanding, concept mining},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3394837,
author = {Balazinska, Magda and Chaudhuri, Surajit and Ailamaki, Anastasia and Freire, Juliana and Krishnamurthy, Sailesh and Stonebraker, Michael},
title = {The Next 5 Years: What Opportunities Should the Database Community Seize to Maximize Its Impact?},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3394837},
doi = {10.1145/3318464.3394837},
abstract = {The database research community has been spectacularly successful in impacting the industry and academia since the invention of the relational model. Examples of innovation in the last decade include columnar storage for data analytic platforms, cloud data services, HTAP systems, and a new generation of data wrangling systems. Despite this success, critical self-assessment by the community and identifying key opportunities for the future is essential if we are to continue the tradition of impactful research. In the Fall of 2018, following a long tradition that dates back to 1988 [1], and five years after the last such meeting [2], a group of approximately thirty database researchers gathered at the University of Washington, Seattle for two days to discuss the opportunities we have as a community for impactful research. A report from that meeting is now available [3]. The discussions in the Seattle meeting focused not just on technical challenges and opportunities but also on topics related to how we organize ourselves as a community. This SIGMOD panel will provide a forum for the broader database community to review and debate the findings from the Seattle Report on Database Research [3] as well as to identify other challenges, and opportunities that need to be taken into account.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {411–414},
numpages = {4},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380578,
author = {Bourhis, Pierre and Deutch, Daniel and Moskovitch, Yuval},
title = {Equivalence-Invariant Algebraic Provenance for Hyperplane Update Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380578},
doi = {10.1145/3318464.3380578},
abstract = {The algebraic approach for provenance tracking, originating in the semiring model of Green et. al, has proven useful as an abstract way of handling metadata. Commutative Semirings were shown to be the "correct" algebraic structure for Union of Conjunctive Queries, in the sense that its use allows provenance to be invariant under certain expected query equivalence axioms.In this paper we present the first (to our knowledge) algebraic provenance model, for a fragment of update queries, that is invariant under set equivalence. The fragment that we focus on is that of hyperplane queries, previously studied in multiple lines of work. Our algebraic provenance structure and corresponding provenance-aware semantics are based on the sound and complete axiomatization of Karabeg and Vianu. We demonstrate that our construction can guide the design of concrete provenance model instances for different applications. We further study the efficient generation and storage of provenance for hyperplane update queries. We show that a naive algorithm can lead to an exponentially large provenance expression, but remedy this by presenting a normal form which we show may be efficiently computed alongside query evaluation. We experimentally study the performance of our solution and demonstrate its scalability and usefulness, and in particular the effectiveness of our normal form representation.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {415–429},
numpages = {15},
keywords = {provenance, transactions},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389694,
author = {Fariha, Anna and Nath, Suman and Meliou, Alexandra},
title = {Causality-Guided Adaptive Interventional Debugging},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389694},
doi = {10.1145/3318464.3389694},
abstract = {Runtime nondeterminism is a fact of life in modern database applications. Previous research has shown that nondeterminism can cause applications to intermittently crash, become unresponsive, or experience data corruption. We propose Adaptive Interventional Debugging (AID) for debugging such intermittent failures. AID combines existing statistical debugging, causal analysis, fault injection, and group testing techniques in a novel way to (1) pinpoint the root cause of an application's intermittent failure and (2) generate an explanation of how the root cause triggers the failure. AID works by first identifying a set of runtime behaviors (called predicates) that are strongly correlated to the failure. It then utilizes temporal properties of the predicates to (over)-approximate their causal relationships. Finally, it uses fault injection to execute a sequence of interventions on the predicates and discover their true causal relationships. This enables AID to identify the true root cause and its causal relationship to the failure. We theoretically analyze how fast AID can converge to the identification. We evaluate AID with six real-world applications that intermittently fail under specific inputs. In each case, AID was able to identify the root cause and explain how the root cause triggered the failure, much faster than group testing and more precisely than statistical debugging. We also evaluate AID with many synthetically generated applications with known root causes and confirm that the benefits also hold for them.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {431–446},
numpages = {16},
keywords = {concurrency bug, group testing, trace analysis, root-causing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380571,
author = {Wu, Yinjun and Tannen, Val and Davidson, Susan B.},
title = {PrIU: A Provenance-Based Approach for Incrementally Updating Regression Models},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380571},
doi = {10.1145/3318464.3380571},
abstract = {The ubiquitous use of machine learning algorithms brings new challenges to traditional database problems such as incremental view update. Much effort is being put in better understanding and debugging machine learning models, as well as in identifying and repairing errors in training datasets. Our focus is on how to assist these activities when they have to retrain the machine learning model after removing problematic training samples in cleaning or selecting different subsets of training data for interpretability. This paper presents an efficient provenance-based approach, PrIU, and its optimized version, PrIU-opt, for incrementally updating model parameters without sacrificing prediction accuracy. We prove the correctness and convergence of the incrementally updated model parameters, and validate it experimentally. Experimental results show that up to two orders of magnitude speed-ups can be achieved by PrIU-opt compared to simply retraining the model from scratch, yet obtaining highly similar models.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {447–462},
numpages = {16},
keywords = {deletion propagation, machine learning, data provenance},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389763,
author = {Louren\c{c}o, Raoni and Freire, Juliana and Shasha, Dennis},
title = {BugDoc: Algorithms to Debug Computational Processes},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389763},
doi = {10.1145/3318464.3389763},
abstract = {Data analysis for scientific experiments and enterprises, large-scale simulations, and machine learning tasks all entail the use of complex computational pipelines to reach quantitative and qualitative conclusions. If some of the activities in a pipeline produce erroneous outputs, the pipeline may fail to execute or produce incorrect results. Inferring the root cause(s) of such failures is challenging, usually requiring time and much human thought, while still being error-prone. We propose a new approach that makes use of iteration and provenance to automatically infer the root causes and derive succinct explanations of failures. Through a detailed experimental evaluation, we assess the cost, precision, and recall of our approach compared to the state of the art. Our experimental data and processing software is available for use, reproducibility, and enhancement.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {463–478},
numpages = {16},
keywords = {workflow debugging, provenance},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389762,
author = {Tao, Yuchao and He, Xi and Machanavajjhala, Ashwin and Roy, Sudeepa},
title = {Computing Local Sensitivities of Counting Queries with Joins},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389762},
doi = {10.1145/3318464.3389762},
abstract = {Local sensitivity of a query Q given a database instance D, i.e. how much the output Q(D) changes when a tuple is added to D or deleted from D, has many applications including query analysis, outlier detection, and differential privacy. However, it is NP-hard to find local sensitivity of a conjunctive query in terms of the size of the query, even for the class of acyclic queries. Although the complexity is polynomial when the query size is fixed, the naive algorithms are not efficient for large databases and queries involving multiple joins. In this paper, we present a novel approach to compute local sensitivity of counting queries involving join operations by tracking and summarizing tuple sensitivities. We give algorithms for the sensitivity problem for full acyclic join queries using join trees, that run in polynomial time in both the size of the database and query for an interesting sub-class of queries, which we call 'doubly acyclic queries' that include path queries, and in polynomial time in combined complexity when the maximum degree in the join tree is bounded. Our algorithms can be extended to certain non-acyclic queries using generalized hypertree decompositions. We evaluate our approach and show applications of our algorithms to obtain better results for differential privacy by orders of magnitude.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {479–494},
numpages = {16},
keywords = {local sensitivity, join query, differential privacy},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389714,
author = {Kim, Jongbin and Cho, Hyunsoo and Kim, Kihwang and Yu, Jaeseon and Kang, Sooyong and Jung, Hyungsoo},
title = {Long-Lived Transactions Made Less Harmful},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389714},
doi = {10.1145/3318464.3389714},
abstract = {Many systems use snapshot isolation, or something similar, as defaults, and multi-version concurrency control (MVCC) remains essential to offering such point-in-time consistency. One major issue in MVCC is the timely removal of unnecessary versions of data items, especially in the presence of long-lived transactions (LLTs). We have observed that the latest versions of MySQL and PostgreSQL are still vulnerable to LLTs. Our analysis of existing proposals suggests that new solutions to this matter must provide rigorous rules for completely identifying unnecessary versions, and elaborate designs for version cleaning lest old versions required for LLTs should suspend garbage collection. In this paper, we formalize such rules into our version pruning theorem and version classification, of which all form theoretical foundations for our new version management system, vDriver, that bases its record versioning on a new principle: Single In-row Remaining Off-row (SIRO) versioning. We implemented a prototype of vDriver and integrated it with MySQL-8.0 and PostgreSQL-12.0. The experimental evaluation demonstrated that the engines with Driver continue to perform the reclamation of dead versions in the face of LLTs while retaining transaction throughput with reduced space consumption.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {495–510},
numpages = {16},
keywords = {record versioning, long-lived transactions, MVCC},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389724,
author = {Zamanian, Erfan and Shun, Julian and Binnig, Carsten and Kraska, Tim},
title = {Chiller: Contention-Centric Transaction Execution and Data Partitioning for Modern Networks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389724},
doi = {10.1145/3318464.3389724},
abstract = {Distributed transactions on high-overhead TCP/IP-based networks were conventionally considered to be prohibitively expensive and thus were avoided at all costs. To that end, the primary goal of almost any existing partitioning scheme is to minimize the number of cross-partition transactions. However, with the new generation of fast RDMA-enabled networks, this assumption is no longer valid. In fact, recent work has shown that distributed databases can scale even when the majority of transactions are cross-partition. In this paper, we first make the case that the new bottleneck which hinders truly scalable transaction processing in modern RDMA-enabled databases is data contention, and that optimizing for data contention leads to different partitioning layouts than optimizing for the number of distributed transactions. We then present Chiller, a new approach to data partitioning and transaction execution, which aims to minimize data contention for both local and distributed transactions. Finally, we evaluate Chiller using various workloads, and show that our partitioning and execution strategy outperforms traditional partitioning techniques which try to avoid distributed transactions, by up to a factor of 2.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {511–526},
numpages = {16},
keywords = {RDMA, data partitioning, distributed transactions},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389764,
author = {Prasaad, Guna and Cheung, Alvin and Suciu, Dan},
title = {Handling Highly Contended OLTP Workloads Using Fast Dynamic Partitioning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389764},
doi = {10.1145/3318464.3389764},
abstract = {Research on transaction processing has made significant progress towards improving performance of main memory multicore OLTP systems under low contention. However, these systems struggle on workloads with lots of conflicts. Partitioned databases (and variants) perform well on high contention workloads that are statically partitionable, but time-varying workloads often make them impractical. Towards addressing this, we propose Strife---a novel transaction processing scheme that clusters transactions together dynamically and executes most of them without any concurrency control. Strife executes transactions in batches, where each batch is partitioned into disjoint clusters without any cross-cluster conflicts and a small set of residuals. The clusters are then executed in parallel with no concurrency control, followed by residuals separately executed with concurrency control. Strife uses a fast dynamic clustering algorithm that exploits a combination of random sampling and concurrent union-find data structure to partition the batch online, before executing it. Strife outperforms lock-based and optimistic protocols by up to 2x on high contention workloads. While Strife incurs about 50% overhead relative to partitioned systems in the statically partitionable case, it performs 2x better when such static partitioning is not possible and adapts to dynamically varying workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {527–542},
numpages = {16},
keywords = {automatic partitioning, main memory, high contention, transaction processing, OLTP, concurrent data structures},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389693,
author = {Ruan, Pingcheng and Loghin, Dumitrel and Ta, Quang-Trung and Zhang, Meihui and Chen, Gang and Ooi, Beng Chin},
title = {A Transactional Perspective on Execute-Order-Validate Blockchains},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389693},
doi = {10.1145/3318464.3389693},
abstract = {Smart contracts have enabled blockchain systems to evolve from simple cryptocurrency platforms to general transactional systems. A new architecture called execute-order-validate has been proposed in Hyperledger Fabric to support parallel transactions. However, this architecture might render many invalid transactions when serializing them. This problem is further exaggerated as the block formation rate is inherently limited due to other factors beside data processing, such as cryptography and consensus. Inspired by optimistic concurrency control in modern databases, we propose a novel method to enhance the execute-order-validate architecture, by reordering transactions to reduce the abort rate. In contrast to existing blockchains that adopt database's preventive approaches which might over-abort serializable transactions, our method is theoretically more fine-grained: unserializable transactions are aborted before reordering and the rest are guaranteed to be serializable. We implement our method in two blockchains respectively, FabricSharp on top of Hyperledger Fabric, and FastFabricSharp on top of FastFabric. We compare the performance of FabricSharp with vanilla Fabric and three related systems, two of which are respectively implemented with one standard and one state-of-the-art concurrency control techniques from databases. The results demonstrate that FabricSharp achieves 25% higher throughput compared to the other systems in nearly all experimental scenarios. Moreover, the FastFabricSharp's improvement on FastFabric is up to 66%.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {543–557},
numpages = {15},
keywords = {database, blockchain, concurrency control, transaction},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389736,
author = {Gupta, Surabhi and Purandare, Sanket and Ramachandra, Karthik},
title = {Aggify: Lifting the Curse of Cursor Loops Using Custom Aggregates},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389736},
doi = {10.1145/3318464.3389736},
abstract = {Loops that iterate over SQL query results are quite common, both in application programs that run outside the DBMS, as well as User Defined Functions (UDFs) and stored procedures that run within the DBMS. It can be argued that set-oriented operations are more efficient and should be preferred over iteration; but from real world use cases, it is clear that loops over query results are inevitable in many situations, and are preferred by many users. Such loops, known as cursor loops, come with huge trade-offs and overheads w.r.t. performance, resource consumption and concurrency. We present Aggify, a technique for optimizing loops over query results that overcomes these overheads. It achieves this by automatically generating custom aggregates that are equivalent in semantics to the loop. Thereby, Aggify completely eliminates the loop by rewriting the query to use this generated aggregate. This technique has several advantages such as: (i) pipelining of entire cursor loop operations instead of materialization, (ii) pushing down loop computation from the application layer into the DBMS, closer to the data, (iii) leveraging existing work on optimization of aggregate functions, resulting in efficient query plans. We describe the technique underlying Aggify, and present our experimental evaluation over benchmarks as well as real workloads that demonstrate the significant benefits of this technique.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {559–573},
numpages = {15},
keywords = {cursor loops, custom aggregates, query optimization},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389784,
author = {Cao, Yang and Fan, Wenfei and Wang, Yanghao and Yi, Ke},
title = {Querying Shared Data with Security Heterogeneity},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389784},
doi = {10.1145/3318464.3389784},
abstract = {There has been increasing need for secure data sharing. In practice a group of data owners often adopt a heterogeneous security scheme under which each pair of parties decide their own protocol to share data with diverse levels of trust. The scheme also keeps track of how the data is used. This paper studies distributed SQL query answering in the heterogeneous security setting. We define query plans by incorporating toll functions determined by data sharing agreements and reflected in the use of various security facilities. We formalize query answering as a bi-criteria optimization problem, to minimize both data sharing toll and parallel query evaluation cost. We show that this problem is PSPACE-hard for SQL and Σ_3^p-hard for SPC, and it is in NEXPTIME. Despite the hardness, we develop a set of approximate algorithms to generate distributed query plans that minimize data sharing toll and reduce parallel evaluation cost. Using real-life and synthetic data, we empirically verify the effectiveness, scalability and efficiency of our algorithms.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {575–585},
numpages = {11},
keywords = {security protocol, distributed databases, query processing, data sharing, privacy-preserving},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380569,
author = {Hackenjos, Timon and Hahn, Florian and Kerschbaum, Florian},
title = {SAGMA: Secure Aggregation Grouped by Multiple Attributes},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380569},
doi = {10.1145/3318464.3380569},
abstract = {Encryption can protect data in outsourced databases -- in the cloud -- while still enabling query processing over the encrypted data. However, the processing leaks information about the data specific to the type of query, e.g., aggregation queries. Aggregation over user-defined groups using SQL's GROUP BY clause is extensively used in data analytics, e.g., to calculate the total number of visitors each month or the average salary in each department. The information leaked, e.g., the access pattern to a group, may reveal the group's frequency enabling simple, yet detrimental leakage-abuse attacks. In this work we present SAGMA -- an encryption scheme for performing secure aggregation grouped by multiple attributes. The querier can choose any combination of one or multiple attributes in the GROUP BY clause among the set of all grouping attributes. The encryption scheme only stores semantically secure ciphertexts at the cloud and query processing hides the access pattern, i.e., the frequency of each group. We implemented our scheme and our evaluation results underpin its practical feasibility.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {587–601},
numpages = {15},
keywords = {secure data aggregation, encrypted databases},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380596,
author = {Roy Chowdhury, Amrita and Wang, Chenghong and He, Xi and Machanavajjhala, Ashwin and Jha, Somesh},
title = {Cryptϵ: Crypto-Assisted Differential Privacy on Untrusted Servers},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380596},
doi = {10.1145/3318464.3380596},
abstract = {Differential privacy (DP) is currently the de-facto standard for achieving privacy in data analysis, which is typically implemented either in the "central" or "local" model. The local model has been more popular for commercial deployments as it does not require a trusted data collector. This increased privacy, however, comes at the cost of utility and algorithmic expressibility as compared to the central model. In this work, we propose, Cryptε, a system and programming framework that (1) achieves the accuracy guarantees and algorithmic expressibility of the central model (2) without any trusted data collector like in the local model. Cryptε achieves the "best of both worlds" by employing two non-colluding untrusted servers that run DP programs on encrypted data from the data owners. In theory, straightforward implementations of DP programs using off-the-shelf secure multi-party computation tools can achieve the above goal. However, in practice, they are beset with many challenges like poor performance and tricky security proofs. To this end, Cryptε allows data analysts to author logical DP programs that are automatically translated to secure protocols that work on encrypted data. These protocols ensure that the untrusted servers learn nothing more than the noisy outputs, thereby guaranteeing DP (for computationally bounded adversaries) for all Cryptε programs. Cryptε supports a rich class of DP programs that can be expressed via a small set of transformation and measurement operators followed by arbitrary post-processing. Further, we propose performance optimizations leveraging the fact that the output is noisy. We demonstrate Cryptε's practical feasibility with extensive empirical evaluations on real world datasets.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {603–619},
numpages = {17},
keywords = {cryptographic service provider, homomorphic encryption, differential privacy, two-server model},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389700,
author = {Li, Zitao and Wang, Tianhao and Lopuha\"{a}-Zwakenberg, Milan and Li, Ninghui and \v{S}koric, Boris},
title = {Estimating Numerical Distributions under Local Differential Privacy},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389700},
doi = {10.1145/3318464.3389700},
abstract = {When collecting information, local differential privacy (LDP) relieves the concern of privacy leakage from users' perspective, as user's private information is randomized before sent to the aggregator. We study the problem of recovering the distribution over a numerical domain while satisfying LDP. While one can discretize a numerical domain and then apply the protocols developed for categorical domains, we show that taking advantage of the numerical nature of the domain results in better trade-off of privacy and utility. We introduce a new reporting mechanism, called the square wave (SW) mechanism, which exploits the numerical nature in reporting. We also develop an Expectation Maximization with Smoothing (EMS) algorithm, which is applied to aggregated histograms from the SW mechanism to estimate the original distributions. Extensive experiments demonstrate that our proposed approach, SW with EMS, consistently outperforms other methods in a variety of utility metrics.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {621–635},
numpages = {15},
keywords = {density estimation, local differential privacy},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380594,
author = {Peng, Yanqing and Du, Min and Li, Feifei and Cheng, Raymond and Song, Dawn},
title = {FalconDB: Blockchain-Based Collaborative Database},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380594},
doi = {10.1145/3318464.3380594},
abstract = {Nowadays an emerging class of applications are based oncollaboration over a shared database among different entities. However, the existing solutions on shared database may require trust on others, have high hardware demand that is unaffordable for individual users, or have relatively low performance. In other words, there is a trilemma among security, compatibility and efficiency. In this paper, we present FalconDB, which enables different parties with limited hardware resources to efficiently and securely collaborate on a database. FalconDB adopts database servers with verification interfaces accessible to clients and stores the digests for query/update authentications on a blockchain. Using blockchain as a consensus platform and a distributed ledger, FalconDB is able to work without any trust on each other. Meanwhile, FalconDB requires only minimal storage cost on each client, and provides anywhere-available, real-time and concurrent access to the database. As a result, FalconDB over-comes the disadvantages of previous solutions, and enables individual users to participate in the collaboration with high efficiency, low storage cost and blockchain-level security guarantees.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {637–652},
numpages = {16},
keywords = {data integrity, collaborative database, blockchain},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389781,
author = {Wang, Hanzhi and Wei, Zhewei and Yuan, Ye and Du, Xiaoyong and Wen, Ji-Rong},
title = {Exact Single-Source SimRank Computation on Large Graphs},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389781},
doi = {10.1145/3318464.3389781},
abstract = {SimRank is a popular measurement for evaluating the node-to-node similarities based on the graph topology. In recent years, single-source and top-k SimRank queries have received increasing attention due to their applications in web mining, social network analysis, and spam detection. However, a fundamental obstacle in studying SimRank has been the lack of ground truths. The only exact algorithm, Power Method, is computationally infeasible on graphs with more than 106 nodes. Consequently, no existing work has evaluated the actual trade-offs between query time and accuracy on large real-world graphs. In this paper, we present ExSim, the first algorithm that computes the exact single-source and top-k SimRank results on large graphs. With high probability, this algorithm produces ground truths with a rigorous theoretical guarantee. We conduct extensive experiments on real-world datasets to demonstrate the efficiency of ExactSim. The results show that ExactSim provides the ground truth for any single-source SimRank query with a precision up to 7 decimal places within a reasonable query time.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {653–663},
numpages = {11},
keywords = {SimRank, ground truths, exact computation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389735,
author = {Yu, Ziqiang and Yu, Xiaohui and Koudas, Nick and Liu, Yang and Li, Yifan and Chen, Yueting and Yang, Dingyu},
title = {Distributed Processing of k Shortest Path Queries over Dynamic Road Networks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389735},
doi = {10.1145/3318464.3389735},
abstract = {The problem of identifying the k -shortest paths (KSPs for short) in a dynamic road network is essential to many location-based services. Road networks are dynamic in the sense that the weights of the edges in the corresponding graph constantly change over time, representing evolving traffic conditions. Very often such services have to process numerous KSP queries over large road networks at the same time, thus there is a pressing need to identify distributed solutions for this problem. However, most existing approaches are designed to identify KSPs on a static graph in a sequential manner (i.e., the (i+1)-th shortest path is generated based on the i-th shortest path), restricting their scalability and applicability in a distributed setting. We therefore propose KSP-DG, a distributed algorithm for identifying k-shortest paths in a dynamic graph. It is based on partitioning the entire graph into smaller subgraphs, and reduces the problem of determining KSPs into the computation of partial KSPs in relevant subgraphs, which can execute in parallel on a cluster of servers. A distributed two-level index called DTLP is developed to facilitate the efficient identification of relevant subgraphs. A salient feature of DTLP is that it indexes a set of virtual paths that are insensitive to varying traffic conditions, leading to very low maintenance cost in dynamic road networks. This is the first treatment of the problem of processing KSP queries over dynamic road networks. Extensive experiments conducted on real road networks confirm the superiority of our proposal over baseline methods.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {665–679},
numpages = {15},
keywords = {distributed computing, dynamic graph, k-shortest paths},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380567,
author = {Jachiet, Louis and Genev\`{e}s, Pierre and Gesbert, Nils and Layaida, Nabil},
title = {On the Optimization of Recursive Relational Queries: Application to Graph Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380567},
doi = {10.1145/3318464.3380567},
abstract = {Graph databases have received a lot of attention as they are particularly useful in many applications such as social networks, life sciences and the semantic web. Various languages have emerged to query graph databases, many of which embed forms of recursion which reveal essential for navigating in graphs. The relational model has benefited from a huge body of research in the last half century and that is why many graph databases rely on techniques of relational query engines. Since its introduction, the relational model has seen various attempts to extend it with recursion and it is now possible to use recursion in several SQL or Datalog based database systems. The optimization of recursive queries remains, however, a challenge. We propose mu-RA, a variation of the Relational Algebra equipped with a fixpoint operator for expressing recursive relational queries. mu-RA can notably express unions of conjunctive regular path queries. Leveraging the fact that this fixpoint operator makes recursive terms more amenable to algebraic transformations, we propose new rewrite rules. These rules makes it possible to generate new query execution plans, that cannot be obtained with previous approaches. We present the syntax and semantics of mu-RA, and the rewriting rules that we specifically devised to tackle the optimization of recursive queries. We report on practical experiments that show that the newly generated plans can provide significant performance improvements for evaluating recursive queries over graphs.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {681–697},
numpages = {17},
keywords = {relational algebra, graph queries, recursive queries, fixpoint, recursion, extension},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380590,
author = {Ying, Tangwei and Chen, Hanhua and Jin, Hai},
title = {Pensieve: Skewness-Aware Version Switching for Efficient Graph Processing},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380590},
doi = {10.1145/3318464.3380590},
abstract = {Multi-version graph processing has recently attracted much research efforts. Existing multi-version graph storage designs use either copy-based schemes or delta-based schemes. A copy-based scheme stores every version separately and may lead to expensive space cost due to high storage redundancy. On the contrary, a delta based scheme only stores incremental deltas between different versions and relies on delta computation for version switching. In this work, we observe: 1) high degree vertices incur much more significant storage overheads during graph version evolving compared to low degree vertices; 2) the skewed access frequency among graph versions greatly influences the system performance for version reproducing. Based on the observations, we propose Pensieve, a skewness-aware multi-version graph processing system. Two factors contribute to the efficiency of Pensieve. First, Pensieve leverages a differentiated graph storage strategy that stores low degree vertices using copy-based scheme while stores high degree ones using delta-based scheme. Such a design achieves a good trade-off between storage cost and version switching time for multi-version graph processing. Second, the Pensieve graph storage exploits the time locality of graph version access and designs a novel last-root version switching scheme, which significantly improves the access efficiency for recent versions. We implement Pensieve on top of Ligra, and conduct comprehensive experiments to evaluate the performance of this design using large-scale datasets collected from real world systems. The results show that Pensieve substantially outperforms state-of-the-art designs in terms of memory consumption and version switching time.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {699–713},
numpages = {15},
keywords = {graph version switching, graph storage, graph processing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380585,
author = {Fan, Grace and Fan, Wenfei and Li, Yuanhao and Lu, Ping and Tian, Chao and Zhou, Jingren},
title = {Extending Graph Patterns with Conditions},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380585},
doi = {10.1145/3318464.3380585},
abstract = {We propose an extension of graph patterns, referred to as conditional graph patterns and denoted as CGPs. In a CGP,one can specify a simple condition on each edge such that the edge exists if and only if the condition is satisfied. We show that CGPs allow us to catch missing links, increase the expressivity of graph functional dependencies, and provide a succinct representation of graph patterns. We settle the complexity of their consistency, matching, incremental matching and containment problems, in linear time,NP-complete,NP-complete and p2-complete, respectively. These tell us that despite the increased expressive power of CGPs, the matching and incremental matching problems for CGPs are no harder than their counterparts for conventional patterns. We develop algorithms for matching and incremental matching of CGPs, and for (incremental) multi-CGP matching and optimization. Using real-life and synthetic graphs, we empirically verify the efficiency and effectiveness of our algorithms.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {715–729},
numpages = {15},
keywords = {conditional graph pattern, multi-pattern matching, incremental matching},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386126,
author = {Liberty, Edo and Karnin, Zohar and Xiang, Bing and Rouesnel, Laurence and Coskun, Baris and Nallapati, Ramesh and Delgado, Julio and Sadoughi, Amir and Astashonok, Yury and Das, Piali and Balioglu, Can and Chakravarty, Saswata and Jha, Madhav and Gautier, Philip and Arpin, David and Januschowski, Tim and Flunkert, Valentin and Wang, Yuyang and Gasthaus, Jan and Stella, Lorenzo and Rangapuram, Syama and Salinas, David and Schelter, Sebastian and Smola, Alex},
title = {Elastic Machine Learning Algorithms in Amazon SageMaker},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386126},
doi = {10.1145/3318464.3386126},
abstract = {There is a large body of research on scalable machine learning (ML). Nevertheless, training ML models on large, continuously evolving datasets is still a difficult and costly undertaking for many companies and institutions. We discuss such challenges and derive requirements for an industrial-scale ML platform. Next, we describe the computational model behind Amazon SageMaker, which is designed to meet such challenges. SageMaker is an ML platform provided as part of Amazon Web Services (AWS), and supports incremental training, resumable and elastic learning as well as automatic hyperparameter optimization. We detail how to adapt several popular ML algorithms to its computational model. Finally, we present an experimental evaluation on large datasets, comparing SageMaker to several scalable, JVM-based implementations of ML algorithms, which we significantly outperform with regard to computation time and cost.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {731–737},
numpages = {7},
keywords = {scalable machine learning, elastic machine learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386136,
author = {Cao, Wei and Gao, Yusong and Li, Feifei and Wang, Sheng and Lin, Bingchen and Xu, Ke and Feng, Xiaojie and Wang, Yucong and Liu, Zhenjun and Zhang, Gejin},
title = {Timon: A Timestamped Event Database for Efficient Telemetry Data Processing and Analytics},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386136},
doi = {10.1145/3318464.3386136},
abstract = {With the increasing demand for real-time system monitoring and tracking in various contexts, the amount of time-stamped event data grows at an astonishing rate. Analytics on time-stamped events must be real time and the aggregated results need to be accurate even when data arrives out of order. Unfortunately, frequent occurrences of out-of-order data will significantly slow down the processing, and cause a large delay in the query response. Timon is a timestamped event database that aims to support aggregations and handle late arrivals both correctly (i.e., upholding the exactly-once semantics) and efficiently. Our insight is that a broad range of applications can be implemented with data structures and corresponding operators that satisfy associative and commutative properties. Records arriving after the low watermark are appended to Timon directly, allowing aggregations to be performed lazily. To improve query efficiency, Timon maintains a TS-LSM-Tree, which keeps the most recent data in memory and contains a time-partitioning tree on disk for high-volume data accumulated over long time span. Besides, Timon supports materialized aggregation views and correlation analysis across multiple streams. Timon has been successfully deployed at Alibaba Cloud and is a critical building block for Alibaba cloud's continuous monitoring and anomaly analysis infrastructure.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {739–753},
numpages = {15},
keywords = {real-time data analytics, cloud computing, time series database, data processing system, out-of-order events},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386137,
author = {Fard, Arash and Le, Anh and Larionov, George and Dhillon, Waqas and Bear, Chuck},
title = {Vertica-ML: Distributed Machine Learning in Vertica Database},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386137},
doi = {10.1145/3318464.3386137},
abstract = {A growing number of companies rely on machine learning as a key element for gaining a competitive edge from their collected Big Data. An in-database machine learning system can provide many advantages in this scenario, e.g., eliminating the overhead of data transfer, avoiding the maintenance costs of a separate analytical system, and addressing data security and provenance concerns. In this paper, we present our distributed machine learning subsystem within the Vertica database. This subsystem, Vertica-ML, includes machine learning functionalities with SQL API which cover a complete data science workflow as well as model management. We treat machine learning models in Vertica as first-class database objects like tables and views; therefore, they enjoy a similar mechanism for archiving and managing. We explain the architecture of the subsystem, and present a set of experiments to evaluate the performance of the machine learning algorithms implemented on top of it.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {755–768},
numpages = {14},
keywords = {big data, distributed computing, database, machine learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386140,
author = {Higginson, Antony S. and Dediu, Mihaela and Arsene, Octavian and Paton, Norman W. and Embury, Suzanne M.},
title = {Database Workload Capacity Planning Using Time Series Analysis and Machine Learning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386140},
doi = {10.1145/3318464.3386140},
abstract = {When procuring or administering any I.T. system or a component of an I.T. system, it is crucial to understand the computational resources required to run the critical business functions that are governed by any Service Level Agreements. Predicting the resources needed for future consumption is like looking into the proverbial crystal ball. In this paper we look at the forecasting techniques in use today and evaluate if those techniques are applicable to the deeper layers of the technological stack such as clustered database instances, applications and groups of transactions that make up the database workload. The approach has been implemented to use supervised machine learning to identify traits such as reoccurring patterns, shocks and trends that the workloads exhibit and account for those traits in the forecast. An experimental evaluation shows that the approach we propose reduces the complexity of performing a forecast, and accurate predictions have been produced for complex workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {769–783},
numpages = {15},
keywords = {forecasting, machine learning, database capacity planning, supervised learning, DBAAS, time series analysis},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386146,
author = {Smith, Micah J. and Sala, Carles and Kanter, James Max and Veeramachaneni, Kalyan},
title = {The Machine Learning Bazaar: Harnessing the ML Ecosystem for Effective System Development},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386146},
doi = {10.1145/3318464.3386146},
abstract = {As machine learning is applied more widely, data scientists often struggle to find or create end-to-end machine learning systems for specific tasks. The proliferation of libraries and frameworks and the complexity of the tasks have led to the emergence of "pipeline jungles" - brittle, ad hoc ML systems. To address these problems, we introduce the Machine Learning Bazaar, a new framework for developing machine learning and automated machine learning software systems. First, we introduce ML primitives, a unified API and specification for data processing and ML components from different software libraries. Next, we compose primitives into usable ML pipelines, abstracting away glue code, data flow, and data storage. We further pair these pipelines with a hierarchy of AutoML strategies - Bayesian optimization and bandit learning. We use these components to create a general-purpose, multi-task, end-to-end AutoML system that provides solutions to a variety of data modalities (image, text, graph, tabular, relational, etc.) and problem types (classification, regression, anomaly detection, graph matching, etc.). We demonstrate 5 real-world use cases and 2 case studies of our approach. Finally, we present an evaluation suite of 456 real-world ML tasks and describe the characteristics of 2.5 million pipelines searched over this task suite.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {785–800},
numpages = {16},
keywords = {ML pipelines, ML primitives, AutoML, machine learning, software development},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3393815,
author = {Noy, Natasha},
title = {When the Web is Your Data Lake: Creating a Search Engine for Datasets on the Web},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3393815},
doi = {10.1145/3318464.3393815},
abstract = {There are thousands of data repositories on the Web, providing access to millions of datasets. National and regional governments, scientific publishers and consortia, commercial data providers, and others publish data for fields ranging from social science to life science to high-energy physics to climate science and more. Access to this data is critical to facilitating reproducibility of research results, enabling scientists to build on others' work, and providing data journalists easier access to information and its provenance. In this talk, I will discuss our work on Dataset Search, which provides search capabilities over potentially all dataset repositories on the Web. I will talk about the open ecosystem for describing and citing datasets that we hope to encourage and the technical details on how we went about building Dataset Search. Finally, I will highlight research challenges in building a vibrant, heterogeneous, and open ecosystem where data becomes a first-class citizen.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {801},
numpages = {1},
keywords = {knowledge graphs, structured data, web search},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3393816,
author = {Syed, Awez},
title = {The Challenge of Building Effective, Enterprise-Scale Data Lakes},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3393816},
doi = {10.1145/3318464.3393816},
abstract = {There has been a rapid rise in the popularity of data lakes as the data infrastructure for modern analytics and data science. The combination of cloud storage and fast, elastic processing provides an inexpensive and scalable solution for building analytical applications. While data lakes make it easy to ingest and store vast amounts of data, the ability to effectively make use of that data is still limited. This data often lacks context, doesn't meet the quality required for applications, and is not easily understandable or discoverable by users. Problems of data consistency and accuracy make it hard to derive value from data lakes and to trust the analytics based on this data. The traditional methods of manually documenting, classifying and assessing the data don't scale to the volume of cloud-based data lakes. New automated, learning-based approaches are required to discover, curate and make the data usable for a wide variety of users. In this talk, we describe the real-world implementation patterns of data lakes and give an overview of the many open challenges in deploying successful, enterprise-scale data lakes.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {803},
numpages = {1},
keywords = {metadata, data profiling, data quality, data discovery, data provenance},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389775,
author = {Giannakopoulou, Stella and Karpathiotakis, Manos and Ailamaki, Anastasia},
title = {Cleaning Denial Constraint Violations through Relaxation},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389775},
doi = {10.1145/3318464.3389775},
abstract = {Data cleaning is a time-consuming process that depends on the data analysis that users perform. Existing solutions treat data cleaning as a separate offline process that takes place before analysis begins. Applying data cleaning before analysis assumes a priori knowledge of the inconsistencies and the query workload, thereby requiring effort on understanding and cleaning the data that is unnecessary for the analysis. We propose an approach that performs probabilistic repair of denial constraint violations on-demand, driven by the exploratory analysis that users perform. We introduce Daisy, a system that seamlessly integrates data cleaning into the analysis by relaxing query results. Daisy executes analytical query-workloads over dirty data by weaving cleaning operators into the query plan. Our evaluation shows that Daisy adapts to the workload and outperforms traditional offline cleaning on both synthetic and real-world workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {805–815},
numpages = {11},
keywords = {adaptive cleaning, data cleaning, denial constraints},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389721,
author = {Gilad, Amir and Deutch, Daniel and Roy, Sudeepa},
title = {On Multiple Semantics for Declarative Database Repairs},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389721},
doi = {10.1145/3318464.3389721},
abstract = {We study the problem of database repairs through a rule-based framework that we refer to as Delta Rules. Delta rules are highly expressive and allow specifying complex, cross-relations repair logic associated with Denial Constraints, Causal Rules, and allowing to capture Database Triggers of interest. We show that there are no one-size-fits-all semantics for repairs in this inclusive setting, and we consequently introduce multiple alternative semantics, presenting the case for using each of them. We then study the relationships between the semantics in terms of their output and the complexity of computation. Our results formally establish the tradeoff between the permissiveness of the semantics and its computational complexity. We demonstrate the usefulness of the framework in capturing multiple data repair scenarios for an academic search database and the TPC-H databases, showing how using different semantics affects the repair in terms of size and runtime, and examining the relationships between the repairs. We also compare our approach with SQL triggers and a state-of-the-art data repair system.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {817–831},
numpages = {15},
keywords = {triggers, provenance, repairs, database constraints},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389786,
author = {Wei, Ziheng and Hartmann, Sven and Link, Sebastian},
title = {Discovery Algorithms for Embedded Functional Dependencies},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389786},
doi = {10.1145/3318464.3389786},
abstract = {Embedded functional dependencies (eFDs) advance data management applications by data completeness and integrity requirements. We show that the discovery problem of eFDs is NP-complete, W[2]-complete in the output, and has a minimum solution space that is larger than the maximum solution space for functional dependencies. Nevertheless, we use novel data structures and search strategies to develop row-efficient, column-efficient, and hybrid algorithms for eFD discovery. Our experiments demonstrate that the algorithms scale well in terms of their design targets, and that ranking the eFDs by the number of redundant data values they cause can provide useful guidance in identifying meaningful eFDs for applications. Finally, we demonstrate the benefits of introducing completeness requirements and ranking by the number of redundant data values for approximate and genuine functional dependencies.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {833–843},
numpages = {11},
keywords = {missing data, embedded functional dependency, discovery},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380568,
author = {Yan, Jing Nathan and Schulte, Oliver and Zhang, MoHan and Wang, Jiannan and Cheng, Reynold},
title = {SCODED: Statistical Constraint Oriented Data Error Detection},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380568},
doi = {10.1145/3318464.3380568},
abstract = {Statistical Constraints (SCs) play an important role in statistical modeling and analysis. This paper brings the concept to data cleaning and studies how to leverage SCs for error detection. SCs provide a novel approach that has various application scenarios and works harmoniously with downstream statistical modeling. Entailment relationships between SCs and integrity constraints provide analytical insight into SCs. We develop SCODED, an SC-Oriented Data Error Detection system, comprising two key components: (1) SC Violation Detection : checks whether an SC is violated on a given dataset, and (2) Error Drill Down : identifies the top-k records that contribute most to the violation of an SC. Experiments on synthetic and real-world data show that SCs are effective in detecting data errors that violate them, compared to state-of-the-art approaches.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {845–860},
numpages = {16},
keywords = {error detection, statistical constraints, machine learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389749,
author = {Zhang, Yunjia and Guo, Zhihan and Rekatsinas, Theodoros},
title = {A Statistical Perspective on Discovering Functional Dependencies in Noisy Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389749},
doi = {10.1145/3318464.3389749},
abstract = {We study the problem of discovering functional dependencies (FD) from a noisy data set. We adopt a statistical perspective and draw connections between FD discovery and structure learning in probabilistic graphical models. We show that discovering FDs from a noisy data set is equivalent to learning the structure of a model over binary random variables, where each random variable corresponds to a functional of the data set attributes. We build upon this observation to introduce FDX a conceptually simple framework in which learning functional dependencies corresponds to solving a sparse regression problem. We show that FDX can recover true functional dependencies across a diverse array of real-world and synthetic data sets, even in the presence of noisy or missing data. We find that FDX scales to large data instances with millions of tuples and hundreds of attributes while it yields an average F1 improvement of 2x against state-of-the-art FD discovery methods.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {861–876},
numpages = {16},
keywords = {functional dependencies, structure learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389716,
author = {Haubenschild, Michael and Sauer, Caetano and Neumann, Thomas and Leis, Viktor},
title = {Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389716},
doi = {10.1145/3318464.3389716},
abstract = {For decades, ARIES has been the standard for logging and recovery in database systems. ARIES offers important features like support for arbitrary workloads, fuzzy checkpoints, and transparent index recovery. Nevertheless, many modern in-memory database systems use more lightweight approaches that have less overhead and better multi-core scalability but only work well for the in-memory setting. Recently, a new class of high-performance storage engines has emerged, which exploit fast SSDs to achieve performance close to pure in-memory systems but also allow out-of-memory workloads. For these systems, ARIES is too slow whereas in-memory logging proposals are not applicable. In this work, we propose a new logging and recovery design that supports incremental and fuzzy checkpointing, index recovery, out-of-memory workloads, and low-latency transaction commits. Our continuous checkpointing algorithm guarantees bounded recovery time. Using per-thread logging with minimal synchronization, our implementation achieves near-linear scalability on multi-core CPUs. We implemented and evaluated these techniques in our LeanStore storage engine. For working sets that fit in main memory, we achieve performance close to that of an in-memory approach, even with logging, checkpointing, and dirty page writing enabled. For the out-of-memory scenario, we outperform a state-of-the-art ARIES implementation by a factor of two.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {877–892},
numpages = {16},
keywords = {storage engine, checkpointing, leanstore, recovery, logging},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389757,
author = {Sarkar, Subhadeep and Papon, Tarikul Islam and Staratzis, Dimitris and Athanassoulis, Manos},
title = {Lethe: A Tunable Delete-Aware LSM Engine},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389757},
doi = {10.1145/3318464.3389757},
abstract = {Data-intensive applications fueled the evolution of log structured merge (LSM) based key-value engines that employ the out-of-place paradigm to support high ingestion rates with low read/write interference. These benefits, however, come at the cost of treating deletes as a second-class citizen. A delete inserts a tombstone that invalidates older instances of the deleted key. State-of-the-art LSM engines do not provide guarantees as to how fast a tombstone will propagate to persist the deletion. Further, LSM engines only support deletion on the sort key. To delete on another attribute (e.g., timestamp), the entire tree is read and re-written. We highlight that fast persistent deletion without affecting read performance is key to support: (i) streaming systems operating on a window of data, (ii) privacy with latency guarantees on the right-to-be-forgotten, and (iii) en masse cloud deployment of data systems that makes storage a precious resource. To address these challenges, in this paper, we build a new key-value storage engine, Lethe, that uses a very small amount of additional metadata, a set of new delete-aware compaction policies, and a new physical data layout that weaves the sort and the delete key order. We show that Lethe supports any user-defined threshold for the delete persistence latency offering higher read throughput (1.17-1.4x) and lower space amplification (2.1-9.8x), with a modest increase in write amplification (between 4% and 25%). In addition, Lethe supports efficient range deletes on a secondary delete key by dropping entire data pages without sacrificing read performance nor employing a costly full tree merge.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {893–908},
numpages = {16},
keywords = {sort key, delete persistence threshold, compaction, compaction policy, out-of-place deletes, interweaved storage layout, compaction trigger, delete key, fade, delete persistence latency, lethe, TTL-driven compaction, LSM-trees, delete persistence, kiwi},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380563,
author = {Li, Linwei and Zhang, Kai and Guo, Jiading and He, Wen and He, Zhenying and Jing, Yinan and Han, Weili and Wang, X. Sean},
title = {BinDex: A Two-Layered Index for Fast and Robust Scans},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380563},
doi = {10.1145/3318464.3380563},
abstract = {In modern analytical database systems, the performance of the data scan operation is of key importance to the performance of query execution. Existing approaches may be categorized into index scan and sequential scan. However, both approaches have inherent inefficiencies. Indeed, sequential scan may need to access a large amount of unneeded data, especially for queries with low selectivity. Instead, index scan may involve a large number of expensive random memory accesses when the query selectivity is high. Moreover, with the growing complexities in database query workloads, it has become hard to predict which approach is better for a particular query. In order to obtain fast and robust scans under all selectivities, this paper proposes BinDex, a two-layered index structure based on binned bitmaps that can be used to significantly accelerate the scan operations for in-memory column stores. The first layer of BinDex consists of a set of binned bitmaps which filter out most unneeded values in a column. The second layer provides some auxiliary information to correct the bits that have incorrect values. By varying the number of bit vectors in the first layer, BinDex can make a tradeoff between memory space and performance. Experimental results show that BinDex outperforms the state-of-the-art approaches with less memory than a B+-tree would use. And by enlarging the memory space, BinDex can achieve up to 2.9 times higher performance, eliminating the need for making a choice between sequential or index scans.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {909–923},
numpages = {15},
keywords = {in-memory column stores, indexing, scan},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389773,
author = {Yue, Cong and Xie, Zhongle and Zhang, Meihui and Chen, Gang and Ooi, Beng Chin and Wang, Sheng and Xiao, Xiaokui},
title = {Analysis of Indexing Structures for Immutable Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389773},
doi = {10.1145/3318464.3389773},
abstract = {In emerging applications such as blockchains and collaborative data analytics, there are strong demands for data immutability, multi-version accesses, and tamper-evident controls. To provide efficient support for lookup and merge operations, three new index structures for immutable data, namely Merkle Patricia Trie (MPT), Merkle Bucket Tree(MBT), and Pattern-Oriented-Split Tree (POS-Tree), have been proposed. Although these structures have been adopted in real applications, there is no systematic evaluation of their pros and cons in the literature, making it difficult for practitioners to choose the right index structure for their applications. To alleviate the above problem, we present a comprehensive analysis of the existing index structures for immutable data, and evaluate both their asymptotic and empirical performance. Specifically, we show that MPT, MBT, and POS-Tree are all instances of a recently proposed framework, dubbed Structurally Invariant and Reusable Indexes (SIRI). We propose to evaluate the SIRI instances on their index performance and deduplication capability. We establish the worst-case guarantees of each index, and experimentally evaluate all indexes in a wide variety of settings. Based on our theoretical and empirical analysis, we conclude that POS-Tree is a favorable choice for indexing immutable data.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {925–935},
numpages = {11},
keywords = {versioning, indexing, immutable data, deduplication},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380588,
author = {Lang, Harald and Beischl, Alexander and Leis, Viktor and Boncz, Peter and Neumann, Thomas and Kemper, Alfons},
title = {Tree-Encoded Bitmaps},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380588},
doi = {10.1145/3318464.3380588},
abstract = {We propose a novel method to represent compressed bitmaps. Similarly to existing bitmap compression schemes, we exploit the compression potential of bitmaps populated with consecutive identical bits, i.e., 0-runs and 1-runs. But in contrast to prior work, our approach employs a binary tree structure to represent runs of various lengths. Leaf nodes in the upper tree levels thereby represent longer runs, and vice versa. The tree-based representation results in high compression ratios and enables efficient random access, which in turn allows for the fast intersection of bitmaps. Our experimental analysis with randomly generated bitmaps shows that our approach significantly improves over state-of-the-art compression techniques when bitmaps are dense and/or only barely clustered. Further, we evaluate our approach with real-world data sets, showing that our tree-encoded bitmaps can save up to one third of the space over existing techniques.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {937–967},
numpages = {31},
keywords = {succinct, compression, bitmap, indexing, data structure},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389711,
author = {Ding, Jialin and Minhas, Umar Farooq and Yu, Jia and Wang, Chi and Do, Jaeyoung and Li, Yinan and Zhang, Hantian and Chandramouli, Badrish and Gehrke, Johannes and Kossmann, Donald and Lomet, David and Kraska, Tim},
title = {ALEX: An Updatable Adaptive Learned Index},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389711},
doi = {10.1145/3318464.3389711},
abstract = {Recent work on "learned indexes" has changed the way we look at the decades-old field of DBMS indexing. The key idea is that indexes can be thought of as "models" that predict the position of a key in a dataset. Indexes can, thus, be learned. The original work by Kraska et al. shows that a learned index beats a B+ tree by a factor of up to three in search time and by an order of magnitude in memory footprint. However, it is limited to static, read-only workloads. In this paper, we present a new learned index called ALEX which addresses practical issues that arise when implementing learned indexes for workloads that contain a mix of point lookups, short range queries, inserts, updates, and deletes. ALEX effectively combines the core insights from learned indexes with proven storage and indexing techniques to achieve high performance and low memory footprint. On read-only workloads, ALEX beats the learned index from Kraska et al. by up to 2.2X on performance with up to 15X smaller index size. Across the spectrum of read-write workloads, ALEX beats B+ trees by up to 4.1X while never performing worse, with up to 2000X smaller index size. We believe ALEX presents a key step towards making learned indexes practical for a broader class of database workloads with dynamic updates.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {969–984},
numpages = {16},
keywords = {access methods, learned indexes, learned data structures, B+ tree},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380579,
author = {Nathan, Vikram and Ding, Jialin and Alizadeh, Mohammad and Kraska, Tim},
title = {Learning Multi-Dimensional Indexes},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380579},
doi = {10.1145/3318464.3380579},
abstract = {Scanning and filtering over multi-dimensional tables are key operations in modern analytical database engines. To optimize the performance of these operations, databases often create clustered indexes over a single dimension or multi-dimensional indexes such as R-Trees, or use complex sort orders (e.g., Z-ordering). However, these schemes are often hard to tune and their performance is inconsistent across different datasets and queries. In this paper, we introduce Flood, a multi-dimensional in-memory read-optimized index that automatically adapts itself to a particular dataset and workload by jointly optimizing the index structure and data storage layout. Flood achieves up to three orders of magnitude faster performance for range scans with predicates than state-of-the-art multi-dimensional indexes or sort orders on real-world datasets and workloads. Our work serves as a building block towards an end-to-end learned database system.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {985–1000},
numpages = {16},
keywords = {primary index, in-memory, databases, multi-dimensional, indexing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389752,
author = {Kristo, Ani and Vaidya, Kapil and \c{C}etintemel, Ugur and Misra, Sanchit and Kraska, Tim},
title = {The Case for a Learned Sorting Algorithm},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389752},
doi = {10.1145/3318464.3389752},
abstract = {Sorting is one of the most fundamental algorithms in Computer Science and a common operation in databases not just for sorting query results but also as part of joins (i.e., sort-merge-join) or indexing. In this work, we introduce a new type of distribution sort that leverages a learned model of the empirical CDF of the data. Our algorithm uses a model to efficiently get an approximation of the scaled empirical CDF for each record key and map it to the corresponding position in the output array. We then apply a deterministic sorting algorithm that works well on nearly-sorted arrays (e.g., Insertion Sort) to establish a totally sorted order. We compared this algorithm against common sorting approaches and measured its performance for up to 1 billion normally-distributed double-precision keys. The results show that our approach yields an average 3.38x performance improvement over C++ STL sort, which is an optimized Quicksort hybrid, 1.49x improvement over sequential Radix Sort, and 5.54x improvement over a C++ implementation of Timsort, which is the default sorting function for Java and Python.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1001–1016},
numpages = {16},
keywords = {CDF, RMI, ML for systems, learned algorithm, linear interpolation, sorting, linear models, sorting algorithm},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389727,
author = {Park, Yongjoo and Zhong, Shucheng and Mozafari, Barzan},
title = {QuickSel: Quick Selectivity Learning with Mixture Models},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389727},
doi = {10.1145/3318464.3389727},
abstract = {Estimating the selectivity of a query is a key step in almost any cost-based query optimizer. Most of today's databases rely on histograms or samples that are periodically refreshed by re-scanning the data as the underlying data changes. Since frequent scans are costly, these statistics are often stale and lead to poor selectivity estimates. As an alternative to scans, query-driven histograms have been proposed, which refine the histograms based on the actual selectivities of the observed queries. Unfortunately, these approaches are either too costly to use in practice---i.e., require an exponential number of buckets---or quickly lose their advantage as they observe more queries. In this paper, we propose a selectivity learning framework, called QuickSel, which falls into the query-driven paradigm but does not use histograms. Instead, it builds an internal model of the underlying data, which can be refined significantly faster (e.g., only 1.9 milliseconds for 300 queries). This fast refinement allows QuickSel to continuously learn from each query and yield increasingly more accurate selectivity estimates over time. Unlike query-driven histograms, QuickSel relies on a mixture model and a new optimization algorithm for training its model. Our extensive experiments on two real-world datasets confirm that, given the same target accuracy, QuickSel is 34.0x--179.4x faster than state-of-the-art query-driven histograms, including ISOMER and STHoles. Further, given the same space budget, QuickSel is 26.8%--91.8% more accurate than periodically-updated histograms and samples, respectively.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1017–1033},
numpages = {17},
keywords = {approximate query processing, database learning, selectivity learning, cardinality estimation, selectivity estimation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389741,
author = {Hasan, Shohedul and Thirumuruganathan, Saravanan and Augustine, Jees and Koudas, Nick and Das, Gautam},
title = {Deep Learning Models for Selectivity Estimation of Multi-Attribute Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389741},
doi = {10.1145/3318464.3389741},
abstract = {Selectivity estimation - the problem of estimating the result size of queries - is a fundamental problem in databases. Accurate estimation of query selectivity involving multiple correlated attributes is especially challenging. Poor cardinality estimates could result in the selection of bad plans by the query optimizer. Recently, deep learning has been applied to this problem with promising results. However, many of the proposed approaches often struggle to provide accurate results for multi attribute queries involving large number of predicates and with low selectivity. In this paper, we propose two complementary approaches that are effective for this scenario. Our first approach models selectivity estimation as a density estimation problem where one seeks to estimate the joint probability distribution from a finite number of samples. We leverage techniques from neural density estimation to build an accurate selectivity estimator. The key idea is to decompose the joint distribution into a set of tractable conditional probability distributions such that they satisfy the autoregressive property. Our second approach formulates selectivity estimation as a supervised deep learning problem that predicts the selectivity of a given query. We describe how to extend our algorithms for range queries. We also introduce and address a number of practical challenges arising when adapting deep learning for relational data. These include query/data featurization, incorporating query workload information in a deep learning framework and the dynamic scenario where both data and workload queries could be updated. Our extensive experiments with a special emphasis on queries with a large number of predicates and/or small result sizes demonstrates that our proposed techniques provide fast and accurate selective estimates with minimal space overhead.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1035–1050},
numpages = {16},
keywords = {made, cardinality estimation, density estimation, deep learning, neural autoregressive models, selectivity estimation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389697,
author = {Ma, Chenhao and Fang, Yixiang and Cheng, Reynold and Lakshmanan, Laks V.S. and Zhang, Wenjie and Lin, Xuemin},
title = {Efficient Algorithms for Densest Subgraph Discovery on Large Directed Graphs},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389697},
doi = {10.1145/3318464.3389697},
abstract = {Given a directed graph G, the directed densest subgraph (DDS) problem refers to the finding of a subgraph from G, whose density is the highest among all the subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fraud detection, community mining, and graph compression. However, existing DDS solutions suffer from efficiency and scalability problems: on a three-thousand-edge graph, it takes three days for one of the best exact algorithms to complete. In this paper, we develop an efficient and scalable DDS solution. We introduce the notion of [x, y]-core, which is a dense subgraph for G, and show that the densest subgraph can be accurately located through the [x, y]-core with theoretical guarantees. Based on the [x, y]-core, we develop exact and approximation algorithms. We have performed an extensive evaluation of our approaches on eight real large datasets. The results show that our proposed solutions are up to six orders of magnitude faster than the state-of-the-art.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1051–1066},
numpages = {16},
keywords = {directed graph, densest subgraph discovery},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389699,
author = {Guo, Wentian and Li, Yuchen and Sha, Mo and He, Bingsheng and Xiao, Xiaokui and Tan, Kian-Lee},
title = {GPU-Accelerated Subgraph Enumeration on Partitioned Graphs},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389699},
doi = {10.1145/3318464.3389699},
abstract = {Subgraph enumeration is important for many applications such as network motif discovery and community detection. Recent works utilize graphics processing units (GPUs) to parallelize subgraph enumeration, but they can only handle graphs that fit into the GPU memory. In this paper, we propose a new approach for GPU-accelerated subgraph enumeration that can efficiently scale to large graphs beyond the GPU memory. Our approach divides the graph into partitions, each of which fits into the GPU memory. The GPU processes one partition at a time and searches the matched subgraphs of a given pattern (i.e., instances) within the partition as in the small graph. The key challenge is on enumerating the instances across different partitions, because this search would enumerate considerably redundant subgraphs and cause the expensive data transfer cost via the PCI-e bus. Therefore, we propose a novel shared execution approach to eliminate the redundant subgraph searches and correctly generate all the instances across different partitions. The experimental evaluation shows that our approach can scale to large graphs and achieve significantly better performance than the existing single-machine solutions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1067–1082},
numpages = {16},
keywords = {GPU, subgraph enumeration, partitioned graph},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380581,
author = {Sun, Shixuan and Luo, Qiong},
title = {In-Memory Subgraph Matching: An In-Depth Study},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380581},
doi = {10.1145/3318464.3380581},
abstract = {We study the performance of eight representative in-memory subgraph matching algorithms. Specifically, we put QuickSI, GraphQL, CFL, CECI, DP-iso, RI and VF2++ in a common framework to compare them on the following four aspects: (1) method of filtering candidate vertices in the data graph; (2) method of ordering query vertices; (3) method of enumerating partial results; and (4) other optimization techniques. Then, we compare the overall performance of these algorithms with Glasgow, an algorithm based on the constraint programming. Through experiments, we find that (1) the filtering method of GraphQL is competitive to that of the latest algorithms CFL, CECI and DP-iso in terms of pruning power; (2) the ordering methods in GraphQL and RI are usually the most effective; (3) the set intersection based local candidate computation in CECI and DP-iso performs the best in the enumeration; and (4) the failing sets pruning in DP-iso can significantly improve the performance when queries become large. Our source code is publicly available at https://github.com/RapidsAtHKUST/SubgraphMatching.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1083–1098},
numpages = {16},
keywords = {comparison and analysis, graph, subgraph matching},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389702,
author = {Park, Yeonsu and Ko, Seongyun and Bhowmick, Sourav S. and Kim, Kyoungmin and Hong, Kijae and Han, Wook-Shin},
title = {G-CARE: A Framework for Performance Benchmarking of Cardinality Estimation Techniques for Subgraph Matching},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389702},
doi = {10.1145/3318464.3389702},
abstract = {Despite the crucial role of cardinality estimation in query optimization, there has been no systematic and in-depth study of the existing cardinality estimation techniques for subgraph matching queries. In this paper, for the first time, we present a comprehensive study of the existing cardinality estimation techniques for subgraph matching queries, scaling far beyond the original experiments. We first introduce a novel framework called g-care that enables us to realize all existing techniques on top of it and that provides insights on their performance. By using g-care, we then reimplement representative cardinality estimation techniques for graph databases as well as relational databases. We next evaluate these techniques w.r.t accuracy on rdf and non-rdf graphs from different domains with subgraph matching queries of various topologies so far considered. Surprisingly, our results reveal that all existing techniques have serious problems in accuracy for various scenarios and datasets. Intriguingly, a simple sampling method based on an online aggregation technique designed for relational data, consistently outperforms all existing techniques.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1099–1114},
numpages = {16},
keywords = {subgraph matching, graph query, graph database, cardinality estimation, query optimization},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380566,
author = {Reza, Tashin and Ripeanu, Matei and Sanders, Geoffrey and Pearce, Roger},
title = {Approximate Pattern Matching in Massive Graphs with Precision and Recall Guarantees},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380566},
doi = {10.1145/3318464.3380566},
abstract = {There are multiple situations where supporting approximation in graph pattern matching tasks is highly desirable: (i) the data acquisition process can be noisy; (ii) a user may only have an imprecise idea of the search query; and (iii) approximation can be used for high volume vertex labeling when extracting machine learning features from graph data. We present a new algorithmic pipeline for approximate matching that combines edit-distance based matching with systematic graph pruning. We formalize the problem as identifying all exact matches for up to k edit-distance subgraphs of a user-supplied template. We design a solution which exploits unique optimization opportunities within the design space, not explored previously. Our solution is (i) highly scalable, (ii) supports arbitrary patterns and edit-distance, (iii) offers 100% precision and 100% recall guarantees, and (vi) supports a set of popular data analysis scenarios. We demonstrate its advantages through an implementation that offers good strong and weak scaling on massive real-world (257 billion edges) and synthetic (1.1 trillion edges) labeled graphs, respectively, and when operating on a massive cluster (256 nodes/9,216 cores), orders of magnitude larger than previously used for similar problems. Empirical comparison with the state-of-the-art highlights the advantages of our solution when handling massive graphs and complex patterns.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1115–1131},
numpages = {17},
keywords = {graph processing, distributed computing, pattern matching},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380597,
author = {Meduri, Venkata Vamsikrishna and Popa, Lucian and Sen, Prithviraj and Sarwat, Mohamed},
title = {A Comprehensive Benchmark Framework for Active Learning Methods in Entity Matching},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380597},
doi = {10.1145/3318464.3380597},
abstract = {Entity Matching (EM) is a core data cleaning task, aiming to identify different mentions of the same real-world entity. Active learning is one way to address the challenge of scarce labeled data in practice, by dynamically collecting the necessary examples to be labeled by an Oracle and refining the learned model (classifier) upon them. In this paper, we build a unified active learning benchmark framework for EM that allows users to easily combine different learning algorithms with applicable example selection algorithms. The goal of the framework is to enable concrete guidelines for practitioners as to what active learning combinations will work well for EM. Towards this, we perform comprehensive experiments on publicly available EM datasets from product and publication domains to evaluate active learning methods, using a variety of metrics including EM quality, #labels and example selection latencies. Our most surprising result finds that active learning with fewer labels can learn a classifier of comparable quality as supervised learning. In fact, for several of the datasets, we show that there is an active learning combination that beats the state-of-the-art supervised learning result. Our framework also includes novel optimizations that improve the quality of the learned model by roughly 9% in terms of F1-score and reduce example selection latencies by up to 10\texttimes{} without affecting the quality of the model.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1133–1147},
numpages = {15},
keywords = {rule-based models, learner-agnostic selectors, neural networks, perfect and noisy oracles, ensembles, margin, random forests, blocking dimensions, example selectors, unified active learning, SVM, learner-aware selectors, query by committee, entity matching},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389743,
author = {Wu, Renzhi and Chaba, Sanya and Sawlani, Saurabh and Chu, Xu and Thirumuruganathan, Saravanan},
title = {ZeroER: Entity Resolution Using Zero Labeled Examples},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389743},
doi = {10.1145/3318464.3389743},
abstract = {Entity resolution (ER) refers to the problem of matching records in one or more relations that refer to the same real-world entity. While supervised machine learning (ML) approaches achieve the state-of-the-art results, they require a large amount of labeled examples that are expensive to obtain and often times infeasible. We investigate an important problem that vexes practitioners: is it possible to design an effective algorithm for ER that requires Zero labeled examples, yet can achieve performance comparable to supervised approaches? In this paper, we answer in the affirmative through our proposed approach dubbed ZeroER. Our approach is based on a simple observation --- the similarity vectors for matches should look different from that of unmatches. Operationalizing this insight requires a number of technical innovations. First, we propose a simple yet powerful generative model based on Gaussian Mixture Models for learning the match and unmatch distributions. Second, we propose an adaptive regularization technique customized for ER that ameliorates the issue of feature overfitting. Finally, we incorporate the transitivity property into the generative model in a novel way resulting in improved accuracy. On five benchmark ER datasets, we show that ZeroER greatly outperforms existing unsupervised approaches and achieves comparable performance to supervised approaches.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1149–1164},
numpages = {16},
keywords = {unsupervised learning, entity matching, entity resolution},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380572,
author = {Chen, Zhaoqiang and Chen, Qun and Hou, Boyi and Li, Zhanhuai and Li, Guoliang},
title = {Towards Interpretable and Learnable Risk Analysis for Entity Resolution},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380572},
doi = {10.1145/3318464.3380572},
abstract = {Machine-learning-based entity resolution has been widely studied. However, some entity pairs may be mislabeled by machine learning models and existing studies do not study the risk analysis problem -- predicting and interpreting which entity pairs are mislabeled. In this paper, we propose an interpretable and learnable framework for risk analysis, which aims to rank the labeled pairs based on their risks of being mislabeled. We first describe how to automatically generate interpretable risk features, and then present a learnable risk model and its training technique. Finally, we empirically evaluate the performance of the proposed approach on real data. Our extensive experiments have shown that the learning risk model can identify the mislabeled pairs with considerably higher accuracy than the existing alternatives.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1165–1180},
numpages = {16},
keywords = {entity resolution, learnable risk model, interpretability},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389761,
author = {Bas\"{\i}k, Fuat and Ferhatosmano\u{g}lu, Hakan and Gedik, Bu\'{c}ra},
title = {SLIM: Scalable Linkage of Mobility Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389761},
doi = {10.1145/3318464.3389761},
abstract = {We present a scalable solution to link entities across mobility datasets using their spatio-temporal information. This is a fundamental problem in many applications such as linking user identities for security, understanding privacy limitations of location based services, or producing a unified dataset from multiple sources for urban planning. Such integrated datasets are also essential for service providers to optimise their services and improve business intelligence. In this paper, we first propose a mobility based representation and similarity computation for entities. An efficient matching process is then developed to identify the final linked pairs, with an automated mechanism to decide when to stop the linkage. We scale the process with a locality-sensitive hashing (LSH) based approach that significantly reduces candidate pairs for matching. To realize the effectiveness and efficiency of our techniques in practice, we introduce an algorithm called SLIM. In the experimental evaluation, SLIM outperforms the two existing state-of-the-art approaches in terms of precision and recall. Moreover, the LSH-based approach brings two to four orders of magnitude speedup.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1181–1196},
numpages = {16},
keywords = {data integration, mobility data, entity linkage, scalability},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380570,
author = {Wang, Yaoshu and Xiao, Chuan and Qin, Jianbin and Cao, Xin and Sun, Yifang and Wang, Wei and Onizuka, Makoto},
title = {Monotonic Cardinality Estimation of Similarity Selection: A Deep Learning Approach},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380570},
doi = {10.1145/3318464.3380570},
abstract = {In this paper, we investigate the possibilities of utilizing deep learning for cardinality estimation of similarity selection. Answering this problem accurately and efficiently is essential to many data management applications, especially for query optimization. Moreover, in some applications the estimated cardinality is supposed to be consistent and interpretable. Hence a monotonic estimation w.r.t. the query threshold is preferred. We propose a novel and generic method that can be applied to any data type and distance function. Our method consists of a feature extraction model and a regression model. The feature extraction model transforms original data and threshold to a Hamming space, in which a deep learning-based regression model is utilized to exploit the incremental property of cardinality w.r.t. the threshold for both accuracy and monotonicity. We develop a training strategy tailored to our model as well as techniques for fast estimation. We also discuss how to handle updates. We demonstrate the accuracy and the efficiency of our method through experiments, and show how it improves the performance of a query optimizer.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1197–1212},
numpages = {16},
keywords = {similarity selection, machine learning for data management, cardinality estimation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380607,
author = {Deep, Shaleen and Hu, Xiao and Koutris, Paraschos},
title = {Fast Join Project Query Evaluation Using Matrix Multiplication},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380607},
doi = {10.1145/3318464.3380607},
abstract = {In the last few years, much effort has been devoted to developing join algorithms to achieve worst-case optimality for join queries over relational databases. Towards this end, the database community has had considerable success in developing efficient algorithms that achieve worst-case optimal runtime for full join queries, i.e., joins without projections. However, not much is known about join evaluation with projections beyond some simple techniques of pushing down the projection operator in the query execution plan. Such queries have a large number of applications in entity matching, graph analytics and searching over compressed graphs. In this paper, we study how a class of join queries with projections can be evaluated faster using worst-case optimal algorithms together with matrix multiplication. Crucially, our algorithms are parameterized by the output size of the final result, allowing for choosing the best execution strategy. We implement our algorithms as a subroutine and compare the performance with state-of-the-art techniques to show they can be improved upon by as much as 50x. More importantly, our experiments indicate that matrix multiplication is a useful operation that can help speed up join processing owing to highly optimized open source libraries that are also highly parallelizable.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1213–1223},
numpages = {11},
keywords = {projections, relational algebra, join queries, matrix multiplication},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380586,
author = {Wang, Qichen and Yi, Ke},
title = {Maintaining Acyclic Foreign-Key Joins under Updates},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380586},
doi = {10.1145/3318464.3380586},
abstract = {A large number of analytical queries (e.g., all the 22 queries in the TPC-H benchmark) are based on acyclic foreign-key joins. In this paper, we study the problem of incrementally maintaining the query results of these joins under updates, i.e., insertion and deletion of tuples to any of the relations. Prior work has shown that this problem is inherently hard, requiring at least Ω(|db|1/2 -ε) time per update, where |db| is the size of the database, and ε &gt; 0 can be any small constant. However, this negative result holds only on adversarially constructed update sequences; on the other hand, most real-world update sequences are "nice", nowhere near these worst-case scenarios. We introduce a measure λ, which we call the enclosureness of the update sequence, to more precisely characterize its intrinsic difficulty. We present an algorithm to maintain the query results of any acyclic foreign-key join in O(λ) time amortized, on any update sequence whose enclosureness is λ. This is complemented with a lower bound of Ω(λ1-ε), showing that our algorithm is essentially optimal with respect to λ. Next, using this algorithm as the core component, we show how all the 22 queries in the TPC-H benchmark can be supported in ~O(\l{}ambda) time. Finally, based on the algorithms developed, we built a continuous query processing system on top of Flink, and experimental results show that our system outperforms previous ones significantly.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1225–1239},
numpages = {15},
keywords = {query evaluation under updates, incremental view maintenance, acyclic joins, sliding windows},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389756,
author = {Tang, Dixin and Shang, Zechao and Elmore, Aaron J. and Krishnan, Sanjay and Franklin, Michael J.},
title = {Thrifty Query Execution via Incrementability},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389756},
doi = {10.1145/3318464.3389756},
abstract = {Many applications schedule queries before all data is ready. To return fast query results, database systems can eagerly process existing data and incrementally incorporate new data into prior intermediate results, which often relies on incremental view maintenance (IVM) techniques. However, incrementally maintaining a query result can increase the total amount of work mainly as some early work is not useful for computing the final query result. In this paper, we propose a new metric incrementability to quantify the cost-effectiveness of IVM to decide how eagerly or lazily databases should incrementally execute a query. We further observe that different parts of a query have different levels of incrementability and the query execution should have a decomposed control flow based on the difference. Therefore, to address these needs, we propose a new query processing method Incrementability-aware Query Processing (InQP). We build a prototype InQP system based on Spark and show that InQP significantly reduces resource consumption with a similar latency compared with incrementability-oblivious approaches.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1241–1256},
numpages = {16},
keywords = {query service, incremental view maintenance, resource efficiency, non-positive query, cloud database, incrementability},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389766,
author = {He, Wenjia and Anderson, Michael R. and Strome, Maxwell and Cafarella, Michael},
title = {A Method for Optimizing Opaque Filter Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389766},
doi = {10.1145/3318464.3389766},
abstract = {An important class of database queries in machine learning and data science workloads is the opaque filter query: a query with a selection predicate that is implemented with a UDF, with semantics that are unknown to the query optimizer. Some typical examples would include a CNN-style trained image classifier, or a textual sentiment classifier. Because the optimizer does not know the predicate's semantics, it cannot employ standard optimizations, yielding long query times. We propose voodoo indexing, a two-phase method for optimizing opaque filter queries. Before any query arrives, the method builds a hierarchical "query-independent" index of the database contents, which groups together similar objects. At query-time, the method builds a map of how much each group satisfies the predicate, while also exploiting the map to accelerate execution. Unlike past methods, voodoo indexing does not require insight into predicate semantics, works on any data type, and does not require in-query model training. We describe both standalone and SparkSQL-specific implementations, plus experiments on both image and text data, on more than 100 distinct opaque predicates. We show voodoo indexing can yield up to an 88% improvement over standard scan behavior, and a 79% improvement over the previous best method adapted from research literature.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1257–1272},
numpages = {16},
keywords = {opaque filter query, user-defined function (UDF) optimization, hierarchical multi-armed bandit algorithm},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389707,
author = {Duta, Christian and Grust, Torsten},
title = {Functional-Style SQL UDFs With a Capital 'F'},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389707},
doi = {10.1145/3318464.3389707},
abstract = {We advocate to express complex in-database computation using a functional style in which SQL UDFs use plain self-invocation to recurse. The resulting UDFs are concise and readable, but their run time performance on contemporary RDBMSs is sobering. This paper describes how to compile such functional-style UDFs into SQL:1999 recursive common table expressions. We build on function call graphs to build the compiler's core and to realize a series of optimizations (reference counting, memoization, exploitation of linear and tail recursion). The compiled UDFs evaluate efficiently, challenging the performance of manually tweaked (but often convoluted) SQL code. SQL UDFs can indeed be functional and fast.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1273–1287},
numpages = {15},
keywords = {memoization, user-defined functions, recursion, call graph, functional programming, SQL},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380604,
author = {Schelter, Sebastian and Rukat, Tammo and Biessmann, Felix},
title = {Learning to Validate the Predictions of Black Box Classifiers on Unseen Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380604},
doi = {10.1145/3318464.3380604},
abstract = {Machine Learning (ML) models are difficult to maintain in production settings. In particular, deviations of the unseen serving data (for which we want to compute predictions) from the source data (on which the model was trained) pose a central challenge, especially when model training and prediction are outsourced via cloud services. Errors or shifts in the serving data can affect the predictive quality of a model, but are hard to detect for engineers operating ML deployments.We propose a simple approach to automate the validation of deployed ML models by estimating the model's predictive performance on unseen, unlabeled serving data. In contrast to existing work, we do not require explicit distributional assumptions on the dataset shift between the source and serving data. Instead, we rely on a programmatic specification of typical cases of dataset shift and data errors. We use this information to learn a performance predictor for a pretrained black box model that automatically raises alarms when it detects performance drops on unseen serving data.We experimentally evaluate our approach on various datasets, models and error types. We find that it reliably predicts the performance of black box models in the majority of cases, and outperforms several baselines even in the presence of unspecified data errors.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1289–1299},
numpages = {11},
keywords = {model monitoring, shift detection, performance prediction},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389708,
author = {Picado, Jose and Davis, John and Termehchy, Arash and Lee, Ga Young},
title = {Learning Over Dirty Data Without Cleaning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389708},
doi = {10.1145/3318464.3389708},
abstract = {Real-world datasets are dirty and contain many errors, such as violations of integrity constraints and entity duplicates. Learning over dirty databases may result in inaccurate models. Data scientists spend most of their time on preparing and repairing data errors to create clean databases for learning. Moreover, as the information required to repair these errors is not often available, there may be numerous possible clean versions for a dirty database. We propose Dirty Learn, DLearn, a novel learning system that learns directly over dirty databases effectively and efficiently without any preprocessing. DLearn leverages database constraints to learn accurate relational models over inconsistent and heterogeneous data. Its learned models represent patterns over all possible clean versions of the data in a usable form. Our empirical study indicates that DLearn learns accurate models over large real-world databases efficiently.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1301–1316},
numpages = {16},
keywords = {data integration, relational learning, data cleaning, machine learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389696,
author = {Wu, Weiyuan and Flokas, Lampros and Wu, Eugene and Wang, Jiannan},
title = {Complaint-Driven Training Data Debugging for Query 2.0},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389696},
doi = {10.1145/3318464.3389696},
abstract = {As the need for machine learning (ML) increases rapidly across all industry sectors, there is a significant interest among commercial database providers to support "Query 2.0", which integrates model inference into SQL queries. Debugging Query 2.0 is very challenging since an unexpected query result may be caused by the bugs in training data (e.g., wrong labels, corrupted features). In response, we propose Rain, a complaint-driven training data debugging system. Rain allows users to specify complaints over the query's intermediate or final output, and aims to return a minimum set of training examples so that if they were removed, the complaints would be resolved. To the best of our knowledge, we are the first to study this problem. A naive solution requires retraining an exponential number of ML models. We propose two novel heuristic approaches based on influence functions which both require linear retraining steps. We provide an in-depth analytical and empirical analysis of the two approaches and conduct extensive experiments to evaluate their effectiveness using four real-world datasets. Results show that Rain achieves the highest recall@k among all the baselines while still returns results interactively.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1317–1334},
numpages = {18},
keywords = {data debugging, machine learning workflow debugging, software 2.0, machine learning explanation, data provenance},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389742,
author = {Cappuzzo, Riccardo and Papotti, Paolo and Thirumuruganathan, Saravanan},
title = {Creating Embeddings of Heterogeneous Relational Datasets for Data Integration Tasks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389742},
doi = {10.1145/3318464.3389742},
abstract = {Deep learning based techniques have been recently used with promising results for data integration problems. Some methods directly use pre-trained embeddings that were trained on a large corpus such as Wikipedia. However, they may not always be an appropriate choice for enterprise datasets with custom vocabulary. Other methods adapt techniques from natural language processing to obtain embeddings for the enterprise's relational data. However, this approach blindly treats a tuple as a sentence, thus losing a large amount of contextual information present in the tuple. We propose algorithms for obtaining local embeddings that are effective for data integration tasks on relational databases. We make four major contributions. First, we describe a compact graph-based representation that allows the specification of a rich set of relationships inherent in the relational world. Second, we propose how to derive sentences from such a graph that effectively "describe" the similarity across elements (tokens, attributes, rows) in the two datasets. The embeddings are learned based on such sentences. Third, we propose effective optimization to improve the quality of the learned embeddings and the performance of integration tasks. Finally, we propose a diverse collection of criteria to evaluate relational embeddings and perform an extensive set of experiments validating them against multiple baseline methods. Our experiments show that our framework, EmbDI, produces meaningful results for data integration tasks such as schema matching and entity resolution both in supervised and unsupervised settings.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1335–1349},
numpages = {15},
keywords = {data integration, entity resolution, deep learning, embeddings, schema matching},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389755,
author = {Gershtein, Shay and Milo, Tova and Morami, Gefen and Novgorodov, Slava},
title = {Minimization of Classifier Construction Cost for Search Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389755},
doi = {10.1145/3318464.3389755},
abstract = {Search over massive sets of items is the cornerstone of many modern applications. Users express a set of properties and expect the system to retrieve qualifying items. A common difficulty, however, is that the information on whether an item satisfies the search criteria is not explicitly recorded in the repository. Instead, it may be general knowledge or "hidden" in a picture/description, leading to incomplete search results. To overcome this problem, companies build dedicated classifiers that determine which items satisfy the given criteria. However, building classifiers requires volumes of high-quality labeled training data. Since the costs of training classifiers for different subsets of properties can vastly differ, the choice of which classifiers to train has great monetary significance. The goal of our research is to devise effective algorithms to choose which classifiers one should train to address a given query load while minimizing the cost. Previous work considered a simplified model with uniform classifier costs, and queries with two properties. We remove these restrictions in our model. We prove NP-hard inapproximability bounds and devise several algorithms with approximation guarantees. Moreover, we identify a common special case for which we provide an exact algorithm. Our experiments, performed over real-life datasets, demonstrate the effectiveness and efficiency of our algorithms.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1351–1365},
numpages = {15},
keywords = {classifiers, e-commerce, search queries, classifiers construction cost},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389748,
author = {Li, Wentao and Qiao, Miao and Qin, Lu and Zhang, Ying and Chang, Lijun and Lin, Xuemin},
title = {Scaling Up Distance Labeling on Graphs with Core-Periphery Properties},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389748},
doi = {10.1145/3318464.3389748},
abstract = {In indexing a graph for distance queries, distance labeling is a common practice; in particular, 2-hop labeling which guarantees the exactness of the query results is widely adopted. When it comes to a massive real graph with a relatively large treewidth such as social networks and web graphs, however, 2-hop labeling can hardly be constructed due to the oversized index. This paper discloses the theoretical relationships between the graph treewidth and 2-hop labeling's index size and query time. To scale up distance labeling, this paper proposes Core-Tree (CT) Index to facilitate a critical and effective trade-off between the index size and query time. The reduced index size enables CT-Index to handle massive graphs that no existing approaches can process while the cost in the query time is negligible: the query time is below 0.4 milliseconds on all tested graphs including one graph with 5.5 billion edges.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1367–1381},
numpages = {15},
keywords = {shortest distance, algorithm, tree decomposition, indexing, 2-hop labeling, treewidth},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380577,
author = {Kumar P., Krishna and Langton, Paul and Gatterbauer, Wolfgang},
title = {Factorized Graph Representations for Semi-Supervised Learning from Sparse Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380577},
doi = {10.1145/3318464.3380577},
abstract = {Node classification is an important problem in graph data management. It is commonly solved by various label propagation methods that iteratively pass messages along edges, starting from a few labeled seed nodes. For graphs with arbitrary compatibilities between classes, these methods crucially depend on knowing the compatibility matrix, which must thus be provided by either domain experts or heuristics. We instead suggest a principled and scalable method for directly estimating the compatibilities from a sparsely labeled graph. This method, which we call distant compatibility estimation, works even on extremely sparsely labeled graphs (e.g., 1 in 10,000 nodes is labeled) in a fraction of the time it later takes to label the remaining nodes. Our approach first creates multiple factorized graph representations (with size independent of the graph) and then performs estimation on these smaller graph sketches. We refer to algebraic amplification as the underlying idea of leveraging algebraic properties of an algorithm's update equations to amplify sparse signals in data. We show that our estimator is by orders of magnitude faster than alternative approaches and that the end-to-end classification accuracy is comparable to using gold standard compatibilities. This makes it a cheap pre-processing step for any existing label propagation method and removes the current dependence on heuristics.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1383–1398},
numpages = {16},
keywords = {iterative classification methods, random walks with restarts, graph data, sparse data, linear algebra optimization, inference, belief propagation, compatibility estimation, non-backtracking walks, semi-supervised learning, label propagation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389706,
author = {Zhang, Wentao and Miao, Xupeng and Shao, Yingxia and Jiang, Jiawei and Chen, Lei and Ruas, Olivier and Cui, Bin},
title = {Reliable Data Distillation on Graph Convolutional Network},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389706},
doi = {10.1145/3318464.3389706},
abstract = {Graph Convolutional Network (GCN) is a widely used method for learning from graph-based data. However, it fails to use the unlabeled data to its full potential, thereby hindering its ability. Given some pseudo labels of the unlabeled data, the GCN can benefit from this extra supervision. Based on Knowledge Distillation and Ensemble Learning, lots of methods use a teacher-student architecture to make better use of the unlabeled data and then make a better prediction. However, these methods introduce unnecessary training costs and a high bias of student model if the teacher's predictions are unreliable. Besides, the final ensemble gains are limited due to limited diversity in the combined models. Therefore, we propose Reliable Data Distillation, a reliable data driven semi-supervised GCN training method. By defining the node reliability and edge reliability in a graph, we can make better use of high quality data and improve the graph representation learning. Furthermore, considering the data reliability and data importance, we propose a new ensemble learning method for GCN and a novel Self-Boosting SSL Framework to combine the above optimizations. Finally, our extensive evaluation of Reliable Data Distillation on real-world datasets shows that our approach outperforms the state-of-the-art methods on semi-supervised node classification tasks.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1399–1414},
numpages = {16},
keywords = {semi-supervised learning, ensemble learning, knowledge distillation, graph convolutional network},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389733,
author = {Pacaci, Anil and Bonifati, Angela and \"{O}zsu, M. Tamer},
title = {Regular Path Query Evaluation on Streaming Graphs},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389733},
doi = {10.1145/3318464.3389733},
abstract = {We study persistent query evaluation over streaming graphs, which is becoming increasingly important. We focus on navigational queries that determine if there exists a path between two entities that satisfies a user-specified constraint. We adopt the Regular Path Query (RPQ) model that specifies navigational patterns with labeled constraints. We propose deterministic algorithms to efficiently evaluate persistent RPQs under both arbitrary and simple path semantics in a uniform manner. Experimental analysis on real and synthetic streaming graphs shows that the proposed algorithms can process up to tens of thousands of edges per second and efficiently answer RPQs that are commonly used in real-world workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1415–1430},
numpages = {16},
keywords = {regular path queries, streaming graphs, persistent query evaluation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380598,
author = {Pandey, Prashant and Singh, Shikha and Bender, Michael A. and Berry, Jonathan W. and Farach-Colton, Mart\'{\i}n and Johnson, Rob and Kroeger, Thomas M. and Phillips, Cynthia A.},
title = {Timely Reporting of Heavy Hitters Using External Memory},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380598},
doi = {10.1145/3318464.3380598},
abstract = {Given an input stream of size N, a φ-heavy hitter is an item that occurs at least φ N times in S. The problem of finding heavy-hitters is extensively studied in the database literature. We study a real-time heavy-hitters variant in which an element must be reported shortly after we see its T = φ N-th occurrence (and hence becomes a heavy hitter). We call this the Timely Event Detection (TED) Problem. The TED problem models the needs of many real-world monitoring systems, which demand accurate (i.e., no false negatives) and timely reporting of all events from large, high-speed streams, and with a low reporting threshold (high sensitivity). Like the classic heavy-hitters problem, solving the TED problem without false-positives requires large space (Ω(N) words). Thus in-RAM heavy-hitters algorithms typically sacrifice accuracy (i.e., allow false positives), sensitivity, or timeliness (i.e., use multiple passes). We show how to adapt heavy-hitters algorithms to external memory to solve the TED problem on large high-speed streams while guaranteeing accuracy, sensitivity, and timeliness. Our data structures are limited only by I/O-bandwidth (not latency) and support a tunable trade-off between reporting delay and I/O overhead. With a small bounded reporting delay, our algorithms incur only a logarithmic I/O overhead. We implement and validate our data structures empirically using the Firehose streaming benchmark. Multi-threaded versions of our structures can scale to process 11M observations per second before becoming CPU bound. In comparison, a naive adaptation of the standard heavy-hitters algorithm to external memory would be limited by the storage device's random I/O throughput, i.e., ~100K observations per second.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1431–1446},
numpages = {16},
keywords = {dictionary data structure, external-memory algorithms, streaming algorithms},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386128,
author = {Soliman, Mohamed A. and Antova, Lyublena and Sugiyama, Marc and Duller, Michael and Aleyasen, Amirhossein and Mitra, Gourab and Abdelhamid, Ehab and Morcos, Mark and Gage, Michele and Korablev, Dmitri and Waas, Florian M.},
title = {A Framework for Emulating Database Operations in Cloud Data Warehouses},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386128},
doi = {10.1145/3318464.3386128},
abstract = {In recent years, increased interest in cloud-based data warehousing technologies has emerged with many enterprises moving away from on-premise data warehousing solutions. The incentives for adopting cloud data warehousing technologies are many: cost-cutting, on-demand pricing, offloading data centers, unlimited hardware resources, built-in disaster recovery, to name a few. There is inherent difference in the language surface and feature sets of on-premise and cloud data warehousing solutions. This could range from subtle syntactic and semantic differences, with potentially big impact on result correctness, to complete features that exist in one system but are missing in other systems. While there have been some efforts to help automate the migration of on-premise applications to new cloud environments, a major challenge that slows down the migration pace is the handling of features not yet supported, or partially supported, by the cloud technologies. In this paper we build on our earlier work in adaptive data virtualization and present novel techniques that allow running applications utilizing sophisticated database features within foreign query engines lacking the native support of such features. In particular, we introduce a framework to manage discrepancy of metadata across heterogeneous query engines, and various mechanisms to emulate database applications code in cloud environments without any need to rewrite or change the application code.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1447–1461},
numpages = {15},
keywords = {query processing, data warehousing, query rewriting, database migration, database emulation, cloud data warehousing, metadata management},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386129,
author = {Depoutovitch, Alex and Chen, Chong and Chen, Jin and Larson, Paul and Lin, Shu and Ng, Jack and Cui, Wenlin and Liu, Qiang and Huang, Wei and Xiao, Yong and He, Yongjun},
title = {Taurus Database: How to Be Fast, Available, and Frugal in the Cloud},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386129},
doi = {10.1145/3318464.3386129},
abstract = {Using cloud Database as a Service (DBaaS) offerings instead of on-premise deployments is increasingly common. Key advantages include improved availability and scalability at a lower cost than on-premise alternatives. In this paper, we describe the design of Taurus, a new multi-tenant cloud database system. Taurus separates the compute and storage layers in a similar manner to Amazon Aurora and Microsoft Socrates and provides similar benefits, such as read replica support, low network utilization, hardware sharing and scalability. However, the Taurus architecture has several unique advantages. Taurus offers novel replication and recovery algorithms providing better availability than existing approaches using the same or fewer replicas. Also, Taurus is highly optimized for performance, using no more than one network hop on critical paths and exclusively using append-only storage, delivering faster writes, reduced device wear, and constant-time snapshots. This paper describes Taurus and provides a detailed description and analysis of the storage node architecture, which has not been previously available from the published literature.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1463–1478},
numpages = {16},
keywords = {cloud, architecture, availability, databases, reliability},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386130,
author = {Demarne, Mathieu B. and Gramling, Jim and Verona, Tomer and Cilimdzic, Miso},
title = {Reliability Analytics for Cloud Based Distributed Databases},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386130},
doi = {10.1145/3318464.3386130},
abstract = {We present RADD, an innovative analytic pipeline used to measure reliability and availability for cloud-based distributed databases by leveraging the vast amount of telemetry present in the cloud. RADD can perform root cause analysis (RCA) to provide a minute-by-minute summary of the availability of a database close to real-time. On top of this data, RADD can raise alerts, analyze the stability of new versions during their deployment, and provide Key Performance Indicators (KPIs) that allow us to understand the stability of our system across all deployed databases. RADD implements an event correlation framework that puts the emphasis on data compliance and uses information entropy to measure causality and reduce noisy signals. It also uses statistical modelling to analyze new versions of the product and detect potential regressions early in our software development lifecycle. We demonstrate the application of RADD on top of Azure Synapse Analytics, where the system has helped us identify top-hitting and new issues and support on-call teams regarding every aspect of database health.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1479–1492},
numpages = {14},
keywords = {azure synapse analytics, distributed databases, telemetry analysis, reliability analysis, azure SQL data warehouse},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386134,
author = {Taft, Rebecca and Sharif, Irfan and Matei, Andrei and VanBenschoten, Nathan and Lewis, Jordan and Grieger, Tobias and Niemi, Kai and Woods, Andy and Birzin, Anne and Poss, Raphael and Bardea, Paul and Ranade, Amruta and Darnell, Ben and Gruneir, Bram and Jaffray, Justin and Zhang, Lucy and Mattis, Peter},
title = {CockroachDB: The Resilient Geo-Distributed SQL Database},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386134},
doi = {10.1145/3318464.3386134},
abstract = {We live in an increasingly interconnected world, with many organizations operating across countries or even continents. To serve their global user base, organizations are replacing their legacy DBMSs with cloud-based systems capable of scaling OLTP workloads to millions of users. CockroachDB is a scalable SQL DBMS that was built from the ground up to support these global OLTP workloads while maintaining high availability and strong consistency. Just like its namesake, CockroachDB is resilient to disasters through replication and automatic recovery mechanisms. This paper presents the design of CockroachDB and its novel transaction model that supports consistent geo-distributed transactions on commodity hardware. We describe how CockroachDB replicates and distributes data to achieve fault tolerance and high performance, as well as how its distributed SQL layer automatically scales with the size of the database cluster while providing the standard SQL interface that users expect. Finally, we present a comprehensive performance evaluation and share a couple of case studies of CockroachDB users. We conclude by describing lessons learned while building CockroachDB over the last five years.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1493–1509},
numpages = {17},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386141,
author = {Antonopoulos, Panagiotis and Arasu, Arvind and Singh, Kunal D. and Eguro, Ken and Gupta, Nitish and Jain, Rajat and Kaushik, Raghav and Kodavalla, Hanuma and Kossmann, Donald and Ogg, Nikolas and Ramamurthy, Ravi and Szymaszek, Jakub and Trimmer, Jeffrey and Vaswani, Kapil and Venkatesan, Ramarathnam and Zwilling, Mike},
title = {Azure SQL Database Always Encrypted},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386141},
doi = {10.1145/3318464.3386141},
abstract = {This paper presents Always Encrypted, a recently released feature of Microsoft SQL Server that uses column granularity encryption to provide cryptographic data protection guarantees. Always Encrypted can be used to outsource database administration while keeping the data confidential from an administrator, including cloud operators. The first version of Always Encrypted was released in Azure SQL Database and as part of SQL Server 2016, and supported equality operations over deterministically encrypted columns. The second version, released as part of SQL Server 2019, uses an enclave running within a trusted execution environment to provide richer functionality that includes comparison and string pattern matching for an IND-CPA-secure (randomized) encryption scheme. We present the security, functionality, and design of Always Encrypted, and provide a performance evaluation using the TPC-C benchmark.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1511–1525},
numpages = {15},
keywords = {SQL server, always encrypted, information leakage, enclave, encryption},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389779,
author = {Bar El, Ori and Milo, Tova and Somech, Amit},
title = {Automatically Generating Data Exploration Sessions Using Deep Reinforcement Learning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389779},
doi = {10.1145/3318464.3389779},
abstract = {Exploratory Data Analysis (EDA) is an essential yet highly demanding task. To get a head start before exploring a new dataset, data scientists often prefer to view existing EDA notebooks -- illustrative, curated exploratory sessions, on the same dataset, that were created by fellow data scientists who shared them online. Unfortunately, such notebooks are not always available (e.g., if the dataset is new or confidential). To address this, we present ATENA, a system that takes an input dataset and auto-generates a compelling exploratory session, presented in an EDA notebook. We shape EDA into a control problem, and devise a novel Deep Reinforcement Learning (DRL) architecture to effectively optimize the notebook generation. Though ATENA uses a limited set of EDA operations, our experiments show that it generates useful EDA notebooks, allowing users to gain actual insights.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1527–1537},
numpages = {11},
keywords = {auto generated, data exploration, EDA, notebooks, auto EDA, autogenerated, interactive data analysis, EDA notebooks},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389738,
author = {Yan, Cong and He, Yeye},
title = {Auto-Suggest: Learning-to-Recommend Data Preparation Steps Using Data Science Notebooks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389738},
doi = {10.1145/3318464.3389738},
abstract = {Data preparation is widely recognized as the most time-consuming process in modern business intelligence (BI) and machine learning (ML) projects. Automating complex data preparation steps (e.g., Pivot, Unpivot, Normalize-JSON, etc.)holds the potential to greatly improve user productivity, and has therefore become a central focus of research. We propose a novel approach to "auto-suggest" contextualized data preparation steps, by "learning" from how data scientists would manipulate data, which are documented by data science notebooks widely available today. Specifically, we crawled over 4M Jupyter notebooks on GitHub, and replayed them step-by-step, to observe not only full input/output tables (data-frames) at each step, but also the exact data-preparation choices data scientists make that they believe are best suited to the input data (e.g., how input tables are Joined/Pivoted/Unpivoted, etc.). By essentially "logging" how data scientists interact with diverse tables, and using the resulting logs as a proxy of "ground truth", we can learn-to-recommend data preparation steps best suited to given user data, just like how search engines (Google or Bing) leverage their click-through logs to learn-to-rank documents. This data-driven and log-driven approach leverages the "collective wisdom" of data scientists embodied in the notebooks, and is shown to significantly outperform strong baselines including commercial systems in terms of accuracy.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1539–1554},
numpages = {16},
keywords = {learning-to-recommend, data wrangling, pivot and unpivot, data preparation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380574,
author = {Eichmann, Philipp and Zgraggen, Emanuel and Binnig, Carsten and Kraska, Tim},
title = {IDEBench: A Benchmark for Interactive Data Exploration},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380574},
doi = {10.1145/3318464.3380574},
abstract = {In recent years, many query processing techniques have been developed to better support interactive data exploration (IDE) of large structured datasets. To evaluate and compare database engines in terms of how well they support such workloads, experimenters have mostly used self-designed evaluation procedures rather than established benchmarks. In this paper we argue that this is due to the fact that the workloads and metrics of popular analytical benchmarks such as TPC-H or TPC-DS were designed for traditional performance reporting scenarios, and do not capture distinctive IDE characteristics. Guided by the findings of several user studies we present a new benchmark called IDEBench, designed to evaluate database engines based on common IDE workflows and metrics that matter to the end-user. We demonstrate the applicability of IDEBench through a number of experiments with five different database engines, and present and discuss our findings.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1555–1569},
numpages = {15},
keywords = {visual analytics, database benchmark, interactive data exploration},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389732,
author = {Battle, Leilani and Eichmann, Philipp and Angelini, Marco and Catarci, Tiziana and Santucci, Giuseppe and Zheng, Yukun and Binnig, Carsten and Fekete, Jean-Daniel and Moritz, Dominik},
title = {Database Benchmarking for Supporting Real-Time Interactive Querying of Large Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389732},
doi = {10.1145/3318464.3389732},
abstract = {In this paper, we present a new benchmark to validate the suitability of database systems for interactive visualization workloads. While there exist proposals for evaluating database systems on interactive data exploration workloads, none rely on real user traces for database benchmarking. To this end, our long term goal is to collect user traces that represent workloads with different exploration characteristics. In this paper, we present an initial benchmark that focuses on "crossfilter"-style applications, which are a popular interaction type for data exploration and a particularly demanding scenario for testing database system performance. We make our benchmark materials, including input datasets, interaction sequences, corresponding SQL queries, and analysis code, freely available as a community resource, to foster further research in this area: https://osf.io/9xerb/?view_only=81de1a3f99d04529b6b173a3bd5b4d23.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1571–1587},
numpages = {17},
keywords = {visualization performance benchmarks, interactive data exploration},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389782,
author = {Rahman, Sajjadur and Mack, Kelly and Bendre, Mangesh and Zhang, Ruilin and Karahalios, Karrie and Parameswaran, Aditya},
title = {Benchmarking Spreadsheet Systems},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389782},
doi = {10.1145/3318464.3389782},
abstract = {Spreadsheet systems are used for storing and analyzing data across domains by programmers and non-programmers alike.While spreadsheet systems have continued to support increasingly large datasets, they are prone to hanging and freezing while performing computations even on much smaller ones. We present a benchmarking study that evaluates and compares the performance of three popular systems, Microsoft Excel, LibreOffice Calc, and Google Sheets, on a range of canonical spreadsheet computation operations. We find that spreadsheet systems lack interactivity for several operations, on datasets well below their advertised scalability limits. We further evaluate whether spreadsheet systems adopt database optimization techniques such as indexing, intelligent data layout, and incremental and shared computation,to efficiently execute computation operations. We outline several ways future spreadsheet systems can be redesigned to offer interactive response times on large datasets.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1589–1599},
numpages = {11},
keywords = {use cases, spreadsheet systems, scalability},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380583,
author = {Zhang, Huanchen and Liu, Xiaoxuan and Andersen, David G. and Kaminsky, Michael and Keeton, Kimberly and Pavlo, Andrew},
title = {Order-Preserving Key Compression for In-Memory Search Trees},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380583},
doi = {10.1145/3318464.3380583},
abstract = {We present the High-speed Order-Preserving Encoder (HOPE) for in-memory search trees. HOPE is a fast dictionary-based compressor that encodes arbitrary keys while preserving their order. HOPE's approach is to identify common key patterns at a fine granularity and exploit the entropy to achieve high compression rates with a small dictionary. we first develop a theoretical model to reason about order-preserving dictionary designs. We then select six representative compression schemes using this model and implement them in HOPE. These schemes make different trade-offs between compression rate and encoding speed. We evaluate HOPE on five data structures used in databases: SuRF, ART, HOT, B+tree, and Prefix B+tree. Our experiments show that using HOPE allows the search trees to achieve lower query latency (up to 40% lower) and better memory efficiency (up to 30% smaller) simultaneously for most string key workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1601–1615},
numpages = {15},
keywords = {string-axis model, entropy, order-preserving, key compression, in-memory search trees},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380595,
author = {Shanbhag, Anil and Madden, Samuel and Yu, Xiangyao},
title = {A Study of the Fundamental Performance Characteristics of GPUs and CPUs for Database Analytics},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380595},
doi = {10.1145/3318464.3380595},
abstract = {There has been significant amount of excitement and recent work on GPU-based database systems. Previous work has claimed that these systems can perform orders of magnitude better than CPU-based database systems on analytical workloads such as those found in decision support and business intelligence applications. A hardware expert would view these claims with suspicion. Given the general notion that database operators are memory-bandwidth bound, one would expect the maximum gain to be roughly equal to the ratio of the memory bandwidth of GPU to that of CPU. In this paper, we adopt a model-based approach to understand when and why the performance gains of running queries on GPUs vs on CPUs vary from the bandwidth ratio (which is roughly 16\texttimes{} on modern hardware). We propose Crystal, a library of parallel routines that can be combined together to run full SQL queries on a GPU with minimal materialization overhead. We implement individual query operators to show that while the speedups for selection, projection, and sorts are near the bandwidth ratio, joins achieve less speedup due to differences in hardware capabilities. Interestingly, we show on a popular analytical workload that full query performance gain from running on GPU exceeds the bandwidth ratio despite individual operators having speedup less than bandwidth ratio, as a result of limitations of vectorizing chained operators on CPUs, resulting in a 25\texttimes{} speedup for GPUs over CPUs on the benchmark.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1617–1632},
numpages = {16},
keywords = {in-memory analytics, GPU data analytics, heterogenous systems, GPU query performance},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389705,
author = {Lutz, Clemens and Bre\ss{}, Sebastian and Zeuch, Steffen and Rabl, Tilmann and Markl, Volker},
title = {Pump Up the Volume: Processing Large Data on GPUs with Fast Interconnects},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389705},
doi = {10.1145/3318464.3389705},
abstract = {GPUs have long been discussed as accelerators for database query processing because of their high processing power and memory bandwidth. However, two main challenges limit the utility of GPUs for large-scale data processing: (1) the on-board memory capacity is too small to store large data sets, yet (2) the interconnect bandwidth to CPU main-memory is insufficient for ad hoc data transfers. As a result, GPU-based systems and algorithms run into a transfer bottleneck and do not scale to large data sets. In practice, CPUs process large-scale data faster than GPUs with current technology. In this paper, we investigate how a fast interconnect can resolve these scalability limitations using the example of NVLink 2.0. NVLink 2.0 is a new interconnect technology that links dedicated GPUs to a CPU@. The high bandwidth of NVLink 2.0 enables us to overcome the transfer bottleneck and to efficiently process large data sets stored in main-memory on GPUs. We perform an in-depth analysis of NVLink 2.0 and show how we can scale a no-partitioning hash join beyond the limits of GPU memory. Our evaluation shows speed-ups of up to 18x over PCI-e 3.0 and up to 7.3x over an optimized CPU implementation. Fast GPU interconnects thus enable GPUs to efficiently accelerate query processing.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1633–1649},
numpages = {17},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389725,
author = {Bang, Tiemo and Oukid, Ismail and May, Norman and Petrov, Ilia and Binnig, Carsten},
title = {Robust Performance of Main Memory Data Structures by Configuration},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389725},
doi = {10.1145/3318464.3389725},
abstract = {In this paper, we present a new approach for achieving robust performance of data structures making it easier to reuse the same design for different hardware generations but also for different workloads. To achieve robust performance, the main idea is to strictly separate the data structure design from the actual strategies to execute access operations and adjust the actual execution strategies by means of so-called configurations instead of hard-wiring the execution strategy into the data structure. In our evaluation we demonstrate the benefits of this configuration approach for individual data structures as well as complex OLTP workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1651–1666},
numpages = {16},
keywords = {delegation, data structure configuration, hardware independent design, robust scalability, data structure optimisation, robust performance},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380591,
author = {Kunjir, Mayuresh and Babu, Shivnath},
title = {Black or White? How to Develop an AutoTuner for Memory-Based Analytics},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380591},
doi = {10.1145/3318464.3380591},
abstract = {There is a lot of interest today in building autonomous (or, self-driving) data processing systems. An emerging school of thought is to leverage AI-driven "black box" algorithms for this purpose. In this paper, we present a contrarian view. We study the problem of autotuning the memory allocation for applications running on modern distributed data processing systems. We show that an empirically-driven "white-box" algorithm, called RelM, that we have developed provides a close-to-optimal tuning at a fraction of the overheads compared to state-of-the-art AI-driven "black box" algorithms, namely, Bayesian Optimization (BO) and Deep Distributed Policy Gradient (DDPG). The main reason for RelM's superior performance is that the memory management in modern memory-based data analytics systems is an interplay of algorithms at multiple levels: (i) at the resource-management level across various containers allocated by resource managers like Kubernetes and YARN, (ii) at the container level among the OS, pods, and processes such as the Java Virtual Machine (JVM), (iii) at the application level for caching, aggregation, data shuffles, and application data structures, and (iv) at the JVM level across various pools such as the Young and Old Generation. RelM understands these interactions and uses them in building an analytical solution to autotune the memory management knobs. In another contribution, called Guided-BO (GBO), we use RelM's analytical models to speed up BO. Through an evaluation based on Apache Spark, we showcase that the RelM's recommendations are significantly better than what commonly-used Spark deployments provide, and are close to the ones obtained by brute-force exploration; while GBO provides optimality guarantees for a higher, but still significantly lower cost overhead compared to the state-of-the-art AI-driven policies.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1667–1683},
numpages = {17},
keywords = {memory management, automated configuration tuning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389709,
author = {Nakandala, Supun and Kumar, Arun},
title = {Vista: Optimized System for Declarative Feature Transfer from Deep CNNs at Scale},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389709},
doi = {10.1145/3318464.3389709},
abstract = {Scalable systems for machine learning (ML) are largely siloed into dataflow systems for structured data and deep learning systems for unstructured data. This gap has left workloads that jointly analyze both forms of data with poor systems support, leading to both low system efficiency and grunt work for users. We bridge this gap for an important class of such workloads: feature transfer from deep convolutional neural networks (CNNs) for analyzing images along with structured data. Executing feature transfer on scalable dataflow and deep learning systems today faces two key systems issues: inefficiency due to redundant computations and crash-proneness due to mismanaged memory. We present Vista, a new data system that resolves these issues by elevating this workload to a declarative level on top of dataflow and deep learning systems. Vista automatically optimizes the configuration and execution of this workload to reduce both computational redundancy and the potential for workload crashes. Experiments on real datasets show that apart from making feature transfer easier, Vista avoids workload crashes and reduces runtimes by 58% to 92% compared to baselines.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1685–1700},
numpages = {16},
keywords = {systems for machine learning, materialization optimization, multi-query optimization, convolution neural networks, multimodal analytics},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389715,
author = {Derakhshan, Behrouz and Rezaei Mahdiraji, Alireza and Abedjan, Ziawasch and Rabl, Tilmann and Markl, Volker},
title = {Optimizing Machine Learning Workloads in Collaborative Environments},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389715},
doi = {10.1145/3318464.3389715},
abstract = {Effective collaboration among data scientists results in high-quality and efficient machine learning (ML) workloads. In a collaborative environment, such as Kaggle or Google Colabratory, users typically re-execute or modify published scripts to recreate or improve the result. This introduces many redundant data processing and model training operations. Reusing the data generated by the redundant operations leads to the more efficient execution of future workloads. However, existing collaborative environments lack a data management component for storing and reusing the result of previously executed operations. In this paper, we present a system to optimize the execution of ML workloads in collaborative environments by reusing previously performed operations and their results. We utilize a so-called Experiment Graph (EG) to store the artifacts, i.e., raw and intermediate data or ML models, as vertices and operations of ML workloads as edges. In theory, the size of EG can become unnecessarily large, while the storage budget might be limited. At the same time, for some artifacts, the overall storage and retrieval cost might outweigh the recomputation cost. To address this issue, we propose two algorithms for materializing artifacts based on their likelihood of future reuse. Given the materialized artifacts inside EG, we devise a linear-time reuse algorithm to find the optimal execution plan for incoming ML workloads. Our reuse algorithm only incurs a negligible overhead and scales for the high number of incoming ML workloads in collaborative environments. Our experiments show that we improve the run-time by one order of magnitude for repeated execution of the workloads and 50% for the execution of modified workloads in collaborative environments.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1701–1716},
numpages = {16},
keywords = {materialization and reuse, collaborative ML, machine learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380592,
author = {Das, Nilaksh and Chaba, Sanya and Wu, Renzhi and Gandhi, Sakshi and Chau, Duen Horng and Chu, Xu},
title = {GOGGLES: Automatic Image Labeling with Affinity Coding},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380592},
doi = {10.1145/3318464.3380592},
abstract = {Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels). We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set. We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1717–1732},
numpages = {16},
keywords = {affinity coding, probabilistic labels, image labeling, computer vision, weak supervision, data programming},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389734,
author = {Ilkhechi, Amir and Crotty, Andrew and Galakatos, Alex and Mao, Yicong and Fan, Grace and Shi, Xiran and Cetintemel, Ugur},
title = {DeepSqueeze: Deep Semantic Compression for Tabular Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389734},
doi = {10.1145/3318464.3389734},
abstract = {With the rapid proliferation of large datasets, efficient data compression has become more important than ever. Columnar compression techniques (e.g., dictionary encoding, run-length encoding, delta encoding) have proved highly effective for tabular data, but they typically compress individual columns without considering potential relationships among columns, such as functional dependencies and correlations. Semantic compression techniques, on the other hand, are designed to leverage such relationships to store only a subset of the columns necessary to infer the others, but existing approaches cannot effectively identify complex relationships across more than a few columns at a time. We propose DeepSqueeze, a novel semantic compression framework that can efficiently capture these complex relationships within tabular data by using autoencoders to map tuples to a lower-dimensional representation. DeepSqueeze also supports guaranteed error bounds for lossy compression of numerical data and works in conjunction with common columnar compression formats. Our experimental evaluation uses real-world datasets to demonstrate that DeepSqueeze can achieve over a 4x size reduction compared to state-of-the-art alternatives.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1733–1746},
numpages = {14},
keywords = {semantic compression, data compression},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389720,
author = {Zheng, Kaiping and Cai, Shaofeng and Chua, Horng Ruey and Wang, Wei and Ngiam, Kee Yuan and Ooi, Beng Chin},
title = {TRACER: A Framework for Facilitating Accurate and Interpretable Analytics for High Stakes Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389720},
doi = {10.1145/3318464.3389720},
abstract = {In high stakes applications such as healthcare and finance analytics, the interpretability of predictive models is required and necessary for domain practitioners to trust the predictions. Traditional machine learning models, e.g., logistic regression (LR), are easy to interpret in nature. However, many of these models aggregate time-series data without considering the temporal correlations and variations. Therefore, their performance cannot match up to recurrent neural network (RNN) based models, which are nonetheless difficult to interpret. In this paper, we propose a general framework TRACER to facilitate accurate and interpretable predictions, with a novel model TITV devised for healthcare analytics and other high stakes applications such as financial investment and risk management. Different from LR and other existing RNN-based models, TITV is designed to capture both the time-invariant and the time-variant feature importance using a feature-wise transformation subnetwork and a self-attention subnetwork, for the feature influence shared over the entire time series and the time-related importance respectively. Healthcare analytics is adopted as a driving use case, and we note that the proposed TRACER is also applicable to other domains, e.g., fintech. We evaluate the accuracy of TRACER extensively in two real-world hospital datasets, and our doctors/clinicians further validate the interpretability of TRACER in both the patient level and the feature level. Besides, TRACER is also validated in a critical financial application. The experimental results confirm that TRACER facilitates both accurate and interpretable analytics for high stakes applications.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1747–1763},
numpages = {17},
keywords = {healthcare analytics, interpretation, feature importance, high stakes applications},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389745,
author = {Fan, Wenfei and Jin, Ruochun and Liu, Muyang and Lu, Ping and Luo, Xiaojian and Xu, Ruiqi and Yin, Qiang and Yu, Wenyuan and Zhou, Jingren},
title = {Application Driven Graph Partitioning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389745},
doi = {10.1145/3318464.3389745},
abstract = {Graph partitioning is crucial to parallel computations on large graphs. The choice of partitioning strategies has strong impact on not only the performance of graph algorithms, but also the design of the algorithms. For an algorithm of our interest, what partitioning strategy fits it the best and improves its parallel execution? Is it possible to develop graph algorithms with partition transparency, such that the algorithms work under different partitions without changes? This paper aims to answer these questions. We propose an application-driven hybrid partitioning strategy that, given a graph algorithm A, learns a cost model for A as polynomial regression. We develop partitioners that given the learned cost model, refine an edge-cut or vertex-cut partition to a hybrid partition and reduce the parallel cost of A. Moreover, we identify a general condition under which graph-centric algorithms are partition transparent. We show that a number of graph algorithms can be made partition transparent. Using real-life and synthetic graphs, we experimentally verify that our partitioning strategy improves the performance of a variety of graph computations, up to 22.5 times.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1765–1779},
numpages = {15},
keywords = {machine learning, partition transparency, graph partition},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389746,
author = {Ouyang, Dian and Wen, Dong and Qin, Lu and Chang, Lijun and Zhang, Ying and Lin, Xuemin},
title = {Progressive Top-K Nearest Neighbors Search in Large Road Networks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389746},
doi = {10.1145/3318464.3389746},
abstract = {Computing top-k nearest neighbors (kNN) is a fundamental problem in road networks. Existing solutions either need a complicated parameter configuration in index construction or incur high costs when scanning an unbounded number of vertices in query processing. In this paper, we propose a novel parameter-free index-based solution for the kNN query based on the concept of tree decomposition in large road networks. Based on our index structure, we propose an efficient and progressive algorithm that returns each result in a bounded delay. We also optimize the index structure, which improves the efficiency of both index construction and index maintenance in large road networks. We conduct extensive experiments to show the efficiency of our proposed algorithms and the effectiveness of our optimization techniques in real-world road networks from ten regions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1781–1795},
numpages = {15},
keywords = {tree decomposition, road network, KNN},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380562,
author = {Shao, Yingxia and Huang, Shiyue and Miao, Xupeng and Cui, Bin and Chen, Lei},
title = {Memory-Aware Framework for Efficient Second-Order Random Walk on Large Graphs},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380562},
doi = {10.1145/3318464.3380562},
abstract = {Second-order random walk is an important technique for graph analysis. Many applications use it to capture higher-order patterns in the graph, thus improving the model accuracy. However, the memory explosion problem of this technique hinders it from analyzing large graphs. When processing a billion-edge graph like Twitter, existing solutions (e.g., alias method) of the second-order random walk may take up 1796TB memory. Such high memory overhead comes from the memory-unaware strategies for node sampling across the graph. In this paper, to clearly study the efficiency of various node sampling methods in the context of second-order random walk, we design a cost model, and then propose a new node sampling method following the acceptance-rejection paradigm to achieve a better balance between memory and time cost. Further, to guarantee the efficiency of the second-order random walk within arbitrary memory budgets, we propose a memory-aware framework on the basis of the cost model. The framework applies a cost-based optimizer to assign desirable node sampling method for each node in the graph within a memory budget while minimizing the time cost. Finally, we provide general programming interfaces for users to benefit from the memory-aware framework easily. The empirical studies demonstrate that our memory-aware framework is robust with respect to memory and is able to achieve considerable efficiency by reducing 90% of the memory cost.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1797–1812},
numpages = {16},
keywords = {graph algorithm, random walk, memory efficient, large-scale},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389737,
author = {Zhang, Yikai and Yu, Jeffrey Xu},
title = {Hub Labeling for Shortest Path Counting},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389737},
doi = {10.1145/3318464.3389737},
abstract = {The notion of shortest path is fundamental in graph analytics. While many works have devoted to devising efficient distance oracles to compute the shortest distance between any vertices s and t, we study the problem of efficiently counting the number of shortest paths between s and t in light of its applications in tasks such as betweenness-related analysis. Specifically, we propose a hub labeling scheme based on hub pushing and discuss several graph reduction techniques to reduce the index size. Furthermore, we prove several theoretical results on the performance of the scheme for some special graph classes. Our empirical study verifies the efficiency and effectiveness of the algorithms. In particular, a query evaluation takes only hundreds of microseconds in average for graphs with up to hundreds of millions of edges. We report our findings in this paper.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1813–1828},
numpages = {16},
keywords = {shortest path, hub labeling, counting, algorithms},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389780,
author = {Li, Hui and Li, Hui and Bhowmick, Sourav S.},
title = {CHASSIS: Conformity Meets Online Information Diffusion},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389780},
doi = {10.1145/3318464.3389780},
abstract = {Online information diffusion generates huge volumes of social activities (eg. tweets, retweets posts, comments, likes) among individuals. Existing information diffusion modeling techniques are oblivious to conformity of individuals during the diffusion process, a fundamental human trait according to social psychology theories. Intuitively, conformity captures the extent to which an individual complies with social norms or expectations. In this paper, we present a novel framework called chassis to characterize online information diffusion by bridging classical information diffusion model with conformity from social psychology. To this end, we first extend "Hawkes Process", a well-known statistical technique utilized to model information diffusion, to quantitatively capture two flavors of conformity, informational conformity and normative conformity, hidden in activity sequences. Next, we present a novel semi-parametric inference approach to learn the proposed model. Experimental study with real-world datasets demonstrates the superiority of chassis to state-of-the-art conformity-unaware information diffusion models.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1829–1840},
numpages = {12},
keywords = {social psychology, online social networks, information diffusion, model inference, hawkes processes, conformity},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389718,
author = {Wei, Victor Junqiu and Wong, Raymond Chi-Wing and Long, Cheng},
title = {Architecture-Intact Oracle for Fastest Path and Time Queries on Dynamic Spatial Networks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389718},
doi = {10.1145/3318464.3389718},
abstract = {Given two vertices of interest (POIs) s and t on a spatial network, a distance (path) query returns the shortest network distance (shortest path) from s to t. This query has a variety of applications in practice and is a fundamental operation for many database and data mining algorithms. In this paper, we propose an efficient distance and path oracle on dynamic road networks using the randomization technique. Our oracle has a good performance in practice and remarkably, and at the same time, it has a favorable theoretical bound. Specifically, it has O(n log2 n) (resp. O(n log2n)) preprocessing time (resp. space) and O(log4n log log n) (resp. O(log4n log log n+l)) distance query time (resp. shortest path query time) as well as O(log3n) update time with high probability (w.h.p.), where n is the number of vertices in the spatial network and l is the number of edges on the shortest path. Our experiments show that the existing oracles suffer from a huge updating time that renders them impractical and our oracle enjoys a negligible updating time and meanwhile has comparable query time and indexing cost with the best existing oracle.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1841–1856},
numpages = {16},
keywords = {dynamic road networks, shortest distance},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389751,
author = {Gogolou, Anna and Tsandilas, Theophanis and Echihabi, Karima and Bezerianos, Anastasia and Palpanas, Themis},
title = {Data Series Progressive Similarity Search with Probabilistic Quality Guarantees},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389751},
doi = {10.1145/3318464.3389751},
abstract = {Existing systems dealing with the increasing volume of data series cannot guarantee interactive response times, even for fundamental tasks such as similarity search. Therefore, it is necessary to develop analytic approaches that support exploration and decision making by providing progressive results, before the final and exact ones have been computed. Prior works lack both efficiency and accuracy when applied to large-scale data series collections. We present and experimentally evaluate a new probabilistic learning-based method that provides quality guarantees for progressive Nearest Neighbor (NN) query answering. We provide both initial and progressive estimates of the final answer that are getting better during the similarity search, as well suitable stopping criteria for the progressive queries. Experiments with synthetic and diverse real datasets demonstrate that our prediction methods constitute the first practical solution to the problem, significantly outperforming competing approaches.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1857–1873},
numpages = {17},
keywords = {progressive query answering, data series, similarity search},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389774,
author = {Doraiswamy, Harish and Freire, Juliana},
title = {A GPU-Friendly Geometric Data Model and Algebra for Spatial Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389774},
doi = {10.1145/3318464.3389774},
abstract = {The availability of low cost sensors has led to an unprecedented growth in the volume of spatial data. Unfortunately, the time required to evaluate even simple spatial queries over large data sets greatly hampers our ability to interactively explore these data sets and extract actionable insights. While Graphics Processing Units~(GPUs) are increasingly being used to speed up spatial queries, existing solutions have two important drawbacks: they are often tightly coupled to the specific query types they target, making it hard to adapt them for other queries; and since their design is based on CPU-based approaches, it can be difficult to effectively utilize all the benefits provided by the GPU. As a first step towards making GPU spatial query processing mainstream, we propose a new model that represents spatial data as geometric objects and define an algebra consisting of GPU-friendly composable operators that operate over these objects. We demonstrate the expressiveness of the proposed algebra and present a proof-of-concept prototype that supports a subset of the operators, which shows that it is orders of magnitude faster than a CPU-based implementation and outperforms custom GPU-based approaches.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1875–1885},
numpages = {11},
keywords = {GPU processing, spatial query model},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389760,
author = {Paparrizos, John and Liu, Chunwei and Elmore, Aaron J. and Franklin, Michael J.},
title = {Debunking Four Long-Standing Misconceptions of Time-Series Distance Measures},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389760},
doi = {10.1145/3318464.3389760},
abstract = {Distance measures are core building blocks in time-series analysis and the subject of active research for decades. Unfortunately, the most detailed experimental study in this area is outdated (over a decade old) and, naturally, does not reflect recent progress. Importantly, this study (i) omitted multiple distance measures, including a classic measure in the time-series literature; (ii) considered only a single time-series normalization method; and (iii) reported only raw classification error rates without statistically validating the findings, resulting in or fueling four misconceptions in the time-series literature. Motivated by the aforementioned drawbacks and our curiosity to shed some light on these misconceptions, we comprehensively evaluate 71 time-series distance measures. Specifically, our study includes (i) 8 normalization methods; (ii) 52 lock-step measures; (iii) 4 sliding measures; (iv) 7 elastic measures; (v) 4 kernel functions; and (vi) 4 embedding measures. We extensively evaluate these measures across 128 time-series datasets using rigorous statistical analysis. Our findings debunk four long-standing misconceptions that significantly alter the landscape of what is known about existing distance measures. With the new foundations in place, we discuss open challenges and promising directions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1887–1905},
numpages = {19},
keywords = {time series, sliding measures, kernel functions, elastic measures, statistical analysis, lock-step measures, nearest-neighbor classifier, embedding measures, distance measures},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389692,
author = {Bastani, Favyen and He, Songtao and Balasingam, Arjun and Gopalakrishnan, Karthik and Alizadeh, Mohammad and Balakrishnan, Hari and Cafarella, Michael and Kraska, Tim and Madden, Sam},
title = {MIRIS: Fast Object Track Queries in Video},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389692},
doi = {10.1145/3318464.3389692},
abstract = {Video databases that enable queries with object-track predicates are useful in many applications. Such queries include selecting objects that move from one region of the camera frame to another (e.g., finding cars that turn right through a junction) and selecting objects with certain speeds (e.g., finding animals that stop to drink water from a lake). Processing such predicates efficiently is challenging because they involve the movement of an object over several video frames. We propose a novel query-driven tracking approach that integrates query processing with object tracking to efficiently process object track queries and address the computational complexity of object detection methods. By processing video at low framerates when possible, but increasing the framerate when needed to ensure high-accuracy on a query, our approach substantially speeds up query execution. We have implemented query-driven tracking in MIRIS, a video query processor, and compare MIRIS against four baselines on a diverse dataset consisting of five sources of video and nine distinct queries. We find that, at the same accuracy, MIRIS accelerates video query processing by 9x on average over the IOU tracker, an overlap-based tracking-by-detection method used in existing video database systems.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1907–1921},
numpages = {15},
keywords = {video query optimization, video data management},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3394026,
author = {Faleiro, Jose M.},
title = {ACM SIGMOD Jim Gray Dissertation Award W Talk},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3394026},
doi = {10.1145/3318464.3394026},
abstract = {The increasing democratization of server hardware with multi-core CPUs and large main memories has been one of the dominant hardware trends of the last decade. "Bare metal" servers with tens of CPU cores and over 100 gigabytes of main memory have been available for several years now. Recently, this large scale hardware has also been available via the cloud. Database systems, with their roots in uniprocessors and paucity of main memory, have unsurprisingly been found wanting on modern hardware. In addition to changes in hardware, database systems have had to contend with changing application requirements and deployment environments. Database systems have long provided applications with an interactive interface, in which an application can communicate with the database over several round-trips in the course of a single request. A large class of applications, however, does not require interactive interfaces, and is unwilling to pay the performance cost associated with overly flexible interfaces. Some of these applications have eschewed database systems altogether in favor of high-performance key-value stores. Finally, modern applications are increasingly deployed at ever increasing scales, often serving hundreds of thousands to millions of simultaneous clients. These large scale deployments are more prone to errors due to consistency issues in their underlying database systems. Ever since their inception, database systems have allowed applications to tradeoff consistency for performance, and often nudge applications towards weak consistency. When deployed at scale, weak consistency exposes latent consistency-related bugs, in the same way that failures are more likely to occur at scale. Nearly every widely deployed database system provides applications with weak consistency consistency by default, and its widespread use in practice significantly complicates application development, leading to latent Heisenbugs that are only exposed in production. This dissertation proposes and explores the use of deterministic execution to address these concerns. Database systems have traditionally been non-deterministic; given an input list of transactions, the final state of the database, which corresponds to some totally ordered execution of transactions, is dependent on non-deterministic factors such as thread scheduling decisions made by the operating system and failures. Deterministic execution, on the other hand, ensures that the database's final state is always determined by its input list of transactions; in other words, the input list of transactions is the same as the total order of transactions that determines the database's state. While non-deterministic database systems expend significant resources in determining valid total orders of transactions, we show that deterministic systems can exploit simple and low-cost up-front total ordering of transactions to execute and schedule transactions much more efficiently. We show that deterministic execution enables low-overhead, highly-parallel scheduling mechanisms, that can address the performance limitations of existing database systems on modern hardware. Deterministic database systems are designed based on the assumption that applications can submit their transactions in one-shot prepared transactions, instead of multiple round-trips. Finally, we attempt to understand the fundamental reason for the observed performance differences between various consistency levels in database systems, and based on this understanding, show that we can exploit deterministic execution to provide strong consistency at a cost that is competitive with that offered by weak consistency levels.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1923},
numpages = {1},
keywords = {deterministic execution, award talk},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3394027,
author = {Huang, Silu},
title = {Effective Data Versioning for Collaborative Data Analytics},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3394027},
doi = {10.1145/3318464.3394027},
abstract = {With the massive proliferation of datasets in a variety of sec-tors, data science teams in these sectors spend vast amounts of time collaboratively constructing, curating, and analyzing these datasets. Versions of datasets are routinely generated during this data science process, via various data processing operations like data transformation and cleaning, feature engineering and normalization, among others. However, no existing systems enable us to effectively store, track, and query these versioned datasets, leading to massive redundancy in versioned data storage and making true collaboration and sharing impossible. In my PhD thesis, we develop solutions for versioned data management for collaborative data analytics. In the first part of my dissertation, we extend a relational database to support versioning of structured data. Specifically, we build a system, OrpheusDB, on top of a relational database with a carefully designed data representation and an intelligent partitioning algorithm for fast version control operations. OrpheusDB inherits much of the same benefits of relational databases, while also compactly storing, keeping track of, and recreating versions on demand. However, OrpheusDB implicitly makes a few assumptions, namely that:(a) the SQL assumption: a SQL-like language is the best fit for querying data and versioning information;(b) the structural assumption: the data is in a relational for-mat with a regular structure;(c) the from-scratch assumption: users adopt OrpheusDB from the very beginning of their project and register each data version along with full meta-data in the system. In the second part of my dissertation, we remove each of these assumptions, one at a time. First, we remove the SQL assumption and propose a generalized query language for querying data along with versioning and provenance information. Second, we remove the structural assumption and develop solutions for compact storage and fast retrieval of arbitrary data representations [4]. Finally, we remove the "from-scratch" assumption, by developing techniques to infer lineage relationships among versions residing in an existing data repository.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1925–1938},
numpages = {14},
keywords = {partitioning, lineage inference, query language, compact storage, data versioning, data representation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380605,
author = {Nargesian, Fatemeh and Pu, Ken Q. and Zhu, Erkang and Ghadiri Bashardoost, Bahar and Miller, Ren\'{e}e J.},
title = {Organizing Data Lakes for Navigation},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380605},
doi = {10.1145/3318464.3380605},
abstract = {We consider the problem of creating an effective navigation structure over a data lake. We define an organization as a navigation graph that contains nodes representing sets of attributes within a data lake and edges indicating subset relationships among nodes. We propose the data lake organization problem as the problem of finding an organization that allows a user to most effectively navigate a data lake. We present a new probabilistic model of how users interact with an organization and propose an approximate algorithm for the data lake organization problem. We show the effectiveness of the algorithm on both a real data lake containing data from open data portals and on a benchmark that contains rich metadata emulating the observed characteristics of real data lakes. Through a formal user study, we show that navigation can help users find relevant tables that cannot be found by keyword search.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1939–1950},
numpages = {12},
keywords = {data lakes, structure learning, dataset discovery and search},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389726,
author = {Zhang, Yi and Ives, Zachary G.},
title = {Finding Related Tables in Data Lakes for Interactive Data Science},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389726},
doi = {10.1145/3318464.3389726},
abstract = {Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1951–1966},
numpages = {16},
keywords = {table search, data lakes, interactive data science, notebooks},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380608,
author = {Raza, Mohammad and Gulwani, Sumit},
title = {Web Data Extraction Using Hybrid Program Synthesis: A Combination of Top-down and Bottom-up Inference},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380608},
doi = {10.1145/3318464.3380608},
abstract = {Automatic synthesis of web data extraction programs has been explored in a variety of settings, but in practice there remain various robustness and usability challenges. In this work we present a novel program synthesis approach which combines the benefits of deductive and enumerative synthesis strategies, yielding a semi-supervised technique with which concise programs expressible in standard languages can be synthesized from very few examples. We demonstrate improvement over existing techniques in terms of overall accuracy, number of examples required, and program complexity. Our method has been deployed as a web extraction feature in the mass market Microsoft Power BI product.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1967–1978},
numpages = {12},
keywords = {wrapper induction, program synthesis, web data extraction},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389695,
author = {Jian, Xun and Wang, Yue and Lei, Xiayu and Zheng, Libin and Chen, Lei},
title = {SPARQL Rewriting: Towards Desired Results},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389695},
doi = {10.1145/3318464.3389695},
abstract = {Recent years witnessed the emergence of various applications on knowledge graphs, which are often represented as RDF graphs. However, due to the lack of data schema and the complexity of SPARQL language, there is usually a gap between the user's real desire and the actual meaning of a SPARQL query, especially when the query itself is complicated. In this paper, we try to narrow this gap by modifying a given query with a set of modifiers, so that its result approaches a user-provided example set. Specifically, we model this problem as two individual sub-problems, query-restricting, and query-relaxing, both of which are shown to be NP-hard. We further prove that unless P=NP, query-restricting has no polynomial-time approximation scheme (PTAS), and query-relaxing has no polynomial-time constant-factor approximation algorithm. Despite their hardness, we propose a (1-1/ε)-approximation method for query-restricting and 2 heuristics for query-relaxing. Extensive experiments have been conducted on real-world knowledge graphs to evaluate the effectiveness and efficiency of our proposed solutions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1979–1993},
numpages = {15},
keywords = {SPARQL queries, query rewriting},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380599,
author = {Akrami, Farahnaz and Saeef, Mohammed Samiul and Zhang, Qingheng and Hu, Wei and Li, Chengkai},
title = {Realistic Re-Evaluation of Knowledge Graph Completion Methods: An Experimental Study},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380599},
doi = {10.1145/3318464.3380599},
abstract = {In the active research area of employing embedding models for knowledge graph completion, particularly for the task of link prediction, most prior studies used two benchmark datasets FB15k and WN18 in evaluating such models. Most triples in these and other datasets in such studies belong to reverse and duplicate relations which exhibit high data redundancy due to semantic duplication, correlation or data incompleteness. This is a case of excessive data leakage---a model is trained using features that otherwise would not be available when the model needs to be applied for real prediction. There are also Cartesian product relations for which every triple formed by the Cartesian product of applicable subjects and objects is a true fact. Link prediction on the aforementioned relations is easy and can be achieved with even better accuracy using straightforward rules instead of sophisticated embedding models. A more fundamental defect of these models is that the link prediction scenario, given such data, is non-existent in the real-world. This paper is the first systematic study with the main objective of assessing the true effectiveness of embedding models when the unrealistic triples are removed. Our experiment results show these models are much less accurate than what we used to perceive. Their poor accuracy renders link prediction a task without truly effective automated solution. Hence, we call for re-investigation of possible effective approaches.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1995–2010},
numpages = {16},
keywords = {embedding models, knowledge graph completion, link prediction},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389769,
author = {Ding, Bailu and Chaudhuri, Surajit and Narasayya, Vivek},
title = {Bitvector-Aware Query Optimization for Decision Support Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389769},
doi = {10.1145/3318464.3389769},
abstract = {Bitvector filtering is an important query processing technique that can significantly reduce the cost of execution, especially for complex decision support queries with multiple joins. Despite its wide application, however, its implication to query optimization is not well understood. In this work, we study how bitvector filters impact query optimization. We show that incorporating bitvector filters into query optimization straightforwardly can increase the plan space complexity by an exponential factor in the number of relations in the query. We analyze the plans with bitvector filters for star and snowflake queries in the plan space of right deep trees without cross products. Surprisingly, with some simplifying assumptions, we prove that, the plan of the minimal cost with bitvector filters can be found from a linear number of plans in the number of relations in the query. This greatly reduces the plan space complexity for such queries from exponential to linear. Motivated by our analysis, we propose an algorithm that accounts for the impact of bitvector filters in query optimization. Our algorithm optimizes the join order for an arbitrary decision support query by choosing from a linear number of candidate plans in the number of relations in the query. We implement our algorithm in a commercial database DBMS-X as a transformation rule. Our evaluation on both industry standard benchmarks and customer workload shows that, compared with DBMS-X, our technique reduces the total CPU execution time by 22%-64% for the workloads, with up to two orders of magnitude reduction in CPU execution time for individual queries.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2011–2026},
numpages = {16},
keywords = {bloom filter, query optimization, bitvector filter, join order enumeration, database, query processing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389717,
author = {Zhao, Zhuoyue and Li, Feifei and Liu, Yuxi},
title = {Efficient Join Synopsis Maintenance for Data Warehouse},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389717},
doi = {10.1145/3318464.3389717},
abstract = {Various sources such as daily business operations and sensors from different IoT applications constantly generate a lot of data. They are often loaded into a data warehouse system to perform complex analysis over. It, however, can be extremely costly if the query involves joins, especially many-to-many joins over multiple large tables. A join synopsis, i.e., a small uniform random sample over the join result, often suffices as a representative alternative to the full join result for many applications such as histogram construction, model training and etc. Towards that end, we propose a novel algorithm SJoin that can maintain a join synopsis over a pre-specified general θ-join query in a dynamic database with continuous inflows of updates. Central to SJoin is maintaining a weighted join graph index, which assists to efficiently replace join results in the synopsis upon update. We conduct extensive experiments using TPC-DS and a simulated road sensor data over several complex join queries and they demonstrate the clear advantage of SJoin over the best available baseline.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2027–2042},
numpages = {16},
keywords = {random sampling, join synopsis},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389783,
author = {Raza, Aunn and Chrysogelos, Periklis and Anadiotis, Angelos Christos and Ailamaki, Anastasia},
title = {Adaptive HTAP through Elastic Resource Scheduling},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389783},
doi = {10.1145/3318464.3389783},
abstract = {Modern Hybrid Transactional/Analytical Processing (HTAP) systems use an integrated data processing engine that performs analytics on fresh data, which are ingested from a transactional engine. HTAP systems typically consider data freshness at design time, and are optimized for a fixed range of freshness requirements, addressed at a performance cost for either OLTP or OLAP. The data freshness and the performance requirements of both engines, however, may vary with the workload. We approach HTAP as a scheduling problem, addressed at runtime through elastic resource management. We model an HTAP system as a set of three individual engines: an OLTP, an OLAP and a Resource and Data Exchange (RDE) engine. We devise a scheduling algorithm which traverses the HTAP design spectrum through elastic resource management, to meet the workload data freshness requirements. We propose an in-memory system design which is non-intrusive to the current state-of-art OLTP and OLAP engines, and we use it to evaluate the performance of our approach. Our evaluation shows that the performance benefit of our system for OLAP queries increases over time, reaching up to 50% compared to static schedules for 100 query sequences, while maintaining a small, and controlled, drop in the OLTP throughput.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2043–2054},
numpages = {12},
keywords = {OLTP, OLAP, HTAP, DBMS},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380565,
author = {Nam, Yoon-Min Nam and Han, Donghyoung Han and Kim, Min-Soo Kim},
title = {SPRINTER: A Fast n-Ary Join Query Processing Method for Complex OLAP Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380565},
doi = {10.1145/3318464.3380565},
abstract = {The concept of OLAP query processing is now being widely adopted in various applications. The number of complex queries containing the joins between non-unique keys (called FK-FK joins) increases in those applications. However, the existing in-memory OLAP systems tend not to handle such complex queries efficiently since they generate a large amount of intermediate results or incur a huge amount of probe cost. In this paper, we propose an effective query planning method for complex OLAP queries. It generates a query plan containing n-ary join operators based on a cost model. The plan does not generate intermediate results for processing FK-FK joins and significantly reduces the probe cost. We also propose an efficient processing method for n-ary join operators. We implement the prototype system SPRINTER by integrating our proposed methods into an open-source in-memory OLAP system. Through experiments using the TPC-DS benchmark, we have shown that SPRINTER outperforms the state-of-the-art OLAP systems for complex queries.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2055–2070},
numpages = {16},
keywords = {complex OLAP query processing, query optimization, co-processing, query planning, n-ary join operator, FK-FK join},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389731,
author = {Luo, Siqiang and Chatterjee, Subarna and Ketsetsidis, Rafael and Dayan, Niv and Qin, Wilson and Idreos, Stratos},
title = {Rosetta: A Robust Space-Time Optimized Range Filter for Key-Value Stores},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389731},
doi = {10.1145/3318464.3389731},
abstract = {We introduce Rosetta, a probabilistic range filter designed specifically for LSM-tree based key-value stores. The core intuition is that we can sacrifice filter probe time because it is not visible in end-to-end key-value store performance, which in turn allows us to significantly reduce the filter false positive rate for every level of the tree. Rosetta indexes all binary prefixes of a key using a hierarchically arranged set of Bloom filters. It then converts each range query into multiple probes, one for each non-overlapping binary prefix. Rosetta has the ability to track workload patterns and adopt a beneficial tuning for each individual LSM-tree run by adjusting the number of Bloom filters it uses and how memory is spread among them to optimize the FPR/CPU cost balance. We show how to integrate Rosetta in a full system, RocksDB, and we demonstrate that it brings as much as a 40x improvement compared to default RocksDB and 2-5x improvement compared to state-of-the-art range filters in a variety of workloads and across different levels of the memory hierarchy (memory, SSD, hard disk). We also show that, unlike state-of-the-art filters, Rosetta brings a net benefit in RocksDB's overall performance, i.e., it improves range queries without losing any performance for point queries.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2071–2086},
numpages = {16},
keywords = {NoSQL, range filters},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380580,
author = {Tsikoudis, Nikos and Shrira, Liuba},
title = {RID: Deduplicating Snapshot Computations},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380580},
doi = {10.1145/3318464.3380580},
abstract = {One can audit SQL applications by running SQL programs over sequences of persistent snapshots, but care is needed to avoid wasteful duplicate computation. This paper describes the design, implementation, and performance of RID, the first language-independent optimization framework that eliminates duplicate computations in SQL programs running over low-level snapshots by exploiting snapshot metadata efficiently.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2087–2101},
numpages = {15},
keywords = {temporal databases, query processing and optimization},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389701,
author = {Tahboub, Ruby Y. and Rompf, Tiark},
title = {Architecting a Query Compiler for Spatial Workloads},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389701},
doi = {10.1145/3318464.3389701},
abstract = {Modern location-based applications rely extensively on the efficient processing of spatial data and queries. Spatial query engines are commonly engineered as an extension to a relational database or a cluster-computing framework. Large parts of the spatial processing runtime is spent on evaluating spatial predicates and traversing spatial indexing structures. Typical high-level implementations of these spatial structures incur significant interpretive overhead, which increases latency and lowers throughput. A promising idea to improve the performance of spatial workloads is to leverage native code generation techniques that have become popular in relational query engines. However, architecting a spatial query compiler is challenging since spatial processing has fundamentally different execution characteristics from relational workloads in terms of data dimensionality, indexing structures, and predicate evaluation. In this paper, we discuss the underlying reasons why standard query compilation techniques are not fully effective when applied to spatial workloads, and we demonstrate how a particular style of query compilation based on techniques borrowed from partial evaluation and generative programming manages to avoid most of these difficulties by extending the scope of custom code generation into the data structures layer. We extend the LB2 main-memory query compiler, a relational engine developed in this style, with spatial data types, predicates, indexing structures, and operators. We show that the spatial extension matches the performance of specialized library code and outperforms relational and map-reduce extensions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2103–2118},
numpages = {16},
keywords = {code generation, extensible compilers, futamura projections, query compilation, spatial query processing, staging},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389703,
author = {Li, Pengfei and Lu, Hua and Zheng, Qian and Yang, Long and Pan, Gang},
title = {LISA: A Learned Index Structure for Spatial Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389703},
doi = {10.1145/3318464.3389703},
abstract = {In spatial query processing, the popular index R-tree may incur large storage consumption and high IO cost. Inspired by the recent learned index [17] that replaces B-tree with machine learning models, we study an analogy problem for spatial data. We propose a novel Learned Index structure for Spatial dAta (LISA for short). Its core idea is to use machine learning models, through several steps, to generate searchable data layout in disk pages for an arbitrary spatial dataset. In particular, LISA consists of a mapping function that maps spatial keys (points) into 1-dimensional mapped values, a learned shard prediction function that partitions the mapped space into shards, and a series of local models that organize shards into pages. Based on LISA, a range query algorithm is designed, followed by a lattice regression model that enables us to convert a KNN query to range queries. Algorithms are also designed for LISA to handle data updates. Extensive experiments demonstrate that LISA clearly outperforms R-tree and other alternatives in terms of storage consumption and IO cost for queries. Moreover, LISA can handle data insertions and deletions efficiently.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2119–2133},
numpages = {15},
keywords = {learned index, spatial index, machine learning, database},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389771,
author = {Yuan, Haitao and Li, Guoliang and Bao, Zhifeng and Feng, Ling},
title = {Effective Travel Time Estimation: When Historical Trajectories over Road Networks Matter},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389771},
doi = {10.1145/3318464.3389771},
abstract = {In this paper, we study the problem of origin-destination (OD) travel time estimation where the OD input consists of an OD pair and a departure time. We propose a novel neural network based prediction model that fully exploits an important fact neglected by the literature -- for a past OD trip its travel time is usually affiliated with the trajectory it travels along, whereas it does not exist during prediction. At the training phase, our goal is to design novel representations for the OD input and its affiliated trajectory, such that they are close to each other in the latent space. First, we match the OD pairs and their affiliated (historical) trajectories to road networks, and utilize road segment embeddings to represent their spatial properties. Later, we match the timestamps associated with trajectories to time slots and utilize time slot embeddings to represent the temporal properties. Next, we build a temporal graph to capture the weekly and daily periodicity of time slot embeddings. Last, we design an effective encoding to represent the spatial and temporal properties of trajectories. To bind each OD input to its affiliated trajectory, we also encode the OD input into a hidden representation, and make the hidden representation close to the spatio-temporal representation of the trajectory. At the prediction phase, we only use the OD input, get the hidden representation of the OD input, and use it to generate the travel time. Extensive experiments on real datasets show that our method achieves high effectiveness and outperforms existing methods.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2135–2149},
numpages = {15},
keywords = {OD travel time estimation, road networks, trajectory},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380564,
author = {Ohsaka, Naoto},
title = {The Solution Distribution of Influence Maximization: A High-Level Experimental Study on Three Algorithmic Approaches},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380564},
doi = {10.1145/3318464.3380564},
abstract = {Influence maximization is among the most fundamental algorithmic problems in social influence analysis. Over the last decade, a great effort has been devoted to developing efficient algorithms for influence maximization, so that identifying the "best" algorithm has become a demanding task. In SIGMOD'17, Arora, Galhotra, and Ranu reported benchmark results on eleven existing algorithms and demonstrated that there is no single state-of-the-art offering the best trade-off between computational efficiency and solution quality. In this paper, we report a high-level experimental study on three well-established algorithmic approaches for influence maximization, referred to as Oneshot, Snapshot, and Reverse Influence Sampling (RIS). Different from Arora et al., our experimental methodology is so designed that we examine the distribution of random solutions, characterize the relation between the sample number and the actual solution quality, and avoid implementation dependencies. Our main findings are as follows: 1. For a sufficiently large sample number, we obtain a unique solution regardless of algorithms. 2. The average solution quality of Oneshot, Snapshot, and RIS improves at the same rate up to scaling of sample number. 3. Oneshot requires more samples than Snapshot, and Snapshot requires fewer but larger samples than RIS. We discuss the time efficiency when conditioning Oneshot, Snapshot, and RIS to be of identical accuracy. Our conclusion is that Oneshot is suitable only if the size of available memory is limited, and RIS is more efficient than Snapshot for large networks; Snapshot is preferable for small, low-probability networks.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2151–2166},
numpages = {16},
keywords = {graph algorithms, performance evaluation, influence maximization},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389740,
author = {Guo, Qintian and Wang, Sibo and Wei, Zhewei and Chen, Ming},
title = {Influence Maximization Revisited: Efficient Reverse Reachable Set Generation with Bound Tightened},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389740},
doi = {10.1145/3318464.3389740},
abstract = {Given a social network G with n nodes and m edges, a positive integer k, and a cascade model C, the influence maximization (IM) problem asks for k nodes in G such that the expected number of nodes influenced by the k nodes under cascade model C is maximized. The state-of-the-art approximate solutions run in O(k(n+m)log(n)/ε2) expected time while returning a (1-1/e -ε) approximate solution with at least 1-1/n probability. A key phase of these IM algorithms is the random reverse reachable (RR) set generation, and this phase significantly affects the efficiency and scalability of the state-of-the-art IM algorithms. In this paper, we present a study on this key phase and propose an efficient random RR set generation algorithm under IC model. With the new algorithm, we show that the expected running time of existing IM algorithms under IC model can be improved to O(k· n log(n)/ε2), when for any node v, the total weight of its incoming edges is no larger than a constant. Moreover, existing approximate IM algorithms suffer from scalability issues in high influence networks where the size of random RR sets is usually quite large. We tackle this challenging issue by reducing the average size of random RR sets without sacrificing the approximation guarantee. The proposed solution is orders of magnitude faster than states of the art as shown in our experiment.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2167–2181},
numpages = {15},
keywords = {influence maximization, sampling},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380587,
author = {Liu, Qing and Zhao, Minjun and Huang, Xin and Xu, Jianliang and Gao, Yunjun},
title = {Truss-Based Community Search over Large Directed Graphs},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380587},
doi = {10.1145/3318464.3380587},
abstract = {Community search enables personalized community discovery and has wide applications in large real-world graphs. While community search has been extensively studied for undirected graphs, the problem for directed graphs has received attention only recently. However, existing studies suffer from several drawbacks, e.g., the vertices with varied in-degrees and out-degrees cannot be included in a community at the same time. To address the limitations, in this paper, we systematically study the problem of community search over large directed graphs. We start by presenting a novel community model, called D-truss, based on two distinct types of directed triangles, i.e., flow triangle and cycle triangle. The D-truss model brings nice structural and computational properties and has many advantages in comparison with the existing models. With this new model, we then formulate the D-truss community search problem, which is proved to be NP-hard. In view of its hardness, we propose two efficient 2-approximation algorithms, named Global and Local, that run in polynomial time yet with quality guarantee. To further improve the efficiency of the algorithms, we devise an indexing method based on D-truss decomposition. Consequently, the D-truss community search can be solved upon the D-truss index without time-consuming accesses to the original graph. Experimental studies on real-world graphs with ground-truth communities validate the quality of the solutions we obtain and the efficiency of the proposed algorithms.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2183–2197},
numpages = {15},
keywords = {directed graph, d-truss, community search},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380603,
author = {Kim, Junghoon and Guo, Tao and Feng, Kaiyu and Cong, Gao and Khan, Arijit and Choudhury, Farhana M.},
title = {Densely Connected User Community and Location Cluster Search in Location-Based Social Networks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380603},
doi = {10.1145/3318464.3380603},
abstract = {Searching for a community based on query nodes in a graph is a fundamental problem and has been extensively investigated. Most of the existing approaches focus on finding a community in a social network, and very few studies consider location-based social networks where users can check in locations. In this paper we propose the GeoSocial Community Search problem (GCS) which aims to find a social community and a cluster of spatial locations that are densely connected in a location-based social network simultaneously. The GCS can be useful for marketing and user/location recommendation. To the best of our knowledge, this is the first work to find a social community and a cluster of spatial locations that are densely connected from location-based social networks. We prove that the problem is NP-hard, and is not in APX, unless P = NP. To solve this problem, we propose three algorithms: core-based basic algorithm, top-down greedy removing algorithm, and an expansion algorithm. Finally, we report extensive experimental studies that offer insights into the efficiency and effectiveness of the proposed solutions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2199–2209},
numpages = {11},
keywords = {community search, location-based social network analysis},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389744,
author = {Linghu, Qingyuan and Zhang, Fan and Lin, Xuemin and Zhang, Wenjie and Zhang, Ying},
title = {Global Reinforcement of Social Networks: The Anchored Coreness Problem},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389744},
doi = {10.1145/3318464.3389744},
abstract = {The stability of a social network has been widely studied as an important indicator for both the network holders and the participants. Existing works on reinforcing networks focus on a local view, e.g., the anchored k-core problem aims to enlarge the size of the k-core with a fixed input k. Nevertheless, it is more promising to reinforce a social network in a global manner: considering the engagement of every user (vertex) in the network. Since the coreness of a user has been validated as the "best practice" for capturing user engagement, we propose and study the anchored coreness problem in this paper: anchoring a small number of vertices to maximize the coreness gain (the total increment of coreness) of all the vertices in the network. We prove the problem is NP-hard and show it is more challenging than the existing local-view problems. An efficient heuristic algorithm is proposed with novel techniques on pruning search space and reusing the intermediate results. Extensive experiments on real-life data demonstrate that our model is effective for reinforcing social networks and our algorithm is efficient.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2211–2226},
numpages = {16},
keywords = {user engagement, core decomposition, social network},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386127,
author = {Yan, Ying and Wei, Changzheng and Guo, Xuepeng and Lu, Xuming and Zheng, Xiaofu and Liu, Qi and Zhou, Chenhui and Song, Xuyang and Zhao, Boran and Zhang, Hui and Jiang, Guofei},
title = {Confidentiality Support over Financial Grade Consortium Blockchain},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386127},
doi = {10.1145/3318464.3386127},
abstract = {Confidentiality is an indispensable requirement in financial applications of blockchain technology, and supporting it along with high performance and friendly programmability is technically challenging. In this paper, we present a system design called CONFIDE to support on-chain confidentiality by leveraging Trust Execution Environment (TEE). CONFIDE's secure data transmission protocol and data encryption protocol, together with a highly efficient virtual machine run in TEE, guarantee the confidentiality in the life cycle of a transaction from end to end. CONFIDE proposes a secure data model along with an application-driven secure protocol to guarantee data confidentiality and integrity. Its smart contract language extension offers users the flexibility to define complex confidentiality models. CONFIDE is implemented as a plugin module to Antfin Blockchain's proprietary platform, and can be plugged into other blockchain platforms as well with its universal interface design. Nowadays, CONFIDE is supporting millions of commercial transactions daily on consortium blockchain running financial applications including supply chain finance, ABS, commodity provenance, and cold-chain logistics.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2227–2240},
numpages = {14},
keywords = {blockchain, trusted execution environment, confidentiality},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386131,
author = {Yang, Wen and Li, Tao and Fang, Gai and Wei, Hong},
title = {PASE: PostgreSQL Ultra-High-Dimensional Approximate Nearest Neighbor Search Extension},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386131},
doi = {10.1145/3318464.3386131},
abstract = {Similarity search has been widely used in various fields, particularly in the Alibaba ecosystem. The open-source solutions to a similarity search of vectors can only support a query with a single vector, whereas real-life scenarios generally require a processing of compound queries. Moreover, existing open-source implementations only provide runtime libraries, which have difficulty meeting the requirements of industrial applications. To address these issues, we designed a novel scheme for extending the index-type of PostgreSQL (PG), which enables a similar vector search and achieves a high-performance level and strong reliability of PG. Two representative types of nearest neighbor search (NNS) algorithms are presented herein. These algorithms achieve a high performance, and afford advantages such as the support of composite queries and seamless integration of existing business data. The other NNS algorithms can be easily implemented under the proposed framework. Experiments were conducted on large datasets to illustrate the efficiency of the proposed retrieval mechanism.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2241–2253},
numpages = {13},
keywords = {index, approximate nearest neighbor search (ANN), PostgreSQL, nearest neighbor search, high dimensional similarity search, HNSW},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386133,
author = {Maschi, Fabio and Owaida, Muhsen and Alonso, Gustavo and Casalino, Matteo and Hock-Koon, Anthony},
title = {Making Search Engines Faster by Lowering the Cost of Querying Business Rules Through FPGAs},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386133},
doi = {10.1145/3318464.3386133},
abstract = {Business Rule Management Systems (BRMSs) are widely used in industry for a variety of tasks. Their main advantage is to codify in a succinct and queryable manner vast amounts of constantly evolving logic. In BRMSs, rules are typically captured as facts (tuples) over a collection of criteria, and checking them involves querying the collection of rules to find the best match. In this paper, we focus on a real-world use case from the airline industry: determining the minimum connection time (MCT) between flights. The MCT module is part of the flight search engine, and captures the ever changing constraints at each airport that determine the time to allocate between an arriving and a departing flight for a connection to be feasible. We explore how to use hardware acceleration to (i) improve the performance of the MCT module (lower latency, higher throughput); and (ii) reduce the amount of computing resources needed. A key aspect of the solution is the transformation of a collection of rules into a Non-deterministic Finite state Automaton efficiently implemented on FPGA. Experiments performed on-premises and in the cloud show several orders of magnitude improvement over the existing solution, and the potential to reduce by 40% the number of machines needed for the flight search engine.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2255–2270},
numpages = {16},
keywords = {search engine, business rule, cloud, FPGA, database-as-a-service, NFA},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386142,
author = {Wang, Ke and Floratou, Avrilia and Agrawal, Ashvin and Musgrave, Daniel},
title = {Spur: Mitigating Slow Instances in Large-Scale Streaming Pipelines},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386142},
doi = {10.1145/3318464.3386142},
abstract = {Bing's monetization pipeline is one of the largest and most critical streaming workloads deployed in Microsoft's internal data lake. The pipeline runs 24/7 at a scale of 3500 YARN containers and is required to meet a Service Level Objective (SLO) of low tail latency. In this paper, we highlight some of the unique challenges imposed by this large scale of operation: other concurrent workloads sharing the cluster may cause random performance deterioration; unavailability of external dependencies may cause temporary stalls in the pipeline; scarcity in the underlying resource manager may cause arbitrarily long delays or rejection of container allocation requests. Weathering these challenges requires specially tailored dynamic control policies that react to these issues as and when they arise. We focus on the problem of reducing the latency in the tail, i.e., 99th percentile (p99), by detecting and mitigating slow instances through speculative replication. We show that widely used approaches do not satisfactorily solve this issue at our scale. A conservative approach is hesitant to acquire additional resources, reacts too slowly to the changes in the environment and therefore achieves little improvement in p99 latency. On the other hand, an aggressive approach overwhelms the underlying resource manager with unnecessary resource requests and paradoxically worsens the p99 latency. Our proposed approach, Spur, is designed for this challenging environment. It combines aggressive detection of slow instances with smart pruning of false positives to achieve a far better trade-off between these conflicting objectives. Using only 0.5% additional resources (similar to the conservative approach), we demonstrate a 10% -38% improvement in the tail latency compared to both conservative and aggressive approaches.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2271–2285},
numpages = {15},
keywords = {large-scale streaming pipelines, tail latency, slow instances, resource consumption},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3386143,
author = {Yan, Yan and Meyles, Stephen and Haghighi, Aria and Suciu, Dan},
title = {Entity Matching in the Wild: A Consistent and Versatile Framework to Unify Data in Industrial Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386143},
doi = {10.1145/3318464.3386143},
abstract = {Entity matching -- the task of clustering duplicated database records to underlying entities -- has become an increasingly critical component in modern data integration management. Amperity provides a platform for businesses to manage customer data that utilizes a machine-learning approach to entity matching, resolving billions of customer records on a daily basis. We face several challenges in deploying entity matching to industrial applications at scale, and they are less prominent in the literature. These challenges include: (1) Providing not just a single entity clustering, but supporting clusterings at multiple confidence levels to enable downstream applications with varying precision/recall trade-off needs. (2) Many customer record attributes may be systematically missing from different sources of data, creating many pairs of records in a cluster that appear to not match due to incomplete, rather than conflicting information. Allowing these records to connect transitively without introducing conflicts is invaluable to businesses because they can acquire a more comprehensive profile of their customers without incorrect entity merges. (3) How to cluster records over time and assign persistent cluster IDs that can be used for downstream use cases such as A/B tests or predictive model training; this is made more challenging by the fact that we receive new customer data every day and clusters naturally evolving over time still require persistent IDs that refer to the same entity. In this work, we describe Amperity's entity matching framework, Fusion, and how its design provides solutions to these challenges. In particular, we describe our pairwise matching model based on ordinal regression that permits a well-defined way to produce entity clusterings at different confidence levels, a novel clustering algorithm that separates conflicting record pairs in clusters while allowing for pairs that may appear dissimilar due to missing data, and a persistent ID generation algorithm which balances stability of the identifier with ever-evolving entities.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2287–2301},
numpages = {15},
keywords = {multi-level entity matching, cluster id assignment, conflict resolution in clustering},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389767,
author = {Leventidis, Aristotelis and Zhang, Jiahui and Dunne, Cody and Gatterbauer, Wolfgang and Jagadish, H.V. and Riedewald, Mirek},
title = {QueryVis: Logic-Based Diagrams Help Users Understand Complicated SQL Queries Faster},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389767},
doi = {10.1145/3318464.3389767},
abstract = {Understanding the meaning of existing SQL queries is critical for code maintenance and reuse. Yet SQL can be hard to read, even for expert users or the original creator of a query. We conjecture that it is possible to capture the logical intent of queries in automatically-generated visual diagrams that can help users understand the meaning of queries faster and more accurately than SQL text alone. We present initial steps in that direction with visual diagrams that are based on the first-order logic foundation of SQL and can capture the meaning of deeply nested queries. Our diagrams build upon a rich history of diagrammatic reasoning systems in logic and were designed using a large body of human-computer interaction best practices: they are minimal in that no visual element is superfluous; they are unambiguous in that no two queries with different semantics map to the same visualization; and they extend previously existing visual representations of relational schemata and conjunctive queries in a natural way. An experimental evaluation involving 42 users on Amazon Mechanical Turk shows that with only a 2--3 minute static tutorial, participants could interpret queries meaningfully faster with our diagrams than when reading SQL alone. Moreover, we have evidence that our visual diagrams result in participants making fewer errors than with SQL. We believe that more regular exposure to diagrammatic representations of SQL can give rise to a pattern-based and thus more intuitive use and re-use of SQL. A full version of this paper with all appendices and supplemental material for the experimental study (stimuli, raw data, and analysis code) are available at https://osf.io/btszh.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2303–2318},
numpages = {16},
keywords = {first-order logic, query visualization, query interpretation, visualization design, database usability, user study, query patterns, SQL, diagrammatic reasoning, nested queries, visual query language, query understanding},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389776,
author = {Baik, Christopher and Jin, Zhongjun and Cafarella, Michael and Jagadish, H. V.},
title = {Duoquest: A Dual-Specification System for Expressive SQL Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389776},
doi = {10.1145/3318464.3389776},
abstract = {Querying a relational database is difficult because it requires users to be familiar with both the SQL language and the schema. However, many users possess enough domain expertise to describe their desired queries by alternative means. For such users, two major alternatives to writing SQL are natural language interfaces (NLIs) and programming-by-example (PBE). Both of these alternatives face certain pitfalls: natural language queries (NLQs) are often ambiguous, even for human interpreters, while current PBE approaches limit functionality to be tractable. Consequently, we propose dual-specification query synthesis, which consumes both a NLQ and an optional PBE-like table sketch query that enables users to express varied levels of domain knowledge. We introduce the novel dual-specification Duoquest system, which leverages guided partial query enumeration to efficiently explore the space of possible queries. We present results from user studies in which Duoquest demonstrates a 62.5% absolute increase in query construction accuracy over a state-of-the-art NLI and comparable accuracy to a PBE system on a limited workload supported by the PBE system. In a simulation study on the Spider benchmark, Duoquest demonstrates a &gt;2x increase in top-1 accuracy over both NLI and PBE.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2319–2329},
numpages = {11},
keywords = {program synthesis, query by example, database usability, dual-specification interface, SQL},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389754,
author = {Dintyala, Prashanth and Narechania, Arpit and Arulraj, Joy},
title = {SQLCheck: Automated Detection and Diagnosis of SQL Anti-Patterns},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389754},
doi = {10.1145/3318464.3389754},
abstract = {The emergence of database-as-a-service platforms has made deploying database applications easier than before. Now, developers can quickly create scalable applications. However, designing performant, maintainable, and accurate applications is challenging. Developers may unknowingly introduce anti-patterns in the application's SQL statements. These anti-patterns are design decisions that are intended to solve a problem but often lead to other problems by violating fundamental design principles. In this paper, we present SQLCheck, a holistic toolchain for automatically finding and fixing anti-patterns in database applications. We introduce techniques for automatically (1) detecting anti-patterns with high precision and recall, (2) ranking the anti-patterns based on their impact on performance, maintainability, and accuracy of applications, and (3) suggesting alternative queries and changes to the database design to fix these anti-patterns. We demonstrate the prevalence of these anti-patterns in a large collection of queries and databases collected from open-source repositories. We introduce an anti-pattern detection algorithm that augments query analysis with data analysis. We present a ranking model for characterizing the impact of frequently occurring anti-patterns. We discuss how SQLCheck suggests fixes for high-impact anti-patterns using rule-based query refactoring techniques. Our experiments demonstrate that SQLCheck enables developers to create more performant, maintainable, and accurate applications.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2331–2345},
numpages = {15},
keywords = {anti-patterns, database applications},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380589,
author = {Weir, Nathaniel and Utama, Prasetya and Galakatos, Alex and Crotty, Andrew and Ilkhechi, Amir and Ramaswamy, Shekar and Bhushan, Rohin and Geisler, Nadja and H\"{a}ttasch, Benjamin and Eger, Steffen and Cetintemel, Ugur and Binnig, Carsten},
title = {DBPal: A Fully Pluggable NL2SQL Training Pipeline},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380589},
doi = {10.1145/3318464.3380589},
abstract = {Natural language is a promising alternative interface to DBMSs because it enables non-technical users to formulate complex questions in a more concise manner than SQL. Recently, deep learning has gained traction for translating natural language to SQL, since similar ideas have been successful in the related domain of machine translation. However, the core problem with existing deep learning approaches is that they require an enormous amount of training data in order to provide accurate translations. This training data is extremely expensive to curate, since it generally requires humans to manually annotate natural language examples with the corresponding SQL queries (or vice versa). Based on these observations, we propose DBPal, a new approach that augments existing deep learning techniques in order to improve the performance of models for natural language to SQL translation. More specifically, we present a novel training pipeline that automatically generates synthetic training data in order to (1) improve overall translation accuracy, (2) increase robustness to linguistic variation, and (3) specialize the model for the target database. As we show, our DBPal training pipeline is able to improve both the accuracy and linguistic robustness of state-of-the-art natural language to SQL translation models.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2347–2361},
numpages = {15},
keywords = {NLIDB, natural language to SQL, natural language interface to database, NL2SQL},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389777,
author = {Shah, Vraj and Li, Side and Kumar, Arun and Saul, Lawrence},
title = {SpeakQL: Towards Speech-Driven Multimodal Querying of Structured Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389777},
doi = {10.1145/3318464.3389777},
abstract = {Speech-driven querying is becoming popular in new device environments such as smartphones, tablets, and even conversational assistants. However, such querying is largely restricted to natural language. Typed SQL remains the gold standard for sophisticated structured querying although it is painful in many environments, which restricts when and how users consume their data. In this work, we propose to bridge this gap by designing a speech-driven querying system and interface for structured data we call SpeakQL. We support a practically useful subset of regular SQL and allow users to query in any domain with novel touch/speech based human-in-the-loop correction mechanisms. Automatic speech recognition (ASR) introduces myriad forms of errors in transcriptions, presenting us with a technical challenge. We exploit our observations of SQL's properties, its grammar, and the queried database to build a modular architecture. We present the first dataset of spoken SQL queries and a generic approach to generate them for any arbitrary schema. Our experiments show that SpeakQL can automatically correct a large fraction of errors in ASR transcriptions. User studies show that SpeakQL can help users specify SQL queries significantly faster with a speedup of average 2.7x and up to 6.7x compared to typing on a tablet device. SpeakQL also reduces the user effort in specifying queries by a factor of average 10x and up to 60x compared to raw typing effort.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2363–2374},
numpages = {12},
keywords = {SQL, multimodal, speech-driven},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389750,
author = {Li, Rundong and Gatterbauer, Wolfgang and Riedewald, Mirek},
title = {Near-Optimal Distributed Band-Joins through Recursive Partitioning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389750},
doi = {10.1145/3318464.3389750},
abstract = {We consider running-time optimization for band-joins in a distributed system, e.g., the cloud. To balance load across worker machines, input has to be partitioned, which causes duplication. We explore how to resolve this tension between maximum load per worker and input duplication for band-joins between two relations. Previous work suffered from high optimization cost or considered partitionings that were too restricted (resulting in suboptimal join performance). Our main insight is that recursive partitioning of the join-attribute space with the appropriate split scoring measure can achieve both low optimization cost and low join cost. It is the first approach that is not only effective for one-dimensional band-joins but also for joins on multiple attributes. Experiments indicate that our method is able to find partitionings that are within 10% of the lower bound for both maximum load per worker and input duplication for a broad range of settings, significantly improving over previous work.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2375–2390},
numpages = {16},
keywords = {band-join, running-time optimization, distributed joins},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380593,
author = {Glasbergen, Brad and Langendoen, Kyle and Abebe, Michael and Daudjee, Khuzaima},
title = {ChronoCache: Predictive and Adaptive Mid-Tier Query Result Caching},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380593},
doi = {10.1145/3318464.3380593},
abstract = {The performance of data-driven, web-scale client applications is sensitive to access latency. To address this concern, enterprises strive to cache data on edge nodes that are closer to users, thereby avoiding expensive round-trips to remote data centers. However, these geo-distributed approaches are limited to caching static data. In this paper we present ChronoCache, a mid-tier caching system that exploits the presence of geo-distributed edge nodes to cache database query results closer to users. ChronoCache transparently learns and leverages client application access patterns to predictively combine query requests and cache their results ahead of time, thereby reducing costly round-trips to the remote database. We show that ChronoCache reduces query response times by up to 2/3 over prior approaches on multiple representative benchmark workloads. representative benchmark workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2391–2406},
numpages = {16},
keywords = {prefetching, distributed data management, predictive caching},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389698,
author = {Tirmazi, Muhammad and Ben Basat, Ran and Gao, Jiaqi and Yu, Minlan},
title = {Cheetah: Accelerating Database Queries with Switch Pruning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389698},
doi = {10.1145/3318464.3389698},
abstract = {Modern database systems are growing increasingly distributed and struggle to reduce query completion time with a large volume of data. In this paper, we leverage programmable switches in the network to partially offload query computation to the switch. While switches provide high performance, they have resource and programming constraints that make implementing diverse queries difficult. To fit in these constraints, we introduce the concept of data pruning -- filtering out entries that are guaranteed not to affect output. The database system then runs the same query but on the pruned data, which significantly reduces processing time. We propose pruning algorithms for a variety of queries. We implement our system, Cheetah, on a Barefoot Tofino switch and Spark. Our evaluation on multiple workloads shows 40 - 200% improvement in the query completion time compared to Spark.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2407–2422},
numpages = {16},
keywords = {programmable switch, P4, cheetah},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389729,
author = {Chronis, Yannis and Do, Thanh and Graefe, Goetz and Peters, Keith},
title = {External Merge Sort for Top-K Queries: Eager Input Filtering Guided by Histograms},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389729},
doi = {10.1145/3318464.3389729},
abstract = {Business intelligence and web log analysis workloads often use queries with top-k clauses to produce the most relevant results. Values ofk range from small to rather large and sometimes the requested output exceeds the capacity of the available main memory. When the requested output fits in the available memory existing top-k algorithms are efficient, as they can eliminate almost all but the topk results before sorting them. When the requested output exceeds the main memory capacity, existing algorithms externally sort the entire input, which can be very expensive. Furthermore, the drastic difference in execution cost when the memory capacity is exceeded results in an unpleasant user experience. Every day, tens of thousands of production top-k queries executed on F1 Query resort to an external sort of the input. To address these challenges, we introduce a new top-k algorithm that is able to eliminate parts of the input before sorting or writing them to secondary storage, regardless of whether the requested output fits in the available memory. To achieve this, at execution time our algorithm creates a concise model of the input using histograms. The proposed algorithm is implemented as part of F1 Query and is used in production, where significantly accelerates top-k queries with outputs larger than the available memory. We evaluate our algorithm against existing top-k algorithms and show that it reduces I/O traffic and can be up to 11 times faster.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2423–2437},
numpages = {15},
keywords = {out-of-core, top-k, query operators},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389712,
author = {Wang, Qiange and Zhang, Yanfeng and Wang, Hao and Geng, Liang and Lee, Rubao and Zhang, Xiaodong and Yu, Ge},
title = {Automating Incremental and Asynchronous Evaluation for Recursive Aggregate Data Processing},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389712},
doi = {10.1145/3318464.3389712},
abstract = {In database and large-scale data analytics, recursive aggregate processing plays an important role, which is generally implemented under a framework of incremental computing and executed synchronously and/or asynchronously. We identify three barriers in existing recursive aggregate data processing. First, the processing scope is largely limited to monotonic programs. Second, checking on conditions for monotonicity and correctness for async processing is sophisticated and manually done. Third, execution engines may be suboptimal due to separation of sync and async execution. In this paper, we lay an analytical foundation for conditions to check if a recursive aggregate program that is monotonic or even non-monotonic can be executed incrementally and asynchronously with its correct result. We design and implement a condition verification tool that can automatically check if a given program satisfies the conditions. We further propose a unified sync-async engine to execute these programs for high performance. To integrate all these effective methods together, we have developed a distributed Datalog system, called PowerLog. Our evaluation shows that PowerLog can outperform three representative Datalog systems on both monotonic and non-monotonic recursive programs.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2439–2454},
numpages = {16},
keywords = {graph processing, recursive programs, aggregate operations, asynchronous execution, datalog, monotonic sequences},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389713,
author = {Abdelhamid, Ahmed S. and Mahmood, Ahmed R. and Daghistani, Anas and Aref, Walid G.},
title = {Prompt: Dynamic Data-Partitioning for Distributed Micro-Batch Stream Processing Systems},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389713},
doi = {10.1145/3318464.3389713},
abstract = {Advances in real-world applications require high-throughput processing over large data streams. Micro-batching has been proposed to support the needs of these applications. In micro-batching, the processing and batching of the data are interleaved, where the incoming data tuples are first buffered as data blocks, and then are processed collectively using parallel function constructs (e.g., Map-Reduce). The size of a micro-batch is set to guarantee a certain response-time latency that is to conform to the application's service-level agreement. In contrast to tuple-at-a-time data stream processing, micro-batching has the potential to sustain higher data rates. However, existing micro-batch stream processing systems use basic data-partitioning techniques that do not account for data skew and variable data rates. Load-awareness is necessary to maintain performance and to enhance resource utilization. A new data partitioning scheme termed Prompt is presented that leverages the characteristics of the micro-batch processing model. In the batching phase, a frequency-aware buffering mechanism is introduced that progressively maintains run-time statistics, and provides online key-based sorting as data tuples arrive. Because achieving optimal data partitioning is NP-Hard in this context, a workload-aware greedy algorithm is introduced that partitions the buffered data tuples efficiently for the Map stage. In the processing phase, a load-aware distribution mechanism is presented that balances the size of the input to the Reduce stage without incurring inter-task communication overhead. Moreover, Prompt elastically adapts resource consumption according to workload changes. Experimental results using real and synthetic data sets demonstrate that Prompt is robust against fluctuations in data distribution and arrival rates. Furthermore, Prompt achieves up to 200% improvement in system throughput over state-of-the-art techniques without degradation in latency.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2455–2469},
numpages = {15},
keywords = {micro-batch stream processing, elastic stream processing, distributed data processing, data partitioning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389723,
author = {Del Monte, Bonaventura and Zeuch, Steffen and Rabl, Tilmann and Markl, Volker},
title = {Rhino: Efficient Management of Very Large Distributed State for Stream Processing Engines},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389723},
doi = {10.1145/3318464.3389723},
abstract = {Scale-out stream processing engines (SPEs) are powering large big data applications on high velocity data streams. Industrial setups require SPEs to sustain outages, varying data rates, and low-latency processing. SPEs need to transparently reconfigure stateful queries during runtime. However, state-of-the-art SPEs are not ready yet to handle on-the-fly reconfigurations of queries with terabytes of state due to three problems. These are network overhead for state migration, consistency, and overhead on data processing. In this paper, we propose Rhino, a library for efficient reconfigurations of running queries in the presence of very large distributed state. Rhino provides a handover protocol and a state migration protocol to consistently and efficiently migrate stream processing among servers. Overall, our evaluation shows that Rhino scales with state sizes of up to TBs, reconfigures a running query 15 times faster than the state-of-the-art, and reduces latency by three orders of magnitude upon a reconfiguration.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2471–2486},
numpages = {16},
keywords = {stateful stream processing, distributed and parallel databases},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389739,
author = {Grulich, Philipp M. and Sebastian, Bre\ss{} and Zeuch, Steffen and Traub, Jonas and Bleichert, Janis von and Chen, Zongxiong and Rabl, Tilmann and Markl, Volker},
title = {Grizzly: Efficient Stream Processing Through Adaptive Query Compilation},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389739},
doi = {10.1145/3318464.3389739},
abstract = {Stream Processing Engines (SPEs) execute long-running queries on unbounded data streams. They follow an interpretation-based processing model and do not perform runtime optimizations. This limits the utilization of modern hardware and neglects changing data characteristics at runtime. In this paper, we present Grizzly, a novel adaptive query compilation-based SPE, to enable highly efficient query execution. We extend query compilation and task-based parallelization for the unique requirements of stream processing and apply adaptive compilation to enable runtime re-optimizations. The combination of light-weight statistic gathering with just-in-time compilation enables Grizzly to adjust to changing data-characteristics dynamically at runtime. Our experiments show that Grizzly outperforms state-of-the-art SPEs by up to an order of magnitude in throughput.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2487–2503},
numpages = {17},
keywords = {code generation, mutli-core, stream processing, adaptive query processing, hardware, query compilation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389753,
author = {Theodorakis, Georgios and Koliousis, Alexandros and Pietzuch, Peter and Pirk, Holger},
title = {LightSaber: Efficient Window Aggregation on Multi-Core Processors},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389753},
doi = {10.1145/3318464.3389753},
abstract = {Window aggregation queries are a core part of streaming applications. To support window aggregation efficiently, stream processing engines face a trade-off between exploiting parallelism (at the instruction/multi-core levels) and incremental computation (across overlapping windows and queries). Existing engines implement ad-hoc aggregation and parallelization strategies. As a result, they only achieve high performance for specific queries depending on the window definition and the type of aggregation function. We describe a general model for the design space of window aggregation strategies. Based on this, we introduce LightSaber, a new stream processing engine that balances parallelism and incremental processing when executing window aggregation queries on multi-core CPUs. Its design generalizes existing approaches: (i) for parallel processing, LightSaber constructs a parallel aggregation tree (PAT) that exploits the parallelism of modern processors. The PAT divides window aggregation into intermediate steps that enable the efficient use of both instruction-level (i.e., SIMD) and task-level (i.e., multi-core) parallelism; and (ii) to generate efficient incremental code from the PAT, LightSaber uses a generalized aggregation graph (GAG), which encodes the low-level data dependencies required to produce aggregates over the stream. A GAG thus generalizes state-of-the-art approaches for incremental window aggregation and supports work-sharing between overlapping windows. LightSaber achieves up to an order of magnitude higher throughput compared to existing systems-on a 16-core server, it processes 470 million records/s with 132 ?s average latency.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2505–2521},
numpages = {17},
keywords = {stream processing, incremental computation, code generation, window aggregation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380576,
author = {Shahvarani, Amirhesam and Jacobsen, Hans-Arno},
title = {Parallel Index-Based Stream Join on a Multicore CPU},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380576},
doi = {10.1145/3318464.3380576},
abstract = {Indexing sliding window content to enhance the performance of streaming queries can be greatly improved by utilizing the computational capabilities of a multicore processor. Conventional indexing data structures optimized for frequent search queries on a prestored dataset do not meet the demands of indexing highly dynamic data as in streaming environments. In this paper, we introduce an index data structure, called the partitioned in-memory merge tree, to address the challenges that arise when indexing highly dynamic data, which are common in streaming settings. Utilizing the specific pattern of streaming data and the distribution of queries, we propose a low-cost and effective concurrency control mechanism to meet the demands of high-rate update queries. To complement the index, we design an algorithm to realize a parallel index-based stream join that exploits the computational power of multicore processors. Our experiments using an octa-core processor show that our parallel stream join achieves up to 5.5 times higher throughput than a single-threaded approach.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2523–2537},
numpages = {15},
keywords = {parallel computing, data indexing, multicore processors, data stream processing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380600,
author = {Li, Conglong and Zhang, Minjia and Andersen, David G. and He, Yuxiong},
title = {Improving Approximate Nearest Neighbor Search through Learned Adaptive Early Termination},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380600},
doi = {10.1145/3318464.3380600},
abstract = {In applications ranging from image search to recommendation systems, the problem of identifying a set of "similar" real-valued vectors to a query vector plays a critical role. However, retrieving these vectors and computing the corresponding similarity scores from a large database is computationally challenging. Approximate nearest neighbor (ANN) search relaxes the guarantee of exactness for efficiency by vector compression and/or by only searching a subset of database vectors for each query. Searching a larger subset increases both accuracy and latency. State-of-the-art ANN approaches use fixed configurations that apply the same termination condition (the size of subset to search) for all queries, which leads to undesirably high latency when trying to achieve the last few percents of accuracy. We find that due to the index structures and the vector distributions, the number of database vectors that must be searched to find the ground-truth nearest neighbor varies widely among queries. Critically, we further identify that the intermediate search result after a certain amount of search is an important runtime feature that indicates how much more search should be performed. To achieve a better tradeoff between latency and accuracy, we propose a novel approach that adaptively determines search termination conditions for individual queries. To do so, we build and train gradient boosting decision tree models to learn and predict when to stop searching for a certain query. These models enable us to achieve the same accuracy with less total amount of search compared to the fixed configurations. We apply the learned adaptive early termination to state-of-the-art ANN approaches, and evaluate the end-to-end performance on three million to billion-scale datasets. Compared with fixed configurations, our approach consistently improves the average end-to-end latency by up to 7.1 times faster under the same high accuracy targets. Our approach is open source at github.com/efficient/faiss-learned-termination.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2539–2554},
numpages = {16},
keywords = {approximate nearest neighbor search, information retrieval},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380582,
author = {Wang, Yiqiu and Gu, Yan and Shun, Julian},
title = {Theoretically-Efficient and Practical Parallel DBSCAN},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380582},
doi = {10.1145/3318464.3380582},
abstract = {The DBSCAN method for spatial clustering has received significant attention due to its applicability in a variety of data analysis tasks. There are fast sequential algorithms for DBSCAN in Euclidean space that take O(n\l{}og n) work for two dimensions, sub-quadratic work for three or more dimensions, and can be computed approximately in linear work for any constant number of dimensions. However, existing parallel DBSCAN algorithms require quadratic work in the worst case. This paper bridges the gap between theory and practice of parallel DBSCAN by presenting new parallel algorithms for Euclidean exact DBSCAN and approximate DBSCAN that match the work bounds of their sequential counterparts, and are highly parallel (polylogarithmic depth). We present implementations of our algorithms along with optimizations that improve their practical performance. We perform a comprehensive experimental evaluation of our algorithms on a variety of datasets and parameter settings. Our experiments on a 36-core machine with two-way hyper-threading show that our implementations outperform existing parallel implementations by up to several orders of magnitude, and achieve speedups of up to 33x over the best sequential algorithms.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2555–2571},
numpages = {17},
keywords = {DBScan, spatial clustering, parallel algorithms},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389747,
author = {Dolmatova, Oksana and Augsten, Nikolaus and B\"{o}hlen, Michael H.},
title = {A Relational Matrix Algebra and Its Implementation in a Column Store},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389747},
doi = {10.1145/3318464.3389747},
abstract = {Analytical queries often require a mixture of relational and linear algebra operations applied to the same data. This poses a challenge to analytic systems that must bridge the gap between relations and matrices. Previous work has mainly strived to fix the problem at the implementation level. This paper proposes a principled solution at the logical level. We introduce the relational matrix algebra (RMA), which seamlessly integrates linear algebra operations into the relational model and eliminates the dichotomy between matrices and relations. RMA is closed: All our relational matrix operations are performed on relations and result in relations; no additional data structure is required. Our implementation in MonetDB shows the feasibility of our approach, and empirical evaluations suggest that in-database analytics performs well for mixed workloads.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2573–2587},
numpages = {15},
keywords = {matrix algebra, contextual information, column store},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3389778,
author = {Lei, Yifan and Huang, Qiang and Kankanhalli, Mohan and Tung, Anthony K. H.},
title = {Locality-Sensitive Hashing Scheme Based on Longest Circular Co-Substring},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389778},
doi = {10.1145/3318464.3389778},
abstract = {Locality-Sensitive Hashing (LSH) is one of the most popular methods for c-Approximate Nearest Neighbor Search (c-ANNS) in high-dimensional spaces. In this paper, we propose a novel LSH scheme based on the Longest Circular Co-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee. We introduce a novel concept of LCCS and a new data structure named Circular Shift Array (CSA) for k-LCCS search. The insight of LCCS search framework is that close data objects will have a longer LCCS than the far-apart ones with high probability. LCCS-LSH is LSH-family-independent, and it supports c-ANNS with different kinds of distance metrics. We also introduce a multi-probe version of LCCS-LSH and conduct extensive experiments over five real-life datasets. The experimental results demonstrate that LCCS-LSH outperforms state-of-the-art LSH schemes.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2589–2599},
numpages = {11},
keywords = {locality-sensitive hashing, suffix tree, nearest neighbor search, euclidean distance, angular distance},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3380601,
author = {Zhang, Huayi and Cao, Lei and Yan, Yizhou and Madden, Samuel and Rundensteiner, Elke A.},
title = {Continuously Adaptive Similarity Search},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380601},
doi = {10.1145/3318464.3380601},
abstract = {Similarity search is the basis for many data analytics techniques, including k-nearest neighbor classification and outlier detection. Similarity search over large data sets relies on i) a distance metric learned from input examples and ii) an index to speed up search based on the learned distance metric. In interactive systems, input to guide the learning of the distance metric may be provided over time. As this new input changes the learned distance metric, a naive approach would adopt the costly process of re-indexing all items after each metric change. In this paper, we propose the first solution, called OASIS, to instantaneously adapt the index to conform to a changing distance metric without this prohibitive re-indexing process. To achieve this, we prove that locality-sensitive hashing (LSH) provides an invariance property, meaning that an LSH index built on the original distance metric is equally effective at supporting similarity search using an updated distance metric as long as the transform matrix learned for the new distance metric satisfies certain properties. This observation allows OASIS to avoid recomputing the index from scratch in most cases. Further, for the rare cases when an adaption of the LSH index is shown to be necessary, we design an efficient incremental LSH update strategy that re-hashes only a small subset of the items in the index. In addition, we develop an efficient distance metric learning strategy that incrementally learns the new metric as inputs are received. Our experimental study using real world public datasets confirms the effectiveness of OASIS at improving the accuracy of various similarity search-based data analytics tasks by instantaneously adapting the distance metric and its associated index in tandem, while achieving an up to 3 orders of magnitude speedup over the state-of-art techniques.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2601–2616},
numpages = {16},
keywords = {distance metric learning, nearest neighbor search, LSH},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3383126,
author = {Milo, Tova and Somech, Amit},
title = {Automating Exploratory Data Analysis via Machine Learning: An Overview},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383126},
doi = {10.1145/3318464.3383126},
abstract = {Exploratory Data Analysis (EDA) is an important initial step for any knowledge discovery process, in which data scientists interactively explore unfamiliar datasets by issuing a sequence of analysis operations (e.g. filter, aggregation, and visualization). Since EDA is long known as a difficult task, requiring profound analytical skills, experience, and domain knowledge, a plethora of systems have been devised over the last decade in order to facilitate EDA.In particular, advancements in machine learning research have created exciting opportunities, not only for better facilitating EDA, but to fully automate the process. In this tutorial, we review recent lines of work for automating EDA. Starting from recommender systems for suggesting a single exploratory action, going through kNN-based classifiers and active-learning methods for predicting users' interestingness preferences, and finally to fully automating EDA using state-of-the-art methods such as deep reinforcement learning and sequence-to-sequence models.We conclude the tutorial with a discussion on the main challenges and open questions to be dealt with in order to ultimately reduce the manual effort required for EDA.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2617–2622},
numpages = {6},
keywords = {EDA, exploratory data analysis, data exploration},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3383127,
author = {Drutsa, Alexey and Fedorova, Valentina and Ustalov, Dmitry and Megorskaya, Olga and Zerminova, Evfrosiniya and Baidakova, Daria},
title = {Crowdsourcing Practice for Efficient Data Labeling: Aggregation, Incremental Relabeling, and Pricing},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383127},
doi = {10.1145/3318464.3383127},
abstract = {In this tutorial, we present a portion of unique industry experience in efficient data labeling via crowdsourcing shared by both leading researchers and engineers from Yandex. We will make an introduction to data labeling via public crowdsourcing marketplaces and will present the key components of efficient label collection. This will be followed by a practice session, where participants will choose one of the real label collection tasks, experiment with selecting settings for the labeling process, and launch their label collection project on one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session. While the crowd performers are annotating the project set up by the attendees, we will present the major theoretical results in efficient aggregation, incremental relabeling, and dynamic pricing. We will also discuss their strengths and weaknesses as well as applicability to real-world tasks, summarizing our five year-long research and industrial expertise in crowdsourcing. Finally, participants will receive a feedback about their projects and practical advice on how to make them more efficient. We invite beginners, advanced specialists, and researchers to learn how to collect high quality labeled data and do it efficiently.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2623–2627},
numpages = {5},
keywords = {quality control, dynamic pricing, answer aggregation, incremental relabeling, crowdsourcing, task design, data annotation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3383128,
author = {undefinedzcan, Fatma and Quamar, Abdul and Sen, Jaydeep and Lei, Chuan and Efthymiou, Vasilis},
title = {State of the Art and Open Challenges in Natural Language Interfaces to Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383128},
doi = {10.1145/3318464.3383128},
abstract = {Recent advances in natural language understanding and processing resulted in renewed interest in natural language based interfaces to data, which provide an easy mechanism for non-technical users to access and query the data. While early systems only allowed simple selection queries over a single table, some recent work supports complex BI queries, with many joins and aggregation, and even nested queries. There are various approaches in the literature for interpreting user's natural language query. Rule-based systems try to identify the entities in the query, and understand the intended relationships between those entities. Recent years have seen the emergence and popularity of neural network based approaches which try to interpret the query holistically, by learning the patterns. In this tutorial, we will review these natural language interface solutions in terms of their interpretation approach, as well as the complexity of the queries they can generate. We will also discuss open research challenges.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2629–2636},
numpages = {8},
keywords = {natural language interfaces, natural language query, conversation systems},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3383129,
author = {Shah, Nihar B. and Lipton, Zachary},
title = {SIGMOD 2020 Tutorial on Fairness and Bias in Peer Review and Other Sociotechnical Intelligent Systems},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383129},
doi = {10.1145/3318464.3383129},
abstract = {Questions of fairness and bias abound in all socially-consequential decisions pertaining to collection and management of data. Whether designing protocols for peer review of research papers, setting hiring policies, or framing research question in genetics, any data-management decision with the potential to allocate benefits or confer harms raises concerns about who gains or loses that may fail to surface in naively-chosen performance measures. Data science interacts with these questions in two fundamentally different ways: (i) as the technology driving the very systems responsible for certain social impacts, posing new questions about what it means for such systems to accord with ethical norms and the law; and (ii) as a set of powerful tools for analyzing existing data management systems, e.g., for auditing existing systems for various biases. This tutorial will tackle both angles on the interaction between technology and society vis-a-vis concerns over fairness and bias, particularly focusing on the collection and management of data. Our presentation will cover a wide range of disciplinary perspectives with the first part focusing on the social impacts of technology and the formulations of fairness and bias defined via protected characteristics and the second part taking a deep into peer review and distributed human evaluations, to explore other forms of bias, such as that due to subjectivity, miscalibration, and dishonest behavior.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2637–2640},
numpages = {4},
keywords = {fairness, bias, humans and AI, peer review, sociotechnical systems},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3383130,
author = {Khandelwal, Anurag and Kejariwal, Arun and Ramasamy, Karthikeyan},
title = {Le Taureau: Deconstructing the Serverless Landscape &amp; A Look Forward},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383130},
doi = {10.1145/3318464.3383130},
abstract = {Akin to the natural evolution of programming in assembly language to high-level languages, serverless computing represents the next frontier in the evolution of cloud computing: bare metal -&gt; virtual machines -&gt; containers -&gt; serverless. The genesis of serverless computing can be traced back to the fundamental need of enabling a programmer to singularly focus on writing application code in a high-level language and isolating all facets of system management (for example, but not limited to, instance selection, scaling, deployment, logging, monitoring, fault tolerance and so on). This is particularly critical in light of today's, increasingly tightening, time-to-market constraints. Currently, serverless computing is supported by leading public cloud vendors, such as AWS Lambda, Google Cloud Functions, Azure Cloud Functions and others. While this is an important step in the right direction, there are many challenges going forward. For instance, but not limited to, how to enable support for dynamic optimization, how to extend support for stateful computation, how to efficiently bin-pack applications, how to support hardware heterogeneity (this will be key especially in light of the emergence of hardware accelerators for deep learning workloads). Inspired by Picasso's Le Taureau, in the tutorial proposed herein, we shall deconstruct evolution of serverless --- the overarching intent being to facilitate better understanding of the serverless landscape. This, we hope, would help push the innovation frontier on both fronts, the paradigm itself and the applications built atop of it.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2641–2650},
numpages = {10},
keywords = {ephemeral storage, serverless computing, cloud computing, data analytics, distributed systems, real-time streaming},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3383131,
author = {Carbone, Paris and Fragkoulis, Marios and Kalavri, Vasiliki and Katsifodimos, Asterios},
title = {Beyond Analytics: The Evolution of Stream Processing Systems},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383131},
doi = {10.1145/3318464.3383131},
abstract = {Stream processing has been an active research field for more than 20 years, but it is now witnessing its prime time due to recent successful efforts by the research community and numerous worldwide open-source communities. The goal of this tutorial is threefold. First, we aim to review and highlight noteworthy past research findings, which were largely ignored until very recently. Second, we intend to underline the differences between early ('00-'10) and modern ('11-'18) streaming systems, and how those systems have evolved through the years. Most importantly, we wish to turn the attention of the database community to recent trends: streaming systems are no longer used only for classic stream processing workloads, namely window aggregates and joins. Instead, modern streaming systems are being increasingly used to deploy general event-driven applications in a scalable fashion, challenging the design decisions, architecture and intended use of existing stream processing systems.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2651–2658},
numpages = {8},
keywords = {stream processing, distributed computing, shared-nothing architectures, MapReduce, cloud computing},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3383132,
author = {Tziavelis, Nikolaos and Gatterbauer, Wolfgang and Riedewald, Mirek},
title = {Optimal Join Algorithms Meet Top-k},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383132},
doi = {10.1145/3318464.3383132},
abstract = {Top-k queries have been studied intensively in the database community and they are an important means to reduce query cost when only the "best" or "most interesting" results are needed instead of the full output. While some optimality results exist, e.g., the famous Threshold Algorithm, they hold only in a fairly limited model of computation that does not account for the cost incurred by large intermediate results and hence is not aligned with typical database-optimizer cost models. On the other hand, the idea of avoiding large intermediate results is arguably the main goal of recent work on optimal join algorithms, which uses the standard RAM model of computation to determine algorithm complexity. This research has created a lot of excitement due to its promise of reducing the time complexity of join queries with cycles, but it has mostly focused on full-output computation. We argue that the two areas can and should be studied from a unified point of view in order to achieve optimality in the common model of computation for a very general class of top-k-style join queries. This tutorial has two main objectives. First, we will explore and contrast the main assumptions, concepts, and algorithmic achievements of the two research areas. Second, we will cover recent, as well as some older, approaches that emerged at the intersection to support efficient ranked enumeration of join-query results. These are related to classic work on k-shortest path algorithms and more general optimization problems, some of which dates back to the 1950s. We demonstrate that this line of research warrants renewed attention in the challenging context of ranked enumeration for general join queries.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2659–2665},
numpages = {7},
keywords = {top-k, fractional edge cover, optimal join algorithms, submodular width, factorised databases, k-shortest paths, worst-case-optimal join algorithms, fractional hypertreewidth, dynamic progrmaming, ranked enumeration},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3383133,
author = {Idreos, Stratos and Callaghan, Mark},
title = {Key-Value Storage Engines},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383133},
doi = {10.1145/3318464.3383133},
abstract = {Key-value stores are everywhere. They power a diverse set of data-driven applications across both industry and science. Key-value stores are used as stand-alone NoSQL systems but they are also used as a part of more complex pipelines and systems such as machine learning and relational systems. In this tutorial, we survey the state-of-the-art approaches on how the core storage engine of a key-value store system is designed. We focus on several critical components of the engine, starting with the core data structures to lay out data across the memory hierarchy. We also discuss design issues related to caching, timestamps, concurrency control, updates, shifting workloads, as well as mixed workloads with both analytical and transactional characteristics. We cover designs that are read-optimized, write-optimized as well as hybrids. We draw examples from several state-of-the-art systems but we also put everything together in a general framework which allows us to model storage engine designs under a single unified model and reason about the expected behavior of diverse designs. In addition, we show that given the vast number of possible storage engine designs and their complexity, there is a need to be able to describe and communicate design decisions at a high level descriptive language and we present a first version of such a language. We then use that framework to present several open challenges in the field, especially in terms of supporting increasingly more diverse and dynamic applications in the era of data science and AI, including neural networks, graphs, and data versioning.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2667–2672},
numpages = {6},
keywords = {key-value stores, self-designing systems},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384677,
author = {Wang, Jin and Xiao, Guorui and Gu, Jiaqi and Wu, Jiacheng and Zaniolo, Carlo},
title = {RASQL: A Powerful Language and Its System for Big Data Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384677},
doi = {10.1145/3318464.3384677},
abstract = {There is a growing interest in supporting advanced Big Data applications on distributed data processing platforms. Most of these systems support SQL or its dialect as the query interface due to its portability and declarative nature. However, current SQL standard cannot effectively express advanced analytical queries due to its limitation in supporting recursive queries. In this demonstration, we show that this problem can be resolved via a simple SQL extension that delivers greater expressive power by allowing aggregates in recursion. To this end, we propose the Recursive-aggregate-SQL (RASQL) language and its system on top of Apache Spark to express and execute complex queries and declarative algorithms in many applications, such as graph search and machine learning. With a variety of examples, we will (i) show how complicated analytic queries can be expressed with RASQL; (ii) illustrate formal semantics of the powerful new constructs; and (iii) present a user-friendly interface to interact with the RASQL system and monitor the query results.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2673–2676},
numpages = {4},
keywords = {big data, query language, recursive query},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384678,
author = {Hirn, Denis and Grust, Torsten},
title = {PL/SQL Without the PL},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384678},
doi = {10.1145/3318464.3384678},
abstract = {We demonstrate a source-to-source compilation technique that can translate user-defined PL/SQL functions into plain SQL queries. These PL/SQL functions may feature arbitrarily complex control flow-iteration, in particular. From this imperative-style input code we derive equivalent recursive common table expressions, ready for execution on any relational SQL:1999 back-end. Principally due to the absence of PL/SQL?SQL context switches, the output plain SQL code comes with substantial runtime savings. The demonstration embeds the compiler into an interactive setup that admits function editing while live re-compilation and visualization of intermediate code forms happens in the background.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2677–2680},
numpages = {4},
keywords = {recursive common table expression, SQL, PL/SQL, ANF, tail recursion, SSA, iteration},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384679,
author = {Belmpas, Theofilos and Gkini, Orest and Koutrika, Georgia},
title = {Analysis of Database Search Systems with THOR},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384679},
doi = {10.1145/3318464.3384679},
abstract = {Numerous search systems have been implemented that allow users to pose unstructured queries over databases without the need to use a query language, such as SQL. Unfortunately, the landscape of efforts is fragmented with no clear sight of which system is best, and what open challenges we should pursue in our research. To help towards this direction, we present THOR that makes 4 important contributions: a query benchmark, a framework for comparing different systems, several search system implementations, and a highly interactive tool for comparing different search systems.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2681–2684},
numpages = {4},
keywords = {visualizations, search interfaces, information retrieval},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384680,
author = {Song, Yinglong and Chua, Huey Eng and Bhowmick, Sourav S. and Choi, Byron and Zhou, Shuigeng},
title = {BOOMER: A Tool for Blending Visual P-Homomorphic Queries on Large Networks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384680},
doi = {10.1145/3318464.3384680},
abstract = {The paradigm of interleaving (i.e. blending) visual subgraph query formulation and processing by exploiting the latency offered by the GUI brings in several potential benefits such as superior system response time (SRT) and opportunities to enhance usability of graph databases. Recent efforts at implementing this paradigm are focused on subgraph isomorphism-based queries, which are often restrictive in many real-world graph applications. In this demonstration, we present a novel system called BOOMER to realize this paradigm on more generic but complex bounded 1-1 p-homomorphic(BPH) queries on large networks. Intuitively, a BPH query maps an edge of the query to bounded paths in the data graph. We demonstrate various innovative features of BOOMER, its flexibility, and its promising performance.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2685–2688},
numpages = {4},
keywords = {graph query results visualization, p-homomorphic query, visual graph query formulation, blending, cap index},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384681,
author = {Bhowmick, Sourav S. and Huang, Kai and Chua, Huey Eng and Yuan, Zifeng and Choi, Byron and Zhou, Shuigeng},
title = {AURORA: Data-Driven Construction of Visual Graph Query Interfaces for Graph Databases},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384681},
doi = {10.1145/3318464.3384681},
abstract = {Several commercial and academic frameworks for querying a large collection of small- or medium-sized data graphs (eg. chemical compounds) provide visual graph query interfaces (a.k.a GUI) to facilitate non-programmers to query these sources. However, construction of these visual interfaces is not data-driven. That is, it does not exploit the underlying data graphs to automatically generate the contents of various panels in a GUI. Such data-driven construction has several benefits such as facilitating efficient subgraph query formulation and portability of the interface across different application domains and sources. In this demonstration, we present a novel data-driven visual subgraph query interface construction engine called AURORA. Specifically, given a graph repository D containing a collection of small- or medium-sized data graphs, it automatically generates the GUI for D by populating various components of the interface. We demonstrate various innovative features of AURORA.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2689–2692},
numpages = {4},
keywords = {visual graph query interface, portability, data-driven construction, usability, graph databases},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384682,
author = {Wang, Haixin and Xu, Cheng and Zhang, Ce and Xu, Jianliang},
title = {VChain: A Blockchain System Ensuring Query Integrity},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384682},
doi = {10.1145/3318464.3384682},
abstract = {This demonstration presents vChain, a blockchain system that ensures query integrity. With the proliferation of blockchain applications and services, there has been an increasing demand for querying the data stored in a blockchain database. However, existing solutions either are at the risk of losing query integrity, or require users to maintain a full copy of the blockchain database. In comparison, by employing a novel verifiable query processing framework, vChain enables a lightweight user to authenticate the query results returned from a potentially untrusted service provider. We demonstrate its verifiable query operations, usability, and performance with visualization for better insights. We also showcase how users can detect falsified results in the case that the service provider is compromised.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2693–2696},
numpages = {4},
keywords = {query processing, blockchain, data integrity},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384683,
author = {Hu, Wentao and Zhang, Dongxiang and Jiang, Dawei and Wu, Sai and Chen, Ke and Tan, Kian-Lee and Chen, Gang},
title = {AUDITOR: A System Designed for Automatic Discovery of Complex Integrity Constraints in Relational Databases},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384683},
doi = {10.1145/3318464.3384683},
abstract = {In this demonstration, we present a new definition of integrity constraint that is more powerful for anomalous data discovery. In our definition, a constraint is functioned on both categorical and numerical attributes in relational tables, as well as their derivative attributes, leading to a huge search space. Furthermore, we are the first to take into account attribute value distribution as part of a constraint. Based on the proposed integrity constraint, we build AUDITOR on top of relational tables from the industry of healthcare auditing and demonstrate its effectiveness and ease-of-use for domain experts to discover anomalous data.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2697–2700},
numpages = {4},
keywords = {anomalous data discovery, complex integrity constraint},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384684,
author = {Bonifati, Angela and Martens, Wim and Timm, Thomas},
title = {SHARQL: Shape Analysis of Recursive SPARQL Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384684},
doi = {10.1145/3318464.3384684},
abstract = {We showcase SHARQL, a system that allows to navigate SPARQL query logs, can inspect complex queries by visualizing their shape, and can serve as a back-end to flexibly produce statistics about the logs. Even though SPARQL query logs are increasingly available and have become public recently, their navigation and analysis is hampered by the lack of appropriate tools. SPARQL queries are sometimes hard to understand and their inherent properties, such as their shape, their hypertree properties, and their property paths are even more difficult to be identified and properly rendered. In SHARQL, we show how the analysis and exploration of several hundred million queries is possible. We offer edge rendering which works with complex hyperedges, regular edges, and property paths of SPARQL queries. The underlying database stores more than one hundred attributes per query and is therefore extremely flexible for exploring the query logs and as a back-end to compute and display analytical properties of the entire logs or parts thereof.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2701–2704},
numpages = {4},
keywords = {database queries, SPARQL, query logs},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384685,
author = {Chen, Hongzhi and Wu, Bowen and Deng, Shiyuan and Huang, Chenghuan and Li, Changji and Li, Yichao and Cheng, James},
title = {High Performance Distributed OLAP on Property Graphs with Grasper},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384685},
doi = {10.1145/3318464.3384685},
abstract = {Achieving high performance OLAP over large graphs is a challenging problem and has received great attention recently because of its broad spectrum of applications. Existing systems have various performance bottlenecks due to limitations such as low parallelism and high network overheads. This Demo presents Grasper, an RDMA-enabled distributed graph OLAP system, which adopts a series of new system designs to overcome the challenges of OLAP on graphs. The take-aways for Demo attendees are: (1)~a good understanding of the challenges of processing graph OLAP queries; (2)~useful insights about where Grasper's good performance comes from; (3)~inspirations about how to design an efficient graph OLAP system by comparing Grasper with existing systems.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2705–2708},
numpages = {4},
keywords = {graph query, distributed system, RDMA, OLAP},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384686,
author = {Park, Kisung and Jeong, Taeyoung and Jeong, Chanho and Lee, Jaeha and Lee, Dong-Hun and Lee, Young-Koo},
title = {ProcAnalyzer: Effective Code Analyzer for Tuning Imperative Programs in SAP HANA},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384686},
doi = {10.1145/3318464.3384686},
abstract = {Troubleshooting imperative programs at runtime is very challenging because the final optimized plan is quite different from the original design time model. In this demonstration, we present ProcAnalyzer, an expressive and intuitive tool for troubleshooting issues related to performance, code quality, and security. We propose end-to-end graph (E2EGraph) that provides a holistic view of design time, compile time, and runtime behavior so that end users and engine developers easily find the correlations between design time and runtime. ProcAnalyzer provides suggestions and visualization to find problematic statements through the E2EGraph.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2709–2712},
numpages = {4},
keywords = {tuning, troubleshooting, stored procedure, user-defined function},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384687,
author = {Tan, Sean and S Bhowmick, Sourav and Chua, Huey Eng and Xiao, Xiaokui},
title = {LATTE: Visual Construction of Smart Contracts},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384687},
doi = {10.1145/3318464.3384687},
abstract = {Smart contracts enable developers to run instructions on blockchains (eg. Ethereum) and have broad range of real-world applications. Solidity is the most popular high-level smart contract programming language on Ethereum. Coding in such language, however, demands a user to be proficient in contract programming and debugging to construct smart contracts correctly. In practice, such expectation makes it harder for non-programmers to take advantage of smart contracts. In this demonstration, we present a novel visual smart contract construction system on Ethereum called latte to make smart contract development accessible to non-programmers. Specifically, it allows a user to construct a contract without writing Solidity code by manipulating visual objects in a direct manipulation-based interface. Furthermore, latte interactively guides users and makes them aware of the cost (in units of Gas) of visual actions undertaken by them during contract construction.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2713–2716},
numpages = {4},
keywords = {usability, visual formulation, smart contracts, blockchain, visual interface},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384688,
author = {Toliopoulos, Theodoros and Bellas, Christos and Gounaris, Anastasios and Papadopoulos, Apostolos},
title = {PROUD: PaRallel OUtlier Detection for Streams},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384688},
doi = {10.1145/3318464.3384688},
abstract = {We introduce PROUD, standing for PaRallel OUtlier Detection for streams, which is an extensible engine for continuous multi-parameter parallel distance-based outlier (or anomaly) detection tailored to big data streams. PROUD is built on top of Flink. It defines a simple API for data ingestion. It supports a variety of parallel techniques, including novel ones, for continuous outlier detection that can be easily configured. In addition, it graphically reports metrics of interest and stores main results into a permanent store to enable future analysis. It can be easily extended to support additional techniques. Finally, it is publicly provided in open-source.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2717–2720},
numpages = {4},
keywords = {outlier detection, flink},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384689,
author = {Jin, Zhongjun and Xu, Mengjing and Sun, Chenkai and Asudeh, Abolfazl and Jagadish, H. V.},
title = {MithraCoverage: A System for Investigating Population Bias for Intersectional Fairness},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384689},
doi = {10.1145/3318464.3384689},
abstract = {Data-driven technologies are only as good as the data they work with. On the other hand, data scientists have often limited control on how the data is collected. Failing to contain adequate number of instances from minority (sub)groups, known as population bias, is a major reason for model unfairness and disparate performance across different groups. We demonstrate MithraCoverage, a system for investigating population bias over the intersection of multiple attributes. We use the concept of coverage for identifying intersectional subgroups with inadequate representation in the dataset. MithraCoverage is a web application with an interactive visual interface that allows data scientists to explore the dataset and identify subgroups with poor coverage.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2721–2724},
numpages = {4},
keywords = {data ethics, fairness, responsible data science},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384690,
author = {Gershtein, Shay and Milo, Tova and Morami, Gefen and Novgorodov, Slava},
title = {MC3: A System for Minimization of Classifier Construction Cost},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384690},
doi = {10.1145/3318464.3384690},
abstract = {Search mechanisms over massive sets of items are the cornerstone of many modern applications, particularly in e-commerce websites. Consumers express in search queries a set of properties, and expect the system to retrieve qualifying items. A common difficulty, however, is that the information on whether or not an item satisfies the search criteria is sometimes not explicitly recorded in the repository. Instead, it may be considered as general knowledge or "hidden" in a picture/description, thereby leading to incomplete search results. To overcome these problems companies invest in building dedicated classifiers that determine whether an item satisfies the given search criteria. However, building classifiers typically incurs non-trivial costs due to the required volumes of high-quality labeled training data. In this demo, we introduce MC3, a real-time system that helps data analysts decide which classifiers to construct to minimize the costs of answering a set of search queries. MC3 is interactive and facilitates real-time analysis, by providing detailed classifiers impact information. We demonstrate the effectiveness of MC3 on real-world data and scenarios taken from a large e-commerce system, by interacting with the SIGMOD'20 audience members who act as analysts.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2725–2728},
numpages = {4},
keywords = {e-commerce, classifiers},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384691,
author = {Glasbergen, Brad and Abebe, Michael and Daudjee, Khuzaima and Vogel, Daniel and Zhao, Jian},
title = {Sentinel: Understanding Data Systems},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384691},
doi = {10.1145/3318464.3384691},
abstract = {The complexity of modern data systems and applications greatly increases the challenge in understanding system behaviour and diagnosing performance problems. When these problems arise, system administrators are left with the difficult task of remedying them by relying on large debug log files, vast numbers of metrics, and system-specific tooling. We demonstrate the Sentinel system, which enables administrators to analyze systems and applications by building models of system execution and comparing them to derive key differences in behaviour. The resulting analyses are then presented as system reports to administrators and developers in an intuitive fashion. Users of Sentinel can locate, identify and take steps to resolve the reported performance issues. As Sentinel's models are constructed online by intercepting debug logging library calls, Sentinel's functionality incurs little overhead and works with all systems that use standard debug logging libraries.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2729–2732},
numpages = {4},
keywords = {debug logging, performance diagnosis, system behaviour},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384692,
author = {Louren\c{c}o, Raoni and Freire, Juliana and Shasha, Dennis},
title = {BugDoc: A System for Debugging Computational Pipelines},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384692},
doi = {10.1145/3318464.3384692},
abstract = {Data analysis for scientific experiments and enterprises, large-scale simulations, and machine learning tasks all entail the use of complex computational pipelines to reach quantitative and qualitative conclusions. If some of the activities in a pipeline produce erroneous outputs, the pipeline may fail to execute or produce incorrect results. Inferring the root cause(s) of such failures is challenging, usually requiring time and much human thought, while still being error-prone. We recently proposed a new approach that makes provenance to automatically and iteratively infer root causes and derive succinct explanations of failures; such an approach was implemented in our prototype, BugDoc. In this demonstration, we will illustrate BugDoc's capabilities to debug pipelines using few configuration instances.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2733–2736},
numpages = {4},
keywords = {workflow debugging, provenance},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384693,
author = {Chen, Yueting and Yu, Xiaohui and Koudas, Nick},
title = {TQVS: Temporal Queries over Video Streams in Action},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384693},
doi = {10.1145/3318464.3384693},
abstract = {We present TQVS, a system capable of conducting efficient evaluation of declarative temporal queries over real-time video streams. Users may issue queries to identify video clips in which the same two cars and the same three persons appear jointly in the frames for say 30 seconds. In real-world videos, some of the objects may disappear in frames due to reasons such as occlusion, which introduces challenges to query evaluation. Our system, aiming to address such challenges, consists of two main components: the Object Detection and Tracking (ODT) module and the Query Evaluation module. The ODT module utilizes state-of-art Object Detection and Tracking algorithms to produce a list of identified objects for each frame. Based on these results, we maintain select object combinations through the current window during query evaluation. Those object combinations contain sufficient information to evaluate queries correctly. Since the number of possible combinations could be very large, we introduce a novel technique to structure the possible combinations and facilitate query evaluation. We demonstrate that our approach offers significant performance benefits compared to alternate approaches and constitutes a fundamental building block of the TQVS system.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2737–2740},
numpages = {4},
keywords = {temporal queries, demo system, video streams},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384694,
author = {Fariha, Anna and Tiwari, Ashish and Radhakrishna, Arjun and Gulwani, Sumit},
title = {ExTuNe: Explaining Tuple Non-Conformance},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384694},
doi = {10.1145/3318464.3384694},
abstract = {In data-driven systems, we often encounter tuples on which the predictions of a machine-learned model are untrustworthy. A key cause of such untrustworthiness is non-conformance of a new tuple with respect to the training dataset. To check conformance, we introduce a novel concept of data invariant, which captures a set of implicit constraints that all tuples of a dataset satisfy: a test tuple is non-conforming if it violates the data invariants. Data invariants model complex relationships among multiple attributes; but do not provide interpretable explanations of non-conformance. We present ExTuNe, a system for Explaining causes of Tuple Non-conformance. Based on the principles of causality, ExTuNe assigns responsibility to the attributes for causing non-conformance. The key idea is to observe change in invariant violation under intervention on attribute-values. Through a simple interface, ExTuNe produces a ranked list of the test tuples based on their degree of non-conformance and visualizes tuple-level attribute responsibility for non-conformance through heat maps. ExTuNe further visualizes attribute responsibility, aggregated over the test tuples. We demonstrate how ExTuNe can detect and explain tuple non-conformance and assist the users to make careful decisions towards achieving trusted machine learning.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2741–2744},
numpages = {4},
keywords = {trusted machine learning, causality, data invariants},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384695,
author = {Qin, Xuedi and Chai, Chengliang and Luo, Yuyu and Tang, Nan and Li, Guoliang},
title = {Interactively Discovering and Ranking Desired Tuples without Writing SQL Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384695},
doi = {10.1145/3318464.3384695},
abstract = {The very first step of many data analytics is to find and (possibly) rank desired tuples, typically through writing SQL queries - this is feasible only for data experts who can write SQL queries and know the data very well. Unfortunately, in practice, the queries might be complicated (for example, "find and rank good off-road cars based on a combination of Price, Make, Model, Age, Mileage, and so on" is complicated because it contains many if-then-else, and, or and not logic) such that even data experts cannot precisely specify SQL queries; and the data might be unknown, which is common in data discovery that one tries to discover desired data from a data lake. Naturally, a system that can help users to discover and rank desired tuples without writing SQL queries is needed. We propose to demonstrate such as a system, namely DExPlorer. To use DExPlorer for data exploration, the user only needs to interactively perform two simple operations over a set of system provided tuples: (1) annotate which tuples are desired (i.e., true labels) or not (i.e., false labels), and (2) annotate whether a tuple is more preferred than another one (i.e., partial orders or ranked lists). We will show that DExPlorer can find user's desired tuples and rank them in a few interactions, even for complicated queries.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2745–2748},
numpages = {4},
keywords = {database, data exploration, rank, SQL query},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384696,
author = {Mannino, Miro and Abouzied, Azza},
title = {Synner: Generating Realistic Synthetic Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384696},
doi = {10.1145/3318464.3384696},
abstract = {Synner allows users to generate realistic-looking data. With Synner users can visually and declaratively specify properties of the dataset they wish to generate. Such properties include the domain, and statistical distribution of each field, and relationships between fields. User can also sketch custom distributions and relationships. Synner provides instant feedback on every user interaction by visualizing a preview of the generated data. It also suggests generation specifications from a few user-provided examples of data to generate, column labels and other user interactions. In this demonstration, we showcase Synner and summarize results from our evaluation of Synner's effectiveness at generating realistic-looking data.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2749–2752},
numpages = {4},
keywords = {example-driven interaction, declarative languages, data generation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384697,
author = {Shraga, Roee and Scharf, Coral and Ackerman, Rakefet and Gal, Avigdor},
title = {InCognitoMatch: Cognitive-Aware Matching via Crowdsourcing},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384697},
doi = {10.1145/3318464.3384697},
abstract = {We present InCognitoMatch, the first cognitive-aware crowdsourcing application for matching tasks. InCognitoMatch provides a handy tool to validate, annotate, and correct correspondences using the crowd whilst accounting for human matching biases. In addition, InCognitoMatch enables system administrators to control context information visible for workers and analyze their performance accordingly. For crowd workers, InCognitoMatch is an easy-to-use application that may be accessed from multiple crowdsourcing platforms. In addition, workers completing a task are offered suggestions for followup sessions according to their performance in the current session. For this demo, the audience will be able to experience InCognitoMatch thorough three use-cases, interacting with system as workers and as administrators.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2753–2756},
numpages = {4},
keywords = {crowdsourcing, matching, cognitive-aware, data integration},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384698,
author = {Musleh, Mashaal and Ouzzani, Mourad and Tang, Nan and Doan, AnHai},
title = {CoClean: Collaborative Data Cleaning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384698},
doi = {10.1145/3318464.3384698},
abstract = {High quality data is crucial for many applications but real-life data is often dirty. Unfortunately, automated solutions are often not trustable and are thus seldom employed in practice. In real-world scenarios, it is often necessary to resort to manual cleaning for obtaining pristine data. Existing human-in-the-loop solutions, such as Trifacta and OpenRefine, typically involve a single user. This is often error-prone, limited to a single-person expertise, and cannot scale with the ever growing volume, variety and veracity of data.We propose a crowd-in-the-loop cleaning system, called CoClean, built on top of Python Pandas dataframe, a widely used library for data scientists. The core of CoCleanis a new Python library called Collaborative dataframe (CDF) that allows one to share data represented as a dataframe with other users. CDF is responsible for synchronizing and aggregating annotations obtained from different users. The attendees will have the opportunity to experience the following features:(1)Data Assignment: Given a dataframe, the owner can assign it (or a subset of it) to different users. (2)Supporting both lay and power users: lay users can use a GUI for direct manual cleaning of the data, while power users can work on the assigned data through a Jupyter Notebook where they can write scripts to do batch cleaning. (3)Combining machines and humans: Possible errors and repairs generated by machine algorithms can be highlighted as annotations, which can make the life of users easier for manual cleaning. (4)Collaboration Modes: CoClean supports two modes: blind-on(no user can see the annotations from others) and blind-off.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2757–2760},
numpages = {4},
keywords = {data consolidation, data preparation, data cleaning, data collaboration},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384699,
author = {Chen, Zhida and Cong, Gao and Aref, Walid G.},
title = {STAR: A Distributed Stream Warehouse System for Spatial Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384699},
doi = {10.1145/3318464.3384699},
abstract = {The proliferation of mobile phones and location-based services gives rise to an explosive growth of spatial data. This spatial data contains valuable information, and calls for data stream warehouse systems that can provide real-time analytical results with the latest integrated spatial data. In this demonstration, we present the STAR (Spatial Data Stream Warehouse) system. STAR is a distributed in-memory spatial data stream warehouse system that provides low-latency and up-to-date analytical results over a fast spatial data stream. STAR supports a rich set of aggregate queries for spatial data analytics, e.g., contrasting the frequencies of spatial objects that appear in different spatial regions, or showing the most frequently mentioned topics being tweeted in different cities. STAR processes aggregate queries by maintaining distributed materialized views. Additionally, STAR supports dynamic load adjustment that makes STAR scalable and adaptive. We demonstrate STAR on top of Amazon EC2 clusters using real data sets.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2761–2764},
numpages = {4},
keywords = {spatial data, distributed system, data warehouse, stream},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384700,
author = {Deutch, Daniel and Frost, Nave and Gilad, Amir and Sheffer, Oren},
title = {T-REx: Table Repair Explanations},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384700},
doi = {10.1145/3318464.3384700},
abstract = {Data repair is a common and crucial step in many frameworks today, as applications may use data from different sources and of different levels of credibility. Thus, this step has been the focus of many works, proposing diverse approaches. To assist users in understanding the output of such data repair algorithms, we propose T-REx, a system for providing data repair explanations through Shapley values. The system is generic and not specific to a given repair algorithm or approach: it treats the algorithm as a black box. Given a specific table cell selected by the user, T-REx employs Shapley values to explain the significance of each constraint and each table cell in the repair of the cell of interest. T-REx then ranks the constraints and table cells according to their importance in the repair of this cell. This explanation allows users to understand the repair process, as well as to act based on this knowledge, to modify the most influencing constraints or the original database.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2765–2768},
numpages = {4},
keywords = {database constraints, shapley value, data repairs},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384701,
author = {Chao, Daren and Koudas, Nick and Xarchakos, Ioannis},
title = {SVQ++: Querying for Object Interactions in Video Streams},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384701},
doi = {10.1145/3318464.3384701},
abstract = {Deep neural nets enabled sophisticated information extraction out of images, including video frames. Recently, there has been interest in techniques and algorithms to enable interactive declarative query processing of objects appearing on video frames and their associated interactions on the video feed. SVQ++ is a system for declarative querying on real-time video streams involving objects and their interactions. The system utilizes a sequence of inexpensive and less accurate models (filters), called Progressive Filters (PF), to detect the presence of the query specified objects on frames, and a filtering approach, called Interaction Sheave (IS), to effectively prune frames that are not likely to contain interactions. We demonstrate that this system can efficiently identify frames in a streaming video in which an object is interacting with another in a specific way, increasing the frame processing rate dramatically and speed up query processing by at least two orders of magnitude depending on the query.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2769–2772},
numpages = {4},
keywords = {object interactions, video queries, video streams},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384702,
author = {Nikolic, Milos and Zhang, Haozhe and Kara, Ahmet and Olteanu, Dan},
title = {F-IVM: Learning over Fast-Evolving Relational Data},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384702},
doi = {10.1145/3318464.3384702},
abstract = {F-IVM is a system for real-time analytics such as machine learning applications over training datasets defined by queries over fast-evolving relational databases. We will demonstrate F-IVM for three such applications: model selection, Chow-Liu trees, and ridge linear regression.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2773–2776},
numpages = {4},
keywords = {incremental maintenance, feature selection, regression},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384703,
author = {Fang, Ziquan and Gao, Yunjun and Pan, Lu and Chen, Lu and Miao, Xiaoye and Jensen, Christian S.},
title = {CoMing: A Real-Time Co-Movement Mining System for Streaming Trajectories},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384703},
doi = {10.1145/3318464.3384703},
abstract = {The aim of real-time co-movement pattern mining for streaming trajectories is to discover co-moving objects that satisfy specific spatio-temporal constraints in real time. This functionality serves a range of real-world applications, such as traffic monitoring and management. However, little work targets the visualization and interaction with such co-movement detection on streaming trajectories. To this end, we develop CoMing, a real-time co-movement pattern mining system, to handle streaming trajectories. CoMing leverages ICPE, a real-time distributed co-movement pattern detection framework, and thus, it has its capacity of good performance. This demonstration offers hands-on experience with CoMing's visual and user-friendly interface. Moreover, several applications in the traffic domain, including object monitoring and traffic statistics visualization, are also provided to users.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2777–2780},
numpages = {4},
keywords = {visualization, co-movement pattern, trajectory, system},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384704,
author = {Bori\'{c}, Nemanja and Gildhoff, Hinnerk and Karavelas, Menelaos and Pandis, Ippokratis and Tsalouchidou, Ioanna},
title = {Unified Spatial Analytics from Heterogeneous Sources with Amazon Redshift},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384704},
doi = {10.1145/3318464.3384704},
abstract = {Enterprise companies use spatial data for decision optimization and gain new insights regarding the locality of their business and services. Industries rely on efficiently combining spatial and business data from different sources, such as data warehouses, geospatial information systems, transactional systems, and data lakes, where spatial data can be found in structured or unstructured form. In this demonstration we present the spatial functionality of Amazon Redshift and its integration with other Amazon services, such as Amazon Aurora PostgreSQL and Amazon S3. We focus on the design and functionality of the feature, including the extensions in Redshift's state-of-the-art optimizer to push spatial processing close to where the data is stored.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2781–2784},
numpages = {4},
keywords = {distributed processing, federation, spatial analytics},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384705,
author = {Zhang, Liang and Alghamdi, Noura and Eltabakh, Mohamed Y. and Rundensteiner, Elke A.},
title = {Big Data Series Analytics Using TARDIS and Its Exploitation in Geospatial Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384705},
doi = {10.1145/3318464.3384705},
abstract = {The massive amounts of data series data continuously generated and collected by applications require new indices to speed up data series similarity queries on which various data mining techniques rely. However, the state-of-the-art iSAX-based indexing techniques do not scale well due to the binary fanout that leads to a highly deep index tree and suffer from accuracy degradation due to the character-level cardinality that leads to poor maintenance of the proximity. To address this problem, we recently proposed TARDIS to supports indexing and querying billion-scale data series datasets. It introduces a new iSAX-T signatures to reduce the cardinality conversion cost and corresponding sigTree to construct a compact index structure to preserve better similarity. The framework consists of one centralized index and local distributed indices to efficiently re-partition and index dimensional datasets. Besides, effective query strategies based on sigTree structure are proposed to greatly improve the accuracy. In this demonstration, we present GENET, a new interactive exploration demonstration that allows users to support Big Data Series Approximate Retrieval and Recursive Interactive Clustering in large-scale geospatial datasets using TARDIS index techniques.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2785–2788},
numpages = {4},
keywords = {KNN approximate query, approximate query processing, GENET, geospatial, iSAX-T, word-level cardinality, clustering, TARDIS, data series, distributed indexing, sigtree},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384706,
author = {Marcus, Ryan and Zhang, Emily and Kraska, Tim},
title = {CDFShop: Exploring and Optimizing Learned Index Structures},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384706},
doi = {10.1145/3318464.3384706},
abstract = {Indexes are a critical component of data management applications. While tree-like structures (e.g., B-Trees) have been employed to great success, recent work suggests that index structures powered by machine learning models (learned index structures) can achieve low lookup times with a reduced memory footprint. This demonstration showcases CDFShop, a tool to explore and optimize recursive model indexes (RMIs), a type of learned index structure. This demonstration allows audience members to (1) gain an intuition about various tuning parameters of RMIs and why learned index structures can greatly accelerate search, and (2) understand how automatic optimization techniques can be used to explore space/time tradeoffs within the space of RMIs.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2789–2792},
numpages = {4},
keywords = {learned index structure, index, learned index},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384707,
author = {Caveness, Emily and G. C., Paul Suganthan and Peng, Zhuo and Polyzotis, Neoklis and Roy, Sudip and Zinkevich, Martin},
title = {TensorFlow Data Validation: Data Analysis and Validation in Continuous ML Pipelines},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384707},
doi = {10.1145/3318464.3384707},
abstract = {Machine Learning (ML) research has primarily focused on improving the accuracy and efficiency of the training algorithms while paying much less attention to the equally important problem of understanding, validating, and monitoring the data fed to ML. Irrespective of the ML algorithms used, data errors can adversely affect the quality of the generated model. This indicates that we need to adopt a data-centric approach to ML that treats data as a first-class citizen, on par with algorithms and infrastructure which are the typical building blocks of ML pipelines. In this demonstration we showcase TensorFlow Data Validation (TFDV), a scalable data analysis and validation system for ML that we have developed at Google and recently open-sourced. This system is deployed in production as an integral part of TFX - an end-to-end machine learning platform at Google. It is used by hundreds of product teams at Google and has received significant attention from the open-source community as well.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2793–2796},
numpages = {4},
keywords = {data management, machine learning},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384708,
author = {Wang, Zuozhi and Zeng, Kai and Huang, Botong and Chen, Wei and Cui, Xiaozong and Wang, Bo and Liu, Ji and Fan, Liya and Qu, Dachuan and Hou, Zhenyu and Guan, Tao and Li, Chen and Zhou, Jingren},
title = {Grosbeak: A Data Warehouse Supporting Resource-Aware Incremental Computing},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384708},
doi = {10.1145/3318464.3384708},
abstract = {As the primary approach to deriving decision-support insights, automated recurring routine analytic jobs account for a major part of cluster resource usages in modern enterprise data warehouses. These recurring routine jobs usually have stringent schedule and deadline determined by external business logic, and thus cause dreadful resource skew and severe resource over-provision in the cluster. In this paper, we present Grosbeak, a novel data warehouse that supports resource-aware incremental computing to process recurring routine jobs, smooths the resource skew, and optimizes the resource usage. Unlike batch processing in traditional data warehouses, Grosbeak leverages the fact that data is continuously ingested. It breaks an analysis job into small batches that incrementally process the progressively available data, and schedules these small-batch jobs intelligently when the cluster has free resources. In this demonstration, we showcase Grosbeak using real-world analysis pipelines. Users can interact with the data warehouse by registering recurring queries and observing the incremental scheduling behavior and smoothed resource usage pattern.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2797–2800},
numpages = {4},
keywords = {database query optimization, incremental view maintenance, incremental computation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384709,
author = {Jo, Saehan and Trummer, Immanuel},
title = {Demonstration of BitGourmet: Data Analysis via Deterministic Approximation},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384709},
doi = {10.1145/3318464.3384709},
abstract = {We demonstrate BitGourmet, a novel data analysis system that supports deterministic approximate query processing (DAQ). The system executes aggregation queries and produces deterministic bounds that are guaranteed to contain the true value. The system allows users to set a precision constraint on query results. Given a user-defined target precision, we operate on a carefully selected data subset to satisfy the precision constraint. More precisely, we divide each column vertically, bit-by-bit. Our specialized query processing engine evaluates queries on subsets of these bit vectors. This involves a scenario-specific query optimizer which relies on quality and cost models to decide the optimal bit selection and execution plan. In our demonstration, we show that DAQ realizes an interesting trade-off between result quality and execution time, making data analysis more interactive. We also offer manual control over the query plan, i.e., the bit selection and the execution plan, so that users can gain more insights into our system and DAQ in general.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2801–2804},
numpages = {4},
keywords = {approximate query processing, deterministic approximation, interactive data analysis},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384710,
author = {Pastor, Eliana and Baralis, Elena},
title = {Bring Your Own Data to X-PLAIN},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384710},
doi = {10.1145/3318464.3384710},
abstract = {Exploring and understanding the motivations behind black-box model predictions is becoming essential in many different applications. X-PLAIN is an interactive tool that allows human-in-the-loop inspection of the reasons behind model predictions. Its support for the local analysis of individual predictions enables users to inspect the local behavior of different classifiers and compare the knowledge different classifiers are exploiting for their prediction. The interactive exploration of prediction explanation provides actionable insights for both trusting and validating model predictions and, in case of unexpected behaviors, for debugging and improving the model itself.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2805–2808},
numpages = {4},
keywords = {local rules, prediction explanation, interpretability},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384711,
author = {Ramjit, Lana and Kong, Zhaoning and Netravali, Ravi and Wu, Eugene},
title = {Physical Visualization Design},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384711},
doi = {10.1145/3318464.3384711},
abstract = {We demonstrate PVD, a system that visualization designers can use to co-design the interface and system architecture of scalable and expressive visualization.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2809–2812},
numpages = {4},
keywords = {physical design, query optimization, visualization design},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384712,
author = {Samuel, Mingwei and Yan, Cong and Cheung, Alvin},
title = {Demonstration of Chestnut: An In-Memory Data Layout Designer for Database Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384712},
doi = {10.1145/3318464.3384712},
abstract = {This demonstration showcases Chestnut, a data layout generator for in-memory object-oriented database applications. Given an application and a memory budget, Chestnut generates a customized in-memory data layout and the corresponding query plans that are specialized for the application queries. Our demo will let users design and improve simple web applications using Chestnut. Users can view the Chestnut-generated data layouts using a custom visualization system, which will allow users to see how the application parameters affect Chestnut's design. Finally, users will be able to run queries generated by the application via the customized query plans generated by Chestnut or traditional relational query engines, and can compare the results and observe the speedup achieved by the Chestnut-generated query plans.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2813–2816},
numpages = {4},
keywords = {database applications, in-memory data processing, data layout},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384399,
author = {Luo, Chen},
title = {Breaking Down Memory Walls in LSM-Based Storage Systems},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384399},
doi = {10.1145/3318464.3384399},
abstract = {The log-structured merge-tree (LSM-tree) [16, 18] is widely used in modern NoSQL systems. Different from traditional update-in-place structures, an LSM-tree first buffers all writes in memory, which are subsequently flushed to disk when memory is full. The on-disk components are usually organized into levels of exponentially increasing sizes, where a smaller level is merged into the adjacent larger level when it fills up. To bound the temporary disk space occupied by merges, modern LSM-tree implementations often range partition a disk component into many fixed-size SSTables},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2817–2819},
numpages = {3},
keywords = {storage management, memory management, LSM-trees},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384400,
author = {Susanina, Yuliya},
title = {Context-Free Path Querying via Matrix Equations},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384400},
doi = {10.1145/3318464.3384400},
abstract = {Context-free path querying (CFPQ) widely used for graph-structured data analysis in different areas. It is crucial to develop highly efficient algorithms for CFPQ since the size of the input data is typically large. We show how to reduce GFPQ evaluation to solving systems of matrix equations over R --- a problem for which there exist high-performance solutions. Also, we demonstrate the applicability of our approach to real-world data analysis.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2821–2823},
numpages = {3},
keywords = {context-free path querying, nonlinear matrix equations, context-free grammar, graph databases, Newton's method},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384401,
author = {Chen, Xiaoshuang},
title = {Simulation-Based Approximate Graph Pattern Matching},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384401},
doi = {10.1145/3318464.3384401},
abstract = {Graph pattern matching is a fundamental problem in analyzing attributed graphs, that is to search the matches of a given query graph in a large data graph. However, existing algorithms either encounter with the performance issues or cannot capture reasonable matches. In this paper, we propose a simulation-based approximate pattern matching algorithm that is not only efficient to compute, but also able to capture those reasonable matches (missed by existing algorithms)},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2825–2827},
numpages = {3},
keywords = {graph pattern matching, simulation, inexact matching},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384402,
author = {Echihabi, Karima},
title = {High-Dimensional Vector Similarity Search: From Time Series to Deep Network Embeddings},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384402},
doi = {10.1145/3318464.3384402},
abstract = {Similarity search is an important and challenging problem that is typically modeled as nearest neighbor search in high dimensional space, where objects are represented as high dimensional vectors and their (dis)similarity is evaluated using a distance measure such as the Euclidean distance.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2829–2832},
numpages = {4},
keywords = {indexing, high-dimensional vectors, similarity search, nearest neighbor, data series},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384403,
author = {Makait, Hendrik},
title = {Rethinking Message Brokers on RDMA and NVM},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384403},
doi = {10.1145/3318464.3384403},
abstract = {Over the last years, message brokers have become an important part of enterprise systems. As microservice architectures gain popularity and the need to analyze data produced by these services grows, companies increasingly rely on message brokers to orchestrate the flow of events between different applications as well as between data-producing services and streaming engines that analyze the data in real-time.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2833–2835},
numpages = {3},
keywords = {message broker, NVRAM, messaging, RDMA, distributed, modern hardware},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384404,
author = {Chen, Yiru},
title = {Monte Carlo Tree Search for Generating Interactive Data Analysis Interfaces},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384404},
doi = {10.1145/3318464.3384404},
abstract = {Interactive tools like user interfaces help democratize data access for end-users by hiding underlying programming details and exposing the necessary widget interface to users. Since customized interfaces are costly to build, automated interface generation is desirable. SQL is the dominant way to analyze data and there already exists logs to analyze data. Previous work proposed a syntactic approach to analyze structural changes in SQL query logs and automatically generates a set of widgets to express the changes. However, they do not consider layout usability and the sequential order of queries in the log. We propose to adopt Monte Carlo Tree Search(MCTS) to search for the optimal interface that accounts for hierarchical layout as well as the usability in terms of how easy to express the query log.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2837–2839},
numpages = {3},
keywords = {interface generation, human data interaction, Monte Carlo tree search},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384405,
author = {Mohammed, Haneen},
title = {Continuous Prefetch for Interactive Data Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384405},
doi = {10.1145/3318464.3384405},
abstract = {Interactive data visualization and exploration (DVE) applications, such as the one in Figure 1, have rapidly grown in popularity with use cases in numerous sectors [2, 4, 9, 11, 15]. Like typical web services, DVE applications may be run on heterogeneous client devices and networks, with users expecting fast response times under 100 ms [12]. However, the resource demands of DVE applications are magnified and highly unpredictable, making it difficult to achieve such interactivity.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2841–2843},
numpages = {3},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384406,
author = {Jahangiri, Shiva},
title = {Re-Evaluating the Performance Trade-Offs for Hash-Based Multi-Join Queries},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384406},
doi = {10.1145/3318464.3384406},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2845–2847},
numpages = {3},
keywords = {query plan, multi-join queries, performance evaluation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384407,
author = {Zhang, Xiaozhong},
title = {Interactive View Recommendation},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384407},
doi = {10.1145/3318464.3384407},
abstract = {Existing view recommendation approaches proposed a variety of utility functions in selecting useful views. Even though each utility function might be suitable for specific scenarios, identifying the most appropriate ones, along with their tunable parameters, which represent the user's intention during an exploration, is a challenge for both expert and non-expert users. This paper presents an attempt towards interactive view recommendation that automatically discovers the utility function composition during an exploration that best matches the user's intentions and exploration task.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2849–2851},
numpages = {3},
keywords = {personalized recommendation, data visualization, data analysis, data exploration, active learning, view utility function},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384408,
author = {Jagadeesan, Meena and Tanzer, Garrett},
title = {From Worst-Case to Average-Case Analysis: Accurate Latency Predictions for Key-Value Storage Engines},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384408},
doi = {10.1145/3318464.3384408},
abstract = {Selecting the optimal storage engine and tuning for an application requires comparing the latency of diverse workloads executed on different data structures. In this work, we start to develop an average-case analysis of the performance of storage engines that can achieve significantly more accurate predictions than existing worst-case models. We propose a distribution-aware framework to predict the latency of diverse workloads executed on a vast number of data structures. As a case study, we use our framework to produce cost models for a diverse family of key-value storage engine tunings, and verify our models on RocksDB and WiredTiger.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2853–2855},
numpages = {3},
keywords = {average-case analysis, key-value stores, B-tree, log-structured merge tree, cost models},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384409,
author = {Hao, Kongzhang and Lai, Longbin},
title = {Towards the Scheduling of Vertex-Constrained Multi Subgraph Matching Query},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384409},
doi = {10.1145/3318464.3384409},
abstract = {Subgraph matching is one of the most fundamental problems in graph database, which is associated with a wide spectrum of applications. Researchers have primarily devoted their efforts to improving performance for individual query, while we often need to compute multiple queries all at once in practice. In this paper, we study the problem of vertex-constrained multi subgraph matching query (vMSQ), where we propose a novel scheduling algorithm for processing multiple queries in parallel, while taking into considerations of load balance and maximum possible sharing of computation.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2857–2859},
numpages = {3},
keywords = {query optimization, query planning, graph pattern matching, graph database},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384410,
author = {Ma, William},
title = {Serverless Query Processing on a Budget},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384410},
doi = {10.1145/3318464.3384410},
abstract = {Relational query processing is an ideal candidate for serverless computation with its stateless, idempotent, and short-lived properties. However, current serverless offerings for query processing neither provide millisecond-based pricing nor allow users to optimize the cost of their queries. To have tradeoffs between the cost and performance of their queries, users are limited to using traditional serverful approaches, which we demonstrate to be 50% slower at approximately the same cost as serverless approaches. We propose a model that will allow service providers to dynamically provision clusters to achieve their users' desired time-cost tradeoffs.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2861–2863},
numpages = {3},
keywords = {query planning, serverless, spark},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384411,
author = {Slavitch, Noah},
title = {Workload-Aware Column Imprints},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384411},
doi = {10.1145/3318464.3384411},
abstract = {In-memory columnar databases use indexes to accelerate highly selective queries. The additional storage requirement of indexes becomes prohibitive when kept in memory. For example, an inverted index requires as much space as the column itself. Column Imprints (CI) have been proposed as a space-efficient structure that supports range queries. We examine the limitations of CI and we suggest three enhancements for in-memory databases. We propose a workload-aware approach which considers recent data access patterns when constructing CI. We optimize the histogram towards reducing false positives and cache misses for highly selective queries. We propose efficient algorithms to construct our data structures. Preliminary experiments confirm that: 1) our workload-aware imprints reduce the cache lines scanned anywhere from 30% to 50% when compared to the original CI, and 2) have significantly smaller storage requirements.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2865–2867},
numpages = {3},
keywords = {histogram, column imprints, range query, workload analysis, in-memory columnar database},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384412,
author = {Adam, Justus},
title = {Towards Scalable UDTFs in Noria},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384412},
doi = {10.1145/3318464.3384412},
abstract = {User Defined Functions (UDF) are an important and powerful extension point for database queries. Systems using incremental materialized views largely do not support UDFs because they cannot easily be incrementalized. In this work we design single-tuple UDF and User Defined Aggregates (UDA) interfaces for Noria, a state-of-the art dataflow system with incremental materialized views. We also add limited support for User Defined Table Functions (UDTF), by compiling them to query fragments. We show our UDTFs scale by implementing a motivational example used Friedman et al.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2869–2871},
numpages = {3},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384413,
author = {Shi, Jia},
title = {Column Partition and Permutation for Run Length Encoding in Columnar Databases},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384413},
doi = {10.1145/3318464.3384413},
abstract = {Effective compression is essential when databases are used in Big Data applications. For in-memory columnar databases, compression can help to load the columns faster and speed up query evaluation. In this paper, we consider compressing columns using the Run Length Encoding (RLE). This algorithm encodes each region with identical value using a single run. The question we study in this paper is 'how to rearrange table columns for better compression?' We observe that not every column of a table benefits from column compression in an ideal column arrangement. Because finding the optimal column arrangement is NP-hard, we propose an incremental heuristic that identifies the set of columns to be compressed and the order of rows that offer a better compression ratio. Our preliminary experiments confirm that our algorithm improves the compression rate by up to 25% on test data, compared with compressing all columns of a table.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2873–2874},
numpages = {2},
keywords = {columnar database, data compression},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384414,
author = {Li, Wanxin},
title = {Supporting Database Constraints in Synthetic Data Generation Based on Generative Adversarial Networks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384414},
doi = {10.1145/3318464.3384414},
abstract = {With unprecedented development in machine learning algorithms, it is crucial to have available large amount of data to verify the correctness and efficiency of these algorithms. Due to privacy concerns, we may not always have enough real data to use. In our research, we focus on data synthesization for relational databases where the database constraints of the original data must be imposed to the generated data. To the best of our knowledge, no study has been conducted on supporting database constraints in synthetic data generation. We offer solutions by designing extensions to Tabular Generative Adversarial Network algorithm. We implemented a prototype for our approach, and compared the performance of different extensions by experiments. Related work on synthetic data generation includes classical statistical methods and neural network approaches. Synthetic Data Vault is developed using classical statistical methods. It uses Kolmogorov-Smirnov test to select the best statistical distribution to describe columnar data. TableGAN and Tabular GAN use neural networks to minimize cross entropy or Kullback-Leibler divergence on marginal distributions. The main challenges to our research problem are: Classical statistical distributions cannot describe complex and mixed distributions in relational databases. Database constraints are non-differentiable. Neural networks require loss functions to be differentiable.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2875–2877},
numpages = {3},
keywords = {data synthesization, database constraint, generative adversarial network, logic constraint},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384415,
author = {Spiegel, Jacob},
title = {An Evaluation of Methods of Compressing Doubles},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384415},
doi = {10.1145/3318464.3384415},
abstract = {Data compression is a problem with far-reaching implications across science and industry. In the era of big data, methods for efficient compression are crucial to achieve compact data representation, low-latency data transfers, and high- throughput during query execution. Due to the explosion of Internet-of-Things applications, a large portion of this data is in the form of double-precision floating-point numbers. Despite the plethora of methods for compression, a comprehensive evaluation across real-world data and applications is still missing. In this paper, we perform such a comparison of methods and evaluate their performance in terms of compression ratio and throughput achieved across two dataset repositories of time series and featurized machine-learning problems, as well as on a dataset of machine logs.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2879–2881},
numpages = {3},
keywords = {compression, doubles, data compression, double},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384416,
author = {Band, Neil},
title = {MemFlow: Memory-Aware Distributed Deep Learning},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384416},
doi = {10.1145/3318464.3384416},
abstract = {As the number of layers and the amount of training data increases, the trend is to train deep neural networks in parallel across devices. In such scenarios, neural network training is increasingly bottlenecked by high memory requirements posed by intermediate results, or feature maps, that are produced during the forward pass and consumed during the backward pass. We recognize that the best-performing device parallelization configurations should consider memory usage in addition to the canonical metric of computation time. Towards this we introduce MemFlow, an optimization framework for distributed deep learning that performs joint optimization over memory usage and computation time when searching for a parallelization strategy. MemFlow consists of: (i) a task graph with memory usage estimates; (ii) a memory-aware execution simulator; and (iii) a Markov Chain Monte Carlo search algorithm that considers various degrees of recomputation i.e., discarding feature maps during the forward pass and recomputing them during the backward pass. Our experiments demonstrate that under memory constraints, MemFlow can readily locate valid and superior parallelization strategies unattainable with previous frameworks.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2883–2885},
numpages = {3},
keywords = {recomputation, neural network training, model parallelization, distributed machine learning, deep neural networks, memory optimization},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3318464.3384417,
author = {Waghray, Kunal},
title = {JSON Schema Matching: Empirical Observations},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3384417},
doi = {10.1145/3318464.3384417},
abstract = {Database schema specifies the desired logical organization of the data it stores. A major challenge in database integration is that of schema matching [10, 12], which seeks to determine schema elements in different databases that correspond to the same real world entity. For example, a simple matcher may determine that the attribute ID in one schema is semantically equivalent to Identification in another.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2887–2889},
numpages = {3},
keywords = {JSON, schema matching, semi-structured data},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

