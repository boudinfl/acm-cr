@inproceedings{10.1145/375663.375664,
author = {Han, Jiawei and Pei, Jian and Dong, Guozhu and Wang, Ke},
title = {Efficient Computation of Iceberg Cubes with Complex Measures},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375664},
doi = {10.1145/375663.375664},
abstract = {It is often too expensive to compute and materialize a complete high-dimensional data cube. Computing an iceberg cube, which contains only aggregates above certain thresholds, is an effective way to derive nontrivial multi-dimensional aggregations for OLAP and data mining.In this paper, we study efficient methods for computing iceberg cubes with some popularly used complex measures, such as average, and develop a methodology that adopts a weaker but anti-monotonic condition for testing and pruning search space. In particular, for efficient computation of iceberg cubes with the average measure, we propose a top-k average pruning method and extend two previously studied methods, Apriori and BUC, to Top-k Apriori and Top-k BUC. To further improve the performance, an interesting hypertree structure, called H-tree, is designed and a new iceberg cubing method, called Top-k H-Cubing, is developed. Our performance study shows that Top-k BUC and Top-k H-Cubing are two promising candidates for scalable computation, and Top-k H-Cubing has better performance in most cases.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375664,
author = {Han, Jiawei and Pei, Jian and Dong, Guozhu and Wang, Ke},
title = {Efficient Computation of Iceberg Cubes with Complex Measures},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375664},
doi = {10.1145/376284.375664},
abstract = {It is often too expensive to compute and materialize a complete high-dimensional data cube. Computing an iceberg cube, which contains only aggregates above certain thresholds, is an effective way to derive nontrivial multi-dimensional aggregations for OLAP and data mining.In this paper, we study efficient methods for computing iceberg cubes with some popularly used complex measures, such as average, and develop a methodology that adopts a weaker but anti-monotonic condition for testing and pruning search space. In particular, for efficient computation of iceberg cubes with the average measure, we propose a top-k average pruning method and extend two previously studied methods, Apriori and BUC, to Top-k Apriori and Top-k BUC. To further improve the performance, an interesting hypertree structure, called H-tree, is designed and a new iceberg cubing method, called Top-k H-Cubing, is developed. Our performance study shows that Top-k BUC and Top-k H-Cubing are two promising candidates for scalable computation, and Top-k H-Cubing has better performance in most cases.},
journal = {SIGMOD Rec.},
month = may,
pages = {1–12},
numpages = {12}
}

@inproceedings{10.1145/375663.375665,
author = {Gehrke, Johannes and Korn, Flip and Srivastava, Divesh},
title = {On Computing Correlated Aggregates over Continual Data Streams},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375665},
doi = {10.1145/375663.375665},
abstract = {In many applications from telephone fraud detection to network management, data arrives in a stream, and there is a need to maintain a variety of statistical summary information about a large number of customers in an online fashion. At present, such applications maintain basic aggregates such as running extrema values (MIN, MAX), averages, standard deviations, etc., that can be computed over data streams with limited space in a straightforward way. However, many applications require knowledge of more complex aggregates relating different attributes, so-called correlated aggregates. As an example, one might be interested in computing the percentage of international phone calls that are longer than the average duration of a domestic phone call. Exact computation of this aggregate requires multiple passes over the data stream, which is infeasible.We propose single-pass techniques for approximate computation of correlated aggregates over both landmark and sliding window views of a data stream of tuples, using a very limited amount of space. We consider both the case where the independent aggregate (average duration in the example above) is an extrema value and the case where it is an average value, with any standard aggregate as the dependent aggregate; these can be used as building blocks for more sophisticated aggregates. We present an extensive experimental study based on some real and a wide variety of synthetic data sets to demonstrate the accuracy of our techniques. We show that this effectiveness is explained by the fact that our techniques exploit monotonicity and convergence properties of aggregates over data streams.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {13–24},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375665,
author = {Gehrke, Johannes and Korn, Flip and Srivastava, Divesh},
title = {On Computing Correlated Aggregates over Continual Data Streams},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375665},
doi = {10.1145/376284.375665},
abstract = {In many applications from telephone fraud detection to network management, data arrives in a stream, and there is a need to maintain a variety of statistical summary information about a large number of customers in an online fashion. At present, such applications maintain basic aggregates such as running extrema values (MIN, MAX), averages, standard deviations, etc., that can be computed over data streams with limited space in a straightforward way. However, many applications require knowledge of more complex aggregates relating different attributes, so-called correlated aggregates. As an example, one might be interested in computing the percentage of international phone calls that are longer than the average duration of a domestic phone call. Exact computation of this aggregate requires multiple passes over the data stream, which is infeasible.We propose single-pass techniques for approximate computation of correlated aggregates over both landmark and sliding window views of a data stream of tuples, using a very limited amount of space. We consider both the case where the independent aggregate (average duration in the example above) is an extrema value and the case where it is an average value, with any standard aggregate as the dependent aggregate; these can be used as building blocks for more sophisticated aggregates. We present an extensive experimental study based on some real and a wide variety of synthetic data sets to demonstrate the accuracy of our techniques. We show that this effectiveness is explained by the fact that our techniques exploit monotonicity and convergence properties of aggregates over data streams.},
journal = {SIGMOD Rec.},
month = may,
pages = {13–24},
numpages = {12}
}

@inproceedings{10.1145/375663.375666,
author = {Ng, Raymond T. and Wagner, Alan and Yin, Yu},
title = {Iceberg-Cube Computation with PC Clusters},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375666},
doi = {10.1145/375663.375666},
abstract = {In this paper, we investigate the approach of using low cost PC cluster to parallelize the computation of iceberg-cube queries. We concentrate on techniques directed towards online querying of large, high-dimensional datasets where it is assumed that the total cube has net been precomputed. The algorithmic space we explore considers trade-offs between parallelism, computation and I/0. Our main contribution is the development and a comprehensive evaluation of various novel, parallel algorithms. Specifically: (1) Algorithm RP is a straightforward parallel version of BUC [BR99]; (2) Algorithm BPP attempts to reduce I/0 by outputting results in a more efficient way; (3) Algorithm ASL, which maintains cells in a cuboid in a skiplist, is designed to put the utmost priority on load balancing; and (4) alternatively, Algorithm PT load-balances by using binary partitioning to divide the cube lattice as evenly as possible.We present a thorough performance evaluation on all these algorithms on a variety of parameters, including the dimensionality of the cube, the sparseness of the cube, the selectivity of the constraints, the number of processors, and the size of the dataset. A key finding is that it is not a one-algorithm-fit-all situation. We recommend a “recipe” which uses PT as the default algorithm, but may also deploy ASL under specific circumstances.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {25–36},
numpages = {12},
keywords = {parallel computation, OLAP},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375666,
author = {Ng, Raymond T. and Wagner, Alan and Yin, Yu},
title = {Iceberg-Cube Computation with PC Clusters},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375666},
doi = {10.1145/376284.375666},
abstract = {In this paper, we investigate the approach of using low cost PC cluster to parallelize the computation of iceberg-cube queries. We concentrate on techniques directed towards online querying of large, high-dimensional datasets where it is assumed that the total cube has net been precomputed. The algorithmic space we explore considers trade-offs between parallelism, computation and I/0. Our main contribution is the development and a comprehensive evaluation of various novel, parallel algorithms. Specifically: (1) Algorithm RP is a straightforward parallel version of BUC [BR99]; (2) Algorithm BPP attempts to reduce I/0 by outputting results in a more efficient way; (3) Algorithm ASL, which maintains cells in a cuboid in a skiplist, is designed to put the utmost priority on load balancing; and (4) alternatively, Algorithm PT load-balances by using binary partitioning to divide the cube lattice as evenly as possible.We present a thorough performance evaluation on all these algorithms on a variety of parameters, including the dimensionality of the cube, the sparseness of the cube, the selectivity of the constraints, the number of processors, and the size of the dataset. A key finding is that it is not a one-algorithm-fit-all situation. We recommend a “recipe” which uses PT as the default algorithm, but may also deploy ASL under specific circumstances.},
journal = {SIGMOD Rec.},
month = may,
pages = {25–36},
numpages = {12},
keywords = {parallel computation, OLAP}
}

@inproceedings{10.1145/375663.375668,
author = {Aggarwal, Charu C. and Yu, Philip S.},
title = {Outlier Detection for High Dimensional Data},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375668},
doi = {10.1145/375663.375668},
abstract = {The outlier detection problem has important applications in the field of fraud detection, network robustness analysis, and intrusion detection. Most such applications are high dimensional domains in which the data can contain hundreds of dimensions. Many recent algorithms use concepts of proximity in order to find outliers based on their relationship to the rest of the data. However, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective of proximity-based definitions. Consequently, for high dimensional data, the notion of finding meaningful outliers becomes substantially more complex and non-obvious. In this paper, we discuss new techniques for outlier detection which find the outliers by studying the behavior of projections from the data set.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {37–46},
numpages = {10},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375668,
author = {Aggarwal, Charu C. and Yu, Philip S.},
title = {Outlier Detection for High Dimensional Data},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375668},
doi = {10.1145/376284.375668},
abstract = {The outlier detection problem has important applications in the field of fraud detection, network robustness analysis, and intrusion detection. Most such applications are high dimensional domains in which the data can contain hundreds of dimensions. Many recent algorithms use concepts of proximity in order to find outliers based on their relationship to the rest of the data. However, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective of proximity-based definitions. Consequently, for high dimensional data, the notion of finding meaningful outliers becomes substantially more complex and non-obvious. In this paper, we discuss new techniques for outlier detection which find the outliers by studying the behavior of projections from the data set.},
journal = {SIGMOD Rec.},
month = may,
pages = {37–46},
numpages = {10}
}

@inproceedings{10.1145/375663.375669,
author = {Rinfret, Denis and O'Neil, Patrick and O'Neil, Elizabeth},
title = {Bit-Sliced Index Arithmetic},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375669},
doi = {10.1145/375663.375669},
abstract = {The bit-sliced index (BSI) was originally defined in [ONQ97]. The current paper introduces the concept of BSI arithmetic. For any two BSI's X and Y on a table T, we show how to efficiently generate new BSI's Z, V, and W, such that Z = X + Y, V = X - Y, and W = MIN(X, Y); this means that if a row r in T has a value x represented in BSI X and a value y in BSI Y, the value for r in BSI Z will be x + y, the value in V will be x - y and the value in W will be MIN(x, y). Since a bitmap representing a set of rows is the simplest bit-sliced index, BSI arithmetic is the most straightforward way to determine multisets of rows (with duplicates) resulting from the SQL clauses UNION ALL (addition), EXCEPT ALL (subtraction), and INTERSECT ALL (min) (see [OO00, DB2SQL] for definitions of these clauses). Another contribution of the current paper is to generalize BSI range restrictions from [ONQ97] to a new non-Boolean form: to determine the top k BSI-valued rows, for ally meaningful value k between one and the total number of rows in T. Together with bit-sliced addition, this permits us to solve a common basic problem of text retrieval: given an object-relational table T of rows representing documents, with a collection type column K representing keyword terms, we demonstrate an efficient algorithm to find k documents that share the largest number of terms with some query list Q of terms. A great deal of published work on such problems exists in the Information Retrieval (IR) field. The algorithm we introduce, which we call Bit-Sliced Term-Matching, or BSTM, uses an approach comparable in performance to the most efficient known IR algorithm, a major improvement on current DBMS text searching algorithms, with the advantage that it uses only indexing we propose for native database operations.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {47–57},
numpages = {11},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375669,
author = {Rinfret, Denis and O'Neil, Patrick and O'Neil, Elizabeth},
title = {Bit-Sliced Index Arithmetic},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375669},
doi = {10.1145/376284.375669},
abstract = {The bit-sliced index (BSI) was originally defined in [ONQ97]. The current paper introduces the concept of BSI arithmetic. For any two BSI's X and Y on a table T, we show how to efficiently generate new BSI's Z, V, and W, such that Z = X + Y, V = X - Y, and W = MIN(X, Y); this means that if a row r in T has a value x represented in BSI X and a value y in BSI Y, the value for r in BSI Z will be x + y, the value in V will be x - y and the value in W will be MIN(x, y). Since a bitmap representing a set of rows is the simplest bit-sliced index, BSI arithmetic is the most straightforward way to determine multisets of rows (with duplicates) resulting from the SQL clauses UNION ALL (addition), EXCEPT ALL (subtraction), and INTERSECT ALL (min) (see [OO00, DB2SQL] for definitions of these clauses). Another contribution of the current paper is to generalize BSI range restrictions from [ONQ97] to a new non-Boolean form: to determine the top k BSI-valued rows, for ally meaningful value k between one and the total number of rows in T. Together with bit-sliced addition, this permits us to solve a common basic problem of text retrieval: given an object-relational table T of rows representing documents, with a collection type column K representing keyword terms, we demonstrate an efficient algorithm to find k documents that share the largest number of terms with some query list Q of terms. A great deal of published work on such problems exists in the Information Retrieval (IR) field. The algorithm we introduce, which we call Bit-Sliced Term-Matching, or BSTM, uses an approach comparable in performance to the most efficient known IR algorithm, a major improvement on current DBMS text searching algorithms, with the advantage that it uses only indexing we propose for native database operations.},
journal = {SIGMOD Rec.},
month = may,
pages = {47–57},
numpages = {11}
}

@inproceedings{10.1145/375663.375670,
author = {Greenwald, Michael and Khanna, Sanjeev},
title = {Space-Efficient Online Computation of Quantile Summaries},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375670},
doi = {10.1145/375663.375670},
abstract = {An ∈-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of ∈N.We present a new online algorithm for computing∈-approximate quantile summaries of very large data sequences. The algorithm has a worst-case space requirement of Ο(1undefined∈ log(∈N)). This improves upon the previous best result of Ο(1undefined∈ log2(∈N)). Moreover, in contrast to earlier deterministic algorithms, our algorithm does not require a priori knowledge of the length of the input sequence.Finally, the actual space bounds obtained on experimental data are significantly better than the worst case guarantees of our algorithm as well as the observed space requirements of earlier algorithms.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {58–66},
numpages = {9},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375670,
author = {Greenwald, Michael and Khanna, Sanjeev},
title = {Space-Efficient Online Computation of Quantile Summaries},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375670},
doi = {10.1145/376284.375670},
abstract = {An ∈-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of ∈N.We present a new online algorithm for computing∈-approximate quantile summaries of very large data sequences. The algorithm has a worst-case space requirement of Ο(1undefined∈ log(∈N)). This improves upon the previous best result of Ο(1undefined∈ log2(∈N)). Moreover, in contrast to earlier deterministic algorithms, our algorithm does not require a priori knowledge of the length of the input sequence.Finally, the actual space bounds obtained on experimental data are significantly better than the worst case guarantees of our algorithm as well as the observed space requirements of earlier algorithms.},
journal = {SIGMOD Rec.},
month = may,
pages = {58–66},
numpages = {9}
}

@inproceedings{10.1145/375663.375671,
author = {Ipeirotis, Panagiotis G. and Gravano, Luis and Sahami, Mehran},
title = {Probe, Count, and Classify: Categorizing Hidden Web Databases},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375671},
doi = {10.1145/375663.375671},
abstract = {The contents of many valuable web-accessible databases are only accessible through search interfaces and are hence invisible to traditional web “crawlers.” Recent studies have estimated the size of this “hidden web” to be 500 billion pages, while the size of the “crawlable” web is only an estimated two billion pages. Recently, commercial web sites have started to manually organize web-accessible databases into Yahoo!-like hierarchical classification schemes. In this paper, we introduce a method for automating this classification process by using a small number of query probes. To classify a database, our algorithm does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {67–78},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375671,
author = {Ipeirotis, Panagiotis G. and Gravano, Luis and Sahami, Mehran},
title = {Probe, Count, and Classify: Categorizing Hidden Web Databases},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375671},
doi = {10.1145/376284.375671},
abstract = {The contents of many valuable web-accessible databases are only accessible through search interfaces and are hence invisible to traditional web “crawlers.” Recent studies have estimated the size of this “hidden web” to be 500 billion pages, while the size of the “crawlable” web is only an estimated two billion pages. Recently, commercial web sites have started to manually organize web-accessible databases into Yahoo!-like hierarchical classification schemes. In this paper, we introduce a method for automating this classification process by using a small number of query probes. To classify a database, our algorithm does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.},
journal = {SIGMOD Rec.},
month = may,
pages = {67–78},
numpages = {12}
}

@inproceedings{10.1145/375663.375672,
author = {Breunig, Markus M. and Kriegel, Hans-Peter and Kr\"{o}ger, Peer and Sander, J\"{o}rg},
title = {Data Bubbles: Quality Preserving Performance Boosting for Hierarchical Clustering},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375672},
doi = {10.1145/375663.375672},
abstract = {In this paper, we investigate how to scale hierarchical clustering methods (such as OPTICS) to extremely large databases by utilizing data compression methods (such as BIRCH or random sampling). We propose a three step procedure: 1) compress the data into suitable representative objects; 2) apply the hierarchical clustering algorithm only to these objects; 3) recover the clustering structure for the whole data set, based on the result for the compressed data. The key issue in this approach is to design compressed data items such that not only a hierarchical clustering algorithm can be applied, but also that they contain enough information to infer the clustering structure of the original data set in the third step. This is crucial because the results of hierarchical clustering algorithms, when applied naively to a random sample or to the clustering features (CFs) generated by BIRCH, deteriorate rapidly for higher compression rates. This is due to three key problems, which we identify. To solve these problems, we propose an efficient post-processing step and the concept of a Data Bubble as a special kind of compressed data item. Applying OPTICS to these Data Bubbles allows us to recover a very accurate approximation of the clustering structure of a large data set even for very high compression rates. A comprehensive performance and quality evaluation shows that we only trade very little quality of the clustering result for a great increase in performance.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {79–90},
numpages = {12},
keywords = {sampling, clustering, database mining, data compression},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375672,
author = {Breunig, Markus M. and Kriegel, Hans-Peter and Kr\"{o}ger, Peer and Sander, J\"{o}rg},
title = {Data Bubbles: Quality Preserving Performance Boosting for Hierarchical Clustering},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375672},
doi = {10.1145/376284.375672},
abstract = {In this paper, we investigate how to scale hierarchical clustering methods (such as OPTICS) to extremely large databases by utilizing data compression methods (such as BIRCH or random sampling). We propose a three step procedure: 1) compress the data into suitable representative objects; 2) apply the hierarchical clustering algorithm only to these objects; 3) recover the clustering structure for the whole data set, based on the result for the compressed data. The key issue in this approach is to design compressed data items such that not only a hierarchical clustering algorithm can be applied, but also that they contain enough information to infer the clustering structure of the original data set in the third step. This is crucial because the results of hierarchical clustering algorithms, when applied naively to a random sample or to the clustering features (CFs) generated by BIRCH, deteriorate rapidly for higher compression rates. This is due to three key problems, which we identify. To solve these problems, we propose an efficient post-processing step and the concept of a Data Bubble as a special kind of compressed data item. Applying OPTICS to these Data Bubbles allows us to recover a very accurate approximation of the clustering structure of a large data set even for very high compression rates. A comprehensive performance and quality evaluation shows that we only trade very little quality of the clustering result for a great increase in performance.},
journal = {SIGMOD Rec.},
month = may,
pages = {79–90},
numpages = {12},
keywords = {sampling, database mining, data compression, clustering}
}

@inproceedings{10.1145/375663.375673,
author = {Joshi, Mahesh V. and Agarwal, Ramesh C. and Kumar, Vipin},
title = {Mining Needle in a Haystack: Classifying Rare Classes via Two-Phase Rule Induction},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375673},
doi = {10.1145/375663.375673},
abstract = {Learning models to classify rarely occurring target classes is an important problem with applications in network intrusion detection, fraud detection, or deviation detection in general. In this paper, we analyze our previously proposed two-phase rule induction method in the context of learning complete and precise signatures of rare classes. The key feature of our method is that it separately conquers the objectives of achieving high recall and high precision for the given target class. The first phase of the method aims for high recall by inducing rules with high support and a reasonable level of accuracy. The second phase then tries to improve the precision by learning rules to remove false positives in the collection of the records covered by the first phase rules. Existing sequential covering techniques try to achieve high precision for each individual disjunct learned. In this paper, we claim that such approach is inadequate for rare classes, because of two problems: splintered false positives and error-prone small disjuncts. Motivated by the strengths of our two-phase design, we design various synthetic data models to identify and analyze the situations in which two state-of-the-art methods, RIPPER and C4.5 rules, either fail to learn a model or learn a very poor model. In all these situations, our two-phase approach learns a model with significantly better recall and precision levels. We also present a comparison of the three methods on a challenging real-life network intrusion detection dataset. Our method is significantly better or comparable to the best competitor in terms of achieving better balance between recall and precision.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {91–102},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375673,
author = {Joshi, Mahesh V. and Agarwal, Ramesh C. and Kumar, Vipin},
title = {Mining Needle in a Haystack: Classifying Rare Classes via Two-Phase Rule Induction},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375673},
doi = {10.1145/376284.375673},
abstract = {Learning models to classify rarely occurring target classes is an important problem with applications in network intrusion detection, fraud detection, or deviation detection in general. In this paper, we analyze our previously proposed two-phase rule induction method in the context of learning complete and precise signatures of rare classes. The key feature of our method is that it separately conquers the objectives of achieving high recall and high precision for the given target class. The first phase of the method aims for high recall by inducing rules with high support and a reasonable level of accuracy. The second phase then tries to improve the precision by learning rules to remove false positives in the collection of the records covered by the first phase rules. Existing sequential covering techniques try to achieve high precision for each individual disjunct learned. In this paper, we claim that such approach is inadequate for rare classes, because of two problems: splintered false positives and error-prone small disjuncts. Motivated by the strengths of our two-phase design, we design various synthetic data models to identify and analyze the situations in which two state-of-the-art methods, RIPPER and C4.5 rules, either fail to learn a model or learn a very poor model. In all these situations, our two-phase approach learns a model with significantly better recall and precision levels. We also present a comparison of the three methods on a challenging real-life network intrusion detection dataset. Our method is significantly better or comparable to the best competitor in terms of achieving better balance between recall and precision.},
journal = {SIGMOD Rec.},
month = may,
pages = {91–102},
numpages = {12}
}

@inproceedings{10.1145/375663.375674,
author = {Fernandez, Mary and Morishima, Atsuyuki and Suciu, Dan},
title = {Efficient Evaluation of XML Middle-Ware Queries},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375674},
doi = {10.1145/375663.375674},
abstract = {We address the problem of efficiently constructing materialized XML views of relational databases. In our setting, the XML view is specified by a query in the declarative query language of a middle-ware system, called SilkRoute. The middle-ware system evaluates a query by sending one or more SQL queries to the target relational database, integrating the resulting tuple streams, and adding the XML tags. We focus on how to best choose the SQL queries, without having control over the target RDBMS.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {103–114},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375674,
author = {Fernandez, Mary and Morishima, Atsuyuki and Suciu, Dan},
title = {Efficient Evaluation of XML Middle-Ware Queries},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375674},
doi = {10.1145/376284.375674},
abstract = {We address the problem of efficiently constructing materialized XML views of relational databases. In our setting, the XML view is specified by a query in the declarative query language of a middle-ware system, called SilkRoute. The middle-ware system evaluates a query by sending one or more SQL queries to the target relational database, integrating the resulting tuple streams, and adding the XML tags. We focus on how to best choose the SQL queries, without having control over the target RDBMS.},
journal = {SIGMOD Rec.},
month = may,
pages = {103–114},
numpages = {12}
}

@inproceedings{10.1145/375663.375677,
author = {Fabret, Fran\c{c}oise and Jacobsen, H. Arno and Llirbat, Fran\c{c}ois and Pereira, Jo\u{a}o and Ross, Kenneth A. and Shasha, Dennis},
title = {Filtering Algorithms and Implementation for Very Fast Publish/Subscribe Systems},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375677},
doi = {10.1145/375663.375677},
abstract = {Publish/Subscribe is the paradigm in which users express long-term interests (“subscriptions”) and some agent “publishes” events (e.g., offers). The job of Publish/Subscribe software is to send events to the owners of subscriptions satisfied by those events. For example, a user subscription may consist of an interest in an airplane of a certain type, not to exceed a certain price. A published event may consist of an offer of an airplane with certain properties including price. Each subscription consists of a conjunction of (attribute, comparison operator, value) predicates. A subscription closely resembles a trigger in that it is a long-lived conditional query associated with an action (usually, informing the subscriber). However, it is less general than a trigger so novel data structures and implementations may enable the creation of more scalable, high performance publish/subscribe systems. This paper describes an attempt at the construction of such algorithms and its implementation. Using a combination of data structures, application-specific caching policies, and application-specific query processing our system can handle 600 events per second for a typical workload containing 6 million subscriptions.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {115–126},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375677,
author = {Fabret, Fran\c{c}oise and Jacobsen, H. Arno and Llirbat, Fran\c{c}ois and Pereira, Jo\u{a}o and Ross, Kenneth A. and Shasha, Dennis},
title = {Filtering Algorithms and Implementation for Very Fast Publish/Subscribe Systems},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375677},
doi = {10.1145/376284.375677},
abstract = {Publish/Subscribe is the paradigm in which users express long-term interests (“subscriptions”) and some agent “publishes” events (e.g., offers). The job of Publish/Subscribe software is to send events to the owners of subscriptions satisfied by those events. For example, a user subscription may consist of an interest in an airplane of a certain type, not to exceed a certain price. A published event may consist of an offer of an airplane with certain properties including price. Each subscription consists of a conjunction of (attribute, comparison operator, value) predicates. A subscription closely resembles a trigger in that it is a long-lived conditional query associated with an action (usually, informing the subscriber). However, it is less general than a trigger so novel data structures and implementations may enable the creation of more scalable, high performance publish/subscribe systems. This paper describes an attempt at the construction of such algorithms and its implementation. Using a combination of data structures, application-specific caching policies, and application-specific query processing our system can handle 600 events per second for a typical workload containing 6 million subscriptions.},
journal = {SIGMOD Rec.},
month = may,
pages = {115–126},
numpages = {12}
}

@inproceedings{10.1145/375663.375678,
author = {Slivinskas, Giedrius and Jensen, Christian S. and Snodgrass, Richard Thomas},
title = {Adaptable Query Optimization and Evaluation in Temporal Middleware},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375678},
doi = {10.1145/375663.375678},
abstract = {Time-referenced data are pervasive in most real-world databases. Recent advances in temporal query languages show that such database applications may benefit substantially from built-in temporal support in the DBMS. To achieve this, temporal query optimization and evaluation mechanisms must be provided, either within the DBMS proper or as a source level translation from temporal queries to conventional SQL. This paper proposes a new approach: using a middleware component on top of a conventional DBMS. This component accepts temporal SQL statements and produces a corresponding query plan consisting of algebraic as well as regular SQL parts. The algebraic parts are processed by the middleware, while the SQL parts are processed by the DBMS. The middleware uses performance feedback from the DBMS to adapt its partitioning of subsequent queries into middleware and DBMS parts. The paper describes the architecture and implementation of the temporal middleware component, termed TANGO, which is based on the Volcano extensible query optimizer and the XXL query processing library. Experiments with the system demonstrate the utility of the middleware's internal processing capability and its cost-based mechanism for apportioning the processing between the middleware and the underlying DBMS.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {127–138},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375678,
author = {Slivinskas, Giedrius and Jensen, Christian S. and Snodgrass, Richard Thomas},
title = {Adaptable Query Optimization and Evaluation in Temporal Middleware},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375678},
doi = {10.1145/376284.375678},
abstract = {Time-referenced data are pervasive in most real-world databases. Recent advances in temporal query languages show that such database applications may benefit substantially from built-in temporal support in the DBMS. To achieve this, temporal query optimization and evaluation mechanisms must be provided, either within the DBMS proper or as a source level translation from temporal queries to conventional SQL. This paper proposes a new approach: using a middleware component on top of a conventional DBMS. This component accepts temporal SQL statements and produces a corresponding query plan consisting of algebraic as well as regular SQL parts. The algebraic parts are processed by the middleware, while the SQL parts are processed by the DBMS. The middleware uses performance feedback from the DBMS to adapt its partitioning of subsequent queries into middleware and DBMS parts. The paper describes the architecture and implementation of the temporal middleware component, termed TANGO, which is based on the Volcano extensible query optimizer and the XXL query processing library. Experiments with the system demonstrate the utility of the middleware's internal processing capability and its cost-based mechanism for apportioning the processing between the middleware and the underlying DBMS.},
journal = {SIGMOD Rec.},
month = may,
pages = {127–138},
numpages = {12}
}

@inproceedings{10.1145/375663.375679,
author = {Kim, Kihong and Cha, Sang K. and Kwon, Keunjoo},
title = {Optimizing Multidimensional Index Trees for Main Memory Access},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375679},
doi = {10.1145/375663.375679},
abstract = {Recent studies have shown that cache-conscious indexes such as the CSB+-tree outperform conventional main memory indexes such as the T-tree. The key idea of these cache-conscious indexes is to eliminate most of child pointers from a node to increase the fanout of the tree. When the node size is chosen in the order of the cache block size, this pointer elimination effectively reduces the tree height, and thus improves the cache behavior of the index. However, the pointer elimination cannot be directly applied to multidimensional index structures such as the R-tree, where the size of a key, typically, an MBR (minimum bounding rectangle), is much larger than that of a pointer. Simple elimination of four-byte pointers does not help much to pack more entries in a node.This paper proposes a cache-conscious version of the R-tree called the CR-tree. To pack more entries in a node, the CR-tree compresses MBR keys, which occupy almost 80% of index data in the two-dimensional case. It first represents the coordinates of an MBR key relatively to the lower left corner of its parent MBR to eliminate the leading O's from the relative coordinate representation. Then, it quantizes the relative coordinates with a fixed number of bits to further cut off the trailing less significant bits. Consequently, the CR-tree becomes significantly wider and smaller than the ordinary R-tree. Our experimental and analytical study shows that the two-dimensional CR-tree performs search up to 2.5 times faster than the ordinary R-tree while maintaining similar update performance and consuming about 60% less memory space.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {139–150},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375679,
author = {Kim, Kihong and Cha, Sang K. and Kwon, Keunjoo},
title = {Optimizing Multidimensional Index Trees for Main Memory Access},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375679},
doi = {10.1145/376284.375679},
abstract = {Recent studies have shown that cache-conscious indexes such as the CSB+-tree outperform conventional main memory indexes such as the T-tree. The key idea of these cache-conscious indexes is to eliminate most of child pointers from a node to increase the fanout of the tree. When the node size is chosen in the order of the cache block size, this pointer elimination effectively reduces the tree height, and thus improves the cache behavior of the index. However, the pointer elimination cannot be directly applied to multidimensional index structures such as the R-tree, where the size of a key, typically, an MBR (minimum bounding rectangle), is much larger than that of a pointer. Simple elimination of four-byte pointers does not help much to pack more entries in a node.This paper proposes a cache-conscious version of the R-tree called the CR-tree. To pack more entries in a node, the CR-tree compresses MBR keys, which occupy almost 80% of index data in the two-dimensional case. It first represents the coordinates of an MBR key relatively to the lower left corner of its parent MBR to eliminate the leading O's from the relative coordinate representation. Then, it quantizes the relative coordinates with a fixed number of bits to further cut off the trailing less significant bits. Consequently, the CR-tree becomes significantly wider and smaller than the ordinary R-tree. Our experimental and analytical study shows that the two-dimensional CR-tree performs search up to 2.5 times faster than the ordinary R-tree while maintaining similar update performance and consuming about 60% less memory space.},
journal = {SIGMOD Rec.},
month = may,
pages = {139–150},
numpages = {12}
}

@inproceedings{10.1145/375663.375680,
author = {Keogh, Eamonn and Chakrabarti, Kaushik and Pazzani, Michael and Mehrotra, Sharad},
title = {Locally Adaptive Dimensionality Reduction for Indexing Large Time Series Databases},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375680},
doi = {10.1145/375663.375680},
abstract = {Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data.. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower bounding, but very tight Euclidean distance approximation and show how they can support fast exact searching, and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {151–162},
numpages = {12},
keywords = {content-based retrieval, indexing, dimensionality reduction},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375680,
author = {Keogh, Eamonn and Chakrabarti, Kaushik and Pazzani, Michael and Mehrotra, Sharad},
title = {Locally Adaptive Dimensionality Reduction for Indexing Large Time Series Databases},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375680},
doi = {10.1145/376284.375680},
abstract = {Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data.. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower bounding, but very tight Euclidean distance approximation and show how they can support fast exact searching, and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.},
journal = {SIGMOD Rec.},
month = may,
pages = {151–162},
numpages = {12},
keywords = {indexing, dimensionality reduction, content-based retrieval}
}

@inproceedings{10.1145/375663.375681,
author = {Bohannon, Philip and Mcllroy, Peter and Rastogi, Rajeev},
title = {Main-Memory Index Structures with Fixed-Size Partial Keys},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375681},
doi = {10.1145/375663.375681},
abstract = {The performance of main-memory index structures is increasingly determined by the number of CPU cache misses incurred when traversing the index. When keys are stored indirectly, as is standard in main-memory databases, the cost of key retrieval in terms of cache misses can dominate the cost of an index traversal. Yet it is inefficient in both time and space to store even moderate sized keys directly in index nodes. In this paper, we investigate the performance of tree structures suitable for OLTP workloads in the face of expensive cache misses and non-trivial key sizes. We propose two index structures, pkT-trees and pkB-trees, which significantly reduce cache misses by storing partial-key information in the index. We show that a small, fixed amount of key information allows most cache misses to be avoided, allowing for a simple node structure and efficient implementation. Finally, we study the performance and cache behavior of partial-key trees by comparing them with other main-memory tree structures for a wide variety of key sizes and key value distributions.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {163–174},
numpages = {12},
keywords = {B-trees, main-memory indices, key compression, cache coherence, T-tree},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375681,
author = {Bohannon, Philip and Mcllroy, Peter and Rastogi, Rajeev},
title = {Main-Memory Index Structures with Fixed-Size Partial Keys},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375681},
doi = {10.1145/376284.375681},
abstract = {The performance of main-memory index structures is increasingly determined by the number of CPU cache misses incurred when traversing the index. When keys are stored indirectly, as is standard in main-memory databases, the cost of key retrieval in terms of cache misses can dominate the cost of an index traversal. Yet it is inefficient in both time and space to store even moderate sized keys directly in index nodes. In this paper, we investigate the performance of tree structures suitable for OLTP workloads in the face of expensive cache misses and non-trivial key sizes. We propose two index structures, pkT-trees and pkB-trees, which significantly reduce cache misses by storing partial-key information in the index. We show that a small, fixed amount of key information allows most cache misses to be avoided, allowing for a simple node structure and efficient implementation. Finally, we study the performance and cache behavior of partial-key trees by comparing them with other main-memory tree structures for a wide variety of key sizes and key value distributions.},
journal = {SIGMOD Rec.},
month = may,
pages = {163–174},
numpages = {12},
keywords = {B-trees, main-memory indices, T-tree, cache coherence, key compression}
}

@inproceedings{10.1145/375663.375682,
author = {Borkar, Vinayak and Deshmukh, Kaustubh and Sarawagi, Sunita},
title = {Automatic Segmentation of Text into Structured Records},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375682},
doi = {10.1145/375663.375682},
abstract = {In this paper we present a method for automatically segmenting unformatted text records into structured elements. Several useful data sources today are human-generated as continuous text whereas convenient usage requires the data to be organized as structured records. A prime motivation is the warehouse address cleaning problem of transforming dirty addresses stored in large corporate databases as a single text field into subfields like “City” and “Street”. Existing tools rely on hand-tuned, domain-specific rule-based systems.We describe a tool DATAMOLD that learns to automatically extract structure when seeded with a small number of training examples. The tool enhances on Hidden Markov Models (HMM) to build a powerful probabilistic model that corroborates multiple sources of information including, the sequence of elements, their length distribution, distinguishing words from the vocabulary and an optional external data dictionary. Experiments on real-life datasets yielded accuracy of 90% on Asian addresses and 99% on US addresses. In contrast, existing information extraction methods based on rule-learning techniques yielded considerably lower accuracy.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {175–186},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375682,
author = {Borkar, Vinayak and Deshmukh, Kaustubh and Sarawagi, Sunita},
title = {Automatic Segmentation of Text into Structured Records},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375682},
doi = {10.1145/376284.375682},
abstract = {In this paper we present a method for automatically segmenting unformatted text records into structured elements. Several useful data sources today are human-generated as continuous text whereas convenient usage requires the data to be organized as structured records. A prime motivation is the warehouse address cleaning problem of transforming dirty addresses stored in large corporate databases as a single text field into subfields like “City” and “Street”. Existing tools rely on hand-tuned, domain-specific rule-based systems.We describe a tool DATAMOLD that learns to automatically extract structure when seeded with a small number of training examples. The tool enhances on Hidden Markov Models (HMM) to build a powerful probabilistic model that corroborates multiple sources of information including, the sequence of elements, their length distribution, distinguishing words from the vocabulary and an optional external data dictionary. Experiments on real-life datasets yielded accuracy of 90% on Asian addresses and 99% on US addresses. In contrast, existing information extraction methods based on rule-learning techniques yielded considerably lower accuracy.},
journal = {SIGMOD Rec.},
month = may,
pages = {175–186},
numpages = {12}
}

@inproceedings{10.1145/375663.375684,
author = {Yu, Clement and Meng, Weiyi and Wu, Wensheng and Liu, King-Lup},
title = {Efficient and Effective Metasearch for Text Databases Incorporating Linkages among Documents},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375684},
doi = {10.1145/375663.375684},
abstract = {Linkages among documents have a significant impact on the importance of documents, as it can be argued that important documents are pointed to by many documents or by other important documents. Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). There is a search engine associated with each database. In a large-scale metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against he set of representatives of all databases in order to determine the appropriate databases (search engines) to search (invoke) In previous word, the linkage information between documents has not been utilized in determining the appropriate databases to search. In this paper, such information is employed to determine the degree of relevance of a document with respect to a given query. Specifically, the importance (rank) of each document as determined by the linkages is integrated in each database representative to facilitate the selection of databases for each given query. We establish a necessary and sufficient condition to rank databases optimally, while incorporating the linkage information. A method is provided to estimate the desired quantities stated in the necessary and sufficient condition. The estimation method runs in time linearly proportional to the number of query terms. Experimental results are provided to demonstrate the high retrieval effectiveness of the method.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {187–198},
numpages = {12},
keywords = {metasearch, linkages among documents, information retrieval, distributed collection},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375684,
author = {Yu, Clement and Meng, Weiyi and Wu, Wensheng and Liu, King-Lup},
title = {Efficient and Effective Metasearch for Text Databases Incorporating Linkages among Documents},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375684},
doi = {10.1145/376284.375684},
abstract = {Linkages among documents have a significant impact on the importance of documents, as it can be argued that important documents are pointed to by many documents or by other important documents. Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). There is a search engine associated with each database. In a large-scale metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against he set of representatives of all databases in order to determine the appropriate databases (search engines) to search (invoke) In previous word, the linkage information between documents has not been utilized in determining the appropriate databases to search. In this paper, such information is employed to determine the degree of relevance of a document with respect to a given query. Specifically, the importance (rank) of each document as determined by the linkages is integrated in each database representative to facilitate the selection of databases for each given query. We establish a necessary and sufficient condition to rank databases optimally, while incorporating the linkage information. A method is provided to estimate the desired quantities stated in the necessary and sufficient condition. The estimation method runs in time linearly proportional to the number of query terms. Experimental results are provided to demonstrate the high retrieval effectiveness of the method.},
journal = {SIGMOD Rec.},
month = may,
pages = {187–198},
numpages = {12},
keywords = {information retrieval, linkages among documents, distributed collection, metasearch}
}

@inproceedings{10.1145/375663.375685,
author = {Deshpande, Amol and Garofalakis, Minos and Rastogi, Rajeev},
title = {Independence is Good: Dependency-Based Histogram Synopses for High-Dimensional Data},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375685},
doi = {10.1145/375663.375685},
abstract = {Approximating the joint data distribution of a multi-dimensional data set through a compact and accurate histogram synopsis is a fundamental problem arising in numerous practical scenarios, including query optimization and approximate query answering. Existing solutions either rely on simplistic independence assumptions or try to directly approximate the full joint data distribution over the complete set of attributes. Unfortunately, both approaches are doomed to fail for high-dimensional data sets with complex correlation patterns between attributes. In this paper, we propose a novel approach to histogram-based synopses that employs the solid foundation of statistical interaction models to explicitly identify and exploit the statistical characteristics of the data. Abstractly, our key idea is to break the synopsis into (1) a statistical interaction model that accurately captures significant correlation and independence patterns in data, and (2) a collection of histograms on low-dimensional marginals that, based on the model, can provide accurate approximations of the overall joint data distribution. Extensive experimental results with several real-life data sets verify the effectiveness of our approach. An important aspect of our general, model-based methodology is that it can be used to enhance the performance of other synopsis techniques that are based on data-space partitioning (e.g., wavelets) by providing an effective tool to deal with the “dimensionality curse”.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {199–210},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375685,
author = {Deshpande, Amol and Garofalakis, Minos and Rastogi, Rajeev},
title = {Independence is Good: Dependency-Based Histogram Synopses for High-Dimensional Data},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375685},
doi = {10.1145/376284.375685},
abstract = {Approximating the joint data distribution of a multi-dimensional data set through a compact and accurate histogram synopsis is a fundamental problem arising in numerous practical scenarios, including query optimization and approximate query answering. Existing solutions either rely on simplistic independence assumptions or try to directly approximate the full joint data distribution over the complete set of attributes. Unfortunately, both approaches are doomed to fail for high-dimensional data sets with complex correlation patterns between attributes. In this paper, we propose a novel approach to histogram-based synopses that employs the solid foundation of statistical interaction models to explicitly identify and exploit the statistical characteristics of the data. Abstractly, our key idea is to break the synopsis into (1) a statistical interaction model that accurately captures significant correlation and independence patterns in data, and (2) a collection of histograms on low-dimensional marginals that, based on the model, can provide accurate approximations of the overall joint data distribution. Extensive experimental results with several real-life data sets verify the effectiveness of our approach. An important aspect of our general, model-based methodology is that it can be used to enhance the performance of other synopsis techniques that are based on data-space partitioning (e.g., wavelets) by providing an effective tool to deal with the “dimensionality curse”.},
journal = {SIGMOD Rec.},
month = may,
pages = {199–210},
numpages = {12}
}

@inproceedings{10.1145/375663.375686,
author = {Bruno, Nicolas and Chaudhuri, Surajit and Gravano, Luis},
title = {STHoles: A Multidimensional Workload-Aware Histogram},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375686},
doi = {10.1145/375663.375686},
abstract = {Attributes of a relation are not typically independent. Multidimensional histograms can be an effective tool for accurate multiattribute query selectivity estimation. In this paper, we introduce STHoles, a “workload-aware” histogram that allows bucket nesting to capture data regions with reasonably uniform tuple density. STHoles histograms are built without examining the data sets, but rather by just analyzing query results. Buckets are allocated where needed the most as indicated by the workload, which leads to accurate query selectivity estimations. Our extensive experiments demonstrate that STHoles histograms consistently produce good selectivity estimates across synthetic and real-world data sets and across query workloads, and, in many cases, outperform the best multidimensional histogram techniques that require access to and processing of the full data sets during histogram construction.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {211–222},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375686,
author = {Bruno, Nicolas and Chaudhuri, Surajit and Gravano, Luis},
title = {STHoles: A Multidimensional Workload-Aware Histogram},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375686},
doi = {10.1145/376284.375686},
abstract = {Attributes of a relation are not typically independent. Multidimensional histograms can be an effective tool for accurate multiattribute query selectivity estimation. In this paper, we introduce STHoles, a “workload-aware” histogram that allows bucket nesting to capture data regions with reasonably uniform tuple density. STHoles histograms are built without examining the data sets, but rather by just analyzing query results. Buckets are allocated where needed the most as indicated by the workload, which leads to accurate query selectivity estimations. Our extensive experiments demonstrate that STHoles histograms consistently produce good selectivity estimates across synthetic and real-world data sets and across query workloads, and, in many cases, outperform the best multidimensional histogram techniques that require access to and processing of the full data sets during histogram construction.},
journal = {SIGMOD Rec.},
month = may,
pages = {211–222},
numpages = {12}
}

@inproceedings{10.1145/375663.375687,
author = {Jagadish, H. V. and Jin, Hui and Ooi, Beng Chin and Tan, Kian-Lee},
title = {Global Optimization of Histograms},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375687},
doi = {10.1145/375663.375687},
abstract = {Histograms are frequently used to represent the distribution of data values in an attribute of a relation. Most previous work has focused on identifying the optimal histogram (given a limited number of buckets) for a single attribute independent of other attributes/histograms. In this paper, we propose the idea of global optimization of histograms, i.e., single-attribute histograms for a set of attributes are optimized collectively so as to minimize the overall error in using the histograms. The idea is to allocate more buckets to histograms whose attributes are more frequently used and/or distributions are highly skewed. While the accuracy of some histograms is penalized (being assigned fewer buckets), we expect the global error to be low compared to the traditional method (of allocating equal number of buckets to each histogram).We propose two algorithms to determine the histograms to construct for a collection of attributes. The first is based on dynamic programming, and the second is a greedy algorithm. We compare the overall error of these algorithms against the traditional method. Extensive experiments are conducted and the results confirm the benefits of global optimal histograms in reducing the overall error. The extent of improvement depends on the data and query distributions, ranging from no benefit when there is no significant differences in the data distributions to over a factor of 100 reduction in error in some cases we tried.The time to compute global optimal histogram using dynamic programming is much longer than the time to compute optimal histograms separately for each attribute, and the difference widens at a faster rate as the number of histograms increases. With the greedy algorithm, the time penalty is small, but the error reduction is somewhat less as well. We propose a third algorithm, called greedy algorithm with remedy, that has running time similar to the greedy algorithm, but produces results close to global optimum. In fact, in every experiment that we tried, this algorithm found the exact global optimum.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {223–234},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375687,
author = {Jagadish, H. V. and Jin, Hui and Ooi, Beng Chin and Tan, Kian-Lee},
title = {Global Optimization of Histograms},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375687},
doi = {10.1145/376284.375687},
abstract = {Histograms are frequently used to represent the distribution of data values in an attribute of a relation. Most previous work has focused on identifying the optimal histogram (given a limited number of buckets) for a single attribute independent of other attributes/histograms. In this paper, we propose the idea of global optimization of histograms, i.e., single-attribute histograms for a set of attributes are optimized collectively so as to minimize the overall error in using the histograms. The idea is to allocate more buckets to histograms whose attributes are more frequently used and/or distributions are highly skewed. While the accuracy of some histograms is penalized (being assigned fewer buckets), we expect the global error to be low compared to the traditional method (of allocating equal number of buckets to each histogram).We propose two algorithms to determine the histograms to construct for a collection of attributes. The first is based on dynamic programming, and the second is a greedy algorithm. We compare the overall error of these algorithms against the traditional method. Extensive experiments are conducted and the results confirm the benefits of global optimal histograms in reducing the overall error. The extent of improvement depends on the data and query distributions, ranging from no benefit when there is no significant differences in the data distributions to over a factor of 100 reduction in error in some cases we tried.The time to compute global optimal histogram using dynamic programming is much longer than the time to compute optimal histograms separately for each attribute, and the difference widens at a faster rate as the number of histograms increases. With the greedy algorithm, the time penalty is small, but the error reduction is somewhat less as well. We propose a third algorithm, called greedy algorithm with remedy, that has running time similar to the greedy algorithm, but produces results close to global optimum. In fact, in every experiment that we tried, this algorithm found the exact global optimum.},
journal = {SIGMOD Rec.},
month = may,
pages = {223–234},
numpages = {12}
}

@inproceedings{10.1145/375663.375688,
author = {Chen, Shimin and Gibbons, Phillip B. and Mowry, Todd C.},
title = {Improving Index Performance through Prefetching},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375688},
doi = {10.1145/375663.375688},
abstract = {This paper proposes and evaluate Prefetching B+-Trees (pB+-Trees), which use prefetching to accelerate two important operations on B+-Tree indices: searches and range scans. To accelerate searches, pB+-Trees use prefetching to effectively create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages. These wider nodes reduce the height of the B+-Tree, thereby decreasing the number of expensive misses when going from parent to child without significantly increasing the cost of fetching a given node. Our results show that this technique speeds up search and update times by a factor of 1.21-1.5 for main-memory B+-Trees. In addition, it outperforms and is complementary to “Cache-Sensitive B+-Trees.” To accelerate range scans, pB+-Trees provide arrays of pointers to their leaf nodes. These allow the pB+-Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range. Our results show that this technique yields over a sixfold speedup on range scans of 1000+ keys. Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {235–246},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375688,
author = {Chen, Shimin and Gibbons, Phillip B. and Mowry, Todd C.},
title = {Improving Index Performance through Prefetching},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375688},
doi = {10.1145/376284.375688},
abstract = {This paper proposes and evaluate Prefetching B+-Trees (pB+-Trees), which use prefetching to accelerate two important operations on B+-Tree indices: searches and range scans. To accelerate searches, pB+-Trees use prefetching to effectively create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages. These wider nodes reduce the height of the B+-Tree, thereby decreasing the number of expensive misses when going from parent to child without significantly increasing the cost of fetching a given node. Our results show that this technique speeds up search and update times by a factor of 1.21-1.5 for main-memory B+-Trees. In addition, it outperforms and is complementary to “Cache-Sensitive B+-Trees.” To accelerate range scans, pB+-Trees provide arrays of pointers to their leaf nodes. These allow the pB+-Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range. Our results show that this technique yields over a sixfold speedup on range scans of 1000+ keys. Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency.},
journal = {SIGMOD Rec.},
month = may,
pages = {235–246},
numpages = {12}
}

@inproceedings{10.1145/375663.375689,
author = {Gionis, Aristides and Gunopulos, Dimitrios and Koudas, Nick},
title = {Efficient and Tumble Similar Set Retrieval},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375689},
doi = {10.1145/375663.375689},
abstract = {Set value attributes are a concise and natural way to model complex data sets. Modern Object Relational systems support set value attributes and allow various query capabilities on them. In this paper we initiate a formal study of indexing techniques for set value attributes based on similarity, for suitably defined notions of similarity between sets. Such techniques are necessary in modern applications such as recommendations through collaborative filtering and automated advertising. Our techniques are probabilistic and approximate in nature. As a design principle we create structures that make use of well known and widely used data structuring techniques, as a means to ease integration with existing infrastructure.We show how the problem of indexing a collection of sets based on similarity can be reduced to the problem of indexing suitably encoded (in a way that preserves similarity) binary vectors in Hamming space thus, reducing the problem to one of similarity query processing in Hamming space. Then, we introduce and analyze two data structure primitives that we use in cooperation to perform similarity query processing in a Hamming space. We show how the resulting indexing technique can be optimized for properties of interest by formulating constraint optimization problems based on the space one is willing to devote for indexing. Finally we present experimental results from a prototype implementation of our techniques using real life datasets exploring the accuracy and efficiency of our overall approach as well as the quality of our solutions to problems related to the optimization of the indexing scheme.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {247–258},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375689,
author = {Gionis, Aristides and Gunopulos, Dimitrios and Koudas, Nick},
title = {Efficient and Tumble Similar Set Retrieval},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375689},
doi = {10.1145/376284.375689},
abstract = {Set value attributes are a concise and natural way to model complex data sets. Modern Object Relational systems support set value attributes and allow various query capabilities on them. In this paper we initiate a formal study of indexing techniques for set value attributes based on similarity, for suitably defined notions of similarity between sets. Such techniques are necessary in modern applications such as recommendations through collaborative filtering and automated advertising. Our techniques are probabilistic and approximate in nature. As a design principle we create structures that make use of well known and widely used data structuring techniques, as a means to ease integration with existing infrastructure.We show how the problem of indexing a collection of sets based on similarity can be reduced to the problem of indexing suitably encoded (in a way that preserves similarity) binary vectors in Hamming space thus, reducing the problem to one of similarity query processing in Hamming space. Then, we introduce and analyze two data structure primitives that we use in cooperation to perform similarity query processing in a Hamming space. We show how the resulting indexing technique can be optimized for properties of interest by formulating constraint optimization problems based on the space one is willing to devote for indexing. Finally we present experimental results from a prototype implementation of our techniques using real life datasets exploring the accuracy and efficiency of our overall approach as well as the quality of our solutions to problems related to the optimization of the indexing scheme.},
journal = {SIGMOD Rec.},
month = may,
pages = {247–258},
numpages = {12}
}

@inproceedings{10.1145/375663.375690,
author = {Hristidis, Vagelis and Koudas, Nick and Papakonstantinou, Yannis},
title = {PREFER: A System for the Efficient Execution of Multi-Parametric Ranked Queries},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375690},
doi = {10.1145/375663.375690},
abstract = {Users often need to optimize the selection of objects by appropriately weighting the importance of multiple object attributes. Such optimization problems appear often in operations' research and applied mathematics as well as everyday life; e.g., a buyer may select a home as a weighted function of a number of attributes like its distance from office, its price, its area, etc.We capture such queries in our definition of preference queries that use a weight function over a relation's attributes to derive a score for each tuple. Database systems cannot efficiently produce the top results of a preference query because they need to evaluate the weight function over all tuples of the relation. PREFER answers preference queries efficiently by using materialized views that have been pre-processed and stored.We first show how the result of a preference query can be produced in a pipelined fashion using a materialized view. Then we show that excellent performance can be delivered given a reasonable number of materialized views and we provide an algorithm that selects a number of views to precompute and materialize given space constraints.We have implemented the algorithms proposed in this paper in a prototype system called PREFER, which operates on top of a commercial database management system. We present the results of a performance comparison, comparing our algorithms with prior approaches using synthetic datasets. Our results indicate that the proposed algorithms are superior in performance compared to other approaches, both in preprocessing (preparation of materialized views) as well as execution time.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {259–270},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375690,
author = {Hristidis, Vagelis and Koudas, Nick and Papakonstantinou, Yannis},
title = {PREFER: A System for the Efficient Execution of Multi-Parametric Ranked Queries},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375690},
doi = {10.1145/376284.375690},
abstract = {Users often need to optimize the selection of objects by appropriately weighting the importance of multiple object attributes. Such optimization problems appear often in operations' research and applied mathematics as well as everyday life; e.g., a buyer may select a home as a weighted function of a number of attributes like its distance from office, its price, its area, etc.We capture such queries in our definition of preference queries that use a weight function over a relation's attributes to derive a score for each tuple. Database systems cannot efficiently produce the top results of a preference query because they need to evaluate the weight function over all tuples of the relation. PREFER answers preference queries efficiently by using materialized views that have been pre-processed and stored.We first show how the result of a preference query can be produced in a pipelined fashion using a materialized view. Then we show that excellent performance can be delivered given a reasonable number of materialized views and we provide an algorithm that selects a number of views to precompute and materialize given space constraints.We have implemented the algorithms proposed in this paper in a prototype system called PREFER, which operates on top of a commercial database management system. We present the results of a performance comparison, comparing our algorithms with prior approaches using synthetic datasets. Our results indicate that the proposed algorithms are superior in performance compared to other approaches, both in preprocessing (preparation of materialized views) as well as execution time.},
journal = {SIGMOD Rec.},
month = may,
pages = {259–270},
numpages = {12}
}

@inproceedings{10.1145/375663.375692,
author = {Chen, Zhiyuan and Gehrke, Johannes and Korn, Flip},
title = {Query Optimization in Compressed Database Systems},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375692},
doi = {10.1145/375663.375692},
abstract = {Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand onlyIn this paper, we present an effective approach for database compression based on lightweight, attribute-level compression techniques. We propose a IIierarchical Dictionary Encoding strategy that intelligently selects the most effective compression method for string-valued attributes. We show that eager and lazy decompression strategies produce sub-optimal plans for queries involving compressed string attributes. We then formalize the problem of compression-aware query optimization and propose one provably optimal and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using TPC-H data demonstrate the impact of our string compression methods and show the importance of compression-aware query optimization. Our approach results in up to an order speed up over existing approaches.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {271–282},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375692,
author = {Chen, Zhiyuan and Gehrke, Johannes and Korn, Flip},
title = {Query Optimization in Compressed Database Systems},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375692},
doi = {10.1145/376284.375692},
abstract = {Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand onlyIn this paper, we present an effective approach for database compression based on lightweight, attribute-level compression techniques. We propose a IIierarchical Dictionary Encoding strategy that intelligently selects the most effective compression method for string-valued attributes. We show that eager and lazy decompression strategies produce sub-optimal plans for queries involving compressed string attributes. We then formalize the problem of compression-aware query optimization and propose one provably optimal and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using TPC-H data demonstrate the impact of our string compression methods and show the importance of compression-aware query optimization. Our approach results in up to an order speed up over existing approaches.},
journal = {SIGMOD Rec.},
month = may,
pages = {271–282},
numpages = {12}
}

@inproceedings{10.1145/375663.375693,
author = {Babu, Shivnath and Garofalakis, Minos and Rastogi, Rajeev},
title = {SPARTAN: A Model-Based Semantic Compression System for Massive Data Tables},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375693},
doi = {10.1145/375663.375693},
abstract = {While a variety of lossy compression schemes have been developed for certain forms of digital data (e.g., images, audio, video), the area of lossy compression techniques for arbitrary data tables has been left relatively unexplored. Nevertheless, such techniques are clearly motivated by the ever-increasing data collection rates of modern enterprises and the need for effective, guaranteed-quality approximate answers to queries over massive relational data sets. In this paper, we propose SPARTAN, a system that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive data tables. SPARTAN is based on the novel idea of exploiting predictive data correlations and prescribed error tolerances for individual attributes to construct concise and accurate Classification and Regression Tree (CaRT) models for entire columns of a table. More precisely, SPARTAN selects a certain subset of attributes for which no values are explicitly stored in the compressed table; instead, concise CaRTs that predict these values (within the prescribed error bounds) are maintained. To restrict the huge search space and construction cost of possible CaRT predictors, SPARTAN employs sophisticated learning techniques and novel combinatorial optimization algorithms. Our experimentation with several real-life data sets offers convincing evidence of the effectiveness of SPARTAN's model-based approach — SPARTAN is able to consistently yield substantially better compression ratios than existing semantic or syntactic compression tools (e.g., gzip) while utilizing only small data samples for model inference.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {283–294},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375693,
author = {Babu, Shivnath and Garofalakis, Minos and Rastogi, Rajeev},
title = {SPARTAN: A Model-Based Semantic Compression System for Massive Data Tables},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375693},
doi = {10.1145/376284.375693},
abstract = {While a variety of lossy compression schemes have been developed for certain forms of digital data (e.g., images, audio, video), the area of lossy compression techniques for arbitrary data tables has been left relatively unexplored. Nevertheless, such techniques are clearly motivated by the ever-increasing data collection rates of modern enterprises and the need for effective, guaranteed-quality approximate answers to queries over massive relational data sets. In this paper, we propose SPARTAN, a system that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive data tables. SPARTAN is based on the novel idea of exploiting predictive data correlations and prescribed error tolerances for individual attributes to construct concise and accurate Classification and Regression Tree (CaRT) models for entire columns of a table. More precisely, SPARTAN selects a certain subset of attributes for which no values are explicitly stored in the compressed table; instead, concise CaRTs that predict these values (within the prescribed error bounds) are maintained. To restrict the huge search space and construction cost of possible CaRT predictors, SPARTAN employs sophisticated learning techniques and novel combinatorial optimization algorithms. Our experimentation with several real-life data sets offers convincing evidence of the effectiveness of SPARTAN's model-based approach — SPARTAN is able to consistently yield substantially better compression ratios than existing semantic or syntactic compression tools (e.g., gzip) while utilizing only small data samples for model inference.},
journal = {SIGMOD Rec.},
month = may,
pages = {283–294},
numpages = {12}
}

@inproceedings{10.1145/375663.375694,
author = {Chaudhuri, Surajit and Das, Gautam and Narasayya, Vivek},
title = {A Robust, Optimization-Based Approach for Approximate Answering of Aggregate Queries},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375694},
doi = {10.1145/375663.375694},
abstract = {The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {295–306},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375694,
author = {Chaudhuri, Surajit and Das, Gautam and Narasayya, Vivek},
title = {A Robust, Optimization-Based Approach for Approximate Answering of Aggregate Queries},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375694},
doi = {10.1145/376284.375694},
abstract = {The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work.},
journal = {SIGMOD Rec.},
month = may,
pages = {295–306},
numpages = {12}
}

@inproceedings{10.1145/375663.375703,
author = {Mistry, Hoshi and Roy, Prasan and Sudarshan, S. and Ramamritham, Krithi},
title = {Materialized View Selection and Maintenance Using Multi-Query Optimization},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375703},
doi = {10.1145/375663.375703},
abstract = {Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan — incremental or recomputation — for each view. These three decisions are highly interdependent, and the choice of one affects the choice of the others. We develop a framework that cleanly integrates the various choices in a systematic and efficient manner. Our evaluations show that many-fold improvement in view maintenance time can be achieved using our techniques. Our algorithms can also be used to efficiently select materialized views to speed up workloads containing queries and updates.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {307–318},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375703,
author = {Mistry, Hoshi and Roy, Prasan and Sudarshan, S. and Ramamritham, Krithi},
title = {Materialized View Selection and Maintenance Using Multi-Query Optimization},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375703},
doi = {10.1145/376284.375703},
abstract = {Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan — incremental or recomputation — for each view. These three decisions are highly interdependent, and the choice of one affects the choice of the others. We develop a framework that cleanly integrates the various choices in a systematic and efficient manner. Our evaluations show that many-fold improvement in view maintenance time can be achieved using our techniques. Our algorithms can also be used to efficiently select materialized views to speed up workloads containing queries and updates.},
journal = {SIGMOD Rec.},
month = may,
pages = {307–318},
numpages = {12}
}

@inproceedings{10.1145/375663.375705,
author = {Afrati, Foto N. and Li, Chen and Ullman, Jeffrey D.},
title = {Generating Efficient Plans for Queries Using Views},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375705},
doi = {10.1145/375663.375705},
abstract = {We study the problem or generating efficient, equivalent rewritings using views to compute the answer to a query. We take the closed-world assumption, in which views are materialized from base relations, rather than views describing sources in terms of abstract predicates, as is common when the open-world assumption is used. In the closed-world model, there can be an infinite number of different rewritings that compute the same answer, yet have quite different performance. Query optimizers take a logical plan (a rewriting of the query) as an input, and generate efficient physical plans to compute the answer. Thus our goal is to generate a small subset of the possible logical plans without missing an optimal physical plan.We first consider a cost model that counts the number of subgoals in a physical plan, and show a search space that is guaranteed to include an optimal rewriting, if the query has a rewriting in terms of the views. We also develop an efficient algorithm for finding rewritings with the minimum number of subgoals. We then consider a cost model that counts the sizes of intermediate relations of a physical plan, without dropping any attributes, and give a search space for finding optimal rewritings. Our final cost model allows attributes to be dropped in intermediate relations. We show that, by careful variable renaming, it is possible to do better than the standard “supplementary relation” approach, by dropping attributes that the latter approach would retain. Experiments show that our algorithm of generating optimal rewritings has good efficiency and scalability.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {319–330},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375705,
author = {Afrati, Foto N. and Li, Chen and Ullman, Jeffrey D.},
title = {Generating Efficient Plans for Queries Using Views},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375705},
doi = {10.1145/376284.375705},
abstract = {We study the problem or generating efficient, equivalent rewritings using views to compute the answer to a query. We take the closed-world assumption, in which views are materialized from base relations, rather than views describing sources in terms of abstract predicates, as is common when the open-world assumption is used. In the closed-world model, there can be an infinite number of different rewritings that compute the same answer, yet have quite different performance. Query optimizers take a logical plan (a rewriting of the query) as an input, and generate efficient physical plans to compute the answer. Thus our goal is to generate a small subset of the possible logical plans without missing an optimal physical plan.We first consider a cost model that counts the number of subgoals in a physical plan, and show a search space that is guaranteed to include an optimal rewriting, if the query has a rewriting in terms of the views. We also develop an efficient algorithm for finding rewritings with the minimum number of subgoals. We then consider a cost model that counts the sizes of intermediate relations of a physical plan, without dropping any attributes, and give a search space for finding optimal rewritings. Our final cost model allows attributes to be dropped in intermediate relations. We show that, by careful variable renaming, it is possible to do better than the standard “supplementary relation” approach, by dropping attributes that the latter approach would retain. Experiments show that our algorithm of generating optimal rewritings has good efficiency and scalability.},
journal = {SIGMOD Rec.},
month = may,
pages = {319–330},
numpages = {12}
}

@inproceedings{10.1145/375663.375706,
author = {Goldstein, Jonathan and Larson, Per-\r{A}ke},
title = {Optimizing Queries Using Materialized Views: A Practical, Scalable Solution},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375706},
doi = {10.1145/375663.375706},
abstract = {Materialized views can provide massive improvements in query processing time, especially for aggregation queries over large tables. To realize this potential, the query optimizer must know how and when to exploit materialized views. This paper presents a fast and scalable algorithm for determining whether part or all of a query can be computed from materialized views and describes how it can be incorporated in transformation-based optimizers. The current version handles views composed of selections, joins and a final group-by. Optimization remains fully cost based, that is, a single “best” rewrite is not selected by heuristic rules but multiple rewrites are generated and the optimizer chooses the best alternative in the normal way. Experimental results based on an implementation in Microsoft SQL Server show outstanding performance and scalability. Optimization time increases slowly with the number of views but remains low even up to a thousand.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {331–342},
numpages = {12},
keywords = {view matching, materialized views, query optimization},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375706,
author = {Goldstein, Jonathan and Larson, Per-\r{A}ke},
title = {Optimizing Queries Using Materialized Views: A Practical, Scalable Solution},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375706},
doi = {10.1145/376284.375706},
abstract = {Materialized views can provide massive improvements in query processing time, especially for aggregation queries over large tables. To realize this potential, the query optimizer must know how and when to exploit materialized views. This paper presents a fast and scalable algorithm for determining whether part or all of a query can be computed from materialized views and describes how it can be incorporated in transformation-based optimizers. The current version handles views composed of selections, joins and a final group-by. Optimization remains fully cost based, that is, a single “best” rewrite is not selected by heuristic rules but multiple rewrites are generated and the optimizer chooses the best alternative in the normal way. Experimental results based on an implementation in Microsoft SQL Server show outstanding performance and scalability. Optimization time increases slowly with the number of views but remains low even up to a thousand.},
journal = {SIGMOD Rec.},
month = may,
pages = {331–342},
numpages = {12},
keywords = {materialized views, query optimization, view matching}
}

@inproceedings{10.1145/375663.375709,
author = {Lee, Sang-Ho and Whang, Kyu-Young and Moon, Yang-Sae and Song, Il-Yeol},
title = {Dynamic Buffer Allocation in Video-on-Demand Systems},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375709},
doi = {10.1145/375663.375709},
abstract = {In video-on-demand (VOD) systems, as the size of the buffer allocated to user requests increases, initial latency and memory requirements increase. Hence, the buffer size must be minimized. The existing static buffer allocation scheme, however, determines the buffer size based on the assumption that the system is in the fully loaded state. Thus, when the system is in a partially loaded state, the scheme allocates a buffer larger than necessary to a user request. This paper proposes a dynamic buffer allocation scheme that allocates to user requests buffers of the minimum size in a partially loaded state as well as in the fully loaded state. The inherent difficulty in determining the buffer size in the dynamic buffer allocation scheme is that the size of the buffer currently being allocated is dependent on the number of and the sizes of the buffers to be allocated in the next service period. We solve this problem by the predict-and-enforce strategy, where we predict the number and the sizes of future buffers based on inertia assumptions and enforce these assumptions at runtime. Any violation of these assumptions is resolved by deferring service to the violating new user request until the assumptions are satisfied. Since the size of the current buffer is dependent on the sizes of the future buffers, the size is represented by a recurrence equation. We provide a solution to this equation, which can be computed at the system initialization time for runtime efficiency. We have performed extensive analysis and simulation. The results show that the dynamic buffer allocation scheme reduces initial latency (averaged over the number of user requests in service from one to the maximum capacity) to 1 undefined 29.4 ≁ 1 undefined 11.0 of that for the static one and, by reducing the memory requirement, increases the number of concurrent user requests to 2.36 ∼ 3.25 times that of the static one when averaged over the amount of system memory available. These results demonstrate that the dynamic buffer allocation scheme significantly improves the performance and capacity of VOD systems.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {343–354},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375709,
author = {Lee, Sang-Ho and Whang, Kyu-Young and Moon, Yang-Sae and Song, Il-Yeol},
title = {Dynamic Buffer Allocation in Video-on-Demand Systems},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375709},
doi = {10.1145/376284.375709},
abstract = {In video-on-demand (VOD) systems, as the size of the buffer allocated to user requests increases, initial latency and memory requirements increase. Hence, the buffer size must be minimized. The existing static buffer allocation scheme, however, determines the buffer size based on the assumption that the system is in the fully loaded state. Thus, when the system is in a partially loaded state, the scheme allocates a buffer larger than necessary to a user request. This paper proposes a dynamic buffer allocation scheme that allocates to user requests buffers of the minimum size in a partially loaded state as well as in the fully loaded state. The inherent difficulty in determining the buffer size in the dynamic buffer allocation scheme is that the size of the buffer currently being allocated is dependent on the number of and the sizes of the buffers to be allocated in the next service period. We solve this problem by the predict-and-enforce strategy, where we predict the number and the sizes of future buffers based on inertia assumptions and enforce these assumptions at runtime. Any violation of these assumptions is resolved by deferring service to the violating new user request until the assumptions are satisfied. Since the size of the current buffer is dependent on the sizes of the future buffers, the size is represented by a recurrence equation. We provide a solution to this equation, which can be computed at the system initialization time for runtime efficiency. We have performed extensive analysis and simulation. The results show that the dynamic buffer allocation scheme reduces initial latency (averaged over the number of user requests in service from one to the maximum capacity) to 1 undefined 29.4 ≁ 1 undefined 11.0 of that for the static one and, by reducing the memory requirement, increases the number of concurrent user requests to 2.36 ∼ 3.25 times that of the static one when averaged over the amount of system memory available. These results demonstrate that the dynamic buffer allocation scheme significantly improves the performance and capacity of VOD systems.},
journal = {SIGMOD Rec.},
month = may,
pages = {343–354},
numpages = {12}
}

@inproceedings{10.1145/375663.375710,
author = {Olston, Chris and Loo, Boon Thau and Widom, Jennifer},
title = {Adaptive Precision Setting for Cached Approximate Values},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375710},
doi = {10.1145/375663.375710},
abstract = {Caching approximate values instead of exact values presents an opportunity for performance gains in exchange for decreased precision. To maximize the performance improvement, cached approximations must be of appropriate precision: approximations that are too precise easily become invalid, requiring frequent refreshing, while overly imprecise approximations are likely to be useless to applications, which must then bypass the cache. We present a parameterized algorithm for adjusting the precision of cached approximations adaptively to achieve the best performance as data values, precision requirements, or workload vary. We consider interval approximations to numeric values but our ideas can be extended to other kinds of data and approximations. Our algorithm strictly generalizes previous adaptive caching algorithms for exact copies: we can set parameters to require that all approximations be exact, in which case our algorithm dynamically chooses whether or not to cache each data value.We have implemented our algorithm and tested it on synthetic and real-world data. A number of experimental results are reported, showing the effectiveness of our algorithm at maximizing performance, and also showing that in the special case of exact caching our algorithm performs as well as previous algorithms. In cases where bounded imprecision is acceptable, our algorithm easily outperforms previous algorithms for exact caching.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {355–366},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375710,
author = {Olston, Chris and Loo, Boon Thau and Widom, Jennifer},
title = {Adaptive Precision Setting for Cached Approximate Values},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375710},
doi = {10.1145/376284.375710},
abstract = {Caching approximate values instead of exact values presents an opportunity for performance gains in exchange for decreased precision. To maximize the performance improvement, cached approximations must be of appropriate precision: approximations that are too precise easily become invalid, requiring frequent refreshing, while overly imprecise approximations are likely to be useless to applications, which must then bypass the cache. We present a parameterized algorithm for adjusting the precision of cached approximations adaptively to achieve the best performance as data values, precision requirements, or workload vary. We consider interval approximations to numeric values but our ideas can be extended to other kinds of data and approximations. Our algorithm strictly generalizes previous adaptive caching algorithms for exact copies: we can set parameters to require that all approximations be exact, in which case our algorithm dynamically chooses whether or not to cache each data value.We have implemented our algorithm and tested it on synthetic and real-world data. A number of experimental results are reported, showing the effectiveness of our algorithm at maximizing performance, and also showing that in the special case of exact caching our algorithm performs as well as previous algorithms. In cases where bounded imprecision is acceptable, our algorithm easily outperforms previous algorithms for exact caching.},
journal = {SIGMOD Rec.},
month = may,
pages = {355–366},
numpages = {12}
}

@inproceedings{10.1145/375663.375712,
author = {Kalnis, Panos and Papadias, Dimitris},
title = {Proxy-Server Architectures for OLAP},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375712},
doi = {10.1145/375663.375712},
abstract = {Data warehouses have been successfully employed for assisting decision making by offering a global view of the enterprise data and providing mechanisms for On-Line Analytical processing. Traditionally, data warehouses are utilized within the limits of an enterprise or organization. The growth of Internet and WWW however, has created new opportunities for data sharing among ad-hoc, geographically spanned and possibly mobile users. Since it is impractical for each enterprise to set up a worldwide infrastructure, currently such applications are handled by the central warehouse. This often yields poor performance, due to overloading of the central server and low transfer rate of the network.In this paper we propose an architecture for OLAP cache servers (OCS). An OCS is the equivalent of a proxy-server for web documents, but it is designed to accommodate data from warehouses and support OLAP operations. We allow numerous OCSs to be connected via an arbitrary network, and present a centralized, a semi-centralized and an autonomous control policy. We experimentally evaluate these policies and compare the performance gain against the existing systems where caching is performed only at the client side. Our architecture offers increased autonomy at remote clients, substantial network traffic savings, better scalability, lower response time and is complementary both to existing OLAP cache systems and distributed OLAP approaches.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {367–378},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375712,
author = {Kalnis, Panos and Papadias, Dimitris},
title = {Proxy-Server Architectures for OLAP},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375712},
doi = {10.1145/376284.375712},
abstract = {Data warehouses have been successfully employed for assisting decision making by offering a global view of the enterprise data and providing mechanisms for On-Line Analytical processing. Traditionally, data warehouses are utilized within the limits of an enterprise or organization. The growth of Internet and WWW however, has created new opportunities for data sharing among ad-hoc, geographically spanned and possibly mobile users. Since it is impractical for each enterprise to set up a worldwide infrastructure, currently such applications are handled by the central warehouse. This often yields poor performance, due to overloading of the central server and low transfer rate of the network.In this paper we propose an architecture for OLAP cache servers (OCS). An OCS is the equivalent of a proxy-server for web documents, but it is designed to accommodate data from warehouses and support OLAP operations. We allow numerous OCSs to be connected via an arbitrary network, and present a centralized, a semi-centralized and an autonomous control policy. We experimentally evaluate these policies and compare the performance gain against the existing systems where caching is performed only at the client side. Our architecture offers increased autonomy at remote clients, substantial network traffic savings, better scalability, lower response time and is complementary both to existing OLAP cache systems and distributed OLAP approaches.},
journal = {SIGMOD Rec.},
month = may,
pages = {367–378},
numpages = {12}
}

@inproceedings{10.1145/375663.375714,
author = {B\"{o}hm, Christian and Braunm\"{u}ller, Bernhard and Krebs, Florian and Kriegel, Hans-Peter},
title = {Epsilon Grid Order: An Algorithm for the Similarity Join on Massive High-Dimensional Data},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375714},
doi = {10.1145/375663.375714},
abstract = {The similarity join is an important database primitive which has been successfully applied to speed up applications such as similarity search, data analysis and data mining. The similarity join combines two point sets of a multidimensional vector space such that the result contains all point pairs where the distance does not exceed a parameter ε. In this paper, we propose the Epsilon Grid Order, a new algorithm for determining the similarity join of very large data sets. Our solution is based on a particular sort order of the data points, which is obtained by laying an equi-distant grid with cell length ε over the data space and comparing the grid cells lexicographically. A typical problem of grid-based approaches such as MSJ or the ε-kdB-tree is that large portions of the data sets must be held simultaneously in main memory. Therefore, these approaches do not scale to large data sets. Our technique avoids this problem by an external sorting algorithm and a particular scheduling strategy during the join phase. In the experimental evaluation, a substantial improvement over competitive techniques is shown.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {379–388},
numpages = {10},
keywords = {similarity join, high-dimensional space, similarity search, data mining, knowledge discovery, feature transformation},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375714,
author = {B\"{o}hm, Christian and Braunm\"{u}ller, Bernhard and Krebs, Florian and Kriegel, Hans-Peter},
title = {Epsilon Grid Order: An Algorithm for the Similarity Join on Massive High-Dimensional Data},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375714},
doi = {10.1145/376284.375714},
abstract = {The similarity join is an important database primitive which has been successfully applied to speed up applications such as similarity search, data analysis and data mining. The similarity join combines two point sets of a multidimensional vector space such that the result contains all point pairs where the distance does not exceed a parameter ε. In this paper, we propose the Epsilon Grid Order, a new algorithm for determining the similarity join of very large data sets. Our solution is based on a particular sort order of the data points, which is obtained by laying an equi-distant grid with cell length ε over the data space and comparing the grid cells lexicographically. A typical problem of grid-based approaches such as MSJ or the ε-kdB-tree is that large portions of the data sets must be held simultaneously in main memory. Therefore, these approaches do not scale to large data sets. Our technique avoids this problem by an external sorting algorithm and a particular scheduling strategy during the join phase. In the experimental evaluation, a substantial improvement over competitive techniques is shown.},
journal = {SIGMOD Rec.},
month = may,
pages = {379–388},
numpages = {10},
keywords = {similarity join, data mining, feature transformation, knowledge discovery, similarity search, high-dimensional space}
}

@inproceedings{10.1145/375663.375716,
author = {Lang, Christian A. and Singh, Ambuj K.},
title = {Modeling High-Dimensional Index Structures Using Sampling},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375716},
doi = {10.1145/375663.375716},
abstract = {A large number of index structures for high-dimensional data have been proposed previously. In order to tune and compare such index structures, it is vital to have efficient cost prediction techniques for these structures. Previous techniques either assume uniformity of the data or are not applicable to high-dimensional data. We propose the use of sampling to predict the number of accessed index pages during a query execution. Sampling is independent of the dimensionality and preserves clusters which is important for representing skewed data. We present a general model for estimating the index page layout using sampling and show how to compensate for errors. We then give an implementation of our model under restricted memory assumptions and show that it performs well even under these constraints. Errors are minimal and the overall prediction time is up to two orders of magnitude below the time for building and probing the full index without sampling.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {389–400},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375716,
author = {Lang, Christian A. and Singh, Ambuj K.},
title = {Modeling High-Dimensional Index Structures Using Sampling},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375716},
doi = {10.1145/376284.375716},
abstract = {A large number of index structures for high-dimensional data have been proposed previously. In order to tune and compare such index structures, it is vital to have efficient cost prediction techniques for these structures. Previous techniques either assume uniformity of the data or are not applicable to high-dimensional data. We propose the use of sampling to predict the number of accessed index pages during a query execution. Sampling is independent of the dimensionality and preserves clusters which is important for representing skewed data. We present a general model for estimating the index page layout using sampling and show how to compensate for errors. We then give an implementation of our model under restricted memory assumptions and show that it performs well even under these constraints. Errors are minimal and the overall prediction time is up to two orders of magnitude below the time for building and probing the full index without sampling.},
journal = {SIGMOD Rec.},
month = may,
pages = {389–400},
numpages = {12}
}

@inproceedings{10.1145/375663.375718,
author = {Lazaridis, Iosif and Mehrotra, Sharad},
title = {Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375718},
doi = {10.1145/375663.375718},
abstract = {Answering aggregate queries like SUM, COUNT, MIN, MAX, AVG in an approximate manner is often desirable when the exact answer is not needed or too costly to compute. We present an algorithm for answering such queries in multi-dimensional databases, using selective traversal of a Multi-Resolution Aggregate (MRA) tree structure storing point data. Our approach provides 100% intervals of confidence on the value of the aggregate and works iteratively, coming up with improving quality answers, until some error requirement is satisfied or time constraint as reached. Using the same technique we can also answer aggregate queries exactly and our experiments indicate that even for exact answering the proposed data structure and algorithm are very fast.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {401–412},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375718,
author = {Lazaridis, Iosif and Mehrotra, Sharad},
title = {Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375718},
doi = {10.1145/376284.375718},
abstract = {Answering aggregate queries like SUM, COUNT, MIN, MAX, AVG in an approximate manner is often desirable when the exact answer is not needed or too costly to compute. We present an algorithm for answering such queries in multi-dimensional databases, using selective traversal of a Multi-Resolution Aggregate (MRA) tree structure storing point data. Our approach provides 100% intervals of confidence on the value of the aggregate and works iteratively, coming up with improving quality answers, until some error requirement is satisfied or time constraint as reached. Using the same technique we can also answer aggregate queries exactly and our experiments indicate that even for exact answering the proposed data structure and algorithm are very fast.},
journal = {SIGMOD Rec.},
month = may,
pages = {401–412},
numpages = {12}
}

@inproceedings{10.1145/375663.375720,
author = {Tatarinov, Igor and Ives, Zachary G. and Halevy, Alon Y. and Weld, Daniel S.},
title = {Updating XML},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375720},
doi = {10.1145/375663.375720},
abstract = {As XML has developed over the past few years, its role has expanded beyond its original domain as a semantics-preserving markup language for online documents, and it is now also the de facto format for interchanging data between heterogeneous systems. Data sources expert XML “views” over their data, and other system can directly import or query these views. As a result, there has been great interest in languages and systems for expressing queries over XML data, whether the XML is stored in a repository or generated as a view over some other data storage format.Clearly, in order to fully evolve XML into a universal data representation and sharing format, we must allow users to specify updates to XML documents and must develop techniques to process them efficiently. Update capabilities are important not only for modifying XML documents, but also for propagating changes through XML view and for expressing and transmitting changes to documents. This paper begins by proposing a set of basic update operations for both ordered and unordered XML data. We next describe extensions to the proposed standard XML query language, XQuery, to incorporate the update operations. We then consider alternative methods for implementing update operations when the XML data is mapped into a relational database. Finally, we describe an experimental evaluation of the alternative techniques for implementing our extensions.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {413–424},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375720,
author = {Tatarinov, Igor and Ives, Zachary G. and Halevy, Alon Y. and Weld, Daniel S.},
title = {Updating XML},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375720},
doi = {10.1145/376284.375720},
abstract = {As XML has developed over the past few years, its role has expanded beyond its original domain as a semantics-preserving markup language for online documents, and it is now also the de facto format for interchanging data between heterogeneous systems. Data sources expert XML “views” over their data, and other system can directly import or query these views. As a result, there has been great interest in languages and systems for expressing queries over XML data, whether the XML is stored in a repository or generated as a view over some other data storage format.Clearly, in order to fully evolve XML into a universal data representation and sharing format, we must allow users to specify updates to XML documents and must develop techniques to process them efficiently. Update capabilities are important not only for modifying XML documents, but also for propagating changes through XML view and for expressing and transmitting changes to documents. This paper begins by proposing a set of basic update operations for both ordered and unordered XML data. We next describe extensions to the proposed standard XML query language, XQuery, to incorporate the update operations. We then consider alternative methods for implementing update operations when the XML data is mapped into a relational database. Finally, we describe an experimental evaluation of the alternative techniques for implementing our extensions.},
journal = {SIGMOD Rec.},
month = may,
pages = {413–424},
numpages = {12}
}

@inproceedings{10.1145/375663.375722,
author = {Zhang, Chun and Naughton, Jeffrey and DeWitt, David and Luo, Qiong and Lohman, Guy},
title = {On Supporting Containment Queries in Relational Database Management Systems},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375722},
doi = {10.1145/375663.375722},
abstract = {Virtually all proposals for querying XML include a class of query we term “containment queries”. It is also clear that in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises the question of how to support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {425–436},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375722,
author = {Zhang, Chun and Naughton, Jeffrey and DeWitt, David and Luo, Qiong and Lohman, Guy},
title = {On Supporting Containment Queries in Relational Database Management Systems},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375722},
doi = {10.1145/376284.375722},
abstract = {Virtually all proposals for querying XML include a class of query we term “containment queries”. It is also clear that in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises the question of how to support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.},
journal = {SIGMOD Rec.},
month = may,
pages = {425–436},
numpages = {12}
}

@inproceedings{10.1145/375663.375723,
author = {Nguyen, Benjamin and Abiteboul, Serge and Cobena, Gr\'{e}gory and Preda, Miha\'{\i}},
title = {Monitoring XML Data on the Web},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375723},
doi = {10.1145/375663.375723},
abstract = {We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following: a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. a new algorithm for processing alerts that can be used in a wider context. We support monitoring at the page level (e.g., discovery of a new page within a certain semantic domain) as well as at the element level (e.g., insertion of a new electronic product in a catalog).This work is part of the Xyleme system. Xyleme is developed on a cluster of PCs under Linux with Corba communications. The part of the system described in this paper has been implemented. We mention first experiments.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {437–448},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375723,
author = {Nguyen, Benjamin and Abiteboul, Serge and Cobena, Gr\'{e}gory and Preda, Miha\'{\i}},
title = {Monitoring XML Data on the Web},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375723},
doi = {10.1145/376284.375723},
abstract = {We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following: a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. a new algorithm for processing alerts that can be used in a wider context. We support monitoring at the page level (e.g., discovery of a new page within a certain semantic domain) as well as at the element level (e.g., insertion of a new electronic product in a catalog).This work is part of the Xyleme system. Xyleme is developed on a cluster of PCs under Linux with Corba communications. The part of the system described in this paper has been implemented. We mention first experiments.},
journal = {SIGMOD Rec.},
month = may,
pages = {437–448},
numpages = {12}
}

@inproceedings{10.1145/375663.375724,
author = {Wu, Yi-Leh and Agrawal, Divyakant and El Abbadi, Amr},
title = {Applying the Golden Rule of Sampling for Query Estimation},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375724},
doi = {10.1145/375663.375724},
abstract = {Query size estimation is crucial for many database system components. In particular, query optimizers need efficient and accurate query size estimation when deciding among alternative query plans. In this paper we propose a novel sampling technique based on the golden rule of sampling, introduced by von Neumann in 1947, for estimating range queries. The proposed technique randomly samples the frequency domain using the cumulative frequency distribution and yields good estimates without any a priori knowledge of the actual underlying distribution of spatial objects. We show experimentally that the proposed sampling technique gives smaller approximation error than the Min-Skew histogram based and wavelet based approaches for both synthetic and real datasets. Moreover, the proposed technique can be easily extended for higher dimensional datasets.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {449–460},
numpages = {12},
keywords = {random sampling, query estimation, range query, cumulative frequency distribution},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375724,
author = {Wu, Yi-Leh and Agrawal, Divyakant and El Abbadi, Amr},
title = {Applying the Golden Rule of Sampling for Query Estimation},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375724},
doi = {10.1145/376284.375724},
abstract = {Query size estimation is crucial for many database system components. In particular, query optimizers need efficient and accurate query size estimation when deciding among alternative query plans. In this paper we propose a novel sampling technique based on the golden rule of sampling, introduced by von Neumann in 1947, for estimating range queries. The proposed technique randomly samples the frequency domain using the cumulative frequency distribution and yields good estimates without any a priori knowledge of the actual underlying distribution of spatial objects. We show experimentally that the proposed sampling technique gives smaller approximation error than the Min-Skew histogram based and wavelet based approaches for both synthetic and real datasets. Moreover, the proposed technique can be easily extended for higher dimensional datasets.},
journal = {SIGMOD Rec.},
month = may,
pages = {449–460},
numpages = {12},
keywords = {query estimation, range query, random sampling, cumulative frequency distribution}
}

@inproceedings{10.1145/375663.375727,
author = {Getoor, Lise and Taskar, Benjamin and Koller, Daphne},
title = {Selectivity Estimation Using Probabilistic Models},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375727},
doi = {10.1145/375663.375727},
abstract = {Estimating the result size of complex queries that involve selection on multiple attributes and the join of several relations is a difficult but fundamental task in database query processing. It arises in cost-based query optimization, query profiling, and approximate query answering. In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations. Furthermore, our approach is not limited to answering a small set of predetermined queries; a single model can be used to effectively estimate the sizes of a wide collection of potential queries across multiple tables. We present results for our approach on several real-world databases. For both single-table multi-attribute queries and a general class of select-join queries, our approach produces more accurate estimates than standard approaches to selectivity estimation, using comparable space and time.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {461–472},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375727,
author = {Getoor, Lise and Taskar, Benjamin and Koller, Daphne},
title = {Selectivity Estimation Using Probabilistic Models},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375727},
doi = {10.1145/376284.375727},
abstract = {Estimating the result size of complex queries that involve selection on multiple attributes and the join of several relations is a difficult but fundamental task in database query processing. It arises in cost-based query optimization, query profiling, and approximate query answering. In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations. Furthermore, our approach is not limited to answering a small set of predetermined queries; a single model can be used to effectively estimate the sizes of a wide collection of potential queries across multiple tables. We present results for our approach on several real-world databases. For both single-table multi-attribute queries and a general class of select-join queries, our approach produces more accurate estimates than standard approaches to selectivity estimation, using comparable space and time.},
journal = {SIGMOD Rec.},
month = may,
pages = {461–472},
numpages = {12}
}

@inproceedings{10.1145/375663.375728,
author = {Schuster, Assaf and Wolff, Ran},
title = {Communication-Efficient Distributed Mining of Association Rules},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375728},
doi = {10.1145/375663.375728},
abstract = {Mining for associations between items in large transactional databases is a central problem in the field of knowledge discovery. When the database is partitioned among several share-nothing machines, the problem can be addressed using distributed data mining algorithms. One such algorithm, called CD, was proposed by Agrawal and Shafer in [1] and was later enhanced by the FDM algorithm of Cheung, Han et al. [5].The main problem with these algorithms is that they do not scale well with the number of partitions. They are thus impractical for use in modern distributed environments such as peer-to-peer systems, in which hundreds or thousands of computers may interact. In this paper we present a set of new algorithms that solve the Distributed Association Rule Mining problem using far less communication. In addition to being very efficient, the new algorithms are also extremely robust. Unlike existing algorithms, they continue to be efficient even when the data is skewed or the partition sizes are imbalanced. We present both experimental and theoretical results concerning the behavior of these algorithms and explain how they can be implemented in different settings.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {473–484},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375728,
author = {Schuster, Assaf and Wolff, Ran},
title = {Communication-Efficient Distributed Mining of Association Rules},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375728},
doi = {10.1145/376284.375728},
abstract = {Mining for associations between items in large transactional databases is a central problem in the field of knowledge discovery. When the database is partitioned among several share-nothing machines, the problem can be addressed using distributed data mining algorithms. One such algorithm, called CD, was proposed by Agrawal and Shafer in [1] and was later enhanced by the FDM algorithm of Cheung, Han et al. [5].The main problem with these algorithms is that they do not scale well with the number of partitions. They are thus impractical for use in modern distributed environments such as peer-to-peer systems, in which hundreds or thousands of computers may interact. In this paper we present a set of new algorithms that solve the Distributed Association Rule Mining problem using far less communication. In addition to being very efficient, the new algorithms are also extremely robust. Unlike existing algorithms, they continue to be efficient even when the data is skewed or the partition sizes are imbalanced. We present both experimental and theoretical results concerning the behavior of these algorithms and explain how they can be implemented in different settings.},
journal = {SIGMOD Rec.},
month = may,
pages = {473–484},
numpages = {12}
}

@inproceedings{10.1145/375663.375729,
author = {Yan, Ling Ling and Miller, Ren\'{e}e J. and Haas, Laura M. and Fagin, Ronald},
title = {Data-Driven Understanding and Refinement of Schema Mappings},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375729},
doi = {10.1145/375663.375729},
abstract = {At the heart of many data-intensive applications is the problem of quickly and accurately transforming data into a new form. Database researchers have long advocated the use of declarative queries for this process. Yet tools for creating, managing and understanding the complex queries necessary for data transformation are still too primitive to permit widespread adoption of this approach. We present a new framework that uses data examples as the basis for understanding and refining declarative schema mappings. We identify a small set of intuitive operators for manipulating examples. These operators permit a user to follow and refine an example by walking through a data source. We show that our operators are powerful enough both to identify a large class of schema mappings and to distinguish effectively between alternative schema mappings. These operators permit a user to quickly and intuitively build and refine complex data transformation queries that map one data source into another.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {485–496},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375729,
author = {Yan, Ling Ling and Miller, Ren\'{e}e J. and Haas, Laura M. and Fagin, Ronald},
title = {Data-Driven Understanding and Refinement of Schema Mappings},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375729},
doi = {10.1145/376284.375729},
abstract = {At the heart of many data-intensive applications is the problem of quickly and accurately transforming data into a new form. Database researchers have long advocated the use of declarative queries for this process. Yet tools for creating, managing and understanding the complex queries necessary for data transformation are still too primitive to permit widespread adoption of this approach. We present a new framework that uses data examples as the basis for understanding and refining declarative schema mappings. We identify a small set of intuitive operators for manipulating examples. These operators permit a user to follow and refine an example by walking through a data source. We show that our operators are powerful enough both to identify a large class of schema mappings and to distinguish effectively between alternative schema mappings. These operators permit a user to quickly and intuitively build and refine complex data transformation queries that map one data source into another.},
journal = {SIGMOD Rec.},
month = may,
pages = {485–496},
numpages = {12}
}

@inproceedings{10.1145/375663.375730,
author = {Amer-Yahia, Sihem and Cho, SungRan and Lakshmanan, Laks V. S. and Srivastava, Divesh},
title = {Minimization of Tree Pattern Queries},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375730},
doi = {10.1145/375663.375730},
abstract = {Tree patterns forms a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database.When no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM's efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For tree-structured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICS, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating “information labels” up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {497–508},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375730,
author = {Amer-Yahia, Sihem and Cho, SungRan and Lakshmanan, Laks V. S. and Srivastava, Divesh},
title = {Minimization of Tree Pattern Queries},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375730},
doi = {10.1145/376284.375730},
abstract = {Tree patterns forms a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database.When no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM's efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For tree-structured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICS, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating “information labels” up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.},
journal = {SIGMOD Rec.},
month = may,
pages = {497–508},
numpages = {12}
}

@inproceedings{10.1145/375663.375731,
author = {Doan, AnHai and Domingos, Pedro and Halevy, Alon Y.},
title = {Reconciling Schemas of Disparate Data Sources: A Machine-Learning Approach},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375731},
doi = {10.1145/375663.375731},
abstract = {A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {509–520},
numpages = {12},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375731,
author = {Doan, AnHai and Domingos, Pedro and Halevy, Alon Y.},
title = {Reconciling Schemas of Disparate Data Sources: A Machine-Learning Approach},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375731},
doi = {10.1145/376284.375731},
abstract = {A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy.},
journal = {SIGMOD Rec.},
month = may,
pages = {509–520},
numpages = {12}
}

@inproceedings{10.1145/375663.375733,
author = {Cari\~{n}o, Felipe and Kostamaa, Pekka and Kaufmann, Art and Burgess, John},
title = {StorHouse Metanoia - New Applications for Database, Storage &amp; Data Warehousing},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375733},
doi = {10.1145/375663.375733},
abstract = {This paper describes the StorHouse/Relational Manager (RM) database system that uses and exploits an active storage hierarchy. By active storage hierarchy, we mean that StorHouse/RM executes SQL queries directly against data stored on all hierarchical storage (i.e. disk, optical, and tape) without post processing a file or a DBA having to manage a data set. We describe and analyze StorHouse/RM features and internals. We also describe how StorHouse/RM differs from traditional HSM (Hierarchical Storage Management) systems. For commercial applications we describe an evolution to the Data Warehouse concept, called Atomic Data Store, whereby atomic data is stored in the database system. Atomic data is defined as storing all the historic data values and executing queries against them. We also describe a Hub-and-Spoke Data Warehouse architecture, which is used to feed or fuel data into Data Marts.Furthermore, we provide analysis how StorHouse/RM can be federated with DB2, Oracle and Microsoft SQL Server 7 (SS7) and thus provide these databases an active storage hierarchy (i.e. tape). We then show two federated data modeling techniques (a) logical horizontal partitioning (LHP) of tuples and (b) logical vertical partitioning (LVP) of columns to demonstrate our database extension capabilities. We conclude with a TPC-like performance analysis of data stored on tape and disk.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {521–531},
numpages = {11},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375733,
author = {Cari\~{n}o, Felipe and Kostamaa, Pekka and Kaufmann, Art and Burgess, John},
title = {StorHouse Metanoia - New Applications for Database, Storage &amp; Data Warehousing},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375733},
doi = {10.1145/376284.375733},
abstract = {This paper describes the StorHouse/Relational Manager (RM) database system that uses and exploits an active storage hierarchy. By active storage hierarchy, we mean that StorHouse/RM executes SQL queries directly against data stored on all hierarchical storage (i.e. disk, optical, and tape) without post processing a file or a DBA having to manage a data set. We describe and analyze StorHouse/RM features and internals. We also describe how StorHouse/RM differs from traditional HSM (Hierarchical Storage Management) systems. For commercial applications we describe an evolution to the Data Warehouse concept, called Atomic Data Store, whereby atomic data is stored in the database system. Atomic data is defined as storing all the historic data values and executing queries against them. We also describe a Hub-and-Spoke Data Warehouse architecture, which is used to feed or fuel data into Data Marts.Furthermore, we provide analysis how StorHouse/RM can be federated with DB2, Oracle and Microsoft SQL Server 7 (SS7) and thus provide these databases an active storage hierarchy (i.e. tape). We then show two federated data modeling techniques (a) logical horizontal partitioning (LHP) of tuples and (b) logical vertical partitioning (LVP) of columns to demonstrate our database extension capabilities. We conclude with a TPC-like performance analysis of data stored on tape and disk.},
journal = {SIGMOD Rec.},
month = may,
pages = {521–531},
numpages = {11}
}

@inproceedings{10.1145/375663.375736,
author = {Candan, K. Sel\c{c}uk and Li, Wen-Syan and Luo, Qiong and Hsiung, Wang-Pin and Agrawal, Divyakant},
title = {Enabling Dynamic Content Caching for Database-Driven Web Sites},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375736},
doi = {10.1145/375663.375736},
abstract = {Web performance is a key differentiation among content providers. Snafus and slowdowns at major web sites demonstrate the difficulty that companies face trying to scale to a large amount of web traffic. One solution to this problem is to store web content at server-side and edge-caches for fast delivery to the end users. However, for many e-commerce sites, web pages are created dynamically based on the current state of business processes, represented in application servers and databases. Since application servers, databases, web servers, and caches are independent components, there is no efficient mechanism to make changes in the database content reflected to the cached web pages. As a result, most application servers have to mark dynamically generated web pages as non-cacheable. In this paper, we describe the architectural framework of the CachePortal system for enabling dynamic content caching for database-driven e-commerce sites. We describe techniques for intelligently invalidating dynamically generated web pages in the caches, thereby enabling caching of web pages generated based on database contents. We use some of the most popular components in the industry to illustrate the deployment and applicability of the proposed architecture.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {532–543},
numpages = {12},
keywords = {JDBC, web acceleration, application server, database driven web site, dynamic content caching, invalidation},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375736,
author = {Candan, K. Sel\c{c}uk and Li, Wen-Syan and Luo, Qiong and Hsiung, Wang-Pin and Agrawal, Divyakant},
title = {Enabling Dynamic Content Caching for Database-Driven Web Sites},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375736},
doi = {10.1145/376284.375736},
abstract = {Web performance is a key differentiation among content providers. Snafus and slowdowns at major web sites demonstrate the difficulty that companies face trying to scale to a large amount of web traffic. One solution to this problem is to store web content at server-side and edge-caches for fast delivery to the end users. However, for many e-commerce sites, web pages are created dynamically based on the current state of business processes, represented in application servers and databases. Since application servers, databases, web servers, and caches are independent components, there is no efficient mechanism to make changes in the database content reflected to the cached web pages. As a result, most application servers have to mark dynamically generated web pages as non-cacheable. In this paper, we describe the architectural framework of the CachePortal system for enabling dynamic content caching for database-driven e-commerce sites. We describe techniques for intelligently invalidating dynamically generated web pages in the caches, thereby enabling caching of web pages generated based on database contents. We use some of the most popular components in the industry to illustrate the deployment and applicability of the proposed architecture.},
journal = {SIGMOD Rec.},
month = may,
pages = {532–543},
numpages = {12},
keywords = {dynamic content caching, JDBC, web acceleration, database driven web site, invalidation, application server}
}

@inproceedings{10.1145/375663.375737,
author = {Navas, Julio C. and Wynblatt, Michael},
title = {The Network is the Database: Data Management for Highly Distributed Systems},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375737},
doi = {10.1145/375663.375737},
abstract = {This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {544–551},
numpages = {8},
keywords = {distributed data management, logistics, wide-area data management, sensor networks},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375737,
author = {Navas, Julio C. and Wynblatt, Michael},
title = {The Network is the Database: Data Management for Highly Distributed Systems},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375737},
doi = {10.1145/376284.375737},
abstract = {This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.},
journal = {SIGMOD Rec.},
month = may,
pages = {544–551},
numpages = {8},
keywords = {wide-area data management, distributed data management, sensor networks, logistics}
}

@inproceedings{10.1145/375663.375739,
author = {Stonebraker, Michael and Hellerstein, Joseph M.},
title = {Content Integration for E-Business},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375739},
doi = {10.1145/375663.375739},
abstract = {We define the problem of content integration for E-Business, and show how it differs in fundamental ways from traditional issues surrounding data integration, application integration, data warehousing and OLTP. Content integration includes catalog integration as a special case, but encompasses a broader set of applications and challenges. We explore the characteristics of content integration and required services for any solution. In addition, we explore architectural alternatives and discuss the use of XML in this arena.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {552–560},
numpages = {9},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375739,
author = {Stonebraker, Michael and Hellerstein, Joseph M.},
title = {Content Integration for E-Business},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375739},
doi = {10.1145/376284.375739},
abstract = {We define the problem of content integration for E-Business, and show how it differs in fundamental ways from traditional issues surrounding data integration, application integration, data warehousing and OLTP. Content integration includes catalog integration as a special case, but encompasses a broader set of applications and challenges. We explore the characteristics of content integration and required services for any solution. In addition, we explore architectural alternatives and discuss the use of XML in this arena.},
journal = {SIGMOD Rec.},
month = may,
pages = {552–560},
numpages = {9}
}

@inproceedings{10.1145/375663.375742,
author = {Maguire, Thomas},
title = {Catalog Management in Websphere Commerce Suite},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375742},
doi = {10.1145/375663.375742},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {561},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375742,
author = {Maguire, Thomas},
title = {Catalog Management in Websphere Commerce Suite},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375742},
doi = {10.1145/376284.375742},
journal = {SIGMOD Rec.},
month = may,
pages = {561},
numpages = {1}
}

@inproceedings{10.1145/375663.375743,
author = {Nazeri, Zohreh and Bloedorn, Eric and Ostwald, Paul},
title = {Experiences in Mining Aviation Safety Data},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375743},
doi = {10.1145/375663.375743},
abstract = {The goal of data analysis in aviation safety is simple: improve safety. However, the path to this goal is hard to identify. What data mining methods are most applicable to this task? What data are available and how should they be analyzed? How do we focus on the most interesting results? Our answers to these questions are based on a recent research project we completed. The encouraging news is that we found a number of aviation safety offices doing commendable work to collect and analyze safety-related data. But we also found a number of areas where data mining techniques could provide new tools that either perform analyses that were not considered before, or that can now be done more easily.Currently, Aviation Safety offices collect and analyze the incident reports by a combination of manual and automated methods. Data analysis is done by safety officers who are well familiar with the domain, but not with data mining methods. Some Aviation Safety officers have tools to automate the database query and report generation process. However, the actual analysis is done by the officer with only fairly rudimentary tools to help extract the useful information from the data.Our research project looked at the application of data mining techniques to aviation safety data to help Aviation Safety officers with their analysis task. This effort led to the creation of a tool called the “Aviation Safety Data Mining Workbench”. This paper describes the research effort, the workbench, the experience with data mining of Aviation Safety data, and lessons learned.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {562–566},
numpages = {5},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375743,
author = {Nazeri, Zohreh and Bloedorn, Eric and Ostwald, Paul},
title = {Experiences in Mining Aviation Safety Data},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375743},
doi = {10.1145/376284.375743},
abstract = {The goal of data analysis in aviation safety is simple: improve safety. However, the path to this goal is hard to identify. What data mining methods are most applicable to this task? What data are available and how should they be analyzed? How do we focus on the most interesting results? Our answers to these questions are based on a recent research project we completed. The encouraging news is that we found a number of aviation safety offices doing commendable work to collect and analyze safety-related data. But we also found a number of areas where data mining techniques could provide new tools that either perform analyses that were not considered before, or that can now be done more easily.Currently, Aviation Safety offices collect and analyze the incident reports by a combination of manual and automated methods. Data analysis is done by safety officers who are well familiar with the domain, but not with data mining methods. Some Aviation Safety officers have tools to automate the database query and report generation process. However, the actual analysis is done by the officer with only fairly rudimentary tools to help extract the useful information from the data.Our research project looked at the application of data mining techniques to aviation safety data to help Aviation Safety officers with their analysis task. This effort led to the creation of a tool called the “Aviation Safety Data Mining Workbench”. This paper describes the research effort, the workbench, the experience with data mining of Aviation Safety data, and lessons learned.},
journal = {SIGMOD Rec.},
month = may,
pages = {562–566},
numpages = {5}
}

@inproceedings{10.1145/375663.375744,
author = {Draper, Denise and Halevy, Alon Y. and Weld, Daniel S.},
title = {The Nimble Integration Engine},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375744},
doi = {10.1145/375663.375744},
abstract = {The consensus that XML has become the de facto standard for data interchange will spur demand for technology that allows users to integrate data from a variety of applications, repositories, and legacy systems which are located across the corporate intranet or at partner companies on the Internet.In the past two years, Nimble Technology has developed a product for this market. Spawned from over a person-decade of data integration research, the product has been deployed at several Fortune-500 beta-customer sites. This abstract reports on the key challenges we faced in the design of our product and highlights some issues we think require more attention from the research community.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {567–568},
numpages = {2},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375744,
author = {Draper, Denise and Halevy, Alon Y. and Weld, Daniel S.},
title = {The Nimble Integration Engine},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375744},
doi = {10.1145/376284.375744},
abstract = {The consensus that XML has become the de facto standard for data interchange will spur demand for technology that allows users to integrate data from a variety of applications, repositories, and legacy systems which are located across the corporate intranet or at partner companies on the Internet.In the past two years, Nimble Technology has developed a product for this market. Spawned from over a person-decade of data integration research, the product has been deployed at several Fortune-500 beta-customer sites. This abstract reports on the key challenges we faced in the design of our product and highlights some issues we think require more attention from the research community.},
journal = {SIGMOD Rec.},
month = may,
pages = {567–568},
numpages = {2}
}

@inproceedings{10.1145/375663.375745,
author = {Meseck, Reed M.},
title = {Data Management: Lasting Impact on Wild, Wild, Web},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375745},
doi = {10.1145/375663.375745},
abstract = {This paper describes some of the ways the Internet and World Wide Web have affected databases and data warehousing and the lasting impact in these areas.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {569–570},
numpages = {2},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375745,
author = {Meseck, Reed M.},
title = {Data Management: Lasting Impact on Wild, Wild, Web},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375745},
doi = {10.1145/376284.375745},
abstract = {This paper describes some of the ways the Internet and World Wide Web have affected databases and data warehousing and the lasting impact in these areas.},
journal = {SIGMOD Rec.},
month = may,
pages = {569–570},
numpages = {2}
}

@inproceedings{10.1145/375663.375748,
author = {Galindo-Legaria, C\'{e}sar and Joshi, Milind},
title = {Orthogonal Optimization of Subqueries and Aggregation},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375748},
doi = {10.1145/375663.375748},
abstract = {There is considerable overlap between strategies proposed for subquery evaluation, and those for grouping and aggregation. In this paper we show how a number of small, independent primitives generate a rich set of efficient execution strategies —covering standard proposals for subquery evaluation suggested in earlier literature. These small primitives fall into two main, orthogonal areas: Correlation removal, and efficient processing of outerjoins and GroupBy. An optimization approach based on these pieces provides syntax-independence of query processing with respect to subqueries, i. e. equivalent queries written with or without subquery produce the same efficient plan.We describe techniques implemented in Microsoft SQL Server (releases 7.0 and 8.0) for queries containing sub-queries and/or aggregations, based on a number of orthogonal optimizations. We concentrate separately on removing correlated subqueries, also called “query flattening,” and on efficient execution of queries with aggregations. The end result is a modular, flexible implementation, which produces very efficient execution plans. To demonstrate the validity of our approach, we present results for some queries from the TPC-H benchmark. From all published TPC-H results in the 300GB scale, at the time of writing (November 2000), SQL Server has the fastest results on those queries, even on a fraction of the processors used by other systems.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {571–581},
numpages = {11},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375748,
author = {Galindo-Legaria, C\'{e}sar and Joshi, Milind},
title = {Orthogonal Optimization of Subqueries and Aggregation},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375748},
doi = {10.1145/376284.375748},
abstract = {There is considerable overlap between strategies proposed for subquery evaluation, and those for grouping and aggregation. In this paper we show how a number of small, independent primitives generate a rich set of efficient execution strategies —covering standard proposals for subquery evaluation suggested in earlier literature. These small primitives fall into two main, orthogonal areas: Correlation removal, and efficient processing of outerjoins and GroupBy. An optimization approach based on these pieces provides syntax-independence of query processing with respect to subqueries, i. e. equivalent queries written with or without subquery produce the same efficient plan.We describe techniques implemented in Microsoft SQL Server (releases 7.0 and 8.0) for queries containing sub-queries and/or aggregations, based on a number of orthogonal optimizations. We concentrate separately on removing correlated subqueries, also called “query flattening,” and on efficient execution of queries with aggregations. The end result is a modular, flexible implementation, which produces very efficient execution plans. To demonstrate the validity of our approach, we present results for some queries from the TPC-H benchmark. From all published TPC-H results in the 300GB scale, at the time of writing (November 2000), SQL Server has the fastest results on those queries, even on a fraction of the processors used by other systems.},
journal = {SIGMOD Rec.},
month = may,
pages = {571–581},
numpages = {11}
}

@inproceedings{10.1145/375663.375749,
author = {Godfrey, Parke and Gryz, Jarek and Zuzarte, Calisto},
title = {Exploiting Constraint-like Data Characterizations in Query Optimization},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375749},
doi = {10.1145/375663.375749},
abstract = {Query optimizers nowadays draw upon many sources of information about the database to optimize queries. They employ runtime statistics in cost-based estimation of query plans. They employ integrity constraints in the query rewrite process. Primary and foreign key constraints have long played a role in the optimizer, both for rewrite opportunities and for providing more accurate cost predictions. More recently, other types of integrity constraints are being exploited by optimizers in commercial systems, for which certain semantic query optimization techniques have now been implemented.These new optimization strategies that exploit constraints hold the promise for good improvement. Their weakness, however, is that often the “constraints” that would be useful for optimization for a given database and workload are not explicitly available for the optimizer. Data mining tools can find such “constraints” that are true of the database, but then there is the question of how this information can be kept by the database system, and how to make this information available to, and effectively usable by, the optimizer.We present our work on soft constraints in DB2. A soft constraint is a syntactic statement equivalent to an integrity constraint declaration. A soft constraint is not really a constraint, per se, since future updates may undermine it. While a soft constraint is valid, however, it can be used by the optimizer in the same way integrity constraints are. We present two forms of soft constraint: absolute and statistical. An absolute soft constraint is consistent with respect to the current state of the database, just in the same way an integrity constraint must be. They can be used in rewrite, as well as in cost estimation. A statistical soft constraint differs in that it may have some degree of violation with respect to the state of the database. Thus, statistical soft constraints cannot be used in rewrite, but they can still be used in cost estimation.We are working long-term on absolute soft constraints. We discuss the issues involved in implementing a facility for absolute soft constraints in a database system (and in DB2), and the strategies that we are researching. The current DB2 optimizer is more amenable to adding facilities for statistical soft constraints. In the short-term, we have been implementing pathways in the optimizer for statistical soft constraints. We discuss this implementation.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {582–592},
numpages = {11},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375749,
author = {Godfrey, Parke and Gryz, Jarek and Zuzarte, Calisto},
title = {Exploiting Constraint-like Data Characterizations in Query Optimization},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375749},
doi = {10.1145/376284.375749},
abstract = {Query optimizers nowadays draw upon many sources of information about the database to optimize queries. They employ runtime statistics in cost-based estimation of query plans. They employ integrity constraints in the query rewrite process. Primary and foreign key constraints have long played a role in the optimizer, both for rewrite opportunities and for providing more accurate cost predictions. More recently, other types of integrity constraints are being exploited by optimizers in commercial systems, for which certain semantic query optimization techniques have now been implemented.These new optimization strategies that exploit constraints hold the promise for good improvement. Their weakness, however, is that often the “constraints” that would be useful for optimization for a given database and workload are not explicitly available for the optimizer. Data mining tools can find such “constraints” that are true of the database, but then there is the question of how this information can be kept by the database system, and how to make this information available to, and effectively usable by, the optimizer.We present our work on soft constraints in DB2. A soft constraint is a syntactic statement equivalent to an integrity constraint declaration. A soft constraint is not really a constraint, per se, since future updates may undermine it. While a soft constraint is valid, however, it can be used by the optimizer in the same way integrity constraints are. We present two forms of soft constraint: absolute and statistical. An absolute soft constraint is consistent with respect to the current state of the database, just in the same way an integrity constraint must be. They can be used in rewrite, as well as in cost estimation. A statistical soft constraint differs in that it may have some degree of violation with respect to the state of the database. Thus, statistical soft constraints cannot be used in rewrite, but they can still be used in cost estimation.We are working long-term on absolute soft constraints. We discuss the issues involved in implementing a facility for absolute soft constraints in a database system (and in DB2), and the strategies that we are researching. The current DB2 optimizer is more amenable to adding facilities for statistical soft constraints. In the short-term, we have been implementing pathways in the optimizer for statistical soft constraints. We discuss this implementation.},
journal = {SIGMOD Rec.},
month = may,
pages = {582–592},
numpages = {11}
}

@inproceedings{10.1145/375663.375751,
author = {Lahiri, Tirthankar and Ganesh, Amit and Weiss, Ron and Joshi, Ashok},
title = {Fast-Start: Quick Fault Recovery in Oracle},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375751},
doi = {10.1145/375663.375751},
abstract = {Availability requirements for database systems are more stringent than ever before with the widespread use of databases as the foundation for ebusiness. This paper highlights Fast-Start™ Fault Recovery, an important availability feature in Oracle, designed to expedite recovery from unplanned outages. Fast-Start allows the administrator to configure a running system to impose predictable bounds on the time required for crash recovery. For instance, fast-start allows fine-grained control over the duration of the roll-forward phase of crash recovery by adaptively varying the rate of checkpointing with minimal impact on online performance. Persistent transaction locking in Oracle allows normal online processing to be resumed while the rollback phase of recovery is still in progress, and fast-start allows quick and transparent rollback of changes made by uncommitted transactions prior to a crash.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {593–598},
numpages = {6},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375751,
author = {Lahiri, Tirthankar and Ganesh, Amit and Weiss, Ron and Joshi, Ashok},
title = {Fast-Start: Quick Fault Recovery in Oracle},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375751},
doi = {10.1145/376284.375751},
abstract = {Availability requirements for database systems are more stringent than ever before with the widespread use of databases as the foundation for ebusiness. This paper highlights Fast-Start™ Fault Recovery, an important availability feature in Oracle, designed to expedite recovery from unplanned outages. Fast-Start allows the administrator to configure a running system to impose predictable bounds on the time required for crash recovery. For instance, fast-start allows fine-grained control over the duration of the roll-forward phase of crash recovery by adaptively varying the rate of checkpointing with minimal impact on online performance. Persistent transaction locking in Oracle allows normal online processing to be resumed while the rollback phase of recovery is still in progress, and fast-start allows quick and transparent rollback of changes made by uncommitted transactions prior to a crash.},
journal = {SIGMOD Rec.},
month = may,
pages = {593–598},
numpages = {6}
}

@inproceedings{10.1145/375663.375752,
author = {Deolasee, Pavan and Katkar, Amol and Panchbudhe, Ankur and Ramamritham, Krithi and Shenoy, Prashant},
title = {Dissemination of Dynamic Data},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375752},
doi = {10.1145/375663.375752},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {599},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375752,
author = {Deolasee, Pavan and Katkar, Amol and Panchbudhe, Ankur and Ramamritham, Krithi and Shenoy, Prashant},
title = {Dissemination of Dynamic Data},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375752},
doi = {10.1145/376284.375752},
journal = {SIGMOD Rec.},
month = may,
pages = {599},
numpages = {1}
}

@inproceedings{10.1145/375663.375754,
author = {Kieβling, Werner and Holland, Stefan and Fischer, Stefan and Ehm, Thorsten},
title = {COSIMA- Your Smart, Speaking E-Salesperson},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375754},
doi = {10.1145/375663.375754},
abstract = {We present a new cooperative user interface for e-shopping. COSIMA is an intelligent Internet avatar with dynamic voice output that assists customers through their e-shopping tours and advises them like a real salesperson. COSIMA uses a meta search engine based on Preference SQL, computing best matching results to the customer's wishes. COSIMA can qualify these results and generates proper voice output. Our presentation shows COSIMA in action for comparison shopping.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {600},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375754,
author = {Kieβling, Werner and Holland, Stefan and Fischer, Stefan and Ehm, Thorsten},
title = {COSIMA- Your Smart, Speaking E-Salesperson},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375754},
doi = {10.1145/376284.375754},
abstract = {We present a new cooperative user interface for e-shopping. COSIMA is an intelligent Internet avatar with dynamic voice output that assists customers through their e-shopping tours and advises them like a real salesperson. COSIMA uses a meta search engine based on Preference SQL, computing best matching results to the customer's wishes. COSIMA can qualify these results and generates proper voice output. Our presentation shows COSIMA in action for comparison shopping.},
journal = {SIGMOD Rec.},
month = may,
pages = {600},
numpages = {1}
}

@inproceedings{10.1145/375663.375755,
author = {Shou, L. and Chionh, C. H. and Huang, Z. and Ruan, Y. and Tan, Kian-Lee},
title = {REVIEW: A Real-Time Virtual Walkthrough System},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375755},
doi = {10.1145/375663.375755},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {601},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375755,
author = {Shou, L. and Chionh, C. H. and Huang, Z. and Ruan, Y. and Tan, Kian-Lee},
title = {REVIEW: A Real-Time Virtual Walkthrough System},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375755},
doi = {10.1145/376284.375755},
journal = {SIGMOD Rec.},
month = may,
pages = {601},
numpages = {1}
}

@inproceedings{10.1145/375663.375757,
author = {Sahuguet, Arnaud},
title = {Kweelt: More than Just “yet Another Framework to Query XML!”},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375757},
doi = {10.1145/375663.375757},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {602},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375757,
author = {Sahuguet, Arnaud},
title = {Kweelt: More than Just “yet Another Framework to Query XML!”},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375757},
doi = {10.1145/376284.375757},
journal = {SIGMOD Rec.},
month = may,
pages = {602},
numpages = {1}
}

@inproceedings{10.1145/375663.375761,
author = {Chawathe, Sudarshan S. and Baby, Thomas and Yoo, Jihwang},
title = {VQBD: Exploring Semistructured Data},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375761},
doi = {10.1145/375663.375761},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {603},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375761,
author = {Chawathe, Sudarshan S. and Baby, Thomas and Yoo, Jihwang},
title = {VQBD: Exploring Semistructured Data},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375761},
doi = {10.1145/376284.375761},
journal = {SIGMOD Rec.},
month = may,
pages = {603},
numpages = {1}
}

@inproceedings{10.1145/375663.375762,
author = {Buttler, David and Liu, Ling and Pu, Calton and Paques, Henrique and Han, Wei and Tang, Wei},
title = {OminiSearch: A Method for Searching Dynamic Content on the Web},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375762},
doi = {10.1145/375663.375762},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {604},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375762,
author = {Buttler, David and Liu, Ling and Pu, Calton and Paques, Henrique and Han, Wei and Tang, Wei},
title = {OminiSearch: A Method for Searching Dynamic Content on the Web},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375762},
doi = {10.1145/376284.375762},
journal = {SIGMOD Rec.},
month = may,
pages = {604},
numpages = {1}
}

@inproceedings{10.1145/375663.375764,
author = {Bertino, Elisa and Castano, Silvana and Ferrari, Elena},
title = {Securing XML Documents: The Author-X Project Demonstration},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375764},
doi = {10.1145/375663.375764},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {605},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375764,
author = {Bertino, Elisa and Castano, Silvana and Ferrari, Elena},
title = {Securing XML Documents: The Author-X Project Demonstration},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375764},
doi = {10.1145/376284.375764},
journal = {SIGMOD Rec.},
month = may,
pages = {605},
numpages = {1}
}

@inproceedings{10.1145/375663.375765,
author = {Claypool, Kajal T. and Rundensteiner, Elke A. and Zhang, Xin and Hong, Su and Kuno, Harumi and Lee, Wang-chien and Mitchell, Gail},
title = {Sangam - a Solution to Support Multiple Data Models, Their Mappings and Maintenance},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375765},
doi = {10.1145/375663.375765},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {606},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375765,
author = {Claypool, Kajal T. and Rundensteiner, Elke A. and Zhang, Xin and Hong, Su and Kuno, Harumi and Lee, Wang-chien and Mitchell, Gail},
title = {Sangam - a Solution to Support Multiple Data Models, Their Mappings and Maintenance},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375765},
doi = {10.1145/376284.375765},
journal = {SIGMOD Rec.},
month = may,
pages = {606},
numpages = {1}
}

@inproceedings{10.1145/375663.375767,
author = {Hern\'{a}ndez, Mauricio A. and Miller, Ren\'{e}e J. and Haas, Laura M.},
title = {Clio: A Semi-Automatic Tool for Schema Mapping},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375767},
doi = {10.1145/375663.375767},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {607},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375767,
author = {Hern\'{a}ndez, Mauricio A. and Miller, Ren\'{e}e J. and Haas, Laura M.},
title = {Clio: A Semi-Automatic Tool for Schema Mapping},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375767},
doi = {10.1145/376284.375767},
journal = {SIGMOD Rec.},
month = may,
pages = {607},
numpages = {1}
}

@inproceedings{10.1145/375663.375769,
author = {Agrawal, Sanjay and Chaudhuri, Surajit and Narasayya, Vivek},
title = {Materialized View and Index Selection Tool for Microsoft SQL Server 2000},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375769},
doi = {10.1145/375663.375769},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {608},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375769,
author = {Agrawal, Sanjay and Chaudhuri, Surajit and Narasayya, Vivek},
title = {Materialized View and Index Selection Tool for Microsoft SQL Server 2000},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375769},
doi = {10.1145/376284.375769},
journal = {SIGMOD Rec.},
month = may,
pages = {608},
numpages = {1}
}

@inproceedings{10.1145/375663.375771,
author = {Catarci, Tiziana and Santucci, Giuseppe},
title = {The Prototype of the DARE System},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375771},
doi = {10.1145/375663.375771},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {609},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375771,
author = {Catarci, Tiziana and Santucci, Giuseppe},
title = {The Prototype of the DARE System},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375771},
doi = {10.1145/376284.375771},
journal = {SIGMOD Rec.},
month = may,
pages = {609},
numpages = {1}
}

@inproceedings{10.1145/375663.375759,
author = {Adii, Asaf and Botzer, David and Etzion, Opher and Yatzkar-Haham, Tali},
title = {Monitoring Business Processes through Event Correlation Based on Dependency Model},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375759},
doi = {10.1145/375663.375759},
abstract = {Events are at the core of reactive and proactive applications, which have become popular in many domains.This demo shows the monitoring of incoming events as a means to detect possible problems in the course of business processes using a dependency model.Contemporary modeling tools lack the capability to express the event semantics and relationships to other entities. This capability is useful when the events are based on a dependency model among business processes, applications and resources. The ability to express an event by employing a general dependency model, an to use it through a designated event correlation monitoring tool, enables the accomplishment of tasks such as impact analysis and business processes monitoring, including prediction of violation of constraints (such as: service level agreements).This demonstrated tool provides the system designer with the ability to define and describe events and their relationships to other events, objects and tasks. The model employs various conditional dependencies that are specific to the event domain. The demo shows how systems (business processes) are monitored using the dependency / event model, by applying rules using an event correlation engine with strong expressive power.This demo proposal describes the generic application development tool, the middleware architecture and framework and the demo.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {610},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375759,
author = {Adii, Asaf and Botzer, David and Etzion, Opher and Yatzkar-Haham, Tali},
title = {Monitoring Business Processes through Event Correlation Based on Dependency Model},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375759},
doi = {10.1145/376284.375759},
abstract = {Events are at the core of reactive and proactive applications, which have become popular in many domains.This demo shows the monitoring of incoming events as a means to detect possible problems in the course of business processes using a dependency model.Contemporary modeling tools lack the capability to express the event semantics and relationships to other entities. This capability is useful when the events are based on a dependency model among business processes, applications and resources. The ability to express an event by employing a general dependency model, an to use it through a designated event correlation monitoring tool, enables the accomplishment of tasks such as impact analysis and business processes monitoring, including prediction of violation of constraints (such as: service level agreements).This demonstrated tool provides the system designer with the ability to define and describe events and their relationships to other events, objects and tasks. The model employs various conditional dependencies that are specific to the event domain. The demo shows how systems (business processes) are monitored using the dependency / event model, by applying rules using an event correlation engine with strong expressive power.This demo proposal describes the generic application development tool, the middleware architecture and framework and the demo.},
journal = {SIGMOD Rec.},
month = may,
pages = {610},
numpages = {1}
}

@inproceedings{10.1145/375663.375773,
author = {Shah, Mehul A. and Chandrasekaran, Sirish},
title = {Fault-Tolerant, Load-Balancing Queries in Telegraph},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375773},
doi = {10.1145/375663.375773},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {611},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375773,
author = {Shah, Mehul A. and Chandrasekaran, Sirish},
title = {Fault-Tolerant, Load-Balancing Queries in Telegraph},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375773},
doi = {10.1145/376284.375773},
journal = {SIGMOD Rec.},
month = may,
pages = {611},
numpages = {1}
}

@inproceedings{10.1145/375663.375774,
author = {Agichtein, Eugene and Gravano, Luis and Pavel, Jeff and Sokolova, Viktoriya and Voskoboynik, Aleksandr},
title = {Snowball: A Prototype System for Extracting Relations from Large Text Collections},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375774},
doi = {10.1145/375663.375774},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {612},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375774,
author = {Agichtein, Eugene and Gravano, Luis and Pavel, Jeff and Sokolova, Viktoriya and Voskoboynik, Aleksandr},
title = {Snowball: A Prototype System for Extracting Relations from Large Text Collections},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375774},
doi = {10.1145/376284.375774},
journal = {SIGMOD Rec.},
month = may,
pages = {612},
numpages = {1}
}

@inproceedings{10.1145/375663.375776,
author = {Chang, Edward and Cheng, Kwang-Ting and Chang, Lihyuarn L.},
title = {PBIR - Perception-Based Image Retrieval},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375776},
doi = {10.1145/375663.375776},
abstract = {We demonstrate a system that we have built on our proposed perception-based image retrieval (PBIR) paradigm. This PBIR system achieves accurate similarity measurements by rooting image characterization in human perception and by learning user's query concept through an intelligent sampling process. We show that our system can usually grasp a user's query concept with a small number of labeled instances.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {613},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375776,
author = {Chang, Edward and Cheng, Kwang-Ting and Chang, Lihyuarn L.},
title = {PBIR - Perception-Based Image Retrieval},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375776},
doi = {10.1145/376284.375776},
abstract = {We demonstrate a system that we have built on our proposed perception-based image retrieval (PBIR) paradigm. This PBIR system achieves accurate similarity measurements by rooting image characterization in human perception and by learning user's query concept through an intelligent sampling process. We show that our system can usually grasp a user's query concept with a small number of labeled instances.},
journal = {SIGMOD Rec.},
month = may,
pages = {613},
numpages = {1}
}

@inproceedings{10.1145/375663.375777,
author = {Kriegel, Hans-Peter and M\"{u}ller, Andreas and P\"{o}tke, Marco and Seidl, Thomas},
title = {Spatial Data Management for Computer Aided Design},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375777},
doi = {10.1145/375663.375777},
abstract = {This demonstration presents a spatial database integration for novel CAD applications into off-the-shelf database systems. Spatial queries on even large product databases for digital mockup or haptic rendering are performed at interactive response times.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {614},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375777,
author = {Kriegel, Hans-Peter and M\"{u}ller, Andreas and P\"{o}tke, Marco and Seidl, Thomas},
title = {Spatial Data Management for Computer Aided Design},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375777},
doi = {10.1145/376284.375777},
abstract = {This demonstration presents a spatial database integration for novel CAD applications into off-the-shelf database systems. Spatial queries on even large product databases for digital mockup or haptic rendering are performed at interactive response times.},
journal = {SIGMOD Rec.},
month = may,
pages = {614},
numpages = {1}
}

@inproceedings{10.1145/375663.375779,
author = {Lam, Kam-Yiu and Chan, Edward and Kuo, Tei-Wei and Ng, S. W. and Hung, Dick},
title = {RETINA: A Real-Time Traffic Navigation System},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375779},
doi = {10.1145/375663.375779},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {615},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375779,
author = {Lam, Kam-Yiu and Chan, Edward and Kuo, Tei-Wei and Ng, S. W. and Hung, Dick},
title = {RETINA: A Real-Time Traffic Navigation System},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375779},
doi = {10.1145/376284.375779},
journal = {SIGMOD Rec.},
month = may,
pages = {615},
numpages = {1}
}

@inproceedings{10.1145/375663.375780,
author = {Datta, Anindya and Dutta, Kaushik and Ramamritham, Krithi and Thomas, Helen and VanderMeer, Debra},
title = {Dynamic Content Acceleration: A Caching Solution to Enable Scalable Dynamic Web Page Generation},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375780},
doi = {10.1145/375663.375780},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {616},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375780,
author = {Datta, Anindya and Dutta, Kaushik and Ramamritham, Krithi and Thomas, Helen and VanderMeer, Debra},
title = {Dynamic Content Acceleration: A Caching Solution to Enable Scalable Dynamic Web Page Generation},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375780},
doi = {10.1145/376284.375780},
journal = {SIGMOD Rec.},
month = may,
pages = {616},
numpages = {1}
}

@inproceedings{10.1145/375663.375783,
author = {Whitney, Arthur and Shasha, Dennis},
title = {Lots o'Ticks: Real Time High Performance Time Series Queries on Billions of Trades and Quotes},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375783},
doi = {10.1145/375663.375783},
abstract = {Financial mathematicians think they can predict the future by looking at time series of trades and quotes (called ticks) from the past. The main evidence for this hypothesis is that prices fluctuate only by a small amount in a given day and more or less obey the mathematics of a random walk. The hypothesis allows traders to price options and to speculate on stocks. This demonstration presents a query language and a parallel database (50-way parallelism) to support traders who want to analyze every tick, not just end-of-day ticks, using temporal statistical queries such as time-delayed correlations and tick trends. This is the first attempt that we know of to store and analyze hundreds of gigabytes of time series data and to query that data using a declarative time series extension to SQL (available at www.kx.com).},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {617},
numpages = {1},
keywords = {time series, finance, parallel databases},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375783,
author = {Whitney, Arthur and Shasha, Dennis},
title = {Lots o'Ticks: Real Time High Performance Time Series Queries on Billions of Trades and Quotes},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375783},
doi = {10.1145/376284.375783},
abstract = {Financial mathematicians think they can predict the future by looking at time series of trades and quotes (called ticks) from the past. The main evidence for this hypothesis is that prices fluctuate only by a small amount in a given day and more or less obey the mathematics of a random walk. The hypothesis allows traders to price options and to speculate on stocks. This demonstration presents a query language and a parallel database (50-way parallelism) to support traders who want to analyze every tick, not just end-of-day ticks, using temporal statistical queries such as time-delayed correlations and tick trends. This is the first attempt that we know of to store and analyze hundreds of gigabytes of time series data and to query that data using a declarative time series extension to SQL (available at www.kx.com).},
journal = {SIGMOD Rec.},
month = may,
pages = {617},
numpages = {1},
keywords = {time series, finance, parallel databases}
}

@inproceedings{10.1145/375663.375786,
author = {Han, Jiawei and Jamil, Hasan and Lu, Ying and Chen, Liangyou and Liao, Yaqin and Pei, Jian},
title = {DNA-Miner: A System Prototype for Mining DNA Sequences},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375786},
doi = {10.1145/375663.375786},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {618},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375786,
author = {Han, Jiawei and Jamil, Hasan and Lu, Ying and Chen, Liangyou and Liao, Yaqin and Pei, Jian},
title = {DNA-Miner: A System Prototype for Mining DNA Sequences},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375786},
doi = {10.1145/376284.375786},
journal = {SIGMOD Rec.},
month = may,
pages = {618},
numpages = {1}
}

@inproceedings{10.1145/375663.375789,
author = {Chen, Jun and Zhang, Xin and Chen, Songting and Koeller, Andreas and Rundensteiner, Elke A.},
title = {DyDa: Data Warehouse Maintenance in Fully Concurrent Environments},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375789},
doi = {10.1145/375663.375789},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {619},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375789,
author = {Chen, Jun and Zhang, Xin and Chen, Songting and Koeller, Andreas and Rundensteiner, Elke A.},
title = {DyDa: Data Warehouse Maintenance in Fully Concurrent Environments},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375789},
doi = {10.1145/376284.375789},
journal = {SIGMOD Rec.},
month = may,
pages = {619},
numpages = {1}
}

@inproceedings{10.1145/375663.375792,
author = {Larson, Per-\r{A}ke and Florescu, Dana and Graefe, Goetz and Moerkotte, Guido and Pirahesh, Hamid and Sch\"{o}ning, Harald},
title = {XML Data Management (Panel Session): Go Native or Spruce up Relational Systems?},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375792},
doi = {10.1145/375663.375792},
abstract = {XML data is likely to be widely used as a data exchange format but users also need to store and query XML data. The purpose of this panel is to explore whether and how to best provide this functionality.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {620},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375792,
author = {Larson, Per-\r{A}ke and Florescu, Dana and Graefe, Goetz and Moerkotte, Guido and Pirahesh, Hamid and Sch\"{o}ning, Harald},
title = {XML Data Management (Panel Session): Go Native or Spruce up Relational Systems?},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375792},
doi = {10.1145/376284.375792},
abstract = {XML data is likely to be widely used as a data exchange format but users also need to store and query XML data. The purpose of this panel is to explore whether and how to best provide this functionality.},
journal = {SIGMOD Rec.},
month = may,
pages = {620},
numpages = {1}
}

@inproceedings{10.1145/375663.375796,
author = {Rosenthal, Arnon and Dittrich, Klaus and Donahue, Jim and Maimone, Bill},
title = {Will Database Researchers Have Any Role in Data Security? (Panel Session)},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375796},
doi = {10.1145/375663.375796},
abstract = {Data security issues today go far beyond the traditional questions of grant/revoke in an RDBMS. We will discuss what the new research agenda should be.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {621},
numpages = {1},
keywords = {security, application server, intellectual property},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375796,
author = {Rosenthal, Arnon and Dittrich, Klaus and Donahue, Jim and Maimone, Bill},
title = {Will Database Researchers Have Any Role in Data Security? (Panel Session)},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375796},
doi = {10.1145/376284.375796},
abstract = {Data security issues today go far beyond the traditional questions of grant/revoke in an RDBMS. We will discuss what the new research agenda should be.},
journal = {SIGMOD Rec.},
month = may,
pages = {621},
numpages = {1},
keywords = {intellectual property, application server, security}
}

@inproceedings{10.1145/375663.375798,
author = {Mohan, C. and Cable, Larry and Devin, Matthieu and Dietzen, Scott and Helland, Pat and Wolfson, Dan},
title = {Application Servers (Panel Session): Born-Again TP Monitors for the Web},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375798},
doi = {10.1145/375663.375798},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {622},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375798,
author = {Mohan, C. and Cable, Larry and Devin, Matthieu and Dietzen, Scott and Helland, Pat and Wolfson, Dan},
title = {Application Servers (Panel Session): Born-Again TP Monitors for the Web},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375798},
doi = {10.1145/376284.375798},
journal = {SIGMOD Rec.},
month = may,
pages = {622},
numpages = {1}
}

@inproceedings{10.1145/375663.375800,
author = {Haas, Peter J. and Hellerstein, Joseph M.},
title = {Online Query Processing: A Tutorial},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375800},
doi = {10.1145/375663.375800},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {623},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375800,
author = {Haas, Peter J. and Hellerstein, Joseph M.},
title = {Online Query Processing: A Tutorial},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375800},
doi = {10.1145/376284.375800},
journal = {SIGMOD Rec.},
month = may,
pages = {623},
numpages = {1}
}

@inproceedings{10.1145/375663.375808,
author = {Gunopulos, Dimitrios and Das, Gautam},
title = {Time Series Similarity Measures and Time Series Indexing (Abstract Only)},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375808},
doi = {10.1145/375663.375808},
abstract = {Time series is the simplest form of temporal data. A time series is a sequence of real numbers collected regularly in time, where each number represents a value. Time series data come up in a variety of domains, including stock market analysis, environmental data, telecommunications data, medical data and financial data. Web data that count the number of clicks on given cites, or model the usage of different pages are also modeled as time series. Therefore time series account for a large fraction of the data stored in commercial databases. There is recently increasing recognition of this fact, and support for time series as a different data type in commercial data bases management systems is increasing. IBM DB2 for example implements support for time series using data-blades.The pervasiveness and importance of time series data has sparked a lot of research work on the topic. While the statistics literature on time series is vast, it has not studied methods that would be appropriate for the time series similarity and indexing problems we discuss here; much of the relevant work on these problems has been done by the computer science community.One interesting problem with time series data is finding whether different time series display similar behavior. More formally, the problem can be stated as: Given two time series X and Y, determine whether they are similar or not (in other words, define and compute a distance function dist(X, Y)). Typically each time series describes the evolution of an object, for example the price of a stock, or the levels of pollution as a function of time at a given data collection station. The objective can be to cluster the different objects to similar groups, or to classify an object based on a set of known object examples. The problem is hard because the similarity model should allow for imprecise matches. One interesting variation is the subsequence similarity problem, where given two time series X and Y, we have to determine those subsequences of X that are similar to pattern Y. To answer these problems, different notions of similarity between time series have been proposed in data mining research.In the tutorial we examine the different time series similarity models that have been proposed, in terms of efficiency and accuracy. The solutions encompass techniques from a wide variety of disciplines, such as databases, signal processing, speech recognition, pattern matching, combinatorics and statistics. We survey proposed similarity techniques, including the Lp norms, time warping, longest common subsequence measures, baselines, moving averaging, or deformable Markov model templates.Another problem that comes up in applications is the indexing problem: given a time series X, and a set of time series S = {Y1,…,YN}, find the time series in S that are most similar to the query X. A variation is the subsequence indexing problem, where given a set of sequences S, and a query sequence (pattern) X, find the sequences in S that contain subsequences that are similar to X. To solve these problems efficiently, appropriate indexing techniques have to be used. Typically, the similarity problem is related to the indexing problem: simple (and possibly inaccurate) similarity measures are usually easy to build indexes for, while more sophisticated similarity measures make the indexing problem hard and interesting.We examine the indexing techniques that can be used for different models, and the dimensionality reduction techniques that have been proposed to improve indexing performance. A time series of length n can be considered as a tuple in an n-dimensional space. Indexing this space directly is inefficient because of the very high dimensionality. The main idea to improve on it is to use a dimensionality reduction technique that takes the n item long time series, and maps it to a lower dimensional space with k dimensions (hopefully, k &lt;&lt; n).We give a detailed description of the most important techniques used for dimensionality reduction. These include: the SVD decomposition, the Fourier transform (and the similar Discrete Cosine transform), the Wavelet decomposition, Multidimensional Scaling, random projection techniques, FastMap (and variants), and Linear partitioning. These techniques have specific strengths and weaknesses, making some of them better suited for specific applications and settings.Finally we consider extensions to the problem of indexing subsequences, as well as to the problem of finding similar high-dimensional sequences, such as trajectories or video frame sequences.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {624},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375808,
author = {Gunopulos, Dimitrios and Das, Gautam},
title = {Time Series Similarity Measures and Time Series Indexing (Abstract Only)},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375808},
doi = {10.1145/376284.375808},
abstract = {Time series is the simplest form of temporal data. A time series is a sequence of real numbers collected regularly in time, where each number represents a value. Time series data come up in a variety of domains, including stock market analysis, environmental data, telecommunications data, medical data and financial data. Web data that count the number of clicks on given cites, or model the usage of different pages are also modeled as time series. Therefore time series account for a large fraction of the data stored in commercial databases. There is recently increasing recognition of this fact, and support for time series as a different data type in commercial data bases management systems is increasing. IBM DB2 for example implements support for time series using data-blades.The pervasiveness and importance of time series data has sparked a lot of research work on the topic. While the statistics literature on time series is vast, it has not studied methods that would be appropriate for the time series similarity and indexing problems we discuss here; much of the relevant work on these problems has been done by the computer science community.One interesting problem with time series data is finding whether different time series display similar behavior. More formally, the problem can be stated as: Given two time series X and Y, determine whether they are similar or not (in other words, define and compute a distance function dist(X, Y)). Typically each time series describes the evolution of an object, for example the price of a stock, or the levels of pollution as a function of time at a given data collection station. The objective can be to cluster the different objects to similar groups, or to classify an object based on a set of known object examples. The problem is hard because the similarity model should allow for imprecise matches. One interesting variation is the subsequence similarity problem, where given two time series X and Y, we have to determine those subsequences of X that are similar to pattern Y. To answer these problems, different notions of similarity between time series have been proposed in data mining research.In the tutorial we examine the different time series similarity models that have been proposed, in terms of efficiency and accuracy. The solutions encompass techniques from a wide variety of disciplines, such as databases, signal processing, speech recognition, pattern matching, combinatorics and statistics. We survey proposed similarity techniques, including the Lp norms, time warping, longest common subsequence measures, baselines, moving averaging, or deformable Markov model templates.Another problem that comes up in applications is the indexing problem: given a time series X, and a set of time series S = {Y1,…,YN}, find the time series in S that are most similar to the query X. A variation is the subsequence indexing problem, where given a set of sequences S, and a query sequence (pattern) X, find the sequences in S that contain subsequences that are similar to X. To solve these problems efficiently, appropriate indexing techniques have to be used. Typically, the similarity problem is related to the indexing problem: simple (and possibly inaccurate) similarity measures are usually easy to build indexes for, while more sophisticated similarity measures make the indexing problem hard and interesting.We examine the indexing techniques that can be used for different models, and the dimensionality reduction techniques that have been proposed to improve indexing performance. A time series of length n can be considered as a tuple in an n-dimensional space. Indexing this space directly is inefficient because of the very high dimensionality. The main idea to improve on it is to use a dimensionality reduction technique that takes the n item long time series, and maps it to a lower dimensional space with k dimensions (hopefully, k &lt;&lt; n).We give a detailed description of the most important techniques used for dimensionality reduction. These include: the SVD decomposition, the Fourier transform (and the similar Discrete Cosine transform), the Wavelet decomposition, Multidimensional Scaling, random projection techniques, FastMap (and variants), and Linear partitioning. These techniques have specific strengths and weaknesses, making some of them better suited for specific applications and settings.Finally we consider extensions to the problem of indexing subsequences, as well as to the problem of finding similar high-dimensional sequences, such as trajectories or video frame sequences.},
journal = {SIGMOD Rec.},
month = may,
pages = {624},
numpages = {1}
}

@inproceedings{10.1145/375663.375810,
author = {Bussler, Christoph},
title = {Semantic B2B Integration},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375810},
doi = {10.1145/375663.375810},
abstract = {The tutorial “Semantic B2B Integration” will give an introduction to the field of business-to-business (B2B) integration from a technical viewpoint with the focus on semantic integration aspects. The set of B2B integration concepts is introduced as well as their implementation in form of a technical semantic B2B integration architecture. A mix of examples is taken illustrating the problems that need to be solved in semantic B2B integration projects. The tutorial enables the audience to identify semantic B2B integration problems as well as to determine the benefits and deficiencies of various technical integration architecture approaches or B2B integration technologies.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {625},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375810,
author = {Bussler, Christoph},
title = {Semantic B2B Integration},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375810},
doi = {10.1145/376284.375810},
abstract = {The tutorial “Semantic B2B Integration” will give an introduction to the field of business-to-business (B2B) integration from a technical viewpoint with the focus on semantic integration aspects. The set of B2B integration concepts is introduced as well as their implementation in form of a technical semantic B2B integration architecture. A mix of examples is taken illustrating the problems that need to be solved in semantic B2B integration projects. The tutorial enables the audience to identify semantic B2B integration problems as well as to determine the benefits and deficiencies of various technical integration architecture approaches or B2B integration technologies.},
journal = {SIGMOD Rec.},
month = may,
pages = {625},
numpages = {1}
}

@inproceedings{10.1145/375663.375812,
author = {Casati, Fabio and Shan, Ming-Chien},
title = {Models and Languages for Describing and Discovering E-Services},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375812},
doi = {10.1145/375663.375812},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {626},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375812,
author = {Casati, Fabio and Shan, Ming-Chien},
title = {Models and Languages for Describing and Discovering E-Services},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375812},
doi = {10.1145/376284.375812},
journal = {SIGMOD Rec.},
month = may,
pages = {626},
numpages = {1}
}

@inproceedings{10.1145/375663.375814,
author = {Smith, John R.},
title = {Standard for Multimedia Databases},
year = {2001},
isbn = {1581133324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375663.375814},
doi = {10.1145/375663.375814},
abstract = {The Moving Picture Experts Group (MPEG) is developing a new standard called the “Multimedia Content Description Interface,” also known as MPEG-7. The goal of MPEG-7 is to enable fast and effective searching and filtering of multimedia content. The effort is being driven by requirements taken from a large number of applications related to multimedia databases, interactive media services (music, TV programs), video libraries, and so forth. MPEG-7 is achieving this goal by developing an XML-Schema based standard for describing features of multimedia content. In this tutorial, we study the emerging MPEG-7 standard and describe the new challenges for MPEG-7 multimedia databases.},
booktitle = {Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data},
pages = {627},
numpages = {1},
keywords = {digital libraries, similarity search, content-based retrieval, multimedia databases, MPEG-7, indexing},
location = {Santa Barbara, California, USA},
series = {SIGMOD '01}
}

@article{10.1145/376284.375814,
author = {Smith, John R.},
title = {Standard for Multimedia Databases},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/376284.375814},
doi = {10.1145/376284.375814},
abstract = {The Moving Picture Experts Group (MPEG) is developing a new standard called the “Multimedia Content Description Interface,” also known as MPEG-7. The goal of MPEG-7 is to enable fast and effective searching and filtering of multimedia content. The effort is being driven by requirements taken from a large number of applications related to multimedia databases, interactive media services (music, TV programs), video libraries, and so forth. MPEG-7 is achieving this goal by developing an XML-Schema based standard for describing features of multimedia content. In this tutorial, we study the emerging MPEG-7 standard and describe the new challenges for MPEG-7 multimedia databases.},
journal = {SIGMOD Rec.},
month = may,
pages = {627},
numpages = {1},
keywords = {similarity search, digital libraries, indexing, MPEG-7, multimedia databases, content-based retrieval}
}

