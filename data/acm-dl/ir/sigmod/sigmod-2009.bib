@inproceedings{10.1145/3257449,
author = {Miklau, Gerome Miklau},
title = {Session Details: Research Session 1: Security I},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257449},
doi = {10.1145/3257449},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559849,
author = {Yang, Yin and Papadias, Dimitris and Papadopoulos, Stavros and Kalnis, Panos},
title = {Authenticated Join Processing in Outsourced Databases},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559849},
doi = {10.1145/1559845.1559849},
abstract = {Database outsourcing requires that a query server constructs a proof of result correctness, which can be verified by the client using the data owner's signature. Previous authentication techniques deal with range queries on a single relation using an authenticated data structure (ADS). On the other hand, authenticated join processing is inherently more complex than ranges since only the base relations (but not their combination) are signed by the owner. In this paper, we present three novel join algorithms depending on the ADS availability: (i) Authenticated Indexed Sort Merge Join (AISM), which utilizes a single ADS on the join attribute, (ii) Authenticated Index Merge Join (AIM) that requires an ADS (on the join attribute) for both relations, and (iii) Authenticated Sort Merge Join (ASM), which does not rely on any ADS. We experimentally demonstrate that the proposed methods outperform two benchmark algorithms, often by several orders of magnitude, on all performance metrics, and effectively shift the workload to the outsourcing service. Finally, we extend our techniques to complex queries that combine multi-way joins with selections and projections.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {5–18},
numpages = {14},
keywords = {join algorithms, query authentication, database outsourcing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559850,
author = {McSherry, Frank D.},
title = {Privacy Integrated Queries: An Extensible Platform for Privacy-Preserving Data Analysis},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559850},
doi = {10.1145/1559845.1559850},
abstract = {We report on the design and implementation of the Privacy Integrated Queries (PINQ) platform for privacy-preserving data analysis. PINQ provides analysts with a programming interface to unscrubbed data through a SQL-like language. At the same time, the design of PINQ's analysis language and its careful implementation provide formal guarantees of differential privacy for any and all uses of the platform. PINQ's unconditional structural guarantees require no trust placed in the expertise or diligence of the analysts, substantially broadening the scope for design and deployment of privacy-preserving data analysis, especially by non-experts.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {19–30},
numpages = {12},
keywords = {anonymization, differential privacy, confidentiality, linq},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559851,
author = {Nath, Suman and Yu, Haifeng and Chan, Haowen},
title = {Secure Outsourced Aggregation via One-Way Chains},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559851},
doi = {10.1145/1559845.1559851},
abstract = {We consider the Outsourced Aggregation model, where sensing services outsource their sensor data collection and aggregation tasks to third-party service providers called aggregators. As aggregators can be untrusted or compromised, it is essential for a sensing service to be able to verify the correctness of aggregation results. This work presents SECOA, a framework with a family of novel and optimally-secure protocols for secure outsourced aggregation. Our framework is based on a unified use of one-way chains. It supports a large and diverse set of aggregate functions, can have multiple hierarchically organized aggregators, can deterministically detect any malicious aggregation behavior without communication with sensors, and incurs a small and workload-independent communication load on sensors. We also present extensive evaluation results to demonstrate the feasibility of our framework.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {31–44},
numpages = {14},
keywords = {secure outsourced aggregation, wide-area sensing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257450,
author = {Ailamaki, Natassa},
title = {Session Details: Research Session 2: Databases on Modern Hardware},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257450},
doi = {10.1145/3257450},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559853,
author = {Han, Wook-Shin and Lee, Jinsoo},
title = {Dependency-Aware Reordering for Parallelizing Query Optimization in Multi-Core CPUs},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559853},
doi = {10.1145/1559845.1559853},
abstract = {The state of the art commercial query optimizers employ cost-based optimization and exploit dynamic programming (DP) to find the optimal query execution plan (QEP) without evaluating redundant sub-plans. The number of alternative QEPs enumerated by the DP query optimizer can increase exponentially, as the number of joins in the query increases. Recently, by exploiting the coming wave of multi-core processor architectures, a state of the art parallel optimization algorithm [14], referred to as PDPsva, has been proposed to parallelize the "time-consuming" DP query optimization process itself. While PDPsva significantly extends the practical use of DP to queries having up to 20-25 tables, it has several limitations: 1) supporting only the size-driven DP enumerator, 2) statically allocating search space, and 3) not fully exploiting parallelism. In this paper, we propose the first generic solution for parallelizing any type of bottom-up optimizer, including the graph-traversal driven type, and for supporting dynamic search allocation and full parallelism. This is a challenging problem, since recently developed, state of art DP optimizers such as DPcpp [21] and DPhyp [22] are very difficult to parallelize due to tangled dependencies in the join pairs they generate. Unless the solution is very carefully devised, a lot of synchronization conflicts are bound to occur. By viewing a serial bottom-up optimizer as one which generates a totally ordered sequence of join pairs in a streaming fashion, we propose a novel concept of dependency-aware reordering, which minimizes waiting time caused by dependencies of join pairs. To maximize parallelism, we also introduce a series of novel performance optimization techniques: 1) pipelining of join pair generation and plan generation; 2) the synchronization-free global MEMO; and 3) threading across dependencies. Through extensive experiments with various query topologies, we show that our solution supports any type of bottom up optimization, achieving linear speedup for each type. Despite the fact that our solution is generic, due to sophisticated optimization techniques, our generic parallel optimizer outperforms PDPsva tailored to size-driven enumeration. Experimental results also show that our solution is much more robust than PDPsva with respect to search space allocation.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {45–58},
numpages = {14},
keywords = {parallel databases, query optimization, multi-cores},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559854,
author = {Tsirogiannis, Dimitris and Harizopoulos, Stavros and Shah, Mehul A. and Wiener, Janet L. and Graefe, Goetz},
title = {Query Processing Techniques for Solid State Drives},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559854},
doi = {10.1145/1559845.1559854},
abstract = {Solid state drives perform random reads more than 100x faster than traditional magnetic hard disks, while offering comparable sequential read and write bandwidth. Because of their potential to speed up applications, as well as their reduced power consumption, these new drives are expected to gradually replace hard disks as the primary permanent storage media in large data centers. However, although they may benefit applications that stress random reads immediately, they may not improve database applications, especially those running long data analysis queries. Database query processing engines have been designed around the speed mismatch between random and sequential I/O on hard disks and their algorithms currently emphasize sequential accesses for disk-resident data.In this paper, we investigate data structures and algorithms that leverage fast random reads to speed up selection, projection, and join operations in relational query processing. We first demonstrate how a column-based layout within each page reduces the amount of data read during selections and projections. We then introduce FlashJoin, a general pipelined join algorithm that minimizes accesses to base and intermediate relational data. FlashJoin's binary join kernel accesses only the join attributes, producing partial results in the form of a join index. Subsequently, its fetch kernel retrieves the attributes for later nodes in the query plan as they are needed. FlashJoin significantly reduces memory and I/O requirements for each join in the query. We implemented these techniques inside Postgres and experimented with an enterprise SSD drive. Our techniques improved query runtimes by up to 6x for queries ranging from simple relational scans and joins to full TPC-H queries.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {59–72},
numpages = {14},
keywords = {late materialization, flash memory, semi-join reduction, join index, ssd, columnar storage},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559855,
author = {Chen, Shimin},
title = {FlashLogging: Exploiting Flash Devices for Synchronous Logging Performance},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559855},
doi = {10.1145/1559845.1559855},
abstract = {Synchronous transactional logging is the central mechanism for ensuring data persistency and recoverability in database systems. Unfortunately, magnetic disks are ill-suited for the small sequential write pattern of synchronous logging. Alternative solutions (e.g., backup servers or sophisticated battery-backed write caches in high-end disk arrays) are either expensive or complicated.In this paper, we exploit flash devices for synchronous logging based on the observation that flash devices support small sequential writes well. Comparing a wide variety of flash devices, we find that USB flash drives are a good match for this task because of its unique characteristics: widely available USB ports, hot-plug capability useful for coping with flash wear, and low price so that multiple drives are affordable. We propose FlashLogging, a logging solution that exploits multiple (USB) flash drives for synchronous logging. We identify and address four challenges: (i) efficiently exploiting multiple flash drives for logging; (ii) coping with the large variance of write latencies because of device erasure operations; (iii) efficient recovery processing; and (iv) combining flash drives and disks for better logging and recovery performance. We implemented our solution within MySQL-InnoDB. Our real machine experiments running online transaction processing workloads (TPCC) show that FlashLogging achieves up to 5.7X improvements over magnetic-disk-based logging, and obtains up to 98.6% of the ideal performance. We further compare our design with one that uses Solid-State Drives (SSDs), and find that although SSDs improve logging performance, multiple USB flash drives can achieve comparable or better performance with much lower price.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {73–86},
numpages = {14},
keywords = {synchronous logging, unconventional array organization, flashlogging, near-zero-delay archival disk, flash devices, outlier detection and hiding, recovery processing, online transaction processing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257451,
author = {Riedewal, Mirek},
title = {Session Details: Research Session 3: Information Extraction},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257451},
doi = {10.1145/3257451},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559857,
author = {Chai, Xiaoyong and Vuong, Ba-Quy and Doan, AnHai and Naughton, Jeffrey F.},
title = {Efficiently Incorporating User Feedback into Information Extraction and Integration Programs},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559857},
doi = {10.1145/1559845.1559857},
abstract = {Many applications increasingly employ information extraction and integration (IE/II) programs to infer structures from unstructured data. Automatic IE/II are inherently imprecise. Hence such programs often make many IE/II mistakes, and thus can significantly benefit from user feedback. Today, however, there is no good way to automatically provide and process such feedback. When finding an IE/II mistake, users often must alert the developer team (e.g., via email or Web form) about the mistake, and then wait for the team to manually examine the program internals to locate and fix the mistake, a slow, error-prone, and frustrating process.In this paper we propose a solution for users to directly provide feedback and for IE/II programs to automatically process such feedback. In our solution a developer U uses hlog, a declarative IE/II language, to write an IE/II program P. Next, U writes declarative user feedback rules that specify which parts of P's data (e.g., input, intermediate, or output data) users can edit, and via which user interfaces. Next, the so-augmented program P is executed, then enters a loop of waiting for and incorporating user feedback. Given user feedback F on a data portion of P, we show how to automatically propagate F to the rest of P, and to seamlessly combine F with prior user feedback. We describe the syntax and semantics of hlog, a baseline execution strategy, and then various optimization techniques. Finally, we describe experiments with real-world data that demonstrate the promise of our solution.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {87–100},
numpages = {14},
keywords = {incremental execution, provenance, information extraction, user feedback, information integration},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559858,
author = {Michelakis, Eirinaios and Krishnamurthy, Rajasekar and Haas, Peter J. and Vaithyanathan, Shivakumar},
title = {Uncertainty Management in Rule-Based Information Extraction Systems},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559858},
doi = {10.1145/1559845.1559858},
abstract = {Rule-based information extraction is a process by which structured objects are extracted from text based on user-defined rules. The compositional nature of rule-based information extraction also allows rules to be expressed over previously extracted objects. Such extraction is inherently uncertain, due to the varying precision associated with the rules used in a specific extraction task. Quantifying this uncertainty is crucial for querying the extracted objects in probabilistic databases, and for improving the recall of extraction tasks that use compositional rules. In this paper, we provide a probabilistic framework for handling the uncertainty in rule-based information extraction. Specifically, for each extraction task, we build a parametric exponential model of uncertainty that captures the interaction between the different rules, as well as the compositional nature of the rules; the exponential form of our model follows from maximum-entropy considerations. We also give model-decomposition techniques that make the learning algorithms scalable to large numbers of rules and constraints. Experiments over multiple real-world extraction tasks confirm that our approach yields accurate probability estimates with only a small performance overhead. Moreover, our framework supports incremental pay-as-you-go improvements in the accuracy of probability estimates as new rules, data, or constraints are added.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {101–114},
numpages = {14},
keywords = {maximum entropy, information extraction, probabilistic modeling},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559859,
author = {Kim, Jong Wook and Candan, K. Sel\c{c}uk},
title = {Skip-and-Prune: Cosine-Based Top-k Query Processing for Efficient Context-Sensitive Document Retrieval},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559859},
doi = {10.1145/1559845.1559859},
abstract = {Keyword search and ranked retrieval together emerged as popular data access paradigms for various kinds of data, from web pages to XML and relational databases. A user can submit keywords without knowing much (sometimes nothing) about the complex structure underlying a data collection, yet the system can identify, rank, and return a set of relevant matches by exploiting statistics about the distribution and structure of the data. Keyword-based data models are also suitable for capturing user's search context in terms of weights associated to the keywords in the query. Given a search context, the data in the database can also be re-interpreted for semantically correct retrieval. This option, however, is often ignored as the cost of re-assessing the content in the database naively tends to be prohibitive. In this paper, we first argue that top-k query processing can help tackle this challenge by re-assessing only the relevant parts of the database, efficiently. A road-block in this process, however, is that most efficient implementations of top-k query processing assume that the scoring function is monotonic, whereas the cosine-based scoring function needed for re-interpretation of content based on user context is not. In this paper, we develop an efficient top-k query processing algorithm, skip-and-prune (SnP), which is able to process top-k queries under cosine-based non-monotonic scoring functions. We compare the use of proposed algorithm against the alternative implementations of the context-aware retrieval, including naive top-k, accumulator-based inverted files, and full-scan. The experiment results show that while being fast, naive top-k is not an effective solution due to the non-monotonicity of underlying scoring function. The proposed technique, SnP, however, matches the precision of accumulator-based inverted files and full-scan, yet it is orders of magnitude faster than these.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {115–126},
numpages = {12},
keywords = {top-k, ranking},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257452,
author = {Suciu, Dan},
title = {Session Details: Research Session 4: Security II},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257452},
doi = {10.1145/3257452},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559861,
author = {Kifer, Daniel},
title = {Attacks on Privacy and DeFinetti's Theorem},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559861},
doi = {10.1145/1559845.1559861},
abstract = {In this paper we present a method for reasoning about privacy using the concepts of exchangeability and deFinetti's theorem. We illustrate the usefulness of this technique by using it to attack a popular data sanitization scheme known as Anatomy. We stress that Anatomy is not the only sanitization scheme that is vulnerable to this attack. In fact, any scheme that uses the random worlds model, i.i.d. model, or tuple-independent model needs to be re-evaluated.The difference between the attack presented here and others that have been proposedin the past is that we do not need extensive background knowledge. An attacker only needs to know the nonsensitive attributes of one individual in the data, and can carry out this attack just by building a machine learning model over the sanitized data. The reason this attack is successful is that it exploits a subtle flaw in the way prior work computed the probability of disclosure of a sensitive attribute. We demonstrate this theoretically, empirically, and with intuitive examples. We also discuss how this generalizes to many other privacy schemes.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {127–138},
numpages = {12},
keywords = {privacy, random worlds},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559862,
author = {Wong, Wai Kit and Cheung, David Wai-lok and Kao, Ben and Mamoulis, Nikos},
title = {Secure KNN Computation on Encrypted Databases},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559862},
doi = {10.1145/1559845.1559862},
abstract = {Service providers like Google and Amazon are moving into the SaaS (Software as a Service) business. They turn their huge infrastructure into a cloud-computing environment and aggressively recruit businesses to run applications on their platforms. To enforce security and privacy on such a service model, we need to protect the data running on the platform. Unfortunately, traditional encryption methods that aim at providing "unbreakable" protection are often not adequate because they do not support the execution of applications such as database queries on the encrypted data. In this paper we discuss the general problem of secure computation on an encrypted database and propose a SCONEDB Secure Computation ON an Encrypted DataBase) model, which captures the execution and security requirements. As a case study, we focus on the problem of k-nearest neighbor (kNN) computation on an encrypted database. We develop a new asymmetric scalar-product-preserving encryption (ASPE) that preserves a special type of scalar product. We use APSE to construct two secure schemes that support kNN computation on encrypted data; each of these schemes is shown to resist practical attacks of a different background knowledge level, at a different overhead cost. Extensive performance studies are carried out to evaluate the overhead and the efficiency of the schemes.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {139–152},
numpages = {14},
keywords = {encryption, knn, security},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559863,
author = {Dasgupta, Arjun and Zhang, Nan and Das, Gautam and Chaudhuri, Surajit},
title = {Privacy Preservation of Aggregates in Hidden Databases: Why and How?},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559863},
doi = {10.1145/1559845.1559863},
abstract = {Many websites provide form-like interfaces which allow users to execute search queries on the underlying hidden databases. In this paper, we explain the importance of protecting sensitive aggregate information of hidden databases from being disclosed through individual tuples returned by the search queries. This stands in contrast to the traditional privacy problem where individual tuples must be protected while ensuring access to aggregating information. We propose techniques to thwart bots from sampling the hidden database to infer aggregate information. We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {153–164},
numpages = {12},
keywords = {hidden databases, privacy preservation},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257453,
author = {Tatbul, Nesime},
title = {Session Details: Research Session 5: Large-Scale Data Analysis},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257453},
doi = {10.1145/3257453},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559865,
author = {Pavlo, Andrew and Paulson, Erik and Rasin, Alexander and Abadi, Daniel J. and DeWitt, David J. and Madden, Samuel and Stonebraker, Michael},
title = {A Comparison of Approaches to Large-Scale Data Analysis},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559865},
doi = {10.1145/1559845.1559865},
abstract = {There is currently considerable enthusiasm around the MapReduce (MR) paradigm for large-scale data analysis [17]. Although the basic control flow of this framework has existed in parallel SQL database management systems (DBMS) for over 20 years, some have called MR a dramatically new computing model [8, 17]. In this paper, we describe and compare both paradigms. Furthermore, we evaluate both kinds of systems in terms of performance and development complexity. To this end, we define a benchmark consisting of a collection of tasks that we have run on an open source version of MR as well as on two parallel DBMSs. For each task, we measure each system's performance for various degrees of parallelism on a cluster of 100 nodes. Our results reveal some interesting trade-offs. Although the process to load data into and tune the execution of parallel DBMSs took much longer than the MR system, the observed performance of these DBMSs was strikingly better. We speculate about the causes of the dramatic performance difference and consider implementation concepts that future systems should take from both kinds of architectures.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {165–178},
numpages = {14},
keywords = {benchmarks, parallel database, mapreduce},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559866,
author = {Agrawal, Parag and Silberstein, Adam and Cooper, Brian F. and Srivastava, Utkarsh and Ramakrishnan, Raghu},
title = {Asynchronous View Maintenance for VLSD Databases},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559866},
doi = {10.1145/1559845.1559866},
abstract = {The query models of the recent generation of very large scale distributed (VLSD) shared-nothing data storage systems, including our own PNUTS and others (e.g. BigTable, Dynamo, Cassandra, etc.) are intentionally simple, focusing on simple lookups and scans and trading query expressiveness for massive scale. Indexes and views can expand the query expressiveness of such systems by materializing more complex access paths and query results. In this paper, we examine mechanisms to implement indexes and views in a massive scale distributed database. For web applications, minimizing update latencies is critical, so we advocate deferring the work of maintaining views and indexes as much as possible. We examine the design space, and conclude that two types of view implementations, called remote view tables (RVTs) and local view tables (LVTs), provide good tradeoff between system throughput and minimizing view staleness. We describe how to construct and maintain such view tables, and how they can be used to implement indexes, group-by-aggregate views, equijoin views and selection views. We also introduce and analyze a consistency model that makes it easier for application developers to cope with the impact of deferred view maintenance. An empirical evaluation quantifies the maintenance costs of our views, and shows that they can significantly improve the cost of evaluating complex queries.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {179–192},
numpages = {14},
keywords = {indexes, distributed and parallel databases, views},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559867,
author = {Mei, Yuan and Madden, Samuel},
title = {ZStream: A Cost-Based Query Processor for Adaptively Detecting Composite Events},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559867},
doi = {10.1145/1559845.1559867},
abstract = {Composite (or Complex) event processing (CEP) systems search sequences of incoming events for occurrences of user-specified event patterns. Recently, they have gained more attention in a variety of areas due to their powerful and expressive query language and performance potential. Sequentiality (temporal ordering) is the primary way in which CEP systems relate events to each other. In this paper, we present a CEP system called ZStream to efficiently process such sequential patterns. Besides simple sequential patterns, ZStream is also able to detect other patterns, including conjunction, disjunction, negation and Kleene closure.Unlike most recently proposed CEP systems, which use non-deterministic finite automata (NFA's) to detect patterns, ZStream uses tree-based query plans for both the logical and physical representation of query patterns. By carefully designing the underlying infrastructure and algorithms, ZStream is able to unify the evaluation of sequence, conjunction, disjunction, negation, and Kleene closure as variants of the join operator. Under this framework, a single pattern in ZStream may have several equivalent physical tree plans, with different evaluation costs. We propose a cost model to estimate the computation costs of a plan. We show that our cost model can accurately capture the actual runtime behavior of a plan, and that choosing the optimal plan can result in a factor of four or more speedup versus an NFA based approach. Based on this cost model and using a simple set of statistics about operator selectivity and data rates, ZStream is able to adaptively and seamlessly adjust the order in which it detects patterns on the fly. Finally, we describe a dynamic programming algorithm used in our cost model to efficiently search for an optimal query plan for a given pattern.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {193–206},
numpages = {14},
keywords = {streaming, optimization, complex event processing, algorithm},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257454,
author = {Benjelloun, Omar},
title = {Session Details: Research Session 6: Entity Resolution},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257454},
doi = {10.1145/3257454},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559869,
author = {Chen, Zhaoqi and Kalashnikov, Dmitri V. and Mehrotra, Sharad},
title = {Exploiting Context Analysis for Combining Multiple Entity Resolution Systems},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559869},
doi = {10.1145/1559845.1559869},
abstract = {Entity Resolution (ER) is an important real world problem that has attracted significant research interest over the past few years. It deals with determining which object descriptions co-refer in a dataset. Due to its practical significance for data mining and data analysis tasks many different ER approaches has been developed to address the ER challenge. This paper proposes a new ER Ensemble framework. The task of ER Ensemble is to combine the results of multiple base-level ER systems into a single solution with the goal of increasing the quality of ER. The framework proposed in this paper leverages the observation that often no single ER method always performs the best, consistently outperforming other ER techniques in terms of quality. Instead, different ER solutions perform better in different contexts. The framework employs two novel combining approaches, which are based on supervised learning. The two approaches learn a mapping of the clustering decisions of the base-level ER systems, together with the local context, into a combined clustering decision. The paper empirically studies the framework by applying it to different domains. The experiments demonstrate that the proposed framework achieves significantly higher disambiguation quality compared to the current state of the art solutions.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {207–218},
numpages = {12},
keywords = {context analysis, er ensemble, entity resolution},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559870,
author = {Whang, Steven Euijong and Menestrina, David and Koutrika, Georgia and Theobald, Martin and Garcia-Molina, Hector},
title = {Entity Resolution with Iterative Blocking},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559870},
doi = {10.1145/1559845.1559870},
abstract = {Entity Resolution (ER) is the problem of identifying which records in a database refer to the same real-world entity. An exhaustive ER process involves computing the similarities between pairs of records, which can be very expensive for large datasets. Various blocking techniques can be used to enhance the performance of ER by dividing the records into blocks in multiple ways and only comparing records within the same block. However, most blocking techniques process blocks separately and do not exploit the results of other blocks. In this paper, we propose an iterative blocking framework where the ER results of blocks are reflected to subsequently processed blocks. Blocks are now iteratively processed until no block contains any more matching records. Compared to simple blocking, iterative blocking may achieve higher accuracy because reflecting the ER results of blocks to other blocks may generate additional record matches. Iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks. We implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and efficient than blocking for large datasets.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {219–232},
numpages = {14},
keywords = {entity resolution, blocking, iterative blocking},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559871,
author = {Arasu, Arvind and Kaushik, Raghav},
title = {A Grammar-Based Entity Representation Framework for Data Cleaning},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559871},
doi = {10.1145/1559845.1559871},
abstract = {Fundamental to data cleaning is the need to account for multiple data representations. We propose a formal framework that can be used to reason about and manipulate data representations. The framework is declarative and combines elements of a generative grammar with database querying. It also incorporates actions in the spirit of programming language compilers. This framework has multiple applications such as parsing and data normalization. Data normalization is interesting in its own right in preparing data for analysis as well as in pre-processing data for further cleansing. We empirically study the utility of the framework over several real-world data cleaning scenarios and find that with the right normalization, often the need for further cleansing is minimized.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {233–244},
numpages = {12},
keywords = {data cleaning, entity resolution, deduplication},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257455,
author = {Schiefer, Berni},
title = {Session Details: Research Session 7: Testing and Security},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257455},
doi = {10.1145/3257455},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559873,
author = {Olston, Christopher and Chopra, Shubham and Srivastava, Utkarsh},
title = {Generating Example Data for Dataflow Programs},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559873},
doi = {10.1145/1559845.1559873},
abstract = {While developing data-centric programs, users often run (portions of) their programs over real data, to see how they behave and what the output looks like. Doing so makes it easier to formulate, understand and compose programs correctly, compared with examination of program logic alone. For large input data sets, these experimental runs can be time-consuming and inefficient. Unfortunately, sampling the input data does not always work well, because selective operations such as filter and join can lead to empty results over sampled inputs, and unless certain indexes are present there is no way to generate biased samples efficiently. Consequently new methods are needed for generating example input data for data-centric programs.We focus on an important category of data-centric programs, dataflow programs, which are best illustrated by displaying the series of intermediate data tables that occur between each pair of operations. We introduce and study the problem of generating example intermediate data for dataflow programs, in a manner that illustrates the semantics of the operators while keeping the example data small. We identify two major obstacles that impede naive approaches, namely (1) highly selective operators and (2) noninvertible operators, and offer techniques for dealing with these obstacles. Our techniques perform well on real dataflow programs used at Yahoo! for web analytics.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {245–256},
numpages = {12},
keywords = {dataflow programming, example data},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559874,
author = {Elmongui, Hicham G. and Narasayya, Vivek and Ramamurthy, Ravishankar},
title = {A Framework for Testing Query Transformation Rules},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559874},
doi = {10.1145/1559845.1559874},
abstract = {In order to enable extensibility, modern query optimizers typically leverage a transformation rule based framework. Testing individual rule correctness as well as correctness of rule interactions is crucial in verifying the functionality of a query optimizer. While there has been a lot of work on how to architect optimizers for extensibility using a rule based framework, there has been relatively little work on how to test such optimizers. In this paper we present a framework for testing query transformation rules which enables: (a) efficient generation of queries that exercise a particular transformation rule or a set of rules and (b) efficient execution of corresponding test suites for correctness testing.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {257–268},
numpages = {12},
keywords = {transformation rules, query optimization, database testing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559875,
author = {Corcoran, Brian J. and Swamy, Nikhil and Hicks, Michael},
title = {Cross-Tier, Label-Based Security Enforcement for Web Applications},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559875},
doi = {10.1145/1559845.1559875},
abstract = {This paper presents SELinks, a programming language focused on building secure multi-tier web applications. SELinks provides a uniform programming model, in the style of LINQ and Ruby on Rails, with language syntax for accessing objects residing either in the database or at the server. Object-level security policies are expressed as fully-customizable, first-class labels which may themselves be subject to security policies. Access to labeled data is mediated via trusted, user-provided policy enforcement functions.SELinks has two novel features that ensure security policies are enforced correctly and efficiently. First, SELinks implements a type system called Fable that allows a protected object's type to refer to its protecting label. The type system can check that labeled data is never accessed directly by the program without first consulting the appropriate policy enforcement function. Second, SELinks compiles policy enforcement code to database-resident user-defined functions that can be called directly during query processing. Database-side checking avoids transferring data to the server needlessly, while still allowing policies to be expressed in a customizable and portable manner.Our experience with two sizable web applications, a modelhealth-care database and a secure wiki with fine-grained security policies, indicates that cross-tier policy enforcement in SELinks is flexible, relatively easy to use, and, when compared to a single-tier approach, improves throughput by nearly an order of magnitude. SELinks is freely available.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {269–282},
numpages = {14},
keywords = {type systems, security enforcement, web applications, database programming, compilers},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257456,
author = {Waas, Florian},
title = {Session Details: Research Session 8: Column Stores},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257456},
doi = {10.1145/3257456},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559877,
author = {Binnig, Carsten and Hildenbrand, Stefan and F\"{a}rber, Franz},
title = {Dictionary-Based Order-Preserving String Compression for Main Memory Column Stores},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559877},
doi = {10.1145/1559845.1559877},
abstract = {Column-oriented database systems [19, 23] perform better than traditional row-oriented database systems on analytical workloads such as those found in decision support and business intelligence applications. Moreover, recent work [1, 24] has shown that lightweight compression schemes significantly improve the query processing performance of these systems. One such a lightweight compression scheme is to use a dictionary in order to replace long (variable-length) values of a certain domain with shorter (fixedlength) integer codes. In order to further improve expensive query operations such as sorting and searching, column-stores often use order-preserving compression schemes.In contrast to the existing work, in this paper we argue that orderpreserving dictionary compression does not only pay off for attributes with a small fixed domain size but also for long string attributes with a large domain size which might change over time. Consequently, we introduce new data structures that efficiently support an order-preserving dictionary compression for (variablelength) string attributes with a large domain size that is likely to change over time. The main idea is that we model a dictionary as a table that specifies a mapping from string-values to arbitrary integer codes (and vice versa) and we introduce a novel indexing approach that provides efficient access paths to such a dictionary while compressing the index data. Our experiments show that our data structures are as fast as (or in some cases even faster than) other state-of-the-art data structures for dictionaries while being less memory intensive.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {283–296},
numpages = {14},
keywords = {column store, string compression},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559878,
author = {Idreos, Stratos and Kersten, Martin L. and Manegold, Stefan},
title = {Self-Organizing Tuple Reconstruction in Column-Stores},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559878},
doi = {10.1145/1559845.1559878},
abstract = {Column-stores gained popularity as a promising physical design alternative. Each attribute of a relation is physically stored as a separate column allowing queries to load only the required attributes. The overhead incurred is on-the-fly tuple reconstruction for multi-attribute queries. Each tuple reconstruction is a join of two columns based on tuple IDs, making it a significant cost component. The ultimate physical design is to have multiple presorted copies of each base table such that tuples are already appropriately organized in multiple different orders across the various columns. This requires the ability to predict the workload, idle time to prepare, and infrequent updates.In this paper, we propose a novel design, partial sideways cracking, that minimizes the tuple reconstruction cost in a self-organizing way. It achieves performance similar to using presorted data, but without requiring the heavy initial presorting step itself. Instead, it handles dynamic, unpredictable workloads with no idle time and frequent updates. Auxiliary dynamic data structures, called cracker maps, provide a direct mapping between pairs of attributes used together in queries for tuple reconstruction. A map is continuously physically reorganized as an integral part of query evaluation, providing faster and reduced data access for future queries. To enable flexible and self-organizing behavior in storage-limited environments, maps are materialized only partially as demanded by the workload. Each map is a collection of separate chunks that are individually reorganized, dropped or recreated as needed. We implemented partial sideways cracking in an open-source column-store. A detailed experimental analysis demonstrates that it brings significant performance benefits for multi-attribute queries.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {297–308},
numpages = {12},
keywords = {database cracking, self-organization},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559879,
author = {Ivanova, Milena G. and Kersten, Martin L. and Nes, Niels J. and Gon\c{c}alves, Romulo A.P.},
title = {An Architecture for Recycling Intermediates in a Column-Store},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559879},
doi = {10.1145/1559845.1559879},
abstract = {Automatically recycling (intermediate) results is a grand challenge for state-of-the-art databases to improve both query response time and throughput. Tuples are loaded and streamed through a tuple-at-a-time processing pipeline avoiding materialization of intermediates as much as possible. This limits the opportunities for reuse of overlapping computations to DBA-defined materialized views and function/result cache tuning.In contrast, the operator-at-a-time execution paradigm produces fully materialized results in each step of the query plan. To avoid resource contention, these intermediates are evicted as soon as possible.In this paper we study an architecture that harvests the by-products of the operator-at-a-time paradigm in a column store system using a lightweight mechanism, the recycler. The key challenge then becomes selection of the policies to admit intermediates to the resource pool, their retention period, and the eviction strategy when facing resource limitations.The proposed recycling architecture has been implemented in an open-source system. An experimental analysis against the TPC-H ad-hoc decision support benchmark and a complex, real-world application (SkyServer) demonstrates its effectiveness in terms of self-organizing behavior and its significant performance gains. The results indicate the potentials of recycling intermediates and charters a route for further development of database kernels.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {309–320},
numpages = {12},
keywords = {databasekernels, caching, column-stores},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257457,
author = {Gravano, Luis},
title = {Session Details: Research Session 9: Data on the Web},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257457},
doi = {10.1145/3257457},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559881,
author = {Chen, Fei and Gao, Byron J. and Doan, AnHai and Yang, Jun and Ramakrishnan, Raghu},
title = {Optimizing Complex Extraction Programs over Evolving Text Data},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559881},
doi = {10.1145/1559845.1559881},
abstract = {Most information extraction (IE) approaches have considered only static text corpora, over which we apply IE only once. Many real-world text corpora however are dynamic. They evolve over time, and so to keep extracted information up to date we often must apply IE repeatedly, to consecutive corpus snapshots. Applying IE from scratch to each snapshot can take a lot of time. To avoid doing this, we have recently developed Cyclex, a system that recycles previous IE results to speed up IE over subsequent corpus snapshots. Cyclex clearly demonstrated the promise of the recycling idea. The work itself however is limited in that it considers only IE programs that contain a single IE ``blackbox.'' In practice, many IE programs are far more complex, containing multiple IE blackboxes connected in a compositional ``workflow.''In this paper, we present Delex, a system that removes the above limitation. First we identify many difficult challenges raised by Delex, including modeling complex IE programs for recycling purposes, implementing the recycling process efficiently, and searching for an optimal execution plan in a vast plan space with different recycling alternatives. Next we describe our solutions to these challenges. Finally, we describe extensive experiments with both rule-based and learning-based IE programs over two real-world data sets, which demonstrate the utility of our approach.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {321–334},
numpages = {14},
keywords = {evolving text, optimization, information extraction},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559882,
author = {Dalvi, Nilesh and Bohannon, Philip and Sha, Fei},
title = {Robust Web Extraction: An Approach Based on a Probabilistic Tree-Edit Model},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559882},
doi = {10.1145/1559845.1559882},
abstract = {On script-generated web sites, many documents share common HTML tree structure, allowing wrappers to effectively extract information of interest. Of course, the scripts and thus the tree structure evolve over time, causing wrappers to break repeatedly, and resulting in a high cost of maintaining wrappers. In this paper, we explore a novel approach: we use temporal snapshots of web pages to develop a tree-edit model of HTML, and use this model to improve wrapper construction. We view the changes to the tree structure as suppositions of a series of edit operations: deleting nodes, inserting nodes and substituting labels of nodes. The tree structures evolve by choosing these edit operations stochastically.Our model is attractive in that the probability that a source tree has evolved into a target tree can be estimated efficiently--in quadratic time in the size of the trees--making it a potentially useful tool for a variety of tree-evolution problems. We give an algorithm to learn the probabilistic model from training examples consisting of pairs of trees, and apply this algorithm to collections of web-page snapshots to derive HTML-specific tree edit models. Finally, we describe a novel wrapper-construction framework that takes the tree-edit model into account, and compare the quality of resulting wrappers to that of traditional wrappers on synthetic and real HTML document examples.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {335–348},
numpages = {14},
keywords = {probabilistic tree-edit model, xpath, wrappers},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559883,
author = {Chu, Eric and Baid, Akanksha and Chai, Xiaoyong and Doan, AnHai and Naughton, Jeffrey},
title = {Combining Keyword Search and Forms for Ad Hoc Querying of Databases},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559883},
doi = {10.1145/1559845.1559883},
abstract = {A common criticism of database systems is that they are hard to query for users uncomfortable with a formal query language. To address this problem, form-based interfaces and keyword search have been proposed; while both have benefits, both also have limitations. In this paper, we investigate combining the two with the hopes of creating an approach that provides the best of both. Specifically, we propose to take as input a target database and then generate and index a set of query forms offline. At query time, a user with a question to be answered issues standard keyword search queries; but instead of returning tuples, the system returns forms relevant to the question. The user may then build a structured query with one of these forms and submit it back to the system for evaluation. In this paper, we address challenges that arise in form generation, keyword search over forms, and ranking and displaying these forms. We explore techniques to tackle these challenges, and present experimental results suggesting that the approach of combining keyword search and form-based interfaces is promising.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {349–360},
numpages = {12},
keywords = {relational databases, keyword search, query forms},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257458,
author = {Yang, Jun},
title = {Session Details: Research Session 10: Probabilistic Databases I},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257458},
doi = {10.1145/3257458},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559885,
author = {Li, Feifei and Yi, Ke and Jestes, Jeffrey},
title = {Ranking Distributed Probabilistic Data},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559885},
doi = {10.1145/1559845.1559885},
abstract = {Ranking queries are essential tools to process large amounts of probabilistic data that encode exponentially many possible deterministic instances. In many applications where uncertainty and fuzzy information arise, data are collected from multiple sources in distributed, networked locations, e.g., distributed sensor fields with imprecise measurements, multiple scientific institutes with inconsistency in their scientific data. Due to the network delay and the economic cost associated with communicating large amounts of data over a network, a fundamental problem in these scenarios is to retrieve the global top-k tuples from all distributed sites with minimum communication cost. Using the well founded notion of the expected rank of each tuple across all possible worlds as the basis of ranking, this work designs both communication- and computation-efficient algorithms for retrieving the top-k tuples with the smallest ranks from distributed sites. Extensive experiments using both synthetic and real data sets confirm the efficiency and superiority of our algorithms over the straightforward approach of forwarding all data to the server.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {361–374},
numpages = {14},
keywords = {uncertain databases, ranking queries, probabilistic data, top-k, distributed query processing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559886,
author = {Ge, Tingjian and Zdonik, Stan and Madden, Samuel},
title = {Top-<i>k</i> Queries on Uncertain Data: On Score Distribution and Typical Answers},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559886},
doi = {10.1145/1559845.1559886},
abstract = {Uncertain data arises in a number of domains, including data integration and sensor networks. Top-k queries that rank results according to some user-defined score are an important tool for exploring large uncertain data sets. As several recent papers have observed, the semantics of top-k queries on uncertain data can be ambiguous due to tradeoffs between reporting high-scoring tuples and tuples with a high probability of being in the resulting data set. In this paper, we demonstrate the need to present the score distribution of top-k vectors to allow the user to choose between results along this score-probability dimensions. One option would be to display the complete distribution of all potential top-k tuple vectors, but this set is too large to compute. Instead, we propose to provide a number of typical vectors that effectively sample this distribution. We propose efficient algorithms to compute these vectors. We also extend the semantics and algorithms to the scenario of score ties, which is not dealt with in the previous work in the area. Our work includes a systematic empirical study on both real dataset and synthetic datasets.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {375–388},
numpages = {14},
keywords = {top-k, uncertain data, distribution, typical},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559887,
author = {Olteanu, Dan and Huang, Jiewen},
title = {Secondary-Storage Confidence Computation for Conjunctive Queries with Inequalities},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559887},
doi = {10.1145/1559845.1559887},
abstract = {This paper investigates the problem of efficiently computing the confidences of distinct tuples in the answers to conjunctive queries with inequalities (&lt;) on tuple-independent probabilistic databases. This problem is fundamental to probabilistic databases and was recently stated open.Our contributions are of both theoretical and practical importance. We define a class of tractable queries with inequalities, and generalize existing results on #P-hardness of query evaluation, now in the presence of inequalities.For the tractable queries, we introduce a confidence computation technique based on efficient compilation of the lineage of the query answer into Ordered Binary Decision Diagrams (OBDDs), whose sizes are linear in the number of variables of the lineage.We implemented a secondary-storage variant of our technique in PostgreSQL. This variant does not need to materialize the OBDD, but computes, in one scan over the lineage, the probabilities of OBDD fragments and combines them on the fly. Experiments with probabilistic TPC-H data show up to two orders of magnitude improvements when compared with state-of-the-art approaches.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {389–402},
numpages = {14},
keywords = {decision diagrams, query processing, probabilistic databases},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257459,
author = {Korth, Hank},
title = {Session Details: Research Session 11: Database Optimization},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257459},
doi = {10.1145/3257459},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559889,
author = {Neumann, Thomas},
title = {Query Simplification: Graceful Degradation for Join-Order Optimization},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559889},
doi = {10.1145/1559845.1559889},
abstract = {Join ordering is one of the most important, but also most challenging problems of query optimization. In general finding the optimal join order is NP-hard. Existing dynamic programming algorithms exhibit exponential runtime even for the restricted, but highly relevant class of star joins. Therefore, it is infeasible to find the optimal join order when the query includes a large number of joins. Existing approaches for large queries switch to greedy heuristics or randomized algorithms at some point, which can degrade query execution performance by orders of magnitude.We propose a new paradigm for optimizing large queries: when a query is too complex to be optimized exactly, we simplify the query's join graph until the optimization problem becomes tractable within a given time budget. During simplification, we apply safe simplifications before more risky ones. This way join ordering problems are solved optimally if possible, and gracefully degrade with increasing query complexity.This paper presents a general framework for query simplification and a strategy for directing the simplification process. Extensive experiments with different kinds of queries, different join-graph structures, and different cost functions indicate that query simplification is very robust and outperforms previous methods for join-order optimization.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {403–414},
numpages = {12},
keywords = {hypergraphs, complex joins, query optimization},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559890,
author = {Finger, Jonathan and Polyzotis, Neoklis},
title = {Robust and Efficient Algorithms for Rank Join Evaluation},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559890},
doi = {10.1145/1559845.1559890},
abstract = {In the rank join problem we are given a relational join R1 x R2 and a function that assigns numeric scores to the join tuples, and the goal is to return the tuples with the highest score. This problem lies at the core of processing top-k SQL queries, and recent studies have introduced specialized operators that solve the rank join problem by accessing only a subset of the input tuples. A desirable property for such operators is instance-optimality, i.e., their I/O cost should remain within a factor of the optimal for different inputs. However, a recent theoretical study has shown that existing rank join operators are not instance-optimal even though they have been shown to perform well empirically. The same study proposed the PBRJRRoverFR operator that was proved to be instance-optimal, but its performance was not tested empirically and in fact it was hinted that its complexity can be high. Thus, the following important question is raised: Is it possible to design a rank join operator that is both instance-optimal and computationally efficient?In this paper we provide an answer to this challenging question. We perform an empirical study of PBRJRRoverFR and show that its computational cost can offset the benefits of instance-optimality. Using the insights gained by the study, we develop the novel FRPA operator that addresses the efficiency bottlenecks of PBRJRRoverFR. We prove that FRPA is instance-optimal in general and more specifically that it never performs more I/O than PBRJRRoverFR. FRPA is the first operator that possesses these properties and is thus of interest in the theoretical study of rank join operators. We further identify cases where the overhead of FRPA becomes significant, and propose the FRPA operator that automatically adapts its overhead to the characteristics of the input. An extensive experimental study validates the effectiveness of the new operators and demonstrates that they offer significant performance improvements (up to an order of magnitude) over the state-of-the-art.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {415–428},
numpages = {14},
keywords = {ranking queries, rank join, feasible region bound, adaptive pulling},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559891,
author = {Hadjieleftheriou, Marios and Koudas, Nick and Srivastava, Divesh},
title = {Incremental Maintenance of Length Normalized Indexes for Approximate String Matching},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559891},
doi = {10.1145/1559845.1559891},
abstract = {Approximate string matching is a problem that has received a lot of attention recently. Existing work on information retrieval has concentrated on a variety of similarity measures TF/IDF, BM25, HMM, etc.) specifically tailored for document retrieval purposes. As new applications that depend on retrieving short strings are becoming popular(e.g., local search engines like YellowPages.com, Yahoo!Local, and Google Maps) new indexing methods are needed, tailored for short strings. For that purpose, a number of indexing techniques and related algorithms have been proposed based on length normalized similarity measures. A common denominator of indexes for length normalized measures is that maintaining the underlying structures in the presence of incremental updates is inefficient, mainly due to data dependent, precomputed weights associated with each distinct token and string. Incorporating updates usually is accomplished by rebuilding the indexes at regular time intervals. In this paper we present a framework that advocates lazy update propagation with the following key feature: Efficient, incremental updates that immediately reflect the new data in the indexes in a way that gives strict guarantees on the quality of subsequent query answers. More specifically, our techniques guarantee against false negatives and limit the number of false positives produced. We implement a fully working prototype and illustrate that the proposed ideas work really well in practice for real datasets.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {429–440},
numpages = {12},
keywords = {idf, inverted index, string indexing, length normalization, updates},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257460,
author = {Garofalakis, Minos},
title = {Session Details: Research Session 12: Probabilistic Databases II},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257460},
doi = {10.1145/3257460},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559893,
author = {Xu, Fei and Beyer, Kevin and Ercegovac, Vuk and Haas, Peter J. and Shekita, Eugene J.},
title = {E = MC<sup>3</sup>: Managing Uncertain Enterprise Data in a Cluster-Computing Environment},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559893},
doi = {10.1145/1559845.1559893},
abstract = {Modern enterprises must manage uncertain data for purposes of risk assessment and decisionmaking under uncertainty. The Monte Carlo approach embodied in the MCDB system of Jampani et al. is well suited for such a task. MCDB can support industrial strength business-intelligence queries over uncertain warehouse data. Moreover, MCDB's extensible approach to specifying uncertainty can also capture complex stochastic prediction models, allowing sophisticated ``what-if'' analyses within the DBMS. The MCDB computations can be highly CPU intensive, but offer the potential for massive parallelization. To realize this potential, we provide a new system, called MC3 (Monte Carlo Computation on a Cluster), that extends the MCDB approach to the map-reduce processing framework. MC3 can exploit the robustness and scalability of map-reduce, and can handle data stored in non-relational formats. We show how MCDB query plans over ``tuple bundles'' can be translated to sequences of map-reduce operations over nested data, and describe different parallelization schemes. We also provide and analyze several novel distributed algorithms for adding pseudorandom number seeds to tuple bundles. These algorithms ensure statistical correctness of the Monte-Carlo computations while minimizing the seed length. Our experiments show that MC3 can scale well for a variety of workloads.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {441–454},
numpages = {14},
keywords = {json, uncertain data, map-reduce, jaql, monte carlo},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559894,
author = {Kanagal, Bhargav and Deshpande, Amol},
title = {Indexing Correlated Probabilistic Databases},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559894},
doi = {10.1145/1559845.1559894},
abstract = {With large amounts of correlated probabilistic data being generated in a wide range of application domains including sensor networks, information extraction, event detection etc., effectively managing and querying them has become an important research direction. While there is an exhaustive body of literature on querying independent probabilistic data, supporting efficient queries over large-scale, correlated databases remains a challenge. In this paper, we develop efficient data structures and indexes for supporting inference and decision support queries over such databases. Our proposed hierarchical data structure is suitable both for in-memory and disk-resident databases. We represent the correlations in the probabilistic database using a junction tree over the tuple-existence or attribute-value random variables, and use tree partitioning techniques to build an index structure over it. We show how to efficiently answer inference and aggregation queries using such an index, resulting in orders of magnitude performance benefits in most cases. In addition, we develop novel algorithms for efficiently keeping the index structure up-to-date as changes (inserts, updates) are made to the probabilistic database. We present a comprehensive experimental study illustrating the benefits of our approach to query processing in probabilistic databases.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {455–468},
numpages = {14},
keywords = {caching, indexing, inference queries, junction trees, probabilistic databases},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559895,
author = {Cormode, Graham and Golab, Lukasz and Flip, Korn and McGregor, Andrew and Srivastava, Divesh and Zhang, Xi},
title = {Estimating the Confidence of Conditional Functional Dependencies},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559895},
doi = {10.1145/1559845.1559895},
abstract = {Conditional functional dependencies (CFDs) have recently been proposed as extensions of classical functional dependencies that apply to a certain subset of the relation, as specified by a pattern tableau. Calculating the support and confidence of a CFD (i.e., the size of the applicable subset and the extent to which it satisfies the CFD)gives valuable information about data semantics and data quality. While computing the support is easier, computing the confidence exactly is expensive if the relation is large, and estimating it from a random sample of the relation is unreliable unless the sample is large.We study how to efficiently estimate the confidence of a CFD with a small number of passes (one or two) over the input using small space. Our solutions are based on a variety of sampling and sketching techniques, and apply when the pattern tableau is known in advance, and also the harder case when this is given after the data have been seen. We analyze our algorithms, and show that they can guarantee a small additive error; we also show that relative errors guarantees are not possible. We demonstrate the power of these methods empirically, with a detailed study using both real and synthetic data. These experiments show that it is possible to estimate the CFD confidence very accurately with summaries which are much smaller than the size of the data they represent.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {469–482},
numpages = {14},
keywords = {conditional functional dependencies},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257461,
author = {Ooi, Beng Chin},
title = {Session Details: Research Session 13: Skyline Query Processing},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257461},
doi = {10.1145/3257461},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559897,
author = {Zhang, Shiming and Mamoulis, Nikos and Cheung, David W.},
title = {Scalable Skyline Computation Using Object-Based Space Partitioning},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559897},
doi = {10.1145/1559845.1559897},
abstract = {The skyline operator returns from a set of multi-dimensional objects a subset of superior objects that are not dominated by others. This operation is considered very important in multi-objective analysis of large datasets. Although a large number of skyline methods have been proposed, the majority of them focuses on minimizing the I/O cost. However, in high dimensional spaces, the problem can easily become CPU-bound due to the large number of computations required for comparing objects with current skyline points while scanning the database. Based on this observation, we propose a dynamic indexing technique for skyline points that can be integrated into state-of-the-art sort-based skyline algorithms to boost their computational performance. The new indexing and dominance checking approach is supported by a theoretical analysis, while our experiments show that it scales well with the input size and dimensionality not only because unnecessary dominance checks are avoided but also because it allows efficient dominance checking with the help of bitwise operations.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {483–494},
numpages = {12},
keywords = {skyline, space partitioning, preference},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559898,
author = {Zhang, Zhenjie and Cheng, Reynold and Papadias, Dimitris and Tung, Anthony K.H.},
title = {Minimizing the Communication Cost for Continuous Skyline Maintenance},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559898},
doi = {10.1145/1559845.1559898},
abstract = {Existing work in the skyline literature focuses on optimizing the processing cost. This paper aims at minimization of the communication overhead in client-server architectures, where a server continuously maintains the skyline of dynamic objects. Our first contribution is a Filter method that avoids transmission of updates from objects that cannot influence the skyline. Specifically, each object is assigned a filter so that it needs to issue an update only if it violates its filter. Filter achieves significant savings over the naive approach of transmitting all updates. Going one step further, we introduce the concept of frequent skyline query over a sliding window(FSQW). The motivation is that snapshot skylines are not very useful in streaming environments because they keep changing over time. Instead, FSQW reports the objects that appear in the skylines of at least θ ⋅ s of the s most recent timestamps (0 &lt; θ ≤ 1). Filter can be easily adapted to FSQW processing, however, with potentially high overhead for large and frequently updated datasets. To further reduce the communication cost, we propose a Sampling method, which returns approximate FSQW results without computing each snapshot skyline. Finally, we integrate Filter and Sampling in a Hybrid approach that combines their individual advantages.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {495–508},
numpages = {14},
keywords = {skyline query, continuous query, communication},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559899,
author = {Zhang, Zhenjie and Yang, Yin and Cai, Ruichu and Papadias, Dimitris and Tung, Anthony},
title = {Kernel-Based Skyline Cardinality Estimation},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559899},
doi = {10.1145/1559845.1559899},
abstract = {The skyline of a d-dimensional dataset consists of all points not dominated by others. The incorporation of the skyline operator into practical database systems necessitates an efficient and effective cardinality estimation module. However, existing theoretical work on this problem is limited to the case where all d dimensions are independent of each other, which rarely holds for real datasets. The state of the art Log Sampling (LS) technique simply applies theoretical results for independent dimensions to non-independent data anyway, sometimes leading to large estimation errors. To solve this problem, we propose a novel Kernel-Based (KB) approach that approximates the skyline cardinality with nonparametric methods. Extensive experiments with various real datasets demonstrate that KB achieves high accuracy, even in cases where LS fails. At the same time, despite its numerical nature, the efficiency of KB is comparable to that of LS. Furthermore, we extend both LS and KB to the k-dominant skyline, which is commonly used instead of the conventional skyline for high-dimensional data.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {509–522},
numpages = {14},
keywords = {skyline, cardinality estimation, kernel, non-parametric methods},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257462,
author = {Srivastava, Divesh},
title = {Session Details: Research Session 14: Understanding Data and Queries},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257462},
doi = {10.1145/3257462},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559901,
author = {Chapman, Adriane and Jagadish, H. V.},
title = {Why Not?},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559901},
doi = {10.1145/1559845.1559901},
abstract = {As humans, we have expectations for the results of any action, e.g. we expect at least one student to be returned when we query a university database for student records. When these expectations are not met, traditional database users often explore datasets via a series of slightly altered SQL queries. Yet most database access is via limited interfaces that deprive end users of the ability to alter their query in any way to garner better understanding of the dataset and result set. Users are unable to question why a particular data item is Not in the result set of a given query. In this work, we develop a model for answers to WHY NOT? queries. We show through a user study the usefulness of our answers, and describe two algorithms for finding the manipulation that discarded the data item of interest. Moreover, we work through two different methods for tracing the discarded data item that can be used with either algorithm. Using our algorithms, it is feasible for users to find the manipulation that excluded the data item of interest, and can eliminate the need for exhausting debugging.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {523–534},
numpages = {12},
keywords = {user understanding, provenance, result explanations, lineage},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559902,
author = {Tran, Quoc Trung and Chan, Chee-Yong and Parthasarathy, Srinivasan},
title = {Query by Output},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559902},
doi = {10.1145/1559845.1559902},
abstract = {It has recently been asserted that the usability of a database is as important as its capability. Understanding the database schema, the hidden relationships among attributes in the data all play an important role in this context. Subscribing to this viewpoint, in this paper, we present a novel data-driven approach, called Query By Output (QBO), which can enhance the usability of database systems. The central goal of QBO is as follows: given the output of some query Q on a database D, denoted by Q(D), we wish to construct an alternative query Q′ such that Q(D) and Q′ (D) are instance-equivalent. To generate instance-equivalent queries from Q(D), we devise a novel data classification-based technique that can handle the at-least-one semantics that is inherent in the query derivation. In addition to the basic framework, we design several optimization techniques to reduce processing overhead and introduce a set of criteria to rank order output queries by various notions of utility. Our framework is evaluated comprehensively on three real data sets and the results show that the instance-equivalent queries we obtain are interesting and that the approach is scalable and robust to queries of different selectivities.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {535–548},
numpages = {14},
keywords = {instance-equivalent queries, query by output, at-least-one semantics},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559903,
author = {Sun, Peng and Liu, Ziyang and Davidson, Susan B. and Chen, Yi},
title = {Detecting and Resolving Unsound Workflow Views for Correct Provenance Analysis},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559903},
doi = {10.1145/1559845.1559903},
abstract = {Workflow views abstract groups of tasks in a workflow into high level composite tasks, in order to reuse sub-workflows and facilitate provenance analysis. However, unless a view is carefully designed, it may not preserve the dataflow between tasks in the workflow, i.e., it may not be sound. Unsound views can be misleading and cause incorrect provenance analysis.This paper studies the problem of efficiently identifying and correcting unsound workflow views with minimal changes. In particular, given a workflow view, we wish to split each unsound composite task into the minimal number of tasks, such that the resulting view is sound. We prove that this problem is NP-hard by reduction from independent set. We then propose two local optimality conditions (weak and strong), and design polynomial time algorithms for correcting unsound views to meet these conditions. Experiments show that our proposed algorithms are effective and efficient, and that the strong local optimality algorithm produces better solutions than the weak local optimality algorithm with little processing overhead.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {549–562},
numpages = {14},
keywords = {view, soundness, workflow, provenance},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257463,
author = {Gunopulos, Dimitris},
title = {Session Details: Research Session 15: Nearest Neighbor Search},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257463},
doi = {10.1145/3257463},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559905,
author = {Tao, Yufei and Yi, Ke and Sheng, Cheng and Kalnis, Panos},
title = {Quality and Efficiency in High Dimensional Nearest Neighbor Search},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559905},
doi = {10.1145/1559845.1559905},
abstract = {Nearest neighbor (NN) search in high dimensional space is an important problem in many applications. Ideally, a practical solution (i) should be implementable in a relational database, and (ii) its query cost should grow sub-linearly with the dataset size, regardless of the data and query distributions. Despite the bulk of NN literature, no solution fulfills both requirements, except locality sensitive hashing (LSH). The existing LSH implementations are either rigorous or adhoc. Rigorous-LSH ensures good quality of query results, but requires expensive space and query cost. Although adhoc-LSH is more efficient, it abandons quality control, i.e., the neighbor it outputs can be arbitrarily bad. As a result, currently no method is able to ensure both quality and efficiency simultaneously in practice.Motivated by this, we propose a new access method called the locality sensitive B-tree (LSB-tree) that enables fast high-dimensional NN search with excellent quality. The combination of several LSB-trees leads to a structure called the LSB-forest that ensures the same result quality as rigorous-LSH, but reduces its space and query cost dramatically. The LSB-forest also outperforms adhoc-LSH, even though the latter has no quality guarantee. Besides its appealing theoretical properties, the LSB-tree itself also serves as an effective index that consumes linear space, and supports efficient updates. Our extensive experiments confirm that the LSB-tree is faster than (i) the state of the art of exact NN search by two orders of magnitude, and (ii) the best (linear-space) method of approximate retrieval by an order of magnitude, and at the same time, returns neighbors with much better quality.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {563–576},
numpages = {14},
keywords = {locality sensitive hashing, nearest neighbor search},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559906,
author = {Gao, Yunjun and Zheng, Baihua},
title = {Continuous Obstructed Nearest Neighbor Queries in Spatial Databases},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559906},
doi = {10.1145/1559845.1559906},
abstract = {In this paper, we study a novel form of continuous nearest neighbor queries in the presence of obstacles, namely continuous obstructed nearest neighbor (CONN) search. It considers the impact of obstacles on the distance between objects, which is ignored by most of spatial queries. Given a data set P, an obstacle set O, and a query line segment q in a two-dimensional space, a CONN query retrieves the nearest neighbor of each point on q according to the obstructed distance, i.e., the shortest path between them without crossing any obstacle. We formulate CONN search, analyze its unique properties, and develop algorithms for exact CONN query processing, assuming that both P and O are indexed by conventional data-partitioning indices (e.g., R-trees). Our methods tackle the CONN retrieval by performing a single query for the entire query segment, and only process the data points and obstacles relevant to the final result, via a novel concept of control points and an efficient quadratic-based split point computation algorithm. In addition, we extend our solution to handle the continuous obstructed k-nearest neighbor (COkNN) search, which finds the k (≥1)nearest neighbors to every point along q based on obstructed distances. A comprehensive experimental evaluation using both real and synthetic datasets has been conducted to demonstrate the efficiency and effectiveness of our proposed algorithms.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {577–590},
numpages = {14},
keywords = {continuous nearest neighbor, nearest neighbor, obstacle, continuous obstructed nearest neighbor, spatial database},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559907,
author = {Chen, Zaiben and Shen, Heng Tao and Zhou, Xiaofang and Yu, Jeffrey Xu},
title = {Monitoring Path Nearest Neighbor in Road Networks},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559907},
doi = {10.1145/1559845.1559907},
abstract = {This paper addresses the problem of monitoring the k nearest neighbors to a dynamically changing path in road networks. Given a destination where a user is going to, this new query returns the k-NN with respect to the shortest path connecting the destination and the user's current location, and thus provides a list of nearest candidates for reference by considering the whole coming journey. We name this query the k-Path Nearest Neighbor query (k-PNN). As the user is moving and may not always follow the shortest path, the query path keeps changing. The challenge of monitoring the k-PNN for an arbitrarily moving user is to dynamically determine the update locations and then refresh the k-PNN efficiently. We propose a three-phase Best-first Network Expansion (BNE) algorithm for monitoring the k-PNN and the corresponding shortest path. In the searching phase, the BNE finds the shortest path to the destination, during which a candidate set that guarantees to include the k-PNN is generated at the same time. Then in the verification phase, a heuristic algorithm runs for examining candidates' exact distances to the query path, and it achieves significant reduction in the number of visited nodes. The monitoring phase deals with computing update locations as well as refreshing the k-PNN in different user movements. Since determining the network distance is a costly process, an expansion tree and the candidate set are carefully maintained by the BNE algorithm, which can provide efficient update on the shortest path and the k-PNN results. Finally, we conduct extensive experiments on real road networks and show that our methods achieve satisfactory performance.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {591–602},
numpages = {12},
keywords = {road networks, path nearest neighbor, spatial databases},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257464,
author = {Grust, Torsten},
title = {Session Details: Research Session 16: Query Processing on Semi-Structured Data},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257464},
doi = {10.1145/3257464},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559909,
author = {Georgiadis, Haris and Charalambides, Minas and Vassalos, Vasilis},
title = {Cost Based Plan Selection for Xpath},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559909},
doi = {10.1145/1559845.1559909},
abstract = {We present a complete XPath cost-based optimization and execution framework and demonstrate its effectiveness and efficiency for a variety of queries and datasets. The framework is based on a logical XPath algebra with novel features and operators and a comprehensive set of rewriting rules that together enable us to algebraically capture many existing and novel processing strategies for XPath queries. An important part of the framework is PSA, a very efficient cost-based plan selection algorithm for XPath queries. In the presented experimental evaluation, PSA picked the cheapest estimated query plan in 100% of the cases. Our cost-based query optimizer independent of the underlying physical data model and storage system and of the available logical operator implementations, depending on a set of well-defined APIs. We also present an implementation of those APIs, including primitive access methods, a large pool of physical operators, statistics estimators and cost models, and experimentally demonstrate the effectiveness of our end-to-end query optimization system.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {603–614},
numpages = {12},
keywords = {xpath, xml, cost-based optimization, cost models, cardinality estimation, algebraic rewritings},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559910,
author = {Abdel Kader, Riham and Boncz, Peter and Manegold, Stefan and van Keulen, Maurice},
title = {ROX: Run-Time Optimization of XQueries},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559910},
doi = {10.1145/1559845.1559910},
abstract = {Optimization of complex XQueries combining many XPath steps and joins is currently hindered by the absence of good cardinality estimation and cost models for XQuery. Additionally, the state-of-the-art of even relational query optimization still struggles to cope with cost model estimation errors that increase with plan size, as well as with the effect of correlated joins and selections.In this research, we propose to radically depart from the traditional path of separating the query compilation and query execution phases, by having the optimizer execute, materialize partial results, and use sampling based estimation techniques to observe the characteristics of intermediates. The proposed technique takes as input a Join Graph where the edges are either equi-joins or XPath steps, and the execution environment provides value- and structural-join algorithms, as well as structural and value-based indices.While run-time optimization with sampling removes many of the vulnerabilities of classical optimizers, it brings its own challenges with respect to keeping resource usage under control, both with respect to the materialization of intermediates, as well as the cost of plan exploration using sampling. Our approach deals with these issues by limiting the run-time search space to so-called "zero-investment algorithms for which sampling can be guaranteed to be strictly linear in sample size. All operators and XML value indices used by ROX for sampling have the zero-investment property.We perform extensive experimental evaluation on large XML datasets that shows that our run-time query optimizer finds good query plans in a robust fashion and has limited run-time overhead.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {615–626},
numpages = {12},
keywords = {xquery, xml, optimization},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559911,
author = {Neumann, Thomas and Weikum, Gerhard},
title = {Scalable Join Processing on Very Large RDF Graphs},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559911},
doi = {10.1145/1559845.1559911},
abstract = {With the proliferation of the RDF data format, engines for RDF query processing are faced with very large graphs that contain hundreds of millions of RDF triples. This paper addresses the resulting scalability problems. Recent prior work along these lines has focused on indexing and other physical-design issues. The current paper focuses on join processing, as the fine-grained and schema-relaxed use of RDF often entails star- and chain-shaped join queries with many input streams from index scans.We present two contributions for scalable join processing. First, we develop very light-weight methods for sideways information passing between separate joins at query run-time, to provide highly effective filters on the input streams of joins. Second, we improve previously proposed algorithms for join-order optimization by more accurate selectivity estimations for very large RDF graphs. Experimental studies with several RDF datasets, including the UniProt collection, demonstrate the performance gains of our approach, outperforming the previously fastest systems by more than an order of magnitude.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {627–640},
numpages = {14},
keywords = {query processing, rdf},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257465,
author = {Miller, Renee},
title = {Session Details: Research Session 17: Data Integration},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257465},
doi = {10.1145/3257465},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559913,
author = {Radwan, Ahmed and Popa, Lucian and Stanoi, Ioana R. and Younis, Akmal},
title = {Top-k Generation of Integrated Schemas Based on Directed and Weighted Correspondences},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559913},
doi = {10.1145/1559845.1559913},
abstract = {Schema integration is the problem of creating a unified target schema based on a set of existing source schemas and based on a set of correspondences that are the result of matching the source schemas. Previous methods for schema integration rely on the exploration, implicit or explicit, of the multiple design choices that are possible for the integrated schema. Such exploration relies heavily on user interaction; thus, it is time consuming and labor intensive. Furthermore, previous methods have ignored the additional information that typically results from the schema matching process, that is, the weights and in some cases the directions that are associated with the correspondences.In this paper, we propose a more automatic approach to schema integration that is based on the use of directed and weighted correspondences between the concepts that appear in the source schemas. A key component of our approach is a novel top-k ranking algorithm for the automatic generation of the best candidate schemas. The algorithm gives more weight to schemas that combine the concepts with higher similarity or coverage. Thus, the algorithm makes certain decisions that otherwise would likely be taken by a human expert. We show that the algorithm runs in polynomial time and moreover has good performance in practice.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {641–654},
numpages = {14},
keywords = {data merging, interactive schema generation, schema integration, data integration, top-k generation, model management, schema mapping},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559914,
author = {Mecca, Giansalvatore and Papotti, Paolo and Raunich, Salvatore},
title = {Core Schema Mappings},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559914},
doi = {10.1145/1559845.1559914},
abstract = {Research has investigated mappings among data sources under two perspectives. On one side, there are studies of practical tools for schema mapping generation; these focus on algorithms to generate mappings based on visual specifications provided by users. On the other side, we have theoretical researches about data exchange. These study how to generate a solution - i.e., a target instance - given a set of mappings usually specified as tuple generating dependencies. However, despite the fact that the notion of a core of a data exchange solution has been formally identified as an optimal solution, there are yet no mapping systems that support core computations. In this paper we introduce several new algorithms that contribute to bridge the gap between the practice of mapping generation and the theory of data exchange. We show how, given a mapping scenario, it is possible to generate an executable script that computes core solutions for the corresponding data exchange problem. The algorithms have been implemented and tested using common runtime engines to show that they guarantee very good performances, orders of magnitudes better than those of known algorithms that compute the core as a post-processing step.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {655–668},
numpages = {14},
keywords = {core computation, data exchange, schema mappings},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559915,
author = {Zhong, Qian and Li, Hanyu and Li, Juanzi and Xie, Guotong and Tang, Jie and Zhou, Lizhu and Pan, Yue},
title = {A Gauss Function Based Approach for Unbalanced Ontology Matching},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559915},
doi = {10.1145/1559845.1559915},
abstract = {Ontology matching, aiming to obtain semantic correspondences between two ontologies, has played a key role in data exchange, data integration and metadata management. Among numerous matching scenarios, especially the applications cross multiple domains, we observe an important problem, denoted as unbalanced ontology matching which requires to find the matches between an ontology describing a local domain knowledge and another ontology covering the information over multiple domains, is not well studied in the community.In this paper, we propose a novel Gauss Function based ontology matching approach to deal with this unbalanced ontology matching issue. Given a relative lightweight ontology which represents the local domain knowledge, we extract a "similar" sub-ontology from the corresponding heavyweight ontology and then carry out the matching procedure between this lightweight ontology and the newly generated sub-ontology. The sub-ontology generation is based on the influences between concepts in the heavyweight ontology. We propose a Gauss Function based method to properly calculate the influence values between concepts. In addition, we perform an extensive experiment to verify the effectiveness and efficiency of our proposed approach by using OAEI2007 tasks. Experimental results clearly demonstrate that our solution outperforms the existing methods in terms of precision, recall and elapsed time.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {669–680},
numpages = {12},
keywords = {ontology matching, gauss function, unbalance},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257466,
author = {Papakonstantinou, Yannis},
title = {Session Details: Research Session 18: Keyword Search},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257466},
doi = {10.1145/3257466},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559917,
author = {Qin, Lu and Yu, Jeffrey Xu and Chang, Lijun},
title = {Keyword Search in Databases: The Power of RDBMS},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559917},
doi = {10.1145/1559845.1559917},
abstract = {Keyword search in relational databases (RDBs) has been extensively studied recently. A keyword search (or a keyword query) in RDBs is specified by a set of keywords to explore the interconnected tuple structures in an RDB that cannot be easily identified using SQL on RDBMS. In brief, it finds how the tuples containing the given keywords are connected via sequences of connections (foreign key references) among tuples in an RDB. Such interconnected tuple structures can be found as connected trees up to a certain size, sets of tuples that are reachable from a root tuple within a radius, or even multi-center subgraphs within a radius. In the literature, there are two main approaches. One is to generate a set of relational algebra expressions and evaluate every such expression using SQL on an RDBMS directly or in a middleware on top of an RDBMS indirectly. Due to a large number of relational algebra expressions needed to process, most of the existing works take a middleware approach without fully utilizing RDBMSs. The other is to materialize an RDB as a graph and find the interconnected tuple structures using graph-based algorithms in memory.In this paper we focus on using SQL to compute all the interconnected tuple structures for a given keyword query. We use three types of interconnected tuple structures to achieve that and we control the size of the structures. We show that the current commercial RDBMSs are powerful enough to support such keyword queries in RDBs efficiently without any additional new indexing to be built and maintained. The main idea behind our approach is tuple reduction. In our approach, in the first reduction step, we prune tuples that do not participate in any results using SQL, and in the second join step, we process the relational algebra expressions using SQL over the reduced relations. We conducted extensive experimental studies using two commercial RDBMSs and two large real datasets, and we report the efficiency of our approaches in this paper.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {681–694},
numpages = {14},
keywords = {relational database management systems, keyword search},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559918,
author = {Li, Guoliang and Ji, Shengyue and Li, Chen and Feng, Jianhua},
title = {Efficient Type-Ahead Search on Relational Data: A TASTIER Approach},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559918},
doi = {10.1145/1559845.1559918},
abstract = {Existing keyword-search systems in relational databases require users to submit a complete query to compute answers. Often users feel "left in the dark" when they have limited knowledge about the data, and have to use a try-and-see approach for modifying queries and finding answers. In this paper we propose a novel approach to keyword search in the relational world, called Tastier. A Tastier system can bring instant gratification to users by supporting type-ahead search, which finds answers "on the fly" as the user types in query keywords. A main challenge is how to achieve a high interactive speed for large amounts of data in multiple tables, so that a query can be answered efficiently within milliseconds. We propose efficient index structures and algorithms for finding relevant answers on-the-fly by joining tuples in the database. We devise a partition-based method to improve query performance by grouping highly relevant tuples and pruning irrelevant tuples efficiently. We also develop a technique to answer a query efficiently by predicting the highly relevant complete queries for the user. We have conducted a thorough experimental evaluation of the proposed techniques on real data sets to demonstrate the efficiency and practicality of this new search paradigm.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {695–706},
numpages = {12},
keywords = {type-ahead search, keyword search, query prediction},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559919,
author = {Chaudhuri, Surajit and Kaushik, Raghav},
title = {Extending Autocompletion to Tolerate Errors},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559919},
doi = {10.1145/1559845.1559919},
abstract = {Autocompletion is a useful feature when a user is doing a look up from a table of records. With every letter being typed, autocompletion displays strings that are present in the table containing as their prefix the search string typed so far. Just as there is a need for making the lookup operation tolerant to typing errors, we argue that autocompletion also needs to be error-tolerant. In this paper, we take a first step towards addressing this problem. We capture input typing errors via edit distance. We show that a naive approach of invoking an offline edit distance matching algorithm at each step performs poorly and present more efficient algorithms. Our empirical evaluation demonstrates the effectiveness of our algorithms.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {707–718},
numpages = {12},
keywords = {edit distance, autocompletion},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257467,
author = {Shanmugasundaram, Jai},
title = {Session Details: Research Session 19: Semi-Structured Data Management},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257467},
doi = {10.1145/3257467},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559921,
author = {Xu, Liang and Ling, Tok Wang and Wu, Huayu and Bao, Zhifeng},
title = {DDE: From Dewey to a Fully Dynamic XML Labeling Scheme},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559921},
doi = {10.1145/1559845.1559921},
abstract = {Labeling schemes lie at the core of query processing for many XML database management systems. Designing labeling schemes for dynamic XML documents is an important problem that has received a lot of research attention. Existing dynamic labeling schemes, however, often sacrifice query performance and introduce additional labeling cost to facilitate arbitrary updates even when the documents actually seldom get updated. Since the line between static and dynamic XML documents is often blurred in practice, we believe it is important to design a labeling scheme that is compact and efficient regardless of whether the documents are frequently updated or not. In this paper, we propose a novel labeling scheme called DDE (for Dynamic DEwey) which is tailored for both static and dynamic XML documents. For static documents, the labels of DDE are the same as those of dewey which yield compact size and high query performance. When updates take place, DDE can completely avoid re-labeling and its label quality is most resilient to the number and order of insertions compared to the existing approaches. In addition, we introduce Compact DDE (CDDE) which is designed to optimize the performance of DDE for insertions. Both DDE and CDDE can be incorporated into existing systems and applications that are based on dewey labeling scheme with minimum efforts. Experiment results demonstrate the benefits of our proposed labeling schemes over the previous approaches.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {719–730},
numpages = {12},
keywords = {update, labeling scheme, dynamic xml, dewey},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559922,
author = {Bex, Geert Jan and Gelade, Wouter and Martens, Wim and Neven, Frank},
title = {Simplifying XML Schema: Effortless Handling of Nondeterministic Regular Expressions},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559922},
doi = {10.1145/1559845.1559922},
abstract = {Whether beloved or despised, XML Schema is momentarily the only industrially accepted schema language for XML and is unlikely to become obsolete any time soon. Nevertheless, many nontransparent restrictions unnecessarily complicate the design of XSDs. For instance, complex content models in XML Schema are constrained by the infamous unique particle attribution (UPA) constraint. In formal language theoretic terms, this constraint restricts content models to deterministic regular expressions. As the latter constitute a semantic notion and no simple corresponding syntactical characterization is known, it is very difficult for non-expert users to understand exactly when and why content models do or do not violate UPA. In the present paper, we therefore investigate solutions to relieve users from the burden of UPA by automatically transforming nondeterministic expressions into concise deterministic ones defining the same language or constituting good approximations. The presented techniques facilitate XSD construction by reducing the design task at hand more towards the complexity of the modeling task. In addition, our algorithms can serve as a plug-in for any model management tool which supports export to XML Schema format.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {731–744},
numpages = {14},
keywords = {upa, deterministic regular expressions, xml schema},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559923,
author = {Koutrika, Georgia and Bercovitz, Benjamin and Garcia-Molina, Hector},
title = {FlexRecs: Expressing and Combining Flexible Recommendations},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559923},
doi = {10.1145/1559845.1559923},
abstract = {Recommendation systems have become very popular but most recommendation methods are `hard-wired' into the system making experimentation with and implementation of new recommendation paradigms cumbersome. In this paper, we propose FlexRecs, a framework that decouples the definition of a recommendation process from its execution and supports flexible recommendations over structured data. In FlexRecs, a recommendation approach can be defined declaratively as a high-level parameterized workflow comprising traditional relational operators and new operators that generate or combine recommendations. We describe a prototype flexible recommendation engine that realizes the proposed framework and we present example workflows and experimental results that show its potential for capturing multiple, existing or novel, recommendations easily and having a flexible recommendation system that combines extensibility with reasonable performance.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {745–758},
numpages = {14},
keywords = {recommendation queries, flexible recommendations, recommendation operators},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257468,
author = {Jagadish, H. V.},
title = {Session Details: Research Session 20: Data Management Pearls},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257468},
doi = {10.1145/3257468},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559925,
author = {Wang, Wei and Xiao, Chuan and Lin, Xuemin and Zhang, Chengqi},
title = {Efficient Approximate Entity Extraction with Edit Distance Constraints},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559925},
doi = {10.1145/1559845.1559925},
abstract = {Named entity recognition aims at extracting named entities from unstructured text. A recent trend of named entity recognition is finding approximate matches in the text with respect to a large dictionary of known entities, as the domain knowledge encoded in the dictionary helps to improve the extraction performance.In this paper, we study the problem of approximate dictionary matching with edit distance constraints. Compared to existing studies using token-based similarity constraints, our problem definition enables us to capture typographical or orthographical errors, both of which are common in entity extraction tasks yet may be missed by token-based similarity constraints. Our problem is technically challenging as existing approaches based on q-gram filtering have poor performance due to the existence of many short entities in the dictionary. Our proposed solution is based on an improved neighborhood generation method employing novel partitioning and prefix pruning techniques. We also propose an efficient document processing algorithm that minimizes unnecessary comparisons and enumerations and hence achieves good scalability. We have conducted extensive experiments on several publicly available named entity recognition datasets. The proposed algorithm outperforms alternative approaches by up to an order of magnitude.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {759–770},
numpages = {12},
keywords = {named entity recognition, approximate dictionary matching, edit distance},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559926,
author = {Gandhi, Sorabh and Nath, Suman and Suri, Subhash and Liu, Jie},
title = {GAMPS: Compressing Multi Sensor Data by Grouping and Amplitude Scaling},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559926},
doi = {10.1145/1559845.1559926},
abstract = {We consider the problem of collectively approximating a set of sensor signals using the least amount of space so that any individual signal can be efficiently reconstructed within a given maximum (L∞) error ε. The problem arises naturally in applications that need to collect large amounts of data from multiple concurrent sources, such as sensors, servers and network routers, and archive them over a long period of time for offline data mining. We present GAMPS, a general framework that addresses this problem by combining several novel techniques. First, it dynamically groups multiple signals together so that signals within each group are correlated and can be maximally compressed jointly. Second, it appropriately scales the amplitudes of different signals within a group and compresses them within the maximum allowed reconstruction error bound. Our schemes are polynomial time O(α, β approximation schemes, meaning that the maximum (L∞) error is at most α ε and it uses at most β times the optimal memory. Finally, GAMPS maintains an index so that various queries can be issued directly on compressed data. Our experiments on several real-world sensor datasets show that GAMPS significantly reduces space without compromising the quality of search and query.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {771–784},
numpages = {14},
keywords = {multi-sensor data compression, clustering},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559927,
author = {Sharaf, Mohamed A. and Chrysanthis, Panos K. and Labrinidis, Alexandros and Amza, Cristiana},
title = {Optimizing i/o-Intensive Transactions in Highly Interactive Applications},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559927},
doi = {10.1145/1559845.1559927},
abstract = {The performance provided by an interactive online database system is typically measured in terms of meeting certain pre-specified Service Level Agreements (SLAs), with expected transaction latency being the most commonly used type of SLA. This form of SLA acts as a soft deadline for each transaction, and user satisfaction can be measured in terms of minimizing tardiness, that is, the deviation from SLA. This objective is further complicated for I/O-intensive transactions, where the storage system becomes the performance bottleneck. Moreover, common I/O scheduling policies employed by the Operating System with a goal of improving I/O throughput or average latency may run counter to optimizing per-transaction performance since the Operating System is typically oblivious to the application high-level SLA specifications. In this paper, we propose a new SLA-aware policy for scheduling I/O requests of database transactions. Our proposed policy synergistically combines novel deadline-aware scheduling policies for database transactions with features of Operating System scheduling policies designed for improving I/O throughput. This enables our proposed policy to dynamically adapt to workload and consistently provide the best performance.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {785–798},
numpages = {14},
keywords = {i/o scheduling, transaction processing, database systems},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257469,
author = {Lin, Xuemin},
title = {Session Details: Research Session 21: Indexing},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257469},
doi = {10.1145/3257469},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559929,
author = {Beckmann, Norbert and Seeger, Bernhard},
title = {A Revised R*-Tree in Comparison with Related Index Structures},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559929},
doi = {10.1145/1559845.1559929},
abstract = {In this paper we present an improved redesign of the R*-tree that is entirely suitable for running within a DBMS. Most importantly, an insertion is guaranteed to be restricted to a single path because re-insertion could be abandoned. We re-engineered both, subtree choice and split algorithm, to be more robust against specific data distributions and insertion orders, as well as peculiarities often found in real multidimensional data sets. This comes along with a substantial reduction in CPU-time.Our experimental setup covers a wide range of different artificial and real data sets. The experimental comparison shows that the search performance of our revised R*-tree is superior to that of its three most important competitors. In comparison to its predecessor, the original R*-tree, the creation of a tree is substantially faster, while the I/O cost required for processing queries is improved by more than 30% on average for two- and three-dimensional data. For higher dimensional data, particularly for real data sets, much larger improvements are achieved.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {799–812},
numpages = {14},
keywords = {rr*-tree, multi-dimensional data, hilbert-r-tree, revised r*-tree, performance comparison, choosesubtree, split, r-tree, index structures},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559930,
author = {Jin, Ruoming and Xiang, Yang and Ruan, Ning and Fuhry, David},
title = {3-HOP: A High-Compression Indexing Scheme for Reachability Query},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559930},
doi = {10.1145/1559845.1559930},
abstract = {Reachability queries on large directed graphs have attracted much attention recently. The existing work either uses spanning structures, such as chains or trees, to compress the complete transitive closure, or utilizes the 2-hop strategy to describe the reachability. Almost all of these approaches work well for very sparse graphs. However, the challenging problem is that as the ratio of the number of edges to the number of vertices increases, the size of the compressed transitive closure grows very large. In this paper, we propose a new 3-hop indexing scheme for directed graphs with higher density. The basic idea of 3-hop indexing is to use chain structures in combination with hops to minimize the number of structures that must be indexed. Technically, our goal is to find a 3-hop scheme over dense DAGs (directed acyclic graphs) with minimum index size. We develop an efficient algorithm to discover a transitive closure contour, which yields near optimal index size. Empirical studies show that our 3-hop scheme has much smaller index size than state-of-the-art reachability query schemes such as 2-hop and path-tree when DAGs are not very sparse, while our query time is close to path-tree, which is considered to be one of the best reachability query schemes.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {813–826},
numpages = {14},
keywords = {reachability queries, graph indexing, transitive closure, path-tree, 2-hop, 3-hop},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559931,
author = {Ghoting, Amol and Makarychev, Konstantin},
title = {Serial and Parallel Methods for i/o Efficient Suffix Tree Construction},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559931},
doi = {10.1145/1559845.1559931},
abstract = {Over the past three decades, the suffix tree has served as a fundamental data structure in string processing. However, its widespread applicability has been hindered due to the fact that suffix tree construction does not scale well with the size of the input string. With advances in data collection and storage technologies, large strings have become ubiquitous, especially across emerging applications involving text, time series, and biological sequence data. To benefit from these advances, it is imperative that we realize a scalable suffix tree construction algorithm.To deal with the aforementioned challenge, the past few years have seen the emergence of several disk-based suffix tree construction algorithms. However, construction times continue to be daunting -- for e.g., indexing the entire Human genome still takes over 30 hours on a system with 2 gigabytes of physical memory. In this paper, first, we empirically demonstrate and argue that all existing suffix tree construction algorithms have a severe limitation -- to glean reasonable disk I/O efficiency, the input string being indexed must fit in main memory. This limitation is attributed to the poor locality properties of existing suffix tree construction algorithms and inhibits both sequential and parallel scalability. To deal with this limitation, second, we show that through careful algorithm design, one of the simplest suffix tree construction algorithms can be re-architected to build a suffix tree in a tiled fashion, allowing the implementation to maintain a constant working set size and fixed memory footprint when indexing strings of any size. Third, we show how improved locality of reference coupled with effective collective communication facilitates an efficient parallelization on massively parallel systems like the IBM Blue Gene/L. Finally, we empirically show that the proposed approach affords improvements of several orders of magnitude when indexing large strings. Furthermore, we demonstrate that the proposed parallelization is scalable and allows one to index the entire Human genome on a 1024 processor system in under 15 minutes.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {827–840},
numpages = {14},
keywords = {external memory, parallel, genome indexing, disk-based, suffix tree, sequence indexing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257470,
author = {Lehner, Wolfgang},
title = {Session Details: Industrial Session 1: Data Warehousing},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257470},
doi = {10.1145/3257470},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559933,
author = {undefinedlezak, Dominik and Eastwood, Victoria},
title = {Data Warehouse Technology by Infobright},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559933},
doi = {10.1145/1559845.1559933},
abstract = {We discuss Infobright technology with respect to its main features and architectural differentiators. We introduce the upcoming research and development projects that may be of special interest to the academic and industry communities.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {841–846},
numpages = {6},
keywords = {data warehouse, mysql, database knowledge grid, iee, approximate query, ice, column store, data compression, open source, data clustering, two-level computing, rough sets},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559934,
author = {Golab, Lukasz and Johnson, Theodore and Seidel, J. Spencer and Shkapenyuk, Vladislav},
title = {Stream Warehousing with DataDepot},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559934},
doi = {10.1145/1559845.1559934},
abstract = {We describe DataDepot, a tool for generating warehouses from streaming data feeds, such as network-traffic traces, router alerts, financial tickers, transaction logs, and so on. DataDepot is a streaming data warehouse designed to automate the ingestion of streaming data from a wide variety of sources and to maintain complex materialized views over these sources. As a streaming warehouse, DataDepot is similar to Data Stream Management Systems (DSMSs) with its emphasis on temporal data, best-effort consistency, and real-time response. However, as a data warehouse, DataDepot is designed to store tens to hundreds of terabytes of historical data, allow time windows measured in years or decades, and allow both real-time queries on recent data and deep analyses on historical data. In this paper we discuss the DataDepot architecture, with an emphasis on several of its novel and critical features. DataDepot is currently being used for five very large warehousing projects within AT&amp;T; one of these warehouses ingests 500 Mbytes per minute (and is growing). We use these installations to illustrate streaming warehouse use and behavior, and design choices made in developing DataDepot. We conclude with a discussion of DataDepot applications and the efficacy of some optimizations.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {847–854},
numpages = {8},
keywords = {real-time data warehousing, data stream warehousing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559935,
author = {Ahuja, Mona and Chen, Cheng Che and Gottapu, Ravi and Hallmann, J\"{o}rg and Hasan, Waqar and Johnson, Richard and Kozyrczak, Maciek and Pabbati, Ramesh and Pandit, Neeta and Pokuri, Sreenivasulu and Uppala, Krishna},
title = {Peta-Scale Data Warehousing at Yahoo!},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559935},
doi = {10.1145/1559845.1559935},
abstract = {Insights based on detailed data on consumer behavior, product performance and marketplace behavior are driving innovation and competition in the internet space. We introduce Everest, a SQL-compliant data warehousing engine, based on a column architecture that we have built and deployed at Yahoo!. In contrast to commercially available engines, this massively parallel engine, based on commodity hardware, offers scale, flexibility, specialized analytic operations, and lower administrative &amp; hardware costs. In this paper, we describe the business motivation and the software and deployment architecture of Everest. The engine is in production at Yahoo! since 2007 and currently manages over six petabytes of data.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {855–862},
numpages = {8},
keywords = {analytics, data warehousing, vector query processing, column database, column storage, business intelligence, mpp database},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257471,
author = {Balazinska, Magda},
title = {Session Details: Industrial Session 2: Exploiting New Hardware},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257471},
doi = {10.1145/3257471},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559937,
author = {Lee, Sang-Won and Moon, Bongki and Park, Chanik},
title = {Advances in Flash Memory SSD Technology for Enterprise Database Applications},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559937},
doi = {10.1145/1559845.1559937},
abstract = {The past few decades have witnessed a chronic and widening imbalance among processor bandwidth, disk capacity, and access speed of disk. According to Amdhal's law, the performance enhancement possible with a given improvement is limited by the amount that the improved feature is used. This implies that the performance enhancement of an OLTP system would be seriously limited without a considerable improvement in I/O throughput. Since the market debut of flash memory SSD a few years ago, we have made a continued effort to overcome its poor random write performance and to provide stable and sufficient I/O bandwidth. In this paper, we present three different flash memory SSD models prototyped recently by Samsung Electronics. We then show how the flash memory SSD technology has advanced to reverse the widening trend of performance gap between processors and storage devices. We also demonstrate that even a single flash memory drive can outperform a level-0 RAID with eight enterprise class 15k-RPM disk drives with respect to transaction throughput, cost effectiveness and energy consumption.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {863–870},
numpages = {8},
keywords = {flash-memory ssd, tpc-c benchmark, energy},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559938,
author = {Waas, Florian M. and Hellerstein, Joseph M.},
title = {Parallelizing Extensible Query Optimizers},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559938},
doi = {10.1145/1559845.1559938},
abstract = {Query optimization is the most computationally complex task in a database management systems. In many query optimizers, faster CPUs and increased RAM can translate directly to better query plans and thus better overall system performance. Although memory size continues to scale with Moore's Law, processor speeds are leveling off. Chip manufacturers are now focusing on multicore designs that integrate increasing numbers of cores in a single CPU. Query optimizers need to be parallelized in order to continue enjoying the growth trend of Moore's Law.In this paper, we address this problem in the context of the extensible optimizer architectures found in many commercial database systems. We identify the key data dependencies inherent in the dynamic programming at the heart of these optimizers. We use this insight both to design a flexible parallel query optimization implementation, and to assess the opportunities for parallelism in this context.The proposed solutions can serve as a blueprint for retrofitting existing industry-grade optimizers to leverage multicore architectures, without requiring significant rework of the underlying infrastructure.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {871–878},
numpages = {8},
keywords = {multicore, erlang, depedencies, query optimization, parallel processing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559939,
author = {Krishnamurthy, Ravi},
title = {A Data Warehouse Appliance for the Mass Market},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559939},
doi = {10.1145/1559845.1559939},
abstract = {Vast majority of the data warehouses have less than few terabytes of data and their performance for complex queries on traditional database systems are often not very satisfactory. Data warehouse appliances have been announced by vendors (HP Oracle Exadata Storage server, HP Neoview, Neteeza etc.) to address this burgeoning need. Most of these involve creating a large parallel database systems using scale-out of commodity machines and/or pushing filters into disk retrieval system to reduce the data coming to memory; these done along the lines pioneered by research projects such as Gamma, Bubba and other prior database machine research. These approaches deliver performance by deploying many CPUs, large amount of memory, large number of disk-heads &amp; disk space and in effect extracting performance by under utilizing the resources -- albeit very inexpensive commodity resources.In contrast we propose a database system in a box (i.e., a single system) that can deliver high performance for complex queries while utilizing much less resources (memory, disks etc.); i.e., better resource utilization and therefore lower cost. This approach consists of using column store (pioneered in the Bubba project) which has the effect of 1) reducing the need for large number of disk heads (i.e., I/O bandwidth); and 2) reducing the need for large amount of memory for achieving memory-resident query execution. Having mitigated the disk I/O problem using column store &amp; memory, the Von Neumann bottleneck becomes the force majeure. This problem has been pursued by database researchers in the context of cache-conscious query execution. Unfortunately, traditional CPUs provide limited control to "page" the data into the cache and retain it there to leverage the cache effectively.Our approach is to leverage a custom dataflow machine that can be coupled with a large memory and thereby practically eliminating the Von Neumann bottleneck. Besides mitigating this bottleneck, the exploitation of fine-grained pipelined and operator parallelism in hardware provides significant performance improvement. This results in a low-cost high-performance database appliance for vast majority of the data warehouse market. Kickfire has shown that such an appliance can deliver both price/performance and raw performance as compared to the competitive approaches. Note that this high performance appliance does not preclude leveraging scale-out; i.e., it can itself be used to scale-out to a much larger database in the future.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {879–880},
numpages = {2},
keywords = {appliance, data warehouse},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257472,
author = {Cooper, Brian},
title = {Session Details: Industrial Session 3: Data Services},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257472},
doi = {10.1145/3257472},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559941,
author = {Aulbach, Stefan and Jacobs, Dean and Kemper, Alfons and Seibold, Michael},
title = {A Comparison of Flexible Schemas for Software as a Service},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559941},
doi = {10.1145/1559845.1559941},
abstract = {A multi-tenant database system for Software as a Service (SaaS) should offer schemas that are flexible in that they can be extended different versions of the application and dynamically modified while the system is on-line. This paper presents an experimental comparison of five techniques for implementing flexible schemas for SaaS. In three of these techniques, the database "owns" the schema in that its structure is explicitly defined in DDL. Included here is the commonly-used mapping where each tenant is given their own private tables, which we take as the baseline, and a mapping that employs Sparse Columns in Microsoft SQL Server. These techniques perform well, however they offer only limited support for schema evolution in the presence of existing data. Moreover they do not scale beyond a certain level. In the other two techniques, the application "owns" the schema in that it is mapped into generic structures in the database. Included here are XML in DB2 and Pivot Tables in HBase. These techniques give the application complete control over schema evolution, however they can produce a significant decrease in performance. We conclude that the ideal database for SaaS has not yet been developed and offer some suggestions as to how it should be designed.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {881–888},
numpages = {8},
keywords = {multi-tenancy, extensibility, software as a service, flexible schemas, evolution},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559942,
author = {Weissman, Craig D. and Bobrowski, Steve},
title = {The Design of the Force.Com Multitenant Internet Application Development Platform},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559942},
doi = {10.1145/1559845.1559942},
abstract = {Force.com is the preeminent on-demand application development platform in use today, supporting some 55,000+ organizations. Individual enterprises and commercial software-as-a-service (SaaS) vendors trust the platform to deliver robust, reliable, Internet-scale applications. To meet the extreme demands of its large user population, Force.com's foundation is a metadatadriven software architecture that enables multitenant applications.The focus of this paper is multitenancy, a fundamental design approach that can dramatically improve SaaS application management. This paper defines multitenancy, explains its benefits, and demonstrates why metadata-driven architectures are the premier choice for implementing multitenancy.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {889–896},
numpages = {8},
keywords = {object-relational mapping, multi-tenancy, flex schema, domain specific language, query optimization},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559943,
author = {Terlecki, Pawel and Bati, Hardik and Galindo-Legaria, Cesar and Zabback, Peter},
title = {Filtered Statistics},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559943},
doi = {10.1145/1559845.1559943},
abstract = {Column statistics are an important element of cardinality estimation frameworks. More accurate estimates allow the optimizer of a RDBMS to generate better plans and improve the overall system's efficiency. This paper introduces filtered statistics, which model value distribution over a set of rows restricted by a predicate. This feature, available in Microsoft SQL Server, can be used to handle column correlation, as well as focus on interesting data ranges. In particular, it fits well for scenarios with logical subtables, like flexible schema or multi-tenant applications. Integration with the existing cardinality estimation infrastructure is presented.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {897–904},
numpages = {8},
keywords = {cardinality estimation, multi-tenant application, conditional data distribution, flexible schema},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257473,
author = {Markl, Volker},
title = {Session Details: Industrial Session 4: Advances in Query Optimization},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257473},
doi = {10.1145/3257473},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559945,
author = {Chen, Yijou and Cole, Richard L. and McKenna, William J. and Perfilov, Sergei and Sinha, Aman and Szedenits, Eugene},
title = {Partial Join Order Optimization in the Paraccel Analytic Database},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559945},
doi = {10.1145/1559845.1559945},
abstract = {The ParAccel Analytic Database is a fast shared-nothing parallel relational database system with a columnar orientation, adaptive compression, memory-centric design, and an enhanced query optimizer. This modern object-oriented optimizer and its optimizer framework, known as Volt, provide efficient bulk and instance level query expression representation, multiple expression managers, and rule and cost-based expression transformation organized via multiple optimizer instances. Volt has been applied to the problem of ordering very large numbers of joins by partially ordering them for subsequent optimization using standard dynamic programming. Performance analyses show the framework's utility and the optimizer's effectiveness.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {905–908},
numpages = {4},
keywords = {query optimization, extensible optimizer framework},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559946,
author = {Ghazal, Ahmad and Seid, Dawit and Ramesh, Bhashyam and Crolotte, Alain and Koppuravuri, Manjula and G, Vinod},
title = {Dynamic Plan Generation for Parameterized Queries},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559946},
doi = {10.1145/1559845.1559946},
abstract = {Query processing in a DBMS typically involves two distinct phases: compilation, which generates the best plan and its corresponding execution steps, and execution, which evaluates these steps against database objects. For some queries, considerable resource savings can be achieved by skipping the compilation phase when the same query was previously submitted and its plan was already cached. In a number of important applications the same query, called a Parameterized Query (PQ), is repeatedly submitted in the same basic form but with different parameter values. PQ's are extensively used in both data update (e.g. batch update programs) and data access queries. There are tradeoffs associated with caching and re-using query plans such as space utilization and maintenance cost. Besides, pre-compiled plans may be suboptimal for a particular execution due to various reasons including data skew and inability to exploit value-based query transformation like materialized view rewrite and unsatisfiable predicate elimination. We address these tradeoffs by distinguishing two types of plans for PQ's: generic and specific plans. Generic plans are pre-compiled plans that are independent of the actual parameter values. Prior to execution, parameter values are plugged in to generic plans. In specific plans, parameter values are plugged prior to the compilation phase. This paper provides a practical framework for dynamically deciding between specific and generic plans for PQ's based on a mix of rule and cost based heuristics which are implemented in the Teradata 12.0 DBMS.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {909–916},
numpages = {8},
keywords = {compilation, optimizations, dynamic},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559947,
author = {Andrei, Mihnea and Cheng, Xun and Chowdhuri, Sudipto and Johnson, Curtis and Seputis, Edwin},
title = {Ordering, Distinctness, Aggregation, Partitioning and DQP Optimization in Sybase ASE 15},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559947},
doi = {10.1145/1559845.1559947},
abstract = {The Sybase ASE RDBMS version 15 was subject to major enhancements, including semantic partitions and a full QP rewrite. The new ASE QP supports horizontal and vertical parallel processing over semantically partitioned tables, and many other modern QP techniques, as cost-based eager aggregation and cost-based join relocation DQP. In the new query optimizer, the ordering, distinctness, aggregation, partitioning, and DQP optimizations were based on a common framework: plan fragment equivalence classes and logical properties. Our main outcomes are a) an eager enforcement policy for ordering, partitioning and DQP location; b) a distinctness and aggregation optimization policy, opportunistically based on the eager ordering enforcement, and which has an optimization-time computational complexity similar to join processing; c) support for the user to force all of the above optimizer decisions, still guaranteeing a valid plan, based on the Abstract Plan technology. We describe the implementation of this solution in the ASE 15 optimizer. Finally, we give our experimental results: the generation of such complex plans comes with a small increase of the optimizer's SS size, hence within an acceptable optimization time; at execution, we have obtained performance improvements of orders of magnitude for some queries.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {917–924},
numpages = {8},
keywords = {search space, physical operators, logical properties, ordering, search engine, logical operators, eager aggregation, distinctness, optimization, lazy aggregation, operator model, grouping, aggregation, abstract plans, property enforcement, physical properties},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257474,
author = {Kemme, Bettina},
title = {Session Details: Industrial Session 5: Transactions, Security, and Cashing},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257474},
doi = {10.1145/3257474},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559949,
author = {Ghandeharizadeh, Shahram and Goodney, Andrew and Sharma, Chetan and Bissell, Chris and Carino, Felipe and Nannapaneni, Naveen and Wergeles, Alex and Whitcomb, Aber},
title = {Taming the Storage Dragon: The Adventures of HoTMaN},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559949},
doi = {10.1145/1559845.1559949},
abstract = {HoTMaN (HoT-standby MaNager) is a joint project between MySpace and USC Database Laboratory to design and develop a tool to ensure a 24x7 up-time and ease administration of Terabytes of storage that sits underneath hundreds of database servers. The HoTMaN tool's innovation and uniqueness is that it can, with a few clicks, perform operational tasks that require hundreds of keyboard strokes by "trusted trained" experts. With HoTMaN, MySpace can within minutes migrate the relational database(s) of a failed server to a hot-standby. A process that could take over 1 hour and had a high potential for human error is now performed reliably. A database internal to HoTMaN captures all virtual disks, volume and file configurations associated with each SQL Server and candidate hot-standby servers where SQL server processing could be migrated. HoTMaN is deployed in production and its current operational benefits include: (i) enhanced availability of data, and (ii) planned maintenance and patching.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {925–930},
numpages = {6},
keywords = {storage area networks, data availability},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559950,
author = {Yalamanchi, Aravind and Gawlick, Dieter},
title = {Compensation-Aware Data Types in RDBMS},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559950},
doi = {10.1145/1559845.1559950},
abstract = {In a traditional database system, the transaction management protocols and mechanisms are constrained by the fundamental properties of atomicity, consistency, isolation, and durability (ACID). A transaction management system with strict ACID properties typically employs read and write locks, held for the duration of the transaction, to protect its uncommitted data from being seen and modified by some other transaction. While this approach is effective for applications involving short execution times and relatively small number of concurrent operations, it is too restrictive for applications that involve reactive, long-lived, and complex transactions. The common denominator of such applications is the need for transactions to read and possibly modify uncommitted data values [1] and for the database system to still retain the ability to abort a transaction and the ability to recover from failures. This paper proposes a Business Transaction framework that allows long lasting, discontinuous, and resumable transactions to perform shared updates to common data by holding semantic locks on the modified rows. Under this framework, basic SQL data types are made compensation-aware by associating domain-specific shared update semantics with them. These semantics ensure that each data modification operation is compatible with other uncommitted activity on the same data and that the operation can be undone, if needed, without resorting to cascading aborts. This paper describes the key concepts and presents our approach for supporting shared updates in Oracle RDBMS.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {931–938},
numpages = {8},
keywords = {and constraints, business transactions, concurrency control, compensation},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559951,
author = {Borkar, Vinayak and Carey, Michael and Engovatov, Daniel and Lychagin, Dmitry and Reveliotis, Panagiotis and Spiegel, Joshua and Thatte, Sachin and Westmann, Till},
title = {Access Control in the Aqualogic Data Services Platform},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559951},
doi = {10.1145/1559845.1559951},
abstract = {The AquaLogic Data Services Platform (ALDSP) is a middleware platform for building data services that integrate and provide operations over data drawn from spanning multiple heterogeneous information sources. A data service consists of an XML Schema instance, describing its information content, and a collection of XQuery functions and procedures that comprise its set of operations. This paper describes access control in ALDSP. We describe ALDSP's securable resource hierarchy, its fine-grained access control capabilities for securing portions of data service schemas, how XQuery can be used to specify data-driven security policies, and how user identity mapping is supported. We then provide an in-depth overview of how ALDSP works, including implementation techniques to keep access control checking from interacting badly with view rewriting, query optimization, and caching.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {939–946},
numpages = {8},
keywords = {service-oriented architecture, xquery, information integration, access control, xml, data services},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257475,
author = {Shah, Mehul},
title = {Session Details: Industrial Session 6: Industrial Directions},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257475},
doi = {10.1145/3257475},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559953,
author = {Amer-Yahia, Sihem and Huang, Jian and Yu, Cong},
title = {Building Community-Centric Information Exploration Applications on Social Content Sites},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559953},
doi = {10.1145/1559845.1559953},
abstract = {Social content sites [4], which integrate traditional content sites with social networking features, have recently emerged as an exciting new trend on the Web. Users on those sites share content and form various communities based on explicit friendship, shared interest and common user properties. Recently, we proposed SOCIALSCOPE, a three-layered architecture to address the information management challenges in social content sites. In this paper, we focus on the information discovery and the information presentation layers, and describe how our previously proposed language, Jelly [3], is supported in SOCIALSCOPE to build community-centric information exploration applications on social content sites.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {947–952},
numpages = {6},
keywords = {recommender systems, search, declarative languages, social content sites},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559954,
author = {Simitsis, Alkis and Wilkinson, Kevin and Castellanos, Malu and Dayal, Umeshwar},
title = {QoX-Driven ETL Design: Reducing the Cost of ETL Consulting Engagements},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559954},
doi = {10.1145/1559845.1559954},
abstract = {As business intelligence becomes increasingly essential for organizations and as it evolves from strategic to operational, the complexity of Extract-Transform-Load (ETL) processes grows. In consequence, ETL engagements have become very time consuming, labor intensive, and costly. At the same time, additional requirements besides functionality and performance need to be considered in the design of ETL processes. In particular, the design quality needs to be determined by an intricate combination of different metrics like reliability, maintenance, scalability, and others. Unfortunately, there are no methodologies, modeling languages or tools to support ETL design in a systematic, formal way for achieving these quality requirements. The current practice handles them with ad-hoc approaches only based on designers' experience. This results in either poor designs that do not meet the quality objectives or costly engagements that require several iterations to meet them. A fundamental shift that uses automation in the ETL design task is the only way to reduce the cost of these engagements while obtaining optimal designs. Towards this goal, we present a novel approach to ETL design that incorporates a suite of quality metrics, termed QoX, at all stages of the design process. We discuss the challenges and tradeoffs among QoX metrics and illustrate their impact on alternative designs.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {953–960},
numpages = {8},
keywords = {qox, quality, modeling, metrics, data warehouses, etl},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559955,
author = {Chaudhuri, Surajit},
title = {Query Optimizers: Time to Rethink the Contract?},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559955},
doi = {10.1145/1559845.1559955},
abstract = {Query Optimization is expected to produce good execution plans for complex queries while taking relatively small optimization time. Moreover, it is expected to pick the execution plans with rather limited knowledge of data and without any additional input from the application. We argue that it is worth rethinking this prevalent model of the optimizer. Specifically, we discuss how the optimizer may benefit from leveraging rich usage data and from application input. We conclude with a call to action to further advance query optimization technology.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {961–968},
numpages = {8},
keywords = {query optimizer, cardinality estimation},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257476,
author = {Pierce, Jeffrey},
title = {Session Details: Special Invited Session on Human-Computer Interaction with Information},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257476},
doi = {10.1145/3257476},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559957,
author = {Tunkelang, Daniel},
title = {Design for Interaction},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559957},
doi = {10.1145/1559845.1559957},
abstract = {Research in information retrieval has focused on presenting the most relevant results to a user in response to a free-text search query. Research in database systems assumes a model where the user enters a formal query, and the results are exactly those the user requested. Neither community has emphasized user interaction—a critical concern for practical information access.As William Goffman noted in the 1960s and Nick Belkin continually reminds us today, the relationship between a document and query, though necessary, is not sufficient to determine relevance—yet ranked retrieval approaches rely heavily or exclusively on this relationship. Meanwhile, recent work on database usability by Jeff Naughton and H.V. Jagadish surfaces the rigidity of database systems that return nothing unless users know how to formulate precise queries.This talk presents human-computer information retrieval (HCIR) as a general approach that addresses some of the key challenges facing both research communities. A vision first put forward by Gary Marchionini, HCIR expects people and systems to work together to implement information access. Such an approach requires rethinking information access not as a matching or ranking problem, but rather as a communication problem. Specifically, we need interfaces that optimize the bidirectional communication between the user and the system, thus optimizing the symbiotic division of labor between the two.This talk reviews the history of HCIR efforts and presents ongoing work to implement the HCIR vision. In particular, it presents an interactive set retrieval approach that responds to queries with an overview of the user's current context and an organized set of options for incremental exploration.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {969–970},
numpages = {2},
keywords = {information retrieval, human computer information retrieval, information access},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559958,
author = {Heer, Jeffrey},
title = {Voyagers and Voyeurs: Supporting Social Data Analysis},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559958},
doi = {10.1145/1559845.1559958},
abstract = {In recent years, researchers and entrepreneurs have introduced new online services for data collection and analysis, often relying on interactive visualizations to enable mass interaction with data. These sites represent the first step in what looks to be a growing online phenomenon: social data analysis, that is, collective analysis of data supported by social interaction. Engaging crowds of both experts and non-experts in the process of data exploration has applications ranging from political transparency to business intelligence to citizen science. Achieving this vision, however, will require further innovation in the design of systems for collaborative data management.In this talk, I will highlight recent efforts to use the web as a platform for collectively creating, managing, and analyzing data. I will share how web citizens are collaborating with data, and discuss how we might design systems to catalyze social forms of data management and exploration.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {971–972},
numpages = {2},
keywords = {collaboration, data management, social computing, social data analysis, visualization},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559959,
author = {Chi, Ed H.},
title = {Augmented Social Cognition: Using Social Web Technology to Enhance the Ability of Groups to Remember, Think, and Reason},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559959},
doi = {10.1145/1559845.1559959},
abstract = {We are experiencing a new Social Web, where people share, communicate, commiserate, and conflict with each other. As evidenced by systems like Wikipedia, twitter, and delicious.com, these environments are turning people into social information foragers and sharers. Groups interact to resolve conflicts and jointly make sense of topic areas from "Obama vs. Clinton" to "Islam."PARC's Augmented Social Cognition researchers -- who come from cognitive psychology, computer science, HCI, CSCW, and other disciplines -- focus on understanding how to "enhance a group of people's ability to remember, think, and reason". Through Social Web systems like social bookmarking sites, blogs, Wikis, and more, we can finally study, in detail, these types of enhancements on a very large scale.Here we summarize recent work and early findings such as: (1) how conflict and coordination have played out in Wikipedia, and how social transparency might affect reader trust; (2) how decreasing interaction costs might change participation in social tagging systems; and (3) how computation can help organize user-generated content and metadata.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {973–984},
numpages = {12},
keywords = {cscw, augmented social cognition, wikipedia, social system, hci, social web, overview, characterization, research methods, delicious, summary, modeling, social tagging},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/3257477,
author = {Carey, Michael},
title = {Session Details: Special Invited Session on Systems Research and Information Management},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3257477},
doi = {10.1145/3257477},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
numpages = {1},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559961,
author = {Freitas, Richard F.},
title = {Storage Class Memory: Technology, Systems and Applications},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559961},
doi = {10.1145/1559845.1559961},
abstract = {Storage Class Memory (SCM) is the term used to describe a new class of solid-state, nonvolatile memory technologies. There are several (≥10) such technologies currently under active research and development that are vying to become the SCM of choice. As a group, their performance is nearer memory than storage while their cost is nearer storage than memory. In the systems designer's eye, these performance and cost characteristics blur the historic distinction between memory and storage. Exploiting SCM will require significant changes in the design of memory and storage systems. These changes at the system level will force or enable changes at the application level as well as opening up new application opportunities.In this talk we will discuss several of today's leading SCM technologies, discuss their potential impact on the design of memory and storage systems, consider the changes that might be needed to existing applications and explore some of the potential opportunities for future applications.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {985–986},
numpages = {2},
keywords = {storage class memory, phase change memory, nonvolatile memory},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559962,
author = {Isard, Michael and Yu, Yuan},
title = {Distributed Data-Parallel Computing Using a High-Level Programming Language},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559962},
doi = {10.1145/1559845.1559962},
abstract = {The Dryad and DryadLINQ systems offer a new programming model for large scale data-parallel computing. They generalize previous execution environments such as SQL and MapReduce in three ways: by providing a general-purpose distributed execution engine for data-parallel applications; by adopting an expressive data model of strongly typed .NET objects; and by supporting general-purpose imperative and declarative operations on datasets within a traditional high-level programming language.A DryadLINQ program is a sequential program composed of LINQ expressions performing arbitrary side-effect-free operations on datasets, and can be written and debugged using standard .NET development tools. The DryadLINQ system automatically and transparently translates the data-parallel portions of the program into a distributed execution plan which is passed to the Dryad execution platform. Dryad, which has been in continuous operation for several years on production clusters made up of thousands of computers, ensures efficient, reliable execution of this plan on a large compute cluster.This paper describes the programming model, provides a high-level overview of the design and implementation of the Dryad and DryadLINQ systems, and discusses the tradeoffs and connections to parallel and distributed databases.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {987–994},
numpages = {8},
keywords = {distributed programming, cloud computing, concurrency},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559964,
author = {Babu, Shivnath and Guha, Sudipto and Munagala, Kamesh},
title = {Large-Scale Uncertainty Management Systems: Learning and Exploiting Your Data},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559964},
doi = {10.1145/1559845.1559964},
abstract = {The database community has made rapid strides in capturing, representing, and querying uncertain data. Probabilistic databases capture the inherent uncertainty in derived tuples as probability estimates. Data acquisition and stream systems can produce succinct summaries of very large and time-varying datasets. This tutorial addresses the natural next step in harnessing uncertain data: How can we efficiently and quantifiably determine what, how, and how much to learn in order to make good decisions based on the imprecise information available.The material in this tutorial is drawn from a range of fields including database systems, control and information theory, operations research, convex optimization, and statistical learning. The focus of the tutorial is on the natural constraints that are imposed in a database context and the demands of imprecise information from an optimization point of view. We look both into the past as well as into the future; to discuss general tools and techniques that can serve as a guide to database researchers and practitioners, and to enumerate the challenges that lie ahead.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {995–998},
numpages = {4},
keywords = {measurement, performance tuning, algorithms},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559965,
author = {Mueller, Rene and Teubner, Jens},
title = {FPGA: What's in It for a Database?},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559965},
doi = {10.1145/1559845.1559965},
abstract = {While there seems to be a general agreement that next years' systems will include many processing cores, it is often overlooked that these systems will also include an increasing number of different cores (we already see dedicated units for graphics or network processing). Orchestrating the diversity of processing functionality is going to be a major challenge in the upcoming years, be it to optimize for performance or for minimal energy consumption.We expect field-programmable gate arrays (FPGAs or "programmable hardware") to soon play the role of yet another processing unit, found in commodity computers. It is clear that the new resource is going to be too precious to be ignored by database systems, but it is unclear how FPGAs could be integrated into a DBMS. With a focus on database use, this tutorial introduces into the emerging technology, demonstrates its potential, but also pinpoints some challenges that need to be addressed before FPGA-accelerated database systems can go mainstream. Attendees will gain an intuition of an FPGA development cycle, receive guidelines for a "good" FPGA design, but also learn the limitations that hardware-implemented database processing faces. Our more high-level ambition is to spur a broader interest in database processing on novel hardware technology.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {999–1004},
numpages = {6},
keywords = {fpga, data processing vlsi, hardware acceleration},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559966,
author = {Chen, Yi and Wang, Wei and Liu, Ziyang and Lin, Xuemin},
title = {Keyword Search on Structured and Semi-Structured Data},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559966},
doi = {10.1145/1559845.1559966},
abstract = {Empowering users to access databases using simple keywords can relieve the users from the steep learning curve of mastering a structured query language and understanding complex and possibly fast evolving data schemas. In this tutorial, we give an overview of the state-of-the-art techniques for supporting keyword search on structured and semi-structured data, including query result definition, ranking functions, result generation and top-k query processing, snippet generation, result clustering, query cleaning, performance optimization, and search quality evaluation. Various data models will be discussed, including relational data, XML data, graph-structured data, data streams, and workflows. We also discuss applications that are built upon keyword search, such as keyword based database selection, query generation, and analytical processing. Finally we identify the challenges and opportunities of future research to advance the field.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1005–1010},
numpages = {6},
keywords = {databases, top-k, keyword search, xml},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559967,
author = {Demers, Alan and Gehrke, Johannes and Koch, Christoph and Sowell, Ben and White, Walker},
title = {Database Research in Computer Games},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559967},
doi = {10.1145/1559845.1559967},
abstract = {This tutorial presents an overview of the data management issues faced by computer games today. While many games do not use databases directly, they still have to process large amounts of data, and could benefit from the application of database technology. Other games, such as massively multiplayer online games (MMOs), must communicate with commercial databases and have their own unique challenges. In this tutorial we will present the state-of-the-art of data management in games that we learned from our interaction with various game studios. We will show how the issues involved motivate current research, and illustrate several possibilities for future work.Our tutorial will start with a description of data-driven design, which is the source of many of the data management issues that games face. We will show some of the tools that game developers use to create and manage content. We will discuss how this type of design can affect performance, and the data structures and techniques that developers use to ensure that the game is responsive. We will discuss the problem of consistency in games, and how games ensure that players all share the same view of the world. Finally, we will examine some of the engineering issues that game developers have to deal with when interacting with traditional databases. This tutorial is intended to be self-contained, and provides the background necessary for understanding how databases and database technology are relevant to computer games.This tutorial is accessible to students and researchers who, while perhaps not hardcore gamers themselves, are interested in ways in which they can use their expertise to solve problems in computer games.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1011–1014},
numpages = {4},
keywords = {scripting, games, aggregates, indexing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559968,
author = {Cormode, Graham and Srivastava, Divesh},
title = {Anonymized Data: Generation, Models, Usage},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559968},
doi = {10.1145/1559845.1559968},
abstract = {Data anonymization techniques have been the subject of intense investigation in recent years, for many kinds of structured data, including tabular, graph and item set data. They enable publication of detailed information, which permits ad hoc queries and analyses, while guaranteeing the privacy of sensitive information in the data against a variety of attacks. In this tutorial, we aim to present a unified framework of data anonymization techniques, viewed through the lens of uncertainty. Essentially, anonymized data describes a set of possible worlds, one of which corresponds to the original data. We show that anonymization approaches such as suppression, generalization, perturbation and permutation generate different working models of uncertain data, some of which have been well studied, while others open new directions for research. We demonstrate that the privacy guarantees offered by methods such as k-anonymization and l-diversity can be naturally understood in terms of similarities and differences in the sets of possible worlds that correspond to the anonymized data. We describe how the body of work in query evaluation over uncertain databases can be used for answering ad hoc queries over anonymized data in a principled manner. A key benefit of the unified approach is the identification of a rich set of new problems for both the Data Anonymization and the Uncertain Data communities.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1015–1018},
numpages = {4},
keywords = {privacy, uncertain databases, anonymization},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559970,
author = {Moon, Hyun J. and Curino, Carlo A. and Ham, Myungwon and Zaniolo, Carlo},
title = {PRIMA: Archiving and Querying Historical Data with Evolving Schemas},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559970},
doi = {10.1145/1559845.1559970},
abstract = {Schema evolution poses serious challenges in historical data management. Traditionally, historical data have been archived either by (i) migrating them into the current schema version that is well-understood by users but compromising archival quality, or (ii) by maintaining them under the original schema version in which the data was originally created, leading to perfect archival quality, but forcing users to formulate queries against complex histories of evolving schemas. In the PRIMA system, we achieve the best of both approaches, by (i) archiving historical data under the schema version under which they were originally created, and (ii) letting users express temporal queries using the current schema version. Thus, in PRIMA, the system rewrites the queries to the (potentially many) pertinent versions of the evolving schema. Moreover, the system o ers automatic documentation of the schema history, and allows the users to pose temporal queries over the metadata history itself. The proposed demonstration highlights the system features exploiting both a synthetic-educational running example and the real-life evolution histories (schemas and data), which include hundreds of schema versions from Wikipedia and Ensembl. The demonstration off ers a thorough walk-through of the system features and a hands-on system testing phase, where the audiences are invited to directly interact with the advanced query interface of PRIMA.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1019–1022},
numpages = {4},
keywords = {temporal databases, transaction-time databases, schema evolution},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559971,
author = {Dindar, Nihal and G\"{u}\c{c}, Baris and Lau, Patrick and Ozal, Asli and Soner, Merve and Tatbul, Nesime},
title = {DejaVu: Declarative Pattern Matching over Live and Archived Streams of Events},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559971},
doi = {10.1145/1559845.1559971},
abstract = {DejaVu is an event processing system that integrates declarative pattern matching over live and archived streams of events on top of a novel system architecture. We propose to demonstrate the key aspects of the DejaVu query language and architecture using two different application scenarios, namely a smart RFID library system and a financial market data analysis application. The demonstration will illustrate how DejaVu can uniformly handle one-time, continuous, and hybrid pattern matching queries over live and archived stream stores, using highly interactive visual monitoring tools including one that is based on the Second Life virtual world.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1023–1026},
numpages = {4},
keywords = {second life, mysql, rfid, data streams, pattern matching, complex event processing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559972,
author = {Nehme, Rimma V. and Lim, Hyo-Sang and Bertino, Elisa and Rundensteiner, Elke A.},
title = {<i>StreamShield</i>: A Stream-Centric Approach towards Security and Privacy in Data Stream Environments},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559972},
doi = {10.1145/1559845.1559972},
abstract = {We propose to demonstrate the StreamShield, a system designed to address the problem of security and privacy in the context of Data Stream Management Systems (DSMSs). In StreamShield, continuous access control is enforced by taking a novel "stream-centric" approach towards security. Security policies are not persistently stored on the server, but rather are depicted by security metadata, called "security punctuations", and get embedded into streams together with the data. We distinguish between two types of security punctuations: (1) the "data security punctuations" (dsps) describing the data-side security policies, and (2) the "query security punctuations" (qsps) representing the query-side security policies. The advantages of such stream-centric security model include flexibility, dynamicity and speed of enforcement. Furthermore, DSMSs can adapt to not only data-related but also to security-related selectivities, which helps reduce the waste of resources, when few subjects have access to streaming data.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1027–1030},
numpages = {4},
keywords = {security punctuations, data streams, access control},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559973,
author = {Wei, Mingzhu and Liu, Mo and Li, Ming and Golovnya, Denis and Rundensteiner, Elke A. and Claypool, Kajal},
title = {Supporting a Spectrum of Out-of-Order Event Processing Technologies: From Aggressive to Conservative Methodologies},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559973},
doi = {10.1145/1559845.1559973},
abstract = {This demonstration presents a complex event processing system which focuses on out-of-order handling. State-of-the-art event stream processing technology experiences significant challenges when faced with out-of-order data arrival including huge system latencies, missing results, and incorrect result generation. We propose two out-of-order handling techniques, conservative and aggressive strategies. We will show the efficiency of our techniques and how they can satisfy various QoS requirements of different applications.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1031–1034},
numpages = {4},
keywords = {stream processing, out-of-order data, complex event processing},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559974,
author = {Kumar, Arvind and Purandare, Amey and Chen, Jay and Meacham, Arthur and Subramanian, Lakshminarayanan},
title = {ELMR: Lightweight Mobile Health Records},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559974},
doi = {10.1145/1559845.1559974},
abstract = {Cell phones are increasingly being used as common clients for a wide suite of distributed, database-centric healthcare applications in developing regions. This is particularly true for rural developing regions where the bulk of the healthcare is handled by health workers due to lack of doctors; the widespread availability of cellular services have made mobile devices as an important computing platform for enabling healthcare applications for these health workers. Unfortunately, the current SQL model for distributed client/server systems is far too heavy-weight for these applications, particularly in light of the high communications cost and extremely limited data transmission capacity available in these environments.In this demonstration, we describe the Efficient Lightweight Mobile Records (ELMR) system that provides a practical and lightweight database access protocol for accessing and updating records remotely from mobile devices under an extremely bandwidth and cost-constrained Short Messaging Service (SMS) channel comprising of 140 byte packets. We have implemented ELMR using the RMS functionality in J2ME, and integrated it into an HIV treatment application we are developing for use by African health workers.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1035–1038},
numpages = {4},
keywords = {cell phones, rural healthcare, user interface},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559975,
author = {Chaudhuri, Surajit and Narasayya, Vivek and Syamala, Manoj},
title = {Bridging the Application and DBMS Divide Using Static Analysis and Dynamic Profiling},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559975},
doi = {10.1145/1559845.1559975},
abstract = {Relational database management systems (RDBMSs) today serve as the backend for many real-world data intensive applications. Database developers use data access APIs such as ADO.NET to execute SQL queries and access data. While modern program analysis and code profilers are extensively used during the software development life cycle, there is a significant gap in these technologies for database applications because these tools have little or no understanding of data access APIs or the DBMS. We have developed tools that: (a) Enhance traditional static analysis of programs by leveraging understanding of database APIs to help developers identify security, correctness and performance problems in the application. This enables such problems to be detected early in the application lifecycle. (b) Extend the existing DBMS and application profiling infrastructure to enable correlation of application events with DBMS events. This allows profiling across application, data access and DBMS layers. We demonstrate how our tools enable a rich class of analysis, tuning and profiling tasks that are otherwise not possible today.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1039–1042},
numpages = {4},
keywords = {dynamic profiling, static analysis, database application},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559976,
author = {Bruno, Nicolas and Chaudhuri, Surajit and Ramamurthy, Ravishankar},
title = {Interactive Plan Hints for Query Optimization},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559976},
doi = {10.1145/1559845.1559976},
abstract = {Commercial database systems expose query hints to fix poor plans produced by the query optimizer. However, current query hints are not flexible enough to deal with a variety of non-trivial scenarios, and can be at times cumbersome for DBAs to interact with. In this demonstration we present a framework that enables visual specification of hints to influence the optimizer to pick better plans. Our framework goes considerably beyond existing hinting mechanisms and significantly improves the usability of such functionality.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1043–1046},
numpages = {4},
keywords = {query hinting, query optimization},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559977,
author = {Angel, Albert and Koudas, Nick and Sarkas, Nikos and Srivastava, Divesh},
title = {What's on the Grapevine?},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559977},
doi = {10.1145/1559845.1559977},
abstract = {User generated content and social media (in the form of blogs, wikis, online video, microblogs, etc) are proliferating online. Grapevine conducts large scale data analysis on the social media collective, distilling and extracting information in real time. It aims to track entities and stories of interest in millions of blog posts, thousands of tweets, news items, etc., daily. Grapevine facilitates the interactive exploration of content, allowing users to discover interesting or surprising stories, optionally narrowed down on a specific demographic of interest (e.g. "What are Torontonians talking about on blogs?", "What are popular stories across news sources in Canada?", "What are financiers in Texas blogging about today?"). Stories of interest can be explored in a variety of ways, such as modifying their scope, obtaining related content (blog posts, news, etc), and examining their temporal evolution.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1047–1050},
numpages = {4},
keywords = {social media, text exploration, blogs, text mining},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559979,
author = {Xiao, Xiaokui and Wang, Guozhang and Gehrke, Johannes},
title = {Interactive Anonymization of Sensitive Data},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559979},
doi = {10.1145/1559845.1559979},
abstract = {There has been much recent work on algorithms for limiting disclosure in data publishing, however they have not been put to use in any toolkit for practicioners. We will demonstrate CAT, the Cornell Anonymization Toolkit, designed for interactive anonymization. CAT has an interface that is easy to use; it guides users through the process of preparing a dataset for publication while limiting disclosure through the identification of records that have high risk under various attacker models.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1051–1054},
numpages = {4},
keywords = {l-diversity, data anonymization},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559980,
author = {Glavic, Boris and Alonso, Gustavo},
title = {The Perm Provenance Management System in Action},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559980},
doi = {10.1145/1559845.1559980},
abstract = {In this demonstration we present the Perm provenance management system (PMS). Perm is capable of computing, storing and querying provenance information for the relational data model. Provenance is computed by using query rewriting techniques to annotate tuples with provenance information. Thus, provenance data and provenance computations are represented as relational data and queries and, hence, can be queried, stored and optimized using standard relational database techniques. This demo shows the complete Perm system and lets attendants examine in detail the process of query rewriting and provenance retrieval in Perm, the most complete data provenance system available today. For example, Perm supports lazy and eager provenance computation, external provenance and various contribution semantics.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1055–1058},
numpages = {4},
keywords = {query rewrite, provenance},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559981,
author = {Brodsky, Alexander and Bhot, Mayur M. and Chandrashekar, Manasa and Egge, Nathan E. and Wang, X. Sean},
title = {A Decisions Query Language (DQL): High-Level Abstraction for Mathematical Programming over Databases},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559981},
doi = {10.1145/1559845.1559981},
abstract = {The demonstrated, high-level decisions query language DQL combines the decision optimization capability of mathematical programming and the data manipulation capability of traditional database query languages. DQL benefits application developers in two aspects. First, it avoids a conceptual impedance mismatch between mathematical programming and data access and makes decision optimization functionality readily accessible to database programmers with no prior experience in operations research. Second, a tight integration provides unique opportunities for more efficient evaluation as compared to a loosely coupled system. This demonstration uses an emergency response scenario to illustrate the power of the language and its implementation.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1059–1062},
numpages = {4},
keywords = {mathematical programming., decision query, databases},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559982,
author = {Grust, Torsten and Mayr, Manuel and Rittinger, Jan and Schreiber, Tom},
title = {FERRY: Database-Supported Program Execution},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559982},
doi = {10.1145/1559845.1559982},
abstract = {We demonstrate the language Ferry and its editing, compilation, and execution environment FerryDeck. Ferry's type system and operations match those of scripting or programming languages; its compiler has been designed to emit (bundles of) compliant and efficient SQL:1999 statements. Ferry acts as glue that permits a programming style in which developers access database tables using their programming language's own syntax and idioms -- the Ferry-expressible fragments of such programs may be executed by a relational database back-end, i.e., close to the data. The demonstrator FerryDeck implements compile-and-execute-as-you-type interactivity for Ferry and offers a variety of (graphical) hooks to explore and inspect this approach to database-supported program execution.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1063–1066},
numpages = {4},
keywords = {sql:1999, ferry, pathfinder, linq},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559983,
author = {Nica, Anisoara and Brotherston, Daniel Scott and Hillis, David William},
title = {Extreme Visualisation of Query Optimizer Search Space},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559983},
doi = {10.1145/1559845.1559983},
abstract = {This demonstration showcases a system for visualizing and analyzing search spaces generated by the SQL Anywhere optimizer during the optimization process of a SQL statement. SQL Anywhere dynamically optimizes each statement every time it is executed. The decisions made by the optimizer during the optimization process are both cost-based and heuristics adapted to the current state of the server and the database instance. Many performance issues can be understood and resolved by analyzing the search space generated when optimizing a certain request. In our experience, there are two main classes of performance issues related to the decisions made by a query optimizer:(1) a request is very slow due to a suboptimal access plan; and (2) a request has a different, less optimal access plan than a previous execution. We have enhanced SQL Anywhere to log, in a very compact format, its search space during the optimization process when tracing mode is on. These search space logs can be used for performance analysis in the absence of the database instances or of extra information about the SQL Anywhere server state at the time the logs were generated. This demonstration introduces the SearchSpaceAnalyzer System, a research prototype used to analyze the search spaces of the SQL Anywhere optimizer. The system visualizes and analyzes (1) a single search space and (2) the differences between two search spaces generated for the same query by two different optimization processes. The SearchSpaceAnalyze System can be used for the analysis of any query optimizer search spaces as long as the logged data is recorded using the syntax understood by the system.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1067–1070},
numpages = {4},
keywords = {tree difference algorithm, query optimization, sql anywhere, enumeration algorithm, visualisation, search space},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559984,
author = {Huang, Jiewen and Antova, Lyublena and Koch, Christoph and Olteanu, Dan},
title = {MayBMS: A Probabilistic Database Management System},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559984},
doi = {10.1145/1559845.1559984},
abstract = {MayBMS is a state-of-the-art probabilistic database management system which leverages the strengths of previous database research for achieving scalability. As a proof of concept for its ease of use, we have built on top of MayBMS a Web-based application that offers NBA-related information based on what-if analysis of team dynamics using data available at www.nba.com.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1071–1074},
numpages = {4},
keywords = {query processing, probabilistic databases},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559985,
author = {Franke, Conny and Gertz, Michael},
title = {ORDEN: Outlier Region Detection and Exploration in Sensor Networks},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559985},
doi = {10.1145/1559845.1559985},
abstract = {Sensor networks play a central role in applications that monitor variables in geographic areas such as the traffic volume on roads or the temperature in the environment. A key feature users are often interested in when employing such systems is the detection of unusual phenomena, that is, anomalous values measured by the sensors. In this demonstration, we present a system, called ORDEN, that allows for the detection and (visual) exploration of outliers and anomalous events in sensor networks in real-time. In particular, the system constructs outlier regions from anomalous sensor measurements to provide for a comprehensive description of the spatial extent of phenomena of interest. With our system, users can interactively explore displayed outlier regions and investigate the heterogeneity within individual regions using different parameter and threshold settings. Using real-world sensor data streams from different application domains, we demonstrate the effectiveness and utility of our system.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1075–1078},
numpages = {4},
keywords = {stream processing, sensor data, outlier detection},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559987,
author = {Kashyap, Abhijith and Hristidis, Vagelis and Petropoulos, Michalis and Tavoulari, Sotiria},
title = {Exploring Biomedical Databases with BioNav},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559987},
doi = {10.1145/1559845.1559987},
abstract = {We demonstrate the BioNav system, a novel search interface for biomedical databases, such as PubMed. BioNav enables users to navigate large number of query results by categorizing them using MeSH; a comprehensive concept hierarchy used by PubMed. Once the query results are organized into a navigation tree, BioNav reveals only a small subset of the concept nodes at each step, selected such that the expected user navigation cost is minimized. In contrast, previous works expand the hierarchy in a predefined static manner, without navigation cost modeling. BioNav is available at http://db.cse.buffalo.edu/bionav.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1079–1082},
numpages = {4},
keywords = {query results navigation, taxonomy, user interfaces},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559988,
author = {Wang, Tengjiao and Yang, Bishan and Gao, Jun and Yang, Dongqing and Tang, Shiwei and Wu, Haoyu and Liu, Kedong and Pei, Jian},
title = {MobileMiner: A Real World Case Study of Data Mining in Mobile Communication},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559988},
doi = {10.1145/1559845.1559988},
abstract = {Mobile communication data analysis has been often used as a background application to motivate many data mining problems. However, very few data mining researchers have a chance to see a working data mining system on real mobile communication data. In this demo, we showcase our new system MobileMiner on a real mobile communication data set, which presents a case study of business solutions using state-of-the-art data mining techniques. MobileMiner adaptively profiles users' behavior from their calling and moving record streams. Customer segmentation and social community analysis can be conducted based on user profiles. We show how data mining techniques can help in mobile communication data analysis. Moreover, we also show some interesting observations which still cannot be mined by the current techniques, and thus may motivate new research and development.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1083–1086},
numpages = {4},
keywords = {sequence clustering, community discovery, data stream mining, mobile communication data},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559989,
author = {Chen, Zhibo and Ordonez, Carlos and Garcia-Alvarado, Carlos},
title = {Fast and Dynamic OLAP Exploration Using UDFs},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559989},
doi = {10.1145/1559845.1559989},
abstract = {OLAP is a set of database exploratory techniques to efficiently retrieve multiple sets of aggregations from a large dataset. Generally, these techniques have either involved the use of an external OLAP server or required the dataset to be exported to a specialized OLAP tool for more efficient processing. In this work, we show that OLAP techniques can be performed within a modern DBMS without external servers or the exporting of datasets, using standard SQL queries and UDFs. The main challenge of such approach is that SQL and UDFs are not as flexible as the C language to explore the OLAP lattice and therefore it is more difficult to develop optimizations. We compare three different ways of performing OLAP exploration: plain SQL queries, a UDF implementing a lattice structure, and a UDF programming the star cube structure. We demonstrate how such methods can be used to efficiently explore typical OLAP datasets.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1087–1090},
numpages = {4},
keywords = {udf, olap, cube},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559990,
author = {Jiang, Yunliang and Yang, Hui-Ting and Chang, Kevin Chen-chuan and Chen, Yi-Shin},
title = {AIDE: Ad-Hoc Intents Detection Engine over Query Logs},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559990},
doi = {10.1145/1559845.1559990},
abstract = {While keyword queries have become the "standard" query language of web search and many other database applications, their brevity and unstructuredness make it difficult to detect what users really want. In this demonstration, we aim to detect such hidden query intents, which we define as the frequent phrases that users co-ask with the query term, by exploring query logs. Toward building an online search system AIDE, we offer users the function to detect general and unique intents using arbitrary ad-hoc queries at run time. We will also demonstrate the effectiveness of the system which achieves indexing and searching over 14M MSN query log records.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1091–1094},
numpages = {4},
keywords = {unique, indexing, intent, general, ad-hoc},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559991,
author = {Chen, Kuang and Madhavan, Jayant and Halevy, Alon},
title = {Exploring Schema Repositories with Schemr},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559991},
doi = {10.1145/1559845.1559991},
abstract = {Schemr is a schema search engine, and provides users the ability to search for and visualize schemas stored in a metadata repository. Users may search by keywords and by example -- using schema fragments as query terms. Schemr uses a novel search algorithm, based on a combination of text search and schema matching techniques, as well as a structurally-aware scoring metric. Schemr presents search results in a GUI that allows users to explore which elements match and how well they do. The GUI supports interactions, including panning, zooming, layout and drilling-in. We demonstrate schema search and visualization, introduce Schemr as a new component of the information integration toolbox, and discuss its benefits in several applications.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1095–1098},
numpages = {4},
keywords = {schema search},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559992,
author = {Chen, Jidong and Guo, Hang and Wu, Wentao and Xie, Chunxin},
title = {Search Your Memory ! - An Associative Memory Based Desktop Search System},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559992},
doi = {10.1145/1559845.1559992},
abstract = {We present XSearcher, an associative memory based desktop search system, which exploits associations by creating semantic links of personal desktop resources from explicit and implicit user activities. With these links, associations among memory fragments can be built or rebuilt in a user's brain during a search. The personalized ranking scheme uses these links together with a user's personal preferences to rank results by both relevance and importance. XSearcher enhances traditional keyword based search systems since it is closer to the way that human associative memory works.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1099–1102},
numpages = {4},
keywords = {pesonalized ranking, faceted search, personal information management, associative search, desktop search},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559993,
author = {Kimelfeld, Benny and Sagiv, Yehoshua and Weber, Gidi},
title = {ExQueX: Exploring and Querying XML Documents},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559993},
doi = {10.1145/1559845.1559993},
abstract = {ExQueX is an interactive system for exploring and querying XML documents. The exploration is done by searching, ranking and filtering, and it enables users to discover relationships that exist in a given document. The results of the exploration can be used either directly as tree queries or as building blocks in the process of formulating more complex queries. The latter is done by formulating an abstract tree query, whose edges represent relationships that are resolved by using concrete results from the exploration phase. ExQueX facilitates fast and clear understanding of both the contents and complex structures of XML documents.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1103–1106},
numpages = {4},
keywords = {data exploration, interconnection semantics, querying, xml},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559994,
author = {Bercovitz, Benjamin and Kaliszan, Filip and Koutrika, Georgia and Liou, Henry and Mohammadi Zadeh, Zahra and Garcia-Molina, Hector},
title = {CourseRank: A Social System for Course Planning},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559994},
doi = {10.1145/1559845.1559994},
abstract = {Special-purpose social sites can offer valuable services to well-defined, closed, communities, e.g., in a university or in a corporation. The purpose of this demo is to show the challenges, special features and potential of a focused social system in action through CourseRank, a course evaluation and planning social system.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1107–1110},
numpages = {4},
keywords = {course planning, flexible recommendations, social sites, tag clouds},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559996,
author = {Liu, Mengmeng and Mihaylov, Svilen R. and Bao, Zhuowei and Jacob, Marie and Ives, Zachary G. and Loo, Boon Thau and Guha, Sudipto},
title = {SmartCIS: Integrating Digital and Physical Environments},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559996},
doi = {10.1145/1559845.1559996},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1111–1114},
numpages = {4},
keywords = {intelligent building, data integration, sensor, stream},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559997,
author = {Liu, Bin and Jagadish, H. V.},
title = {DataLens: Making a Good First Impression},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559997},
doi = {10.1145/1559845.1559997},
abstract = {When a database query has a large number of results, the user can only be shown one page of results at a time. One popular approach is to rank results such that the "best" results appear first. This approach is well-suited for information retrieval, and for some database queries, such as similarity queries or under-specified (or keyword) queries with known (or guessable) user preferences. However, standard database query results comprise a set of tuples, with no associated ranking. It is typical to allow users the ability to sort results on selected attributes, but no actual ranking is defined.An alternative approach is not to try to show the estimated best results on the first page, but instead to help users learn what is available in the whole result set and direct them to finding what they need. We present DataLens, a framework that: i) generates the most representative data points to display on the first page without sorting or ranking, ii) allows users to drill-down to more similar items in a hierarchical fashion, and iii) dynamically adjusts the representatives based on the user's new query conditions. To the best of our knowledge, DataLens is the first to allow hierarchical database result browsing and searching at the same time.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1115–1118},
numpages = {4},
keywords = {interface, exploration, representative},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559998,
author = {Gupta, Amarnath and Rafatirad, Setareh and Gao, Mingyan and Jain, Ramesh},
title = {MEDIALIFE: From Images to a Life Chronicle},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559998},
doi = {10.1145/1559845.1559998},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1119–1122},
numpages = {4},
keywords = {graph queries, annotation, ontology, multimedia},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1559999,
author = {Simmen, David E. and Reiss, Frederick and Li, Yunyao and Thalamati, Suresh},
title = {Enabling Enterprise Mashups over Unstructured Text Feeds with InfoSphere MashupHub and SystemT},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1559999},
doi = {10.1145/1559845.1559999},
abstract = {Enterprise mashup scenarios often involve feeds derived from data created primarily for eye consumption, such as email, news, calendars, blogs, and web feeds. These data sources can test the capabilities of current data mashup products, as the attributes needed to perform join, aggregation, and other operations are often buried within unstructured feed text. Information extraction technology is a key enabler in such scenarios, using annotators to convert unstructured text into structured information that can facilitate mashup operations.Our demo presents the integration of SystemT, an information extraction system from IBM Research, with IBM's InfoSphere MashupHub. We show how to build domain-specific annotators with SystemT's declarative rule language, AQL, and how to use these annotators to combine structured and unstructured information in an enterprise mashup.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1123–1126},
numpages = {4},
keywords = {information integration, feeds, text analytics, mashups},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1560000,
author = {Paparizos, Stelios and Ntoulas, Alexandros and Shafer, John and Agrawal, Rakesh},
title = {Answering Web Queries Using Structured Data Sources},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1560000},
doi = {10.1145/1559845.1560000},
abstract = {In web search today, a user types a few keywords which are then matched against a large collection of unstructured web pages. This leaves a lot to be desired for when the best answer to a query is contained in structured data stores and/or when the user includes some structural semantics in the query.In our work, we include information from structured data sources into web results. Such sources can vary from fully relational DBs, to flat tables and XML files. In addition, we take advantage of information in such sources to automatically extract corresponding semantics from the query and use them appropriately in improving the overall relevance of results.For this demonstration, we show how we effectively capture, annotate and translate web user queries such as 'popular digital camera around $425' returning results from a shopping-like DB.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1127–1130},
numpages = {4},
keywords = {web databases},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1560001,
author = {Maiti, Anirban and Dasgupta, Arjun and Zhang, Nan and Das, Gautam},
title = {HDSampler: Revealing Data behind Web Form Interfaces},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1560001},
doi = {10.1145/1559845.1560001},
abstract = {A large number of online databases are hidden behind the web. Users to these systems can form queries through web forms to retrieve a small sample of the database. Sampling such hidden databases is widely desired for understanding the nature and quality of data stored in them. We have developed HDSampler, which to the best of our knowledge is the first practical system for sampling structured hidden web databases. It enables efficient sampling of the databases and accurate answering of aggregate queries, to provide analysts with valuable information for data analytics, as well as help power a multitude of third-party applications such as web-mashups and meta-search engines. For the purpose of this demo, we present an instance of HDSampler on Google Base - a content-rich hidden web database maintained by Google. By using HDSampler, the demo reveals a snapshot of the marginal distribution of various attributes of Google Base in a matter of minutes.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1131–1134},
numpages = {4},
keywords = {top-k interfaces, sampling, hidden databases},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1560002,
author = {Wang, Haofen and Penin, Thomas and Xu, Kaifeng and Chen, Junquan and Sun, Xinruo and Fu, Linyun and Liu, Qiaoling and Yu, Yong and Tran, Thanh and Haase, Peter and Studer, Rudi},
title = {Hermes: A Travel through Semantics on the Data Web},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1560002},
doi = {10.1145/1559845.1560002},
abstract = {The Web as a global information space is developing from a Web of documents to a Web of data. This development opens new ways for addressing complex information needs. Search is no longer limited to matching keywords against documents, but instead complex information needs can be expressed in a structured way, with precise answers as results. In this paper, we demonstrate Hermes, an infrastructure for data web search. To provide an end-user oriented interface, we support expressive keyword search by translating user information needs into structured queries. We integrate heterogeneous web data sources with automatically computed mappings. Schema-level mappings are exploited in constructing structured queries against the integrated schema. These structured queries are decomposed into queries against the local web data sources, which are then processed in a distributed way.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1135–1138},
numpages = {4},
keywords = {keyword search, data integration, web of data, structured search},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

@inproceedings{10.1145/1559845.1560003,
author = {Chan, Bryan and Talbot, Justin and Wu, Leslie and Sakunkoo, Nathan and Cammarano, Mike and Hanrahan, Pat},
title = {Vispedia: On-Demand Data Integration for Interactive Visualization and Exploration},
year = {2009},
isbn = {9781605585512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1559845.1560003},
doi = {10.1145/1559845.1560003},
abstract = {Wikipedia is an example of the large, collaborative, semi-structured data sets emerging on the Web. Typically, before these data sets can be used, they must transformed into structured tables via data integration. We present Vispedia, a Web-based visualization system which incorporates data integration into an iterative, interactive data exploration and analysis process. This reduces the upfront cost of using heterogeneous data sets like Wikipedia. Vispedia is driven by a keyword-query-based integration interface implemented using a fast graph search. The search occurs interactively over DBpedia's semantic graph of Wikipedia, without depending on the existence of a structured ontology. This combination of data integration and visualization enables a broad class of non-expert users to more effectively use the semi-structured data available on the Web.},
booktitle = {Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data},
pages = {1139–1142},
numpages = {4},
keywords = {semantic web, information visualization, data integration},
location = {Providence, Rhode Island, USA},
series = {SIGMOD '09}
}

