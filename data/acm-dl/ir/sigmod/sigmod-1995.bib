@inproceedings{10.1145/223784.223785,
author = {Berenson, Hal and Bernstein, Phil and Gray, Jim and Melton, Jim and O'Neil, Elizabeth and O'Neil, Patrick},
title = {A Critique of ANSI SQL Isolation Levels},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223785},
doi = {10.1145/223784.223785},
abstract = {ANSI SQL-92 [MS, ANSI] defines Isolation Levels in terms of phenomena: Dirty Reads, Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to properly characterize several popular isolation levels, including the standard locking implementations of the levels covered. Ambiguity in the statement of the phenomena is investigated and a more formal statement is arrived at; in addition new phenomena that better characterize isolation types are introduced. Finally, an important multiversion isolation type, called Snapshot Isolation, is defined.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {1–10},
numpages = {10},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223785,
author = {Berenson, Hal and Bernstein, Phil and Gray, Jim and Melton, Jim and O'Neil, Elizabeth and O'Neil, Patrick},
title = {A Critique of ANSI SQL Isolation Levels},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223785},
doi = {10.1145/568271.223785},
abstract = {ANSI SQL-92 [MS, ANSI] defines Isolation Levels in terms of phenomena: Dirty Reads, Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to properly characterize several popular isolation levels, including the standard locking implementations of the levels covered. Ambiguity in the statement of the phenomena is investigated and a more formal statement is arrived at; in addition new phenomena that better characterize isolation types are introduced. Finally, an important multiversion isolation type, called Snapshot Isolation, is defined.},
journal = {SIGMOD Rec.},
month = may,
pages = {1–10},
numpages = {10}
}

@inproceedings{10.1145/223784.223786,
author = {Molesky, Lory D. and Ramamritham, Krithi},
title = {Recovery Protocols for Shared Memory Database Systems},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223786},
doi = {10.1145/223784.223786},
abstract = {Significant performance advantages can be gained by implementing a database system on a cache-coherent shared memory multiprocessor. However, problems arise when failures occur. A single node (where a node refers to a processor/memory pair) crash may require a reboot of the entire shared memory system. Fortunately, shared memory multiprocessors that isolate individual node failures are currently being developed. Even with these, because of the side effects of the cache coherency protocol, a transaction executing strictly on a single node may become dependent on the validity of the memory of many nodes thereby inducing unnecessary transaction aborts. This happens when database objects, such as records, and database support structures, such as index structures and shared lock tables, are stored in shared memory.In this paper, we propose crash recovery protocols for shared memory database systems which avoid the unnecessary transaction aborts induced by the effects of using shared physical memory. Our recovery protocols guarantee that if one or more nodes crash, all the effects of active transactions running on the crashed nodes will be undone, and no effects of transactions running on nodes which did not crash will be undone. In order to show the practicality of our protocols, we discuss how existing features of cache-coherent multiprocessors can be utilized to implement these recovery protocols. Specifically, we demonstrate that (1) for many types of database objects and support structures, volatile (in-memory) logging is sufficient to avoid unnecessary transaction aborts, and (2) a very low overhead implementation of this strategy can be achieved with existing multiprocessor features.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {11–22},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223786,
author = {Molesky, Lory D. and Ramamritham, Krithi},
title = {Recovery Protocols for Shared Memory Database Systems},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223786},
doi = {10.1145/568271.223786},
abstract = {Significant performance advantages can be gained by implementing a database system on a cache-coherent shared memory multiprocessor. However, problems arise when failures occur. A single node (where a node refers to a processor/memory pair) crash may require a reboot of the entire shared memory system. Fortunately, shared memory multiprocessors that isolate individual node failures are currently being developed. Even with these, because of the side effects of the cache coherency protocol, a transaction executing strictly on a single node may become dependent on the validity of the memory of many nodes thereby inducing unnecessary transaction aborts. This happens when database objects, such as records, and database support structures, such as index structures and shared lock tables, are stored in shared memory.In this paper, we propose crash recovery protocols for shared memory database systems which avoid the unnecessary transaction aborts induced by the effects of using shared physical memory. Our recovery protocols guarantee that if one or more nodes crash, all the effects of active transactions running on the crashed nodes will be undone, and no effects of transactions running on nodes which did not crash will be undone. In order to show the practicality of our protocols, we discuss how existing features of cache-coherent multiprocessors can be utilized to implement these recovery protocols. Specifically, we demonstrate that (1) for many types of database objects and support structures, volatile (in-memory) logging is sufficient to avoid unnecessary transaction aborts, and (2) a very low overhead implementation of this strategy can be achieved with existing multiprocessor features.},
journal = {SIGMOD Rec.},
month = may,
pages = {11–22},
numpages = {12}
}

@inproceedings{10.1145/223784.223787,
author = {Adya, Atul and Gruber, Robert and Liskov, Barbara and Maheshwari, Umesh},
title = {Efficient Optimistic Concurrency Control Using Loosely Synchronized Clocks},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223787},
doi = {10.1145/223784.223787},
abstract = {This paper describes an efficient optimistic concurrency control scheme for use in distributed database systems in which objects are cached and manipulated at client machines while persistent storage and transactional support are provided by servers. The scheme provides both serializability and external consistency for committed transactions; it uses loosely synchronized clocks to achieve global serialization. It stores only a single version of each object, and avoids maintaining any concurrency control information on a per-object basis; instead, it tracks recent invalidations on a per-client basis, an approach that has low in-memory space overhead and no per-object disk overhead. In addition to its low space overheads, the scheme also performs well. The paper presents a simulation study that compares the scheme to adaptive callback locking, the best concurrency control scheme for client-server object-oriented database systems studied to date. The study shows that our scheme outperforms adaptive callback locking for low to moderate contention workloads, and scales better with the number of clients. For high contention workloads, optimism can result in a high abort rate; the scheme presented here is a first step toward a hybrid scheme that we expect to perform well across the full range of workloads.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {23–34},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223787,
author = {Adya, Atul and Gruber, Robert and Liskov, Barbara and Maheshwari, Umesh},
title = {Efficient Optimistic Concurrency Control Using Loosely Synchronized Clocks},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223787},
doi = {10.1145/568271.223787},
abstract = {This paper describes an efficient optimistic concurrency control scheme for use in distributed database systems in which objects are cached and manipulated at client machines while persistent storage and transactional support are provided by servers. The scheme provides both serializability and external consistency for committed transactions; it uses loosely synchronized clocks to achieve global serialization. It stores only a single version of each object, and avoids maintaining any concurrency control information on a per-object basis; instead, it tracks recent invalidations on a per-client basis, an approach that has low in-memory space overhead and no per-object disk overhead. In addition to its low space overheads, the scheme also performs well. The paper presents a simulation study that compares the scheme to adaptive callback locking, the best concurrency control scheme for client-server object-oriented database systems studied to date. The study shows that our scheme outperforms adaptive callback locking for low to moderate contention workloads, and scales better with the number of clients. For high contention workloads, optimism can result in a high abort rate; the scheme presented here is a first step toward a hybrid scheme that we expect to perform well across the full range of workloads.},
journal = {SIGMOD Rec.},
month = may,
pages = {23–34},
numpages = {12}
}

@inproceedings{10.1145/223784.223788,
author = {Brodsky, Alexander and Kornatzky, Yoram},
title = {The LyriC Language: Querying Constraint Objects},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223788},
doi = {10.1145/223784.223788},
abstract = {We propose a novel data model and its language for querying object-oriented databases where objects may hold spatial, temporal or constraint data, conceptually represented by linear equality and inequality constraints. The proposed LyriC language is designed to provide a uniform and flexible framework for diverse application realms such as (1) constraint-based design in two-, three-, or higher-dimensional space, (2) large-scale optimization and analysis, based mostly on linear programming techniques, and (3) spatial and geographic databases. LyriC extends flat constraint query languages, especially those for linear constraint databases, to structurally complex objects. The extension is based on the object-oriented paradigm, where constraints are treated as first-class objects that are organized in classes. The query language is an extension of the language XSQL, and is built around the idea of extended path expressions. Path expressions in a query traverse nested structures in one sweep. Constraints are used in a query to filter stored constraints and to create new constraint objects.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {35–46},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223788,
author = {Brodsky, Alexander and Kornatzky, Yoram},
title = {The LyriC Language: Querying Constraint Objects},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223788},
doi = {10.1145/568271.223788},
abstract = {We propose a novel data model and its language for querying object-oriented databases where objects may hold spatial, temporal or constraint data, conceptually represented by linear equality and inequality constraints. The proposed LyriC language is designed to provide a uniform and flexible framework for diverse application realms such as (1) constraint-based design in two-, three-, or higher-dimensional space, (2) large-scale optimization and analysis, based mostly on linear programming techniques, and (3) spatial and geographic databases. LyriC extends flat constraint query languages, especially those for linear constraint databases, to structurally complex objects. The extension is based on the object-oriented paradigm, where constraints are treated as first-class objects that are organized in classes. The query language is an extension of the language XSQL, and is built around the idea of extended path expressions. Path expressions in a query traverse nested structures in one sweep. Constraints are used in a query to filter stored constraints and to create new constraint objects.},
journal = {SIGMOD Rec.},
month = may,
pages = {35–46},
numpages = {12}
}

@inproceedings{10.1145/223784.223789,
author = {Fegaras, Leonidas and Maier, David},
title = {Towards an Effective Calculus for Object Query Languages},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223789},
doi = {10.1145/223784.223789},
abstract = {We define a standard of effectiveness for a database calculus relative to a query language. Effectiveness judges suitability to serve as a processing framework for the query language, and comprises aspects of coverage, manipulability and efficient evaluation. We present the monoid calculus, and argue its effectiveness for object-oriented query languages, exemplified by OQL of ODMG-93. The monoid calculus readily captures such features as multiple collection types, aggregations, arbitrary composition of type constructors and nested query expressions. We also show how to extend the monoid calculus to deal with vectors and arrays in more expressive ways than current query languages do, and illustrate how it can handle identity and updates.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {47–58},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223789,
author = {Fegaras, Leonidas and Maier, David},
title = {Towards an Effective Calculus for Object Query Languages},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223789},
doi = {10.1145/568271.223789},
abstract = {We define a standard of effectiveness for a database calculus relative to a query language. Effectiveness judges suitability to serve as a processing framework for the query language, and comprises aspects of coverage, manipulability and efficient evaluation. We present the monoid calculus, and argue its effectiveness for object-oriented query languages, exemplified by OQL of ODMG-93. The monoid calculus readily captures such features as multiple collection types, aggregations, arbitrary composition of type constructors and nested query expressions. We also show how to extend the monoid calculus to deal with vectors and arrays in more expressive ways than current query languages do, and illustrate how it can handle identity and updates.},
journal = {SIGMOD Rec.},
month = may,
pages = {47–58},
numpages = {12}
}

@inproceedings{10.1145/223784.223791,
author = {Gardarin, Georges and Machuca, Fernando and Pucheral, Philippe},
title = {OFL: A Functional Execution Model for Object Query Languages},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223791},
doi = {10.1145/223784.223791},
abstract = {We present a functional paradigm for querying efficiently abstract collections of complex objects. Abstract collections are used to model class extents, multivalued attributes as well as indexes or hashing tables. Our paradigm includes a functional language called OFL (Object Functional Language) and a supporting execution model based on graph traversals. OFL is able to support any complex object algebra with recursion as macros. It is an appropriate target language for OQL-like query compilers. The execution model provides various strategies including set-oriented and pipelined traversals. OFL has been implemented on top of an object manager. Measures of a typical query extracted from a geographical benchmark show the value of hybrid strategies integrating pipelined and set-oriented evaluations. They also show the potential of function result memorization, a typical optimization approach known as "Memoization" 2 in functional languages.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {59–70},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223791,
author = {Gardarin, Georges and Machuca, Fernando and Pucheral, Philippe},
title = {OFL: A Functional Execution Model for Object Query Languages},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223791},
doi = {10.1145/568271.223791},
abstract = {We present a functional paradigm for querying efficiently abstract collections of complex objects. Abstract collections are used to model class extents, multivalued attributes as well as indexes or hashing tables. Our paradigm includes a functional language called OFL (Object Functional Language) and a supporting execution model based on graph traversals. OFL is able to support any complex object algebra with recursion as macros. It is an appropriate target language for OQL-like query compilers. The execution model provides various strategies including set-oriented and pipelined traversals. OFL has been implemented on top of an object manager. Measures of a typical query extracted from a geographical benchmark show the value of hybrid strategies integrating pipelined and set-oriented evaluations. They also show the potential of function result memorization, a typical optimization approach known as "Memoization" 2 in functional languages.},
journal = {SIGMOD Rec.},
month = may,
pages = {59–70},
numpages = {12}
}

@inproceedings{10.1145/223784.223794,
author = {Roussopoulos, Nick and Kelley, Stephen and Vincent, Fr\'{e}d\'{e}ric},
title = {Nearest Neighbor Queries},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223794},
doi = {10.1145/223784.223794},
abstract = {A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {71–79},
numpages = {9},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223794,
author = {Roussopoulos, Nick and Kelley, Stephen and Vincent, Fr\'{e}d\'{e}ric},
title = {Nearest Neighbor Queries},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223794},
doi = {10.1145/568271.223794},
abstract = {A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm.},
journal = {SIGMOD Rec.},
month = may,
pages = {71–79},
numpages = {9}
}

@inproceedings{10.1145/223784.223796,
author = {Freeston, Michael},
title = {A General Solution of the N-Dimensional B-Tree Problem},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223796},
doi = {10.1145/223784.223796},
abstract = {We present a generic solution to a problem which lies at the heart of the unpredictable worst-case performance characteristics of a wide class of multi-dimensional index designs: those which employ a recursive partitioning of the data space. We then show how this solution can produce modified designs with fully predictable and controllable worst-case characteristics. In particular, we show how the recursive partitioning of an n-dimensional dataspace can be represented in such a way that the characteristics of the one-dimensional B-tree are preserved in n dimensions, as far as is topologically possible i.e. a representation guaranteeing logarithmic access and update time, while also guaranteeing a one-third minimum occupancy of both data and index nodes.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {80–91},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223796,
author = {Freeston, Michael},
title = {A General Solution of the N-Dimensional B-Tree Problem},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223796},
doi = {10.1145/568271.223796},
abstract = {We present a generic solution to a problem which lies at the heart of the unpredictable worst-case performance characteristics of a wide class of multi-dimensional index designs: those which employ a recursive partitioning of the data space. We then show how this solution can produce modified designs with fully predictable and controllable worst-case characteristics. In particular, we show how the recursive partitioning of an n-dimensional dataspace can be represented in such a way that the characteristics of the one-dimensional B-tree are preserved in n dimensions, as far as is topologically possible i.e. a representation guaranteeing logarithmic access and update time, while also guaranteeing a one-third minimum occupancy of both data and index nodes.},
journal = {SIGMOD Rec.},
month = may,
pages = {80–91},
numpages = {12}
}

@inproceedings{10.1145/223784.223798,
author = {Papadias, Dimitris and Sellis, Timos and Theodoridis, Yannis and Egenhofer, Max J.},
title = {Topological Relations in the World of Minimum Bounding Rectangles: A Study with R-Trees},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223798},
doi = {10.1145/223784.223798},
abstract = {Recent developments in spatial relations have led to their use in numerous applications involving spatial databases. This paper is concerned with the retrieval of topological relations in Minimum Bounding Rectangle-based data structures. We study the topological information that Minimum Bounding Rectangles convey about the actual objects they enclose, using the concept of projections. Then we apply the results to R-trees and their variations, R+-trees and R*-trees in order to minimise disk accesses for queries involving topological relations. We also investigate queries that involve complex spatial conditions in the form of disjunctions and conjunctions and we discuss possible extensions.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {92–103},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223798,
author = {Papadias, Dimitris and Sellis, Timos and Theodoridis, Yannis and Egenhofer, Max J.},
title = {Topological Relations in the World of Minimum Bounding Rectangles: A Study with R-Trees},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223798},
doi = {10.1145/568271.223798},
abstract = {Recent developments in spatial relations have led to their use in numerous applications involving spatial databases. This paper is concerned with the retrieval of topological relations in Minimum Bounding Rectangle-based data structures. We study the topological information that Minimum Bounding Rectangles convey about the actual objects they enclose, using the concept of projections. Then we apply the results to R-trees and their variations, R+-trees and R*-trees in order to minimise disk accesses for queries involving topological relations. We also investigate queries that involve complex spatial conditions in the form of disjunctions and conjunctions and we discuss possible extensions.},
journal = {SIGMOD Rec.},
month = may,
pages = {92–103},
numpages = {12}
}

@inproceedings{10.1145/223784.223801,
author = {Shatdal, Ambuj and Naughton, Jeffrey F.},
title = {Adaptive Parallel Aggregation Algorithms},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223801},
doi = {10.1145/223784.223801},
abstract = {Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {104–114},
numpages = {11},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223801,
author = {Shatdal, Ambuj and Naughton, Jeffrey F.},
title = {Adaptive Parallel Aggregation Algorithms},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223801},
doi = {10.1145/568271.223801},
abstract = {Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches.},
journal = {SIGMOD Rec.},
month = may,
pages = {104–114},
numpages = {11}
}

@inproceedings{10.1145/223784.223803,
author = {Wilschut, Annita N. and Flokstra, Jan and Apers, Peter M. G.},
title = {Parallel Evaluation of Multi-Join Queries},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223803},
doi = {10.1145/223784.223803},
abstract = {A number of execution strategies for parallel evaluation of multi-join queries have been proposed in the literature; their performance was evaluated by simulation. In this paper we give a comparative performance evaluation of four execution strategies by implementing all of them on the same parallel database system, PRISMA/DB. Experiments have been done up to 80 processors. The basic strategy is to first determine an execution schedule with minimum total cost and then parallelize this schedule with one of the four execution strategies. These strategies, coming from the literature, are named: Sequential Parallel, Synchronous Execution, Segmented Right-Deep, and Full Parallel. Based on the experiments clear guidelines are given when to use which strategy.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {115–126},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223803,
author = {Wilschut, Annita N. and Flokstra, Jan and Apers, Peter M. G.},
title = {Parallel Evaluation of Multi-Join Queries},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223803},
doi = {10.1145/568271.223803},
abstract = {A number of execution strategies for parallel evaluation of multi-join queries have been proposed in the literature; their performance was evaluated by simulation. In this paper we give a comparative performance evaluation of four execution strategies by implementing all of them on the same parallel database system, PRISMA/DB. Experiments have been done up to 80 processors. The basic strategy is to first determine an execution schedule with minimum total cost and then parallelize this schedule with one of the four execution strategies. These strategies, coming from the literature, are named: Sequential Parallel, Synchronous Execution, Segmented Right-Deep, and Full Parallel. Based on the experiments clear guidelines are given when to use which strategy.},
journal = {SIGMOD Rec.},
month = may,
pages = {115–126},
numpages = {12}
}

@inproceedings{10.1145/223784.223807,
author = {Hern\'{a}ndez, Mauricio A. and Stolfo, Salvatore J.},
title = {The Merge/Purge Problem for Large Databases},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223807},
doi = {10.1145/223784.223807},
abstract = {Many commercial organizations routinely gather large numbers of databases for various marketing and business analysis functions. The task is to correlate information from different databases by identifying distinct individuals that appear in a number of different databases typically in an inconsistent and often incorrect fashion. The problem we study here is the task of merging data from multiple sources in as efficient manner as possible, while maximizing the accuracy of the result. We call this the merge/purge problem. In this paper we detail the sorted neighborhood method that is used by some to solve merge/purge and present experimental results that demonstrates this approach may work well in practice but at great expense. An alternative method based upon clustering is also presented with a comparative evaluation to the sorted neighborhood method. We show a means of improving the accuracy of the results based upon a multi-pass approach that succeeds by computing the Transitive Closure over the results of independent runs considering alternative primary key attributes in each pass.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {127–138},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223807,
author = {Hern\'{a}ndez, Mauricio A. and Stolfo, Salvatore J.},
title = {The Merge/Purge Problem for Large Databases},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223807},
doi = {10.1145/568271.223807},
abstract = {Many commercial organizations routinely gather large numbers of databases for various marketing and business analysis functions. The task is to correlate information from different databases by identifying distinct individuals that appear in a number of different databases typically in an inconsistent and often incorrect fashion. The problem we study here is the task of merging data from multiple sources in as efficient manner as possible, while maximizing the accuracy of the result. We call this the merge/purge problem. In this paper we detail the sorted neighborhood method that is used by some to solve merge/purge and present experimental results that demonstrates this approach may work well in practice but at great expense. An alternative method based upon clustering is also presented with a comparative evaluation to the sorted neighborhood method. We show a means of improving the accuracy of the results based upon a multi-pass approach that succeeds by computing the Transitive Closure over the results of independent runs considering alternative primary key attributes in each pass.},
journal = {SIGMOD Rec.},
month = may,
pages = {127–138},
numpages = {12}
}

@inproceedings{10.1145/223784.223809,
author = {Ramaswamy, Sridhar and Kanellakis, Paris C.},
title = {OODB Indexing by Class-Division},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223809},
doi = {10.1145/223784.223809},
abstract = {Indexing a class hierarchy, in order to efficiently search or update the objects of a class according to a (range of) value(s) of an attribute, impacts OODB performance heavily. For this indexing problem, most systems use the class hierarchy index (CH) technique of [15] implemented using B+-trees. Other techniques, such as those of [14, 18,31], can lead to improved average-case performance but involve the implementation of new data-structures. As a special form of external dynamic two-dimensional range searching, this OODB indexing problem is solvable within reasonable worst-case bounds [12]. Based on this insight, we have developed a technique, called indexing by class-division (CD), which we believe can be used as a practical alternative to CH. We present an optimized implementation and experimental validation of CD's average-case performance. The main advantages of the CD technique are: (1) CD is an extension of CH that provides a significant speed-up over CH for a wide spectrum of range queries--this speed-up is at least linear in the number of classes queried for uniform data and larger otherwise; and (2) CD queries, updates and concurrent use are implementable using existing B+-tree technology. The basic idea of class-division involves a time-space tradeoff and CD requires some space and update overhead in comparison to CH. In practice, this overhead is a small factor (2 to 3) and, in worst-case, is bounded by the depth of the hierarchy and the logarithm of its size.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {139–150},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223809,
author = {Ramaswamy, Sridhar and Kanellakis, Paris C.},
title = {OODB Indexing by Class-Division},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223809},
doi = {10.1145/568271.223809},
abstract = {Indexing a class hierarchy, in order to efficiently search or update the objects of a class according to a (range of) value(s) of an attribute, impacts OODB performance heavily. For this indexing problem, most systems use the class hierarchy index (CH) technique of [15] implemented using B+-trees. Other techniques, such as those of [14, 18,31], can lead to improved average-case performance but involve the implementation of new data-structures. As a special form of external dynamic two-dimensional range searching, this OODB indexing problem is solvable within reasonable worst-case bounds [12]. Based on this insight, we have developed a technique, called indexing by class-division (CD), which we believe can be used as a practical alternative to CH. We present an optimized implementation and experimental validation of CD's average-case performance. The main advantages of the CD technique are: (1) CD is an extension of CH that provides a significant speed-up over CH for a wide spectrum of range queries--this speed-up is at least linear in the number of classes queried for uniform data and larger otherwise; and (2) CD queries, updates and concurrent use are implementable using existing B+-tree technology. The basic idea of class-division involves a time-space tradeoff and CD requires some space and update overhead in comparison to CH. In practice, this overhead is a small factor (2 to 3) and, in worst-case, is bounded by the depth of the hierarchy and the logarithm of its size.},
journal = {SIGMOD Rec.},
month = may,
pages = {139–150},
numpages = {12}
}

@inproceedings{10.1145/223784.223811,
author = {Aref, Walid and Barbar\'{a}, Daniel and Vallabhaneni, Padmavathi},
title = {The Handwritten Trie: Indexing Electronic Ink},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223811},
doi = {10.1145/223784.223811},
abstract = {The emergence of the pen as the main interface device for personal digital assistants and pen-computers has made handwritten text, and more generally ink, a first-class object. As for any other type of data, the need of retrieval is a prevailing one. Retrieval of handwritten text is more difficult than that of conventional data since it is necessary to identify a handwritten word given slightly different variations in its shape. The current way of addressing this is by using handwriting recognition, which is prone to errors and limits the expressiveness of ink. Alternatively, one can retrieve from the database handwritten words that are similar to a query handwritten word using techniques borrowed from pattern and speech recognition. In particular, Hidden Markov Models (HMM) can be used as representatives of the handwritten words in the database. However, using HMM techniques to match the input against every item in the database (sequential searching) is unacceptably slow and does not scale up for large ink databases. In this paper, an indexing technique based on HMMs is proposed. The new index is a variation of the trie data structure that uses HMMs and a new search algorithm to provide approximate matching. Each node in the tree contains handwritten letters, where each letter is represented by an HMM. Branching in the trie is based on the ranking of matches given by the HMMs. The new search algorithm is parametrized so that it provides means for controlling the matching quality of the search process via a time-based budget. The index dramatically improves the search time in a database of handwritten words. Due to the variety of platforms for which this work is aimed, ranging from personal digital assistants to desktop computers, we implemented both main-memory and disk-based systems. The implementations are reported in this paper, along with performance results that show the practicality of the technique under a variety of conditions.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {151–162},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223811,
author = {Aref, Walid and Barbar\'{a}, Daniel and Vallabhaneni, Padmavathi},
title = {The Handwritten Trie: Indexing Electronic Ink},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223811},
doi = {10.1145/568271.223811},
abstract = {The emergence of the pen as the main interface device for personal digital assistants and pen-computers has made handwritten text, and more generally ink, a first-class object. As for any other type of data, the need of retrieval is a prevailing one. Retrieval of handwritten text is more difficult than that of conventional data since it is necessary to identify a handwritten word given slightly different variations in its shape. The current way of addressing this is by using handwriting recognition, which is prone to errors and limits the expressiveness of ink. Alternatively, one can retrieve from the database handwritten words that are similar to a query handwritten word using techniques borrowed from pattern and speech recognition. In particular, Hidden Markov Models (HMM) can be used as representatives of the handwritten words in the database. However, using HMM techniques to match the input against every item in the database (sequential searching) is unacceptably slow and does not scale up for large ink databases. In this paper, an indexing technique based on HMMs is proposed. The new index is a variation of the trie data structure that uses HMMs and a new search algorithm to provide approximate matching. Each node in the tree contains handwritten letters, where each letter is represented by an HMM. Branching in the trie is based on the ranking of matches given by the HMMs. The new search algorithm is parametrized so that it provides means for controlling the matching quality of the search process via a time-based budget. The index dramatically improves the search time in a database of handwritten words. Due to the variety of platforms for which this work is aimed, ranging from personal digital assistants to desktop computers, we implemented both main-memory and disk-based systems. The implementations are reported in this paper, along with performance results that show the practicality of the technique under a variety of conditions.},
journal = {SIGMOD Rec.},
month = may,
pages = {151–162},
numpages = {12}
}

@inproceedings{10.1145/223784.223812,
author = {Faloutsos, Christos and Lin, King-Ip},
title = {FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223812},
doi = {10.1145/223784.223812},
abstract = {A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the 'Query By Example' type (which translates to a range query); the 'all pairs' query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc.However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points.This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient retrieval, in conjunction with a SAM, as discussed before and (b) visualization and data-mining: the objects can now be plotted as points in 2-d or 3-d space, revealing potential clusters, correlations among attributes and other regularities that data-mining is looking for.We introduce an older method from pattern recognition, namely, Multi-Dimensional Scaling (MDS) [51]; although unsuitable for indexing, we use it as yardstick for our method. Then, we propose a much faster algorithm to solve the problem in hand, while in addition it allows for indexing. Experiments on real and synthetic data indeed show that the proposed algorithm is significantly faster than MDS, (being linear, as opposed to quadratic, on the database size N), while it manages to preserve distances and the overall structure of the data-set.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {163–174},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223812,
author = {Faloutsos, Christos and Lin, King-Ip},
title = {FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223812},
doi = {10.1145/568271.223812},
abstract = {A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the 'Query By Example' type (which translates to a range query); the 'all pairs' query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc.However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points.This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient retrieval, in conjunction with a SAM, as discussed before and (b) visualization and data-mining: the objects can now be plotted as points in 2-d or 3-d space, revealing potential clusters, correlations among attributes and other regularities that data-mining is looking for.We introduce an older method from pattern recognition, namely, Multi-Dimensional Scaling (MDS) [51]; although unsuitable for indexing, we use it as yardstick for our method. Then, we propose a much faster algorithm to solve the problem in hand, while in addition it allows for indexing. Experiments on real and synthetic data indeed show that the proposed algorithm is significantly faster than MDS, (being linear, as opposed to quadratic, on the database size N), while it manages to preserve distances and the overall structure of the data-set.},
journal = {SIGMOD Rec.},
month = may,
pages = {163–174},
numpages = {12}
}

@inproceedings{10.1145/223784.223813,
author = {Park, Jong Soo and Chen, Ming-Syan and Yu, Philip S.},
title = {An Effective Hash-Based Algorithm for Mining Association Rules},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223813},
doi = {10.1145/223784.223813},
abstract = {In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation. Explicitly, the number of candidate 2-itemsets generated by the proposed algorithm is, in orders of magnitude, smaller than that by previous methods, thus resolving the performance bottleneck. Note that the generation of smaller candidate sets enables us to effectively trim the transaction database size at a much earlier stage of the iterations, thereby reducing the computational cost for later iterations significantly. Extensive simulation study is conducted to evaluate performance of the proposed algorithm.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {175–186},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223813,
author = {Park, Jong Soo and Chen, Ming-Syan and Yu, Philip S.},
title = {An Effective Hash-Based Algorithm for Mining Association Rules},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223813},
doi = {10.1145/568271.223813},
abstract = {In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation. Explicitly, the number of candidate 2-itemsets generated by the proposed algorithm is, in orders of magnitude, smaller than that by previous methods, thus resolving the performance bottleneck. Note that the generation of smaller candidate sets enables us to effectively trim the transaction database size at a much earlier stage of the iterations, thereby reducing the computational cost for later iterations significantly. Extensive simulation study is conducted to evaluate performance of the proposed algorithm.},
journal = {SIGMOD Rec.},
month = may,
pages = {175–186},
numpages = {12}
}

@inproceedings{10.1145/223784.223814,
author = {White, Seth J. and DeWitt, David J.},
title = {Implementing Crash Recovery in QuickStore: A Performance Study},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223814},
doi = {10.1145/223784.223814},
abstract = {Implementing crash recovery in an Object-Oriented Database System (OODBMS) raises several challenging issues for performance that are not present in traditional DBMSs. These performance concerns result both from significant architectural differences between OODBMSs and traditional database systems and differences in OODBMS's target applications. This paper compares the performance of several alternative approaches to implementing crash recovery in an OODBMS based on a client-server architecture. The four basic recovery techniques examined in the paper are termed page differencing, sub-page differencing, whole-page logging, and redo-at-server. All of the recovery techniques were implemented in the context of QuickStore, a memory-mapped store built using the EXODUS Storage Manager, and their performance is compared using the OO7 database benchmark. The results of the performance study show that the techniques based on differencing generally provide superior performance to whole-page logging.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {187–198},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223814,
author = {White, Seth J. and DeWitt, David J.},
title = {Implementing Crash Recovery in QuickStore: A Performance Study},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223814},
doi = {10.1145/568271.223814},
abstract = {Implementing crash recovery in an Object-Oriented Database System (OODBMS) raises several challenging issues for performance that are not present in traditional DBMSs. These performance concerns result both from significant architectural differences between OODBMSs and traditional database systems and differences in OODBMS's target applications. This paper compares the performance of several alternative approaches to implementing crash recovery in an OODBMS based on a client-server architecture. The four basic recovery techniques examined in the paper are termed page differencing, sub-page differencing, whole-page logging, and redo-at-server. All of the recovery techniques were implemented in the context of QuickStore, a memory-mapped store built using the EXODUS Storage Manager, and their performance is compared using the OO7 database benchmark. The results of the performance study show that the techniques based on differencing generally provide superior performance to whole-page logging.},
journal = {SIGMOD Rec.},
month = may,
pages = {187–198},
numpages = {12}
}

@inproceedings{10.1145/223784.223816,
author = {Acharya, Swarup and Alonso, Rafael and Franklin, Michael and Zdonik, Stanley},
title = {Broadcast Disks: Data Management for Asymmetric Communication Environments},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223816},
doi = {10.1145/223784.223816},
abstract = {This paper proposes the use of repetitive broadcast as a way of augmenting the memory hierarchy of clients in an asymmetric communication environment. We describe a new technique called "Broadcast Disks" for structuring the broadcast in a way that provides improved performance for non-uniformly accessed data. The Broadcast Disk superimposes multiple disks spinning at different speeds on a single broadcast channel--in effect creating an arbitrarily fine-grained memory hierarchy. In addition to proposing and defining the mechanism, a main result of this work is that exploiting the potential of the broadcast structure requires a re-evaluation of basic cache management policies. We examine several "pure" cache management policies and develop and measure implementable approximations to these policies. These results and others are presented in a set of simulation studies that substantiates the basic idea and develops some of the intuitions required to design a particular broadcast program.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {199–210},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223816,
author = {Acharya, Swarup and Alonso, Rafael and Franklin, Michael and Zdonik, Stanley},
title = {Broadcast Disks: Data Management for Asymmetric Communication Environments},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223816},
doi = {10.1145/568271.223816},
abstract = {This paper proposes the use of repetitive broadcast as a way of augmenting the memory hierarchy of clients in an asymmetric communication environment. We describe a new technique called "Broadcast Disks" for structuring the broadcast in a way that provides improved performance for non-uniformly accessed data. The Broadcast Disk superimposes multiple disks spinning at different speeds on a single broadcast channel--in effect creating an arbitrarily fine-grained memory hierarchy. In addition to proposing and defining the mechanism, a main result of this work is that exploiting the potential of the broadcast structure requires a re-evaluation of basic cache management policies. We examine several "pure" cache management policies and develop and measure implementable approximations to these policies. These results and others are presented in a set of simulation studies that substantiates the basic idea and develops some of the intuitions required to design a particular broadcast program.},
journal = {SIGMOD Rec.},
month = may,
pages = {199–210},
numpages = {12}
}

@inproceedings{10.1145/223784.223817,
author = {Gupta, Ashish and Mumick, Inderpal S. and Ross, Kenneth A.},
title = {Adapting Materialized Views after Redefinitions},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223817},
doi = {10.1145/223784.223817},
abstract = {We consider a variant of the view maintenance problem: How does one keep a materialized view up-to-date when the view definition itself changes? Can one do better than recomputing the view from the base relations? Traditional view maintenance tries to maintain the materialized view in response to modifications to the base relations; we try to "adapt" the view in response to changes in the view definition.Such techniques are needed for applications where the user can change queries dynamically and see the changes in the results fast. Data archaeology, data visualization, and dynamic queries are examples of such applications.We consider all possible redefinitions of SQL SELECT-FROM-WHERE-GROUPBY, UNION, and EXCEPT views, and show how these views can be adapted using the old materialization for the cases where it is possible to do so. We identify extra information that can be kept with a materialization to facilitate redefinition. Multiple simultaneous changes to a view can be handled without necessarily materializing intermediate results. We identify guidelines for users and database administrators that can be used to facilitate efficient view adaptation.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {211–222},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223817,
author = {Gupta, Ashish and Mumick, Inderpal S. and Ross, Kenneth A.},
title = {Adapting Materialized Views after Redefinitions},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223817},
doi = {10.1145/568271.223817},
abstract = {We consider a variant of the view maintenance problem: How does one keep a materialized view up-to-date when the view definition itself changes? Can one do better than recomputing the view from the base relations? Traditional view maintenance tries to maintain the materialized view in response to modifications to the base relations; we try to "adapt" the view in response to changes in the view definition.Such techniques are needed for applications where the user can change queries dynamically and see the changes in the results fast. Data archaeology, data visualization, and dynamic queries are examples of such applications.We consider all possible redefinitions of SQL SELECT-FROM-WHERE-GROUPBY, UNION, and EXCEPT views, and show how these views can be adapted using the old materialization for the cases where it is possible to do so. We identify extra information that can be kept with a materialization to facilitate redefinition. Multiple simultaneous changes to a view can be handled without necessarily materializing intermediate results. We identify guidelines for users and database administrators that can be used to facilitate efficient view adaptation.},
journal = {SIGMOD Rec.},
month = may,
pages = {211–222},
numpages = {12}
}

@inproceedings{10.1145/223784.223840,
author = {Hou, Wen-Chi and Zhang, Zhongyang},
title = {Enhancing Database Correctness: A Statistical Approach},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223840},
doi = {10.1145/223784.223840},
abstract = {In this paper, we introduce a new type of integrity constraint, which we call a statistical constraint, and discuss its applicability to enhancing database correctness. Statistical constraints manifest embedded relationships among current attribute values in the database and are characterized by their probabilistic nature. They can be used to detect potential errors not easily detected by the conventional constraints. Methods for extracting statistical constraints from a relation and enforcement of such constraints are described. Preliminary performance evaluation of enforcing statistical constraints on a real life database is also presented.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {223–232},
numpages = {10},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223840,
author = {Hou, Wen-Chi and Zhang, Zhongyang},
title = {Enhancing Database Correctness: A Statistical Approach},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223840},
doi = {10.1145/568271.223840},
abstract = {In this paper, we introduce a new type of integrity constraint, which we call a statistical constraint, and discuss its applicability to enhancing database correctness. Statistical constraints manifest embedded relationships among current attribute values in the database and are characterized by their probabilistic nature. They can be used to detect potential errors not easily detected by the conventional constraints. Methods for extracting statistical constraints from a relation and enforcement of such constraints are described. Preliminary performance evaluation of enforcing statistical constraints on a real life database is also presented.},
journal = {SIGMOD Rec.},
month = may,
pages = {223–232},
numpages = {10}
}

@inproceedings{10.1145/223784.223841,
author = {Ioannidis, Yannis E. and Poosala, Viswanath},
title = {Balancing Histogram Optimality and Practicality for Query Result Size Estimation},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223841},
doi = {10.1145/223784.223841},
abstract = {Many current database systems use histograms to approximate the frequency distribution of values in the attributes of relations and based on them estimate query result sizes and access plan costs. In choosing among the various histograms, one has to balance between two conflicting goals: optimality, so that generated estimates have the least error, and practicality, so that histograms can be constructed and maintained efficiently. In this paper, we present both theoretical and experimental results on several issues related to this trade-off. Our overall conclusion is that the most effective approach is to focus on the class of histograms that accurately maintain the frequencies of a few attribute values and assume the uniform distribution for the rest, and choose for each relation the histogram in that class that is optimal for a self-join query.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {233–244},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223841,
author = {Ioannidis, Yannis E. and Poosala, Viswanath},
title = {Balancing Histogram Optimality and Practicality for Query Result Size Estimation},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223841},
doi = {10.1145/568271.223841},
abstract = {Many current database systems use histograms to approximate the frequency distribution of values in the attributes of relations and based on them estimate query result sizes and access plan costs. In choosing among the various histograms, one has to balance between two conflicting goals: optimality, so that generated estimates have the least error, and practicality, so that histograms can be constructed and maintained efficiently. In this paper, we present both theoretical and experimental results on several issues related to this trade-off. Our overall conclusion is that the most effective approach is to focus on the class of histograms that accurately maintain the frequencies of a few attribute values and assume the uniform distribution for the rest, and choose for each relation the histogram in that class that is optimal for a self-join query.},
journal = {SIGMOD Rec.},
month = may,
pages = {233–244},
numpages = {12}
}

@inproceedings{10.1145/223784.223842,
author = {Adelberg, B. and Garcia-Molina, H. and Kao, B.},
title = {Applying Update Streams in a Soft Real-Time Database System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223842},
doi = {10.1145/223784.223842},
abstract = {Many papers have examined how to efficiently export a materialized view but to our knowledge none have studied how to efficiently import one. To import a view, i.e., to install a stream of updates, a real-time database system must process new updates in a timely fashion to keep the database "fresh," but at the same time must process transactions and ensure they meet their time constraints. In this paper, we discuss the various properties of updates and views (including staleness) that affect this tradeoff. We also examine, through simulation, four algorithms for scheduling transactions and installing updates in a soft real-time database.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {245–256},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223842,
author = {Adelberg, B. and Garcia-Molina, H. and Kao, B.},
title = {Applying Update Streams in a Soft Real-Time Database System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223842},
doi = {10.1145/568271.223842},
abstract = {Many papers have examined how to efficiently export a materialized view but to our knowledge none have studied how to efficiently import one. To import a view, i.e., to install a stream of updates, a real-time database system must process new updates in a timely fashion to keep the database "fresh," but at the same time must process transactions and ensure they meet their time constraints. In this paper, we discuss the various properties of updates and views (including staleness) that affect this tradeoff. We also examine, through simulation, four algorithms for scheduling transactions and installing updates in a soft real-time database.},
journal = {SIGMOD Rec.},
month = may,
pages = {245–256},
numpages = {12}
}

@inproceedings{10.1145/223784.223843,
author = {Bettini, Claudio and Wang, X. Sean and Bertino, Elisa and Jajodia, Sushil},
title = {Semantic Assumptions and Query Evaluation in Temporal Databases},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223843},
doi = {10.1145/223784.223843},
abstract = {When querying a temporal database, a user often makes certain semantic assumptions on stored temporal data. This paper formalizes and studies two types of semantic assumptions: point-based and interval-based. The point-based assumptions include those assumptions that use interpolation methods, while the interval-based assumptions include those that involve different temporal types (time granularities). Each assumption is viewed as a way to derive certain implicit data from the explicit data stored in the database. The database system must use all explicit as well as (possibly infinite) implicit data to answer user queries. This paper introduces a new method to facilitate such query evaluations. A user query is translated into a system query such that the answer of this system query over the explicit data is the same as that of the user query over the explicit and the implicit data. The paper gives such a translation procedure and studies the properties (safety in particular) of user queries and system queries.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {257–268},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223843,
author = {Bettini, Claudio and Wang, X. Sean and Bertino, Elisa and Jajodia, Sushil},
title = {Semantic Assumptions and Query Evaluation in Temporal Databases},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223843},
doi = {10.1145/568271.223843},
abstract = {When querying a temporal database, a user often makes certain semantic assumptions on stored temporal data. This paper formalizes and studies two types of semantic assumptions: point-based and interval-based. The point-based assumptions include those assumptions that use interpolation methods, while the interval-based assumptions include those that involve different temporal types (time granularities). Each assumption is viewed as a way to derive certain implicit data from the explicit data stored in the database. The database system must use all explicit as well as (possibly infinite) implicit data to answer user queries. This paper introduces a new method to facilitate such query evaluations. A user query is translated into a system query such that the answer of this system query over the explicit data is the same as that of the user query over the explicit and the implicit data. The paper gives such a translation procedure and studies the properties (safety in particular) of user queries and system queries.},
journal = {SIGMOD Rec.},
month = may,
pages = {257–268},
numpages = {12}
}

@inproceedings{10.1145/223784.223844,
author = {Sistla, A. Prasad and Wolfson, Ouri},
title = {Temporal Conditions and Integrity Constraints in Active Database Systems},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223844},
doi = {10.1145/223784.223844},
abstract = {In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {269–280},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223844,
author = {Sistla, A. Prasad and Wolfson, Ouri},
title = {Temporal Conditions and Integrity Constraints in Active Database Systems},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223844},
doi = {10.1145/568271.223844},
abstract = {In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.},
journal = {SIGMOD Rec.},
month = may,
pages = {269–280},
numpages = {12}
}

@inproceedings{10.1145/223784.223845,
author = {Davison, Diane L. and Graefe, Goetz},
title = {Dynamic Resource Brokering for Multi-User Query Execution},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223845},
doi = {10.1145/223784.223845},
abstract = {We propose a new framework for resource allocation based on concepts from microeconomics. Specifically, we address the difficult problem of managing resources in a multiple-query environment composed of queries with widely varying resource requirements. The central element of the framework is a resource broker that realizes a profit by "selling" resources to competing operators using a performance-based "currency." The guiding principle for brokering resources is profit maximization. In other words, since the currency is derived from the performance objective, the broker can achieve the best performance by making the scheduling and resource allocation decisions that maximize profit. Moreover, the broker employs dynamic techniques and adapts by changing previous allocation decisions while queries are executing. In a first validation study of the framework, we developed a prototype broker that manages memory and disk bandwidth for a multi-user query workload. The performance objective for the prototype broker is to minimize slowdown with the constraint of fairness. Slowdown measures how much higher the response time is in a multi-user environment than a single-user environment, and fairness measures how even is the degradation in response time among all queries as the system load increases, Our simulation results show the viability of the broker framework and the effectiveness of our query admission and resource allocation policies for multi-user workloads.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {281–292},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223845,
author = {Davison, Diane L. and Graefe, Goetz},
title = {Dynamic Resource Brokering for Multi-User Query Execution},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223845},
doi = {10.1145/568271.223845},
abstract = {We propose a new framework for resource allocation based on concepts from microeconomics. Specifically, we address the difficult problem of managing resources in a multiple-query environment composed of queries with widely varying resource requirements. The central element of the framework is a resource broker that realizes a profit by "selling" resources to competing operators using a performance-based "currency." The guiding principle for brokering resources is profit maximization. In other words, since the currency is derived from the performance objective, the broker can achieve the best performance by making the scheduling and resource allocation decisions that maximize profit. Moreover, the broker employs dynamic techniques and adapts by changing previous allocation decisions while queries are executing. In a first validation study of the framework, we developed a prototype broker that manages memory and disk bandwidth for a multi-user query workload. The performance objective for the prototype broker is to minimize slowdown with the constraint of fairness. Slowdown measures how much higher the response time is in a multi-user environment than a single-user environment, and fairness measures how even is the degradation in response time among all queries as the system load increases, Our simulation results show the viability of the broker framework and the effectiveness of our query admission and resource allocation policies for multi-user workloads.},
journal = {SIGMOD Rec.},
month = may,
pages = {281–292},
numpages = {12}
}

@inproceedings{10.1145/223784.223846,
author = {Du, Weimin and Shan, Ming-Chien and Dayal, Umeshwar},
title = {Reducing Multidatabase Query Response Time by Tree Balancing},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223846},
doi = {10.1145/223784.223846},
abstract = {Execution of multidatabase queries differs from that of traditional queries in that sort merge and hash joins are more often favored, as nested loop join requires repeated accesses to external data sources. As a consequence, left deep join trees obtained by traditional (e.g., System-R style) optimizers for multidatabase queries are often suboptimal, with respect to response time, due to the long delay for a sort merge (or hash) join node to produce its last result after the subordinate join node did. In this paper, we present an optimization strategy that first produces an optimal left deep join tree and then reduces the response time using simple tree transformations. This strategy has the advantages of guaranteed minimum total resource usage, improved response time, and low optimization overhead. We describe a class of basic transformations that is the cornerstone of our approach. Then we present algorithms that effectively apply basic transformations to balance a left deep join tree, and discuss how the technique can be incorporated into existing query optimizers.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {293–303},
numpages = {11},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223846,
author = {Du, Weimin and Shan, Ming-Chien and Dayal, Umeshwar},
title = {Reducing Multidatabase Query Response Time by Tree Balancing},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223846},
doi = {10.1145/568271.223846},
abstract = {Execution of multidatabase queries differs from that of traditional queries in that sort merge and hash joins are more often favored, as nested loop join requires repeated accesses to external data sources. As a consequence, left deep join trees obtained by traditional (e.g., System-R style) optimizers for multidatabase queries are often suboptimal, with respect to response time, due to the long delay for a sort merge (or hash) join node to produce its last result after the subordinate join node did. In this paper, we present an optimization strategy that first produces an optimal left deep join tree and then reduces the response time using simple tree transformations. This strategy has the advantages of guaranteed minimum total resource usage, improved response time, and low optimization overhead. We describe a class of basic transformations that is the cornerstone of our approach. Then we present algorithms that effectively apply basic transformations to balance a left deep join tree, and discuss how the technique can be incorporated into existing query optimizers.},
journal = {SIGMOD Rec.},
month = may,
pages = {293–303},
numpages = {11}
}

@inproceedings{10.1145/223784.223847,
author = {Bhargava, Gautam and Goel, Piyush and Iyer, Bala},
title = {Hypergraph Based Reorderings of Outer Join Queries with Complex Predicates},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223847},
doi = {10.1145/223784.223847},
abstract = {Complex queries containing outer joins are, for the most part, executed by commercial DBMS products in an "as written" manner. Only a very few reorderings of the operations are considered and the benefits of considering comprehensive reordering schemes are not exploited. This is largely due to the fact there are no readily usable results for reordering such operations for relations with duplicates and/or outer join predicates that are other than "simple." Most previous approaches have ignored duplicates and complex predicates; the very few that have considered these aspects have suggested approaches that lead to a possibly exponential number of, and redundant intermediate joins. Since traditional query graph models are inadequate for modeling outer join queries with complex predicates, we present the needed hypergraph abstraction and algorithms for reordering such queries with joins and outer joins. As a result, the query optimizer can explore a significantly larger space of execution plans, and choose one with a low cost. Further, these algorithms are easily incorporated into well known and widely used enumeration methods such as dynamic programming.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {304–315},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223847,
author = {Bhargava, Gautam and Goel, Piyush and Iyer, Bala},
title = {Hypergraph Based Reorderings of Outer Join Queries with Complex Predicates},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223847},
doi = {10.1145/568271.223847},
abstract = {Complex queries containing outer joins are, for the most part, executed by commercial DBMS products in an "as written" manner. Only a very few reorderings of the operations are considered and the benefits of considering comprehensive reordering schemes are not exploited. This is largely due to the fact there are no readily usable results for reordering such operations for relations with duplicates and/or outer join predicates that are other than "simple." Most previous approaches have ignored duplicates and complex predicates; the very few that have considered these aspects have suggested approaches that lead to a possibly exponential number of, and redundant intermediate joins. Since traditional query graph models are inadequate for modeling outer join queries with complex predicates, we present the needed hypergraph abstraction and algorithms for reordering such queries with joins and outer joins. As a result, the query optimizer can explore a significantly larger space of execution plans, and choose one with a low cost. Further, these algorithms are easily incorporated into well known and widely used enumeration methods such as dynamic programming.},
journal = {SIGMOD Rec.},
month = may,
pages = {304–315},
numpages = {12}
}

@inproceedings{10.1145/223784.223848,
author = {Zhuge, Yue and Garc\'{\i}a-Molina, H\'{e}ctor and Hammer, Joachim and Widom, Jennifer},
title = {View Maintenance in a Warehousing Environment},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223848},
doi = {10.1145/223784.223848},
abstract = {A warehouse is a repository of integrated information drawn from remote data sources. Since a warehouse effectively implements materialized views, we must maintain the views as the data sources are updated. This view maintenance problem differs from the traditional one in that the view definition and the base data are now decoupled. We show that this decoupling can result in anomalies if traditional algorithms are applied. We introduce a new algorithm, ECA (for "Eager Compensating Algorithm"), that eliminates the anomalies. ECA is based on previous incremental view maintenance algorithms, but extra "compensating" queries are used to eliminate anomalies. We also introduce two streamlined versions of ECA for special cases of views and updates, and we present an initial performance study that compares ECA to a view recomputation algorithm in terms of messages transmitted, data transferred, and I/O costs.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {316–327},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223848,
author = {Zhuge, Yue and Garc\'{\i}a-Molina, H\'{e}ctor and Hammer, Joachim and Widom, Jennifer},
title = {View Maintenance in a Warehousing Environment},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223848},
doi = {10.1145/568271.223848},
abstract = {A warehouse is a repository of integrated information drawn from remote data sources. Since a warehouse effectively implements materialized views, we must maintain the views as the data sources are updated. This view maintenance problem differs from the traditional one in that the view definition and the base data are now decoupled. We show that this decoupling can result in anomalies if traditional algorithms are applied. We introduce a new algorithm, ECA (for "Eager Compensating Algorithm"), that eliminates the anomalies. ECA is based on previous incremental view maintenance algorithms, but extra "compensating" queries are used to eliminate anomalies. We also introduce two streamlined versions of ECA for special cases of views and updates, and we present an initial performance study that compares ECA to a view recomputation algorithm in terms of messages transmitted, data transferred, and I/O costs.},
journal = {SIGMOD Rec.},
month = may,
pages = {316–327},
numpages = {12}
}

@inproceedings{10.1145/223784.223849,
author = {Griffin, Timothy and Libkin, Leonid},
title = {Incremental Maintenance of Views with Duplicates},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223849},
doi = {10.1145/223784.223849},
abstract = {We study the problem of efficient maintenance of materialized views that may contain duplicates. This problem is particularly important when queries against such views involve aggregate functions, which need duplicates to produce correct results. Unlike most work on the view maintenance problem that is based on an algorithmic approach, our approach is algebraic and based on equational reasoning. This approach has a number of advantages: it is robust and easily extendible to new language constructs, it produces output that can be used by query optimizers, and it simplifies correctness proofs.We use a natural extension of the relational algebra operations to bags (multisets) as our basic language. We present an algorithm that propagates changes from base relations to materialized views. This algorithm is based on reasoning about equivalence of bag-valued expressions. We prove that it is correct and preserves a certain notion of minimality that ensures that no unnecessary tuples are computed. Although it is generally only a heuristic that computing changes to the view rather than recomputing the view from scratch is more efficient, we prove results saying that under normal circumstances one should expect, the change propagation algorithm to be significantly faster and more space efficient than complete recomputing of the view. We also show that our approach interacts nicely with aggregate functions, allowing their correct evaluation on views that change.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {328–339},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223849,
author = {Griffin, Timothy and Libkin, Leonid},
title = {Incremental Maintenance of Views with Duplicates},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223849},
doi = {10.1145/568271.223849},
abstract = {We study the problem of efficient maintenance of materialized views that may contain duplicates. This problem is particularly important when queries against such views involve aggregate functions, which need duplicates to produce correct results. Unlike most work on the view maintenance problem that is based on an algorithmic approach, our approach is algebraic and based on equational reasoning. This approach has a number of advantages: it is robust and easily extendible to new language constructs, it produces output that can be used by query optimizers, and it simplifies correctness proofs.We use a natural extension of the relational algebra operations to bags (multisets) as our basic language. We present an algorithm that propagates changes from base relations to materialized views. This algorithm is based on reasoning about equivalence of bag-valued expressions. We prove that it is correct and preserves a certain notion of minimality that ensures that no unnecessary tuples are computed. Although it is generally only a heuristic that computing changes to the view rather than recomputing the view from scratch is more efficient, we prove results saying that under normal circumstances one should expect, the change propagation algorithm to be significantly faster and more space efficient than complete recomputing of the view. We also show that our approach interacts nicely with aggregate functions, allowing their correct evaluation on views that change.},
journal = {SIGMOD Rec.},
month = may,
pages = {328–339},
numpages = {12}
}

@inproceedings{10.1145/223784.223850,
author = {Lu, James J. and Moerkotte, Guido and Schue, Joachim and Subrahmanian, V. S.},
title = {Efficient Maintenance of Materialized Mediated Views},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223850},
doi = {10.1145/223784.223850},
abstract = {Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {340–351},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223850,
author = {Lu, James J. and Moerkotte, Guido and Schue, Joachim and Subrahmanian, V. S.},
title = {Efficient Maintenance of Materialized Mediated Views},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223850},
doi = {10.1145/568271.223850},
abstract = {Integrating data and knowledge from multiple heterogeneous sources -- like databases, knowledge bases or specific software packages -- is often required for answering certain queries. Recently, a powerful framework for defining mediated views spanning multiple knowledge bases by a set of constrained rules was proposed [24, 4, 16]. We investigate the materialization of these views by unfolding the view definition and the efficient maintenance of the resulting materialized mediated view in case of updates. Thereby, we consider two kinds of updates: updates to the view and updates to the underlying sources. For each of these two cases several efficient algorithms maintaining materialized mediated views are given. We improve on previous algorithms like the DRed algorithm [12] and introduce a new fixpoint operator WP which -- opposed to the standard fixpoint operator TP [9] -- allows us to correctly capture the update's semantics without any recomputation of the materialized view.},
journal = {SIGMOD Rec.},
month = may,
pages = {340–351},
numpages = {12}
}

@inproceedings{10.1145/223784.223851,
author = {Freedman, Craig S. and DeWitt, David J.},
title = {The SPIFFI Scalable Video-on-Demand System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223851},
doi = {10.1145/223784.223851},
abstract = {This paper presents a simulation study of a video-on-demand system. We present video server algorithms for real-time disk scheduling, prefetching, and buffer pool management. The performance of these algorithms is compared against the performance of simpler algorithms such as elevator and round-robin disk scheduling and global LRU buffer pool management. Finally, we show that the SPIFFI video-on-demand system scales nearly linearly as the number of disks, videos, and terminals is increased.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {352–363},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223851,
author = {Freedman, Craig S. and DeWitt, David J.},
title = {The SPIFFI Scalable Video-on-Demand System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223851},
doi = {10.1145/568271.223851},
abstract = {This paper presents a simulation study of a video-on-demand system. We present video server algorithms for real-time disk scheduling, prefetching, and buffer pool management. The performance of these algorithms is compared against the performance of simpler algorithms such as elevator and round-robin disk scheduling and global LRU buffer pool management. Finally, we show that the SPIFFI video-on-demand system scales nearly linearly as the number of disks, videos, and terminals is increased.},
journal = {SIGMOD Rec.},
month = may,
pages = {352–363},
numpages = {12}
}

@inproceedings{10.1145/223784.223852,
author = {Berson, Steven and Golubchik, Leana and Muntz, Richard R.},
title = {Fault Tolerant Design of Multimedia Servers},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223852},
doi = {10.1145/223784.223852},
abstract = {Recent technological advances have made multimedia on-demand servers feasible. Two challenging tasks in such systems are: a) satisfying the real-time requirement for continuous delivery of objects at specified bandwidths and b) efficiently servicing multiple clients simultaneously. To accomplish these tasks and realize economies of scale associated with servicing a large user population, the multimedia server can require a large disk subsystem. Although a single disk is fairly reliable, a large disk farm can have an unacceptably high probability of disk failure. Further, due to the real-time constraint, the reliability and availability requirements of multimedia systems are very stringent. In this paper we investigate techniques for providing a high degree of reliability and availability, at low disk storage, bandwidth, and memory costs for on-demand multimedia servers.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {364–375},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223852,
author = {Berson, Steven and Golubchik, Leana and Muntz, Richard R.},
title = {Fault Tolerant Design of Multimedia Servers},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223852},
doi = {10.1145/568271.223852},
abstract = {Recent technological advances have made multimedia on-demand servers feasible. Two challenging tasks in such systems are: a) satisfying the real-time requirement for continuous delivery of objects at specified bandwidths and b) efficiently servicing multiple clients simultaneously. To accomplish these tasks and realize economies of scale associated with servicing a large user population, the multimedia server can require a large disk subsystem. Although a single disk is fairly reliable, a large disk farm can have an unacceptably high probability of disk failure. Further, due to the real-time constraint, the reliability and availability requirements of multimedia systems are very stringent. In this paper we investigate techniques for providing a high degree of reliability and availability, at low disk storage, bandwidth, and memory costs for on-demand multimedia servers.},
journal = {SIGMOD Rec.},
month = may,
pages = {364–375},
numpages = {12}
}

@inproceedings{10.1145/223784.223853,
author = {Dan, Asit and Sitaram, Dinkar},
title = {An Online Video Placement Policy Based on Bandwidth to Space Ratio (BSR)},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223853},
doi = {10.1145/223784.223853},
abstract = {In a video-on-demand server, resource reservation is required to guarantee continuous delivery. Hence any given storage device (or a striping group treated as a single logical device) can serve only up to a fixed number of client access streams. Each storage device is also limited by the number of video files it can store. For the reasons of availability, incremental growth, and heterogeneity, there may be multiple storage devices in a video server environment. Hence, one or more copies of a particular video may be placed on different storage devices. Since the access rates to different videos are not uniform, there may be load imbalance among the devices. In this paper, we propose a dynamic placement policy (called the Bandwidth to Space Ratio (BSR) Policy) that creates and/or deletes replica of a video, and mixes hot and cold videos so as to make the best use of bandwidth and space of a storage device. The proposed policy is evaluated using a simulation study.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {376–385},
numpages = {10},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223853,
author = {Dan, Asit and Sitaram, Dinkar},
title = {An Online Video Placement Policy Based on Bandwidth to Space Ratio (BSR)},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223853},
doi = {10.1145/568271.223853},
abstract = {In a video-on-demand server, resource reservation is required to guarantee continuous delivery. Hence any given storage device (or a striping group treated as a single logical device) can serve only up to a fixed number of client access streams. Each storage device is also limited by the number of video files it can store. For the reasons of availability, incremental growth, and heterogeneity, there may be multiple storage devices in a video server environment. Hence, one or more copies of a particular video may be placed on different storage devices. Since the access rates to different videos are not uniform, there may be load imbalance among the devices. In this paper, we propose a dynamic placement policy (called the Bandwidth to Space Ratio (BSR) Policy) that creates and/or deletes replica of a video, and mixes hot and cold videos so as to make the best use of bandwidth and space of a storage device. The proposed policy is evaluated using a simulation study.},
journal = {SIGMOD Rec.},
month = may,
pages = {376–385},
numpages = {10}
}

@inproceedings{10.1145/223784.223854,
author = {Abiteboul, Serge and Cluet, Sophie and Milo, Tova},
title = {A Database Interface for File Update},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223854},
doi = {10.1145/223784.223854},
abstract = {Database systems are concerned with structured data. Unfortunately, data is still often available in an unstructured manner (e.g., in files) even when it does have a strong internal structure (e.g., electronic documents or programs). In a previous paper [2], we focussed on the use of high-level query languages to access such files and developed optimization techniques to do so. In this paper, we consider how structured data stored in files can be updated using database update languages.The interest of using database languages to manipulate files is twofold. First, it opens database systems to external data. This concerns data residing in files or data transiting on communication channels and possibly coming from other databases [2]. Secondly, it provides high level query/update facilities to systems that usually rely on very primitive linguistic support. (See [6] for recent works in this direction). Similar motivations appear in [4, 5, 7, 8, 11, 12, 13, 14, 15, 17, 19, 20, 21]In a previous paper, we introduced the notion of structuring schemas as a mean of providing a database view on structured data residing in a file. A structuring schema consists of a grammar together with semantic actions (in a database language). We also showed how queries on files expressed in a high-level query language (O2-SQL [3]) could be evaluated efficiently using variations of standard database optimization techniques. The problem of update was mentioned there but remained largely unexplored. This is the topic of the present paper.We argue that updates on files can be expressed conveniently using high-level database update languages that work on the database view of the file. The key problem is how to propagate an update specified on the database (here a view) to the file (here the physical storage). As a first step, we propose a naive way of update propagation: the database view of the file is materialized; the update is performed on the database; the database is "unparsed" to produce an updated file. For this, we develop an unparsing technique. The problems that we meet while developing this technique are related to the well-known view update problem. ( See, for instance [9, 10, 16, 23].) The technique relies on the existence of an inverse mapping from the database to the file. We show that the existence of such an inverse mapping results from the use of restricted structuring schemas.The naive technique presents two major drawbacks. It is inefficient: it entails intense data construction and unparsing, most of which dealing with data not involved in the update. It may result in information loss: information in the file, that is not recorded in the database, may be lost in the process. The major contribution of this paper is a combination of techniques that allows to minimize both the data construction and the unparsing work. First, we briefly show how optimization techniques from [2] can be used to focus on the relevant portion of the database and to avoid constructing the entire database. Then we show that for a class of structuring schemas satisfying a locality condition, it is possible to carefully circumscribe the unparsing.Some of the results in the paper are negative. They should not come as a surprise since we are dealing with complex theoretical foundations: language theory (for parsing and unparsing), and first-order logic (for database languages). However, we do present positive results for particular classes of structuring schemas. We believe that the restrictions imposed on these schemas are very acceptable in practice. (For instance, all "real" examples of structuring schemas that we examined are local.)The paper is organized as follows. In Section 2, we present the update problem and the structuring schemas; in Section 3, a naive technique for update propagation and the unparsing technique. Section 4 introduces a locality condition, and presents a more efficient technique for propagating updates in local structuring schemas. The last section is a conclusion.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {386–397},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223854,
author = {Abiteboul, Serge and Cluet, Sophie and Milo, Tova},
title = {A Database Interface for File Update},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223854},
doi = {10.1145/568271.223854},
abstract = {Database systems are concerned with structured data. Unfortunately, data is still often available in an unstructured manner (e.g., in files) even when it does have a strong internal structure (e.g., electronic documents or programs). In a previous paper [2], we focussed on the use of high-level query languages to access such files and developed optimization techniques to do so. In this paper, we consider how structured data stored in files can be updated using database update languages.The interest of using database languages to manipulate files is twofold. First, it opens database systems to external data. This concerns data residing in files or data transiting on communication channels and possibly coming from other databases [2]. Secondly, it provides high level query/update facilities to systems that usually rely on very primitive linguistic support. (See [6] for recent works in this direction). Similar motivations appear in [4, 5, 7, 8, 11, 12, 13, 14, 15, 17, 19, 20, 21]In a previous paper, we introduced the notion of structuring schemas as a mean of providing a database view on structured data residing in a file. A structuring schema consists of a grammar together with semantic actions (in a database language). We also showed how queries on files expressed in a high-level query language (O2-SQL [3]) could be evaluated efficiently using variations of standard database optimization techniques. The problem of update was mentioned there but remained largely unexplored. This is the topic of the present paper.We argue that updates on files can be expressed conveniently using high-level database update languages that work on the database view of the file. The key problem is how to propagate an update specified on the database (here a view) to the file (here the physical storage). As a first step, we propose a naive way of update propagation: the database view of the file is materialized; the update is performed on the database; the database is "unparsed" to produce an updated file. For this, we develop an unparsing technique. The problems that we meet while developing this technique are related to the well-known view update problem. ( See, for instance [9, 10, 16, 23].) The technique relies on the existence of an inverse mapping from the database to the file. We show that the existence of such an inverse mapping results from the use of restricted structuring schemas.The naive technique presents two major drawbacks. It is inefficient: it entails intense data construction and unparsing, most of which dealing with data not involved in the update. It may result in information loss: information in the file, that is not recorded in the database, may be lost in the process. The major contribution of this paper is a combination of techniques that allows to minimize both the data construction and the unparsing work. First, we briefly show how optimization techniques from [2] can be used to focus on the relevant portion of the database and to avoid constructing the entire database. Then we show that for a class of structuring schemas satisfying a locality condition, it is possible to carefully circumscribe the unparsing.Some of the results in the paper are negative. They should not come as a surprise since we are dealing with complex theoretical foundations: language theory (for parsing and unparsing), and first-order logic (for database languages). However, we do present positive results for particular classes of structuring schemas. We believe that the restrictions imposed on these schemas are very acceptable in practice. (For instance, all "real" examples of structuring schemas that we examined are local.)The paper is organized as follows. In Section 2, we present the update problem and the structuring schemas; in Section 3, a naive technique for update propagation and the unparsing technique. Section 4 introduces a locality condition, and presents a more efficient technique for propagating updates in local structuring schemas. The last section is a conclusion.},
journal = {SIGMOD Rec.},
month = may,
pages = {386–397},
numpages = {12}
}

@inproceedings{10.1145/223784.223855,
author = {Brin, Sergey and Davis, James and Garc\'{\i}a-Molina, H\'{e}ctor},
title = {Copy Detection Mechanisms for Digital Documents},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223855},
doi = {10.1145/223784.223855},
abstract = {In a digital library system, documents are available in digital form and therefore are more easily copied and their copyrights are more easily violated. This is a very serious problem, as it discourages owners of valuable information from sharing it with authorized users. There are two main philosophies for addressing this problem: prevention and detection. The former actually makes unauthorized use of documents difficult or impossible while the latter makes it easier to discover such activity.In this paper we propose a system for registering documents and then detecting copies, either complete copies or partial copies. We describe algorithms for such detection, and metrics required for evaluating detection mechanisms (covering accuracy, efficiency, and security). We also describe a working prototype, called COPS, describe implementation issues, and present experimental results that suggest the proper settings for copy detection parameters.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {398–409},
numpages = {12},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223855,
author = {Brin, Sergey and Davis, James and Garc\'{\i}a-Molina, H\'{e}ctor},
title = {Copy Detection Mechanisms for Digital Documents},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223855},
doi = {10.1145/568271.223855},
abstract = {In a digital library system, documents are available in digital form and therefore are more easily copied and their copyrights are more easily violated. This is a very serious problem, as it discourages owners of valuable information from sharing it with authorized users. There are two main philosophies for addressing this problem: prevention and detection. The former actually makes unauthorized use of documents difficult or impossible while the latter makes it easier to discover such activity.In this paper we propose a system for registering documents and then detecting copies, either complete copies or partial copies. We describe algorithms for such detection, and metrics required for evaluating detection mechanisms (covering accuracy, efficiency, and security). We also describe a working prototype, called COPS, describe implementation issues, and present experimental results that suggest the proper settings for copy detection parameters.},
journal = {SIGMOD Rec.},
month = may,
pages = {398–409},
numpages = {12}
}

@inproceedings{10.1145/223784.223856,
author = {Chaudhuri, Surajit and Dayal, Umeshwar and Yan, Tak W.},
title = {Join Queries with External Text Sources: Execution and Optimization Techniques},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223856},
doi = {10.1145/223784.223856},
abstract = {Text is a pervasive information type, and many applications require querying over text sources in addition to structured data. This paper studies the problem of query processing in a system that loosely integrates an extensible database system and a text retrieval system. We focus on a class of conjunctive queries that include joins between text and structured data, in addition to selections over these two types of data. We adapt techniques from distributed query processing and introduce a novel class of join methods based on probing that is especially useful for joins with text systems, and we present a cost model for the various alternative query processing methods. Experimental results confirm the utility of these methods. The space of query plans is extended due to the additional techniques, and we describe an optimization algorithm for searching this extended space. The techniques we describe in this paper are applicable to other types of external data managers loosely integrated with a database system.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {410–422},
numpages = {13},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223856,
author = {Chaudhuri, Surajit and Dayal, Umeshwar and Yan, Tak W.},
title = {Join Queries with External Text Sources: Execution and Optimization Techniques},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223856},
doi = {10.1145/568271.223856},
abstract = {Text is a pervasive information type, and many applications require querying over text sources in addition to structured data. This paper studies the problem of query processing in a system that loosely integrates an extensible database system and a text retrieval system. We focus on a class of conjunctive queries that include joins between text and structured data, in addition to selections over these two types of data. We adapt techniques from distributed query processing and introduce a novel class of join methods based on probing that is especially useful for joins with text systems, and we present a cost model for the various alternative query processing methods. Experimental results confirm the utility of these methods. The space of query plans is extended due to the additional techniques, and we describe an optimization algorithm for searching this extended space. The techniques we describe in this paper are applicable to other types of external data managers loosely integrated with a database system.},
journal = {SIGMOD Rec.},
month = may,
pages = {410–422},
numpages = {13}
}

@inproceedings{10.1145/223784.223857,
author = {Fox Development Team Microsoft, CORPORATE},
title = {Object-Oriented, Rapid Application Development in a PC Database Environment},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223857},
doi = {10.1145/223784.223857},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {423–424},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223857,
author = {Fox Development Team Microsoft, CORPORATE},
title = {Object-Oriented, Rapid Application Development in a PC Database Environment},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223857},
doi = {10.1145/568271.223857},
journal = {SIGMOD Rec.},
month = may,
pages = {423–424},
numpages = {2}
}

@inproceedings{10.1145/223784.223858,
author = {The Access Team Microsoft, CORPORATE},
title = {Upsizing Form File Server to Client Server Architectures},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223858},
doi = {10.1145/223784.223858},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {425–426},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223858,
author = {The Access Team Microsoft, CORPORATE},
title = {Upsizing Form File Server to Client Server Architectures},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223858},
doi = {10.1145/568271.223858},
journal = {SIGMOD Rec.},
month = may,
pages = {425–426},
numpages = {2}
}

@inproceedings{10.1145/223784.223859,
author = {Moore, Kenneth},
title = {The Lotus Notes Storage System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223859},
doi = {10.1145/223784.223859},
abstract = {Lotus Notes is a commercial product that empowers individuals and organizations to collaborate and share information [1].Notes enables the easy development of applications such as messaging, document management, workflow, and asynchronous conferencing. Notes applications can be deployed globally, across independent organizations, among a heterogeneous network of loosely coupled computers that range in size from small notebooks to large multi-processor systems.The third major release of Lotus Notes occurred in May 1993. Notes is a client-server product, with clients available on Windows, OS/2, Macintosh, SCO UNIX, HP-UX, AIX, and Solaris. The server is available on Windows, OS/2, Windows NT (for Intel processors), NetWare, SCO UNIX, HP-UX, AIX, and Solaris.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {427–428},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223859,
author = {Moore, Kenneth},
title = {The Lotus Notes Storage System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223859},
doi = {10.1145/568271.223859},
abstract = {Lotus Notes is a commercial product that empowers individuals and organizations to collaborate and share information [1].Notes enables the easy development of applications such as messaging, document management, workflow, and asynchronous conferencing. Notes applications can be deployed globally, across independent organizations, among a heterogeneous network of loosely coupled computers that range in size from small notebooks to large multi-processor systems.The third major release of Lotus Notes occurred in May 1993. Notes is a client-server product, with clients available on Windows, OS/2, Macintosh, SCO UNIX, HP-UX, AIX, and Solaris. The server is available on Windows, OS/2, Windows NT (for Intel processors), NetWare, SCO UNIX, HP-UX, AIX, and Solaris.},
journal = {SIGMOD Rec.},
month = may,
pages = {427–428},
numpages = {2}
}

@inproceedings{10.1145/223784.223860,
author = {Gettys, William L.},
title = {DIRECTV and Oracle Rdb: The Challenge of VLDB Transaction Processing},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223860},
doi = {10.1145/223784.223860},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {429–430},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223860,
author = {Gettys, William L.},
title = {DIRECTV and Oracle Rdb: The Challenge of VLDB Transaction Processing},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223860},
doi = {10.1145/568271.223860},
journal = {SIGMOD Rec.},
month = may,
pages = {429–430},
numpages = {2}
}

@inproceedings{10.1145/223784.223861,
author = {Hope, Greg},
title = {Enterprise Transaction Processing on Windows NT},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223861},
doi = {10.1145/223784.223861},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {431–432},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223861,
author = {Hope, Greg},
title = {Enterprise Transaction Processing on Windows NT},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223861},
doi = {10.1145/568271.223861},
journal = {SIGMOD Rec.},
month = may,
pages = {431–432},
numpages = {2}
}

@inproceedings{10.1145/223784.223862,
author = {Ivinskis, Kestutis},
title = {High Availability of Commercial Applications},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223862},
doi = {10.1145/223784.223862},
abstract = {The increased performance capabilities of UNIX server systems have led to their acceptance as the server of choice for medium-sized and large organizations. But performance is just one facet. Another facet is the end user perception of the availability of an information system.Traditional mainframe based IS shops have a long experience in supplying computing services to their commercial end users. The ultimate goal of the end user is to have no downtimes for his work at his PC or workstation terminal. Key issues related to system availability in client/server based information systems remain the same as in the mainframe based world, e.g. system responsiveness, maximum downtime per year and maximum number of system outages per year. But there are also new aspects, which have been introduced into the discussion.In a multi-tiered client/server based information system the OLTP workload is distributed on different servers. Hence one can ask: Why should a failure of one server automatically imply downtime for the whole system ? Can't most of the system continue to operate ? Redistribution of the workload on the remaining active servers can be used to attack this problem. Workload balancing can be applied for replicated system services. Other techniques have to be applied for non-replicated system services.This paper considers client/server based applications running in a local or wide area network of computers in a distributed system.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {433–434},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223862,
author = {Ivinskis, Kestutis},
title = {High Availability of Commercial Applications},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223862},
doi = {10.1145/568271.223862},
abstract = {The increased performance capabilities of UNIX server systems have led to their acceptance as the server of choice for medium-sized and large organizations. But performance is just one facet. Another facet is the end user perception of the availability of an information system.Traditional mainframe based IS shops have a long experience in supplying computing services to their commercial end users. The ultimate goal of the end user is to have no downtimes for his work at his PC or workstation terminal. Key issues related to system availability in client/server based information systems remain the same as in the mainframe based world, e.g. system responsiveness, maximum downtime per year and maximum number of system outages per year. But there are also new aspects, which have been introduced into the discussion.In a multi-tiered client/server based information system the OLTP workload is distributed on different servers. Hence one can ask: Why should a failure of one server automatically imply downtime for the whole system ? Can't most of the system continue to operate ? Redistribution of the workload on the remaining active servers can be used to attack this problem. Workload balancing can be applied for replicated system services. Other techniques have to be applied for non-replicated system services.This paper considers client/server based applications running in a local or wide area network of computers in a distributed system.},
journal = {SIGMOD Rec.},
month = may,
pages = {433–434},
numpages = {2}
}

@inproceedings{10.1145/223784.223863,
author = {Buneman, Peter and Maier, David},
title = {The Data That You Won't Find in Databases: Tutorial Panel on Data Exchange Formats},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223863},
doi = {10.1145/223784.223863},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {435},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223863,
author = {Buneman, Peter and Maier, David},
title = {The Data That You Won't Find in Databases: Tutorial Panel on Data Exchange Formats},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223863},
doi = {10.1145/568271.223863},
journal = {SIGMOD Rec.},
month = may,
pages = {435},
numpages = {1}
}

@inproceedings{10.1145/223784.223864,
author = {Gray, Jim},
title = {Parallel Database Systems 101},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223864},
doi = {10.1145/223784.223864},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {436},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223864,
author = {Gray, Jim},
title = {Parallel Database Systems 101},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223864},
doi = {10.1145/568271.223864},
journal = {SIGMOD Rec.},
month = may,
pages = {436},
numpages = {1}
}

@inproceedings{10.1145/223784.277954,
author = {Ellison, Larry J.},
title = {Keynote Address},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.277954},
doi = {10.1145/223784.277954},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {437},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.277954,
author = {Ellison, Larry J.},
title = {Keynote Address},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.277954},
doi = {10.1145/568271.277954},
journal = {SIGMOD Rec.},
month = may,
pages = {437},
numpages = {1}
}

@inproceedings{10.1145/223784.277955,
author = {Epstein, Robert S.},
title = {Keynote Address},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.277955},
doi = {10.1145/223784.277955},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {438},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.277955,
author = {Epstein, Robert S.},
title = {Keynote Address},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.277955},
doi = {10.1145/568271.277955},
journal = {SIGMOD Rec.},
month = may,
pages = {438},
numpages = {1}
}

@inproceedings{10.1145/223784.223865,
author = {Goldring, Rob},
title = {Things Every Update Replication Customer Should Know (Abstract)},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223865},
doi = {10.1145/223784.223865},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {439–440},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223865,
author = {Goldring, Rob},
title = {Things Every Update Replication Customer Should Know (Abstract)},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223865},
doi = {10.1145/568271.223865},
journal = {SIGMOD Rec.},
month = may,
pages = {439–440},
numpages = {2}
}

@inproceedings{10.1145/223784.223866,
author = {Shyy, Yuh-Ming and Au-Yeung, H. Stephen and Chou, C. P.},
title = {VERSANT Replication: Supporting Fault-Tolerant Object Databases},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223866},
doi = {10.1145/223784.223866},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {441–442},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223866,
author = {Shyy, Yuh-Ming and Au-Yeung, H. Stephen and Chou, C. P.},
title = {VERSANT Replication: Supporting Fault-Tolerant Object Databases},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223866},
doi = {10.1145/568271.223866},
journal = {SIGMOD Rec.},
month = may,
pages = {441–442},
numpages = {2}
}

@inproceedings{10.1145/223784.223867,
author = {Woelk, D. and Bohrer, B. and Jacobs, N. and Ong, K. and Tomlinson, C. and Unnikrishnan, C.},
title = {Carnot and InfoSleuth: Database Technology and the World Wide Web},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223867},
doi = {10.1145/223784.223867},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {443–444},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223867,
author = {Woelk, D. and Bohrer, B. and Jacobs, N. and Ong, K. and Tomlinson, C. and Unnikrishnan, C.},
title = {Carnot and InfoSleuth: Database Technology and the World Wide Web},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223867},
doi = {10.1145/568271.223867},
journal = {SIGMOD Rec.},
month = may,
pages = {443–444},
numpages = {2}
}

@inproceedings{10.1145/223784.223868,
author = {Edelstein, Herb},
title = {Research and Products—Are They Relevant to Each Other? (Panel Session)},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223868},
doi = {10.1145/223784.223868},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {445},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223868,
author = {Edelstein, Herb},
title = {Research and Products—Are They Relevant to Each Other? (Panel Session)},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223868},
doi = {10.1145/568271.223868},
journal = {SIGMOD Rec.},
month = may,
pages = {445},
numpages = {1}
}

@inproceedings{10.1145/223784.223869,
author = {Squire, Case},
title = {Data Extraction and Transformation for the Data Warehouse},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223869},
doi = {10.1145/223784.223869},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {446–447},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223869,
author = {Squire, Case},
title = {Data Extraction and Transformation for the Data Warehouse},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223869},
doi = {10.1145/568271.223869},
journal = {SIGMOD Rec.},
month = may,
pages = {446–447},
numpages = {2}
}

@inproceedings{10.1145/223784.223870,
author = {Bansal, Sanju K.},
title = {Real World Requirements for Decision Support—Implications for RDBMS},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223870},
doi = {10.1145/223784.223870},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {448},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223870,
author = {Bansal, Sanju K.},
title = {Real World Requirements for Decision Support—Implications for RDBMS},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223870},
doi = {10.1145/568271.223870},
journal = {SIGMOD Rec.},
month = may,
pages = {448},
numpages = {1}
}

@inproceedings{10.1145/223784.223871,
author = {French, Clark D.},
title = {“One Size Fits All” Database Architectures Do Not Work for DSS},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223871},
doi = {10.1145/223784.223871},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {449–450},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223871,
author = {French, Clark D.},
title = {“One Size Fits All” Database Architectures Do Not Work for DSS},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223871},
doi = {10.1145/568271.223871},
journal = {SIGMOD Rec.},
month = may,
pages = {449–450},
numpages = {2}
}

@inproceedings{10.1145/223784.223872,
author = {Perna, Janet},
title = {Leveraging the Information Asset},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223872},
doi = {10.1145/223784.223872},
abstract = {Data is a corporate asset, and being able to derive more information from data can provide database users with a competitive advantage. For example, catching on to trends quickly can reduce unwanted store inventory and lower capital outlay for the same profit. If you have store sales data by product analyzed on a daily basis, that can make a 2-3% difference in margin -- and in a business where margins might be 4%, this is a significant competitive edge. This paper will cover what technology is needed by customers to leverage their information assets. Real-time access to production point of sale information, database mining for analysis to detect trends immediately, high performance, and multi-vendor database connectivity, cooperation among heterogeneous clients and servers are among the customer needs we are seeing in the marketplace.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {451–452},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223872,
author = {Perna, Janet},
title = {Leveraging the Information Asset},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223872},
doi = {10.1145/568271.223872},
abstract = {Data is a corporate asset, and being able to derive more information from data can provide database users with a competitive advantage. For example, catching on to trends quickly can reduce unwanted store inventory and lower capital outlay for the same profit. If you have store sales data by product analyzed on a daily basis, that can make a 2-3% difference in margin -- and in a business where margins might be 4%, this is a significant competitive edge. This paper will cover what technology is needed by customers to leverage their information assets. Real-time access to production point of sale information, database mining for analysis to detect trends immediately, high performance, and multi-vendor database connectivity, cooperation among heterogeneous clients and servers are among the customer needs we are seeing in the marketplace.},
journal = {SIGMOD Rec.},
month = may,
pages = {451–452},
numpages = {2}
}

@inproceedings{10.1145/223784.223873,
author = {Olson, Michael A.},
title = {Cover Your Assets},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223873},
doi = {10.1145/223784.223873},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {453},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223873,
author = {Olson, Michael A.},
title = {Cover Your Assets},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223873},
doi = {10.1145/568271.223873},
journal = {SIGMOD Rec.},
month = may,
pages = {453},
numpages = {1}
}

@inproceedings{10.1145/223784.223874,
author = {Atkinson, Robert},
title = {Use of a Component Architecture in Integrating Relational and Non-Relational},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223874},
doi = {10.1145/223784.223874},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {454},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223874,
author = {Atkinson, Robert},
title = {Use of a Component Architecture in Integrating Relational and Non-Relational},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223874},
doi = {10.1145/568271.223874},
journal = {SIGMOD Rec.},
month = may,
pages = {454},
numpages = {1}
}

@inproceedings{10.1145/223784.223875,
author = {Kleissner, Charly},
title = {Enterprise Objects Framework: A Second Generation Object-Relational Enabler},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223875},
doi = {10.1145/223784.223875},
abstract = {Today's information system executives desperately need to improve programmer productivity and reduce software maintenance costs. They are demanding flexibility in frameworks and architectures in order to meet unforeseen changes (see [Yankee 94]). Adaptability is a major requirement of most company's information systems efforts. Management of change is one of the key computing concepts of the 1990s.Object-oriented tools and development frameworks are starting to deliver the benefits of increased productivity and flexibility. These next-generation products now need to be combined with relational databases to leverage investments and facilitate access to business data. Object-Relational Enablers automate the process of storing complex objects in a relational database management system (see [Aberdeen 94]).The Enterprise Objects Framework product is a second generation product bringing the benefits of object-oriented programming to relational database application development. Enterprise Objects Framework enables developers to construct reusable business objects that combine business logic with persistent storage in industry-standard relational databases. Enterprise objects are first class citizens in the NEXTSTEP and OpenStep developer and user environments. They can be geographically distributed throughout heterogeneous servers within an enterprise using the Portable Distributed Objects product (see [NeXT-DO 94]).In this extended abstract we first describe the enterprise object distribution model and then give a brief synopsis of how relational data is mapped into objects. We then present an outline of the system architecture, explain how objects are mapped to multiple tables, and summarize the transaction semantics as well as the application development life-cycle. We conclude with an outlook on future development.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {455–459},
numpages = {5},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223875,
author = {Kleissner, Charly},
title = {Enterprise Objects Framework: A Second Generation Object-Relational Enabler},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223875},
doi = {10.1145/568271.223875},
abstract = {Today's information system executives desperately need to improve programmer productivity and reduce software maintenance costs. They are demanding flexibility in frameworks and architectures in order to meet unforeseen changes (see [Yankee 94]). Adaptability is a major requirement of most company's information systems efforts. Management of change is one of the key computing concepts of the 1990s.Object-oriented tools and development frameworks are starting to deliver the benefits of increased productivity and flexibility. These next-generation products now need to be combined with relational databases to leverage investments and facilitate access to business data. Object-Relational Enablers automate the process of storing complex objects in a relational database management system (see [Aberdeen 94]).The Enterprise Objects Framework product is a second generation product bringing the benefits of object-oriented programming to relational database application development. Enterprise Objects Framework enables developers to construct reusable business objects that combine business logic with persistent storage in industry-standard relational databases. Enterprise objects are first class citizens in the NEXTSTEP and OpenStep developer and user environments. They can be geographically distributed throughout heterogeneous servers within an enterprise using the Portable Distributed Objects product (see [NeXT-DO 94]).In this extended abstract we first describe the enterprise object distribution model and then give a brief synopsis of how relational data is mapped into objects. We then present an outline of the system architecture, explain how objects are mapped to multiple tables, and summarize the transaction semantics as well as the application development life-cycle. We conclude with an outlook on future development.},
journal = {SIGMOD Rec.},
month = may,
pages = {455–459},
numpages = {5}
}

@inproceedings{10.1145/223784.223876,
author = {Baru, Chaitanya and Fecteau, Gilles},
title = {An Overview of DB2 Parallel Edition},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223876},
doi = {10.1145/223784.223876},
abstract = {In this paper, we describe the architecture and features of DB2 Parallel Edition (PE). DB2 PE belongs to the IBM family of open DB2 client/server database products including DB2/6000, DB2/2, DB2 for HP-UX, and DB2 for the Solaris Operating Environment. DB2 PE employs a shared nothing architecture in which the database system consists of a set of independent logical database nodes. Each logical node represents a collection of system resources including, processes, main memory, disk storage, and communications, managed by an independent database manager. The logical nodes use message passing to exchange data with each other. Tables are partitioned across nodes using a hash partitioning strategy. The cost-based parallel query optimizer takes table partitioning information into account when generating parallel plans for execution by the runtime system. A DB2 PE system can be configured to contain one or more logical nodes per physical processor. For example, the system can be configured to implement one node per processor in a shared-nothing, MPP system or multiple nodes in a symmetric multiprocessor (SMP) system. This paper provides an overview of the storage model, query optimization, runtime system, utilities, and performance of DB2 Parallel Edition.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {460–462},
numpages = {3},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223876,
author = {Baru, Chaitanya and Fecteau, Gilles},
title = {An Overview of DB2 Parallel Edition},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223876},
doi = {10.1145/568271.223876},
abstract = {In this paper, we describe the architecture and features of DB2 Parallel Edition (PE). DB2 PE belongs to the IBM family of open DB2 client/server database products including DB2/6000, DB2/2, DB2 for HP-UX, and DB2 for the Solaris Operating Environment. DB2 PE employs a shared nothing architecture in which the database system consists of a set of independent logical database nodes. Each logical node represents a collection of system resources including, processes, main memory, disk storage, and communications, managed by an independent database manager. The logical nodes use message passing to exchange data with each other. Tables are partitioned across nodes using a hash partitioning strategy. The cost-based parallel query optimizer takes table partitioning information into account when generating parallel plans for execution by the runtime system. A DB2 PE system can be configured to contain one or more logical nodes per physical processor. For example, the system can be configured to implement one node per processor in a shared-nothing, MPP system or multiple nodes in a symmetric multiprocessor (SMP) system. This paper provides an overview of the storage model, query optimization, runtime system, utilities, and performance of DB2 Parallel Edition.},
journal = {SIGMOD Rec.},
month = may,
pages = {460–462},
numpages = {3}
}

@inproceedings{10.1145/223784.223877,
author = {Gerber, Bob},
title = {Informix Online XPS},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223877},
doi = {10.1145/223784.223877},
abstract = {Parallel database systems have delivered on the promise of providing excellent, linear performance speedups for both decision support and transaction processing workloads across a variety of parallel system platforms. The challenge is now to extend those results across the entire open systems spectrum of loosely coupled environments while realizing the inherent high availability of such systems. The Informix eXtended Parallel Server (XPS) will accomplish that goal.XPS is the third and latest in the series of Online database servers that are based upon the Informix Dynamic Scalable Architecture (DSA). Earlier Online DSA servers have proven the effectiveness of the SMP-based high performance parallel data query (PDQ) technology that is the foundation of XPS: multi-threaded process groups, table partitioning, pipelined hash-partitioned operators, light access methods, and parallel resource management. XPS extends that PDQ technology across the continuum from small clusters of large SMP systems to massively parallel clusters of SMP or uniprocessor systems.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {463},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223877,
author = {Gerber, Bob},
title = {Informix Online XPS},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223877},
doi = {10.1145/568271.223877},
abstract = {Parallel database systems have delivered on the promise of providing excellent, linear performance speedups for both decision support and transaction processing workloads across a variety of parallel system platforms. The challenge is now to extend those results across the entire open systems spectrum of loosely coupled environments while realizing the inherent high availability of such systems. The Informix eXtended Parallel Server (XPS) will accomplish that goal.XPS is the third and latest in the series of Online database servers that are based upon the Informix Dynamic Scalable Architecture (DSA). Earlier Online DSA servers have proven the effectiveness of the SMP-based high performance parallel data query (PDQ) technology that is the foundation of XPS: multi-threaded process groups, table partitioning, pipelined hash-partitioned operators, light access methods, and parallel resource management. XPS extends that PDQ technology across the continuum from small clusters of large SMP systems to massively parallel clusters of SMP or uniprocessor systems.},
journal = {SIGMOD Rec.},
month = may,
pages = {463},
numpages = {1}
}

@inproceedings{10.1145/223784.223878,
author = {Levine, Charles},
title = {Order-of-Magnitude Advantage on TPC-C through Massive Parallelism},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223878},
doi = {10.1145/223784.223878},
abstract = {TPC Benchmark™ C (TPC-C) is the modern standard for measuring OLTP performance. Running TPC-C, Tandem demonstrated a massively parallel configuration of 112 CPUs which achieved ten times higher performance than any other system previously measured (and today is still better by a factor of five). This result qualifies as the largest industry-standard benchmark ever run.This paper briefly describes how the benchmark was configured and the results which were obtained.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {464–465},
numpages = {2},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223878,
author = {Levine, Charles},
title = {Order-of-Magnitude Advantage on TPC-C through Massive Parallelism},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223878},
doi = {10.1145/568271.223878},
abstract = {TPC Benchmark™ C (TPC-C) is the modern standard for measuring OLTP performance. Running TPC-C, Tandem demonstrated a massively parallel configuration of 112 CPUs which achieved ten times higher performance than any other system previously measured (and today is still better by a factor of five). This result qualifies as the largest industry-standard benchmark ever run.This paper briefly describes how the benchmark was configured and the results which were obtained.},
journal = {SIGMOD Rec.},
month = may,
pages = {464–465},
numpages = {2}
}

@inproceedings{10.1145/223784.223880,
author = {Deutsch, Donald R.},
title = {Objects and SQL (Panel Session): Strange Relations?},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223880},
doi = {10.1145/223784.223880},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {466},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223880,
author = {Deutsch, Donald R.},
title = {Objects and SQL (Panel Session): Strange Relations?},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223880},
doi = {10.1145/568271.223880},
journal = {SIGMOD Rec.},
month = may,
pages = {466},
numpages = {1}
}

@inproceedings{10.1145/223784.223879,
author = {Faloutsos, Christos},
title = {Indexing Multimedia Databases},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223879},
doi = {10.1145/223784.223879},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {467},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223879,
author = {Faloutsos, Christos},
title = {Indexing Multimedia Databases},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223879},
doi = {10.1145/568271.223879},
journal = {SIGMOD Rec.},
month = may,
pages = {467},
numpages = {1}
}

@inproceedings{10.1145/223784.223881,
author = {Melton, Jim and Mattos, Nelson Mendoca},
title = {An Overview of the Emerging Third-Generation SQL Standard},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223881},
doi = {10.1145/223784.223881},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {468},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223881,
author = {Melton, Jim and Mattos, Nelson Mendoca},
title = {An Overview of the Emerging Third-Generation SQL Standard},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223881},
doi = {10.1145/568271.223881},
journal = {SIGMOD Rec.},
month = may,
pages = {468},
numpages = {1}
}

@inproceedings{10.1145/223784.223882,
author = {Sheth, Amit},
title = {Workflow Automation: Applications, Technology and Research},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223882},
doi = {10.1145/223784.223882},
abstract = {With increasing global exposure, today's enterprises must react quickly to changes, rapidly develop new services and products, and at the same time improve productivity and quality and reduce cost. Business process re-engineering and workflow automation to coordinate activities throughout the enterprise are recognized as important emerging technologies to support these requirements. Rosy estimates of a multi-billion dollar marketplace for workflow software has resulted in significant commercial activities in the area, with nearly hundred products now claiming to support workflow automation. While many help to automate document- and image-driven office applications, therefore helping to improve the productivity of small groups, most current products fail to support:• mission critical and enterprise-wide applications with requirements such as failure handling and recovery, and• interoperability with existing heterogeneous information systems.In this tutorial, we will discuss requirements for applications involving workflow automation, present an overview of the current state-of-the-art in products, and present some of the research efforts that are attempting to respond to unmet challenges.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {469},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223882,
author = {Sheth, Amit},
title = {Workflow Automation: Applications, Technology and Research},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223882},
doi = {10.1145/568271.223882},
abstract = {With increasing global exposure, today's enterprises must react quickly to changes, rapidly develop new services and products, and at the same time improve productivity and quality and reduce cost. Business process re-engineering and workflow automation to coordinate activities throughout the enterprise are recognized as important emerging technologies to support these requirements. Rosy estimates of a multi-billion dollar marketplace for workflow software has resulted in significant commercial activities in the area, with nearly hundred products now claiming to support workflow automation. While many help to automate document- and image-driven office applications, therefore helping to improve the productivity of small groups, most current products fail to support:• mission critical and enterprise-wide applications with requirements such as failure handling and recovery, and• interoperability with existing heterogeneous information systems.In this tutorial, we will discuss requirements for applications involving workflow automation, present an overview of the current state-of-the-art in products, and present some of the research efforts that are attempting to respond to unmet challenges.},
journal = {SIGMOD Rec.},
month = may,
pages = {469},
numpages = {1}
}

@inproceedings{10.1145/223784.223883,
author = {Norrie, Moira C.},
title = {Integration Approaches for CIM},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223883},
doi = {10.1145/223784.223883},
abstract = {In response to pressures to reduce product lead times, manufacturing companies are increasingly aware of the need for some form of integration along the whole product chain. Engineering tasks must be coordinated and data exchanged between the various specialised tools. An enterprise has two main tracks of information flow, namely technical and managerial, and product data management spans both tracks. On the technical track, applications are highly specialised supporting tasks such as product design (CAD) and the programming of numerically controlled machines (CAM). Generally, the various application systems on the technical track are referred to as CAx systems. CAx systems may not only differ in terms of functionality but also in terms of the amount and type of data managed, the run-time environment and performance characteristics.For complete support of Computer Integrated Manufacturing (CIM), we must be able to integrate existing technical and administrative component application systems. These component systems vary in their data management support and many CAx systems store their data directly in files rather than in a database system. The issues are how to describe the dependencies between these component systems and ensure system-wide data consistency. A particularly difficult problem is that of how to interface existing application systems in such a way that their operation can be monitored and controlled and a global transaction scheme provided.Integration must be achieved in a way that supports system evolution in terms of the introduction and replacement of application systems. This is particularly important given the trend towards the notions of the extended enterprise and virtual factories in which a particular product chain may span several enterprises. Further, emerging legal statutes (especially in relation to environmental factors) are resulting in changes to the requirements for product data management. Enterprise integration must be both flexible and dynamic. The best way of achieving this is to integrate component systems by mans of a control layer which coordinates tasks based on explicit inter-system dependencies amenable to both direct view and update.Product Data Management systems (also known as Engineering Databases) have been developed for the integration of CAx systems by managing product data centrally. One problem with a centralised system controlling access to data is that its availability is critical to the operation of all component systems. In an effort to overcome this, systems are being developed which replicate the metadata and data required for coordination.Alternatively, coordination approaches have been proposed which aim to maximise component subsystem autonomy and increase CIM system flexibility. These systems place less emphasis on data sharing and more emphasis on task coordination. The integration effort is minimised and only that information strictly essential to coordination is managed centrally.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {470},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223883,
author = {Norrie, Moira C.},
title = {Integration Approaches for CIM},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223883},
doi = {10.1145/568271.223883},
abstract = {In response to pressures to reduce product lead times, manufacturing companies are increasingly aware of the need for some form of integration along the whole product chain. Engineering tasks must be coordinated and data exchanged between the various specialised tools. An enterprise has two main tracks of information flow, namely technical and managerial, and product data management spans both tracks. On the technical track, applications are highly specialised supporting tasks such as product design (CAD) and the programming of numerically controlled machines (CAM). Generally, the various application systems on the technical track are referred to as CAx systems. CAx systems may not only differ in terms of functionality but also in terms of the amount and type of data managed, the run-time environment and performance characteristics.For complete support of Computer Integrated Manufacturing (CIM), we must be able to integrate existing technical and administrative component application systems. These component systems vary in their data management support and many CAx systems store their data directly in files rather than in a database system. The issues are how to describe the dependencies between these component systems and ensure system-wide data consistency. A particularly difficult problem is that of how to interface existing application systems in such a way that their operation can be monitored and controlled and a global transaction scheme provided.Integration must be achieved in a way that supports system evolution in terms of the introduction and replacement of application systems. This is particularly important given the trend towards the notions of the extended enterprise and virtual factories in which a particular product chain may span several enterprises. Further, emerging legal statutes (especially in relation to environmental factors) are resulting in changes to the requirements for product data management. Enterprise integration must be both flexible and dynamic. The best way of achieving this is to integrate component systems by mans of a control layer which coordinates tasks based on explicit inter-system dependencies amenable to both direct view and update.Product Data Management systems (also known as Engineering Databases) have been developed for the integration of CAx systems by managing product data centrally. One problem with a centralised system controlling access to data is that its availability is critical to the operation of all component systems. In an effort to overcome this, systems are being developed which replicate the metadata and data required for coordination.Alternatively, coordination approaches have been proposed which aim to maximise component subsystem autonomy and increase CIM system flexibility. These systems place less emphasis on data sharing and more emphasis on task coordination. The integration effort is minimised and only that information strictly essential to coordination is managed centrally.},
journal = {SIGMOD Rec.},
month = may,
pages = {470},
numpages = {1}
}

@inproceedings{10.1145/223784.223884,
author = {Gibson, Garth A.},
title = {Tutorial on Storage Technology: RAID and Beyond},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223884},
doi = {10.1145/223784.223884},
abstract = {In stark contrast to the 25% per year increase in areal density delivered by the magnetic disk industry during the 1970s and 1980s, yearly increases today are 60%, on par with DRAM density increases. Moreover, the storage industry is also delivering substantially higher data rates, smart disk-embedded readahead and writebehind, and a new generation of high-speed serial interconnects. This industry has also embraced Redundant Arrays of Inexpensive (or Independent) Disks (RAID) technology - 1997's RAID market is expected to be 13 billion dollars. With this rapidly evolving market and technology base, parallel storage systems must evolve beyond RAID levels 1 through 5. This talk is intended for researchers and practitioners interested in current trends in storage systems. It will highlight storage technology trends, RAID technology trends, and trends toward RAID-style support for network-based parallel storage systems.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {471},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223884,
author = {Gibson, Garth A.},
title = {Tutorial on Storage Technology: RAID and Beyond},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223884},
doi = {10.1145/568271.223884},
abstract = {In stark contrast to the 25% per year increase in areal density delivered by the magnetic disk industry during the 1970s and 1980s, yearly increases today are 60%, on par with DRAM density increases. Moreover, the storage industry is also delivering substantially higher data rates, smart disk-embedded readahead and writebehind, and a new generation of high-speed serial interconnects. This industry has also embraced Redundant Arrays of Inexpensive (or Independent) Disks (RAID) technology - 1997's RAID market is expected to be 13 billion dollars. With this rapidly evolving market and technology base, parallel storage systems must evolve beyond RAID levels 1 through 5. This talk is intended for researchers and practitioners interested in current trends in storage systems. It will highlight storage technology trends, RAID technology trends, and trends toward RAID-style support for network-based parallel storage systems.},
journal = {SIGMOD Rec.},
month = may,
pages = {471},
numpages = {1}
}

@inproceedings{10.1145/223784.223885,
author = {Imielinski, T. and Virmani, A.},
title = {DataMine—Interactive Rule Discovery System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223885},
doi = {10.1145/223784.223885},
abstract = {DataMine is a statistical database mining system with strong emphasis on interactiveness and nice graphical representation of information produced. It also supports an offline mode of discovery, and provides an extensive API which allows users to write "mining applications" just as easily as routine database applications, The central idea is to perform discovery with a "human in the loop" guiding the system using his initial hypothesis and the feedback from the system. Users can pose a rule-query against a rulebase and the system can generate all rules matching their query. The rulebase could either be pregenerated (using offline mode) or could be realized in real-time as the discovery progresses.Rules generated by the system are of the form:Body -&gt; Consequentwhere Body is a conjunction of the elementary predicates of the form (A=a), where A is an attribute and a is a value from the attribute domain of A. Consequent is a single elementary predicate. Each rule can have several parameters like support, confidence, atypicality, color etc. (the definitions have been left out) which can also be used by the user in framing the rule query.For continuous attributes, the system also allows the user some control in deciding how they are discretized. It also allows for the creation of extra attributes at run time which can then be used in queries like the rest.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {472},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223885,
author = {Imielinski, T. and Virmani, A.},
title = {DataMine—Interactive Rule Discovery System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223885},
doi = {10.1145/568271.223885},
abstract = {DataMine is a statistical database mining system with strong emphasis on interactiveness and nice graphical representation of information produced. It also supports an offline mode of discovery, and provides an extensive API which allows users to write "mining applications" just as easily as routine database applications, The central idea is to perform discovery with a "human in the loop" guiding the system using his initial hypothesis and the feedback from the system. Users can pose a rule-query against a rulebase and the system can generate all rules matching their query. The rulebase could either be pregenerated (using offline mode) or could be realized in real-time as the discovery progresses.Rules generated by the system are of the form:Body -&gt; Consequentwhere Body is a conjunction of the elementary predicates of the form (A=a), where A is an attribute and a is a value from the attribute domain of A. Consequent is a single elementary predicate. Each rule can have several parameters like support, confidence, atypicality, color etc. (the definitions have been left out) which can also be used by the user in framing the rule query.For continuous attributes, the system also allows the user some control in deciding how they are discretized. It also allows for the creation of extra attributes at run time which can then be used in queries like the rest.},
journal = {SIGMOD Rec.},
month = may,
pages = {472},
numpages = {1}
}

@inproceedings{10.1145/223784.223886,
author = {Ceri, Stefano and Fraternali, Piero and Paraboschi, Stefano and Psaila, Giuseppe},
title = {The Algres Testbed of CHIMERA: An Active Object-Oriented Database System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223886},
doi = {10.1145/223784.223886},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {473},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223886,
author = {Ceri, Stefano and Fraternali, Piero and Paraboschi, Stefano and Psaila, Giuseppe},
title = {The Algres Testbed of CHIMERA: An Active Object-Oriented Database System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223886},
doi = {10.1145/568271.223886},
journal = {SIGMOD Rec.},
month = may,
pages = {473},
numpages = {1}
}

@inproceedings{10.1145/223784.223887,
author = {Collet, C. and Coupaye, T.},
title = {The NAOS System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223887},
doi = {10.1145/223784.223887},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {474},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223887,
author = {Collet, C. and Coupaye, T.},
title = {The NAOS System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223887},
doi = {10.1145/568271.223887},
journal = {SIGMOD Rec.},
month = may,
pages = {474},
numpages = {1}
}

@inproceedings{10.1145/223784.223888,
author = {Ashley, Jonathan and Flickner, Myron and Hafner, James and Lee, Denis and Niblack, Wayne and Petkovic, Dragutin},
title = {The Query by Image Content (QBIC) System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223888},
doi = {10.1145/223784.223888},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {475},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223888,
author = {Ashley, Jonathan and Flickner, Myron and Hafner, James and Lee, Denis and Niblack, Wayne and Petkovic, Dragutin},
title = {The Query by Image Content (QBIC) System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223888},
doi = {10.1145/568271.223888},
journal = {SIGMOD Rec.},
month = may,
pages = {475},
numpages = {1}
}

@inproceedings{10.1145/223784.223889,
author = {Buchmann, A. P. and Deutsch, A. and Zimmermann, J. and Higa, M.},
title = {The REACH Active OODBMS},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223889},
doi = {10.1145/223784.223889},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {476},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223889,
author = {Buchmann, A. P. and Deutsch, A. and Zimmermann, J. and Higa, M.},
title = {The REACH Active OODBMS},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223889},
doi = {10.1145/568271.223889},
journal = {SIGMOD Rec.},
month = may,
pages = {476},
numpages = {1}
}

@inproceedings{10.1145/223784.223890,
author = {Massari, Antonio and Pavani, Stefano and Saladini, Lorenzo and Chrysanthis, Panos K.},
title = {QBI: Query by Icons},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223890},
doi = {10.1145/223784.223890},
abstract = {QBI is an icon-based query processing and exploration facility for large distributed databases [3]. As opposed to other interactive query interfaces, it combines (1) a pure iconic specification, i.e., no diagrams of any form, only icon manipulation, with (2) intensional browsing or metaquery tools that assist in the formulation of complete queries without involving path specification or access to the actual data in the database.Path expressions are automatically generated by QBI and irrespective of their length, represented by a single icon, allowing for better use of the screen. It requires no special knowledge of the content of the underlying database nor understanding of the details of the database schema. Hence, QBI is domain independent and equally useful to both unsophisticated and expert users.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {477},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223890,
author = {Massari, Antonio and Pavani, Stefano and Saladini, Lorenzo and Chrysanthis, Panos K.},
title = {QBI: Query by Icons},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223890},
doi = {10.1145/568271.223890},
abstract = {QBI is an icon-based query processing and exploration facility for large distributed databases [3]. As opposed to other interactive query interfaces, it combines (1) a pure iconic specification, i.e., no diagrams of any form, only icon manipulation, with (2) intensional browsing or metaquery tools that assist in the formulation of complete queries without involving path specification or access to the actual data in the database.Path expressions are automatically generated by QBI and irrespective of their length, represented by a single icon, allowing for better use of the screen. It requires no special knowledge of the content of the underlying database nor understanding of the details of the database schema. Hence, QBI is domain independent and equally useful to both unsophisticated and expert users.},
journal = {SIGMOD Rec.},
month = may,
pages = {477},
numpages = {1}
}

@inproceedings{10.1145/223784.223891,
author = {Shklar, Leon and Sheth, Amit and Kashyap, Vipul and Thatte, Satish},
title = {InfoHarness: A System for Search and Retrieval of Heterogeneous Information},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223891},
doi = {10.1145/223784.223891},
abstract = {Enormous amounts of heterogeneous information have been accumulated within corporations, government organizations and universities. It is becoming increasingly easier to create new information, but the knowledge about the existence, location, and means of retrieval of information, have become so confusing as to give rise to the phenomenon of write-only databases.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {478},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223891,
author = {Shklar, Leon and Sheth, Amit and Kashyap, Vipul and Thatte, Satish},
title = {InfoHarness: A System for Search and Retrieval of Heterogeneous Information},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223891},
doi = {10.1145/568271.223891},
abstract = {Enormous amounts of heterogeneous information have been accumulated within corporations, government organizations and universities. It is becoming increasingly easier to create new information, but the knowledge about the existence, location, and means of retrieval of information, have become so confusing as to give rise to the phenomenon of write-only databases.},
journal = {SIGMOD Rec.},
month = may,
pages = {478},
numpages = {1}
}

@inproceedings{10.1145/223784.223892,
author = {Thomas, J. and De\ss{}loch, S. and Mattos, N.},
title = {Design and Implementation of Advanced Knowledge Processing in the KBMS KRISYS},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223892},
doi = {10.1145/223784.223892},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {479},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223892,
author = {Thomas, J. and De\ss{}loch, S. and Mattos, N.},
title = {Design and Implementation of Advanced Knowledge Processing in the KBMS KRISYS},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223892},
doi = {10.1145/568271.223892},
journal = {SIGMOD Rec.},
month = may,
pages = {479},
numpages = {1}
}

@inproceedings{10.1145/223784.223893,
author = {Gatziu, Stella and Geppert, Andreas and Dittrich, Klaus R.},
title = {The SAMOS Active DBMS Prototype},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223893},
doi = {10.1145/223784.223893},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {480},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223893,
author = {Gatziu, Stella and Geppert, Andreas and Dittrich, Klaus R.},
title = {The SAMOS Active DBMS Prototype},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223893},
doi = {10.1145/568271.223893},
journal = {SIGMOD Rec.},
month = may,
pages = {480},
numpages = {1}
}

@inproceedings{10.1145/223784.223894,
author = {Radeke, E. and B\"{o}ttger, R. and Burkert, B. and Engel, Y. and Kachel, G. and Kolmschlag, S. and Nolte, D.},
title = {Efendi: Federated Database System of Cadlab},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223894},
doi = {10.1145/223784.223894},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {481},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223894,
author = {Radeke, E. and B\"{o}ttger, R. and Burkert, B. and Engel, Y. and Kachel, G. and Kolmschlag, S. and Nolte, D.},
title = {Efendi: Federated Database System of Cadlab},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223894},
doi = {10.1145/568271.223894},
journal = {SIGMOD Rec.},
month = may,
pages = {481},
numpages = {1}
}

@inproceedings{10.1145/223784.223895,
author = {Keim, Daniel A. and Kriegel, Hans-Peter},
title = {VisDB: A System for Visualizing Large Databases},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223895},
doi = {10.1145/223784.223895},
abstract = {The VisDB system developed at the University of Munich is a sophisticated tool for visualizing and analyzing large databases. The key idea of the VisDB system is to support the exploration of large databases by using the phenomenal abilities of the human vision system which is able to analyze visualizations of mid-size to large amounts of data very efficiently. The goal of the VisDB system is to provide visualizations of large portions of the database, allowing properties of the data and structure in the data to become perceptually apparent.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {482},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223895,
author = {Keim, Daniel A. and Kriegel, Hans-Peter},
title = {VisDB: A System for Visualizing Large Databases},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223895},
doi = {10.1145/568271.223895},
abstract = {The VisDB system developed at the University of Munich is a sophisticated tool for visualizing and analyzing large databases. The key idea of the VisDB system is to support the exploration of large databases by using the phenomenal abilities of the human vision system which is able to analyze visualizations of mid-size to large amounts of data very efficiently. The goal of the VisDB system is to provide visualizations of large portions of the database, allowing properties of the data and structure in the data to become perceptually apparent.},
journal = {SIGMOD Rec.},
month = may,
pages = {482},
numpages = {1}
}

@inproceedings{10.1145/223784.223896,
author = {Hammer, Joachim and Garc\'{\i}a-Molina, H\'{e}ctor and Ireland, Kelly and Papakonstantinou, Yannis and Ullman, Jeffrey and Widom, Jennifer},
title = {Information Translation, Mediation, and Mosaic-Based Browsing in the TSIMMIS System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223896},
doi = {10.1145/223784.223896},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {483},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223896,
author = {Hammer, Joachim and Garc\'{\i}a-Molina, H\'{e}ctor and Ireland, Kelly and Papakonstantinou, Yannis and Ullman, Jeffrey and Widom, Jennifer},
title = {Information Translation, Mediation, and Mosaic-Based Browsing in the TSIMMIS System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223896},
doi = {10.1145/568271.223896},
journal = {SIGMOD Rec.},
month = may,
pages = {483},
numpages = {1}
}

@inproceedings{10.1145/223784.223897,
author = {Li, Wen-Syan and Clifton, Chris},
title = {Semint: A System Prototype for Semantic Integration in Heterogeneous Databases},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223897},
doi = {10.1145/223784.223897},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {484},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223897,
author = {Li, Wen-Syan and Clifton, Chris},
title = {Semint: A System Prototype for Semantic Integration in Heterogeneous Databases},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223897},
doi = {10.1145/568271.223897},
journal = {SIGMOD Rec.},
month = may,
pages = {484},
numpages = {1}
}

@inproceedings{10.1145/223784.223898,
author = {The Paradise Team, CORPORATE},
title = {Paradise: A Database System for GIS Applications},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223898},
doi = {10.1145/223784.223898},
abstract = {The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.At the outset, we organized the Paradise project as two phases. The goal of first phase was to produce a client-server version of Paradise. The second phase of the project is to add support for tertiary storage and extend the software to run on clusters of workstations and "shared nothing" multiprocessors [Sto86]. Phase one of the project is now complete and has produced a usable, client-server version of the system whose performance and functionality is comparable to other integrated GIS systems [DKL+94].},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {485},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223898,
author = {The Paradise Team, CORPORATE},
title = {Paradise: A Database System for GIS Applications},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223898},
doi = {10.1145/568271.223898},
abstract = {The goal of the Paradise project is to apply object-oriented and parallel database technology to the task of implementing a parallel GIS system capable of managing extremely large (multi-terabyte) data sets such as those that will be produced by the upcoming NASA EOSDIS project [Car92]. The project is focusing its resources on algorithms, processing, and storage techniques, and not on making new contributions to the data modeling, query language, or user interface domains.At the outset, we organized the Paradise project as two phases. The goal of first phase was to produce a client-server version of Paradise. The second phase of the project is to add support for tertiary storage and extend the software to run on clusters of workstations and "shared nothing" multiprocessors [Sto86]. Phase one of the project is now complete and has produced a usable, client-server version of the system whose performance and functionality is comparable to other integrated GIS systems [DKL+94].},
journal = {SIGMOD Rec.},
month = may,
pages = {485},
numpages = {1}
}

@inproceedings{10.1145/223784.223899,
author = {The SHORE Team, CORPORATE},
title = {Shore: Combining the Best Features of OODBMS and File Systems},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223899},
doi = {10.1145/223784.223899},
abstract = {Shore is a persistent object system under development at the University of Wisconsin that represents a merger of key features of object-oriented database (OODB) and file system technologies: Concurrency control and recovery and bulk operations on clustered data as in traditional databases; objects of widely varying sizes with individual identity, globally unique identifiers, and a rich object-oriented type system as in OODB systems; and an open architecture, a uniform, universal hierarchical name space, and controlled sharing of disks with reliably enforced ownership of data and space accounting. A more complete overview of the goals and structure of Share were presented at this conference last year [Carey et al, SIGMOD 94].},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {486},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223899,
author = {The SHORE Team, CORPORATE},
title = {Shore: Combining the Best Features of OODBMS and File Systems},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223899},
doi = {10.1145/568271.223899},
abstract = {Shore is a persistent object system under development at the University of Wisconsin that represents a merger of key features of object-oriented database (OODB) and file system technologies: Concurrency control and recovery and bulk operations on clustered data as in traditional databases; objects of widely varying sizes with individual identity, globally unique identifiers, and a rich object-oriented type system as in OODB systems; and an open architecture, a uniform, universal hierarchical name space, and controlled sharing of disks with reliably enforced ownership of data and space accounting. A more complete overview of the goals and structure of Share were presented at this conference last year [Carey et al, SIGMOD 94].},
journal = {SIGMOD Rec.},
month = may,
pages = {486},
numpages = {1}
}

@inproceedings{10.1145/223784.223900,
author = {Wang, Jason T. L. and Zhang, Kaizhong and Shasha, Dennis},
title = {Pattern Matching and Pattern Discovery in Scientific, Program, and Document Databases},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223900},
doi = {10.1145/223784.223900},
abstract = {Over the past several years we have created or borrowed algorithms for combinatorial pattern matching and pattern discovery on sequences [2] and trees.In matching problems, given a pattern, a set of data objects and a distance metric, we find the distance between the pattern and one or more data objects. In discovery problems by contrast, given a set of objects, a metric, and a distance, we seek a pattern that matches many of those objects within the given distance. (So, discovery is a lot like data mining.) Our toolkit performs both matching and discovery with current targeted applications in molecular biology and document comparison.},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {487},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223900,
author = {Wang, Jason T. L. and Zhang, Kaizhong and Shasha, Dennis},
title = {Pattern Matching and Pattern Discovery in Scientific, Program, and Document Databases},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223900},
doi = {10.1145/568271.223900},
abstract = {Over the past several years we have created or borrowed algorithms for combinatorial pattern matching and pattern discovery on sequences [2] and trees.In matching problems, given a pattern, a set of data objects and a distance metric, we find the distance between the pattern and one or more data objects. In discovery problems by contrast, given a set of objects, a metric, and a distance, we seek a pattern that matches many of those objects within the given distance. (So, discovery is a lot like data mining.) Our toolkit performs both matching and discovery with current targeted applications in molecular biology and document comparison.},
journal = {SIGMOD Rec.},
month = may,
pages = {487},
numpages = {1}
}

@inproceedings{10.1145/223784.223901,
author = {Grossman, R. L. and Hanley, D. and Qin, X.},
title = {PTool: A Light Weight Persistent Object Manager},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223901},
doi = {10.1145/223784.223901},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {488},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223901,
author = {Grossman, R. L. and Hanley, D. and Qin, X.},
title = {PTool: A Light Weight Persistent Object Manager},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223901},
doi = {10.1145/568271.223901},
journal = {SIGMOD Rec.},
month = may,
pages = {488},
numpages = {1}
}

@inproceedings{10.1145/223784.223902,
author = {Dreyer, Werner and Dittrich, Angelika Kotz and Schmidt, Duri},
title = {Using the CALANDA Time Series Management System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223902},
doi = {10.1145/223784.223902},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {489},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223902,
author = {Dreyer, Werner and Dittrich, Angelika Kotz and Schmidt, Duri},
title = {Using the CALANDA Time Series Management System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223902},
doi = {10.1145/568271.223902},
journal = {SIGMOD Rec.},
month = may,
pages = {489},
numpages = {1}
}

@inproceedings{10.1145/223784.223903,
author = {Jonker, Willem and Sch\"{u}tz, Heribert},
title = {The ECRC Multi Database System},
year = {1995},
isbn = {0897917316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/223784.223903},
doi = {10.1145/223784.223903},
booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
pages = {490},
numpages = {1},
location = {San Jose, California, USA},
series = {SIGMOD '95}
}

@article{10.1145/568271.223903,
author = {Jonker, Willem and Sch\"{u}tz, Heribert},
title = {The ECRC Multi Database System},
year = {1995},
issue_date = {May 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/568271.223903},
doi = {10.1145/568271.223903},
journal = {SIGMOD Rec.},
month = may,
pages = {490},
numpages = {1}
}

