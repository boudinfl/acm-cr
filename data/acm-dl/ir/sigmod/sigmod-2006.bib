@inproceedings{10.1145/1142473.1142475,
author = {Jagadish, H. V. and Ooi, Beng Chin and Tan, Kian-Lee and Vu, Quang Hieu and Zhang, Rong},
title = {Speeding up Search in Peer-to-Peer Networks with a Multi-Way Tree Structure},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142475},
doi = {10.1145/1142473.1142475},
abstract = {Peer-to-Peer systems have recently become a popular means to share resources. Effective search is a critical requirement in such systems, and a number of distributed search structures have been proposed in the literature. Most of these structures provide "log time search" capability, where the logarithm is taken base 2. That is, in a system with N nodes, the cost of the search is O(log2N).In database systems, the importance of large fanout index structures has been well recognized. In P2P search too, the cost could be reduced considerably if this logarithm were taken to a larger base. In this paper, we propose a multi-way tree search structure, which reduces the cost of search to O(logmN), where m is the fanout. The penalty paid is a larger update cost, but we show how to keep this penalty to be no worse than linear in m. We experimentally explore this tradeoff between search and update cost as a function of m, and suggest how to find a good trade-off point.The multi-way tree structure we propose, BATON*, is derived from the BATON structure that has recently been suggested. In addition to multi-way fanout, BATON* also adds support for multi-attribute queries to BATON.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
keywords = {multi-way tree structure, peer-to-peer, system architecture},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142476,
author = {Taylor, Nicholas E. and Ives, Zachary G.},
title = {Reconciling While Tolerating Disagreement in Collaborative Data Sharing},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142476},
doi = {10.1145/1142473.1142476},
abstract = {In many data sharing settings, such as within the biological and biomedical communities, global data consistency is not always attainable: different sites' data may be dirty, uncertain, or even controversial. Collaborators are willing to share their data, and in many cases they also want to selectively import data from others --- but must occasionally diverge when they disagree about uncertain or controversial facts or values. For this reason, traditional data sharing and data integration approaches are not applicable, since they require a globally consistent data instance. Additionally, many of these approaches do not allow participants to make updates; if they do, concurrency control algorithms or inconsistency repair techniques must be used to ensure a consistent view of the data for all users.In this paper, we develop and present a fully decentralized model of collaborative data sharing, in which participants publish their data on an ad hoc basis and simultaneously reconcile updates with those published by others. Individual updates are associated with provenance information, and each participant accepts only updates with a sufficient authority ranking, meaning that each participant may have a different (though conceptually overlapping) data instance. We define a consistency semantics for database instances under this model of disagreement, present algorithms that perform reconciliation for distributed clusters of participants, and demonstrate their ability to handle typical update and conflict loads in settings involving the sharing of curated data.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {13–24},
numpages = {12},
keywords = {reconciliation, collaborative data sharing, data integration, transactions, peer-to-peer, updates},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142477,
author = {Deng, Fan and Rafiei, Davood},
title = {Approximately Detecting Duplicates for Streaming Data Using Stable Bloom Filters},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142477},
doi = {10.1145/1142473.1142477},
abstract = {Traditional duplicate elimination techniques are not applicable to many data stream applications. In general, precisely eliminating duplicates in an unbounded data stream is not feasible in many streaming scenarios. Therefore, we target at approximately eliminating duplicates in streaming environments given a limited space. Based on a well-known bitmap sketch, we introduce a data structure, Stable Bloom Filter, and a novel and simple algorithm. The basic idea is as follows: since there is no way to store the whole history of the stream, SBF continuously evicts the stale information so that SBF has room for those more recent elements. After finding some properties of SBF analytically, we show that a tight upper bound of false positive rates is guaranteed. In our empirical study, we compare SBF to alternative methods. The results show that our method is superior in terms of both accuracy and time effciency when a fixed small space and an acceptable false positive rate are given.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {25–36},
numpages = {12},
keywords = {duplicate detection, algorithms, data stream, approximation},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inbook{10.1145/1142473.1142479,
author = {Gou, Gang and Kormilitsin, Maxim and Chirkova, Rada},
title = {Query Evaluation Using Overlapping Views: Completeness and Efficiency},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142479},
abstract = {We study the problem of finding efficient equivalent view-based rewritings of relational queries, focusing on query optimization using materialized views under the assumption that base relations cannot contain duplicate tuples. A lot of work in the literature addresses the problems of answering queries using views and query optimization. However, most of it proposes solutions for special cases, such as for conjunctive queries (CQs) or for aggregate queries only. In addition, most of it addresses the problems separately under set or bag-set semantics for query evaluation, and some of it proposes heuristics without formal proofs for completeness or soundness. In this paper we look at the two problems by considering CQ/A queries - that is, both pure conjunctive and aggregate queries, with aggregation functions SUM, COUNT, MIN, and MAX; the DISTINCT keyword in (SQL versions of) our queries is also allowed. We build on past work to provide algorithms that handle this general setting. This is possible because recent results on rewritings of CQ/A queries [1, 8] show that there are sound and complete algorithms based on containment tests of CQs.Our focus is that our algorithms are efficient as well as sound and complete. Besides the contribution we make in putting and addressing the problems in this general setting, we make two additional contributions for bag-set and set semantics. First, we propose efficient sound and complete tests for equivalence of CQ/A queries to rewritings that use overlapping views (the algorithms are complete with respect to the language of rewritings). These results apply not only to query optimization, but to all areas where the goal is to obtain efficient equivalent view-based query rewritings. Second, based on these results we propose two sound algorithms, BDPV and CDPV, that find efficient execution plans for CQ/A queries in terms of materialized views. Both algorithms extend the cost-based query-optimization approach of System R [19]. The efficient sound algorithm BDPV is also complete in some cases, whereas CDPV is sound and complete for all CQ/A queries we consider. We present a study of the completeness-efficiency tradeoff in the algorithms, and provide experimental results that show the viability of our approach and test the limits of query optimization using overlapping views.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {37–48},
numpages = {12}
}

@inproceedings{10.1145/1142473.1142480,
author = {Cohen, Sara},
title = {User-Defined Aggregate Functions: Bridging Theory and Practice},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142480},
doi = {10.1145/1142473.1142480},
abstract = {The ability to create user-defined aggregate functions (UDAs) is rapidly becoming a standard feature in relational database systems. Therefore, problems such as query optimization, query rewriting and view maintenance must take into account queries (or views) with UDAs. There is a wealth of research on these problems for queries with general aggregate functions. Unfortunately, there is a mismatch between the manner in which UDAs are created, and the information that the database system requires in order to apply previous research.The purpose of this paper is to explore this mismatch and to bridge the gap between theory and practice, thereby enabling UDAs to become first-class citizens within the database. Specifically, we consider query optimization, query rewriting and view maintenance for queries with UDAs. For each of these problems we first survey previous results and explore the mismatch between theory and practice. We then present theoretical and practical insights that can be combined to derive a coherent framework for defining UDAs within a database system.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {49–60},
numpages = {12},
keywords = {query optimization, view maintenance, query rewriting, aggregate queries},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142481,
author = {Li, Chengkai and Chen-Chuan Chang, Kevin and Ilyas, Ihab F.},
title = {Supporting Ad-Hoc Ranking Aggregates},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142481},
doi = {10.1145/1142473.1142481},
abstract = {This paper presents a principled framework for efficient processing of ad-hoc top-k (ranking) aggregate queries, which provide the k groups with the highest aggregates as results. Essential support of such queries is lacking in current systems, which process the queries in a na\"{\i}ve materialize-group-sort scheme that can be prohibitively inefficient. Our framework is based on three fundamental principles. The Upper-Bound Principle dictates the requirements of early pruning, and the Group-Ranking and Tuple-Ranking Principles dictate group-ordering and tuple-ordering requirements. They together guide the query processor toward a provably optimal tuple schedule for aggregate query processing. We propose a new execution framework to apply the principles and requirements. We address the challenges in realizing the framework and implementing new query operators, enabling efficient group-aware and rank-aware query plans. The experimental study validates our framework by demonstrating orders of magnitude performance improvement in the new query plans, compared with the traditional plans.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {61–72},
numpages = {12},
keywords = {top-k query processing, decision support, ranking, OLAP, aggregate query},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142483,
author = {Deshpande, Amol and Madden, Samuel},
title = {MauveDB: Supporting Model-Based User Views in Database Systems},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142483},
doi = {10.1145/1142473.1142483},
abstract = {Real-world data --- especially when generated by distributed measurement infrastructures such as sensor networks --- tends to be incomplete, imprecise, and erroneous, making it impossible to present it to users or feed it directly into applications. The traditional approach to dealing with this problem is to first process the data using statistical or probabilistic models that can provide more robust interpretations of the data. Current database systems, however, do not provide adequate support for applying models to such data, especially when those models need to be frequently updated as new data arrives in the system. Hence, most scientists and engineers who depend on models for managing their data do not use database systems for archival or querying at all; at best, databases serve as a persistent raw data store.In this paper we define a new abstraction called model-based views and present the architecture of MauveDB, the system we are building to support such views. Just as traditional database views provide logical data independence, model-based views provide independence from the details of the underlying data generating mechanism and hide the irregularities of the data by using models to present a consistent view to the users. MauveDB supports a declarative language for defining model-based views, allows declarative querying over such views using SQL, and supports several different materialization strategies and techniques to efficiently maintain them in the face of frequent updates. We have implemented a prototype system that currently supports views based on regression and interpolation, using the Apache Derby open source DBMS, and we present results that show the utility and performance benefits that can be obtained by supporting several different types of model-based views in a database system.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {73–84},
numpages = {12},
keywords = {query processing, statistical models, regression, sensor networks, uncertain data, views},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142484,
author = {Kini, Ameet and Shankar, Srinath and Naughton, Jeffrey F. and Dewitt, David J.},
title = {Database Support for Matching: Limitations and Opportunities},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142484},
doi = {10.1145/1142473.1142484},
abstract = {We define a match join of R and S with predicate θ to be a subset of the θ-join of R and S such that each tuple of R and S contributes to at most one result tuple. Match joins and their generalizations belong to a broad class of matching problems that have attracted a great deal of attention in disciplines including operations research and theoretical computer science. Instances of these problems arise in practice in resource allocation scenarios. To the best of our knowledge no one uses an RDBMS as a tool to help solve these problems; our goal in this paper is to explore whether or not this needs to be the case. We show that the simple approach of computing the full θ-join and then applying standard graph-matching algorithms to the result is ineffective for all but the smallest of problem instances. By contrast, a closer study shows that the DBMS primitives of grouping, sorting, and joining can be exploited to yield efficient match join operations. This suggests that RDBMSs can play a role in matching related problems beyond merely serving as expensive file systems exporting data sets to external user programs.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {85–96},
numpages = {12},
keywords = {join, advanced query types, matching},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142485,
author = {Loo, Boon Thau and Condie, Tyson and Garofalakis, Minos and Gay, David E. and Hellerstein, Joseph M. and Maniatis, Petros and Ramakrishnan, Raghu and Roscoe, Timothy and Stoica, Ion},
title = {Declarative Networking: Language, Execution and Optimization},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142485},
doi = {10.1145/1142473.1142485},
abstract = {The networking and distributed systems communities have recently explored a variety of new network architectures, both for application-level overlay networks, and as prototypes for a next-generation Internet architecture. In this context, we have investigated declarative networking: the use of a distributed recursive query engine as a powerful vehicle for accelerating innovation in network architectures [23, 24, 33]. Declarative networking represents a significant new application area for database research on recursive query processing. In this paper, we address fundamental database issues in this domain. First, we motivate and formally define the Network Datalog (NDlog) language for declarative network specifications. Second, we introduce and prove correct relaxed versions of the traditional semi-na\"{\i}ve query evaluation technique, to overcome fundamental problems of the traditional technique in an asynchronous distributed setting. Third, we consider the dynamics of network state, and formalize the iheventual consistencyl. of our programs even when bursts of updates can arrive in the midst of query execution. Fourth, we present a number of query optimization opportunities that arise in the declarative networking context, including applications of traditional techniques as well as new optimizations. Last, we present evaluation results of the above ideas implemented in our P2 declarative networking system, running on 100 machines over the Emulab network testbed.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {97–108},
numpages = {12},
keywords = {recursive queries, declarative networks},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142487,
author = {Pavlou, Kyriacos and Snodgrass, Richard T.},
title = {Forensic Analysis of Database Tampering},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142487},
doi = {10.1145/1142473.1142487},
abstract = {Mechanisms now exist that detect tampering of a database, through the use of cryptographically-strong hash functions. This paper addresses the next problem, that of determining who, when, and what, by providing a systematic means of performing forensic analysis after such tampering has been uncovered. We introduce a schematic representation termed a "corruption diagram" that aids in intrusion investigation. We use these diagrams to fully analyze the original proposal, that of a linked sequence of hash values. We examine the various kinds of intrusions that are possible, including retroactive, introactive, backdating, and postdating intrusions. We then introduce successively more sophisticated forensic analysis algorithms: the monochromatic, RGB, and polychromatic algorithms, and characterize the "forensic strength" of these algorithms. We show how forensic analysis can efficiently extract a good deal of information concerning a corruption event.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {109–120},
numpages = {12},
keywords = {forensic strength, append-only, corruption diagram, cryptographic hash function},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142488,
author = {Li, Feifei and Hadjieleftheriou, Marios and Kollios, George and Reyzin, Leonid},
title = {Dynamic Authenticated Index Structures for Outsourced Databases},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142488},
doi = {10.1145/1142473.1142488},
abstract = {In outsourced database (ODB)systems the database owner publishes its data through a number of remote servers, with the goal of enabling clients at the edge of the network to access and query the data more efficiently. As servers might be untrusted or can be compromised, query authentication becomes an essential component of ODB systems. Existing solutions for this problem concentrate mostly on static scenarios and are based on idealistic properties for certain cryptographic primitives. In this work, first we define a variety of essential and practical cost metrics associated with ODB systems. Then, we analytically evaluate a number of different approaches, in search for a solution that best leverages all metrics. Most importantly, we look at solutions that can handle dynamic scenarios, where owners periodically update the data residing at the servers. Finally, we discuss query freshness, a new dimension in data authentication that has not been explored before. A comprehensive experimental evaluation of the proposed and existing approaches is used to validate the analytical models and verify our claims. Our findings exhibit that the proposed solutions improve performance substantially over existing approaches, both for static and dynamic environments.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {121–132},
numpages = {12},
keywords = {authentication, cryptography, indexing, merkle trees, outsourced databases},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142489,
author = {Kabra, Govind and Ramamurthy, Ravishankar and Sudarshan, S.},
title = {Redundancy and Information Leakage in Fine-Grained Access Control},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142489},
doi = {10.1145/1142473.1142489},
abstract = {The current SQL standard for access control is coarse grained, in that it grants access to all rows of a table or none. Fine-grained access control, which allows control of access at the granularity of individual rows, and to specific columns within those rows, is required in practically all database applications. There are several models for fine grained access control, but the majority of them follow a view replacement strategy. There are two significant problems with most implementations of the view replacement model, namely (a) the unnecessary overhead of the access control predicates when they are redundant and (b) the potential of information leakage through channels such as user-defined functions, and operations that cause exceptions and error messages. We first propose techniques for redundancy removal. We then define when a query plan is safe with respect to UDFs and other unsafe functions, and propose techniques to generate safe query plans. We have prototyped redundancy removal and safe UDF pushdown on the Microsoft SQL Server query optimizer, and present a preliminary performance study.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {133–144},
numpages = {12},
keywords = {fine-grained access control, redundancy, information leakage, query optimization},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142491,
author = {Xue, Wenwei and Luo, Qiong and Chen, Lei and Liu, Yunhao},
title = {Contour Map Matching for Event Detection in Sensor Networks},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142491},
doi = {10.1145/1142473.1142491},
abstract = {Many sensor network applications, such as object tracking and disaster monitoring, require effective techniques for event detection. In this paper, we propose a novel event detection mechanism based on matching the contour maps of in-network sensory data distribution. Our key observation is that events in sensor networks can be abstracted into spatio-temporal patterns of sensory data and that pattern matching can be done efficiently through contour map matching. Therefore, we propose simple SQL extensions to allow users to specify common types of events as patterns in contour maps and study energy-efficient techniques of contour map construction and maintenance for our pattern-based event detection. Our experiments with synthetic workloads derived from a real-world coal mine surveillance application validate the effectiveness and efficiency of our approach.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {145–156},
numpages = {12},
keywords = {patterns, contour maps, sensor networks, event detection},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142492,
author = {Silberstein, Adam and Braynard, Rebecca and Yang, Jun},
title = {Constraint Chaining: On Energy-Efficient Continuous Monitoring in Sensor Networks},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142492},
doi = {10.1145/1142473.1142492},
abstract = {Wireless sensor networks have created new opportunities for data collection in a variety of scenarios, such as environmental and industrial, where we expect data to be temporally and spatially correlated. Researchers may want to continuously collect all sensor data from the network for later analysis. Suppression, both temporal and spatial, provides opportunities for reducing the energy cost of sensor data collection. We demonstrate how both types can be combined for maximal benefit. We frame the problem as one of monitoring node and edge constraints. A monitored node triggers a report if its value changes. A monitored edge triggers a report if the difference between its nodes' values changes. The set of reports collected at the base station is used to derive all node values. We fully exploit the potential of this global inference in our algorithm, CONCH, short for constraint chaining. Constraint chaining builds a network of constraints that are maintained locally, but allow a global view of values to be maintained with minimal cost. Network failure complicates the use of suppression, since either causes an absence of reports. We add enhancements to CONCH to build in redundant constraints and provide a method to interpret the resulting reports in case of uncertainty. Using simulation we experimentally evaluate CONCH's effectiveness against competing schemes in a number of interesting scenarios.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {157–168},
numpages = {12},
keywords = {continuous queries, sensor networks},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142493,
author = {Silberstein, Adam and Munagala, Kamesh and Yang, Jun},
title = {Energy-Efficient Monitoring of Extreme Values in Sensor Networks},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142493},
doi = {10.1145/1142473.1142493},
abstract = {Monitoring extreme values (MAX or MIN) is a fundamental problem in wireless sensor networks (and in general, complex dynamic systems). This problem presents very different algorithmic challenges from aggregate and selection queries, in the sense that an individual node cannot by itself determine its inclusion in the query result. We present novel query processing algorithms for this problem, with the goal of minimizing message traffic in the network. These algorithms employ a hierarchy of local constraints, or thresholds, to leverage network topology such that message-passing is localized. We evaluate all algorithms using simulated and real-world data to study various trade-offs.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {169–180},
numpages = {12},
keywords = {aggregate, max, sensor networks, continuous queries},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142495,
author = {Korn, Flip and Muthukrishnan, S. and Wu, Yihua},
title = {Modeling Skew in Data Streams},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142495},
doi = {10.1145/1142473.1142495},
abstract = {Data stream applications have made use of statistical summaries to reason about the data using nonparametric tools such as histograms, heavy hitters, and join sizes. However, relatively little attention has been paid to modeling stream data parametrically, despite the potential this approach has for mining the data. The challenges to do model fitting at streaming speeds are both technical -- how to continually find fast and reliable parameter estimates on high speed streams of skewed data using small space -- and conceptual -- how to validate the goodness-of-fit and stability of the model online.In this paper, we show how to fit hierarchical (binomial multifractal) and non-hierarchical (Pareto) power-law models on a data stream. We address the technical challenges using an approach that maintains a sketch of the data stream and fits least-squares straight lines; it yields algorithms that are fast, space-efficient, and provide approximations of parameter value estimates with a priori quality guarantees relative to those obtained offline. We address the conceptual challenge by designing fast methods for online goodness-of-fit measurements on a data stream; we adapt the statistical testing technique of examining the quantile-quantile (q-q) plot, to perform online model validation at streaming speeds.As a concrete application of our techniques, we focus on network traffic data which has been shown to exhibit skewed distributions. We complement our analytic and algorithmic results with experiments on IP traffic streams in AT&amp;T's Gigascope® data stream management system, to demonstrate practicality of our methods at line speeds. We measured the stability and robustness of these models over weeks of operational packet data in an IP network. In addition, we study an intrusion detection application, and demonstrate the potential of online parametric modeling.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {181–192},
numpages = {12},
keywords = {estimation, modeling, skew, streaming algorithms},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142496,
author = {Rusu, Florin and Dobra, Alin},
title = {Fast Range-Summable Random Variables for Efficient Aggregate Estimation},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142496},
doi = {10.1145/1142473.1142496},
abstract = {Exact computation for aggregate queries usually requires large amounts of memory - constrained in data-streaming - or communication - constrained in distributed computation - and large processing times. In this situation, approximation techniques with provable guarantees, like sketches, are the only viable solution. The performance of sketches crucially depends on the ability to efficiently generate particular pseudo-random numbers. In this paper we investigate both theoretically and empirically the problem of generating k-wise independent pseudo-random numbers and, in particular, that of generating 3 and 4-wise independent pseudo-random numbers that are fast range-summable (i.e., they can be summed up in sub-linear time). Our specific contributions are: (a) we provide an empirical comparison of the various pseudo-random number generating schemes, (b) we study both theoretically and empirically the fast range-summation practicality for the 3 and 4-wise independent generating schemes and we provide efficient implementations for the 3-wise independent schemes, (c) we show convincing theoretical and empirical evidence that the extended Hamming scheme performs as well as any 4-wise independent scheme for estimating the size of join using AMS-sketches, even though it is only 3-wise independent. We use this generating scheme to produce estimators that significantly out-perform the state-of-the-art solutions for two problems - size of spatial joins and selectivity estimation.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {193–204},
numpages = {12},
keywords = {approximate query processing, data-streaming, sketches},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142497,
author = {Spiegel, Joshua and Polyzotis, Neoklis},
title = {Graph-Based Synopses for Relational Selectivity Estimation},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142497},
doi = {10.1145/1142473.1142497},
abstract = {This paper introduces the Tuple Graph (TUG) synopses, a new class of data summaries that enable accurate selectivity estimates for complex relational queries. The proposed summarization framework adopts a "semi-structured" view of the relational database, modeling a relational data set as a graph of tuples and join queries as graph traversals respectively. The key idea is to approximate the structure of the induced data graph in a concise synopsis, and to estimate the selectivity of a query by performing the corresponding traversal over the summarized graph. We detail the TUG synopsis model that is based on this novel approach, and we describe an efficient and scalable construction algorithm for building accurate TUGs within a specific storage budget. We validate the performance of TUGs with an extensive experimental study on real-life and synthetic data sets. Our results verify the effectiveness of TUGs in generating accurate selectivity estimates for complex join queries, and demonstrate their benefits over existing summarization techniques.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {205–216},
numpages = {12},
keywords = {selectivity, approximation, synopses, relational},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142499,
author = {Kifer, Daniel and Gehrke, Johannes},
title = {Injecting Utility into Anonymized Datasets},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142499},
doi = {10.1145/1142473.1142499},
abstract = {Limiting disclosure in data publishing requires a careful balance between privacy and utility. Information about individuals must not be revealed, but a dataset should still be useful for studying the characteristics of a population. Privacy requirements such as k-anonymity and l-diversity are designed to thwart attacks that attempt to identify individuals in the data and to discover their sensitive information. On the other hand, the utility of such data has not been well-studied.In this paper we will discuss the shortcomings of current heuristic approaches to measuring utility and we will introduce a formal approach to measuring utility. Armed with this utility metric, we will show how to inject additional information into k-anonymous and l-diverse tables. This information has an intuitive semantic meaning, it increases the utility beyond what is possible in the original k-anonymity and l-diversity frameworks, and it maintains the privacy guarantees of k-anonymity and l-diversity.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {217–228},
numpages = {12},
keywords = {l-diversity, marginals, k-anonymity, graphical models, loglinear models},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142500,
author = {Xiao, Xiaokui and Tao, Yufei},
title = {Personalized Privacy Preservation},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142500},
doi = {10.1145/1142473.1142500},
abstract = {We study generalization for preserving privacy in publication of sensitive data. The existing methods focus on a universal approach that exerts the same amount of preservation for all persons, with-out catering for their concrete needs. The consequence is that we may be offering insufficient protection to a subset of people, while applying excessive privacy control to another subset. Motivated by this, we present a new generalization framework based on the concept of personalized anonymity. Our technique performs the minimum generalization for satisfying everybody's requirements, and thus, retains the largest amount of information from the microdata. We carry out a careful theoretical study that leads to valuable insight into the behavior of alternative solutions. In particular, our analysis mathematically reveals the circumstances where the previous work fails to protect privacy, and establishes the superiority of the proposed solutions. The theoretical findings are verified with extensive experiments.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {229–240},
numpages = {12},
keywords = {privacy preservation, k-anonymity},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142501,
author = {Manjhi, Amit and Ailamaki, Anastassia and Maggs, Bruce M. and Mowry, Todd C. and Olston, Christopher and Tomasic, Anthony},
title = {Simultaneous Scalability and Security for Data-Intensive Web Applications},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142501},
doi = {10.1145/1142473.1142501},
abstract = {For Web applications in which the database component is the bottleneck, scalability can be provided by a third-party Database Scalability Service Provider (DSSP) that caches application data and supplies query answers on behalf of the application. Cost-effective DSSPs will need to cache data from many applications, inevitably raising concerns about security. However, if all data passing through a DSSP is encrypted to enhance security, then data updates trigger invalidation of large regions of cache. Consequently, achieving good scalability becomes virtually impossible. There is a tradeoff between security and scalability, which requires careful consideration.In this paper we study the security-scalability tradeoff, both formally and empirically. We begin by providing a method for statically identifying segments of the database that can be encrypted without impacting scalability. Experiments over a prototype DSSP system show the effectiveness of our static analysis method--for all three realistic bench-mark applications that we study, our method enables a significant fraction of the database to be encrypted without impacting scalability. Moreover, most of the data that can be encrypted without impacting scalability is of the type that application designers will want to encrypt, all other things being equal. Based on our static analysis method, we propose a new scalability-conscious security design methodology that features: (a) compulsory encryption of highly sensitive data like credit card information, and (b) encryption of data for which encryption does not impair scalability. As a result, the security-scalability tradeoff needs to be considered only over data for which encryption impacts scalability, thus greatly simplifying the task of managing the tradeoff.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {241–252},
numpages = {12},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142503,
author = {Petropoulos, Michalis and Deutsch, Alin and Papakonstantinou, Yannis},
title = {Interactive Query Formulation over Web Service-Accessed Sources},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142503},
doi = {10.1145/1142473.1142503},
abstract = {Integration systems typically support only a restricted set of queries over the schema they export. The reason is that the participating information sources contribute limited content and limited access methods. In prior work, these limited access methods have often been specified using a set of parameterized views, with the understanding that the integration system accepts only queries which have an equivalent rewriting using the views. These queries are called feasible. Infeasible queries are rejected without an explanatory feedback. To help a developer, who is building an integration application, avoid a frustrating trial-and-error cycle, we introduce the CLIDE query formulation interface, which extends the QBE-like query builder of Microsoft's SQL Server with a coloring scheme that guides the user toward formulating feasible queries. We provide guarantees that the suggested query edit actions are complete (i.e. each feasible query can be built by following only suggestions), rapidly convergent (the suggestions are tuned to lead to the closest feasible completions of the query) and suitably summarized (at each interaction step, only a minimal number of actions needed to preserve completeness are suggested). We present the algorithms, implementation and performance evaluation showing that CLIDE is a viable on-line tool.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {253–264},
numpages = {12},
keywords = {interactive query formulation, data integration, web services},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142504,
author = {Ipeirotis, Panagiotis G. and Agichtein, Eugene and Jain, Pranay and Gravano, Luis},
title = {To Search or to Crawl? Towards a Query Optimizer for Text-Centric Tasks},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142504},
doi = {10.1145/1142473.1142504},
abstract = {Text is ubiquitous and, not surprisingly, many important applications rely on textual data for a variety of tasks. As a notable example, information extraction applications derive structured relations from unstructured text; as another example, focused crawlers explore the web to locate pages about specific topics. Execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan, or 'crawl," the text database or, alternatively, we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. The choice between crawl- and query-based execution plans can have a substantial impact on both execution time and output "completeness" (e.g., in terms of recall). Nevertheless, this choice is typically ad-hoc and based on heuristics or plain intuition. In this paper, we present fundamental building blocks to make the choice of execution plans for text-centric tasks in an informed, cost-based way. Towards this goal, we show how to analyze query- and crawl-based plans in terms of both execution time and output completeness. We adapt results from random-graph theory and statistics to develop a rigorous cost model for the execution plans. Our cost model reflects the fact that the performance of the plans depends on fundamental task-specific properties of the underlying text databases. We identify these properties and present efficient techniques for estimating the associated cost-model parameters. Overall, our approach helps predict the most appropriate execution plans for a task, resulting in significant efficiency and output completeness benefits. We complement our results with a large-scale experimental evaluation for three important text-centric tasks and over multiple real-life data sets.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {265–276},
numpages = {12},
keywords = {information extraction, text databases, metasearching, query optimization, focused crawling, research},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142505,
author = {Chen, Yen-Yu and Suel, Torsten and Markowetz, Alexander},
title = {Efficient Query Processing in Geographic Web Search Engines},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142505},
doi = {10.1145/1142473.1142505},
abstract = {Geographic web search engines allow users to constrain and order search results in an intuitive manner by focusing a query on a particular geographic region. Geographic search technology, also called local search, has recently received significant interest from major search engine companies. Academic research in this area has focused primarily on techniques for extracting geographic knowledge from the web. In this paper, we study the problem of efficient query processing in scalable geographic search engines. Query processing is a major bottleneck in standard web search engines, and the main reason for the thousands of machines used by the major engines. Geographic search engine query processing is different in that it requires a combination of text and spatial data processing techniques. We propose several algorithms for efficient query processing in geographic search engines, integrate them into an existing web search query processor, and evaluate them on large sets of real data and query traces.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {277–288},
numpages = {12},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142507,
author = {Keralapura, Ram and Cormode, Graham and Ramamirtham, Jeyashankher},
title = {Communication-Efficient Distributed Monitoring of Thresholded Counts},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142507},
doi = {10.1145/1142473.1142507},
abstract = {Monitoring is an issue of primary concern in current and next generation networked systems. For ex, the objective of sensor networks is to monitor their surroundings for a variety of different applications like atmospheric conditions, wildlife behavior, and troop movements among others. Similarly, monitoring in data networks is critical not only for accounting and management, but also for detecting anomalies and attacks. Such monitoring applications are inherently continuous and distributed, and must be designed to minimize the communication overhead that they introduce. In this context we introduce and study a fundamental class of problems called "thresholded counts" where we must return the aggregate frequency count of an event that is continuously monitored by distributed nodes with a user-specified accuracy whenever the actual count exceeds a given threshold value.In this paper we propose to address the problem of thresholded counts by setting local thresholds at each monitoring node and initiating communication only when the locally observed data exceeds these local thresholds. We explore algorithms in two categories: static and adaptive thresholds. In the static case, we consider thresholds based on a linear combination of two alternate strategies, and show that there exists an optimal blend of the two strategies that results in minimum communication overhead. We further show that this optimal blend can be found using a steepest descent search. In the adaptive case, we propose algorithms that adjust the local thresholds based on the observed distributions of updated information. We use extensive simulations not only to verify the accuracy of our algorithms and validate our theoretical results, but also to evaluate the performance of our algorithms. We find that both approaches yield significant savings over the naive approach of centralized processing.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {289–300},
numpages = {12},
keywords = {approximate counts, communication efficiency, distributed monitoring, thresholded counts},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142508,
author = {Sharfman, Izchak and Schuster, Assaf and Keren, Daniel},
title = {A Geometric Approach to Monitoring Threshold Functions over Distributed Data Streams},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142508},
doi = {10.1145/1142473.1142508},
abstract = {Monitoring data streams in a distributed system is the focus of much research in recent years. Most of the proposed schemes, however, deal with monitoring simple aggregated values, such as the frequency of appearance of items in the streams. More involved challenges, such as the important task of feature selection (e.g., by monitoring the information gain of various features), still require very high communication overhead using naive, centralized algorithms. We present a novel geometric approach by which an arbitrary global monitoring task can be split into a set of constraints applied locally on each of the streams. The constraints are used to locally filter out data increments that do not affect the monitoring outcome, thus avoiding unnecessary communication. As a result, our approach enables monitoring of arbitrary threshold functions over distributed data streams in an efficient manner. We present experimental results on real-world data which demonstrate that our algorithms are highly scalable, and considerably reduce communication load in comparison to centralized algorithms.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {301–312},
numpages = {12},
keywords = {data streams, distributed monitoring},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142509,
author = {Lim, Hyo-Sang and Lee, Jae-Gil and Lee, Min-Jae and Whang, Kyu-Young and Song, Il-Yeol},
title = {Continuous Query Processing in Data Streams Using Duality of Data and Queries},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142509},
doi = {10.1145/1142473.1142509},
abstract = {Recent data stream systems such as TelegraphCQ have employed the well-known property of duality between data and queries. In these systems, query processing methods are classified into two dual categories -- data-initiative and query-initiative -- depending on whether query processing is initiated by selecting a data element or a query. Although the duality property has been widely recognized, previous data stream systems do not fully take advantages of this property since they use the two dual methods independently: data-initiative methods only for continuous queries and query-initiative methods only for ad-hoc queries. We contend that continuous query processing can be better optimized by adopting an approach that integrates the two dual methods. Our primary contribution is based on the observation that spatial join is a powerful tool for achieving this objective. In this paper, we first present a new viewpoint of transforming the continuous query processing problem to a multi-dimensional spatial join problem. We then present a continuous query processing algorithm based on spatial join, which we name Spatial Join CQ. This algorithm processes continuous queries by finding the pairs of overlapping regions from a set of data elements and a set of queries, both defined as regions in the multi-dimensional space. The algorithm achieves the advantages of the two dual methods simultaneously. Experimental results show that the proposed algorithm outperforms earlier algorithms by up to 36 times for simple selection continuous queries and by up to 7 times for sliding window join queries.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {313–324},
numpages = {12},
keywords = {duality, batch processing, spatial join, continuous queries, data streams},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142511,
author = {Govindaraju, Naga and Gray, Jim and Kumar, Ritesh and Manocha, Dinesh},
title = {GPUTeraSort: High Performance Graphics Co-Processor Sorting for Large Database Management},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142511},
doi = {10.1145/1142473.1142511},
abstract = {We present a novel external sorting algorithm using graphics processors (GPUs) on large databases composed of billions of records and wide keys. Our algorithm uses the data parallelism within a GPU along with task parallelism by scheduling some of the memory-intensive and compute-intensive threads on the GPU. Our new sorting architecture provides multiple memory interfaces on the same PC -- a fast and dedicated memory interface on the GPU along with the main memory interface for CPU computations. As a result, we achieve higher memory bandwidth as compared to CPU-based algorithms running on commodity PCs. Our approach takes into account the limited communication bandwidth between the CPU and the GPU, and reduces the data communication between the two processors. Our algorithm also improves the performance of disk transfers and achieves close to peak I/O performance. We have tested the performance of our algorithm on the SortBenchmark and applied it to large databases composed of a few hundred Gigabytes of data. Our results on a 3 GHz Pentium IV PC with $300 NVIDIA 7800 GT GPU indicate a significant performance improvement over optimized CPU-based algorithms on high-end PCs with 3.6 GHz Dual Xeon processors. Our implementation is able to outperform the current high-end PennySort benchmark and results in a higher performance to price ratio. Overall, our results indicate that using a GPU as a co-processor can significantly improve the performance of sorting algorithms on large databases.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {325–336},
numpages = {12},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142512,
author = {Lomet, David and Vagena, Zografoula and Barga, Roger},
title = {Recovery from "Bad" User Transactions},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142512},
doi = {10.1145/1142473.1142512},
abstract = {User written transaction code is responsible for the "C" in ACID transactions, i.e., taking the database from one consistent state to the next. However, user transactions can be flawed and lead to inconsistent (or invalid) states. Database systems usually correct invalid data using "point in time" recovery, a costly process that installs a backup and rolls it forward. The result is long outages and the "de-commit" of many valid transactions, which must then be re-submitted, frequently manually. We have implemented in our transaction-time database system a technique in which only data tainted by a flawed transaction and transactions dependent upon its updates are "removed". This process identifies and quarantines tainted data despite the complication of determining transactions dependent on data written by the flawed transaction. A further property of our implementation is that no backup needs to be installed for this because the prior transaction-time states provide an online backup.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {337–346},
numpages = {10},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142513,
author = {Liu, Bin and Zhu, Yali and Rundensteiner, Elke},
title = {Run-Time Operator State Spilling for Memory Intensive Long-Running Queries},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142513},
doi = {10.1145/1142473.1142513},
abstract = {Main memory is a critical resource when processing long-running queries over data streams with state intensive operators. In this work, we investigate state spill strategies that handle run-time memory shortage when processing such complex queries by selectively pushing operator states into disks. Unlike previous solutions which all focus on one single operator only, we instead target queries with multiple state intensive operators. We observe an interdependency among multiple operators in the query plan when spilling operator states. We illustrate that existing strategies, which do not take account of this interdependency, become largely ineffective in this query context. Clearly, a consolidated plan level spill strategy must be devised to address this problem. Several data spill strategies are proposed in this paper to maximize the run-time query throughput in memory constrained environments. The bottom-up state spill strategy is an operator-level strategy that treats all data in one operator state equally. More sophisticated partition-level data spill strategies are then proposed to take different characteristics of the input data into account, including the local output, the global output and the global output with penalty strategies. All proposed state spill strategies have been implemented in the D-CAPE continuous query system. The experimental results confirm the effectiveness of our proposed strategies. In particular, the global output strategy and the global output with penalty strategy have shown favorable results as compared to the other two more localized strategies.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {347–358},
numpages = {12},
keywords = {stateful operators, state spill, long running queries, data streams, data partitions},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142515,
author = {Zhang, Zhen and Hwang, Seung-won and Chang, Kevin Chen-Chuan and Wang, Min and Lang, Christian A. and Chang, Yuan-chi},
title = {Boolean + Ranking: Querying a Database by k-Constrained Optimization},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142515},
doi = {10.1145/1142473.1142515},
abstract = {The wide spread of databases for managing structured data, compounded with the expanded reach of the Internet, has brought forward interesting data retrieval and analysis scenarios to RDBMS. In such settings, queries often take the form of k-constrained optimization, with a Boolean constraint and a numeric optimization expression as the goal function, retrieving only the top-k tuples. This paper proposes the concept of supporting such queries, as their nature implies, by a functional optimization machinery over the search space of multiple indices. To realize this concept, we combine the dual perspectives of discrete state search (from the view of indices) and continuous function optimization (from the view of goal functions). We present, as the marriage of the two perspectives, the OPT* framework, which encodes k-constrained optimization as an A* search over the composite space of multiple indices, driven by functional optimization for providing tight heuristics. By processing queries as optimization, OPT* significantly outperforms baseline approaches, with up to 3 orders of magnitude margins.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {359–370},
numpages = {12},
keywords = {index, top-k query, constrained optimization, query processing, A* search},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142516,
author = {Chakrabarti, Kaushik and Ganti, Venkatesh and Han, Jiawei and Xin, Dong},
title = {Ranking Objects Based on Relationships},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142516},
doi = {10.1145/1142473.1142516},
abstract = {In many document collections, documents are related to objects such as document authors, products described in the document, or persons referred to in the document. In many applications, the goal is to find these objects that best match a set of keywords. However, the keywords may not necessarily occur in the target objects; they occur only in the documents. For example, in a product review database, a user might search for names of products (say, laptops) using keywords like "lightweight" and "business use" that occur only in the reviews but not in the names of laptops. In order to answer these queries, we need to exploit relationships between documents containing the keywords and the target objects related to those documents. Current keyword query paradigms do not exploit these relationships effectively and hence are inefficient for these queries.In this paper, we consider a class of queries called the "object finder" queries. Our main intuition is to exploit the relationships between searchable documents and related objects and further "aggregate" the document scores from these relationships in order to find the best ranking target objects. Building upon existing keyword search engines such as full text search, we design efficient algorithms that exploit the requirement of only the best k target objects to terminate early. The main challenge here is to push early termination through blocking operators such as group by and aggregation. Our experiments with real datasets and workloads demonstrate the effectiveness of our techniques. Although we present our techniques in the context of keyword search, our techniques apply to other types of ranked searches (e.g., multimedia search) as well.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {371–382},
numpages = {12},
keywords = {top-k queries, keyword search, ranking, aggregation, relationships, named entities, early termination},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@dataset{10.1145/review-1142473.1142516_R41279,
author = {Papadopoulos, Apostolos N},
title = {Review ID:R41279 for DOI: 10.1145/1142473.1142516},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1142473.1142516_R41279}
}

@inproceedings{10.1145/1142473.1142517,
author = {Agrawal, Rakesh and Rantzau, Ralf and Terzi, Evimaria},
title = {Context-Sensitive Ranking},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142517},
doi = {10.1145/1142473.1142517},
abstract = {Contextual preferences take the form that item i1 is preferred to item i2 in the context of X. For example, a preference might state the choice for Nicole Kidman over Penelope Cruz in drama movies, whereas another preference might choose Penelope Cruz over Nicole Kidman in the context of Spanish dramas. Various sources provide preferences independently and thus preferences may contain cycles and contradictions. We reconcile democratically the preferences accumulated from various sources and use them to create a priori orderings of tuples in an off-line preprocessing step. Only a few representative orders are saved, each corre-sponding to a set of contexts. These orders and associated contexts are used at query time to expeditiously provide ranked answers. We formally define contextual preferences, provide algorithms for creating orders and processing queries, and present experimental results that show their efficacy and practical utility.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {383–394},
numpages = {12},
keywords = {ranking, preferences},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142518,
author = {Das, Gautam and Hristidis, Vagelis and Kapoor, Nishant and Sudarshan, S.},
title = {Ordering the Attributes of Query Results},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142518},
doi = {10.1145/1142473.1142518},
abstract = {There has been a great deal of interest in the past few years on ranking of results of queries on structured databases, including work on probabilistic information retrieval, rank aggregation, and algorithms for merging of ordered lists. In many applications, for example sales of homes, used cars or electronic goods, data items have a very large number of attributes. When displaying a (ranked) list of items to users, only a few attributes can be shown. Traditionally, these are selected manually. We argue that automatic selection of attributes is required to deal with different requirements of different users. We formulate the problem as an optimization problem of choosing the most "useful" set of attributes, that is, the attributes that are most influential in the ranking of the items. We discuss different variants of our notion of attribute usefulness, and propose a hybrid Split-Pane approach that returns a composite of the top attributes of each variant. We conduct both a performance and a user study illustrating the benefits of our algorithms in terms of efficiency and quality of explanation.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {395–406},
numpages = {12},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142520,
author = {Wu, Eugene and Diao, Yanlei and Rizvi, Shariq},
title = {High-Performance Complex Event Processing over Streams},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142520},
doi = {10.1145/1142473.1142520},
abstract = {In this paper, we present the design, implementation, and evaluation of a system that executes complex event queries over real-time streams of RFID readings encoded as events. These complex event queries filter and correlate events to match specific patterns, and transform the relevant events into new composite events for the use of external monitoring applications. Stream-based execution of these queries enables time-critical actions to be taken in environments such as supply chain management, surveillance and facility management, healthcare, etc. We first propose a complex event language that significantly extends existing event languages to meet the needs of a range of RFID-enabled monitoring applications. We then describe a query plan-based approach to efficiently implementing this language. Our approach uses native operators to efficiently handle query-defined sequences, which are a key component of complex event processing, and pipeline such sequences to subsequent operators that are built by leveraging relational techniques. We also develop a large suite of optimization techniques to address challenges such as large sliding windows and intermediate result sizes. We demonstrate the effectiveness of our approach through a detailed performance analysis of our prototype implementation under a range of data and query workloads as well as through a comparison to a state-of-the-art stream processor.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {407–418},
numpages = {12},
keywords = {events, query optimization, RFID, complex event language, sequences, streams},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142521,
author = {Gedik, Bugra and Liu, Ling},
title = {Quality-Aware Dstributed Data Delivery for Continuous Query Services},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142521},
doi = {10.1145/1142473.1142521},
abstract = {We consider the problem of distributed continuous data delivery services in an overlay network of heterogeneous nodes. Each node in the system can be a source for any number of data streams and at the same time be a consumer node that is receiving streams sourced at other nodes. A consumer node may define a filter on a source stream such that only the desired portion of the stream is delivered, minimizing the amount of unnecessary bandwidth consumption. By heterogeneous, we mean that nodes not only may have varying network bandwidths and computing resources but also different interests in terms of the filters and the rates of the data streams they are interested in. Our objective is to construct an efficient stream delivery network in which nodes cooperate in forwarding data streams in the presence of constrained resources. We formalize this distributed stream delivery problem as an optimization one by starting with a simple setup where the network topology is fixed and node bandwidth characteristics are known. The goal of the optimization is to find valid delivery graphs with minimum bandwidth consumption. We extend this problem formulation to QoS-aware stream delivery, in order to handle the bandwidth constrained cases in which unwanted drops and delays are inevitable. We provide a classification of delivery graph construction schemes, and in light of this classification we develop pragmatic quality-aware stream delivery (QASD) algorithms. These algorithms aim at constructing efficient stream delivery graphs in a distributed setting, where global knowledge is not available and network characteristics are not known in advance. We introduce a set of evaluation metrics and provide experimental results to illustrate the effectiveness of our proposed algorithms under these metrics.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {419–430},
numpages = {12},
keywords = {stream delivery, quality of service},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142522,
author = {Jain, Navendu and Amini, Lisa and Andrade, Henrique and King, Richard and Park, Yoonho and Selo, Philippe and Venkatramani, Chitra},
title = {Design, Implementation, and Evaluation of the Linear Road Bnchmark on the Stream Processing Core},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142522},
doi = {10.1145/1142473.1142522},
abstract = {Stream processing applications have recently gained significant attention in the networking and database community. At the core of these applications is a stream processing engine that performs resource allocation and management to support continuous tracking of queries over collections of physically-distributed and rapidly-updating data streams. While numerous stream processing systems exist, there has been little work on understanding the performance characteristics of these applications in a distributed setup. In this paper, we examine the performance bottlenecks of streaming data applications, in particular the Linear Road stream data management benchmark, in achieving good performance in large-scale distributed environments, using the Stream Processing Core (SPC), a stream processing middleware we have developed. First, we present the design and implementation of the Linear Road benchmark on the SPC middleware. SPC has been designed to scale to tens of thousands of processing nodes, while supporting concurrent applications and multiple simultaneous queries. Second, we identify the main performance bottlenecks in the Linear Road application in achieving scalability and low query response latency. Our results show that data locality, buffer capacity, physical allocation of processing elements to infrastructure nodes, and packaging for transporting streamed data are important factors in achieving good application performance. Though we evaluate our system primarily for the Linear Road application, we believe it also provides useful insights into the overall system behavior for supporting other distributed and large-scale continuous streaming data applications. Finally, we examine how SPC can be used and tuned to enable a very efficient implementation of the Linear Road application in a distributed environment.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {431–442},
numpages = {12},
keywords = {distributed stream processing systems, linear road, bottleneck analysis, performance evaluation},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142524,
author = {Onose, Nicola and Deutsch, Alin and Papakonstantinou, Yannis and Curtmola, Emiran},
title = {Rewriting Nested XML Queries Using Nested Views},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142524},
doi = {10.1145/1142473.1142524},
abstract = {We present and analyze an algorithm for equivalent rewriting of XQuery queries using XQuery views, which is complete for a large class of XQueries featuring nested FLWR blocks, XML construction and join equalities by value and identity. These features pose significant challenges which lead to fundamental extension of prior work on the problems of rewriting conjunctive and tree pattern queries. Our solution exploits the Nested XML Tableaux (NEXT) notation which enables a logical foundation for specifying XQuery semantics. We present a tool which inputs XQuery queries and views and outputs an XQuery rewriting, thus being usable on top of any of the existing XQuery processing engines. Our experimental evaluation shows that the tool scales well for large numbers of views and complex queries.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {443–454},
numpages = {12},
keywords = {XML nested query, reformulation, views, rewriting},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142525,
author = {Cho, SungRan and Koudas, Nick and Srivastava, Divesh},
title = {Meta-Data Indexing for XPath Location Steps},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142525},
doi = {10.1145/1142473.1142525},
abstract = {XML is the de facto standard for data representation and exchange over the Web. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and sensitivity. When querying such XML data, say using XPath, it is important to efficiently identify the data that meet specified constraints on the meta-data. For example, different users may be satisfied with different levels of quality guarantees, or may only have access to different parts of the XML data based on specified security policies. In this paper, we address the problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints, when the meta-data levels are specifically drawn from an ordered domain (e.g., accuracy in [0,1], recency using timestamps, multi-level security, etc.). More specifically, we develop a family of index structures, which we refer to as meta-data indexes, to address this problem. A meta-data index is easily instantiated using a multi-dimensional index structure, such as an R-tree, incorporating novel query and update algorithms. We show that the full meta-data index (FMI), based on associating each XML element with its meta-data level, has a very high update cost for modifying an element's meta-data level. We resolve this problem by designing the inheritance meta-data index (IMI), in which (i) actual meta-data levels are associated only with elements for which this value is explicitly specified, and (ii) inherited meta-data levels and inheritance source nodes are associated with non-leaf nodes of the index structure. We design efficient query (for all XPath axes) and update (of meta-data levels) algorithms for the IMI, and experimentally demonstrate the superiority of the IMI over the FMI using benchmark data sets.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {455–466},
numpages = {12},
keywords = {hierarchical inheritance, XML, meta-data index},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142526,
author = {Mathis, Christian and H\"{a}rder, Theo and Haustein, Michael},
title = {Locking-Aware Structural Join Operators for XML Query Processing},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142526},
doi = {10.1145/1142473.1142526},
abstract = {As observed in many publications so far, the matching of twig pattern queries (i.e., queries that contain only the child and the descendant axis) is a core operation in XML database management systems (XDBMSs) for which the structural join and the holistic twig join algorithms were proposed. In a single-user environment, especially the latter algorithm provides a good evaluation strategy. However, when it comes to multi-user access to a single XML document, it may lead to extensive blocking situations: The XDBMS has to ensure data consistency and, therefore, has to prevent concurrent modification operations from changing elements in the input sequences, a holistic twig algorithm accesses while operating. To circumvent this problem, we propose a set of new locking-aware operators for twig pattern query evaluation that rely on stable path labels (SPLIDs) as well as document and element set indexes. Furthermore, by running extensive tests on our own XDBMS, we show that their performance is comparable to existing approaches in a single-user environment, and leads to higher throughput rates in the case of multi-user access.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {467–478},
numpages = {12},
keywords = {query processing, XML documents, node labeling, structural joins, concurrency control},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142527,
author = {Boncz, Peter and Grust, Torsten and van Keulen, Maurice and Manegold, Stefan and Rittinger, Jan and Teubner, Jens},
title = {MonetDB/XQuery: A Fast XQuery Processor Powered by a Relational Engine},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142527},
doi = {10.1145/1142473.1142527},
abstract = {Relational XQuery systems try to re-use mature relational data management infrastructures to create fast and scalable XML database technology. This paper describes the main features, key contributions, and lessons learned while implementing such a system. Its architecture consists of (i) a range-based encoding of XML documents into relational tables, (ii) a compilation technique that translates XQuery into a basic relational algebra, (iii) a restricted (order) property-aware peephole relational query optimization strategy, and (iv) a mapping from XML update statements into relational updates. Thus, this system implements all essential XML database functionalities (rather than a single feature) such that we can learn from the full consequences of our architectural decisions. While implementing this system, we had to extend the state-of-the-art with a number of new technical contributions, such as loop-lifted staircase join and efficient relational query evaluation strategies for XQuery theta-joins with existential semantics. These contributions as well as the architectural lessons learned are also deemed valuable for other relational back-end engines. The performance and scalability of the resulting system is evaluated on the XMark benchmark up to data sizes of 11GB. The performance section also provides an extensive benchmark comparison of all major XMark results published previously, which confirm that the goal of purely relational XQuery processing, namely speed and scalability, was met.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {479–490},
numpages = {12},
keywords = {performance, XQuery, XML, scalability, relational XQuery systems, RDBMS},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142529,
author = {Xia, Tian and Zhang, Donghui},
title = {Refreshing the Sky: The Compressed Skycube with Efficient Support for Frequent Updates},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142529},
doi = {10.1145/1142473.1142529},
abstract = {The skyline query is important in many applications such as multi-criteria decision making, data mining, and user-preference queries. Given a set of d-dimensional objects, the skyline query finds the objects that are not dominated by others. In practice, different users may be interested in different dimensions of the data, and issue queries on any subset of d dimensions. This paper focuses on supporting concurrent and unpredictable subspace skyline queries in frequently updated databases. Simply to compute and store the skyline objects of every subspace in a skycube will incur expensive update cost. In this paper, we investigate the important issue of updating the skycube in a dynamic environment. To balance the query cost and update cost, we propose a new structure, the compressed skycube, which concisely represents the complete skycube. We thoroughly explore the properties of the compressed skycube and provide an efficient object-aware update scheme. Experimental results show that the compressed skycube is both query and update efficient.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {491–502},
numpages = {12},
keywords = {skyline, update scheme, compressed skycube},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142530,
author = {Chan, Chee-Yong and Jagadish, H. V. and Tan, Kian-Lee and Tung, Anthony K. H. and Zhang, Zhenjie},
title = {Finding K-Dominant Skylines in High Dimensional Space},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142530},
doi = {10.1145/1142473.1142530},
abstract = {Given a d-dimensional data set, a point p dominates another point q if it is better than or equal to q in all dimensions and better than q in at least one dimension. A point is a skyline point if there does not exists any point that can dominate it. Skyline queries, which return skyline points, are useful in many decision making applications.Unfortunately, as the number of dimensions increases, the chance of one point dominating another point is very low. As such, the number of skyline points become too numerous to offer any interesting insights. To find more important and meaningful skyline points in high dimensional space, we propose a new concept, called k-dominant skyline which relaxes the idea of dominance to k-dominance. A point p is said to k-dominate another point q if there are k ≤ d dimensions in which p is better than or equal to q and is better in at least one of these k dimensions. A point that is not k-dominated by any other points is in the k-dominant skyline.We prove various properties of k-dominant skyline. In particular, because k-dominant skyline points are not transitive, existing skyline algorithms cannot be adapted for k-dominant skyline. We then present several new algorithms for finding k-dominant skyline and its variants. Extensive experiments show that our methods can answer different queries on both synthetic and real data sets efficiently.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {503–514},
numpages = {12},
keywords = {query processing, high dimensional space, ranking, k-dominant, skyline, data mining},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142531,
author = {Achtert, Elke and B\"{o}hm, Christian and Kr\"{o}ger, Peer and Kunath, Peter and Pryakhin, Alexey and Renz, Matthias},
title = {Efficient Reverse K-Nearest Neighbor Search in Arbitrary Metric Spaces},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142531},
doi = {10.1145/1142473.1142531},
abstract = {The reverse k-nearest neighbor (RkNN) problem, i.e. finding all objects in a data set the k-nearest neighbors of which include a specified query object, is a generalization of the reverse 1-nearest neighbor problem which has received increasing attention recently. Many industrial and scientific applications call for solutions of the RkNN problem in arbitrary metric spaces where the data objects are not Euclidean and only a metric distance function is given for specifying object similarity. Usually, these applications need a solution for the generalized problem where the value of k is not known in advance and may change from query to query. However, existing approaches, except one, are designed for the specific R1NN problem. In addition - to the best of our knowledge - all previously proposed methods, especially the one for generalized RkNN search, are only applicable to Euclidean vector data but not for general metric objects. In this paper, we propose the first approach for efficient RkNN search in arbitrary metric spaces where the value of k is specified at query time. Our approach uses the advantages of existing metric index structures but proposes to use conservative and progressive distance approximations in order to filter out true drops and true hits. In particular, we approximate the k-nearest neighbor distance for each data object by upper and lower bounds using two functions of only two parameters each. Thus, our method does not generate any considerable storage overhead. We show in a broad experimental evaluation on real-world data the scalability and the usability of our novel approach.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {515–526},
numpages = {12},
keywords = {metric index structure, reverse nearest neighbor search},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142532,
author = {Vu, Khanh and Hua, Kien A. and Cheng, Hao and Lang, Sheau-Dong},
title = {A Non-Linear Dimensionality-Reduction Technique for Fast Similarity Search in Large Databases},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142532},
doi = {10.1145/1142473.1142532},
abstract = {To enable efficient similarity search in large databases, many indexing techniques use a linear transformation scheme to reduce dimensions and allow fast approximation. In this reduction approach the approximation is unbounded, so that the approximation volume extends across the dataspace. This causes over-estimation of retrieval sets and impairs performance.This paper presents a non-linear transformation scheme that extracts two important parameters specifying the data. We prove that these parameters correspond to a bounded volume around the search sphere, irrespective of dimensionality. We use a special workspace-mapping mechanism to derive tight bounds for the parameters and to prove further results, as well as highlighting insights into the problems and our proposed solutions. We formulate a measure that lower-bounds the Euclidean distance, and discuss the implementation of the technique upon a popular index structure. Extensive experiments confirm the superiority of this technique over recent state-of-the-art schemes.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {527–538},
numpages = {12},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142534,
author = {Buneman, Peter and Chapman, Adriane and Cheney, James},
title = {Provenance Management in Curated Databases},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142534},
doi = {10.1145/1142473.1142534},
abstract = {Curated databases in bioinformatics and other disciplines are the result of a great deal of manual annotation, correction and transfer of data from other sources. Provenance information concerning the creation, attribution, or version history of such data is crucial for assessing its integrity and scientific value. General purpose database systems provide little support for tracking provenance, especially when data moves among databases. This paper investigates general-purpose techniques for recording provenance for data that is copied among databases. We describe an approach in which we track the user's actions while browsing source databases and copying data into a curated database, in order to record the user's actions in a convenient, queryable form. We present an implementation of this technique and use it to evaluate the feasibility of database support for provenance management. Our experiments show that although the overhead of a naive approach is fairly high, it can be decreased to an acceptable level using simple optimizations.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {539–550},
numpages = {12},
keywords = {provenance, storage, curation},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142535,
author = {Papadomanolakis, Stratos and Ailamaki, Anastassia and Lopez, Julio C. and Tu, Tiankai and O'Hallaron, David R. and Heber, Gerd},
title = {Efficient Query Processing on Unstructured Tetrahedral Meshes},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142535},
doi = {10.1145/1142473.1142535},
abstract = {Modern scientific applications such as fluid dynamics and earthquake modeling heavily depend on massive volumes of data produced by computer simulations. Such applications require new data management capabilities in order to scale to terabyte-scale data volumes. The most common way to discretize the application domain is to decompose it into pyramids, forming an unstructured tetrahedral mesh. Modern simulations generate meshes of high resolution and precision, to be queried by a visualization or analysis tool. Tetrahedral meshes are extremely flexible and therefore vital to accurately model complex geometries, but also are difficult to index. To reduce query execution time, applications either use only subsets of the data or rely on different (less flexible) structures, thereby trading accuracy for speed.This paper presents efficient indexing techniques for common spatial (point and range) on tetrahedral meshes. Because the prevailing multidimensional indexing techniques attempt to approximate the tetrahedra using simpler shapes (primarily rectangles) the query performance deteriorates significantly as a function of the mesh's geometric complexity. We develop Directed Local Search (DLS), an efficient indexing algorithm based on mesh topology information that is practically insensitive to the geometric properties of meshes. We show how DLS can be easily and efficiently implemented within modern DBMS without requiring new exotic index structures and complex preprocessing. Finally, we present a new data layout approach for tetrahedral mesh datasets that provides better performance for scientific applications.compared to the traditional space filling curves. In our PostgreSQL implementation DLS reduces the number of disk page accesses by 26% to 4x, and improves the overall query execution time by 25% to 4.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {551–562},
numpages = {12},
keywords = {tetrahedral mesh indexing, spatial indexing},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142536,
author = {Liu, Fang and Yu, Clement and Meng, Weiyi and Chowdhury, Abdur},
title = {Effective Keyword Search in Relational Databases},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142536},
doi = {10.1145/1142473.1142536},
abstract = {With the amount of available text data in relational databases growing rapidly, the need for ordinary users to search such information is dramatically increasing. Even though the major RDBMSs have provided full-text search capabilities, they still require users to have knowledge of the database schemas and use a structured query language to search information. This search model is complicated for most ordinary users. Inspired by the big success of information retrieval (IR) style keyword search on the web, keyword search in relational databases has recently emerged as a new research topic. The differences between text databases and relational databases result in three new challenges: (1) Answers needed by users are not limited to individual tuples, but results assembled from joining tuples from multiple tables are used to form answers in the form of tuple trees. (2) A single score for each answer (i.e. a tuple tree) is needed to estimate its relevance to a given query. These scores are used to rank the most relevant answers as high as possible. (3) Relational databases have much richer structures than text databases. Existing IR strategies to rank relational outputs are not adequate. In this paper, we propose a novel IR ranking strategy for effective keyword search. We are the first that conducts comprehensive experiments on search effectiveness using a real world database and a set of keyword queries collected by a major search company. Experimental results show that our strategy is significantly better than existing strategies. Our approach can be used both at the application level and be incorporated into a RDBMS to support keyword-based search in relational databases.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {563–574},
numpages = {12},
keywords = {information retrieval (IR), relational database, keyword search, effectiveness, term weighting},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142537,
author = {Amer-Yahia, Sihem and Curtmola, Emiran and Deutsch, Alin},
title = {Flexible and Efficient XML Search with Complex Full-Text Predicates},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142537},
doi = {10.1145/1142473.1142537},
abstract = {Recently, there has been extensive research that generated a wealth of new XML full-text query languages, ranging from simple Boolean search to combining sophisticated proximity and order predicates on keywords. While computing least common ancestors of query terms was proposed for efficient evaluation of conjunctive keyword queries by exploiting the document structure, no such solution was developed to evaluate complex full-text queries. We present efficient evaluation algorithms based on a formalization of XML queries in terms of keyword patterns and an algebra which manipulates pattern matches. Our algebra captures most existing languages and their varying semantics and our algorithms combine relational query evaluation techniques with the exploitation of document structure to process queries with complex full-text predicates. We show how scoring can be incorporated into our framework without compromising the algorithms complexity. Our experiments show that considering element nesting dramatically improves the performance of queries with complex full-text predicates.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {575–586},
numpages = {12},
keywords = {XML text search, full-text search, text predicates},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142539,
author = {Chandramouli, Badrish and Xie, Junyi and Yang, Jun},
title = {On the Database/Network Interface in Large-Scale Publish/Subscribe Systems},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142539},
doi = {10.1145/1142473.1142539},
abstract = {The work performed by a publish/subscribe system can conceptually be divided into subscription processing and notification dissemination. Traditionally, research in the database and networking communities has focused on these aspects in isolation. The interface between the database server and the network is often overlooked by previous research. At one extreme, database servers are directly responsible for notifying individual subscribers; at the other extreme, updates are injected directly into the network, and the network is solely responsible for processing subscriptions and forwarding notifications. These extremes are unsuitable for complex and stateful subscription queries. A primary goal of this paper is to explore the design space between the two extremes, and to devise solutions that incorporate both database-side and network-side considerations in order to reduce the communication and server load and maintain system scalability. Our techniques apply to a broad range of stateful query types, and we present solutions for several of them. Our detailed experiments based on real and synthetic workloads with varying characteristics and link-level network simulation show that by exploiting the query semantics and building an appropriate interface between the database and the network, it is possible to achieve orders-of-magnitude savings in network traffic at low server-side processing cost.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {587–598},
numpages = {12},
keywords = {query processing, complex subscriptions, content-based forwarding network, publish/subscribe systems},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142540,
author = {Bernstein, Philip A. and Fekete, Alan and Guo, Hongfei and Ramakrishnan, Raghu and Tamma, Pradeep},
title = {Relaxed-Currency Serializability for Middle-Tier Caching and Replication},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142540},
doi = {10.1145/1142473.1142540},
abstract = {Many applications, such as e-commerce, routinely use copies of data that are not in sync with the database due to heuristic caching strategies used to enhance performance. We study concurrency control for a transactional model that allows update transactions to read out-of-date copies. Each read operation carries a "freshness constraint" that specifies how fresh a copy must be in order to be read. We offer a definition of correctness for this model and present algorithms to ensure several of the most interesting freshness constraints. We outline a serializability-theoretic correctness proof and present the results of a detailed performance study.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {599–610},
numpages = {12},
keywords = {serializability, database, replication, caching, freshness constraint, transaction},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142541,
author = {Papaemmanouil, Olga and Ahmad, Yanif and \c{C}etintemel, U\u{g}ur and Jannotti, John and Yildirim, Yenel},
title = {Extensible Optimization in Overlay Dissemination Trees},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142541},
doi = {10.1145/1142473.1142541},
abstract = {We introduce XPORT, a profile-driven distributed data dissemination system that supports an extensible set of data types, profile types, and optimization metrics. XPORT efficiently implements a generic tree-based overlay network, which can be customized per application using a small number of methods that encapsulate application-specific data filtering, profile aggregation, and optimization logic. The clean separation between the "plumbing" and "application" enables the system to uniformly support disparate dissemination-based applications.We first provide an overview of the basic XPORT model and architecture. We then describe in detail an extensible optimization framework, based on a two-level aggregation model, that facilitates easy specification of a wide range of commonly used performance goals. We discuss distributed tree transformation protocols that allow XPORT to iteratively optimize its operation to achieve these goals under changing network and application conditions. Finally, we demonstrate the flexibility and the effectiveness of XPORT using real-world data and experimental results obtained from both prototype-based LAN emulation and deployment on PlanetLab.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {611–622},
numpages = {12},
keywords = {overlay networks, dissemination},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142543,
author = {Krishnamurthy, Sailesh and Wu, Chung and Franklin, Michael},
title = {On-the-Fly Sharing for Streamed Aggregation},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142543},
doi = {10.1145/1142473.1142543},
abstract = {Data streaming systems are becoming essential for monitoring applications such as financial analysis and network intrusion detection. These systems often have to process many similar but different queries over common data. Since executing each query separately can lead to significant scalability and performance problems, it is vital to share resources by exploiting similarities in the queries. In this paper we present ways to efficiently share streaming aggregate queries with differing periodic windows and arbitrary selection predicates. A major contribution is our sharing technique that does not require any up-front multiple query optimization. This is a significant departure from existing techniques that rely on complex static analyses of fixed query workloads. Our approach is particularly vital in streaming systems where queries can join and leave the system at any point. We present a detailed performance study that evaluates our strategies with an implementation and real data. In these experiments, our approach gives us as much as an order of magnitude performance improvement over the state of the art.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {623–634},
numpages = {12},
keywords = {aggregation, streaming data, shared processing, multiple-query optimization},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142544,
author = {Mouratidis, Kyriakos and Bakiras, Spiridon and Papadias, Dimitris},
title = {Continuous Monitoring of Top-k Queries over Sliding Windows},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142544},
doi = {10.1145/1142473.1142544},
abstract = {Given a dataset P and a preference function f, a top-k query retrieves the k tuples in P with the highest scores according to f. Even though the problem is well-studied in conventional databases, the existing methods are inapplicable to highly dynamic environments involving numerous long-running queries. This paper studies continuous monitoring of top-k queries over a fixed-size window W of the most recent data. The window size can be expressed either in terms of the number of active tuples or time units. We propose a general methodology for top-k monitoring that restricts processing to the sub-domains of the workspace that influence the result of some query. To cope with high stream rates and provide fast answers in an on-line fashion, the data in W reside in main memory. The valid records are indexed by a grid structure, which also maintains book-keeping information. We present two processing techniques: the first one computes the new answer of a query whenever some of the current top-k points expire; the second one partially pre-computes the future changes in the result, achieving better running time at the expense of slightly higher space requirements. We analyze the performance of both algorithms and evaluate their efficiency through extensive experiments. Finally, we extend the proposed framework to other query types and a different data stream model.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {635–646},
numpages = {12},
keywords = {sliding windows, continuous queries, top-k processing},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142545,
author = {Papadimitriou, Spiros and Yu, Philip},
title = {Optimal Multi-Scale Patterns in Time Series Streams},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142545},
doi = {10.1145/1142473.1142545},
abstract = {We introduce a method to discover optimal local patterns, which concisely describe the main trends in a time series. Our approach examines the time series at multiple time scales (i.e., window sizes) and efficiently discovers the key patterns in each. We also introduce a criterion to select the best window sizes, which most concisely capture the key oscillatory as well as aperiodic trends. Our key insight lies in learning an optimal orthonormal transform from the data itself, as opposed to using a predetermined basis or approximating function (such as piecewise constant, short-window Fourier or wavelets), which essentially restricts us to a particular family of trends. We go one step further, lifting even that limitation. Furthermore, our method lends itself to fast, incremental estimation in a streaming setting. Experimental evaluation shows that our method can capture meaningful patterns in a variety of settings. Our streaming approach requires order of magnitude less time and space, while still producing concise and informative patterns.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {647–658},
numpages = {12},
keywords = {empirical orthogonal functions, local patterns, multi-scale, SVD, stream, singular spectrum},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142547,
author = {Li, Cuiping and Ooi, Beng Chin and Tung, Anthony K. H. and Wang, Shan},
title = {DADA: A Data Cube for Dominant Relationship Analysis},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142547},
doi = {10.1145/1142473.1142547},
abstract = {The concept of dominance has recently attracted much interest in the context of skyline computation. Given an N-dimensional data set S, a point p is said to dominate q if p is better than q in at least one dimension and equal to or better than it in the remaining dimensions. In this paper, we propose extending the concept of dominance for business analysis from a microeconomic perspective. More specifically, we propose a new form of analysis, called Dominant Relationship Analysis (DRA), which aims to provide insight into the dominant relationships between products and potential buyers. By analyzing such relationships, companies can position their products more effectively while remaining profitable.To support DRA, we propose a novel data cube called DADA (Data Cube for Dominant Relationship Analysis), which captures the dominant relationships between products and customers. Three types of queries called Dominant Relationship Queries (DRQs) are consequently proposed for analysis purposes: 1)Linear Optimization Queries (LOQ), 2)Subspace Analysis Queries (SAQ), and 3)Comparative Dominant Queries (CDQ). Algorithms are designed for efficient computation of DADA and answering the DRQs using DADA. Results of our comprehensive experiments show the effectiveness and efficiency of DADA and its associated query processing strategies.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {659–670},
numpages = {12},
keywords = {skyline, microeconomic data mining, dominant relationship analysis, data cube},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142548,
author = {Abadi, Daniel and Madden, Samuel and Ferreira, Miguel},
title = {Integrating Compression and Execution in Column-Oriented Database Systems},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142548},
doi = {10.1145/1142473.1142548},
abstract = {Column-oriented database system architectures invite a re-evaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads.In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in row-oriented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {671–682},
numpages = {12},
keywords = {query execution, column-stores, database compression, column-oriented databases},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142549,
author = {Agrawal, Sanjay and Chu, Eric and Narasayya, Vivek},
title = {Automatic Physical Design Tuning: Workload as a Sequence},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142549},
doi = {10.1145/1142473.1142549},
abstract = {The area of automatic selection of physical database design to optimize the performance of a relational database system based on a workload of SQL queries and updates has gained prominence in recent years. Major database vendors have released automated physical database design tools with the goal of reducing the total cost of ownership. An important assumption underlying these tools is that the workload is a set of SQL statements. In this paper, we show that being able to treat the workload as a sequence, i.e., exploiting the ordering of statements can significantly broaden the usage of such tools. We present scenarios where exploiting sequence information in the workload is crucial for performance tuning. We also propose techniques for addressing the technical challenges arising from treating the workload as a sequence. We evaluate the effectiveness of our techniques through experiments on Microsoft SQL Server.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {683–694},
numpages = {12},
keywords = {sequence, workload, tuning, physical design},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142551,
author = {Carey, Michael},
title = {Data Delivery in a Service-Oriented World: The BEA AquaLogic Data Services Platform},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142551},
doi = {10.1145/1142473.1142551},
abstract = {"Wow. I fell asleep listening to SOA music, and when I woke up, I couldn't remember where I'd put my data. Now what?" Has this happened to you? With the new push towards service-oriented architectures (SOA) and process orientation, data seems to have been lost in the shuffle. At the end of the day, however, applications are still about data, and SOA applications are no different. In this paper, we present BEA's approach to serving up data to SOA applications. BEA recently introduced a new middleware product called the AquaLogic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture. ALDSP provides a new, declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources. The paper covers both the foundation and the key features of ALDSP, including its underlying technologies, its overall system architecture, and its most interesting capabilities.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {695–705},
numpages = {11},
keywords = {data services, service-oriented architecture, query languages, XQuery, integration, XML},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142552,
author = {Meijer, Erik and Beckman, Brian and Bierman, Gavin},
title = {LINQ: Reconciling Object, Relations and XML in the .NET Framework},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142552},
doi = {10.1145/1142473.1142552},
abstract = {Many software applications today need to handle data from different data models; typically objects from the host programming language along with the relational and XML data models. The ROX impedance mismatch makes programs awkward to write and hard to maintain.The .NET Language-Integrated Query (LINQ) framework, proposed for the next release of the .NET framework, approaches this problem by defining a pattern of general-purpose standard query operators for traversal, filter, and projection. Based on this pattern, any .NET language can define special query comprehension syntax that is subsequently compiled into these standard operators (our code examples are in VB).Besides the general query operators, the LINQ framework also defines two domain specific APIs that work over XML (XLinq) and relational data (DLinq) respectively. The operators over XML use a lightweight and easy to use in-memory XML representation to provide XQuery-style expressiveness in the host programming language. The operators over relational data provide a simple OR mapping by leveraging remotable queries that are executed directly in the back-end relational store.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {706},
numpages = {1},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142553,
author = {Gutmans, Andi},
title = {PHP: Supporting the New Paradigm of Situational and Composite Web Applications},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142553},
doi = {10.1145/1142473.1142553},
abstract = {In this paper, I describe what we see as a paradigm shift in software development and how PHP plays into this change.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {707},
numpages = {1},
keywords = {web services, open-source, SOA, PHP, web},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142555,
author = {Chidlovskii, Boris and Roustant, Bruno and Brette, Marc},
title = {Documentum ECI Self-Repairing Wrappers: Performance Analysis},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142555},
doi = {10.1145/1142473.1142555},
abstract = {Documentum Enterprise Content Integration (ECI) services is a content integration middleware that provides one-query access to the Intranet and Internet content resources. The ECI Adapter technology offers an interface to any application for data and metadata extraction from unstructured Web pages. It offers a unique frame-work of wrapper production, automatic recovery and maintenance, developed at Xerox Research Centre Europe and based on state-of-art algorithms from machine learning and grammatical inference. In this presentation we analyze the performance of ECI adapters deployed in current commercial installations. We benefit from accessing reports on daily tests for all ECI commercially deployed adapters collected from June 2003 to September 2005. Using the daily reports, we analyze different aspects of the wrapper technology.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {708–717},
numpages = {10},
keywords = {content integration, wrapper maintenance, web wrappers},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142556,
author = {Jonas, Jeff},
title = {Identity Resolution: 23 Years of Practical Experience and Observations at Scale},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142556},
doi = {10.1145/1142473.1142556},
abstract = {Identity Resolution is a semantic reconciliation activity as applied to people and organizations. Identity resolution is most frequently quantified in terms of accuracy (false positives and false negatives), however, there are additional metrics by which to evaluate identity resolution algorithms including: methodology, persistence, streaming versus batch, data survivorship, operationalizing historical data, transaction/window size, ingestion speed, end-to-end latency, sequence neutrality, handling of ambiguous conditions, reconcilability, scalability, sustainability, and operational characteristics at scale. As well, a technique for "analytics in the anonymized data space" will be presented that makes it possible to resolve identities in a more privacy-preserving manner.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {718},
numpages = {1},
keywords = {anonymous identity resolution, semantic reconciliation, identity resolution},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142557,
author = {Koudas, Nick and Marathe, Amit and Srivastava, Divesh},
title = {Using SPIDER: An Experience Report},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142557},
doi = {10.1145/1142473.1142557},
abstract = {At AT&amp;T Labs-Research, we have been developing a prototype system called SPIDER to efficiently support flexible string matching of attribute values in large databases. SPIDER has been used in AT&amp;T, both as a key component of an operational portal for matching customer names and addresses, and for a variety of ad hoc data quality analyses. In this talk, we report on experiences with SPIDER.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {719},
numpages = {1},
keywords = {flexible string matching, data quality},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142559,
author = {Matias, Yossi},
title = {Trends in High Performance Analytics},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142559},
doi = {10.1145/1142473.1142559},
abstract = {With the proliferation of analytic and business intelligence applications, and with the persistent growth in data sizes, there is an ever increasing need to support high performance analytics. This talk will present recent technological trends in addressing this need, and will particularly highlight the approach of facilitating high performance analytics in a relational database via a novel dichotomous combination with a non-relational aggregation-server.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {720},
numpages = {1},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142560,
author = {Hanrahan, Pat},
title = {VizQL: A Language for Query, Analysis and Visualization},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142560},
doi = {10.1145/1142473.1142560},
abstract = {Conventional query languages such as SQL and MDX have limited formatting and visualization capabilities. Thus, although powerful queries can be composed, another layer of software is needed to report or present the results in a useful form to the analyst. VizQL™ is designed to fill that gap. VizQL evolved from the Polaris system at Stanford, which combined query, analysis and visualization into a single framework [1].VizQL is a formal language for describing tables, charts, graphs, maps, time series and tables of visualizations. These different types of visual representations are unified into one framework, making it easy to switch from one visual representation to another (e.g. from a list view to a cross-tab to a chart). Unlike current charting packages and like query languages, VizQL permits an unlimited number of picture expressions. Visualizations can thus be easily customized and controlled. VizQL is a declarative language. The desired picture is described; the low-level operations needed to retrieve the results, to perform analytical calculations, to map the results to a visual representation, and to render the image are generated automatically by the query analyzer. The query analyzer compiles VizQL expressions to SQL and MDX and thus VizQL can be used with relational databases and datacubes. The current implementation supports Hyperion Essbase, Microsoft SQL Server, Microsoft Analysis Services, MySQL, Oracle, as well as desktop data sources such as CSV and Excel files. This analysis phase includes many optimizations that allow large databases to be browsed interactively. VizQL enables a new generation of visual analysis tools that closely couple query, analysis and visualization.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {721},
numpages = {1},
keywords = {OLAP, visualization},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142562,
author = {Gawlick, Dieter and Krishnaprasad, Muralidhar and Liu, Zhen Hua},
title = {Using the Oracle Database as a Declarative RSS Hub},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142562},
doi = {10.1145/1142473.1142562},
abstract = {The interaction with the Web has historically evolved from static bookmarks to dynamic searches to the current usage of active notification mechanisms based on popular protocols like RSS or Atom. In the same time a large volume of important source data is still contained in relational databases. The talk will analyze the way the Oracle database participates to the activation of the data and opening the state changes in a standard and secure way for easy integrating with the rest of the push based Web protocols. We will study the declarative specification of RSS feeds generated based on the state changes detected in the data stored in the Oracle database. On the opposite, external RSS feeds can be injected to the database and processed declaratively in conjunction with the rest of the data. Most of the technical pieces required for such a solution are already supported by the database engine (e.g. declarative XML processing, state change notifications, queues, crawlers, continuous queries), effectively turning the database into a declarative XML hub. The advantages of using database solutions for such problems in an enterprise context are security, scalability and reliability.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {722},
numpages = {1},
keywords = {event, hub, RSS, XML, XQuery, atom, declarative},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142563,
author = {Lyndersay, Sean},
title = {Windows and RSS: Beyond Blogging},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142563},
doi = {10.1145/1142473.1142563},
abstract = {RSS (and related technologies like Atom) are gaining significant traction as a means for allowing users to "subscribe" to content on the web and get notified when new content is available. More recently, "podcasting" -- a simple extension to RSS to enable references to audio files -- has taken off as a means to subscribe to episodal audio content. More generally, RSS feeds are being used in many arenas to communicate all sorts of different types of content, either using extensions to the RSS format, or simply by transmitting binary files.At its heart, RSS is a very simple XML-based format with very simple semantics, but the potential uses appear endless. This talk will examine many of the uses of RSS, and discuss why this simple format has become so important that Microsoft is building native support for RSS into its next generation operating system and browser platforms.It will also cover many of the technical challenges inherent in building scalable support for RSS into a client operating system.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {723},
numpages = {1},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142564,
author = {Zuzarte, Calisto and Yu, Xiaohui},
title = {Fast Approximate Computation of Statistics on Views},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142564},
doi = {10.1145/1142473.1142564},
abstract = {Accurate estimation of the sizes of intermediate query results (cardinality estimation) is of critical importance to plan costing in query optimization. The common practice in current commercial database systems such as IBM DB2 Universal Database (DB2 UDB) is to derive the cardinality estimates from base-table statistics. However, this approach often suffers from simplifying yet unrealistic assumptions that have to be made about the underlying data (for example, different attributes are independently distributed).Ways for exploiting statistics on query expressions (or, statistics on views, or SITs) have been proposed to improve the accuracy of cardinality estimation. We propose a novel method for efficient computation of SITs for joins. In particular, we are concerned with statistics on join queries involving large fact tables and relatively small dimension tables. Rather than materializing the views, we make use of the frequency statistics that are available on the fact tables to obtain an approximate estimate of the statistics on various attributes in the join results. The dimension tables are generally much smaller than the fact table, and therefore we can afford to closely examine the dimension table, while at the same time avoid accessing the fact table. By closely examining the dimension table, we are able to capture the correlations between the attributes in the dimension table as well as the skew and domain range of the fact table join column values. This leads to reasonably accurate statistics on the join result. We prototyped this idea as a module on top of DB2 UDB, and our experience shows that employment of this technique results in a very significant speed-up in the computation of SITs, at the expense of only slight degradation in accuracy compared with the full-materialization method.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {724},
numpages = {1},
keywords = {query optimization, selectivity estimation},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142566,
author = {Hsieh, Wilson and Madhavan, Jayant and Pike, Rob},
title = {Data Management Projects at Google},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142566},
doi = {10.1145/1142473.1142566},
abstract = {This session describes three data management projects at Google. BigTable is a highly scalable system for distributed storage and querying of structured data. Sawzall is a system for large-scale analysis of data sets that have a flat but regular structure. Finally, GoogleBase is a system for storing and searching structured data contributed by external parties.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {725–726},
numpages = {2},
keywords = {heterogeneity, scalability, data analysis, data management},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142568,
author = {Daly, Mark and Mandelbaum, Yitzhak and Walker, David and Fern\'{a}ndez, Mary and Fisher, Kathleen and Gruber, Robert and Zheng, Xuan},
title = {PADS: An End-to-End System for Processing Ad Hoc Data},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142568},
doi = {10.1145/1142473.1142568},
abstract = {Enormous amounts of data exist in "well-behaved" formats such as relational tables and XML, which come equipped with extensive tool support. However, vast amounts of data also exist in non-standard or ad hoc data formats, which often lack standard or extensible tools. This deficiency forces data analysts to implement their own tools for parsing, querying, and analyzing their ad hoc data. The resulting tools typically interleave parsing, querying, and analysis, obscuring the semantics of the data format and making it nearly impossible for others to resuse the tools. This proposal describes PADS, an end-to-end system for processing ad hoc data sources. The core of PADS is a declarative language for describing ad hoc data sources and a data-description compiler that produces customizable libraries for parsing the ad hoc data. A suite of tools built around this core includes statistical data-profiling tools, a query engine that permits viewing ad hoc sources as XML and for querying them with XQuery, and an interactive front-end that helps users produce PADS descriptions quickly.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {727–729},
numpages = {3},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142569,
author = {Bychkovsky, V. and Chen, K. and Goraczko, M. and Hu, H. and Hull, B. and Miu, A. and Shih, E. and Zhang, Y. and Balakrishnan, H. and Madden, S.},
title = {Data Management in the CarTel Mobile Sensor Computing System},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142569},
doi = {10.1145/1142473.1142569},
abstract = {We propose a reusable data management system, called CarTel, for querying and collecting data from intermittently connected devices. CarTel provides a simple, incrementally-deployable platform for developing automobile-based sensor applications. Our platform provides a dynamic query system that allows both continuous (standing) and one-shot geo-spatial queries over car position, speed, and sensory data as well as a both a low-cost/high-bandwidth substrate for communicating with a large network of mobile devices.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {730–732},
numpages = {3},
keywords = {query processing, wireless, mobility, sensor networks, intermittent connectivity},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142570,
author = {Borgwardt, Karsten and B\"{o}ttger, Sebastian and Kriegel, Hans-Peter},
title = {VGM: Visual Graph Mining},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142570},
doi = {10.1145/1142473.1142570},
abstract = {As more and more graph data become available in various application domains, graph mining is of ever increasing importance in data management.Graph kernels are a novel and successful method for data mining in graphs. Unfortunately, implementing graph kernels is not trivial, and few applied researchers have therefore used graph kernels so far. In this demonstration, we present a Java software package called Visual Graph Mining (VGM). VGM allows the user to classify graphs using graph kernels and Support Vector Machines in a graphical user interface that is easy to learn and use. It is linked to LIBSVM for Support Vector Machine computations, yet can be easily transferred to other Support Vector Machine packages. Furthermore, VGM provides basic data mining features such as Nearest Neighbor search, graph algorithms such as Dijkstra, Floyd-Warshall, and computes and visualizes product graphs and topological indices of graphs.VGM 's homepage can be found at: http://www.cip.ifi.lmu.de/~boettger/sigmod.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {733–735},
numpages = {3},
keywords = {support vector machines, graph kernels, kernel methods, graph mining},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142571,
author = {Dohzen, Tiffany and Pamuk, Mujde and Seong, Seok-Won and Hammer, Joachim and Stonebraker, Michael},
title = {Data Integration through Transform Reuse in the Morpheus Project},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142571},
doi = {10.1145/1142473.1142571},
abstract = {We discuss Morpheus, a data transformation construction tool and associated repository. The architecture of Morpheus is motivated by the goal to reuse (pieces of) previously written transformations to solve data integration problems by finding relevant ones in the repository and then modifying them for repurposing. In addition, Morpheus is integrated with a DBMS so as to leverage existing capabilities including the runtime environment for transforms. We discuss the architecture of Morpheus and illustrate its usage with the help of a simple transform construction scenario.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {736–738},
numpages = {3},
keywords = {transformation construction, information integration, data transformation, transformation reuse},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142572,
author = {Binnig, Carsten and Kossmann, Donald and Lo, Eric},
title = {Testing Database Applications},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142572},
doi = {10.1145/1142473.1142572},
abstract = {Testing database application is challenging because most methods and tools developed for application testing do not consider the database state during the test. In this paper we demonstrate three different tools for testing database applications: HTDGen, HTTrace and HTPar. HTDGen generates meaningful test databases for database applications. HTTrace executes database applications testing efficiently and HTPar extends HTTrace to run tests in parallel.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {739–741},
numpages = {3},
keywords = {relational database, database applications, testing tools},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142573,
author = {Borkar, Vinayak and Carey, Michael and Lychagin, Dmitry and Westmann, Till},
title = {The BEA AquaLogic Data Services Platform (Demo)},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142573},
doi = {10.1145/1142473.1142573},
abstract = {We showcase the BEA AquaLogic Data Services Platform (ALDSP), a middleware infrastructure product that enables the declarative development of data services for service-oriented architectures (SOA). ALDSP includes support for modeling networks of interrelated data services, for realizing data services using either graphical or source-based XQuery editors, for testing data services as they are developed, and for identifying and incorporating changes in the structure of the underlying sources of data. Physical data sources supported include relational tables and views, Web services, packaged applications, stored procedures, XML files, delimited files, and custom Java applications. Data service definitions can be layered; as with relational views, such layering is virtual, and is rewritten away at query compilation time. ALDSP supports both read and update data service functions, and the ALDSP XML query runtime includes a number of interesting query operators and distributed query optimizations. In addition, ALDSP supports function caching, fine-grained security, and SQL-based data access as well as providing service-based and XQuery access to SOA data. We plan to demonstrate as much of this as time permits.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {742–744},
numpages = {3},
keywords = {service-oriented architecture, XML, integration, XQuery, query languages, data services},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142574,
author = {Callahan, Steven P. and Freire, Juliana and Santos, Emanuele and Scheidegger, Carlos E. and Silva, Cl\'{a}udio T. and Vo, Huy T.},
title = {VisTrails: Visualization Meets Data Management},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142574},
doi = {10.1145/1142473.1142574},
abstract = {Scientists are now faced with an incredible volume of data to analyze. To successfully analyze and validate various hypothesis, it is necessary to pose several queries, correlate disparate data, and create insightful visualizations of both the simulated processes and observed phenomena. Often, insight comes from comparing the results of multiple visualizations. Unfortunately, today this process is far from interactive and contains many error-prone and time-consuming tasks. As a result, the generation and maintenance of visualizations is a major bottleneck in the scientific process, hindering both the ability to mine scientific data and the actual use of the data. The VisTrails system represents our initial attempt to improve the scientific discovery process and reduce the time to insight. In VisTrails, we address the problem of visualization from a data management perspective: VisTrails manages the data and metadata of a visualization product. In this demonstration, we show the power and flexibility of our system by presenting actual scenarios in which scientific visualization is used and showing how our system improves usability, enables reproducibility, and greatly reduces the time required to create scientific visualizations.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {745–747},
numpages = {3},
keywords = {data provenance, visualization, scientific dataflows},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142575,
author = {Ding, Hui and Trajcevski, Goce and Scheuermann, Peter},
title = {OMCAT: Optimal Maintenance of Continuous Queries' Answers for Trajectories},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142575},
doi = {10.1145/1142473.1142575},
abstract = {We present our prototype system, OMCAT, which optimizes the reevaluation of a set of pending continuous spatio-temporal queries on trajectory data, when some of the trajectories are affected by traffic abnormalities reported. The key observation that motivates OMCAT is that an abnormality in a given geographical region may cause changes to the answers of queries pertaining to future portions of affected trajectories. We investigate the sources of context-switching costs at various levels and propose solutions that utilize the correlation of several context dimensions to orchestrate the reevaluation of the queries. OMCAT, fully implemented on top of an existing Object Relational Database Management System - Oracle 9i, demonstrates that our techniques can substantially reduce the response time during query answer update.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {748–750},
numpages = {3},
keywords = {moving objects databases, triggers, continuous queries},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142577,
author = {Ziegler, Patrick and Kiefer, Christoph and Sturm, Christoph and Dittrich, Klaus R. and Bernstein, Abraham},
title = {Generic Similarity Detection in Ontologies with the SOQA-SimPack Toolkit},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142577},
doi = {10.1145/1142473.1142577},
abstract = {Ontologies are increasingly used to represent the intended real-world semantics of data and services in information systems. Unfortunately, different data sources often do not relate to the same ontologies when describing their semantics. Consequently, it is desirable to have information about the similarity between ontology concepts for ontology alignment and integration. In this demo, we present the SOQA-SimPack Toolkit (SST), an ontology language independent Java API that enables generic similarity detection and visualization in ontologies. We demonstrate SST's usefulness with the SOQA-SimPack Toolkit Browser that allows users to graphically perform similarity calculations in ontologies.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {751–753},
numpages = {3},
keywords = {similarity, ontology},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142578,
author = {Plattner, Christian and Wapf, Andreas and Alonso, Gustavo},
title = {Searching in Time},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142578},
doi = {10.1145/1142473.1142578},
abstract = {This demonstration shows how to use external databases to provide an efficient implementation of a timetravel service. The timetravel semantics are defined using snapshot isolation. The system presented not only allows to retrieve older snapshots but also to identify snapshots of interest.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {754–756},
numpages = {3},
keywords = {time-split replication, satellite database, snapshot isolation, timetravel},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142579,
author = {Klein, Anja and Gemulla, Rainer and R\"{o}sch, Philipp and Lehner, Wolfgang},
title = {Derby/S: A DBMS for Sample-Based Query Answering},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142579},
doi = {10.1145/1142473.1142579},
abstract = {Although approximate query processing is a prominent way to cope with the requirements of data analysis applications, current database systems do not provide integrated and comprehensive support for these techniques. To improve this situation, we propose an SQL extension---called SQL/S---for approximate query answering using random samples, and present a prototypical implementation within the engine of the open-source database system Derby---called Derby/S. Our approach significantly reduces the required expert knowledge by enabling the definition of samples in a declarative way; the choice of the specific sampling scheme and its parametrization is left to the system. SQL/S introduces new DDL commands to easily define and administrate random samples subject to a given set of optimization criteria. Derby/S automatically takes care of sample maintenance if the underlying dataset changes. Finally, samples are transparently used during query processing, and error bounds are provided. Our extensions do not affect traditional queries and provide the means to integrate sampling as a first-class citizen into a DBMS.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {757–759},
numpages = {3},
keywords = {sampling, approximate query answering},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142580,
author = {Gerner, Nicholas and Yang, Fan and Demers, Alan and Gehrke, Johannes and Riedewald, Mirek and Shanmugasundaram, Jayavel},
title = {Automatic Client-Server Partitioning of Data-Driven Web Applications},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142580},
doi = {10.1145/1142473.1142580},
abstract = {Current application development tools provide completely different programming models for the application server (e.g., Java and J2EE) and the client web browser (e.g., JavaScript and HTML). Consequently, the application developer is forced to partition the application code between the server and client at the time of writing the application. However, the partitioning of the code between the client and server may have to be changed during the evolution of the application for performance reasons (it may be better to push more functionality to the client), for correctness reasons (data that conflicts with multiple clients cannot always be pushed to clients), and for supporting clients with different computing power (browsers on desktops vs. PDAs). Since the client and server use different programming models, moving application code from client to server (and vice versa) reduces programmer productivity and also has the potential to introduce concurrency bugs. In this demonstration, we advocate an alternative solution to this problem: we propose developing applications using a unified declarative high-level language called Hilda, and show how a Hilda compiler can automatically (and correctly) partition Hilda code between the client and the server using a real Course Management System application. We illustrate our techniques using two clients: a powerful laptop machine and a less powerful PDA.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {760–762},
numpages = {3},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142581,
author = {Signer, Beat and Norrie, Moira C. and Grossniklaus, Michael and Belotti, Rudi and Decurtins, Corsin and Weibel, Nadir},
title = {Paper-Based Mobile Access to Databases},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142581},
doi = {10.1145/1142473.1142581},
abstract = {Our demonstration is a paper-based interactive guide for visitors to the world's largest international arts festival that was developed as part of a project investigating new forms of context-aware information delivery and interaction in mobile environments. Information stored in a database is accessed from a set of interactive paper documents, including a printed festival brochure, a city map and a bookmark. Active areas are defined within the documents and selection of these using a special digital pen causes the corresponding query request along with context data to be sent to a festival application database and the response is returned to the visitor in the form of generated speech output. In addition to paper-based information browsing and transactions such as ticket booking, the digital pen can also be applied for data capture of event ratings and handwritten comments on events. The system integrates three main database components - a cross-media information platform, a content management framework for multi-channel context-aware publishing of data and the festival application database.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {763–765},
numpages = {3},
keywords = {web publishing, interactive paper, tourist guide, voice interface, mobile information system},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142582,
author = {Duan, Songyun and Babu, Shivnath},
title = {Proactive Identification of Performance Problems},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142582},
doi = {10.1145/1142473.1142582},
abstract = {We propose to demonstrate Fa, an automated tool for timely and accurate prediction of Service-Level-Agreement (SLA) violations caused by performance problems in database systems. Fa periodically collects performance data at three levels: applications, database server, and operating system. This data is used to construct probabilistic models for predicting SLA violations. Fa currently uses graphical Bayesian network models because of their ability to support a wide range of inferences, including prediction and diagnosis, as well as their support for interactive visualization and presentation of complex system behavior in intuitive ways.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {766–768},
numpages = {3},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142583,
author = {Papaemmanouil, Olga and Ahmad, Yanif and \c{C}etintemel, U\u{g}ur and Jannotti, John and Yildirim, Yenel},
title = {XPORT: Extensible Profile-Driven Overlay Routing Trees},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142583},
doi = {10.1145/1142473.1142583},
abstract = {XPORT is a profile-driven distributed data collection and dissemination system that supports an extensible set of data types, profiles, and optimization metrics. XPORT efficiently builds a generic tree-based overlay network, which can be customized per application using a small number of methods that encapsulate application-specific data-profile matching, aggregation, and optimization logic. The clean separation between the "plumbing" and "application" enables XPORT to uniformly and easily support disparate dissemination-based applications such as content-based feed dissemination and application-level multicast. We propose to demonstrate the basic XPORT system, featuring its extensible optimization framework that facilitates easy specification of a wide range of useful performance goals and a continuous, adaptive optimization model to achieve these goals under changing network and application conditions. We will use two different underlying applications, an RSS feed dissemination application and a multiplayer network game, along with visual system-monitoring tools to illustrate the extensibility and the operational aspects of XPORT.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {769–771},
numpages = {3},
keywords = {overlay networks, dissemination},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142584,
author = {Deutsch, Alin and Sui, Liying and Vianu, Victor and Zhou, Dayou},
title = {A System for Specification and Verification of Interactive, Data-Driven Web Applications},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142584},
doi = {10.1145/1142473.1142584},
abstract = {When comparing alternative query execution plans (QEPs), a cost-based query optimizer in a relational database management system needs to estimate the selectivity of conjunctive predicates. To avoid inaccurate independence assumptions, modern optimizers try to exploit multivariate statistics (MVS) that provide knowledge about joint frequencies in a table of a relation. Because the complete joint distribution is almost always too large to store, optimizers are given only partial knowledge about this distribution. As a result, there exist multiple, non-equivalent ways to estimate the selectivity of a conjunctive predicate. To consistently combine the partial knowledge during the estimation process, existing optimizers employ cumbersome ad hoc heuristics. These methods unjustifiably ignore valuable information, and the optimizer tends to favor QEPs for which the least information is available. This bias problem yields poor QEP quality and performance. We demonstrate MAXENT, a novel approach based on the maximum entropy principle, prototyped in IBM DB2 LUW. We illustrate MAXENT's ability to consistently estimate the selectivity of conjunctive predicates on a per-table basis. In contrast to the DB2 optimizer's current ad hoc methods, we show how MAXENT exploits all available information about the joint column distribution and thus avoids the bias problem. For some complex queries against a real-world database, we show that MAXENT improves selectivity estimates by orders of magnitude relative to the current DB2 optimizer, and also show how these improved estimate influence plan choices as well as query execution times.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {772–774},
numpages = {3},
keywords = {automatic verification, database-driven, web application, specification, interactive},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142586,
author = {Markl, V. and Kutsch, M. and Tran, T. M. and Haas, P. J. and Megiddo, N.},
title = {MAXENT: Consistent Cardinality Estimation in Action},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142586},
doi = {10.1145/1142473.1142586},
abstract = {When comparing alternative query execution plans (QEPs), a cost-based query optimizer in a relational database management system needs to estimate the selectivity of conjunctive predicates. To avoid inaccurate independence assumptions, modern optimizers try to exploit multivariate statistics (MVS) that provide knowledge about joint frequencies in a table of a relation. Because the complete joint distribution is almost always too large to store, optimizers are given only partial knowledge about this distribution. As a result, there exist multiple, non-equivalent ways to estimate the selectivity of a conjunctive predicate. To consistently combine the partial knowledge during the estimation process, existing optimizers employ cumbersome ad hoc heuristics. These methods unjustifiably ignore valuable information, and the optimizer tends to favor QEPs for which the least information is available. This bias problem yields poor QEP quality and performance. We demonstrate MAXENT, a novel approach based on the maximum entropy principle, prototyped in IBM DB2 LUW. We illustrate MAXENT's ability to consistently estimate the selectivity of conjunctive predicates on a per-table basis. In contrast to the DB2 optimizer's current ad hoc methods, we show how MAXENT exploits all available information about the joint column distribution and thus avoids the bias problem. For some complex queries against a real-world database, we show that MAXENT improves selectivity estimates by orders of magnitude relative to the current DB2 optimizer, and also show how these improved estimate influence plan choices as well as query execution times.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {775–777},
numpages = {3},
keywords = {query optimization, cardinality bias, selectivity estimation, maximum entropy},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142587,
author = {Shen, Jialie and Shepherd, John and Ngu, Anne},
title = {InMAF: Indexing Music Databases via Multiple Acoustic Features},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142587},
doi = {10.1145/1142473.1142587},
abstract = {Music information processing has become very important due to the ever-growing amount of music data from emerging applications. In this demonstration,we present a novel approach for generating small but comprehensive music descriptors to facilitate efficient content music management (accessing and retrieval, in particular). Unlike previous approaches that rely on low-level spectral features adapted from speech analysis technology, our approach integrates human music perception to enhance the accuracy of the retrieval and classification process via PCA and neural networks. The superiority of our method is demonstrated by comparing it with state-of-the-art approaches in the areas of music classification query effectiveness, and robustness against various audio distortion/alternatives.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {778–780},
numpages = {3},
keywords = {music databases, indexing, retrieval},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142588,
author = {Bhaskar, Anand and Botev, Chavdar and Muthaia Chettiar, Muthiah M. and Guo, Lin and Shanmugasundaram, Jayavel and Shao, Feng and Yang, Fan},
title = {Quark: An Efficient XQuery Full-Text Implementation},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142588},
doi = {10.1145/1142473.1142588},
abstract = {The XQuery 1.0 and XPath 2.0 Full-text (XQFT) language has been developed by the W3C to extend XQuery and XPath with full-text search capabilities. XQFT allows users to specify a mix of structured and complex full-text predicates, and also allows users to score/rank such queries. The power and flexibility of XQFT gives rise to two interesting questions. First, is it possible to efficiently integrate a full-function XML query language with sophisticated full-text search? Second, is it possible to score and rank arbitrary XQuery and XQFT queries? In this demonstration, we present evidence that it is indeed possible to achieve the above goals. We demonstrate the Quark open-source data management system and show how we can seamlessly and efficiently integrate structured and unstructured search over XML data. In particular, we demonstrate (a) techniques for efficiently evaluating keyword search over virtual XML views, and (b) a framework for scoring both structured and full-text predicates.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {781–783},
numpages = {3},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142589,
author = {Ramamritham, Krithi and Bahuman, Anil and Duttagupta, Subhasri},
title = {AAqua: A Database-Backended Multilingual, Multimedia Community Forum},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142589},
doi = {10.1145/1142473.1142589},
abstract = {aAQUA is an online multilingual, multimedia Agricultural portal for disseminating information from and to rural communities. It answers farmers' queries based on the location, season, crop and other information provided by farmers. aAQUA makes use of novel database systems and information retrieval techniques like intelligent caching, offline access with intermittent synchronization, semantic-based search, etc. aAQUA's large scale deployment provides avenues for researchers to contribute in the areas of knowledge management, cross-lingual information retrieval, and providing accessible content for rural populations. Apart from agriculture, aAQUA can be configured and customized for expert advice in education, healthcare and other domains of interest to a developing population. This demonstration showcases the utility of various component DB/IR technologies built into aAQUA to enhance the QoS delivered to rural populations.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {784–786},
numpages = {3},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142590,
author = {Lee, Ken C. K. and Lee, Wang-Chien and Winter, Julian and Zheng, Baihua and Xu, Jianliang},
title = {CS Cache Engine: Data Access Accelerator for Location-Based Service in Mobile Environments},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142590},
doi = {10.1145/1142473.1142590},
abstract = {Location-based services (LBS) have emerged as one of the killer applications for mobile and pervasive computing environments. Due to limited bandwidth and scarce client resources, client-side data caching plays an important role of enhancing the data availability and improving the response time. In this demonstration, we present CS Cache Engine suitable for LBS. The underlying caching model is Complementary Space Caching (CS caching) scheme that we have recently presented in [citation]. Different from conventional data caching schemes, CS caching preserves a global view of the database by maintaining physical objects and capturing those objects in the server but not in the cache as Complementary Regions (CRs) in the cache. As a result, with the CS Cache Engine implementing CS caching, client assertiveness on their own answered queries is enhanced so that unnecessary requests over the wireless channel can be avoided; various kinds of location-based queries are naturally supported; and the client's ability to prefetch objects is introduced such that the response time can be further improved. In this demonstration paper, we discuss the architecture and the functionality of the CS Caching Engine that adopts CS caching. Specifically, for this demonstration, a tourist information named TravelGuide is prototyped with the support of this cache engine.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {787–789},
numpages = {3},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142591,
author = {Kandogan, Eser and Krishnamurthy, Rajasekar and Raghavan, Sriram and Vaithyanathan, Shivakumar and Zhu, Huaiyu},
title = {Avatar Semantic Search: A Database Approach to Information Retrieval},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142591},
doi = {10.1145/1142473.1142591},
abstract = {We present Avatar Semantic Search, a prototype search engine that exploits annotations in the context of classical keyword search. The process of annotations is accomplished offline by using high-precision information extraction techniques to extract facts, con-cepts, and relationships from text. These facts and concepts are represented and indexed in a structured data store. At runtime, keyword queries are interpreted in the context of these extracted facts and converted into one or more precise queries over the structured store. In this demonstration we describe the overall architecture of the Avatar Semantic Search engine. We also demonstrate the superiority of the AVATAR approach over traditional keyword search engines using Enron email data set and a blog corpus.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {790–792},
numpages = {3},
keywords = {database approach to retrieval, semantic search, information extraction},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142592,
author = {Schnaitter, Karl and Abiteboul, Serge and Milo, Tova and Polyzotis, Neoklis},
title = {COLT: Continuous on-Line Tuning},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142592},
doi = {10.1145/1142473.1142592},
abstract = {The physical schema of a database plays a critical role in performance. Self-tuning is a cost-effective and elegant solution to optimize the physical configuration for the characteristics of the query load. Existing techniques operate in an off-line fashion, by choosing a fixed configuration that is tailored to a subset of the query load. The generated configurations therefore ignore any temporal patterns that may exist in the actual load submitted to the system.This demonstration introduces COLT (Continuous On-Line Tuning), a novel self-tuning framework that continuously monitors the incoming queries and adjusts the system configuration in order to maximize query performance. The key idea behind COLT is to gather performance statistics at different levels of detail and to carefully allocate profiling resources to the most promising candidate configurations. Moreover, COLT uses effective heuristics to regulate its own performance, lowering its overhead when the system is well-tuned, and being more aggressive when the workload shifts and it becomes necessary to re-tune the system. We present a specialization of COLT to the important problem of selecting an effective set of relational indices for the current query load. Our demonstration will use an implementation of our proposed framework in the PostgreSQL database system, showing the internal operation of COLT and the adaptive selection of indices as we vary the query load of the server.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {793–795},
numpages = {3},
keywords = {COLT, on-line, demonstration, index, self-tuning, selection},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142593,
author = {Hwang, Heasoo and Hristidis, Vagelis and Papakonstantinou, Yannis},
title = {ObjectRank: A System for Authority-Based Search on Databases},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142593},
doi = {10.1145/1142473.1142593},
abstract = {We present ObjectRank demo system that performs authority-based keyword search on bibliographic databases. We also provide Inverse ObjectRank as a keyword-specific specificity metric and other calibration parameters such as Global ObjectRank. Users can specify various combinations of calibration values to control the behavior of the demo. Finally, we propose a methodology that enables us to extend query results using the ontology graph.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {796–798},
numpages = {3},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142595,
author = {Doan, AnHai and Ramakrishnan, Raghu and Vaithyanathan, Shivakumar},
title = {Managing Information Extraction: State of the Art and Research Directions},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142595},
doi = {10.1145/1142473.1142595},
abstract = {This tutorial makes the case for developing a unified framework that manages information extraction from unstructured data (focusing in particular on text). We first survey research on information extraction in the database, AI, NLP, IR, and Web communities in recent years. Then we discuss why this is the right time for the database community to actively participate and address the problem of managing information extraction (including in particular the challenges of maintaining and querying the extracted information, and accounting for the imprecision and uncertainty inherent in the extraction process). Finally, we show how interested researchers can take the next step, by pointing to open problems, available datasets, applicable standards, and software tools. We do not assume prior knowledge of text management, NLP, extraction techniques, or machine learning.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {799–800},
numpages = {2},
keywords = {information extraction, semantic integration, database management systems},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142597,
author = {Florescu, Daniela and Kossmann, Donald},
title = {Programming for XML},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142597},
doi = {10.1145/1142473.1142597},
abstract = {There are many emerging applications for XML. Although there are many tools availalbe, an open question is the right programming paradigm to process XML data. Today, the most popular solutions are based on extensions to existing programming languages (e.g., Java, Python or PHP) with XML-specific libraries and APIs. Such libraries either represent the XML data as a virtual tree, or they read the XML data in a streaming (push or pull) fashion. This approach has the obvious problems that arise from the impedance mismatch between the XML type system and the type system of the host language. Moreover, the code written in such programming languages cannot be (easily) optimized using traditional techniques; good performance, scalability, and service-level guarantees is difficult to achieve for such programs on large datasets. Recently, several proposals for new programming languages have been made in both industry and the research community. One prominent example is Microsoft's XLinQ language. Another prominent example of XML processing in Web-based applications is AJAX (Asynchronous Java Programming with XML). In academia, XL, XStatic, Links, and several other languages have been proposed. All these solutions follow different philosophies and address critical design questions in different ways. This tutorial gives an overview of the current generation of programming languages for data-intensive XML applications. Furthermore, this tutorial compares the possible solutions based on a few comparative practical criteria. The tutorial shows how each solution addresses the design questions in different ways and gives the tradeoffs in terms of capabilities and optimizability of these languages are.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {801},
numpages = {1},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142599,
author = {Koudas, Nick and Sarawagi, Sunita and Srivastava, Divesh},
title = {Record Linkage: Similarity Measures and Algorithms},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142599},
doi = {10.1145/1142473.1142599},
abstract = {This tutorial provides a comprehensive and cohesive overview of the key research results in the area of record linkage methodologies and algorithms for identifying approximate duplicate records, and available tools for this purpose. It encompasses techniques introduced in several communities including databases, information retrieval, statistics and machine learning. It aims to identify similarities and differences across the techniques as well as their merits and limitations.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {802–803},
numpages = {2},
keywords = {approximate join, data quality},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142601,
author = {Chang, Kevin Chen-Chuan and Cho, Junghoo},
title = {Accessing the Web: From Search to Integration},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142601},
doi = {10.1145/1142473.1142601},
abstract = {We have witnessed the rapid growth of the Web-- It has not only "broadened" but also "deepened": While the "surface Web" has expanded from the 1999 estimate of 800 million to the recent 19.2 billion pages reported by Yahoo index, an equally or even more significant amount of information is hidden on the "deep Web," behind query forms, recently estimated at over 1.2 million, of online databases. Accessing the information on the Web thus requires not only search to locate pages of interests, from the surface Web, but also integration to aggregate data from alternative or complementary sources, from the deep Web. Although the opportunities are unprecedented, the challenges are also immense: On the one hand, for the surface Web, while search seems to have evolved into a standard technology, its maturity and pervasiveness have also invited the attack of spam and the demand of personalization. On the other hand, for the deep Web, while the proliferation of structured sources has promised unlimited possibilities for more precise and aggregated access, it has also presented new challenges for realizing large scale and dynamic information integration. These issues are in essence related to data management, in a large scale, and thus present novel problems and interesting opportunities for our research community. This tutorial will discuss the new access scenarios and research problems in Web information access: from search of the surface Web to integration of the deep Web.},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {804–805},
numpages = {2},
keywords = {information access, deep Web, integration, Web, search},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

@inproceedings{10.1145/1142473.1142603,
author = {Deshpande, A. and Hellerstein, J. M. and Raman, V.},
title = {Adaptive Query Processing: Why, How, When, What Next},
year = {2006},
isbn = {1595934340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142473.1142603},
doi = {10.1145/1142473.1142603},
booktitle = {Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data},
pages = {806–807},
numpages = {2},
location = {Chicago, IL, USA},
series = {SIGMOD '06}
}

