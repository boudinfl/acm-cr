@inproceedings{10.1145/564691.564693,
author = {Buneman, Peter and Khanna, Sanjeev and Tajima, Keishi and Tan, Wang-Chiew},
title = {Archiving Scientific Data},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564693},
doi = {10.1145/564691.564693},
abstract = {We present an archiving technique for hierarchical data with key structure. Our approach is based on the notion of timestamps whereby an element appearing in multiple versions of the database is stored only once along with a compact description of versions in which it appears. The basic idea of timestamping was discovered by Driscoll et. al. in the context of persistent data structures where one wishes to track the sequences of changes made to a data structure. We extend this idea to develop an archiving tool for XML data that is capable of providing meaningful change descriptions and can also efficiently support a variety of basic functions concerning the evolution of data such as retrieval of any specific version from the archive and querying the temporal history of any element. This is in contrast to diff-based approaches where such operations may require undoing a large number of changes or significant reasoning with the deltas. Surprisingly, our archiving technique does not incur any significant space overhead when contrasted with other approaches. Our experimental results support this and also show that the compacted archive file interacts well with other compression techniques. Finally, another useful property of our approach is that the resulting archive is also in XML and hence can directly leverage existing XML tools.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564694,
author = {Riedewald, Mirek and Agrawal, Divyakant and El Abbadi, Amr},
title = {Efficient Integration and Aggregation of Historical Information},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564694},
doi = {10.1145/564691.564694},
abstract = {Data warehouses support the analysis of historical data. This often involves aggregation over a period of time. Furthermore, data is typically incorporated in the warehouse in the increasing order of a time attribute, e.g., date of sale or time of a temperature measurement. In this paper we propose a framework to take advantage of this append only nature of updates due to a time attribute. The framework allows us to integrate large amounts of new data into the warehouse and generate historical summaries efficiently. Query and update costs are virtually independent from the extent of the data set in the time dimension, making our framework an attractive aggregation approach for append-only data streams. A specific instantiation of the general approach is developed for MOLAP data cubes, involving a new data structure for append-only arrays with pre-aggregated values. Our framework is applicable to point data and data with extent, e.g., hyper-rectangles.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {13–24},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564695,
author = {Kalnis, Panos and Ng, Wee Siong and Ooi, Beng Chin and Papadias, Dimitris and Tan, Kian-Lee},
title = {An Adaptive Peer-to-Peer Network for Distributed Caching of OLAP Results},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564695},
doi = {10.1145/564691.564695},
abstract = {Peer-to-Peer (P2P) systems are becoming increasingly popular as they enable users to exchange digital information by participating in complex networks. Such systems are inexpensive, easy to use, highly scalable and do not require central administration. Despite their advantages, however, limited work has been done on employing database systems on top of P2P networks.Here we propose the PeerOLAP architecture for supporting On-Line Analytical Processing queries. A large number low-end clients, each containing a cache with the most useful results, are connected through an arbitrary P2P network. If a query cannot be answered locally (i.e. by using the cache contents of the computer where it is issued), it is propagated through the network until a peer that has cached the answer is found. An answer may also be constructed by partial results from many peers. Thus PeerOLAP acts as a large distributed cache, which amplifies the benefits of traditional client-side caching. The system is fully distributed and can reconfigure itself on-the-fly in order to decrease the query cost for the observed workload. This paper describes the core components of PeerOLAP and presents our results both from simulation and a prototype installation running on geographically remote peers.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {25–36},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564697,
author = {Viglas, Stratis D. and Naughton, Jeffrey F.},
title = {Rate-Based Query Optimization for Streaming Information Sources},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564697},
doi = {10.1145/564691.564697},
abstract = {Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {37–48},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564698,
author = {Madden, Samuel and Shah, Mehul and Hellerstein, Joseph M. and Raman, Vijayshankar},
title = {Continuously Adaptive Continuous Queries over Streams},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564698},
doi = {10.1145/564691.564698},
abstract = {We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive cross-query sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators --- both selections and join state --- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {49–60},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564699,
author = {Dobra, Alin and Garofalakis, Minos and Gehrke, Johannes and Rastogi, Rajeev},
title = {Processing Complex Aggregate Queries over Data Streams},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564699},
doi = {10.1145/564691.564699},
abstract = {Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.In this paper, we consider the problem of approximately answering general aggregate SQL queries over continuous data streams with limited memory. Our method relies on randomizing techniques that compute small "sketch" summaries of the streams that can then be used to provide approximate answers to aggregate queries with provable guarantees on the approximation error. We also demonstrate how existing statistical information on the base data (e.g., histograms) can be used in the proposed framework to improve the quality of the approximation provided by our algorithms. The key idea is to intelligently partition the domain of the underlying attribute(s) and, thus, decompose the sketching problem in a way that provably tightens our guarantees. Results of our experimental study with real-life as well as synthetic data streams indicate that sketches provide significantly more accurate answers compared to histograms for aggregate queries. This is especially true when our domain partitioning methods are employed to further boast the accuracy of the final estimates.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {61–72},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564701,
author = {Olston, Chris and Widom, Jennifer},
title = {Best-Effort Cache Synchronization with Source Cooperation},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564701},
doi = {10.1145/564691.564701},
abstract = {In environments where exact synchronization between source data objects and cached copies is not achievable due to bandwidth or other resource constraints, stale (out-of-date) copies are permitted. It is desirable to minimize the overall divergence between source objects and cached copies by selectively refreshing modified objects. We call the online process of selecting which objects to refresh in order to minimize divergence best-effort synchronization. In most approaches to best-effort synchronization, the cache coordinates the process and selects objects to refresh. In this paper, we propose a best-effort synchronization scheduling policy that exploits cooperation between data sources and the cache. We also propose an implementation of our policy that incurs low communication overhead even in environments with very large numbers of sources. Our algorithm is adaptive to wide fluctuations in available resources and data update rates. Through experimental simulation over synthetic and real-world data, we demonstrate the effectiveness of our algorithm, and we quantify the significant decrease in divergence achievable with source cooperation.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {73–84},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564702,
author = {Zadorozhny, Vladimir and Raschid, Louiqa and Vidal, Maria Esther and Urhan, Tolga and Bright, Laura},
title = {Efficient Evaluation of Queries in a Mediator for WebSources},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564702},
doi = {10.1145/564691.564702},
abstract = {We consider an architecture of mediators and wrappers for Internet accessible WebSources of limited query capability. Each call to a source is a WebSource Implementation (WSI) and it is associated with both a capability and (a possibly dynamic) cost. The multiplicity of WSIs with varying costs and capabilities increases the complexity of a traditional optimizer that must assign WSIs for each remote relation in the query while generating an (optimal) plan. We present a two-phase Web Query Optimizer (WQO). In a pre-optimization phase, the WQO selects one or more WSIs for a pre-plan; a pre-plan represents a space of query evaluation plans (plans) based on this choice of WSIs. The WQO uses cost-based heuristics to evaluate the choice of WSI assignment in the pre-plan and to choose a good pre-plan. The WQO uses the pre-plan to drive the extended relational optimizer to obtain the best plan for a pre-plan. A prototype of the WQO has been developed. We compare the effectiveness of the WQO, i.e., its ability to efficiently search a large space of plans and obtain a low cost plan, in comparison to a traditional optimizer. We also validate the cost-based heuristics by experimental evaluation of queries in the noisy Internet environment.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {85–96},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564703,
author = {Datta, Anindya and Dutta, Kaushik and Thomas, Helen and VanderMeer, Debra and Suresha and Ramamritham, Krithi},
title = {Proxy-Based Acceleration of Dynamically Generated Content on the World Wide Web: An Approach and Implementation},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564703},
doi = {10.1145/564691.564703},
abstract = {As Internet traffic continues to grow and web sites become increasingly complex, performance and scalability are major issues for web sites. Web sites are increasingly relying on dynamic content generation applications to provide web site visitors with dynamic, interactive, and personalized experiences. However, dynamic content generation comes at a cost --- each request requires computation as well as communication across multiple components.To address these issues, various dynamic content caching approaches have been proposed. Proxy-based caching approaches store content at various locations outside the site infrastructure and can improve Web site performance by reducing content generation delays, firewall processing delays, and bandwidth requirements. However, existing proxy-based caching approaches either (a) cache at the page level, which does not guarantee that correct pages are served and provides very limited reusability, or (b) cache at the fragment level, which requires the use of pre-defined page layouts. To address these issues, several back end caching approaches have been proposed, including query result caching and fragment level caching. While back end approaches guarantee the correctness of results and offer the advantages of fine-grained caching, they neither address firewall delays nor reduce bandwidth requirements.In this paper, we present an approach and an implementation of a dynamic proxy caching technique which combines the benefits of both proxy-based and back end caching approaches, yet does not suffer from their above-mentioned limitations. Our dynamic proxy caching technique allows granular, proxy-based caching where both the content and layout can be dynamic. Our analysis of the performance of our approach indicates that it is capable of providing significant reductions in bandwidth. We have also deployed our proposed dynamic proxy caching technique at a major financial institution. The results of this implementation indicate that our technique is capable of providing order-of-magnitude reductions in bandwidth and response times in real-world dynamic Web applications.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {97–108},
numpages = {12},
keywords = {proxy-based caching, edge caching, dynamic content},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564705,
author = {Grust, Torsten},
title = {Accelerating XPath Location Steps},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564705},
doi = {10.1145/564691.564705},
abstract = {This work is a proposal for a database index structure that has been specifically designed to support the evaluation of XPath queries. As such, the index is capable to support all XPath axes (including ancestor, following, preceding-sibling, descendant-or-self, etc.). This feature lets the index stand out among related work on XML indexing structures which had a focus on regular path expressions (which correspond to the XPath axes children and descendant-or-self plus name tests). Its ability to start traversals from arbitrary context nodes in an XML document additionally enables the index to support the evaluation of path traversals embedded in XQuery expressions. Despite its flexibility, the new index can be implemented and queried using purely relational techniques, but it performs especially well if the underlying database host provides support for R-trees. A performance assessment which shows quite promising results completes this proposal.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {109–120},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564706,
author = {Chung, Chin-Wan and Min, Jun-Ki and Shim, Kyuseok},
title = {APEX: An Adaptive Path Index for XML Data},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564706},
doi = {10.1145/564691.564706},
abstract = {The emergence of the Web has increased interests in XML data. XML query languages such as XQuery and XPath use label paths to traverse the irregularly structured data. Without a structural summary and efficient indexes, query processing can be quite inefficient due to an exhaustive traversal on XML data. To overcome the inefficiency, several path indexes have been proposed in the research community. Traditional indexes generally record all label paths from the root element in XML data. Such path indexes may result in performance degradation due to large sizes and exhaustive navigations for partial matching path queries start with the self-or-descendent axis("//").In this paper, we propose APEX, an adaptive path index for XML data. APEX does not keep all paths starting from the root and utilizes frequently used paths to improve the query performance. APEX also has a nice property that it can be updated incrementally according to the changes of query workloads. Experimental results with synthetic and real-life data sets clearly confirm that APEX improves query processing cost typically 2 to 54 times better than the existing indexes, with the performance gap increasing with the irregularity of XML data.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {121–132},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564707,
author = {Kaushik, Raghav and Bohannon, Philip and Naughton, Jeffrey F and Korth, Henry F},
title = {Covering Indexes for Branching Path Queries},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564707},
doi = {10.1145/564691.564707},
abstract = {In this paper, we ask if the traditional relational query acceleration techniques of summary tables and covering indexes have analogs for branching path expression queries over tree- or graph-structured XML data. Our answer is yes --- the forward-and-backward index already proposed in the literature can be viewed as a structure analogous to a summary table or covering index. We also show that it is the smallest such index that covers all branching path expression queries. While this index is very general, our experiments show that it can be so large in practice as to offer little performance improvement over evaluating queries directly on the data. Likening the forward-and-backward index to a covering index on all the attributes of several tables, we devise an index definition scheme to restrict the class of branching path expressions being indexed. The resulting index structures are dramatically smaller and perform better than the full forward-and-backward index for these classes of branching path expressions. This is roughly analogous to the situation in multidimensional or OLAP workloads, in which more highly aggregated summary tables can service a smaller subset of queries but can do so at increased performance. We evaluate the performance of our indexes on both relational decompositions of XML and a native storage technique. As expected, the performance benefit of an index is maximized when the query matches the index definition.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {133–144},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564709,
author = {Zhou, Jingren and Ross, Kenneth A.},
title = {Implementing Database Operations Using SIMD Instructions},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564709},
doi = {10.1145/564691.564709},
abstract = {Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements. SIMD technology was initially built into commodity processors in order to accelerate the performance of multimedia applications. SIMD instructions provide new opportunities for database engine design and implementation. We study various kinds of operations in a database context, and show how the inner loop of the operations can be accelerated using SIMD instructions. The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions.We consider the most important database operations, including sequential scans, aggregation, index operations, and joins. We present techniques for implementing these using SIMD instructions. We show that there are significant benefits in redesigning traditional query processing algorithms so that they can make better use of SIMD technology. Our study shows that using a SIMD parallelism of four, the CPU time for the new algorithms is from 10% to more than four times less than for the traditional algorithms. Superlinear speedups are obtained as a result of the elimination of branch misprediction effects.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {145–156},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564710,
author = {Chen, Shimin and Gibbons, Phillip B. and Mowry, Todd C. and Valentin, Gary},
title = {Fractal Prefetching B<sup>+</sup>-Trees: Optimizing Both Cache and Disk Performance},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564710},
doi = {10.1145/564691.564710},
abstract = {B+-Trees have been traditionally optimized for I/O performance with disk pages as tree nodes. Recently, researchers have proposed new types of B+-Trees optimized for CPU cache performance in main memory environments, where the tree node sizes are one or a few cache lines. Unfortunately, due primarily to this large discrepancy in optimal node sizes, existing disk-optimized B+-Trees suffer from poor cache performance while cache-optimized B+-Trees exhibit poor disk performance. In this paper, we propose fractal prefetching B+-Trees (fpB+-Trees), which embed "cache-optimized" trees within "disk-optimized" trees, in order to optimize both cache and I/O performance. We design and evaluate two approaches to breaking disk pages into cache-optimized nodes: disk-first and cache-first. These approaches are somewhat biased in favor of maximizing disk and cache performance, respectively, as demonstrated by our results. Both implementations of fpB+-Trees achieve dramatically better cache performance than disk-optimized B+-Trees: a factor of 1.1-1.8 improvement for search, up to a factor of 4.2 improvement for range scans, and up to a 20-fold improvement for updates, all without significant degradation of I/O performance. In addition, fpB+-Trees accelerate I/O performance for range scans by using jump-pointer arrays to prefetch leaf pages, thereby achieving a speed-up of 2.5-5 on IBM's DB2 Universal Database.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {157–168},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564711,
author = {Li, Wei and Gao, Dengfeng and Snodgrass, Richard Thomas},
title = {Skew Handling Techniques in Sort-Merge Join},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564711},
doi = {10.1145/564691.564711},
abstract = {Joins are among the most frequently executed operations. Several fast join algorithms have been developed and extensively studied; these can be categorized as sort-merge, hash-based, and index-based algorithms. While all three types of algorithms exhibit excellent performance over most data, ameliorating the performance degradation in the presence of skew has been investigated only for hash-based algorithms. However, for sort-merge join, even a small amount of skew present in realistic data can result in a significant performance hit on a commercial DBMS. This paper examines the negative ramifications of skew in sort-merge join and proposes several refinements that deal effectively with data skew. Experiments show that some of these algorithms also impose virtually no penalty in the absence of data skew and are thus suitable for replacing existing sort-merge implementations. We also show how sort-merge band join performance is significantly enhanced with these refinements.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {169–180},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564713,
author = {Freire, Juliana and Haritsa, Jayant R. and Ramanath, Maya and Roy, Prasan and Sim\'{e}on, J\'{e}r\^{o}me},
title = {StatiX: Making XML Count},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564713},
doi = {10.1145/564691.564713},
abstract = {The availability of summary data for XML documents has many applications, from providing users with quick feedback about their queries, to cost-based storage design and query optimization. StatiX is a novel XML Schema-aware statistics framework that exploits the structure derived by regular expressions (which define elements in an XML Schema) to pinpoint places in the schema that are likely sources of structural skew. As we discuss below, this information can be used to build concise, yet accurate, statistical summaries for XML data. StatiX leverages standard XML technology for gathering statistics, notably XML Schema validators, and it uses histograms to summarize both the structure and values in an XML document. In this paper we describe the StatiX system. We develop algorithms that decompose schemas to obtain statistics at different granularities and discuss how statistics can be gathered as documents are validated. We also present an experimental evaluation which demonstrates the accuracy and scalability of our approach and show an application of these statistics to cost-based XML storage design.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {181–191},
numpages = {11},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564714,
author = {Papakonstantinou, Yannis and Petropoulos, Michalis and Vassalos, Vasilis},
title = {QURSED: Querying and Reporting Semistructured Data},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564714},
doi = {10.1145/564691.564714},
abstract = {QURSED enables the development of web-based query forms and reports (QFRs) that query and report semistructured XML data, i.e., data that are characterized by nesting, irregularities and structural variance. The query aspects of a QFR are captured by its query set specification, which formally encodes multiple parameterized condition fragments and can describe large numbers of queries. The run-time component of QURSED produces XQuery-compliant queries by synthesizing fragments from the query set specification that have been activated during the interaction of the end-user with the QFR. The design-time component of QURSED, called QURSED Editor, semi-automates the development of the query set specification and its association with the visual components of the QFR by translating visual actions into appropriate query set specifications. We describe QURSED and illustrate how it accommodates the intricacies that the semistructured nature of the underlying database introduces. We specifically focus on the formal model of the query set specification, its generation via the QURSED Editor and its coupling with the visual aspects of the web-based form and report.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {192–203},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564715,
author = {Tatarinov, Igor and Viglas, Stratis D. and Beyer, Kevin and Shanmugasundaram, Jayavel and Shekita, Eugene and Zhang, Chun},
title = {Storing and Querying Ordered XML Using a Relational Database System},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564715},
doi = {10.1145/564691.564715},
abstract = {XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to "shred" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {204–215},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564717,
author = {Hacig\"{u}m\"{u}\c{s}, Hakan and Iyer, Bala and Li, Chen and Mehrotra, Sharad},
title = {Executing SQL over Encrypted Data in the Database-Service-Provider Model},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564717},
doi = {10.1145/564691.564717},
abstract = {Rapid advances in networking and Internet technologies have fueled the emergence of the "software as a service" model for enterprise computing. Successful examples of commercially viable software services include rent-a-spreadsheet, electronic mail services, general storage services, disaster protection services. "Database as a Service" model provides users power to create, store, modify, and retrieve data from anywhere in the world, as long as they have access to the Internet. It introduces several challenges, an important issue being data privacy. It is in this context that we specifically address the issue of data privacy.There are two main privacy issues. First, the owner of the data needs to be assured that the data stored on the service-provider site is protected against data thefts from outsiders. Second, data needs to be protected even from the service providers, if the providers themselves cannot be trusted. In this paper, we focus on the second challenge. Specifically, we explore techniques to execute SQL queries over encrypted data. Our strategy is to process as much of the query as possible at the service providers' site, without having to decrypt the data. Decryption and the remainder of the query processing are performed at the client site. The paper explores an algebraic framework to split the query to minimize the computation at the client site. Results of experiments validating our approach are also presented.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {216–227},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564718,
author = {Gillmann, Michael and Weikum, Gerhard and Wonner, Wolfgang},
title = {Workflow Management with Service Quality Guarantees},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564718},
doi = {10.1145/564691.564718},
abstract = {Workflow management systems (WFMS) that are geared for the orchestration of business processes across multiple organizations are complex distributed systems: they consist of multiple workflow engines, application servers, and communication middleware servers such as ORBs, where each of these server types can be replicated on multiple computers for scalability and availability.Finding an appropriate system configuration with guaranteed application-specific quality of service in terms of throughput, response time, and tolerable downtime is a major challenge for human system administrators. This paper presents a tool that largely automates the task of configuring a distributed WFMS. Based on a suite of mathematical models, the tool derives the necessary degrees of replication for the various server types in order to meet specified goals for performance and availability as well as "performability" when service is degraded due to outages of individual servers. The paper describes the configuration tool, with emphasis on how to capture the load behavior of workflows in a realistic manner. We also present extensive experiments that evaluate the accuracy of the tool's underlying models and demonstrate the practical feasibility of automating the task of configuring a distributed WFMS. The experiments use a detailed simulation which in turn has been validated through measurements with the Mentor-lite prototype system.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {228–239},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564719,
author = {Dasu, Tamraparni and Johnson, Theodore and Muthukrishnan, S. and Shkapenyuk, Vladislav},
title = {Mining Database Structure; or, How to Build a Data Quality Browser},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564719},
doi = {10.1145/564691.564719},
abstract = {Data mining research typically assumes that the data to be analyzed has been identified, gathered, cleaned, and processed into a convenient form. While data mining tools greatly enhance the ability of the analyst to make data-driven discoveries, most of the time spent in performing an analysis is spent in data identification, gathering, cleaning and processing the data. Similarly, schema mapping tools have been developed to help automate the task of using legacy or federated data sources for a new purpose, but assume that the structure of the data sources is well understood. However the data sets to be federated may come from dozens of databases containing thousands of tables and tens of thousands of fields, with little reliable documentation about primary keys or foreign keys.We are developing a system, Bellman, which performs data mining on the structure of the database. In this paper, we present techniques for quickly identifying which fields have similar values, identifying join paths, estimating join directions and sizes, and identifying structures in the database. The results of the database structure mining allow the analyst to make sense of the database content. This information can be used to e.g., prepare data for data mining, find foreign key joins for schema mapping, or identify steps to be taken to prevent the database from collapsing under the weight of its complexity.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {240–251},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564721,
author = {Luo, Gang and Ellmann, Curt J. and Haas, Peter J. and Naughton, Jeffrey F.},
title = {A Scalable Hash Ripple Join Algorithm},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564721},
doi = {10.1145/564691.564721},
abstract = {Recently, Haas and Hellerstein proposed the hash ripple join algorithm in the context of online aggregation. Although the algorithm rapidly gives a good estimate for many join-aggregate problem instances, the convergence can be slow if the number of tuples that satisfy the join predicate is small or if there are many groups in the output. Furthermore, if memory overflows (for example, because the user allows the algorithm to run to completion for an exact answer), the algorithm degenerates to block ripple join and performance suffers. In this paper, we build on the work of Haas and Hellerstein and propose a new algorithm that (a) combines parallelism with sampling to speed convergence, and (b) maintains good performance in the presence of memory overflow. Results from a prototype implementation in a parallel DBMS show that its rate of convergence scales with the number of processors, and that when allowed to run to completion, even in the presence of memory overflow, it is competitive with the traditional parallel hybrid hash join algorithm.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {252–262},
numpages = {11},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564722,
author = {Bruno, Nicolas and Chaudhuri, Surajit},
title = {Exploiting Statistics on Query Expressions for Optimization},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564722},
doi = {10.1145/564691.564722},
abstract = {Statistics play an important role in influencing the plans produced by a query optimizer. Traditionally, optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this paper, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present experimental results on an implementation of our approach in Microsoft SQL Server 2000.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {263–274},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564723,
author = {Raman, Vijayshankar and Hellerstein, Joseph M.},
title = {Partial Results for Online Query Processing},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564723},
doi = {10.1145/564691.564723},
abstract = {Traditional query processors generate full, accurate query results, either in batch or in pipelined fashion. We argue that this strict model is too rigid for exploratory queries over diverse and distributed data sources, such as sources on the Internet. Instead, we propose a looser model of querying in which a user submits a broad initial query outline, and the system continually generates partial result tuples that may contain values for only some of the output fields. The user can watch these partial results accumulate at the user interface, and accordingly refine the query by specifying their interest in different kinds of partial results.After describing our querying model and user interface, we present a query processing architecture for this model which is implemented in the Telegraph dataflow system. Our architecture is designed to generate partial results quickly, and to adapt query execution to changing user interests. The crux of this architecture is a dataflow operator that supports two kinds of reorderings: reordering of intermediate tuples within a dataflow, and reordering of query plan operators through which tuples flow. We study reordering policies that optimize for the quality of partial results delivered over time, and experimentally demonstrate the benefits of our architecture in this context.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {275–286},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564725,
author = {Guha, Sudipto and Jagadish, H. V. and Koudas, Nick and Srivastava, Divesh and Yu, Ting},
title = {Approximate XML Joins},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564725},
doi = {10.1145/564691.564725},
abstract = {XML is widely recognized as the data interchange standard for tomorrow, because of its ability to represent data from a wide variety sources. Hence, XML is likely to be the format through which data from multiple sources is integrated.In this paper we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently approximate match in structure, in addition to, content has to be folded in the join operation. We quantify approximate match in structure and content using well defined notions of distance. For structure, we propose computationally inexpensive lower and upper bounds for the tree edit distance metric between two trees. We then show how the tree edit distance, and other metrics that quantify distance between trees, can be incorporated in a join framework. We introduce the notion of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set and we propose sampling based algorithms to identify them. This gives rise to a variety of algorithmic approaches for the problem, which we formulate and analyze. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {287–298},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564726,
author = {Ramanan, Prakash},
title = {Efficient Algorithms for Minimizing Tree Pattern Queries},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564726},
doi = {10.1145/564691.564726},
abstract = {We consider the problem of minimizing tree pattern queries (TPQ) that arise in XML and in LDAP-style network directories. In [Minimization of Tree Pattern Queries, Proc. ACM SIGMOD Intl. Conf. Management of Data, 2001, pp. 497-508], Amer-Yahia, Cho, Lakshmanan and Srivastava presented an O(n4) algorithm for minimizing TPQs in the absence of integrity constraints (Case 1); n is the number of nodes in the query. Then they considered the problem of minimizing TPQs in the presence of three kinds of integrity constraints: required-child, required-descendant and subtype (Case 2). They presented an O(n6) algorithm for minimizing TPQs in the presence of only required-child and required-descendant constraints (i.e., no subtypes allowed; Case 3). We present O(n2), O(n4) and O(n2) algorithms for minimizing TPQs in these three cases, respectively, based on the concept of graph simulation. We believe that our O(n2) algorithms for Cases 1 and 3 are runtime optimal.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {299–309},
numpages = {11},
keywords = {integrity constraints, query minimization, LDAP queries, tree pattern queries, XML queries, graph simulation},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564727,
author = {Bruno, Nicolas and Koudas, Nick and Srivastava, Divesh},
title = {Holistic Twig Joins: Optimal XML Pattern Matching},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564727},
doi = {10.1145/564691.564727},
abstract = {XML employs a tree-structured data model, and, naturally, XML queries specify patterns of selection predicates on multiple elements related by a tree structure. Finding all occurrences of such a twig pattern in an XML database is a core operation for XML query processing. Prior work has typically decomposed the twig pattern into binary structural (parent-child and ancestor-descendant) relationships, and twig matching is achieved by: (i) using structural join algorithms to match the binary relationships against the XML database, and (ii) stitching together these basic matches. A limitation of this approach for matching twig patterns is that intermediate result sizes can get large, even when the input and output sizes are more manageable.In this paper, we propose a novel holistic twig join algorithm, TwigStack, for matching an XML query twig pattern. Our technique uses a chain of linked stacks to compactly represent partial results to root-to-leaf query paths, which are then composed to obtain matches for the twig pattern. When the twig pattern uses only ancestor-descendant relationships between elements, TwigStack is I/O and CPU optimal among all sequential algorithms that read the entire input: it is linear in the sum of sizes of the input lists and the final result list, but independent of the sizes of intermediate results. We then show how to use (a modification of) B-trees, along with TwigStack, to match query twig patterns in sub-linear time. Finally, we complement our analysis with experimental results on a range of real and synthetic data, and query twig patterns.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {310–321},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564729,
author = {de Vries, Arjen P. and Mamoulis, Nikos and Nes, Niels and Kersten, Martin},
title = {Efficient K-NN Search on Vertically Decomposed Data},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564729},
doi = {10.1145/564691.564729},
abstract = {Applications like multimedia retrieval require efficient support for similarity search on large data collections. Yet, nearest neighbor search is a difficult problem in high dimensional spaces, rendering efficient applications hard to realize: index structures degrade rapidly with increasing dimensionality, while sequential search is not an attractive solution for repositories with millions of objects. This paper approaches the problem from a different angle. A solution is sought in an unconventional storage scheme, that opens up a new range of techniques for processing k-NN queries, especially suited for high dimensional spaces. The suggested (physical) database design accommodates well a novel variant of branch-and-bound search, that reduces the high dimensional space quickly to a small candidate set. The paper provides insight in applying this idea to k-NN search using two similarity metrics commonly encountered in image database applications, and discusses techniques for its implementation in relational database systems. The effectiveness of the proposed method is evaluated empirically on both real and synthetic data sets, reporting the significant improvements in response time yielded.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {322–333},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564730,
author = {Tao, Yufei and Papadias, Dimitris},
title = {Time-Parameterized Queries in Spatio-Temporal Databases},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564730},
doi = {10.1145/564691.564730},
abstract = {Time-parameterized queries (TP queries for short) retrieve (i) the actual result at the time that the query is issued, (ii) the validity period of the result given the current motion of the query and the database objects, and (iii) the change that causes the expiration of the result. Due to the highly dynamic nature of several spatio-temporal applications, TP queries are important both as standalone methods, as well as building blocks of more complex operations. However, little work has been done towards their efficient processing. In this paper, we propose a general framework that covers time-parameterized variations of the most common spatial queries, namely window queries, k-nearest neighbors and spatial joins. In particular, each of these TP queries is reduced to nearest neighbor search where the distance functions are defined according to the query type. This reduction allows the application and extension of well-known branch and bound techniques to the current problem. The proposed methods can be applied with mobile queries, mobile objects or both, given a suitable indexing method. Our experimental evaluation is based on R-trees and their extensions for dynamic objects.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {334–345},
numpages = {12},
keywords = {spatio-temporal databases, nearest neighbor queries},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564731,
author = {Chang, Kevin Chen-Chuan and Hwang, Seung-won},
title = {Minimal Probing: Supporting Expensive Predicates for Top-k Queries},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564731},
doi = {10.1145/564691.564731},
abstract = {This paper addresses the problem of evaluating ranked top-k queries with expensive predicates. As major DBMSs now all support expensive user-defined predicates for Boolean queries, we believe such support for ranked queries will be even more important: First ranked queries often need to model user-specific concepts of preference, relevance, or similarity, which call for dynamic user-defined functions. Second, middleware systems must incorporate external predicates for integrating autonomous sources typically accessible only by per-object queries. Third, fuzzy joins are inherently expensive, as they are essentially user-defined operations that dynamically associate multiple relations. These predicates, being dynamically defined or externally accessed, cannot rely on index mechanisms to provide zero-time sorted output, and must instead require per-object probe to evaluate. The current standard sort-merge framework for ranked queries cannot efficiently handle such predicates because it must completely probe all objects, before sorting and merging them to produce top-k answers. To minimize expensive probes, we thus develop the formal principle of "necessary probes," which determines if a probe is absolutely required. We then propose Algorithm MPro which, by implementing the principle, is provably optimal with minimal probe cost. Further, we show that MPro can scale well and can be easily parallelized. Our experiments using both a real-estate benchmark database and synthetic datasets show that MPro enables significant probe reduction, which can be orders of magnitude faster than the standard scheme using complete probing.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {346–357},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564733,
author = {Polyzotis, Neoklis and Garofalakis, Minos},
title = {Statistical Synopses for Graph-Structured XML Databases},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564733},
doi = {10.1145/564691.564733},
abstract = {Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, We introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data graph. To estimate the selectivities of complex path expressions over concise XSKETCH synopses, we develop an estimation framework that relies on appropriate statistical (uniformity and independence) assumptions to compensate for the lack of detailed distribution information. Given our estimation framework, we demonstrate that the problem of building an accuracy-optimal XSKETCH for a given amount of space is 𝒩𝒫-hard, and propose an efficient heuristic algorithm based on greedy forward selection. Briefly, our algorithm constructs an XSKETCH synopsis by successive refinements of the label-split graph, the coarsest summary of the XML data graph. Our refinement operations act locally and attempt to capture important statistical correlations between data paths. Extensive experimental results with synthetic as well as real-life data sets verify the effectiveness of our approach. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured data and complex (branching) path expressions.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {358–369},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564734,
author = {Gao, Like and Wang, X. Sean},
title = {Continually Evaluating Similarity-Based Pattern Queries on a Streaming Time Series},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564734},
doi = {10.1145/564691.564734},
abstract = {In many applications, local or remote sensors send in streams of data, and the system needs to monitor the streams to discover relevant events/patterns and deliver instant reaction correspondingly. An important scenario is that the incoming stream is a continually appended time series, and the patterns are time series in a database. At each time when a new value arrives (called a time position), the system needs to find, from the database, the nearest or near neighbors of the incoming time series up to the time position. This paper attacks the problem by using Fast Fourier Transform (FFT) to efficiently find the cross correlations of time series, which yields, in a batch mode, the nearest and near neighbors of the incoming time series at many time positions. To take advantage of this batch processing in achieving fast response time, this paper uses prediction methods to predict future values. FFT is used to compute the cross correlations of the predicted series (with the values that have already arrived) and the database patterns, and to obtain predicted distances between the incoming time series at many future time positions and the database patterns. When the actual data value arrives, the prediction error together with the predicted distances is used to filter out patterns that are not possible to be the nearest or near neighbors, which provides fast responses. Experiments show that with reasonable prediction errors, the performance gain is significant.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {370–381},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564735,
author = {Moon, Yang-Sae and Whang, Kyu-Young and Han, Wook-Shin},
title = {General Match: A Subsequence Matching Method in Time-Series Databases Based on Generalized Windows},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564735},
doi = {10.1145/564691.564735},
abstract = {We generalize the method of constructing windows in subsequence matching. By this generalization, we can explain earlier subsequence matching methods as special cases of a common framework. Based on the generalization, we propose a new subsequence matching method, General Match. The earlier work by Faloutsos et al. (called FRM for convenience) causes a lot of false alarms due to lack of point-filtering effect. Dual Match, recently proposed as a dual approach of FRM, improves performance significantly over FRM by exploiting point filtering effect. However, it has the problem of having a smaller allowable window size---half that of FRM---given the minimum query length. A smaller window increases false alarms due to window size effect. General Match offers advantages of both methods: it can reduce window size effect by using large windows like FRM and, at the same time, can exploit point-filtering effect like Dual Match. General Match divides data sequences into generalized sliding windows (J-sliding windows) and the query sequence into generalized disjoint windows (J-disjoint windows). We formally prove that General Match is correct, i.e., it incurs no false dismissal. We then propose a method of estimating the optimal value of the sliding factor J that minimizes the number of page accesses. Experimental results for real stock data show that, for low selectivities (10-6∼10-4), General Match improves average performance by 117% over Dual Match and by 998% over FRM; for high selectivities (10-3∼10-1), by 45% over Dual Match and by 64% over FRM. The proposed generalization provides an excellent theoretical basis for understanding the underlying mechanisms of subsequence matching.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {382–393},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564737,
author = {Wang, Haixun and Wang, Wei and Yang, Jiong and Yu, Philip S.},
title = {Clustering by Pattern Similarity in Large Data Sets},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564737},
doi = {10.1145/564691.564737},
abstract = {Clustering is the process of grouping a set of objects into classes of similar objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks. E-commerce applications, such as collaborative filtering, can also benefit from the new model, which captures not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers. Our paper introduces an effective algorithm to detect such clusters, and we perform tests on several real and synthetic data sets to show its effectiveness.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {394–405},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564738,
author = {Yang, Jiong and Wang, Wei and Yu, Philip S. and Han, Jiawei},
title = {Mining Long Sequential Patterns in a Noisy Environment},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564738},
doi = {10.1145/564691.564738},
abstract = {Pattern discovery in long sequences is of great importance in many applications including computational biology study, consumer behavior analysis, system performance analysis, etc. In a noisy environment, an observed sequence may not accurately reflect the underlying behavior. For example, in a protein sequence, the amino acid N is likely to mutate to D with little impact to the biological function of the protein. It would be desirable if the occurrence of D in the observation can be related to a possible mutation from N in an appropriate manner. Unfortunately, the support measure (i.e., the number of occurrences) of a pattern does not serve this purpose. In this paper, we introduce the concept of compatibility matrix as the means to provide a probabilistic connection from the observation to the underlying true value. A new metric match is also proposed to capture the "real support" of a pattern which would be expected if a noise-free environment is assumed. In addition, in the context we address, a pattern could be very long. The standard pruning technique developed for the market basket problem may not work efficiently. As a result, a novel algorithm that combines statistical sampling and a new technique (namely border collapsing) is devised to discover long patterns in a minimal number of scans of the sequence database with sufficiently high confidence. Empirical results demonstrate the robustness of the match model (with respect to the noise) and the efficiency of the probabilistic algorithm.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {406–417},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564739,
author = {Procopiuc, Cecilia M. and Jones, Michael and Agarwal, Pankaj K. and Murali, T. M.},
title = {A Monte Carlo Algorithm for Fast Projective Clustering},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564739},
doi = {10.1145/564691.564739},
abstract = {We propose a mathematical formulation for the notion of optimal projective cluster, starting from natural requirements on the density of points in subspaces. This allows us to develop a Monte Carlo algorithm for iteratively computing projective clusters. We prove that the computed clusters are good with high probability. We implemented a modified version of the algorithm, using heuristics to speed up computation. Our extensive experiments show that our method is significantly more accurate than previous approaches. In particular, we use our techniques to build a classifier for detecting rotated human faces in cluttered images.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {418–427},
numpages = {10},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564741,
author = {Thaper, Nitin and Guha, Sudipto and Indyk, Piotr and Koudas, Nick},
title = {Dynamic Multidimensional Histograms},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564741},
doi = {10.1145/564691.564741},
abstract = {Histograms are a concise and flexible way to construct summary structures for large data sets. They have attracted a lot of attention in database research due to their utility in many areas, including query optimization, and approximate query answering. They are also a basic tool for data visualization and analysis.In this paper, we present a formal study of dynamic multidimensional histogram structures over continuous data streams. At the heart of our proposal is the use of a dynamic summary data structure (vastly different from a histogram) maintaining a succinct approximation of the data distribution of the underlying continuous stream. On demand, an accurate histogram is derived from this dynamic data structure. We propose algorithms for extracting such an accurate histogram and we analyze their behavior and tradeoffs. The proposed algorithms are able to provide approximate guarantees about the quality of the estimation of the histograms they extract.We complement our analytical results with a thorough experimental evaluation using real data sets.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {428–439},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564742,
author = {Choi, Yong-Jin and Chung, Chin-Wan},
title = {Selectivity Estimation for Spatio-Temporal Queries to Moving Objects},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564742},
doi = {10.1145/564691.564742},
abstract = {A query optimizer requires selectivity estimation of a query to choose the most efficient access plan. An effective method of selectivity estimation for the future locations of moving objects has not yet been proposed. Existing methods for spatial selectivity estimation do not accurately estimate the selectivity of a query to moving objects, because they do not consider the future locations of moving objects, which change continuously as time passes.In this paper, we propose an effective method for spatio-temporal selectivity estimation to solve this problem. We present analytical formulas which accurately calculate the selectivity of a spatio-temporal query as a function of spatio-temporal information. Extensive experimental results show that our proposed method accurately estimates the selectivity over various queries to spatio-temporal data combining real-life spatial data and synthetic temporal data. When Tiger/lines is used as real-life spatial data, the application of an existing method for spatial selectivity estimation to the estimation of the selectivity of a query to moving objects has the average error ratio from 14% to 85%, whereas our method for spatio-temporal selectivity estimation has the average error ratio from 9% to 23%.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {440–451},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564743,
author = {Aggarwal, Charu C.},
title = {Hierarchical Subspace Sampling: A Unified Framework for High Dimensional Data Reduction, Selectivity Estimation and Nearest Neighbor Search},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564743},
doi = {10.1145/564691.564743},
abstract = {With the increased abilities for automated data collection made possible by modern technology, the typical sizes of data collections have continued to grow in recent years. In such cases, it may be desirable to store the data in a reduced format in order to improve the storage, transfer time, and processing requirements on the data. One of the challenges of designing effective data compression techniques is to be able to preserve the ability to use the reduced format directly for a wide range of database and data mining applications. In this paper, we propose the novel idea of hierarchical subspace sampling in order to create a reduced representation of the data. The method is naturally able to estimate the local implicit dimensionalities of each point very effectively, and thereby create a variable dimensionality reduced representation of the data. Such a technique has the advantage that it is very adaptive about adjusting its representation depending upon the behavior of the immediate locality of a data point. An interesting property of the subspace sampling technique is that unlike all other data reduction techniques, the overall efficiency of compression improves with increasing database size. This is a highly desirable property for any data reduction system since the problem itself is motivated by the large size of data sets. Because of its sampling approach, the procedure is extremely fast and scales linearly both with data set size and dimensionality. Furthermore, the subspace sampling technique is able to reveal important local subspace characteristics of high dimensional data which can be harnessed for effective solutions to problems such as selectivity estimation and approximate nearest neighbor search.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {452–463},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564745,
author = {Sismanis, Yannis and Deligiannakis, Antonios and Roussopoulos, Nick and Kotidis, Yannis},
title = {Dwarf: Shrinking the PetaCube},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564745},
doi = {10.1145/564691.564745},
abstract = {Dwarf is a highly compressed structure for computing, storing, and querying data cubes. Dwarf identifies prefix and suffix structural redundancies and factors them out by coalescing their store. Prefix redundancy is high on dense areas of cubes but suffix redundancy is significantly higher for sparse areas. Putting the two together fuses the exponential sizes of high dimensional full cubes into a dramatically condensed data structure. The elimination of suffix redundancy has an equally dramatic reduction in the computation of the cube because recomputation of the redundant suffixes is avoided. This effect is multiplied in the presence of correlation amongst attributes in the cube. A Petabyte 25-dimensional cube was shrunk this way to a 2.3GB Dwarf Cube, in less than 20 minutes, a 1:400000 storage reduction ratio. Still, Dwarf provides 100% precision on cube queries and is a self-sufficient structure which requires no access to the fact table. What makes Dwarf practical is the automatic discovery,in a single pass over the fact table, of the prefix and suffix redundancies without user involvement or knowledge of the value distributions.This paper describes the Dwarf structure and the Dwarf cube construction algorithm. Further optimizations are then introduced for improving clustering and query performance. Experiments with the current implementation include comparisons on detailed measurements with real and synthetic datasets against previously published techniques. The comparisons show that Dwarfs by far out-perform these techniques on all counts: storage space, creation time, query response time, and updates of cubes.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {464–475},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564746,
author = {Garofalakis, Minos and Gibbons, Phillip B.},
title = {Wavelet Synopses with Error Guarantees},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564746},
doi = {10.1145/564691.564746},
abstract = {Recent work has demonstrated the effectiveness of the wavelet decomposition in reducing large amounts of data to compact sets of wavelet coefficients (termed "wavelet synopses") that can be used to provide fast and reasonably accurate approximate answers to queries. A major criticism of such techniques is that unlike, for example, random sampling, conventional wavelet synopses do not provide informative error guarantees on the accuracy of individual approximate answers. In fact, as this paper demonstrates, errors can vary widely (without bound) and unpredictably, even for identical queries on nearly-identical values in distinct parts of the data. This lack of error guarantees severely limits the practicality of traditional wavelets as an approximate query-processing tool, because users have no idea of the quality of any particular approximate answer. In this paper, we introduce Probabilistic Wavelet Synopses, the first wavelet-based data reduction technique with guarantees on the accuracy of individual approximate answers. Whereas earlier approaches rely on deterministic thresholding for selecting a set of "good" wavelet coefficients, our technique is based on a novel, probabilistic thresholding scheme that assigns each coefficient a probability of being retained based on its importance to the reconstruction of individual data values, and then flips coins to select the synopsis. We show how our scheme avoids the above pitfalls of deterministic thresholding, providing highly-accurate answers for individual data values in a data vector. We propose several novel optimization algorithms for tuning our probabilistic thresholding scheme to minimize desired error metrics. Experimental results on real-world and synthetic data sets evaluate these algorithms, and demonstrate the effectiveness of our probabilistic wavelet synopses in providing fast, highly-accurate answers with error guarantees.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {476–487},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564747,
author = {Chaudhuri, Surajit and Gupta, Ashish Kumar and Narasayya, Vivek},
title = {Compressing SQL Workloads},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564747},
doi = {10.1145/564691.564747},
abstract = {Recently several important relational database tasks such as index selection, histogram tuning, approximate query processing, and statistics selection have recognized the importance of leveraging workloads. Often these tasks are presented with large workloads, i.e., a set of SQL DML statements, as input. A key factor affecting the scalability of such tasks is the size of the workload. In this paper, we present the novel problem of workload compression which helps improve the scalability of such tasks. We present a principled solution to this challenging problem. Our solution is broadly applicable to a variety of workload-driven tasks, while allowing for incorporation of task specific knowledge. We have implemented this solution and our experiments illustrate its effectiveness in the context of two workload-driven tasks: index selection and approximate query processing.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {488–499},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564749,
author = {Bhattacharya, Suparna and Mohan, C. and Brannon, Karen W. and Narang, Inderpal and Hsiao, Hui-I and Subramanian, Mahadevan},
title = {Coordinating Backup/Recovery and Data Consistency between Database and File Systems},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564749},
doi = {10.1145/564691.564749},
abstract = {Managing a combined store consisting of database data and file data in a robust and consistent manner is a challenge for database systems and content management systems. In such a hybrid system, images, videos, engineering drawings, etc. are stored as files on a file server while meta-data referencing/indexing such files is created and stored in a relational database to take advantage of efficient search. In this paper we describe solutions for two potentially problematic aspects of such a data management system: backup/recovery and data consistency. We present algorithms for performing backup and recovery of the DBMS data in a coordinated fashion with the files on the file servers. Our algorithms for coordinated backup and recovery have been implemented in the IBM DB2/DataLinks product [1]. We also propose an efficient solution to the problem of maintaining consistency between the content of a file and the associated meta-data stored in the DBMS from a reader's point of view without holding long duration locks on meta-data tables. In the model, an object is directly accessed and edited in-place through normal file system APIs using a reference obtained via an SQL Query on the database. To relate file modifications to meta-data updates, the user issues an update through the DBMS, and commits both file and meta-data updates together.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {500–511},
numpages = {12},
keywords = {content management, database recovery, datalinks, database backup, DB2},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564750,
author = {Dogac, Asuman and Tambag, Yusuf and Pembecioglu, Pinar and Pektas, Sait and Laleci, Gokce and Kurt, Gokhan and Toprak, Serkan and Kabak, Yildiray},
title = {An EbXML Infrastructure Implementation through UDDI Registries and RosettaNet PIPs},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564750},
doi = {10.1145/564691.564750},
abstract = {Today's Internet based businesses need a level of interoperability which will allow trading partners to seamlessly and dynamically come together and do business without ad hoc and proprietary integrations. Such a level of interoperability involves being able to find potential business partners, discovering their services and business processes, and conducting business "on the fly". This process of dynamic interoperation is only possible through standard B2B frameworks. Indeed a number of B2B electronic commerce standard frameworks have emerged recently. Although most of these standards are overlapping and competing, each with its own strenghts and weeknesses, a closer investigation reveals that they can be used in a manner to complement one another.In this paper we describe such an implementation where an ebXML infrastructure is developed by exploiting the Universal Description, Discovery and Integration (UDDI) registries and RosettaNet Partner Interface Processes (PIPs). ebXML is an ambitious effort and produced detailed specifications of an infrastructure both for B2B and B2C e-commerce. However a public ebXML compliant registry/repository mechanism is not available yet. On the other hand, UDDI's approach to developing a registry has been a lot simpler and public registries are available. In ebXML, trading parties collaborate by agreeing on the same business process with complementary roles. Therefore there is a need for standardized business processes. In this respect, exploiting the already developed expertise through RosettaNet PIPs becomes indispensable. We show how to create and use ebXML "Binary Collaborations" based on RosettaNet PIPs and provide a GUI tool to allow users to graphically build their ebXML business processes by combining RosettaNet PIPs. In ebXML, trading parties reveal essential information about themselves through Collaboration Protocol Profiles (CPPs). To conduct business, an agreement between parties is necessary and this is expressed},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {512–523},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564751,
author = {Josifovski, Vanja and Schwarz, Peter and Haas, Laura and Lin, Eileen},
title = {Garlic: A New Flavor of Federated Query Processing for DB2},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564751},
doi = {10.1145/564691.564751},
abstract = {In a large modern enterprise, information is almost inevitably distributed among several database management systems. Despite considerable attention from the research community, relatively few commercial systems have attempted to address this issue. This paper describes new technology that enables clients of IBM's DB2 Universal Database to access the data and specialized computational capabilities of a wide range of non-relational data sources. This technology, based on the Garlic prototype developed at the Almaden Research Center, complements and extends DB2's existing ability to federate relational data sources.The paper focuses on three topics. Firstly, we show how the DB2 catalogs are used as an extensible repository for the metadata needed to access remotely-stored information. Secondly, we describe how the Garlic approach to query planning, in which source-specific modules and the federated server cooperate to develop an optimized execution plan, has been realized in DB2. Lastly, we describe how DB2's query execution engine has been extended to support queries and functions that are evaluated remotely.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {524–532},
numpages = {9},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564753,
author = {Bumbulis, Peter and Bowman, Ivan T.},
title = {A Compact B-Tree},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564753},
doi = {10.1145/564691.564753},
abstract = {In this paper we describe a Patricia tree-based B-tree variant suitable for OLTP. In this variant, each page of the B-tree contains a local Patricia tree instead of the usual sorted array of keys. It has been implemented in iAnywhere ASA Version 8.0. Preliminary experience has shown that these indexes can provide significant space and performance benefits over existing ASA indexes.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {533–541},
numpages = {9},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564754,
author = {Weininger, Andreas},
title = {Efficient Execution of Joins in a Star Schema},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564754},
doi = {10.1145/564691.564754},
abstract = {A star schema is very popular for modeling data warehouses and data marts. Therefore, it is important that a database system which is used for implementing such a data warehouse or data mart is able to efficiently handle operations on such a schema. In this paper we will describe how one of these operations, the join operation --- probably the most important operation --- is implemented in the IBM Informix Extended Parallel Server (XPS).},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {542–545},
numpages = {4},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564755,
author = {Kothuri, Ravi Kanth V and Ravada, Siva and Abugov, Daniel},
title = {Quadtree and R-Tree Indexes in Oracle Spatial: A Comparison Using GIS Data},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564755},
doi = {10.1145/564691.564755},
abstract = {Spatial indexing has been one of the active focus areas in recent database research. Several variants of Quadtree and R-tree indexes have been proposed in database literature. In this paper, we first describe briefly our implementation of Quadtree and R-tree index structures and related optimizations in Oracle Spatial. We then examine the relative merits of two structures as implemented in Oracle Spatial and compare their performance for different types of queries and other operations. Finally, we summarize experiences with these different structures in indexing large GIS datasets in Oracle Spatial.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {546–557},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564757,
author = {Rao, Jun and Zhang, Chun and Megiddo, Nimrod and Lohman, Guy},
title = {Automating Physical Database Design in a Parallel Database},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564757},
doi = {10.1145/564691.564757},
abstract = {Physical database design is important for query performance in a shared-nothing parallel database system, in which data is horizontally partitioned among multiple independent nodes. We seek to automate the process of data partitioning. Given a workload of SQL statements, we seek to determine automatically how to partition the base data across multiple nodes to achieve overall optimal (or close to optimal) performance for that workload. Previous attempts use heuristic rules to make those decisions. These approaches fail to consider all of the interdependent aspects of query performance typically modeled by today's sophisticated query optimizers.We present a comprehensive solution to the problem that has been tightly integrated with the optimizer of a commercial shared-nothing parallel database system. Our approach uses the query optimizer itself both to recommend candidate partitions for each table that will benefit each query in the workload, and to evaluate various combinations of these candidates. We compare a rank-based enumeration method with a random-based one. Our experimental results show that the former is more effective.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {558–569},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564758,
author = {Szalay, Alexander S. and Gray, Jim and Thakar, Ani R. and Kunszt, Peter Z. and Malik, Tanu and Raddick, Jordan and Stoughton, Christopher and vandenBerg, Jan},
title = {The SDSS Skyserver: Public Access to the Sloan Digital Sky Server Data},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564758},
doi = {10.1145/564691.564758},
abstract = {The SkyServer provides Internet access to the public Sloan Digital Sky Survey (SDSS) data for both astronomers and for science education. This paper describes the SkyServer goals and architecture. It also describes our experience operating the SkyServer on the Internet. The SDSS data is public and well-documented so it makes a good test platform for research on database algorithms and performance.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {570–581},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564759,
author = {Poess, Meikel and Smith, Bryan and Kollar, Lubor and Larson, Paul},
title = {TPC-DS, Taking Decision Support Benchmarking to the next Level},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564759},
doi = {10.1145/564691.564759},
abstract = {TPC-DS is a new decision support benchmark currently under development by the Transaction Processing Performance Council (TPC). This paper provides a brief overview of the new benchmark. The benchmark models the decision support functions of a retail product supplier, including data loading, multiple types of queries and data maintenance. The database consists of multiple snowflake schemas with shared dimension tables; data is skewed; and the query set is large. Overall, the benchmark is considerably more realistic than previous decision support benchmarks.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {582–587},
numpages = {6},
keywords = {TPC, data warehouse, benchmark, performance evaluation, decision support},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564761,
author = {The TimesTen Team},
title = {Mid-Tier Caching: The TimesTen Approach},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564761},
doi = {10.1145/564691.564761},
abstract = {TimesTen is an in-memory, application-tier data manager that delivers low response time and high throughput. Applications may create tables and manage them exclusively in TimesTen, and they may optionally cache frequently used subsets of a disk-based relational database in TimesTen. Cached tables and tables managed exclusively by TimesTen may coexist in the same database. Queries and updates to the cache are performed by the application through SQL. Applications running on different mid-tier servers may cache different or overlapping subsets of the same back-end database. TimesTen keeps the caches synchronized with each other and with the back-end database.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {588–593},
numpages = {6},
keywords = {main-memory data management, mid-tier caching.},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564762,
author = {Anton, Jesse and Jacobs, Lawrence and Liu, Xiang and Parker, Jordan and Zeng, Zheng and Zhong, Tie},
title = {Web Caching for Database Applications with Oracle Web Cache},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564762},
doi = {10.1145/564691.564762},
abstract = {We discuss several important issues specific to Web caching for content dynamically generated from database applications. We present the techniques employed by Oracle Web Cache to address these issues. They include: content disambiguation based on information in addition to the URL, transparent session management, partial-page caching for personalization, and broad-scope invalidation with performance assurance heuristics.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {594–599},
numpages = {6},
keywords = {dynamic content, caching, consistency, session, partial-page caching, invalidation, personalization, heuristics, disambiguation, fragment, template, performance},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564763,
author = {Luo, Qiong and Krishnamurthy, Sailesh and Mohan, C. and Pirahesh, Hamid and Woo, Honguk and Lindsay, Bruce G. and Naughton, Jeffrey F.},
title = {Middle-Tier Database Caching for e-Business},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564763},
doi = {10.1145/564691.564763},
abstract = {While scaling up to the enormous and growing Internet population with unpredictable usage patterns, E-commerce applications face severe challenges in cost and manageability, especially for database servers that are deployed as those applications' backends in a multi-tier configuration. Middle-tier database caching is one solution to this problem. In this paper, we present a simple extension to the existing federated features in DB2 UDB, which enables a regular DB2 instance to become a DBCache without any application modification. On deployment of a DBCache at an application server, arbitrary SQL statements generated from the unchanged application that are intended for a backend database server, can be answered: at the cache, at the backend database server, or at both locations in a distributed manner. The factors that determine the distribution of workload include the SQL statement type, the cache content, the application requirement on data freshness, and cost-based optimization at the cache. We have developed a research prototype of DBCache, and conducted an extensive set of experiments with an E-Commerce benchmark to show the benefits of this approach and illustrate tradeoffs in caching considerations.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {600–611},
numpages = {12},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564765,
author = {Altinel, Mehmet and Luo, Qiong and Krishnamurthy, Sailesh and Mohan, C. and Pirahesh, Hamid and Lindsay, Bruce G. and Woo, Honguk and Brown, Larry},
title = {DBCache: Database Caching for Web Application Servers},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564765},
doi = {10.1145/564691.564765},
abstract = {Many e-Business applications today are being developed and deployed on multi-tier environments involving browser-based clients, web application servers and backend databases. The dynamic nature of these applications necessitates generating web pages on-demand, making middle-tier database caching an effective approach to achieve high scalability and performance [3]. In the DBCache project, we are incorporating a database cache feature in DB2 UDB by modifying the engine code and leveraging existing federated database functionality. This allows us to take advantage of DB2's sophisticated distributed query processing power for database caching. As a result, the user queries can be executed at either the local database cache or the remote backend server, or more importantly, the query can be partitioned and then distributed to both databases for cost optimum execution.DBCache also includes a cache initialization component that takes a backend database schema and SQL queries in the workload, and generates a middle-tier database schema for the cache. We have implemented an initial prototype of the system that supports table level caching. As DB2's functionality is extended, we will be able to support subtable level caching, XML data caching and caching of execution results of web services.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {612},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564766,
author = {Markl, Volker and Lohman, Guy},
title = {Learning Table Access Cardinalities with LEO},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564766},
doi = {10.1145/564691.564766},
abstract = {LEO is a comprehensive way to repair incorrect statistics and cardinality estimates of a query execution plan. LEO introduces a feedback loop to query optimization that enhances the available information on the database where the most queries have occurred, allowing the optimizer to actually learn from its past mistakes. We demonstrate how LEO learns outdated table access statistics on a TPC-H like database schema and show that LEO improves the estimates for table cardinalities as well as filter factors for single predicates. Thus LEO enables the query optimizer to choose a better query execution plan, resulting in more efficient query processing. We not only demonstrate learning by repetitive execution of a single query, but also illustrate how similar, but not identical queries benefit from learned knowledge. In addition, we show the effect of both learning cardinalities and adjusting related statistics.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {613},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564767,
author = {Zhang, Xin and Mulchandani, Mukesh and Christ, Steffen and Murphy, Brian and Rundensteiner, Elke A.},
title = {Rainbow: Mapping-Driven XQuery Processing System},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564767},
doi = {10.1145/564691.564767},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {614},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564768,
author = {Theobald, Anja and Weikum, Gerhard},
title = {The XXL Search Engine: Ranked Retrieval of XML Data Using Indexes and Ontologies},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564768},
doi = {10.1145/564691.564768},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {615},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564769,
author = {Barbosa, Denilson and Mendelzon, Alberto and Keenleyside, John and Lyons, Kelly},
title = {ToXgene: A Template-Based Data Generator for XML},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564769},
doi = {10.1145/564691.564769},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {616},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564770,
author = {Abadi, Daniel J. and Cherniack, Mitch},
title = {Visual COKO: A Debugger for Query Optimizer Development},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564770},
doi = {10.1145/564691.564770},
abstract = {Query optimization generates plans to retrieve data requested by queries. Query rewriting, which is the first step of this process, rewrites a query expression into an equivalent form to prepare it for plan generation. COKO-KOLA introduced a new approach to query rewriting that enables query rewrites to be formally verified using an automated theorem prover [1]. KOLA is a language for expressing term rewriting rules that can be "fired" on query expressions. COKO is a language for expressing query rewriting transformations that are too complex to express with simple KOLA rules [2].COKO is a programming language designed for query optimizer development. Programming languages require debuggers, and in this demonstration, we illustrate our COKO debugger: Visual COKO. Visual COKO enables a query optimization developer to visually trace the execution of a COKO transformation. At every step of the transformation, the developer can view a tree-display that illustrates how the original query expression has evolved.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {617},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564771,
author = {Chen, Li and Rundensteiner, Elke A. and Wang, Song},
title = {XCache: A Semantic Caching System for XML Queries},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564771},
doi = {10.1145/564691.564771},
abstract = {A wide range of Web applications retrieve desired information from remote XML data sources across the Internet, which is usually costly due to transmission delays for large volumes of data. Therefore we propose to apply the ideas of semantic caching to XML query processing systems [2], in particular the XQuery engine. Semantic caching [3] implies view-based query answering and cache management. While it is well studied in the traditional database context, query containment for XQuery is left unexplored due to its complexity coming with the powerful expressiveness of hierarchy, recursion and result construction. We hence have developed the first solution for XQuery processing using cached views.We exploit the connections between XML and tree automata, and use subtype relations between two regular expression types to tackle the XQuery containment mapping problem. Inspired by XDuce [1], which explores the use of tree-automata-based regular expression types for XML processing, we have designed a containment mapping process to incorporate type inference and subtyping mechanisms provided by XDuce to establish containment mappings between regular-expression-type-based pattern variables of two queries. We have implemented a semantic caching system called XCache (see Figure 1), to realize the proposed containment and rewriting techniques for XQuery.The main modules of XCache include: (1) Query Decomposer. An input query is is decomposed into source-specific subqueries explicitly represented by matching patterns and return structures. (2) Query Pattern Register. By registering a few queries into semantic regions, we warm up XCache at its initialization phase. (3) Query Containment Mapper. The XDuce subtyper is incorporated into the containment mapper for establishing query containment mappings between variables of a new query and each cached query. (4) Query Rewriter. We implement the classical bucket algorithm and further apply heuristics to decide on an "optimal" rewriting plan if several valid ones exist. (5) Replacement Manager. We free space for new regions by both complete and partial replacement. (6) Region Coalescer. We apply a coalescing strategy to control the region granularity over time.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {618},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564773,
author = {Karbhari, Pradnya and Rabinovich, Michael and Xiao, Zhen and Douglis, Fred},
title = {ACDN: A Content Delivery Network for Applications},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564773},
doi = {10.1145/564691.564773},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {619},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564774,
author = {Wang, Tengjiao and Tang, Shiwei and Yang, Dongqing and Gao, Jun and Wu, Yuqing and Pei, Jian},
title = {COMMIX: Towards Effective Web Information Extraction, Integration and Query Answering},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564774},
doi = {10.1145/564691.564774},
abstract = {As WWW becomes more and more popular and powerful, how to search information on the web in database way becomes an important research topic. COMMIX, which is developed in the DB group in Peking University (China), is a system towards building very large database using data from the Web for information extraction, integration and query answering. COMMIX has some innovative features, such as ontology-based wrapper generation, XML-based information integration, view-based query answering, and QBE-style XML query interface.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {620},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564775,
author = {Fung, Wai Fu and Sun, David and Gehrke, Johannes},
title = {COUGAR: The Network is the Database},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564775},
doi = {10.1145/564691.564775},
abstract = {The widespread distribution and availability of small-scale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities [1]. Applications range from environmental control, warehouse inventory, health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offfsline querying and analysis. This approach has two major draw-backs. First, the user cannot change the behavior of the system on the fly. Second, communication in today's networks is orders of magnitude more expensive than local computation, thus in-network processing can vastly reduce resource usage and thus extend the lifetime of a sensor network.This demo demonstrates a database approach to unite the seemingly conflicting requirements of scalability and flexibility in monitoring the physical world. We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {621},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inbook{10.1145/564691.564776,
author = {Madden, Samuel and Hellerstein, Joseph M.},
title = {Distributing Queries over Low-Power Wireless Sensor Networks},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564776},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {622},
numpages = {1}
}

@inproceedings{10.1145/564691.564777,
author = {Cranor, Chuck and Gao, Yuan and Johnson, Theodore and Shkapenyuk, Vlaidslav and Spatscheck, Oliver},
title = {Gigascope: High Performance Network Monitoring with an SQL Interface},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564777},
doi = {10.1145/564691.564777},
abstract = {Operators of large networks and providers of network services need to monitor and analyze the network traffic flowing through their systems. Monitoring requirements range from the long term (e.g., monitoring link utilizations, computing traffic matrices) to the ad-hoc (e.g. detecting network intrusions, debugging performance problems). Many of the applications are complex (e.g., reconstruct TCP/IP sessions), query layer-7 data (find streaming media connections), operate over huge volumes of data (Gigabit and higher speed links), and have real-time reporting requirements (e.g., to raise performance or intrusion alerts).We have found that existing network monitoring technologies have severe limitations. One option is to use TCPdump to monitor a network port and a user-level application program to process the data. While this approach is very flexible, it is not fast enough to handle gigabit speeds on inexpensive equipment. Another approach is to use network monitoring devices. While these devices are capable of high speed monitoring, they are inflexible as the set of monitoring tasks is pre-defined. Adding new functionality is expensive and has long lead times. A similar approach is to use monitoring tools built into routers, such as SNMP, RMON, or NetFlow. These tools have similar characteristics --- fast but inflexible.A further problem with all of these tools is their lack of a query interface. The data from the monitors are dumped to a file or piped through a file stream without an association to the semantics of the data. The burden of managing and interpreting the data is left to the analyst. Due to the volume and complexity of the data, the burden can be severe. These problems make developing new applications needlessly slow and difficult. Also, many mistakes are made leading to incorrect analyses.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {623},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564778,
author = {Crescenzi, Valter and Mecca, Giansalvatore and Merialdo, Paolo},
title = {RoadRunner: Automatic Data Extraction from Data-Intensive Web Sites},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564778},
doi = {10.1145/564691.564778},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {624},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564779,
author = {Florescu, Daniela and Gr\"{u}nhagen, Andreas and Kossmann, Donald and Rost, Steffen},
title = {XL: A Platform for Web Services},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564779},
doi = {10.1145/564691.564779},
abstract = {We present a platform for Web services. Web services are implemented in a special XML programming language called XL [1, 2]. A Web service receives an XML message as input and returns an XML message as output. The platform supports a number of features that are particularly useful to implement Web services; e.g., logging, timetables, conversations, workflow management, automatic transactions, security. Our platform is going to be compliant with all W3C standards and emerging proposals. The programming language is very abstract and can be optimized automatically (like SQL). Furthermore, the platform allows to integrate Web services that are written in XL and other programming languages.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {625},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564781,
author = {Han, Jiawei and Wang, Jianyong and Dong, Guozhu and Pei, Jian and Wang, Ke},
title = {CubeExplorer: Online Exploration of Data Cubes},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564781},
doi = {10.1145/564691.564781},
abstract = {Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation [2], cube-based feature extraction, and gradient analysis [1], and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {626},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564782,
author = {Agrawal, Sanjay and Chaudhuri, Surajit and Das, Gautam},
title = {DBXplorer: Enabling Keyword Search over Relational Databases},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564782},
doi = {10.1145/564691.564782},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {627},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564783,
author = {Phan, Jessica M. and Ng, Raymond},
title = {GEA: A Toolkit for Gene Expression Analysis},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564783},
doi = {10.1145/564691.564783},
abstract = {Currently gene expression data are being produced at a phenomenal rate. The general objective is to try to gain a better understanding of the functions of cellular tissues. In particular, one specific goal is to relate gene expression to cancer diagnosis, prognosis and treatment. However, a key obstacle is that the availability of analysis tools or lack thereof, impedes the use of the data, making it difficult for cancer researchers to perform analysis efficiently and effectively.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {628},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564784,
author = {Hinneburg, Alexander and Keim, Daniel A. and Wawryniuk, Markus},
title = {HD-Eye: Visual Clustering of High Dimensional Data},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564784},
doi = {10.1145/564691.564784},
abstract = {Clustering of large data bases is an important research area with a large variety of applications in the data base context. Missing in most of the research efforts are means for guiding the clustering process and understanding the results, which is especially important for high dimensional data. Visualization technology may help to solve this problem since it provides effective support of different clustering paradigms and allows a visual inspection of the results. The HD-Eye (high-dim. eye) system shows that a tight integration of advanced clustering algorithms and state-of-the-art visualization techniques is powerful for a better understanding and effective guidance of the clustering process, and therefore can help to significantly improve the clustering results.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {629},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564785,
author = {Lu, Hongjun and Wang, Guoren and Yu, Ge and Bao, Yubin and Lv, Jianhua and Yu, Yaxin},
title = {XBase: Making Your Gigabyte Disk Queriable},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564785},
doi = {10.1145/564691.564785},
abstract = {With the rapid development of the Internet and the World Wide Web (WWW), very large amount of information is available and ready for downloading, most of which are free of charge. At the same time, hard disks with large capacity are available at affordable prices. Most of us nowadays often dump a large number of various types of documents into our computers without much thinking. On the other hand, file systems have not changed too much during the past decades. Most of them organize files in directories that form a tree structure, and a file is identified by its name and pathname in the directory tree. Remembering name of files created sometime ago and digging them out from a disk with dozen gigabytes of data in hundred thousands of files becomes never an easy task. Tools available for helping such a search are still far from satisfactory.Xbase (XML-based document BASE) is a prototype system aiming at addressing the above problem. By XML-based, we meant that XML is used to define the metadata. The current version of XBase stores text-based files, including semi-structured data such as XML, HTML, plain text documents (e.g., tex files, computer programs) and those files that can be converted into text (e.g., postscript files, PDF files). In XBase, file name is optional. Users can just load a file into XBase without giving a name and the directory where it should be stored. XBase will automatically associate it with attributes such as the time when the file was saved, its source, its size and type, and etc., To retrieve those files, XBase provides three access methods, explorative browsing, querying using query languages, and keyword based search.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {630},
numpages = {1},
keywords = {XML query processing, multidimensional browsing, DOM},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564786,
author = {Rundensteiner, Elke A. and Ward, Matthew O. and Yang, Jing and Doshi, Punit R.},
title = {XmdvTool: Visual Interactive Data Exploration and Trend Discovery of High-Dimensional Data Sets},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564786},
doi = {10.1145/564691.564786},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {631},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564788,
author = {Hellerstein, Joseph M. and Widom, Jennifer},
title = {Data Streams: Fresh Current or Stagnant Backwater? (Panel)},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564788},
doi = {10.1145/564691.564788},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {632},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564790,
author = {Bonnet, Philippe},
title = {Going Public: Open-Source Databases and Database Research},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564790},
doi = {10.1145/564691.564790},
abstract = {There are a number of database systems available free of charge for the research community, with complete access to the source code. Some of these systems result from completed research projects, others have been developed outside the research community. How can the database community best take advantage of these publically available systems? The most widely used open-source database is MySQL. Their objective is to become the 'best and most used database in the world'. Can they do it without the database research community?},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {633},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564792,
author = {Cotton, Paul},
title = {Implementing XQuery},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564792},
doi = {10.1145/564691.564792},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {634},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564794,
author = {Garofalakis, Minos and Gehrke, Johannes and Rastogi, Rajeev},
title = {Querying and Mining Data Streams: You Only Get One Look a Tutorial},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564794},
doi = {10.1145/564691.564794},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {635},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564796,
author = {Mohan, C.},
title = {Tutorial: Application Servers and Associated Technologies},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564796},
doi = {10.1145/564691.564796},
abstract = {Application Servers (ASs), which have become very popular in the last few years, provide the platforms for the execution of transactional, server-side applications in the online world. ASs are the modern cousins of traditional transaction processing monitors (TPMs) like CICS. In this tutorial, I will provide an introduction to different ASs and their technologies. ASs play a central role in enabling electronic commerce in the web context. They are built on the basis of more standardized protocols and APIs than were the traditional TPMs. The emergence of Java, XML and OMG standards has played a significant role in this regard. Consequently, I will also briefly introduce the related XML, Java and OMG technologies like SOAP, J2EE and CORBA. One of the most important features of ASs is their ability to integrate the modern application environments with legacy data sources like IMS, CICS, VSAM, etc. They provide a number of connectors for this purpose, typically using asynchronous transactional messaging technologies like MQSeries and JMS. Traditional TPM-style requirements for industrial strength features like scalability, availability, reliability and high performance are equally important for ASs also. Security and authentication issues are additional important requirements in the web context. ASs support DBMSs not only as storage engines for user data but also as repositories for tracking their own state. Recently, the ECPerf benchmark has been developed via the Java Community Process to evaluate in a standardized way the cost performance of J2EE-compliant ASs. Several caching technologies have been developed to improve performance of ASs.Soon after this conference is over, the slides of this tutorial will be available on the web at the following URL: http://www.almaden.ibm.com/u/mohan/AppServersTutorial_SIGMOD2002_Slides.pdf},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {636},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564799,
author = {Shasha, Dennis and Bonnet, Philippe},
title = {Database Tuning: Principles, Experiments, and Troubleshooting Techniques (Part II)},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564799},
doi = {10.1145/564691.564799},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {637},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564798,
author = {Shasha, Dennis and Bonnet, Philippe},
title = {Database Tuning: Principles, Experiments, and Troubleshooting Techniques (Part I)},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564798},
doi = {10.1145/564691.564798},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {637},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

@inproceedings{10.1145/564691.564801,
author = {Bussler, Christoph},
title = {Software as a Service: ASP and ASP Aggregation},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564691.564801},
doi = {10.1145/564691.564801},
abstract = {The tutorial "Software as a Service: ASP and ASP aggregation" will give an introduction and overview of the concept of "renting" access to software to customers (subscribers). Application service providers (ASPs) are enterprises hosting one or more applications and provide access to subscribers over the Internet by means of browser technology. Furthermore, the underlying technologies are discussed to enable application hosting. The concept of ASP aggregation is introduced to provide a single access point and a single sign-on capability to subscribers sub-scribing to more than one hosted application in more than one ASP.},
booktitle = {Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data},
pages = {638},
numpages = {1},
location = {Madison, Wisconsin},
series = {SIGMOD '02}
}

