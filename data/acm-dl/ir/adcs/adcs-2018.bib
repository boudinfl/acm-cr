@inproceedings{10.1145/3291992.3292002,
author = {Thomas, Paul and Moffat, Alistair and Bailey, Peter and Scholer, Falk and Craswell, Nick},
title = {Better Effectiveness Metrics for SERPs, Cards, and Rankings},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3292002},
doi = {10.1145/3291992.3292002},
abstract = {Offline metrics for IR evaluation are often derived from a user model that seeks to capture the interaction between the user and the ranking, conflating the interaction with a ranking of documents with the user's interaction with the search results page. A desirable property of any effectiveness metric is if the scores it generates over a set of rankings correlate well with the "satisfaction" or "goodness" scores attributed to those same rankings by a population of searchers.Using data from a large-scale web search engine, we find that offline effectiveness metrics do not correlate well with a behavioural measure of satisfaction that can be inferred from user activity logs. We then examine three mechanisms to improve the correlation: tuning the model parameters; improving the label coverage, so that more kinds of item are labelled and hence included in the evaluation; and modifying the underlying user models that describe the metrics. In combination, these three mechanisms transform a wide range of common metrics into "card-aware" variants which allow for the gain from cards (or snippets), varying probabilities of clickthrough, and good abandonment.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {1},
numpages = {8},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3292004,
author = {Wicaksono, Alfan Farizki and Moffat, Alistair},
title = {Exploring Interaction Patterns in Job Search},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3292004},
doi = {10.1145/3291992.3292004},
abstract = {We analyze interaction logs from Seek.com, a well-known Australasian employment site, with the goal of better understanding the ways in which users pursue their search goals following the issue of each query. Of particular interest are the patterns of job summary viewing and click-through behaviors that arise, and the differences in activity between mobile/tablet-based users (Android/iOS) and computer-based users.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {2},
numpages = {8},
keywords = {user behavior, job search, Interaction logs},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3291995,
author = {Yang, Ziying and Moffat, Alistair and Turpin, Andrew},
title = {Pairwise Crowd Judgments: Preference, Absolute, and Ratio},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3291995},
doi = {10.1145/3291992.3291995},
abstract = {Relevance judgments are conventionally formed by small numbers of experts using ordinal relevance scales defined by two or more relevance categories. Such judgments often contain many ties: documents in the same category that cannot be separated by relevance. Here we explore the use of crowd-sourcing and combined three-way relevance assessments using pairwise preference, absolute relevance, and relevance ratio, with forced choice testing and embedded quality control processes, seeking to reduce assessment ties, and to increase judgment consistency. In particular, the crowd-sourced judgments from these three approaches were normalized into numeric relevance scores, and compared against judgments arising via three previous techniques: NIST binary; Sormunen; and magnitude estimation. The relationship between generated judgment reliability and number of document pairs assessed was also explored, as was the effect that factors such as document length, topic difficulty, number of documents judged, and assessment time, have on assessment reliability. Lastly, we investigate the extent to which the methodology used to collect judgments affects the ability of an experiment to discriminate between IR systems.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {3},
numpages = {8},
keywords = {Pairwise preference, relevance assessment, test collection, crowd-sourcing},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3292001,
author = {Trotman, Andrew and Lilly, Kat},
title = {Elias Revisited: Group Elias SIMD Coding},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3292001},
doi = {10.1145/3291992.3292001},
abstract = {The prior belief that the Elias gamma and delta coding are slow because of the bit-wise manipulations is examined in the light of new CPU instructions that perform those manipulations. It is shown that despite using those instructions, Elias gamma and Elias delta remain slow compared to SIMD codecs such as QMX. We provide a theoretical basis on which to bit-wise encode data, and show that it is equivalent to SIMD extensions to Elias gamma that others have already introduced. Extending this we introduce a new SIMD Elias delta variant. Experiments comparing these two codecs to QMX on public data, and in the JASSv2 search engine show that although the index is slightly larger than QMX, search throughput is increased and latency is decreased.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {4},
numpages = {8},
keywords = {Index Compression, Elias Compression, Integer Compression},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3291999,
author = {Gallagher, Luke and Mackenzie, Joel and Culpepper, J. Shane},
title = {Revisiting Spam Filtering in Web Search},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3291999},
doi = {10.1145/3291992.3291999},
abstract = {The Waterloo spam scores are now a commonly used static document feature in web collections such as ClueWeb. This feature can be used as a post-retrieval filter, as a document prior, or as one of many features in a Learning-to-Rank system. In this work, we highlight the risks associated with using spam scores as a post-retrieval filter, which is now common practice in experiments with the ClueWeb test collection. While it increases the average evaluation score and boosts the performance of some topics, it can significantly harm the performance of others. Through a detailed failure analysis, we show that simple spam filtering is a high risk practice that should be avoided in future work, particularly when working with the ClueWeb12 test collection.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {5},
numpages = {4},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3292003,
author = {Koopman, Bevan and Nguyen, Anthony and Cossio, Danica and Courage, Mary-Jane and Francois, Gary},
title = {Extracting Cancer Mortality Statistics from Free-Text Death Certificates: A View from the Trenches},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3292003},
doi = {10.1145/3291992.3292003},
abstract = {This industry paper describes a deep learning and information retrieval system that allows cancer registries to extract cancer mortality statistics from free-text death certificates. Death certificates may provide an invaluable source of mortality information but to realise this value automated methods for classifying cancer types and searching certificates are needed. We present a system comprising a deep learning classifier to identify cancer related deaths, an IR system to allow users to search death certificates and classifier results, and a deployment architecture that aims to handle issues of scalability and complexity. Empirically, the system can accurately identify cancer deaths for both common and rare cancers. The use of the IR system helps users drill into specific results and convince them of the utility of using an automated approach. The paper aims to touch on a number of issues in applying deep learning and IR techniques to real-world settings.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {6},
numpages = {4},
keywords = {Death certificates, Cancer mortality},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3291993,
author = {Oosterhuis, Harrie and Culpepper, J. Shane and de Rijke, Maarten},
title = {The Potential of Learned Index Structures for Index Compression},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3291993},
doi = {10.1145/3291992.3291993},
abstract = {Inverted indexes are vital in providing fast key-word-based search. For every term in the document collection, a list of identifiers of documents in which the term appears is stored, along with auxiliary information such as term frequency, and position offsets. While very effective, inverted indexes have large memory requirements for web-sized collections. Recently, the concept of learned index structures was introduced, where machine learned models replace common index structures such as B-tree-indexes, hash-indexes, and bloom-filters. These learned index structures require less memory, and can be computationally much faster than their traditional counterparts. In this paper, we consider whether such models may be applied to conjunctive Boolean querying. First, we investigate how a learned model can replace document postings of an inverted index, and then evaluate the compromises such an approach might have. Second, we evaluate the potential gains that can be achieved in terms of memory requirements. Our work shows that learned models have great potential in inverted indexing, and this direction seems to be a promising area for future research.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {7},
numpages = {4},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3291998,
author = {Chappell, Timothy and Geva, Shlomo and Hogan, James and Perrin, Dimitri},
title = {Rapid Hierarchical Clustering of Biological Sequences},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3291998},
doi = {10.1145/3291992.3291998},
abstract = {Genomic sequences can be viewed as special types of documents. These are typically organised and stored as collections of text documents. The collections are vast; a single metagenomics sequencing experiment can produce as much text data as the entire contents of Wikipedia. The increasing scale of the datasets obtained from environmental and clinical metagenomic studies has posed great difficulties to researchers attempting to organise this data with traditional clustering techniques. In this work we introduce a clustering approach that combines the highly effective strategy of using binary signature representations of sequence data -- allowing extremely fast pairwise comparison through hardware level operations -- and couples these representations with an O(N log N) hierarchical clustering approach based on the k-tree data structure. The k-tree approach has previously been applied to the clustering of English-language documents from the ClueWeb collection. Here we extend its use to the clustering of genomic sequence text files.We demonstrate the success of our approach on the largest real biological datasets available from the SILVA project, showing that our methods provide clustering quality comparable to those of a number of standard methods while offering speed performance about an order of magnitude greater than these alternatives. Through the use of synthetic data, we show that this new approach is able to handle even extreme scale data sets in convenient timeframes, allowing rapid analyses to be performed over collections of tens of millions of reads (sequences) that are typically produced in a single sequencing run.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {8},
numpages = {4},
keywords = {document signatures, read sequencing, bioinformatics, hamming distance, Clustering},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3291996,
author = {Albahem, Ameer and Spina, Damiano and Scholer, Falk and Moffat, Alistair and Cavedon, Lawrence},
title = {Desirable Properties for Diversity and Truncated Effectiveness Metrics},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3291996},
doi = {10.1145/3291992.3291996},
abstract = {A wide range of evaluation metrics have been proposed to measure the quality of search results, including in the presence of diversification. Some of these metrics have been adapted for use in search tasks with different complexities, such as where the search system returns lists of different lengths. Given the range of requirements, it can be difficult to compare the behavior of these metrics. In this work, we examine effectiveness metrics using a simple property-based approach. In particular, we present a case-analysis framework to define and study fundamental properties that seem integral to any evaluation metric. An example of a simple property is that a ranking with only one non-relevant document should never score lower than a ranking with two non-relevant documents. The framework facilitates quantifying the ability of metrics to satisfy properties, both separately and simultaneously, and to identify those cases where properties are violated. Our analysis shows that the Average Cube Test and Intent-Aware Average Precision are two metrics which fail to satisfy the desirable properties, and hence should be used with caution.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {9},
numpages = {7},
keywords = {Search result diversification, Evaluation, Axiomatic analysis},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3291997,
author = {Buckingham, Lawrence and Geva, Shlomo and Hogan, James M.},
title = {Protein Database Search Using Compressed K-Mer Vocabularies},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3291997},
doi = {10.1145/3291992.3291997},
abstract = {Efficient and accurate search in biological sequence databases remains a matter of priority due to the on-going rapid accumulation of genomic data becoming available for analysis. An array of accelerated sequence comparison methods have been implemented, including tools which compute explicit pairwise alignments, and alignment-free techniques based on word co-occurrence, locality sensitive hashing, or metric embedding. These methods offer significant speed improvement over standard algorithms, but increased throughput comes at the cost of reduced sensitivity. Strategies such as inverted indexing and hashing enable efficient retrieval of stored sequences which share near-identical common sub-sequences with a query, but their precision diminishes as the level of shared identity decreases, so that sequences which are distantly related to the query go undetected.We present a new sequence database search algorithm which derives a compressed vocabulary consisting of subsequences of length k (k-mers) sampled from the database, and uses the compressed vocabulary to map each sequence to a binary feature vector based on its content. Feature vector similarity is taken as a proxy for more expensive local alignment measurements. Feature vector construction seamlessly incorporates biologically grounded symbol substitutions, so the algorithm remains effective at low levels of sequence identity. Empirical tests conducted with real-world data demonstrate that the binary vector encoding permits ranking accuracy that rivals and in some cases exceeds that of mainstream database search programs, with run times that are faster by an order of magnitude or more.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {10},
numpages = {7},
keywords = {Protein database search, Sequence comparison},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3292005,
author = {Liu, Binsheng and Lu, Xiaolu and Kurland, Oren and Culpepper, J. Shane},
title = {Improving Search Effectiveness with Field-Based Relevance Modeling},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3292005},
doi = {10.1145/3291992.3292005},
abstract = {Fields are a valuable auxiliary source of information in semi-structured HTML web documents. So, it is no surprise that ranking models have been designed to leverage this information to improve search effectiveness. We present the first (initial) study of utilizing field-based information in the relevance modeling framework. Fields play two different, and integrated, roles in our models: sources of information for inducing relevance models and units on which relevance models are applied for ranking. Our preliminary results suggest that field-based relevance modeling can improve precision at top ranks; specifically, to a greater extent than the commonly used BM25F and SDM-Fields field-based models. Further analysis shows that using field-based relevance models mainly improves the effectiveness of tail queries. Our findings suggest that using field-based information together with relevance modeling is a promising area of future exploration.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {11},
numpages = {4},
keywords = {relevance modeling, web search, field-based retrieval models},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3292000,
author = {Crimp, Reuben and Trotman, Andrew},
title = {Refining Query Expansion Terms Using Query Context},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3292000},
doi = {10.1145/3291992.3292000},
abstract = {Query expansion is commonly used to combat the vocabulary mismatch problem, it bridges the disparity between the vocabulary used in the corpus and search queries. However, if expansion terms are not chosen carefully, there is a risk of including spurious expansion terms, which can broaden the potential interpretations of the modified query. Unintentionally increasing the semantic ambiguity in this way is known as query drift.In this short paper we propose using the query context to inform the expansion term selection process. Using WordNet as an initial source of expansion terms, we refine the candidate expansions by discriminating relevancy. We found that our term selection process is more effective than the standard approach. Our technique targets terms which relate to the entire query as a whole, but predominately focuses on excluding spurious expansion terms. Both help reduce query drift and increase query performance.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {12},
numpages = {4},
keywords = {Word-Net, Wu-Palmer Similarity, Query Drift, Ad-hoc Retrieval, Thesaurus, Query Expansion},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

@inproceedings{10.1145/3291992.3291994,
author = {Seneviratne, Dilesha and Geva, Shlomo and Zuccon, Guido and Ferraro, Gabriela and Meireles, Magali},
title = {Linking Patents to Knowledge Sources: A Context Matching Technique Using Automatic Patent Classification},
year = {2018},
isbn = {9781450365499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291992.3291994},
doi = {10.1145/3291992.3291994},
abstract = {Patents contain a great deal of technical knowledge and are not easy to understand without in-depth field expertise, or additional supporting materials. We hypothesise that this technical knowledge can be made more accessible to readers, regardless of their level of knowledge in the field of the patent, by linking the patents' content to external knowledge sources. This paper investigates the process of linking patents to external knowledge sources, proposing a novel context matching technique based on an automated patent classification system. The proposed method can improve the precision of the created links, validating the utility of the context match between patent and knowledge source.},
booktitle = {Proceedings of the 23rd Australasian Document Computing Symposium},
articleno = {13},
numpages = {4},
location = {Dunedin, New Zealand},
series = {ADCS '18}
}

