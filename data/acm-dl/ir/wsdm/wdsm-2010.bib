@inproceedings{10.1145/1718487.1718489,
author = {Elsas, Jonathan L. and Dumais, Susan T.},
title = {Leveraging Temporal Dynamics of Document Content in Relevance Ranking},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718489},
doi = {10.1145/1718487.1718489},
abstract = {Many web documents are dynamic, with content changing in varying amounts at varying frequencies. However, current document search algorithms have a static view of the document content, with only a single version of the document in the index at any point in time. In this paper, we present the first published analysis of using the temporal dynamics of document content to improve relevance ranking. We show that there is a strong relationship between the amount and frequency of content change and relevance. We develop a novel probabilistic document ranking algorithm that allows differential weighting of terms based on their temporal characteristics. By leveraging such content dynamics we show significant performance improvements for navigational queries.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {1–10},
numpages = {10},
keywords = {versioned documents, web search, temporal change},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718490,
author = {Dong, Anlei and Chang, Yi and Zheng, Zhaohui and Mishne, Gilad and Bai, Jing and Zhang, Ruiqiang and Buchner, Karolina and Liao, Ciya and Diaz, Fernando},
title = {Towards Recency Ranking in Web Search},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718490},
doi = {10.1145/1718487.1718490},
abstract = {In web search, recency ranking refers to ranking documents by relevance which takes freshness into account. In this paper, we propose a retrieval system which automatically detects and responds to recency sensitive queries. The system detects recency sensitive queries using a high precision classifier. The system responds to recency sensitive queries by using a machine learned ranking model trained for such queries. We use multiple recency features to provide temporal evidence which effectively represents document recency. Furthermore, we propose several training methodologies important for training recency sensitive rankers. Finally, we develop new evaluation metrics for recency sensitive queries. Our experiments demonstrate the efficacy of the proposed approaches.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {11–20},
numpages = {10},
keywords = {recency sensitive query classification, temporal, recency ranking},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718491,
author = {Das Sarma, Anish and Das Sarma, Atish and Gollapudi, Sreenivas and Panigrahy, Rina},
title = {Ranking Mechanisms in Twitter-like Forums},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718491},
doi = {10.1145/1718487.1718491},
abstract = {We study the problem of designing a mechanism to rank items in forums by making use of the user reviews such as thumb and star ratings. We compare mechanisms where forum users rate individual posts and also mechanisms where the user is asked to perform a pairwise comparison and state which one is better. The main metric used to evaluate a mechanism is the ranking accuracy vs the cost of reviews, where the cost is measured as the average number of reviews used per post. We show that for many reasonable probability models, there is no thumb (or star) based ranking mechanism that can produce approximately accurate rankings with bounded number of reviews per item. On the other hand we provide a review mechanism based on pairwise comparisons which achieves approximate rankings with bounded cost. We have implemented a system, shoutvelocity, which is a twitter-like forum but items (i.e., tweets in Twitter) are rated by using comparisons. For each new item the user who posts the item is required to compare two previous entries. This ensures that over a sequence of n posts, we get at least n comparisons requiring one review per item on average. Our mechanism uses this sequence of comparisons to obtain a ranking estimate. It ensures that every item is reviewed at least once and winning entries are reviewed more often to obtain better estimates of top items.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {21–30},
numpages = {10},
keywords = {thumb-based ranking, ranking mechanisms, comparisons},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718492,
author = {Bendersky, Michael and Metzler, Donald and Croft, W. Bruce},
title = {Learning Concept Importance Using a Weighted Dependence Model},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718492},
doi = {10.1145/1718487.1718492},
abstract = {Modeling query concepts through term dependencies has been shown to have a significant positive effect on retrieval performance, especially for tasks such as web search, where relevance at high ranks is particularly critical. Most previous work, however, treats all concepts as equally important, an assumption that often does not hold, especially for longer, more complex queries. In this paper, we show that one of the most effective existing term dependence models can be naturally extended by assigning weights to concepts. We demonstrate that the weighted dependence model can be trained using existing learning-to-rank techniques, even with a relatively small number of training queries. Our study compares the effectiveness of both endogenous (collection-based) and exogenous (based on external sources) features for determining concept importance. To test the weighted dependence model, we perform experiments on both publicly available TREC corpora and a proprietary web corpus. Our experimental results indicate that our model consistently and significantly outperforms both the standard bag-of-words model and the unweighted term dependence model, and that combining endogenous and exogenous features generally results in the best retrieval effectiveness.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {31–40},
numpages = {10},
keywords = {weighted dependence model, query concept weighting},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718493,
author = {Dang, Van and Croft, Bruce W.},
title = {Query Reformulation Using Anchor Text},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718493},
doi = {10.1145/1718487.1718493},
abstract = {Query reformulation techniques based on query logs have been studied as a method of capturing user intent and improving retrieval effectiveness. The evaluation of these techniques has primarily, however, focused on proprietary query logs and selected samples of queries. In this paper, we suggest that anchor text, which is readily available, can be an effective substitute for a query log and study the effectiveness of a range of query reformulation techniques (including log-based stemming, substitution, and expansion) using standard TREC collections. Our results show that log-based query reformulation techniques are indeed effective with standard collections, but expansion is a much safer form of query modification than word substitution. We also show that using anchor text as a simulated query log is as least as effective as a real log for these techniques.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {41–50},
numpages = {10},
keywords = {anchor log, query substitution, query log, anchor text, query reformulation, query expansion},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718495,
author = {Heymann, Paul and Paepcke, Andreas and Garcia-Molina, Hector},
title = {Tagging Human Knowledge},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718495},
doi = {10.1145/1718487.1718495},
abstract = {A fundamental premise of tagging systems is that regular users can organize large collections for browsing and other tasks using uncontrolled vocabularies. Until now, that premise has remained relatively unexamined. Using library data, we test the tagging approach to organizing a collection. We find that tagging systems have three major large scale organizational features: consistency, quality, and completeness. In addition to testing these features, we present results suggesting that users produce tags similar to the topics designed by experts, that paid tagging can effectively supplement tags in a tagging system, and that information integration may be possible across tagging systems.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {51–60},
numpages = {10},
keywords = {library science, tagging systems},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718496,
author = {Ganti, Venkatesh and K\"{o}nig, Arnd Christian and Li, Xiao},
title = {Precomputing Search Features for Fast and Accurate Query Classification},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718496},
doi = {10.1145/1718487.1718496},
abstract = {Query intent classification is crucial for web search and advertising. It is known to be challenging because web queries contain less than three words on average, and so provide little signal to base classification decisions on. At the same time, the vocabulary used in search queries is vast: thus, classifiers based on word-occurrence have to deal with a very sparse feature space, and often require large amounts of training data. Prior efforts to address the issue of feature sparseness augmented the feature space using features computed from the results obtained by issuing the query to be classified against a web search engine. However, these approaches induce high latency, making them unacceptable in practice.In this paper, we propose a new class of features that realizes the benefit of search-based features without high latency. These leverage co-occurrence between the query keywords and tags applied to documents in search results, resulting in a significant boost to web query classification accuracy. By pre-computing the tag incidence for a suitably chosen set of keyword-combinations, we are able to generate the features online with low latency and memory requirements. We evaluate the accuracy of our approach using a large corpus of real web queries in the context of commercial search.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {61–70},
numpages = {10},
keywords = {query classification, web search, vertical search},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718497,
author = {Wetzker, Robert and Zimmermann, Carsten and Bauckhage, Christian and Albayrak, Sahin},
title = {I Tag, You Tag: Translating Tags for Advanced User Models},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718497},
doi = {10.1145/1718487.1718497},
abstract = {Collaborative tagging services (folksonomies) have been among the stars of the Web 2.0 era. They allow their users to label diverse resources with freely chosen keywords (tags). Our studies of two real-world folksonomies unveil that individual users develop highly personalized vocabularies of tags. While these meet individual needs and preferences, the considerable differences between personal tag vocabularies (personomies) impede services such as social search or customized tag recommendation. In this paper, we introduce a novel user-centric tag model that allows us to derive mappings between personal tag vocabularies and the corresponding folksonomies. Using these mappings, we can infer the meaning of user-assigned tags and can predict choices of tags a user may want to assign to new items. Furthermore, our translational approach helps in reducing common problems related to tag ambiguity, synonymous tags, or multilingualism. We evaluate the applicability of our method in tag recommendation and tag-based social search. Extensive experiments show that our translational model improves the prediction accuracy in both scenarios.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {71–80},
numpages = {10},
keywords = {user modeling, social search, folksonomies, tagging, tag recommendation},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718498,
author = {Rendle, Steffen and Schmidt-Thieme, Lars},
title = {Pairwise Interaction Tensor Factorization for Personalized Tag Recommendation},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718498},
doi = {10.1145/1718487.1718498},
abstract = {Tagging plays an important role in many recent websites. Recommender systems can help to suggest a user the tags he might want to use for tagging a specific item. Factorization models based on the Tucker Decomposition (TD) model have been shown to provide high quality tag recommendations outperforming other approaches like PageRank, FolkRank, collaborative filtering, etc. The problem with TD models is the cubic core tensor resulting in a cubic runtime in the factorization dimension for prediction and learning.In this paper, we present the factorization model PITF (Pairwise Interaction Tensor Factorization) which is a special case of the TD model with linear runtime both for learning and prediction. PITF explicitly models the pairwise interactions between users, items and tags. The model is learned with an adaption of the Bayesian personalized ranking (BPR) criterion which originally has been introduced for item recommendation. Empirically, we show on real world datasets that this model outperforms TD largely in runtime and even can achieve better prediction quality. Besides our lab experiments, PITF has also won the ECML/PKDD Discovery Challenge 2009 for graph-based tag recommendation.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {81–90},
numpages = {10},
keywords = {tensor factorization, tag recommendation, recommender systems, personalization},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718499,
author = {Agarwal, Deepak and Chen, Bee-Chung},
title = {FLDA: Matrix Factorization through Latent Dirichlet Allocation},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718499},
doi = {10.1145/1718487.1718499},
abstract = {We propose fLDA, a novel matrix factorization method to predict ratings in recommender system applications where a "bag-of-words" representation for item meta-data is natural. Such scenarios are commonplace in web applications like content recommendation, ad targeting and web search where items are articles, ads and web pages respectively. Because of data sparseness, regularization is key to good predictive accuracy. Our method works by regularizing both user and item factors simultaneously through user features and the bag of words associated with each item. Specifically, each word in an item is associated with a discrete latent factor often referred to as the topic of the word; item topics are obtained by averaging topics across all words in an item. Then, user rating on an item is modeled as user's affinity to the item's topics where user affinity to topics (user factors) and topic assignments to words in items (item factors) are learned jointly in a supervised fashion. To avoid overfitting, user and item factors are regularized through Gaussian linear regression and Latent Dirichlet Allocation (LDA) priors respectively. We show our model is accurate, interpretable and handles both cold-start and warm-start scenarios seamlessly through a single model. The efficacy of our method is illustrated on benchmark datasets and a new dataset from Yahoo! Buzz where fLDA provides superior predictive accuracy in cold-start scenarios and is comparable to state-of-the-art methods in warm-start scenarios. As a by-product, fLDA also identifies interesting topics that explains user-item interactions. Our method also generalizes a recently proposed technique called supervised LDA (sLDA) to collaborative filtering applications. While sLDA estimates item topic vectors in a supervised fashion for a single regression, fLDA incorporates multiple regressions (one for each user) in estimating the item factors.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {91–100},
numpages = {10},
keywords = {recommender systems, latent factor model, graphical model, supervised topic model, collaborative filtering, bayesian hierarchical model},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718501,
author = {Carlson, Andrew and Betteridge, Justin and Wang, Richard C. and Hruschka, Estevam R. and Mitchell, Tom M.},
title = {Coupled Semi-Supervised Learning for Information Extraction},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718501},
doi = {10.1145/1718487.1718501},
abstract = {We consider the problem of semi-supervised learning to extract categories (e.g., academic fields, athletes) and relations (e.g., PlaysSport(athlete, sport)) from web pages, starting with a handful of labeled training examples of each category or relation, plus hundreds of millions of unlabeled web documents. Semi-supervised training using only a few labeled examples is typically unreliable because the learning task is underconstrained. This paper pursues the thesis that much greater accuracy can be achieved by further constraining the learning task, by coupling the semi-supervised training of many extractors for different categories and relations. We characterize several ways in which the training of category and relation extractors can be coupled, and present experimental results demonstrating significantly improved accuracy as a result.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {101–110},
numpages = {10},
keywords = {semi-supervised learning, web mining, bootstrap learning, information extraction},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718502,
author = {Du, Weifu and Tan, Songbo and Cheng, Xueqi and Yun, Xiaochun},
title = {Adapting Information Bottleneck Method for Automatic Construction of Domain-Oriented Sentiment Lexicon},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718502},
doi = {10.1145/1718487.1718502},
abstract = {Domain-oriented sentiment lexicons are widely used for fine-grained sentiment analysis on reviews; therefore, the automatic construction of domain-oriented sentiment lexicon is a fundamental and important task for sentiment analysis research. Most of existing construction approaches take only the kind of relationships between words into account, which makes them have a lot of room for improvement. This paper proposes an adapted information bottleneck method for the construction of domain-oriented sentiment lexicon. This approach can naturally make full use of the mutual reinforcement between documents and words by fusing three kinds of relationships either from words to documents or from words to words; either homogeneous or heterogeneous; either within-domain or cross-domain. The experimental results demonstrate that proposed method could dramatically improve the accuracy of the baseline approach on the construction of out-of-domain sentiment lexicon.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {111–120},
numpages = {10},
keywords = {information retrieval, sentiment analysis, opinion mining},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718503,
author = {Zhou, Mianwei and Cheng, Tao and Chang, Kevin Chen-Chuan},
title = {Data-Oriented Content Query System: Searching for Data into Text on the Web},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718503},
doi = {10.1145/1718487.1718503},
abstract = {As the Web provides rich data embedded in the immense contents inside pages, we witness many ad-hoc efforts for exploiting fine granularity information across Web text, such as Web information extraction, typed-entity search, and question answering. To unify and generalize these efforts, this paper proposes a general search system--Data-oriented Content Query System(DoCQS)--to search directly into document contents for finding relevant values of desired data types. Motivated by the current limitations, we start by distilling the essential capabilities needed by such content querying. The capabilities call for a conceptually relational model, upon which we design a powerful Content Query Language (CQL). For efficient processing, we design novel index structures and query processing algorithms. We evaluate our proposal over two concrete domains of realistic Web corpora, demonstrating that our query language is rather flexible and expressive, and our query processing is efficient with reasonable index overhead.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {121–130},
numpages = {10},
keywords = {contextual index, data oriented, joint index, inverted index, content query language, content query},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718504,
author = {Galland, Alban and Abiteboul, Serge and Marian, Am\'{e}lie and Senellart, Pierre},
title = {Corroborating Information from Disagreeing Views},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718504},
doi = {10.1145/1718487.1718504},
abstract = {We consider a set of views stating possibly conflicting facts. Negative facts in the views may come, e.g., from functional dependencies in the underlying database schema. We want to predict the truth values of the facts. Beyond simple methods such as voting (typically rather accurate), we explore techniques based on "corroboration", i.e., taking into account trust in the views. We introduce three fixpoint algorithms corresponding to different levels of complexity of an underlying probabilistic model. They all estimate both truth values of facts and trust in the views. We present experimental studies on synthetic and real-world data. This analysis illustrates how and in which context these methods improve corroboration results over baseline methods. We believe that corroboration can serve in a wide range of applications such as source selection in the semantic Web, data quality assessment or semantic annotation cleaning in social networks. This work sets the bases for a wide range of techniques for solving these more complex problems.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {131–140},
numpages = {10},
keywords = {contradiction, fix-point, probabilistic model, view, confidence, corroboration},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718506,
author = {Bian, Jiang and Liu, Tie-Yan and Qin, Tao and Zha, Hongyuan},
title = {Ranking with Query-Dependent Loss for Web Search},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718506},
doi = {10.1145/1718487.1718506},
abstract = {Queries describe the users' search intent and therefore they play an essential role in the context of ranking for information retrieval and Web search. However, most of existing approaches for ranking do not explicitly take into consideration the fact that queries vary significantly along several dimensions and entail different treatments regarding the ranking models. In this paper, we propose to incorporate query difference into ranking by introducing query-dependent loss functions. In the context of Web search, query difference is usually represented as different query categories; and, queries are usually classified according to search intent such as navigational, informational and transactional queries. Based on the observation that such kind of query categorization has high correlation with the user's different expectation on the result accuracy on different rank positions, we develop position-sensitive query-dependent loss functions exploring such kind of query categorization. Beyond the simple learning method that builds ranking functions with pre-defined query categorization, we further propose a new method that learns both ranking functions and query categorization simultaneously. We apply the query-dependent loss functions to two particular ranking algorithms, RankNet and ListMLE. Experimental results demonstrate that query-dependent loss functions can be exploited to significantly improve the accuracy of learned ranking functions. We also show that the ranking function jointly learned with query categorization can achieve better performance than that learned with pre-defined query categorization. Finally, we provide analysis and conduct additional experiments to gain deeper understanding on the advantages of ranking with query-dependent loss functions over other query-dependent ranking approaches and query-independent approaches.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {141–150},
numpages = {10},
keywords = {query-dependent loss function, query difference, ranking for web search},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718507,
author = {Moon, Taesup and Smola, Alex and Chang, Yi and Zheng, Zhaohui},
title = {IntervalRank: Isotonic Regression with Listwise and Pairwise Constraints},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718507},
doi = {10.1145/1718487.1718507},
abstract = {Ranking a set of retrieved documents according to their relevance to a given query has become a popular problem at the intersection of web search, machine learning, and information retrieval. Recent work on ranking focused on a number of different paradigms, namely, pointwise, pairwise, and list-wise approaches. Each of those paradigms focuses on a different aspect of the dataset while largely ignoring others. The current paper shows how a combination of them can lead to improved ranking performance and, moreover, how it can be implemented in log-linear time.The basic idea of the algorithm is to use isotonic regression with adaptive bandwidth selection per relevance grade. This results in an implicitly-defined loss function which can be minimized efficiently by a subgradient descent procedure. Experimental results show that the resulting algorithm is competitive on both commercial search engine data and publicly available LETOR data sets.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {151–160},
numpages = {10},
keywords = {isotonic regression, pairwise constraints, learning to rank, listwise constraints},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718508,
author = {Anagnostopoulos, Aris and Becchetti, Luca and Castillo, Carlos and Gionis, Aristides},
title = {An Optimization Framework for Query Recommendation},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718508},
doi = {10.1145/1718487.1718508},
abstract = {Query recommendation is an integral part of modern search engines. The goal of query recommendation is to facilitate users while searching for information. Query recommendation also allows users to explore concepts related to their information needs.In this paper, we present a formal treatment of the problem of query recommendation. In our framework we model the querying behavior of users by a probabilistic reformula- tion graph, or query-flow graph [Boldi et al. CIKM 2008]. A sequence of queries submitted by a user can be seen as a path on this graph. Assigning score values to queries allows us to define suitable utility functions and to consider the expected utility achieved by a reformulation path on the query-flow graph. Providing recommendations can be seen as adding shortcuts in the query-flow graph that "nudge" the reformulation paths of users, in such a way that users are more likely to follow paths with larger expected utility.We discuss in detail the most important questions that arise in the proposed framework. In particular, we provide examples of meaningful utility functions to optimize, we discuss how to estimate the effect of recommendations on the reformulation probabilities, we address the complexity of the optimization problems that we consider, we suggest efficient algorithmic solutions, and we validate our models and algorithms with extensive experimentation. Our techniques can be applied to other scenarios where user behavior can be modeled as a Markov process.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {161–170},
numpages = {10},
keywords = {query suggestions, query reformulations},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718509,
author = {Xu, Jingfang and Chen, Chuanliang and Xu, Gu and Li, Hang and Abib, Elbio Renato Torres},
title = {Improving Quality of Training Data for Learning to Rank Using Click-through Data},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718509},
doi = {10.1145/1718487.1718509},
abstract = {In information retrieval, relevance of documents with respect to queries is usually judged by humans, and used in evaluation and/or learning of ranking functions. Previous work has shown that certain level of noise in relevance judgments has little effect on evaluation, especially for comparison purposes. Recently learning to rank has become one of the major means to create ranking models in which the models are automatically learned from the data derived from a large number of relevance judgments. As far as we know, there was no previous work about quality of training data for learning to rank, and this paper tries to study the issue. Specifically, we address three problems. Firstly, we show that the quality of training data labeled by humans has critical impact on the performance of learning to rank algorithms. Secondly, we propose detecting relevance judgment errors using click-through data accumulated at a search engine. Two discriminative models, referred to as sequential dependency model and full dependency model, are proposed to make the detection. Both models consider the conditional dependency of relevance labels and thus are more powerful than the conditionally independent model previously proposed for other tasks. Finally, we verify that using training data in which the errors are detected and corrected by our method, we can improve the performance of learning to rank algorithms.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {171–180},
numpages = {10},
keywords = {training data quality, judgment error correction, relevance label prediction},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718510,
author = {Dupret, Georges and Liao, Ciya},
title = {A Model to Estimate Intrinsic Document Relevance from the Clickthrough Logs of a Web Search Engine},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718510},
doi = {10.1145/1718487.1718510},
abstract = {We propose a new model to interpret the clickthrough logs of a web search engine. This model is based on explicit assumptions on the user behavior. In particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. This results in a model based on intrinsic relevance, as opposed to perceived relevance. We use the model to predict document relevance and then use this as feature for a "Learning to Rank" machine learning algorithm. Comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. This is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. A deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. This is important because these types of query is considered to be the most difficult to solve.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {181–190},
numpages = {10},
keywords = {clickthrough data, user behavior model, search engines},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718512,
author = {Tyler, Sarah K. and Teevan, Jaime},
title = {Large Scale Query Log Analysis of Re-Finding},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718512},
doi = {10.1145/1718487.1718512},
abstract = {Although Web search engines are targeted towards helping people find new information, people regularly use them to re-find Web pages they have seen before. Researchers have noted the existence of this phenomenon, but relatively little is understood about how re-finding behavior differs from the finding of new information. This paper dives deeply into the differences via analysis of three large-scale data sources: 1) query logs (queries, clicks, result impressions), 2) Web browsing logs (URL visits), and 3) a daily Web crawl (page content). It appears that people learn valuable information about the pages they find that helps them re-find what they are looking for later; compared to the initial finding query, re-finding queries are typically shorter, and rank the re-found URL higher. While many instances of re-finding probably serve as a type of bookmark for a known URL, others seem to represent the resumption of a previous task; results clicked at the end of a session are more likely than those at the beginning to be re-found during a later session, while re-finding is more likely to happen at the beginning of a session than at the end. Additionally, we observe differences in cross-session and intra-session re-finding that may indicate different types of re-finding tasks. Our findings suggest there is a rich opportunity for search engines to take advantage of re-finding behavior as a means to improve the search experience.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {191–200},
numpages = {10},
keywords = {web search, re-finding, query log analysis},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718513,
author = {Goel, Sharad and Broder, Andrei and Gabrilovich, Evgeniy and Pang, Bo},
title = {Anatomy of the Long Tail: Ordinary People with Extraordinary Tastes},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718513},
doi = {10.1145/1718487.1718513},
abstract = {The success of "infinite-inventory" retailers such as Amazon.com and Netflix has been ascribed to a "long tail" phenomenon. To wit, while the majority of their inventory is not in high demand, in aggregate these "worst sellers," unavailable at limited-inventory competitors, generate a significant fraction of total revenue. The long tail phenomenon, however, is in principle consistent with two fundamentally different theories. The first, and more popular hypothesis, is that a majority of consumers consistently follow the crowds and only a minority have any interest in niche content; the second hypothesis is that everyone is a bit eccentric, consuming both popular and specialty products. Based on examining extensive data on user preferences for movies, music, Web search, and Web browsing, we find overwhelming support for the latter theory. However, the observed eccentricity is much less than what is predicted by a fully random model whereby every consumer makes his product choices independently and proportional to product popularity; so consumers do indeed exhibit at least some a priori propensity toward either the popular or the exotic.Our findings thus suggest an additional factor in the success of infinite-inventory retailers, namely, that tail availability may boost head sales by offering consumers the convenience of "one-stop shopping" for both their mainstream and niche interests. This hypothesis is further supported by our theoretical analysis that presents a simple model in which shared inventory stores, such as Amazon Marketplace, gain a clear advantage by satisfying tail demand, helping to explain the emergence and increasing popularity of such retail arrangements. Hence, we believe that the return-on-investment (ROI) of niche products goes beyond direct revenue, extending to second-order gains associated with increased consumer satisfaction and repeat patronage. More generally, our findings call into question the conventional wisdom that specialty products only appeal to a minority of consumers.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {201–210},
numpages = {10},
keywords = {long tail, infinite inventory},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718514,
author = {Wang, Kuansan and Gloy, Nikolas and Li, Xiaolong},
title = {Inferring Search Behaviors Using Partially Observable Markov (POM) Model},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718514},
doi = {10.1145/1718487.1718514},
abstract = {This article describes an application of the partially observable Markov (POM) model to the analysis of a large scale commercial web search log. Mathematically, POM is a variant of the hidden Markov model in which all the hidden state transitions do not necessarily emit observable events. This property of POM is used to model, as the hidden process, a common search behavior that users would read and skip search results, leaving no observable user actions to record in the search logs. The Markov nature of the model further lends support to cope with the facts that a single observed sequence can be probabilistically associated with many hidden sequences that have variable lengths, and the search results can be read in various temporal orders that are not necessarily reflected in the observed sequence of user actions. To tackle the implementation challenges accompanying the flexibility and analytic powers of POM, we introduce segmental Viterbi algorithm based on segmental decoding and Viterbi training to train the POM model parameters and apply them to uncover hidden processes from the search logs. To validate the model, the latent variables modeling the browsing patterns on the search result page are compared with the experimental data of the eye tracking stu-dies. The close agreements suggest that the search logs do contain rich information of user behaviors in browsing the search result page even though they are not directly observable, and that using POM to understand these sophisticated search behaviors is a promising approach.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {211–220},
numpages = {10},
keywords = {web search behaviors, eye tracking, partially observable markov model, segmental viterbi algorithm, search log mining},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718515,
author = {Hassan, Ahmed and Jones, Rosie and Klinkner, Kristina Lisa},
title = {Beyond DCG: User Behavior as a Predictor of a Successful Search},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718515},
doi = {10.1145/1718487.1718515},
abstract = {Web search engines are traditionally evaluated in terms of the relevance of web pages to individual queries. However, relevance of web pages does not tell the complete picture, since an individual query may represent only a piece of the user's information need and users may have different information needs underlying the same queries. In this work, we address the problem of predicting user search goal success by modeling user behavior. We show empirically that user behavior alone can give an accurate picture of the success of the user's web search goals, without considering the relevance of the documents displayed. In fact, our experiments show that models using user behavior are more predictive of goal success than those using document relevance. We build novel sequence models incorporating time distributions for this task and our experiments show that the sequence and time distribution models are more accurate than static models based on user behavior, or predictions based on document relevance.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {221–230},
numpages = {10},
keywords = {user satisfaction, query log analysis, search sessions, search engine evaluation, user behavior models},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718516,
author = {Carterette, Ben and Gabrilovich, Evgeniy and Josifovski, Vanja and Metzler, Donald},
title = {Measuring the Reusability of Test Collections},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718516},
doi = {10.1145/1718487.1718516},
abstract = {While test collection construction is a time-consuming and expensive process, the true cost is amortized by reusing the collection over hundreds or thousands of experiments. Some of these experiments may involve systems that retrieve documents not judged during the initial construction phase, and some of these systems may be "hard" to evaluate: depending on which judgments are missing and which judged documents were retrieved, the experimenter's confidence in an evaluation could potentially be very low. We propose two methods for quantifying the reusability of a test collection for evaluating new systems. The proposed methods provide simple yet highly effective tests for determining whether an existing set of judgments is useful for evaluating a new system. Empirical evaluations using TREC datasets confirm the usefulness of our proposed reusability measures. In particular, we show that our methods can reliably estimate confidence intervals that are indicative of collection reusability.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {231–240},
numpages = {10},
keywords = {test collections, evaluation, reusability},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718518,
author = {Goyal, Amit and Bonchi, Francesco and Lakshmanan, Laks V.S.},
title = {Learning Influence Probabilities in Social Networks},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718518},
doi = {10.1145/1718487.1718518},
abstract = {Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {241–250},
numpages = {10},
keywords = {influence, viral marketing, social networks},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718519,
author = {Mislove, Alan and Viswanath, Bimal and Gummadi, Krishna P. and Druschel, Peter},
title = {You Are Who You Know: Inferring User Profiles in Online Social Networks},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718519},
doi = {10.1145/1718487.1718519},
abstract = {Online social networks are now a popular way for users to connect, express themselves, and share content. Users in today's online social networks often post a profile, consisting of attributes like geographic location, interests, and schools attended. Such profile information is used on the sites as a basis for grouping users, for sharing content, and for suggesting users who may benefit from interaction. However, in practice, not all users provide these attributes.In this paper, we ask the question: given attributes for some fraction of the users in an online social network, can we infer the attributes of the remaining users? In other words, can the attributes of users, in combination with the social network graph, be used to predict the attributes of another user in the network? To answer this question, we gather fine-grained data from two social networks and try to infer user profile attributes. We find that users with common attributes are more likely to be friends and often form dense communities, and we propose a method of inferring user attributes that is inspired by previous approaches to detecting communities in social networks. Our results show that certain user attributes can be inferred with high accuracy when given information on as little as 20% of the users.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {251–260},
numpages = {10},
keywords = {inferring attributes, communities, social networks},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718520,
author = {Weng, Jianshu and Lim, Ee-Peng and Jiang, Jing and He, Qi},
title = {TwitterRank: Finding Topic-Sensitive Influential Twitterers},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718520},
doi = {10.1145/1718487.1718520},
abstract = {This paper focuses on the problem of identifying influential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called "following", in which each user can choose who she wants to "follow" to receive tweets from without requiring the latter to give permission first. In a dataset prepared for this study, it is observed that (1) 72.4% of the users in Twitter follow more than 80% of their followers, and (2) 80.5% of the users have 80% of users they are following follow them back. Our study reveals that the presence of "reciprocity" can be explained by phenomenon of homophily. Based on this finding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the influence of users in Twitter. TwitterRank measures the influence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {261–270},
numpages = {10},
keywords = {influential, pagerank, twitter},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718521,
author = {Schifanella, Rossano and Barrat, Alain and Cattuto, Ciro and Markines, Benjamin and Menczer, Filippo},
title = {Folks in Folksonomies: Social Link Prediction from Shared Metadata},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718521},
doi = {10.1145/1718487.1718521},
abstract = {Web 2.0 applications have attracted a considerable amount of attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and semantic components of social media has been only partially explored. Here we focus on Flickr and Last.fm, two social media systems in which we can relate the tagging activity of the users with an explicit representation of their social network. We show that a substantial level of local lexical and topical alignment is observable among users who lie close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local alignment between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar topical interests are more likely to be friends, and therefore semantic similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on the Last.fm data set, confirming that the social network constructed from semantic similarity captures actual friendship more accurately than Last.fm's suggestions based on listening patterns.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {271–280},
numpages = {10},
keywords = {folksonomies, lexical and topical alignment, social semantic similarity, maximum information path, social media, web 2.0, social network, collaborative tagging, link prediction},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718522,
author = {Sizov, Sergej},
title = {GeoFolk: Latent Spatial Semantics in Web 2.0 Social Media},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718522},
doi = {10.1145/1718487.1718522},
abstract = {We describe an approach for multi-modal characterization of social media by combining text features (e.g. tags as a prominent example of short, unstructured text labels) with spatial knowledge (e.g. geotags and coordinates of images and videos). Our model-based framework GeoFolk combines these two aspects in order to construct better algorithms for content management, retrieval, and sharing. The approach is based on multi-modal Bayesian models which allow us to integrate spatial semantics of social media in a well-formed, probabilistic manner. We systematically evaluate the solution on a subset of Flickr data, in characteristic scenarios of tag recommendation, content classification, and clustering. Experimental results show that our method outperforms baseline techniques that are based on one of the aspects alone. The approach described in this contribution can also be used in other domains such as Geoweb retrieval.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {281–290},
numpages = {10},
keywords = {geodata, tagging, mcmc, bayesian learning, web2.0},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718524,
author = {Becker, Hila and Naaman, Mor and Gravano, Luis},
title = {Learning Similarity Metrics for Event Identification in Social Media},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718524},
doi = {10.1145/1718487.1718524},
abstract = {Social media sites (e.g., Flickr, YouTube, and Facebook) are a popular distribution outlet for users looking to share their experiences and interests on the Web. These sites host substantial amounts of user-contributed materials (e.g., photographs, videos, and textual content) for a wide variety of real-world events of different type and scale. By automatically identifying these events and their associated user-contributed social media documents, which is the focus of this paper, we can enable event browsing and search in state-of-the-art search engines. To address this problem, we exploit the rich "context" associated with social media content, including user-provided annotations (e.g., title, tags) and automatically generated information (e.g., content creation time). Using this rich context, which includes both textual and non-textual features, we can define appropriate document similarity metrics to enable online clustering of media to events. As a key contribution of this paper, we explore a variety of techniques for learning multi-feature similarity metrics for social media documents in a principled manner. We evaluate our techniques on large-scale, real-world datasets of event images from Flickr. Our evaluation results suggest that our approach identifies events, and their associated social media documents, more effectively than the state-of-the-art strategies on which we build.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {291–300},
numpages = {10},
keywords = {event identification, similarity metric learning, social media},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718525,
author = {Mathioudakis, Michael and Koudas, Nick and Marbach, Peter},
title = {Early Online Identification of Attention Gathering Items in Social Media},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718525},
doi = {10.1145/1718487.1718525},
abstract = {Activity in social media such as blogs, micro-blogs, social networks, etc is manifested via interaction that involves text, images, links and other information items. Naturally, some items attract more attention than others, expressed with large volumes of linking, commenting or tagging activity, to name a few examples. Moreover, high attention can be indicative of emerging events, breaking news or generally indicate information items of interest to a vast set of people. The numbers associated with digital social activity are astonishing: in excess of millions of blog posts, tweets and forums updates per day, millions of tags in photos, news articles or blogs. Being able to identify information items that gather much attention in such a real time information collective is a challenging task.In this paper, we consider the problem of early online identification of items that gather a lot of attention in social media. We model social media activity using ISIS, a stochastic model for Interacting Streaming Information Sources, that intuitively captures the concept of attention gathering information items. Given the challenge of the information overload characterizing digital social activity, we present sequential statistical tests that enable early identification of attention gathering items. This effectively reduces the set of items one has to monitor in real time in order to identify pieces of information attracting a lot of attention.Experiments on real data demonstrate the utility of our model, as well as the efficiency and effectiveness of the proposed sequential statistical tests.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {301–310},
numpages = {10},
keywords = {user activity modeling and exploitation, social media analysis},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718526,
author = {Kumar, Ravi and Lifshits, Yury and Tomkins, Andrew},
title = {Evolution of Two-Sided Markets},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718526},
doi = {10.1145/1718487.1718526},
abstract = {Two-sided markets arise when two different types of users may realize gains by interacting with one another through one or more platforms or mediators. We initiate a study of the evolution of such markets. We present an empirical analysis of the value accruing to members of each side of the market, based on the presence of the other side. We codify the range of value curves into a general theoretical model, characterize the equilibrium states of two-sided markets in our model, and prove that each platform will converge to one of these equilibria. We give some early experimental results of the stability of two-sided markets, and close with a theoretical treatment of the formation of different kinds of coalitions in such markets.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {311–320},
numpages = {10},
keywords = {equilibrium, two-sided markets, coalitions, preferential attachment},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718528,
author = {Zhu, Zeyuan Allen and Chen, Weizhu and Minka, Tom and Zhu, Chenguang and Chen, Zheng},
title = {A Novel Click Model and Its Applications to Online Advertising},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718528},
doi = {10.1145/1718487.1718528},
abstract = {Recent advances in click model have positioned it as an attractive method for representing user preferences in web search and online advertising. Yet, most of the existing works focus on training the click model for individual queries, and cannot accurately model the tail queries due to the lack of training data. Simultaneously, most of the existing works consider the query, url and position, neglecting some other important attributes in click log data, such as the local time. Obviously, the click through rate is different between daytime and midnight. In this paper, we propose a novel click model based on Bayesian network, which is capable of modeling the tail queries because it builds the click model on attribute values, with those values being shared across queries. We called our work General Click Model (GCM) as we found that most of the existing works can be special cases of GCM by assigning different parameters. Experimental results on a large-scale commercial advertisement dataset show that GCM can significantly and consistently lead to better results as compared to the state-of-the-art works.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {321–330},
numpages = {10},
keywords = {bayesian, search engine, attribute, advertisement, gaussian},
location = {New York, New York, USA},
series = {WSDM '10}
}

@dataset{10.1145/review-1718487.1718528_R45731,
author = {Rathinasamy, Bhavanandan L.},
title = {Review ID:R45731 for DOI: 10.1145/1718487.1718528},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1718487.1718528_R45731}
}

@inproceedings{10.1145/1718487.1718529,
author = {Byers, John W. and Mitzenmacher, Michael and Zervas, Georgios},
title = {Adaptive Weighing Designs for Keyword Value Computation},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718529},
doi = {10.1145/1718487.1718529},
abstract = {Attributing a dollar value to a keyword is an essential part of running any profitable search engine advertising campaign. When an advertiser has complete control over the interaction with and monetization of each user arriving on a given keyword, the value of that term can be accurately tracked. However, in many instances, the advertiser may monetize arrivals indirectly through one or more third parties. In such cases, it is typical for the third party to provide only coarse-grained reporting: rather than report each monetization event, users are aggregated into larger channels and the third party reports aggregate information such as total daily revenue for each channel. Examples of third parties that use channels include Amazon and Google AdSense.In such scenarios, the number of channels is generally much smaller than the number of keywords whose value per click (VPC) we wish to learn. However, the advertiser has flexibility as to how to assign keywords to channels over time. We introduce the channelization problem: how do we adaptively assign keywords to channels over the course of multiple days to quickly obtain accurate VPC estimates of all keywords? We relate this problem to classical results in weighing design, devise new adaptive algorithms for this problem, and quantify the performance of these algorithms experimentally. Our results demonstrate that adaptive weighing designs that exploit statistics of term frequency, variability in VPCs across keywords, and flexible channel assignments over time provide the best estimators of keyword VPCs.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {331–340},
numpages = {10},
keywords = {least squares, design of experiments, weighing designs, regression},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718530,
author = {Ravi, Sujith and Broder, Andrei and Gabrilovich, Evgeniy and Josifovski, Vanja and Pandey, Sandeep and Pang, Bo},
title = {Automatic Generation of Bid Phrases for Online Advertising},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718530},
doi = {10.1145/1718487.1718530},
abstract = {One of the most prevalent online advertising methods is textual advertising. To produce a textual ad, an advertiser must craft a short creative (the text of the ad) linking to a landing page, which describes the product or service being promoted. Furthermore, the advertiser must associate the creative to a set of manually chosen bid phrases representing those Web search queries that should trigger the ad. For efficiency, given a landing page, the bid phrases are often chosen first, and then for each bid phrase the creative is produced using a template. Nevertheless, an ad campaign (e.g., for a large retailer) might involve thousands of landing pages and tens or hundreds of thousands of bid phrases, hence the entire process is very laborious.Our study aims towards the automatic construction of online ad campaigns: given a landing page, we propose several algorithmic methods to generate bid phrases suitable for the given input. Such phrases must be both relevant (that is, reflect the content of the page) and well-formed (that is, likely to be used as queries to a Web search engine). To this end, we use a two phase approach. First, candidate bid phrases are generated by a number of methods, including a (mono-lingual) translation model capable of generating phrases contained within the text of the input as well as previously "unseen" phrases. Second, the candidates are ranked in a probabilistic framework using both the translation model, which favors relevant phrases, as well as a bid phrase language model, which favors well-formed phrases.Empirical evaluation based on a real-life corpus of advertiser-created landing pages and associated bid phrases confirms the value of our approach, which successfully re-generates many of the human-crafted bid phrases and performs significantly better than a pure text extraction method.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {341–350},
numpages = {10},
keywords = {content match, sponsored search},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718531,
author = {Cheng, Haibin and Cant\'{u}-Paz, Erick},
title = {Personalized Click Prediction in Sponsored Search},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718531},
doi = {10.1145/1718487.1718531},
abstract = {Sponsored search is a multi-billion dollar business that generates most of the revenue for search engines. Predicting the probability that users click on ads is crucial to sponsored search because the prediction is used to influence ranking, filtering, placement, and pricing of ads. Ad ranking, filtering and placement have a direct impact on the user experience, as users expect the most useful ads to rank high and be placed in a prominent position on the page. Pricing impacts the advertisers' return on their investment and revenue for the search engine. The objective of this paper is to present a framework for the personalization of click models in sponsored search. We develop user-specific and demographic-based features that reflect the click behavior of individuals and groups. The features are based on observations of search and click behaviors of a large number of users of a commercial search engine. We add these features to a baseline non-personalized click model and perform experiments on offline test sets derived from user logs as well as on live traffic. Our results demonstrate that the personalized models significantly improve the accuracy of click prediction.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {351–360},
numpages = {10},
keywords = {maximum entropy modeling, sponsored search, personalization, click feedback, click prediction, user profile, demographic},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718532,
author = {Hillard, Dustin and Schroedl, Stefan and Manavoglu, Eren and Raghavan, Hema and Leggetter, Chirs},
title = {Improving Ad Relevance in Sponsored Search},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718532},
doi = {10.1145/1718487.1718532},
abstract = {We describe a machine learning approach for predicting sponsored search ad relevance. Our baseline model incorporates basic features of text overlap and we then extend the model to learn from past user clicks on advertisements. We present a novel approach using translation models to learn user click propensity from sparse click logs.Our relevance predictions are then applied to multiple sponsored search applications in both offline editorial evaluations and live online user tests. The predicted relevance score is used to improve the quality of the search page in three areas: filtering low quality ads, more accurate ranking for ads, and optimized page placement of ads to reduce prominent placement of low relevance ads. We show significant gains across all three tasks.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {361–370},
numpages = {10},
keywords = {clicks, advertising, relevance modeling, translation},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718534,
author = {Zhang, Fan and Shi, Shuming and Yan, Hao and Wen, Ji-Rong},
title = {Revisiting Globally Sorted Indexes for Efficient Document Retrieval},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718534},
doi = {10.1145/1718487.1718534},
abstract = {There has been a large amount of research on efficient document retrieval in both IR and web search areas. One important technique to improve retrieval efficiency is early termination, which speeds up query processing by avoiding scanning the entire inverted lists. Most early termination techniques first build new inverted indexes by sorting the inverted lists in the order of either the term-dependent information, e.g., term frequencies or term IR scores, or the term-independent information, e.g., static rank of the document; and then apply appropriate retrieval strategies on the resulting indexes. Although the methods based only on the static rank have been shown to be ineffective for the early termination, there are still many advantages of using the methods based on term-independent information. In this paper, we propose new techniques to organize inverted indexes based on the term-independent information beyond static rank and study the new retrieval strategies on the resulting indexes. We perform a detailed experimental evaluation on our new techniques and compare them with the existing approaches. Our results on the TREC GOV and GOV2 data sets show that our techniques can improve query efficiency significantly.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {371–380},
numpages = {10},
keywords = {top-k, dynamic index pruning, globally-sorted index},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718535,
author = {Koppula, Hema Swetha and Leela, Krishna P. and Agarwal, Amit and Chitrapura, Krishna Prasad and Garg, Sachin and Sasturkar, Amit},
title = {Learning URL Patterns for Webpage De-Duplication},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718535},
doi = {10.1145/1718487.1718535},
abstract = {Presence of duplicate documents in the World Wide Web adversely affects crawling, indexing and relevance, which are the core building blocks of web search. In this paper, we present a set of techniques to mine rules from URLs and utilize these rules for de-duplication using just URL strings without fetching the content explicitly. Our technique is composed of mining the crawl logs and utilizing clusters of similar pages to extract transformation rules, which are used to normalize URLs belonging to each cluster. Preserving each mined rule for de-duplication is not efficient due to the large number of such rules. We present a machine learning technique to generalize the set of rules, which reduces the resource footprint to be usable at web-scale. The rule extraction techniques are robust against web-site specific URL conventions. We compare the precision and scalability of our approach with recent efforts in using URLs for de-duplication. Experimental results demonstrate that our approach achieves 2 times more reduction in duplicates with only half the rules compared to the most recent previous approach. Scalability of the framework is demonstrated by performing a large scale evaluation on a set of 3 Billion URLs, implemented using the MapReduce framework.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {381–390},
numpages = {10},
keywords = {webpage de-duplication, decision trees, generalization, mapreduce, page importance, search engines, site-specific delimiters},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718536,
author = {Ferragina, Paolo and Manzini, Giovanni},
title = {On Compressing the Textual Web},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718536},
doi = {10.1145/1718487.1718536},
abstract = {Nowadays we know how to effectively compress most basic components of any modern search engine, such as, the graphs arising from the Web structure and/or its usage, the posting lists, and the dictionary of terms. But we are not aware of any study which has deeply addressed the issue of compressing the raw Web pages. Many Web applications use simple compression algorithms--- e.g. gzip, or word-based Move-to-Front or Huffman coders-and conclude that, even compressed, raw data take more space than Inverted Lists.In this paper we investigate two typical scenarios of use of data compression for large Web collections. In the first scenario, the compressed pages are stored on disk and we only need to support the fast scanning of large parts of the compressed collection (such as for map-reduce paradigms). In the second scenario, we consider the fast access to individual pages of the compressed collection that is distributed among the RAMs of many PCs (such as for search engines and miners). For the first scenario, we provide a thorough experimental comparison among state-of-the-art compressors thus indicating pros and cons of the available solutions. For the second scenario, we compare known compressed-storage solutions with the new algorithmic technology of compressed self-indexes [NM07].Our results show that Web pages are more compressible than expected and, consequently, that some common beliefs in this area should be reconsidered. Our results are novel for the large spectrum of tested approaches and the size of datasets, and provide a threefold contribution: a non-trivial baseline for designing new compressed-storage solutions, a guide for software developers faced with Web-page storage, and a natural complement to the recent figures on InvertedList-compression achieved by [Yan et al, sigir 09 and www 09].},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {391–400},
numpages = {10},
keywords = {compressed (self-)indexes, burrows-wheeler transform, text compression, lossless data compression},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718537,
author = {Das Sarma, Atish and Gollapudi, Sreenivas and Najork, Marc and Panigrahy, Rina},
title = {A Sketch-Based Distance Oracle for Web-Scale Graphs},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718537},
doi = {10.1145/1718487.1718537},
abstract = {We study the fundamental problem of computing distances between nodes in large graphs such as the web graph and social networks. Our objective is to be able to answer distance queries between pairs of nodes in real time. Since the standard shortest path algorithms are expensive, our approach moves the time-consuming shortest-path computation offline, and at query time only looks up precomputed values and performs simple and fast computations on these precomputed values. More specifically, during the offline phase we compute and store a small "sketch" for each node in the graph, and at query-time we look up the sketches of the source and destination nodes and perform a simple computation using these two sketches to estimate the distance.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {401–410},
numpages = {10},
keywords = {embedding, algorithms, shortest path, sketching, distance computation},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718538,
author = {Cambazoglu, B. Barla and Zaragoza, Hugo and Chapelle, Olivier and Chen, Jiang and Liao, Ciya and Zheng, Zhaohui and Degenhardt, Jon},
title = {Early Exit Optimizations for Additive Machine Learned Ranking Systems},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718538},
doi = {10.1145/1718487.1718538},
abstract = {Some commercial web search engines rely on sophisticated machine learning systems for ranking web documents. Due to very large collection sizes and tight constraints on query response times, online efficiency of these learning systems forms a bottleneck. An important problem in such systems is to speedup the ranking process without sacrificing much from the quality of results. In this paper, we propose optimization strategies that allow short-circuiting score computations in additive learning systems. The strategies are evaluated over a state-of-the-art machine learning system and a large, real-life query log, obtained from Yahoo!. By the proposed strategies, we are able to speedup the score computations by more than four times with almost no loss in result quality.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {411–420},
numpages = {10},
keywords = {machine learning, early exit, web search, optimization},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718540,
author = {Yu, Fang and Xie, Yinglian and Ke, Qifa},
title = {SBotMiner: Large Scale Search Bot Detection},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718540},
doi = {10.1145/1718487.1718540},
abstract = {In this paper, we study search bot traffic from search engine query logs at a large scale. Although bots that generate search traffic aggressively can be easily detected, a large number of distributed, low rate search bots are difficult to identify and are often associated with malicious attacks. We present SBotMiner, a system for automatically identifying stealthy, low-rate search bot traffic from query logs. Instead of detecting individual bots, our approach captures groups of distributed, coordinated search bots. Using sampled data from two different months, SBotMiner identifies over 123 million bot-related pageviews, accounting for 3.8% of total traffic. Our in-depth analysis shows that a large fraction of the identified bot traffic may be associated with various malicious activities such as phishing attacks or vulnerability exploits. This finding suggests that detecting search bot traffic holds great promise to detect and stop attacks early on.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {421–430},
numpages = {10},
keywords = {web search, search log analysis, botnet detection, click fraud, search bot},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718541,
author = {Taneva, Bilyana and Kacimi, Mouna and Weikum, Gerhard},
title = {Gathering and Ranking Photos of Named Entities with High Precision, High Recall, and Diversity},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718541},
doi = {10.1145/1718487.1718541},
abstract = {Knowledge-sharing communities like Wikipedia and automated extraction methods like those of DBpedia enable the construction of large machine-processible knowledge bases with relational facts about entities. These endeavors lack multimodal data like photos and videos of people and places. While photos of famous entities are abundant on the Internet, they are much harder to retrieve for less popular entities such as notable computer scientists or regionally interesting churches. Querying the entity names in image search engines yields large candidate lists, but they often have low precision and unsatisfactory recall.Our goal is to populate a knowledge base with photos of named entities, with high precision, high recall, and diversity of photos for a given entity. We harness relational facts about entities for generating expanded queries to retrieve different candidate lists from image search engines. We use a weighted voting method to determine better rankings of an entity's photos. Appropriate weights are dependent on the type of entity (e.g., scientist vs. politician) and automatically computed from a small set of training entities. We also exploit visual similarity measures based on SIFT features, for higher diversity in the final rankings. Our experiments with photos of persons and landmarks show significant improvements of ranking measures like MAP and NDCG, and also for diversity-aware ranking.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {431–440},
numpages = {10},
keywords = {query expansion, knowledge base, ranking, image gathering},
location = {New York, New York, USA},
series = {WSDM '10}
}

@inproceedings{10.1145/1718487.1718542,
author = {Kohlsch\"{u}tter, Christian and Fankhauser, Peter and Nejdl, Wolfgang},
title = {Boilerplate Detection Using Shallow Text Features},
year = {2010},
isbn = {9781605588896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718487.1718542},
doi = {10.1145/1718487.1718542},
abstract = {In addition to the actual content Web pages consist of navigational elements, templates, and advertisements. This boilerplate text typically is not related to the main content, may deteriorate search precision and thus needs to be detected properly. In this paper, we analyze a small set of shallow text features for classifying the individual text elements in a Web page. We compare the approach to complex, state-of-the-art techniques and show that competitive accuracy can be achieved, at almost no cost. Moreover, we derive a simple and plausible stochastic model for describing the boilerplate creation process. With the help of our model, we also quantify the impact of boilerplate removal to retrieval performance and show significant improvements over the baseline. Finally, we extend the principled approach by straight-forward heuristics, achieving a remarkable detection accuracy.},
booktitle = {Proceedings of the Third ACM International Conference on Web Search and Data Mining},
pages = {441–450},
numpages = {10},
keywords = {boilerplate removal, template detection, web document modeling, text cleaning, full-text extraction},
location = {New York, New York, USA},
series = {WSDM '10}
}

