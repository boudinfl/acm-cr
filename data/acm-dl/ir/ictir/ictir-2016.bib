@inproceedings{10.1145/2970398.2970440,
author = {Azzopardi, Leif and Zuccon, Guido},
title = {Advances in Formal Models of Search and Search Behaviour},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970440},
doi = {10.1145/2970398.2970440},
abstract = {Searching is performed in the context of a task and as such the value of the information found is with respect to the task. Recently, there has been a drive to developing formal models of information seeking and retrieval that consider the costs and benefits arising through the interaction with the interface/system and the information surfaced during that interaction. In this full day tutorial we will focus on describing and explaining some of the more recent and latest formal models of Information Seeking and Retrieval. The tutorial is structured into two parts. In the first part we will present a series of models that have been developed based on: (i) economic theory, (ii) decision theory (iii) game theory and (iv) optimal foraging theory. The second part of the day will be dedicated to building models where we will discuss different techniques to build and develop models from which we can draw testable hypotheses from. During the tutorial participants will be challenged to develop various formals models, applying the techniques learnt during the day. We will then conclude with presentations on solutions followed by a summary and overview of challenges and future directions. This tutorial is aimed at participants wanting to know more about the various formal models of information seeking, search and retrieval, that have been proposed. The tutorial will be presented at an intermediate level, and is designed to support participants who want to be able to understand and build such models.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {1–4},
numpages = {4},
keywords = {user models, retrieval strategies, evaluation, search behaviour},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970441,
author = {Dietz, Laura and Kotov, Alexander and Meij, Edgar},
title = {Utilizing Knowledge Bases in Text-Centric Information Retrieval},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970441},
doi = {10.1145/2970398.2970441},
abstract = {General-purpose knowledge bases are increasingly growing in terms of depth (content) and width (coverage). Moreover, algorithms for entity linking and entity retrieval have improved tremendously in the past years. These developments give rise to a new line of research that exploits and combines these developments for the purposes of text-centric information retrieval applications. This tutorial focuses on a) how to retrieve a set of entities for an ad-hoc query, or more broadly, assessing relevance of KB elements for the information need, b) how to annotate text with such elements, and c) how to use this information to assess the relevance of text. We discuss different kinds of information available in a knowledge graph and how to leverage each most effectively.We start the tutorial with a brief overview of different types of knowledge bases, their structure and information contained in popular general-purpose and domain-specific knowledge bases. In particular, we focus on the representation of entity-centric information in the knowledge base through names, terms, relations, and type taxonomies. Next, we will provide a recap on ad-hoc object retrieval from knowledge graphs as well as entity linking and retrieval. This is essential technology, which the remainder of the tutorial builds on. Next we will cover essential components within successful entity linking systems, including the collection of entity name information and techniques for disambiguation with contextual entity mentions. We will present the details of four previously proposed systems that successfully leverage knowledge bases to improve ad-hoc document retrieval. These systems combine the notion of entity retrieval and semantic search on one hand, with text retrieval models and entity linking on the other. Finally, we also touch on entity aspects and links in the knowledge graph as it can help to understand the entities' context.This tutorial is the first to compile, summarize, and disseminate progress in this emerging area and we provide both an overview of state-of-the-art methods and outline open research problems to encourage new contributions.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {5},
numpages = {1},
keywords = {semantic search, ad hoc text retrieval, knowledge base, entity},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970442,
author = {Tamine, Lynda and Soulier, Laure},
title = {Collaborative Information Retrieval: Frameworks, Theoretical Models, and Emerging Topics},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970442},
doi = {10.1145/2970398.2970442},
abstract = {A great amount of research in the IR domain mostly dealt with both the design of enhanced document ranking models allowing search improvement through user-to-system collaboration. However, in addition to user-to-system form of collaboration, user-to-user collaboration is increasingly acknowledged as an effective mean for gathering the complementary skills and/or knowledge of individual users in order to solve complex search tasks. This tutorial will first give an overview of the ways into collaboration has been implemented in IR models with the attempt of improving the search outcomes with respect to several tasks and related frameworks (ad-hoc search, group-based recommendation, social search, collaborative search). Second, as envisioned in collaborative IR domain (CIR), we will focus on the theoretical models that support and drive user-to-user collaboration in order to perform shared IR tasks. Third, we will develop a road map on emerging and relevant topics addressing issues related to collaboration design. Our goal is to provide participants with concepts and motivation allowing them to investigate this emerging IR domain as well as giving them some clues on how to tackle issues related to the optimization of collaborative tasks. More specifically, the tutorial aims to: (a) Give an overview of the key concept of collaboration in IR and related research topics; (b) Present state-of-the art CIR techniques and models; (c) Discuss about the emerging topics that deal with collaboration; (d) Point out some challenges ahead.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {7–8},
numpages = {2},
keywords = {user behavior analysis, collaborative information retrieval},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970443,
author = {Sakai, Tetsuya},
title = {Topic Set Size Design and Power Analysis in Practice},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970443},
doi = {10.1145/2970398.2970443},
abstract = {Topic set size design methods provide principles and procedures for test collection builders to decide on the number of topics to create. These methods can then help us keep improving the test collection design based on accumulated data. Simple Excel tools are available for such purposes. Post-hoc power analysis tools, available as simple R scripts, can help IR researchers examine the achieved power of a reported experiment and determine future sample sizes for ensuring high power. Thus, for example, underpowered user experiments can be detected, and a larger sample size can be proposed. If used appropriately, these Excel and R tools should be able to provide the IR community with better experimentation practices. The main objective of this tutorial is to let IR researchers familiarise themselves with these tools and understand the basic ideas behind them.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {9–10},
numpages = {2},
keywords = {test collections, variances, statistical power, statistical significance, effect sizes, experimental design},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970413,
author = {Lioma, Christina and Tarissan, Fabien and Simonsen, Jakob Grue and Petersen, Casper and Larsen, Birger},
title = {Exploiting the Bipartite Structure of Entity Grids for Document Coherence and Retrieval},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970413},
doi = {10.1145/2970398.2970413},
abstract = {Document coherence describes how much sense text makes in terms of its logical organisation and discourse flow. Even though coherence is a relatively difficult notion to quantify precisely, it can be approximated automatically. This type of coherence modelling is not only interesting in itself, but also useful for a number of other text processing tasks, including Information Retrieval (IR), where adjusting the ranking of documents according to both their relevance and their coherence has been shown to increase retrieval effectiveness [37].The state of the art in unsupervised coherence modelling represents documents as bipartite graphs of sentences and discourse entities, and then projects these bipartite graphs into one--mode undirected graphs. However, one--mode projections may incur significant loss of the information present in the original bipartite structure. To address this we present three novel graph metrics that compute document coherence on the original bipartite graph of sentences and entities. Evaluation on standard settings shows that: (i) one of our coherence metrics beats the state of the art in terms of coherence accuracy; and (ii) all three of our coherence metrics improve retrieval effectiveness because, as closer analysis reveals, they capture aspects of document quality that go undetected by both keyword-based standard ranking and by spam filtering. This work contributes document coherence metrics that are theoretically principled, parameter-free, and useful to IR.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {11–20},
numpages = {10},
keywords = {document coherence, bipartite graphs, discourse, information retrieval},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970404,
author = {Lu, Xiaolu and Moffat, Alistair and Culpepper, J. Shane},
title = {Efficient and Effective Higher Order Proximity Modeling},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970404},
doi = {10.1145/2970398.2970404},
abstract = {Bag-of-words retrieval models are widely used, and provide a robust trade-off between efficiency and effectiveness. These models often make simplifying assumptions about relations between query terms, and treat term statistics independently. However, query terms are rarely independent, and previous work has repeatedly shown that term dependencies can be critical to improving the effectiveness of ranked retrieval results. Among all term-dependency models, the Markov Random Field (MRF) [Metzler and Croft, SIGIR, 2005] model has received the most attention in recent years. Despite clear effectiveness improvements, these models are not deployed in performance-critical applications because of the potentially high computational costs. As a result, bigram models are generally considered to be the best compromise between full term dependence, and term-independent models such as BM25. Here we provide further evidence that term-dependency features not captured by bag-of-words models can reliably improve retrieval effectiveness. We also present a new variation on the highly-effective MRF model that relies on a BM25-derived potential. The benefit of this approach is that it is built from feature functions which require no higher-order global statistics. We empirically show that our new model reduces retrieval costs by up to 60%, with no loss in effectiveness compared to previous approaches.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {21–30},
numpages = {10},
keywords = {proximity, experimentation, measurement},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970419,
author = {Bah Rabiou, Ashraf and Carterette, Ben},
title = {PDF: A Probabilistic Data Fusion Framework for Retrieval and Ranking},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970419},
doi = {10.1145/2970398.2970419},
abstract = {Data fusion has been shown to be a simple and effective way to improve retrieval results. Most existing data fusion methods combine ranked lists from different retrieval functions for a single given query. But in many real search settings, the diversity of retrieval functions required to achieve good fusion performance is not available. Researchers are typically limited to a few variants on a scoring function used by the engine of their choice, with these variants often producing similar results due to being based on the same underlying term statistics.This paper presents a framework for data fusion based on combining ranked lists from different queries that users could have entered for their information need. If we can identify a set of "possible queries" for an information need, and estimate probability distributions concerning the probability of generating those queries, the probability of retrieving certain documents for those queries, and the probability of documents being relevant to that information need, we have the potential to dramatically improve results over a baseline system given a single user query. Our framework is based on several component models that can be mixed and matched. We present several simple estimation methods for components. In order to demonstrate effectiveness, we present experimental results on 5 different datasets covering tasks such as ad-hoc search, novelty and diversity search, and search in the presence of implicit user feedback. Our results show strong performances for our method; it is competitive with state-of-the-art methods on the same datasets, and in some cases outperforms them.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {31–39},
numpages = {9},
keywords = {probabilistic data fusion, diversified ranking, search over sessions, retrieval models},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970435,
author = {Diaz, Fernando},
title = {Learning to Rank with Labeled Features},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970435},
doi = {10.1145/2970398.2970435},
abstract = {Classic learning to rank algorithms are trained using a set of labeled documents, pairs of documents, or rankings of documents. Unfortunately, in many situations, gathering such labels requires significant overhead in terms of time and money. We present an algorithm for training a learning to rank model using a set of labeled features elicited from system designers or domain experts. Labeled features incorporate a system designer's belief about the correlation between certain features and relative relevance. We demonstrate the efficacy of our model on a public learning to rank dataset. Our results show that we outperform our baselines even when using as little as a single feature label.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {41–44},
numpages = {4},
keywords = {learning to rank},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970430,
author = {Clarke, Charles L.A. and Cormack, Gordon V. and Lin, Jimmy and Roegiest, Adam},
title = {Total Recall: Blue Sky on Mars},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970430},
doi = {10.1145/2970398.2970430},
abstract = {There are presently plans to create permanent colonies on Mars so that humanity will have a second home. These colonists will need search, email, entertainment, and indeed most services provided on the modern web. The primary challenge is network latencies, since the two planets are anywhere from 4 to 24 light minutes apart. A recent article sketches out how we might develop search technologies for Mars based on physically transporting a cache of the web to Mars, to which updates are applied via predictive models. Within this general framework, we explore the problem of high-recall retrieval, such as conducting a scientific survey. We explore simple techniques for masking speed-of-light delays and find that "priming" the search process with a small Martian cache is sufficient to mask a moderate amount of network latency. Simulation experiments show that it is possible to engineer high-recall search from Mars to be quite similar to the experience on Earth.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {45–48},
numpages = {4},
keywords = {high-recall retrieval, searching from mars, caching},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970400,
author = {Jiang, Jyun-Yu and Cheng, Pu-Jen},
title = {Classifying User Search Intents for Query Auto-Completion},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970400},
doi = {10.1145/2970398.2970400},
abstract = {The function of query auto-completion in modern search engines is to help users formulate queries fast and precisely. Conventional context-aware methods primarily rank candidate queries according to term- and query- relationships to the context. However, most sessions are extremely short. How to capture search intents with such relationships becomes difficult when the context generally contains only few queries. In this paper, we investigate the feasibility of discovering search intents within short context for query auto-completion. The class distribution of the search session (i.e., issued queries and click behavior) is derived as search intents. Several distribution-based features are proposed to estimate the proximity between candidates and search intents. Finally, we apply learning-to-rank to predict the user's intended query according to these features. Moreover, we also design an ensemble model to combine the benefits of our proposed features and term-based conventional approaches. Extensive experiments have been conducted on the publicly available AOL search engine log. The experimental results demonstrate that our approach significantly outperforms six competitive baselines. The performance of keystrokes is also evaluated in experiments. Furthermore, an in-depth analysis is made to justify the usability of search intent classification for query auto-completion.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {49–58},
numpages = {10},
keywords = {query auto-completion, search intent classification, short context},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970412,
author = {Azzopardi, Leif and Zuccon, Guido},
title = {An Analysis of the Cost and Benefit of Search Interactions},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970412},
doi = {10.1145/2970398.2970412},
abstract = {Interactive Information Retrieval (IR) systems often provide various features and functions, such as query suggestions and relevance feedback, that a user may or may not decide to use. The decision to take such an option has associated costs and may lead to some benefit. Thus, a savvy user would take decisions that maximises their net benefit. In this paper, we formally model the costs and benefits of various decisions that users, implicitly or explicitly, make when searching. We consider and analyse the following scenarios: (i) how long a user's query should be? (ii) should the user pose a specific or vague query? (iii) should the user take a suggestion or re-formulate? (iv) when should a user employ relevance feedback? and (v) when would the "find similar" functionality be worthwhile to the user? To this end, we build a series of cost-benefit models exploring a variety of parameters that affect the decisions at play. Through the analyses, we are able to draw a number of insights into different decisions, provide explanations for observed behaviours and generate numerous testable hypotheses. This work not only serves as a basis for future empirical work, but also as a template for developing other cost-benefit models involving human-computer interaction.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {59–68},
numpages = {10},
keywords = {measures, evaluation, user models, search behaviour, retrieval strategies},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970422,
author = {Van Gysel, Christophe and Kanoulas, Evangelos and de Rijke, Maarten},
title = {Lexical Query Modeling in Session Search},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970422},
doi = {10.1145/2970398.2970422},
abstract = {Lexical query modeling has been the leading paradigm for session search. In this paper, we analyze TREC session query logs and compare the performance of different lexical matching approaches for session search. Naive methods based on term frequency weighing perform on par with specialized session models. In addition, we investigate the viability of lexical query models in the setting of session search. We give important insights into the potential and limitations of lexical query modeling for session search and propose future directions for the field of session search.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {69–72},
numpages = {4},
keywords = {trec track, session search, analysis, lexical matching, semantic matching},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970431,
author = {Damessie, Tadele T. and Scholer, Falk and J\"{a}rvelin, Kalvero and Culpepper, J. Shane},
title = {The Effect of Document Order and Topic Difficulty on Assessor Agreement},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970431},
doi = {10.1145/2970398.2970431},
abstract = {Human relevance judgments are a key component for measuring the effectiveness of information retrieval systems using test collections. Since relevance is not an absolute concept, human assessors can disagree on particular topic-document pairs for a variety of reasons. In this work we investigate the effect that document presentation order has on inter-rater agreement, comparing two presentation ordering approaches similar to those used in IR evaluation campaigns: decreasing relevance order and document identifier order. We make a further distinction between "easy" topics and "hard" topics in order to explore system effects on inter-rater agreement. The results of our pilot user study indicate that assessor agreement is higher when documents are judged in document identifier order. In addition, there is higher overall agreement on easy topics than on hard topics.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {73–76},
numpages = {4},
keywords = {ordering effects, assessor agreement, experimentation, relevance, measurement},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970415,
author = {Yang, Peilin and Fang, Hui},
title = {A Reproducibility Study of Information Retrieval Models},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970415},
doi = {10.1145/2970398.2970415},
abstract = {Developing effective information retrieval models has been a long standing challenge in Information Retrieval (IR), and significant progresses have been made over the years. With the increasing number of developed retrieval functions and the release of new data collections, it becomes more difficult, if not impossible, to compare a new retrieval function with all existing retrieval functions over all available data collections. To tackle thisproblem, this paper describes our efforts on constructing a platform that aims to improve the reproducibility of IR researchand facilitate the evaluation and comparison of retrieval functions. With the developed platform, more than 20 state of the art retrieval functions have been implemented and systematically evaluated over 16 standard TREC collections (including the newly released ClueWeb datasets). Our reproducibility study leads to several interesting observations. First, the performance difference between the reproduced results and those reported in the original papers is small for most retrieval functions. Second, the optimal performance of a few representative retrieval functions is still comparable over the new TREC ClueWeb collections. Finally, the developed platform (i.e., RISE) is made publicly available so that any IR researchers would be able to utilize it to evaluate other retrieval functions.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {77–86},
numpages = {10},
keywords = {reproduciblity, evaluation system, ranking models},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970432,
author = {Azarbonyad, Hosein and Kanoulas, Evangelos},
title = {Power Analysis for Interleaving Experiments by Means of Offline Evaluation},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970432},
doi = {10.1145/2970398.2970432},
abstract = {Evaluation in information retrieval takes one of two forms: collection-based offline evaluation, and in-situ online evaluation. Collections constructed by the former methodology are reusable, and hence able to test the effectiveness of any experimental algorithm, while the latter requires a different experiment for every new algorithm. Due to this a funnel approach is often being used, with experimental algorithms being compared to the baseline in an online experiment only if they outperform the baseline in an offline experiment. One of the key questions in the design of online and offline experiments concerns the number of measurements required to detect a statistically significant difference between two algorithms. Power analysis can provide an answer to this question, however, it requires an a-priori knowledge of the difference in effectiveness to be detected, and the variance in the measurements. The variance is typically estimated using historical data, but setting a detectable difference prior to the experiment can lead to suboptimal, upper-bound results. In this work we make use of the funnel approach in evaluation and test whether the difference in the effectiveness of two algorithms measured by the offline experiment can inform the required number of impression of an online interleaving experiment. Our analysis on simulated data shows that the number of impressions required are correlated with the difference in the offline experiment, but at the same time widely vary for any given difference.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {87–90},
numpages = {4},
keywords = {power analysis, experimental design, information retrieval evaluation, interleaving},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970427,
author = {Paik, Jiaul H. and Lin, Jimmy},
title = {Retrievability in API-Based "Evaluation as a Service"},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970427},
doi = {10.1145/2970398.2970427},
abstract = {"Evaluation as a service" (EaaS) refers to a family of related evaluation methodologies that enables community-wide evaluations and the construction of test collections on documents that cannot be easily distributed. In the API-based approach, the basic idea is that evaluation organizers provide a service API through which the evaluation task can be completed, without providing access to the raw collection. One concern with this evaluation approach is that the API introduces biases and limits the diversity of techniques that can be brought to bear on the problem. In this paper, we tackle the question of API bias using the concept of retrievability. The raw data for our analyses come from a naturally-occurring experiment where we observed the same groups completing the same task with the API and also with access to the raw collection. We find that the retrievability bias of runs generated in both cases are comparable. Moreover, the fraction of relevant tweets retrieved through the API by the participating groups is at least as high as when they had access to the raw collection.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {91–94},
numpages = {4},
keywords = {trec, meta-evaluation, tweet search},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970399,
author = {Sakai, Tetsuya},
title = {A Simple and Effective Approach to Score Standardisation},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970399},
doi = {10.1145/2970398.2970399},
abstract = {Webber, Moffat and Zobel proposed score standardization for information retrieval evaluation with multiple test collections. Given a topic-by-run raw score matrix in terms of some evaluation measure, each score can be standardised using the topic's sample mean and sample standard deviation across a set of past runs so as to quantify how different a system is from the "average" system in standard deviation units. Using standardised scores, researchers can compare systems across different test collections without worrying about topic hardness or normalisation. WhileWebber et al. mapped the standardised scores to the [0, 1] range using a standard normal cumulative density function, the present study demonstrates that linear transformation of the standardised scores, a method widely used in educational research, can be a simple and effective alternative. We use three TREC robust track data sets with graded relevance assessments and official runs to compare these methods by means of leave-one-out tests, discriminative power, swap rate tests, and topic set size design. In particular, we demonstrate that our method is superior to the method of Webber et al. in terms of swap rates and topic set size design: put simply, our method ensures pairwise system comparisons that are more consistent across different data sets, and is arguably more convenient for designing a new test collection from a statistical viewpoint.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {95–104},
numpages = {10},
keywords = {statistical power, statistical significance, test collections, measures, evaluation, topics, variances, standardization},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970429,
author = {Lipani, Aldo and Zuccon, Guido and Lupu, Mihai and Koopman, Bevan and Hanbury, Allan},
title = {The Impact of Fixed-Cost Pooling Strategies on Test Collection Bias},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970429},
doi = {10.1145/2970398.2970429},
abstract = {In Information Retrieval, test collections are usually built using the pooling method. Many pooling strategies have been developed for the pooling method. Herein, we address the question of identifying the best pooling strategy when evaluating systems using precision-oriented measures in presence of budget constraints on the number of documents to be evaluated. As a quality measurement we use the bias introduced by the pooling strategy, measured both in terms of Mean Absolute Error of the scores and in terms of ranking errors. Based on experiments on 15 test collections, we conclude that, for precision-oriented measures, the best strategies are based on Rank-Biased Precision (RBP). These results can inform collection builders because they suggest that, under fixed assessment budget constraints, RBP-based sampling produces less biased pools than other alternatives.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {105–108},
numpages = {4},
keywords = {pooling method, pooling strategies, pool bias},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970410,
author = {Schnabel, Tobias and Swaminathan, Adith and Frazier, Peter I. and Joachims, Thorsten},
title = {Unbiased Comparative Evaluation of Ranking Functions},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970410},
doi = {10.1145/2970398.2970410},
abstract = {Eliciting relevance judgments for ranking evaluation is labor-intensive and costly, motivating careful selection of which documents to judge. Unlike traditional approaches that make this selection deterministically, probabilistic sampling enables the design of estimators that are provably unbiased even when reusing data with missing judgments. In this paper, we first unify and extend these sampling approaches by viewing the evaluation problem as a Monte Carlo estimation task that applies to a large number of common IR metrics. Drawing on the theoretical clarity that this view offers, we tackle three practical evaluation scenarios: comparing two systems, comparing k systems against a baseline, and ranking k systems. For each scenario, we derive an estimator and a variance-optimizing sampling distribution while retaining the strengths of sampling-based evaluation, including unbiasedness, reusability despite missing data, and ease of use in practice. In addition to the theoretical contribution, we empirically evaluate our methods against previously used sampling heuristics and find that they often cut the number of required relevance judgments at least in half.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {109–118},
numpages = {10},
keywords = {evaluation, pooling, crowd sourcing, importance sampling},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970437,
author = {Wilkie, Colin and Azzopardi, Leif},
title = {A Topical Approach to Retrievability Bias Estimation},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970437},
doi = {10.1145/2970398.2970437},
abstract = {Retrievability is an independent evaluation measure that offers insights to an aspect of retrieval systems that performance and efficiency measures do not. Retrievability is often used to calculate the retrievability bias, an indication of how accessible a system makes all the documents in a collection. Generally, computing the retrievability bias of a system requires a colossal number of queries to be issued for the system to gain an accurate estimate of the bias. However, it is often the case that the accuracy of the estimate is not of importance, but the relationship between the estimate of bias and performance when tuning a systems parameters. As such, reaching a stable estimation of bias for the system is more important than getting very accurate retrievability scores for individual documents. This work explores the idea of using topical subsets of the collection for query generation and bias estimation to form a local estimate of bias which correlates with the global estimate of retrievability bias. By using topical subsets, it would be possible to reduce the volume of queries required to reach an accurate estimate of retrievability bias, reducing the time and resources required to perform a retrievability analysis. Findings suggest that this is a viable approach to estimating retrievability bias and that the number of queries required can be reduced to less than a quarter of what was previously thought necessary.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {119–122},
numpages = {4},
keywords = {retrievability, evaluation, bias},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970403,
author = {Zamani, Hamed and Croft, W. Bruce},
title = {Estimating Embedding Vectors for Queries},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970403},
doi = {10.1145/2970398.2970403},
abstract = {The dense vector representation of vocabulary terms, also known as word embeddings, have been shown to be highly effective in many natural language processing tasks. Word embeddings have recently begun to be studied in a number of information retrieval (IR) tasks. One of the main steps in leveraging word embeddings for IR tasks is to estimate the embedding vectors of queries. This is a challenging task, since queries are not always available during the training phase of word embedding vectors. Previous work has considered the average or sum of embedding vectors of all query terms (AWE) to model the query embedding vectors, but no theoretical justification has been presented for such a model. In this paper, we propose a theoretical framework for estimating query embedding vectors based on the individual embedding vectors of vocabulary terms. We then provide a number of different implementations of this framework and show that the AWE method is a special case of the proposed framework. We also introduce pseudo query vectors, the query embedding vectors estimated using pseudo-relevant documents. We further extrinsically evaluate the proposed methods using two well-known IR tasks: query expansion and query classification. The estimated query embedding vectors are evaluated via query expansion experiments over three newswire and web TREC collections as well as query classification experiments over the KDD Cup 2005 test set. The experiments show that the introduced pseudo query vectors significantly outperform the AWE method.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {123–132},
numpages = {10},
keywords = {query embedding vector, word embedding, query classification, pseudo query vector, query expansion},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970409,
author = {Ai, Qingyao and Yang, Liu and Guo, Jiafeng and Croft, W. Bruce},
title = {Analysis of the Paragraph Vector Model for Information Retrieval},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970409},
doi = {10.1145/2970398.2970409},
abstract = {Previous studies have shown that semantically meaningful representations of words and text can be acquired through neural embedding models. In particular, paragraph vector (PV) models have shown impressive performance in some natural language processing tasks by estimating a document (topic) level language model. Integrating the PV models with traditional language model approaches to retrieval, however, produces unstable performance and limited improvements. In this paper, we formally discuss three intrinsic problems of the original PV model that restrict its performance in retrieval tasks. We also describe modifications to the model that make it more suitable for the IR task, and show their impact through experiments and case studies. The three issues we address are (1) the unregulated training process of PV is vulnerable to short document over-fitting that produces length bias in the final retrieval model; (2) the corpus-based negative sampling of PV leads to a weighting scheme for words that overly suppresses the importance of frequent words; and (3) the lack of word-context information makes PV unable to capture word substitution relationships.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {133–142},
numpages = {10},
keywords = {paragraph vector, language model},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970438,
author = {Cohen, Daniel and Croft, W. Bruce},
title = {End to End Long Short Term Memory Networks for Non-Factoid Question Answering},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970438},
doi = {10.1145/2970398.2970438},
abstract = {Retrieving correct answers for non-factoid queries poses significant challenges for current answer retrieval methods. Methods either involve the laborious task of extracting numerous features or are ineffective for longer answers. We approach the task of non-factoid question answering using deep learning methods without the need of feature extraction. Neural networks are capable of learning complex relations based on relatively simple features which make them a prime candidate for relating non-factoid questions to their answers. In this paper, we show that end to end training with a Bidirectional Long Short Term Memory (BLSTM) network with a rank sensitive loss function results in significant performance improvements over previous approaches without the need for combining additional models.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {143–146},
numpages = {4},
keywords = {deep learning, question answering, non-factoid},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970405,
author = {Zamani, Hamed and Croft, W. Bruce},
title = {Embedding-Based Query Language Models},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970405},
doi = {10.1145/2970398.2970405},
abstract = {Word embeddings, which are low-dimensional vector representations of vocabulary terms that capture the semantic similarity between them, have recently been shown to achieve impressive performance in many natural language processing tasks. The use of word embeddings in information retrieval, however, has only begun to be studied. In this paper, we explore the use of word embeddings to enhance the accuracy of query language models in the ad-hoc retrieval task. To this end, we propose to use word embeddings to incorporate and weight terms that do not occur in the query, but are semantically related to the query terms. We describe two embedding-based query expansion models with different assumptions. Since pseudo-relevance feedback methods that use the top retrieved documents to update the original query model are well-known to be effective, we also develop an embedding-based relevance model, an extension of the effective and robust relevance model approach. In these models, we transform the similarity values obtained by the widely-used cosine similarity with a sigmoid function to have more discriminative semantic similarity values. We evaluate our proposed methods using three TREC newswire and web collections. The experimental results demonstrate that the embedding-based methods significantly outperform competitive baselines in most cases. The embedding-based methods are also shown to be more robust than the baselines.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {147–156},
numpages = {10},
keywords = {pseudo-relevance feedback, word embedding, language models, query expansion},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970407,
author = {Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Tolomei, Gabriele},
title = {Learning to Rank User Queries to Detect Search Tasks},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970407},
doi = {10.1145/2970398.2970407},
abstract = {We present a framework for discovering sets of web queries having similar latent needs, called search tasks, from user queries stored in a search engine log. The framework is made of two main modules: Query Similarity Learning (QSL) and Graph-based Query Clustering (GQC). The former is devoted to learning a query similarity function from a ground truth of manually-labeled search tasks. The latter represents each user search log as a graph whose nodes are queries, and uses the learned similarity function to weight edges between query pairs. Finally, search tasks are detected by clustering those queries in the graph which are connected by the strongest links, in fact by detecting the strongest connected components of the graph. To discriminate between "strong" and "weak" links also the GQC module entails a learning phase whose goal is to estimate the best threshold for pruning the edges of the graph. We discuss how the QSL module can be effectively implemented using Learning to Rank (L2R) techniques. Experiments on a real-world search engine log show that query similarity functions learned using L2R lead to better performing GQC implementations when compared to similarity functions induced by other state-of-the-art machine learning solutions, such as logistic regression and decision trees.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {157–166},
numpages = {10},
keywords = {query log mining, search task discovery},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970433,
author = {Gigli, Andrea and Lucchese, Claudio and Nardini, Franco Maria and Perego, Raffaele},
title = {Fast Feature Selection for Learning to Rank},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970433},
doi = {10.1145/2970398.2970433},
abstract = {An emerging research area named Learning-to-Rank (LtR) has shown that effective solutions to the ranking problem can leverage machine learning techniques applied to a large set of features capturing the relevance of a candidate document for the user query. Large-scale search systems must however answer user queries very fast, and the computation of the features for candidate documents must comply with strict back-end latency constraints. The number of features cannot thus grow beyond a given limit, and Feature Selection (FS) techniques have to be exploited to find a subset of features that both meets latency requirements and leads to high effectiveness of the trained models. In this paper, we propose three new algorithms for FS specifically designed for the LtR context where hundreds of continuous or categorical features can be involved. We present a comprehensive experimental analysis conducted on publicly available LtR datasets and we show that the proposed strategies outperform a well-known state-of-the-art competitor.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {167–170},
numpages = {4},
keywords = {feature selection, learning to rank},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970416,
author = {Fang, Yi and Liu, Mengwen},
title = {A Unified Energy-Based Framework for Learning to Rank},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970416},
doi = {10.1145/2970398.2970416},
abstract = {Learning to Rank (L2R) has emerged as one of the core machine learning techniques for IR. On the other hand, Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. They have produced impressive results in many computer vision and speech recognition tasks. In this paper, we introduce a unified view of Learning to Rank that integrates various L2R approaches in an energy-based ranking framework. In this framework, an energy function associates low energies to desired documents and high energies to undesired results. Learning is essentially the process of shaping the energy surface so that desired documents have lower energies. The proposed framework yields new insights into learning to rank. First, we show how various existing L2R models (pointwise, pairwise, and listwise) can be cast in the energy-based framework. Second, new L2R models can be constructed based on existing EBMs. Furthermore, inspired by the intuitive learning process of EBMs, we can devise novel energy-based models for ranking tasks. We introduce several new energy-based ranking models based on the proposed framework. The experiments are conducted on the public LETOR 4.0 benchmarks and demonstrate the effectiveness of the proposed models.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {171–180},
numpages = {10},
keywords = {learning to rank, energy-based models},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970423,
author = {Xiong, Chenyan and Callan, Jamie and Liu, Tie-Yan},
title = {Bag-of-Entities Representation for Ranking},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970423},
doi = {10.1145/2970398.2970423},
abstract = {This paper presents a new bag-of-entities representation for document ranking, with the help of modern knowledge bases and automatic entity linking. Our system represents query and documents by bag-of-entities vectors constructed from their entity annotations, and ranks documents by their matches with the query in the entity space. Our experiments with Freebase on TREC Web Track datasets demonstrate that current entity linking systems can provide sufficient coverage of the general domain search task, and that bag-of-entities representations outperform bag-of-words by as much as 18% in standard document ranking tasks.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {181–184},
numpages = {4},
keywords = {knowledge base, text representation, base-of-entities, document representation},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970408,
author = {Dehghani, Mostafa and Azarbonyad, Hosein and Kamps, Jaap and Marx, Maarten},
title = {On Horizontal and Vertical Separation in Hierarchical Text Classification},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970408},
doi = {10.1145/2970398.2970408},
abstract = {Hierarchy is an effective and common way of organizing data and representing their relationships at different levels of abstraction. However, hierarchical data dependencies cause difficulties in the estimation of "separable" models that can distinguish between the entities in the hierarchy. Extracting separable models of hierarchical entities requires us to take their relative position into account and to consider the different types of dependencies in the hierarchy. In this paper, we present an investigation of the effect of separability in text-based entity classification and argue that in hierarchical classification, a separation property should be established between entities not only in the same layer, but also in different layers.Our main findings are the followings. First, we analyse the importance of separability on the data representation in the task of classification and based on that, we introduce "Strong Separation Principle" for optimizing expected effectiveness of classifiers decision based on separation property. Second, we present Significant Words Language Models (SWLM) which capture all, and only, the essential features of hierarchical entities according to their relative position in the hierarchy resulting in horizontally and vertically separable models. Third, we validate our claims on real world data and demonstrate that how SWLM improves the accuracy of classification and how it provides transferable models over time. Although discussions in this paper focus on the classification problem, the models are applicable to any information access tasks on data that has, or can be mapped to, a hierarchical structure.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {185–194},
numpages = {10},
keywords = {hswlm, language models, significant words language models, swlm, hierarchical significant words language models, hierarchical text classification},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970421,
author = {Roitman, Haggai and Cohen, Doron and Hummel, Shay},
title = {From "More Like This" to "Better Than This"},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970421},
doi = {10.1145/2970398.2970421},
abstract = {In this paper we address a novel retrieval problem we term the "Better Than This" problem. For a given pair of a user query to be answered by some search engine and a single example answer provided by the user that may or may not be a correct answer to the query, we determine whether or not there exists some better answer within the search engine. The approach we take is to test whether the user's provided answer can be used for relevance feedback in order to improve the ability of the search engine to better answer the user's query. If this is indeed the case, then we determine that the original answer provided by the user is good enough and there is no need to consider a better alternative. Otherwise, we decide that the best alternative that the search engine can provide should be considered as a better answer. Using a simulation based evaluation, we demonstrate that, our approach provides a better decision making solution to this problem, compared to several other alternatives.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {195–198},
numpages = {4},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970414,
author = {Weiland, Lydia and Hulpus, Ioana and Ponzetto, Simone Paolo and Dietz, Laura},
title = {Understanding the Message of Images with Knowledge Base Traversals},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970414},
doi = {10.1145/2970398.2970414},
abstract = {The message of news articles is often supported by the pointed use of iconic images. These images together with their captions encourage emotional involvement of the reader. Current algorithms for understanding the semantics of news articles focus on its text, often ignoring the image. On the other side, works that target the semantics of images, mostly focus on recognizing and enumerating the objects that appear in the image. In this work, we explore the problem from another perspective: Can we devise algorithms to understand the message encoded by images and their captions? To answer this question, we study how well algorithms can describe an image-caption pair in terms of Wikipedia entities, thereby casting the problem as an entity-ranking task with an image-caption pair as query. Our proposed algorithm brings together aspects of entity linking, subgraph selection, entity clustering, relatedness measures, and learning-to-rank. In our experiments, we focus on media-iconic image-caption pairs which often reflect complex subjects such as sustainable energy and endangered species. Our test collection includes a gold standard of over 300 image-caption pairs about topics at different levels of abstraction. We show that with a MAP of 0.69, the best results are obtained when aggregating content-based and graph-based features in a Wikipedia-derived knowledge base.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {199–208},
numpages = {10},
keywords = {entity ranking, media-iconic images, image understanding},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970406,
author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
title = {Exploiting Entity Linking in Queries for Entity Retrieval},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970406},
doi = {10.1145/2970398.2970406},
abstract = {The premise of entity retrieval is to better answer search queries by returning specific entities instead of documents. Many queries mention particular entities; recognizing and linking them to the corresponding entry in a knowledge base is known as the task of entity linking in queries. In this paper we make a first attempt at bringing together these two, i.e., leveraging entity annotations of queries in the entity retrieval model. We introduce a new probabilistic component and show how it can be applied on top of any term-based entity retrieval model that can be emulated in the Markov Random Field framework, including language models, sequential dependence models, as well as their fielded variations. Using a standard entity retrieval test collection, we show that our extension brings consistent improvements over all baseline methods, including the current state-of-the-art. We further show that our extension is robust against parameter settings.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {209–218},
numpages = {10},
keywords = {entity linking, entity retrieval, semistructured retrieval},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970402,
author = {Kuzi, Saar and Shtok, Anna and Kurland, Oren},
title = {Query Anchoring Using Discriminative Query Models},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970402},
doi = {10.1145/2970398.2970402},
abstract = {Pseudo-feedback-based query models are induced from a result list of the documents most highly ranked by initial search performed for the query. Since the result list often contains much non-relevant information, query models are anchored to the query using various techniques. We present a novel {em unsupervised} discriminative query model that can be used, by several methods proposed herein, for query anchoring of existing query models. The model is induced from the result list using a learning-to-rank approach, and constitutes a discriminative term-based representation of the initial ranking. We show that applying our methods to generative query models can improve retrieval performance.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {219–228},
numpages = {10},
keywords = {query models, pseudo feedback},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970434,
author = {Elbagoury, Ahmed and Crane, Matt and Lin, Jimmy},
title = {Rank-at-a-Time Query Processing},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970434},
doi = {10.1145/2970398.2970434},
abstract = {Query processing strategies for ranked retrieval have been studied for decades. In this paper we propose a new strategy, which we call rank-at-a-time query processing, that evaluates documents in descending order of quantized scores and is able to directly compute the final document ranking via a sequence of boolean intersections. We show that such a strategy is equivalent to a second-order restricted composition of per-term scores. Rank-at-a-time query processing has the advantage that it is anytime score-safe, which means that the retrieval algorithm can self-adapt to produce an exact ranking given an arbitrary latency constraint. Due to the combinatorial nature of compositions, however, a naive implementation is too slow to be of practical use. To address this issue, we introduce a hybrid variant that is able to reduce query latency to a point that is on par with state-of-the-art retrieval engines.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {229–232},
numpages = {4},
keywords = {query processing, efficiency},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970439,
author = {Balaneshin-kordan, Saeid and Kotov, Alexander},
title = {A Study of Document Expansion Using Translation Models and Dimensionality Reduction Methods},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970439},
doi = {10.1145/2970398.2970439},
abstract = {Over a decade of research on document expansion methods resulted in several independent avenues, including smoothing methods, translation models, and dimensionality reduction techniques, such as matrix decompositions and topic models. Although these research avenues have been individually explored in many previous studies, there is still a lack of understanding of how state-of-the-art methods for each of these directions compare with each other in terms of retrieval accuracy. This paper eliminates this gap by reporting the results of an empirical comparison of document expansion methods using translation models estimated based on word co-occurrence and cosine similarity between low-dimensional word embeddings, Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), on standard TREC collections. Experimental results indicate that LDA-based document expansion consistently outperforms both types of translation models and NMF according to all evaluation metrics for all and difficult queries, which is closely followed by translation model using word embeddings.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {233–236},
numpages = {4},
keywords = {translation models, non-negative matrix factorization, latent dirichlet allocation, word embeddings, document expansion},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970428,
author = {Yang, Peilin and Fang, Hui},
title = {Estimating Retrieval Performance Bound for Single Term Queries},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970428},
doi = {10.1145/2970398.2970428},
abstract = {Various information retrieval models have been studied for decades. Most traditional retrieval models are based on bag-of-termrepresentations, and they model the relevance based on various collection statistics. Despite these efforts, it seems that the performance of "bag-of-term" based retrieval functions has reached plateau, and it becomes increasingly difficult to further improve the retrieval performance. Thus, one important research question is whether we can provide any theoretical justifications on the empirical performance bound of basic retrieval functions.In this paper, we start with single term queries, and aim to estimate the performance bound of retrieval functions that leverage only basic ranking signals such as document term frequency, inverse document frequency and document length normalization. Specifically, we demonstrate that, when only single-term queries are considered, there is a general function that can cover many basic retrieval functions. We then propose to estimate the upper bound performance of this function by applying a cost/gain analysis to search for the optimal value of the function.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {237–240},
numpages = {4},
keywords = {performance upper bound, learning to rank, single term query},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970418,
author = {Balaneshin-kordan, Saeid and Kotov, Alexander},
title = {Optimization Method for Weighting Explicit and Latent Concepts in Clinical Decision Support Queries},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970418},
doi = {10.1145/2970398.2970418},
abstract = {Accurately answering verbose queries that describe a clinical case and aim at finding articles in a collection of medical literature requires capturing many explicit and latent aspects of complex information needs underlying such queries. Proper representation of these aspects often requires query analysis to identify the most important query concepts as well as query transformation by adding new concepts to a query, which can be extracted from the top retrieved documents or medical knowledge bases. Traditionally, query analysis and expansion have been done separately. In this paper, we propose a method for representing verbose domain-specific queries based on weighted unigram, bigram, and multi-term concepts in the query itself, as well as extracted from the top retrieved documents and external knowledge bases. We also propose a graduated non-convexity optimization framework, which allows to unify query analysis and expansion by jointly determining the importance weights for the query and expansion concepts depending on their type and source. Experiments using a collection of PubMed articles and TREC Clinical Decision Support (CDS) track queries indicate that applying our proposed method results in significant improvement of retrieval accuracy over state-of-the-art methods for ad hoc and medical IR.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {241–250},
numpages = {10},
keywords = {feature-based retrieval models, clinical decision support, global optimization, medical literature retrieval},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970401,
author = {Jameel, Shoaib and Liao, Yi and Lam, Wai and Schockaert, Steven and Xie, Xing},
title = {Exploring Urban Lifestyles Using a Nonparametric Temporal Graphical Model},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970401},
doi = {10.1145/2970398.2970401},
abstract = {We propose a new unsupervised nonparametric temporal topic model to discover lifestyle patterns from location-based social networks. By relating the textual content, time stamps, and venue categories associated to user check-ins, our framework detects the predominant lifestyle patterns in a given geographic region. The temporal component of our model allows us to analyse the evolution of lifestyle patterns throughout the year. We provide examples of interesting patterns that have been discovered by our model, and we show that our model compares favourably to existing approaches in terms of lifestyle pattern quality and computation time. We also quantitatively show that our model outperforms existing methods in a time stamp prediction task.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {251–260},
numpages = {10},
keywords = {lifestyle patterns, bayesian nonparametrics, text mining, urban computing, location-based social networks, lbsn, posterior inference, gibbs sampling, twitter, foursquare, graphical models, online social networks, topic models},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970411,
author = {Gupta, Dhruv and Str\"{o}tgen, Jannik and Berberich, Klaus},
title = {EventMiner: Mining Events from Annotated Documents},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970411},
doi = {10.1145/2970398.2970411},
abstract = {Events are central in human history and thus also in Web queries, in particular if they relate to history or news. However, ambiguity issues arise as queries may refer to ambiguous events differing in time, geography, or participating entities. Thus, users would greatly benefit if search results were presented along different events. In this paper, we present EventMiner, an algorithm that mines events from top-k pseudo-relevant documents for a given query. It is a probabilistic framework that leverages semantic annotations in the form of temporal expressions, geographic locations, and named entities to analyze natural language text and determine important events. Using a large news corpus, we show that using semantic annotations, EventMiner detects important events and presents documents covering the identified events in the order of their importance.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {261–270},
numpages = {10},
keywords = {information retrieval, semantic annotations, text mining},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970420,
author = {Liao, Yi and Lam, Wai and Jameel, Shoaib and Schockaert, Steven and Xie, Xing},
title = {Who Wants to Join Me? Companion Recommendation in Location Based Social Networks},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970420},
doi = {10.1145/2970398.2970420},
abstract = {We consider the problem of identifying possible companions for a user who is planning to visit a given venue. Specifically, we study the task of predicting which of the user's current friends, in a location based social network (LBSN), are most likely to be interested in joining the visit. An important underlying assumption of our model is that friendship relations can be clustered based on the kinds of interests that are shared by the friends. To identify these friendship types, we use a latent topic model, which moreover takes into account the geographic proximity of the user to the location of the proposed venue. To the best of our knowledge, our model is the first that addresses the task of recommending companions for a proposed activity. While a number of existing topic models can be adapted to make such predictions, we experimentally show that such methods are significantly outperformed by our model.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {271–280},
numpages = {10},
keywords = {location-based social networks, graphical model, companion recommendation},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970417,
author = {Fang, Yi and Godavarthy, Archana and Lu, Haibing},
title = {A Utility Maximization Framework for Privacy Preservation of User Generated Content},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970417},
doi = {10.1145/2970398.2970417},
abstract = {The prodigious amount of user-generated content continues to grow at an enormous rate. While it greatly facilitates the flow of information and ideas among people and communities, it may pose great threat to our individual privacy. In this paper, we demonstrate that the private traits of individuals can be inferred from user-generated content by using text classification techniques. Specifically, we study three private attributes on Twitter users: religion, political leaning, and marital status. The ground truth labels of the private traits can be readily collected from the Twitter bio field. Based on the tweets posted by the users and their corresponding bios, we show that text classification yields a high accuracy of identification of these personal attributes, which poses a great privacy risk on user-generated content.We further propose a constrained utility maximization framework for preserving user privacy. The goal is to maximize the utility of data when modifying the user-generated content, while degrading the prediction performance of the adversary. The KL divergence is minimized between the prior knowledge about the private attribute and the posterior probability after seeing the user-generated data. Based on this proposed framework, we investigate several specific data sanitization operations for privacy preservation: add, delete, or replace words in the tweets. We derive the exact transformation of the data under each operation. The experiments demonstrate the effectiveness of the proposed framework.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {281–290},
numpages = {10},
keywords = {privacy preservation, user generated content},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970425,
author = {Sen, Procheta and Ganguly, Debasis and Jones, Gareth J.F.},
title = {Joint Estimation of Topics and Hashtag Relevance in Cross-Lingual Tweets},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970425},
doi = {10.1145/2970398.2970425},
abstract = {Twitter is a widely used platform for sharing news articles. An emerging trend in multi-lingual communities is to share non-English news articles using English tweets in order to spread the news to a wider audience. In general, the choice of relevant hashtags for such tweets depends on the topic of the non-English news article. In this paper, we address the problem of automatically detecting the relevance of the hashtags of such tweets. More specifically, we propose a generative model to jointly model the topics within an English tweet and those within the non-English news article shared from it to predict the relevance of the hashtags of the tweet. For conducting experiments, we compiled a collection of English tweets that share news articles in Bengali (a South Asian language). Our experiments on this dataset demonstrate that this joint estimation based approach using the topics from both the non-English news articles and the tweets proves to be more effective for relevance estimation than that of only using the topics of a tweet itself.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {291–294},
numpages = {4},
keywords = {joint estimation of topic and tag relevance, cross-lingual tweet tagging, bilingual topic modelling},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970424,
author = {Rao, Jinfeng and Lin, Jimmy},
title = {Temporal Query Expansion Using a Continuous Hidden Markov Model},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970424},
doi = {10.1145/2970398.2970424},
abstract = {In standard formulations of pseudo-relevance feedback, document timestamps do not play a role in identifying expansion terms. Yet we know that when searching social media posts such as tweets, relevant documents are bursty and usually occur in temporal clusters. The main insight of our work is that term expansions should be biased to draw from documents that occur in bursty temporal clusters. This is formally captured by a continuous hidden Markov model (cHMM), for which we derive an EM algorithm for parameter estimation. Given a query, we estimate the parameters for a cHMM that best explains the observed distribution of an initial set of retrieved documents, and then use Viterbi decoding to compute the most likely state sequence. In identifying expansion terms, we only select documents from bursty states. Experiments on test collections from the TREC 2011 and 2012 Microblog tracks show that our approach is significantly more effective than the popular RM3 pseudo-relevance feedback model.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {295–298},
numpages = {4},
keywords = {tweet search, pseudo-relevance feedback},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970426,
author = {Arora, Piyush and Ganguly, Debasis and Jones, Gareth J.F.},
title = {Nearest Neighbour Based Transformation Functions for Text Classification: A Case Study with StackOverflow},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970426},
doi = {10.1145/2970398.2970426},
abstract = {significant increase in the number of questions in question answering forums has led to the interest in text categorization methods for classifying a newly posted question as good (suitable) or bad (otherwise) for the forum. Standard text categorization approaches, e.g. multinomial Naive Bayes, are likely to be unsuitable for this classification task because of: i) the lack of sufficient informative content in the questions due to their relatively short length; and ii) considerable vocabulary overlap between the classes. To increase the robustness of this classification task, we propose to use the neighbourhood of existing questions which are similar to the newly asked question. Instead of learning the classification boundary from the questions alone, we transform each question vector into a different one in the feature space. We explore two different neighbourhood functions using: the discrete term space, the continuous vector space of real numbers obtained from vector embeddings of documents. Experiments conducted on StackOverflow data show that our approach of using the neighborhood transformation can improve classification accuracy by up to about 8%.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {299–302},
numpages = {4},
keywords = {neighbourhood based transformation, question quality prediction, document embedding},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/2970398.2970436,
author = {Godavarthy, Archana and Fang, Yi},
title = {Cross-Language Microblog Retrieval Using Latent Semantic Modeling},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970436},
doi = {10.1145/2970398.2970436},
abstract = {Microblogging has become one of the major tools of sharing real-time information for people around the world. Finding relevant information across different languages on microblogs is highly desirable especially for the large number of multilingual users. However, the characteristics of microblog content pose great challenges to the existing cross-language information retrieval approaches. In this paper, we address the task of retrieving relevant tweets given another tweet in a different language. We build parallel corpora for tweets in different languages by bridging them via shared hashtags. We propose a latent semantic approach to model the parallel corpora by mapping the parallel tweets to a low-dimensional shared semantic space. The relevance between tweets in different languages is measured in this shared latent space and the model is trained on a pairwise loss function. The preliminary experiments on a Twitter dataset demonstrate the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {303–306},
numpages = {4},
keywords = {cross language information retrieval, microblog retrieval, latent semantic modeling},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

