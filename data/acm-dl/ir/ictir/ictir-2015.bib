@inproceedings{10.1145/2808194.2808195,
author = {McCallum, Andrew},
title = {Embedded Representations of Lexical and Knowledge-Base Semantics},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2808195},
doi = {10.1145/2808194.2808195},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {1},
numpages = {1},
keywords = {data mining, knowledge-base semantics, reinforcement learning, machine learning, natural language processing},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809444,
author = {Azzopardi, Leif},
title = {Theory of Retrieval: The Retrievability of Information},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809444},
doi = {10.1145/2808194.2809444},
abstract = {Retrievability is an important and interesting indicator that can be used in a number of ways to analyse Information Retrieval systems and document collections. Rather than focusing totally on relevance, retrievability examines what is retrieved, how often it is retrieved, and whether a user is likely to retrieve it or not. This is important because a document needs to be retrieved, before it can be judged for relevance. In this tutorial, we shall explain the concept of retrievability along with a number of retrievability measures, how it can be estimated and how it can be used for analysis. Since retrieval precedes relevance, we shall also provide an overview of how retrievability relates to effectiveness - describing some of the insights that researchers have discovered thus far. We shall also show how retrievability relates to efficiency, and how the theory of retrievability can be used to improve both effectiveness and efficiency. Then we shall provide an overview of the different applications of retrievability such as Search Engine Bias, Corpus Profiling, etc., before wrapping up with challenges and opportunities. The final session will look at example problems and ways to analyse and apply retrievability to other problems and domains. Participants are invited to bring their own problems to be discussed after the tutorial. This half-day tutorial is ideal for: (i) researchers curious about retrievability and wanting to see how it can impact their research, (ii) researchers who would like to expand their set of analysis techniques, and/or (iii) researchers who would like to use retrievability to perform their own analysis.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {3–6},
numpages = {4},
keywords = {simulation, effectiveness, retrievability, evaluation},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809445,
author = {Carterette, Ben},
title = {Statistical Significance Testing in Information Retrieval: Theory and Practice},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809445},
doi = {10.1145/2808194.2809445},
abstract = {The past 20 years have seen a great improvement in the rigor of information retrieval experimentation, due primarily to two factors: high-quality, public, portable test collections such as those produced by TREC (the Text REtrieval Conference [28]), and the increased practice of sta- tistical hypothesis testing to determine whether measured improvements can be ascribed to something other than random chance. Together these create a very useful standard for reviewers, program committees, and journal editors; work in information retrieval (IR) increasingly cannot be published unless it has been evaluated using a well-constructed test collection and shown to produce a statistically significant improvement over a good baseline.But, as the saying goes, any tool sharp enough to be useful is also sharp enough to be dangerous. Statistical tests of significance are widely misunderstood. Most researchers and developers treat them as a "black box": evaluation results go in and a p-value comes out. But because significance is such an important factor in determining what research directions to explore and what is published, using p-values obtained without thought can have consequences for everyone doing research in IR. Ioannidis has argued that the main consequence in the biomedical sciences is that most published research findings are false [12]; could that be the case in IR as well?},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {7–9},
numpages = {3},
keywords = {statistical significance testing, information retrieval, evaluation, reproducibility},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809449,
author = {Sebastiani, Fabrizio},
title = {An Axiomatically Derived Measure for the Evaluation of Classification Algorithms},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809449},
doi = {10.1145/2808194.2809449},
abstract = {We address the general problem of finding suitable evaluation measures for classification systems. To this end, we adopt an axiomatic approach, i.e., we discuss a number of properties ("axioms") that an evaluation measure for classification should arguably satisfy. We start our analysis by addressing binary classification. We show that F1, nowadays considered a standard measure for the evaluation of binary classification systems, does not comply with a number of them, and should thus be considered unsatisfactory. We go on to discuss an alternative, simple evaluation measure for binary classification, that we call K, and show that it instead satisfies all the previously proposed axioms. We thus argue that researchers and practitioners should replace F1 with K in their everyday binary classification practice. We carry on our analysis by showing that K can be smoothly extended to deal with single-label multi-class classification, cost-sensitive classification, and ordinal classification.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {11–20},
numpages = {10},
keywords = {classification, evaluation measures, ordinal classification.},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809452,
author = {Ferrante, Marco and Ferro, Nicola and Maistro, Maria},
title = {Towards a Formal Framework for Utility-Oriented Measurements of Retrieval Effectiveness},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809452},
doi = {10.1145/2808194.2809452},
abstract = {In this paper we present a formal framework to define and study the properties of utility-oriented measurements of retrieval effectiveness, like AP, RBP, ERR and many other popular IR evaluation measures. The proposed framework is laid in the wake of the representational theory of measurement, which provides the foundations of the modern theory of measurement in both physical and social sciences, thus contributing to explicitly link IR evaluation to a broader context. The proposed framework is minimal, in the sense that it relies on just one axiom, from which other properties are derived. Finally, it contributes to a better understanding and a clear separation of what issues are due to the inherent problems in comparing systems in terms of retrieval effectiveness and what others are due to the expected numerical properties of a measurement.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {21–30},
numpages = {10},
keywords = {representational theory of measurement, omomorphism, swap, balancing index, replacement},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809469,
author = {Carterette, Ben},
title = {Bayesian Inference for Information Retrieval Evaluation},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809469},
doi = {10.1145/2808194.2809469},
abstract = {A key component of experimentation in IR is statistical hypothesis testing, which researchers and developers use to make inferences about the effectiveness of their system relative to others. A statistical hypothesis test can tell us the likelihood that small mean differences in effectiveness (on the order of 5%, say) is due to randomness or measurement error, and thus is critical for making progress in research. But the tests typically used in IR - the t-test, the Wilcoxon signed-rank test - are very general, not developed specifically for the problems we face in information retrieval evaluation. A better approach would take advantage of the fact that the atomic unit of measurement in IR is the relevance judgment rather than the effectiveness measure, and develop tests that model relevance directly. In this work we present such an approach, showing theoretically that modeling relevance in this way naturally gives rise to the effectiveness measures we care about. We demonstrate the usefulness of our model on both simulated data and a diverse set of runs from various TREC tracks.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {31–40},
numpages = {10},
keywords = {information retrieval, bayesian inference, statistical testing, evaluation},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809463,
author = {Macdonald, Craig and Din\c{c}er, B. Taner and Ounis, Iadh},
title = {Transferring Learning To Rank Models for Web Search},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809463},
doi = {10.1145/2808194.2809463},
abstract = {Learning to rank techniques provide mechanisms for combining document feature values into learned models that produce effective rankings. However, issues concerning the transferability of learned models between different corpora or subsets of the same corpus are not yet well understood. For instance, is the importance of different feature sets consistent between subsets of a corpus, or whether a learned model obtained on a small subset of the corpus effectively transfer to the larger corpus? By formulating our experiments around two null hypotheses, in this work, we apply a full-factorial experiment design to empirically investigate these questions using the ClueWeb09 and ClueWeb12 corpora, combined with queries from the TREC Web track. Among other observations, our experiments reveal that Clue-Web09 remains an effective choice of training corpus for learning effective models for ClueWeb12, and also that the importance of query independent features varies among the ClueWeb09 and ClueWeb12 corpora. In doing so, this work contributes an important study into the transferability of learning to rank models, as well as empirically-derived best practices for effective retrieval on the ClueWeb12 corpus.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {41–50},
numpages = {10},
keywords = {learning-to-rank, web search},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809456,
author = {Ben Basat, Ran and Tennenholtz, Moshe and Kurland, Oren},
title = {The Probability Ranking Principle is Not Optimal in Adversarial Retrieval Settings},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809456},
doi = {10.1145/2808194.2809456},
abstract = {The probability ranking principle (PRP) - ranking documents in response to a query by their relevance probabilities - is the theoretical foundation of most ad hoc document retrieval methods. A key observation that motivates our work is that the PRP does not account for potential post-ranking effects, specifically, changes to documents that result from a given ranking. Yet, in adversarial retrieval settings such as the Web, authors may consistently try to promote their documents in rankings by changing them. We prove that, indeed, the PRP can be sub-optimal in adversarial retrieval settings. We do so by presenting a novel game theoretic analysis of the adversarial setting. The analysis is performed for different types of documents (single topic and multi topic) and is based on different assumptions about the writing qualities of documents' authors. We show that in some cases, introducing randomization into the document ranking function yields overall user utility that transcends that of applying the PRP.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {51–60},
numpages = {10},
keywords = {probability ranking principle, adversarial retrieval},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809457,
author = {Sloan, Marc and Wang, Jun},
title = {Dynamic Information Retrieval: Theoretical Framework and Application},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809457},
doi = {10.1145/2808194.2809457},
abstract = {Theoretical frameworks like the Probability Ranking Principle and its more recent Interactive Information Retrieval variant have guided the development of ranking and retrieval algorithms for decades, yet they are not capable of helping us model problems in Dynamic Information Retrieval which exhibit the following three properties; an observable user signal, retrieval over multiple stages and an overall search intent. In this paper a new theoretical framework for retrieval in these scenarios is proposed. We derive a general dynamic utility function for optimizing over these types of tasks, that takes into account the utility of each stage and the probability of observing user feedback. We apply our framework to experiments over TREC data in the dynamic multi page search scenario as a practical demonstration of its effectiveness and to frame the discussion of its use, its limitations and to compare it against the existing frameworks.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {61–70},
numpages = {10},
keywords = {interactive ir, ranking and retrieval theory, dynamic ir},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809459,
author = {Zhao, Xiaoxue and Wang, Jun},
title = {A Theoretical Analysis of Two-Stage Recommendation for Cold-Start Collaborative Filtering},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809459},
doi = {10.1145/2808194.2809459},
abstract = {In this paper, we present a theoretical framework for tackling the cold-start collaborative filtering problem, where unknown targets (items or users) keep coming to the system, and there is a limited number of resources (users or items) that can be allocated and related to them. The solution requires a trade-off between exploitation and exploration since with the limited recommendation opportunities, we need to, on one hand, allocate the most relevant resources right away, but, on the other hand, it is also necessary to allocate resources that are useful for learning the target's properties in order to recommend more relevant ones in the future. In this paper, we study a simple two-stage recommendation combining a sequential and a batch solution together. We first model the problem with the partially observable Markov decision process (POMDP) and provide its exact solution. Then, through an in-depth analysis over the POMDP value iteration solution, we identify that an exact solution can be abstracted as selecting resources that are not only highly relevant to the target according to the initial-stage information, but also highly correlated, either positively or negatively, with other potential resources for the next stage. With this finding, we propose an approximate solution to ease the intractability of the exact solution. Our initial results on synthetic data and the MovieLens 100K dataset confirm our theoretical development and analysis.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {71–80},
numpages = {10},
keywords = {cold-start problem, recommender systems, collaborative filtering, pomdp},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809447,
author = {Azzopardi, Leif and Zuccon, Guido},
title = {An Analysis of Theories of Search and Search Behavior},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809447},
doi = {10.1145/2808194.2809447},
abstract = {Theories of search and search behavior can be used to glean insights and generate hypotheses about how people interact with retrieval systems. This paper examines three such theories, the long standing Information Foraging Theory, along with the more recently proposed Search Economic Theory and the Interactive Probability Ranking Principle. Our goal is to develop a model for ad-hoc topic retrieval using each approach, all within a common framework, in order to (1) determine what predictions each approach makes about search behavior, and (2) show the relationships, equivalences and differences between the approaches. While each approach takes a different perspective on modeling searcher interactions, we show that under certain assumptions, they lead to similar hypotheses regarding search behavior. Moreover, we show that the models are complementary to each other, but operate at different levels (i.e., sessions, patches and situations). We further show how the differences between the approaches lead to new insights into the theories and new models. This contribution will not only lead to further theoretical developments, but also enables practitioners to employ one of the three equivalent models depending on the data available.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {81–90},
numpages = {10},
keywords = {search economic theory, interactive probability ranking principle, information foraging theory},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809470,
author = {Carterette, Ben and Bah, Ashraf and Zengin, Mustafa},
title = {Dynamic Test Collections for Retrieval Evaluation},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809470},
doi = {10.1145/2808194.2809470},
abstract = {Batch evaluation with test collections of documents, search topics, and relevance judgments has been the bedrock of IR evaluation since its adoption by Salton for his experiments on vector space systems. Such test collections have limitations: they contain no user interaction data; there is typically only one query per topic; they have limited size due to the cost of constructing them. In the last 15-20 years, it has become evident that having a log of user interactions and a large space of queries is invaluable for building effective retrieval systems, but such data is generally only available to search engine companies. Thus there is a gap between what academics can study using static test collections and what industrial researchers can study using dynamic user data.In this work we propose dynamic test collections to help bridge this gap. Like traditional test collections, a dynamic test collection consists of a set of topics and relevance judgments. But instead of static one-time queries, dynamic test collections generate queries in response to the system. They can generate other actions such as clicks and time spent reading documents. Like static test collections, there is no human in the loop, but since the queries are dynamic they can generate much more data for evaluation than static test collections can. And since they can simulate user interactions across a session, they can be used for evaluating retrieval systems that make use of session history or other user information to try to improve results.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {91–100},
numpages = {10},
keywords = {test collections, information retrieval, sessions, evaluation, user simulation},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809465,
author = {Kelly, Diane and Arguello, Jaime and Edwards, Ashlee and Wu, Wan-ching},
title = {Development and Evaluation of Search Tasks for IIR Experiments Using a Cognitive Complexity Framework},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809465},
doi = {10.1145/2808194.2809465},
abstract = {One of the most challenging aspects of designing interactive information retrieval (IIR) experiments with users is the development of search tasks. We describe an evaluation of 20 search tasks that were designed for use in IIR experiments and developed using a cognitive complexity framework from educational theory. The search tasks represent five levels of cognitive complexity and four topical domains. The tasks were evaluated in the context of a laboratory IIR experiment with 48 participants. Behavioral and self-report data were used to characterize and understand differences among tasks. Results showed more cognitively complex tasks required significantly more search activity from participants (e.g., more queries, clicks, and time to complete). However, participants did not evaluate more cognitively complex tasks as more difficult and were equally satisfied with their performances across tasks. Our work makes four contributions: (1) it adds to what is known about the relationship among task, search behaviors and user experience; (2) it presents a framework for task creation and evaluation; (3) it provides tasks and questionnaires that can be reused by others and (4) it raises questions about findings and assumptions of many recent studies that only use behavioral signals from search logs as evidence for task difficulty and searcher satisfaction, as many of our results directly contradict these findings.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {101–110},
numpages = {10},
keywords = {interactive ir, search behavior, search tasks, user studies},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809446,
author = {Xiong, Chenyan and Callan, Jamie},
title = {Query Expansion with Freebase},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809446},
doi = {10.1145/2808194.2809446},
abstract = {Large knowledge bases are being developed to describe entities, their attributes, and their relationships to other entities. Prior research mostly focuses on the construction of knowledge bases, while how to use them in information retrieval is still an open problem. This paper presents a simple and effective method of using one such knowledge base, Freebase, to improve query expansion, a classic and widely studied information retrieval task. It investigates two methods of identifying the entities associated with a query, and two methods of using those entities to perform query expansion. A supervised model combines information derived from Freebase descriptions and categories to select terms that are effective for query expansion. Experiments on the ClueWeb09 dataset with TREC Web Track queries demonstrate that these methods are almost 30% more effective than strong, state-of-the-art query expansion algorithms. In addition to improving average performance, some of these methods have better win/loss ratios than baseline algorithms, with 50% fewer queries damaged.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {111–120},
numpages = {10},
keywords = {pseudo relevance feedback, knowledge base, freebase, query expansion},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809451,
author = {Goswami, Parantapa and Amini, Massih-Reza and Gaussier, Eric},
title = {Language-Independent Query Representation for IR Model Parameter Estimation on Unlabeled Collections},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809451},
doi = {10.1145/2808194.2809451},
abstract = {We study here the problem of estimating the parameters of standard IR models (as BM25 or language models) on new collections without any relevance judgments, by using collections with already available relevance judgements. We propose different query representations that allow mapping queries (with and without relevance judgments, from different collections, potentially in different languages) into a common space. We then introduce a kernel regression approach to learn the parameters of standard IR models individually for each query in the new, unlabeled collection. Our experiments, conducted on standard English and Indian IR collections, show that our approach can be used to efficiently tune, query by query, standard IR models to new collections, potentially written in different languages. In particular, the versions of the standard IR models we obtain not only outperform the versions with default parameters, but can also outperform the versions in which the parameter values have been optimized globally over a set of queries with target relevance judgements.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {121–130},
numpages = {10},
keywords = {transfer learning, ir theory, learning ir parameters},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809467,
author = {Mehrotra, Rishabh and Yilmaz, Emine},
title = {Terms, Topics &amp; Tasks: Enhanced User Modelling for Better Personalization},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809467},
doi = {10.1145/2808194.2809467},
abstract = {Given the distinct preferences of different users while using search engines, search personalization has become an important problem in information retrieval. Most approaches to search personalization are based on identifying topics a user may be interested in and personalizing search results based on this information.While topical interests information of users can be highly valuable in personalizing search results and improving user experience, it ignores the fact that two different users that have similar topical interests may still be interested in achieving very different tasks with respect to this topic (e.g. the type of tasks a broker is likely to perform related to finance is likely to be very different than that of a regular investor). Hence, considering user's topical interests jointly with the type of tasks they are likely to be interested in could result in better personalisedWe present an approach that uses search task information embedded in search logs to represent users by their actions over a task-space as well as over their topical-interest space. In particular, we describe a tensor based approach that represents each user in terms of (i) user's topical interests and (ii) user's search task behaviours in a coupled fashion and use these representations for personalization. Additionally, we also integrate user's historic search behavior in a coupled matrix-tensor factorization framework to learn user representations. Through extensive evaluation via query recommendations and user cohort analysis, we demonstrate the value of considering topic specific task information while developing user models.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {131–140},
numpages = {10},
keywords = {tensor decomposition, user modelling, search tasks},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809471,
author = {Hazimeh, Hussein and Zhai, ChengXiang},
title = {Axiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809471},
doi = {10.1145/2808194.2809471},
abstract = {Pseudo-Relevance Feedback (PRF) is an important general technique for improving retrieval effectiveness without requiring any user effort. Several state-of-the-art PRF models are based on the language modeling approach where a query language model is learned based on feedback documents. In all these models, feedback documents are represented with unigram language models smoothed with a collection language model. While collection language model-based smoothing has proven both effective and necessary in using language models for retrieval, we use axiomatic analysis to show that this smoothing scheme inherently causes the feedback model to favor frequent terms and thus violates the IDF constraint needed to ensure selection of discriminative feedback terms. To address this problem, we propose replacing collection language model-based smoothing in the feedback stage with additive smoothing, which is analytically shown to select more discriminative terms. Empirical evaluation further confirms that additive smoothing indeed significantly outperforms collection-based smoothing methods in multiple language model-based PRF models.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {141–150},
numpages = {10},
keywords = {geometric relevance model, relevance model, divergence minimization model, pseudo relevance feedback, smoothing},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809472,
author = {Chen, Ruey-Cheng and Lee, Chia-Jung and Croft, W. Bruce},
title = {On Divergence Measures and Static Index Pruning},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809472},
doi = {10.1145/2808194.2809472},
abstract = {We study the problem of static index pruning in a renowned divergence minimization framework, using a range of divergence measures such as f-divergence and R\'{e}nyi divergence as the objective. We show that many well-known divergence measures are convex in pruning decisions, and therefore can be exactly minimized using an efficient algorithm. Our approach allows postings be prioritized according to the amount of information they contribute to the index, and through specifying a different divergence measure the contribution is modeled on a different returns curve. In our experiment on GOV2 data, R\'{e}nyi divergence of order infinity appears the most effective. This divergence measure significantly outperforms many standard methods and achieves identical retrieval effectiveness as full data using only 50% of the postings. When top-k precision is of the only concern, 10% of the data is sufficient to achieve the accuracy that one would usually expect from a full index.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {151–160},
numpages = {10},
keywords = {static index pruning, f-divergence, r\'{e}nyi divergence},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809460,
author = {Ganguly, Debasis and Jones, Gareth J.F.},
title = {Partially Labeled Supervised Topic Models for RetrievingSimilar Questions in CQA Forums},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809460},
doi = {10.1145/2808194.2809460},
abstract = {Manual annotations, e.g. tags and links, of user generated content in community question answering forums and social media play an important role in making the content searchable. During the active phase of a new question entered into a CQA forum, a moderator or an answerer often has to make a significant effort to manually search for related question threads (which we refer to as documents), that he may consider linking to the current question. This manual effort can be greatly reduced by an automated search process to suggest a list of candidate documents to be linked to the new document. We described our investigation of link recommendation for this task. We approach the problem as an ad-hoc information retrieval (IR) task in which a new document (question) acts as the query and the intention is to retrieve a list of potentially relevant documents (previously asked questions in the forum), which could then be linked (manually) to the new one. In contrast to standard ad-hoc search, two pieces of human annotated additional information, namely the tags of the documents and the known links between existing document pairs, can potentially be used to improve the search quality for new questions. To utilize this additional information, we propose a generative model of tagged documents which jointly estimates the distribution of topics corresponding to each tag of a document along with the likelihood of a document being linked to another one. The model predictions are then incorporated in the query likelihood estimate of a standard language model (LM) of IR. Experiments conducted on three months of a crawled StackOverflow dataset show that utilizing the tag specific topic distributions results in a significant improvement in retrieval of the candidate set of related documents.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {161–170},
numpages = {10},
keywords = {stackoverflow linked question retrieval, supervised partially labeled topic model},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809473,
author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
title = {Entity Linking in Queries: Tasks and Evaluation},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809473},
doi = {10.1145/2808194.2809473},
abstract = {Annotating queries with entities is one of the core problem areas in query understanding. While seeming similar, the task of entity linking in queries is different from entity linking in documents and requires a methodological departure due to the inherent ambiguity of queries. We differentiate between two specific tasks, semantic mapping and interpretation finding, discuss current evaluation methodology, and propose refinements. We examine publicly available datasets for these tasks and introduce a new manually curated dataset for interpretation finding. To further deepen the understanding of task differences, we present a set of approaches for effectively addressing these tasks and report on experimental results.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {171–180},
numpages = {10},
keywords = {query understanding, semantic mapping, interpretation finding, entity linking},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809462,
author = {Ribas, Sabir and Ribeiro-Neto, Berthier and Santos, Rodrygo L.T. and de Souza e Silva, Edmundo and Ueda, Alberto and Ziviani, Nivio},
title = {Random Walks on the Reputation Graph},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809462},
doi = {10.1145/2808194.2809462},
abstract = {The identification of reputable entities is an important task in business, education, and many other fields. On the other hand, as an arguably subjective, multi-faceted concept, quantifying reputation is challenging. In this paper, instead of relying on a single, precise definition of reputation, we propose to exploit the transference of reputation among entities in order to identify the most reputable ones. To this end, we propose a novel random walk model to infer the reputation of a target set of entities with respect to suitable sources of reputation. We instantiate our model in an academic search setting, by modeling research groups as reputation sources and publication venues as reputation targets. By relying on publishing behavior as a reputation signal, we demonstrate the effectiveness of our model in contrast to standard citation-based approaches for identifying reputable venues as well as researchers in the broad area of computer science. In addition, we demonstrate the robustness of our model to perturbations in the selection of reputation sources. Finally, we show that effective reputation sources can be chosen via the proposed model itself in a semi-automatic fashion.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {181–190},
numpages = {10},
keywords = {academic search, reputation flows, random walks},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809458,
author = {Petersen, Casper and Lioma, Christina and Simonsen, Jakob Grue and Larsen, Birger},
title = {Entropy and Graph Based Modelling of Document Coherence Using Discourse Entities: An Application to IR},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809458},
doi = {10.1145/2808194.2809458},
abstract = {We present two novel models of document coherence and their application to information retrieval (IR). Both models approximate document coherence using discourse entities, e.g. the subject or object of a sentence. Our first model views text as a Markov process generating sequences of discourse entities (entity n-grams); we use the entropy of these entity n-grams to approximate the rate at which new information appears in text, reasoning that as more new words appear, the topic increasingly drifts and text coherence decreases. Our second model extends the work of Guinaudeau &amp; Strube [28] that represents text as a graph of discourse entities, linked by different relations, such as their distance or adjacency in text. We use several graph topology metrics to approximate different aspects of the discourse flow that can indicate coherence, such as the average clustering or betweenness of discourse entities in text. Experiments with several instantiations of these models show that: (i) our models perform on a par with two other well-known models of text coherence even without any parameter tuning, and (ii) reranking retrieval results according to their coherence scores gives notable performance gains, confirming a relation between document coherence and relevance. This work contributes two novel models of document coherence, the application of which to IR complements recent work in the integration of document cohesiveness or comprehensibility to ranking [5, 56].},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {191–200},
numpages = {10},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809455,
author = {Kim, Youngho and Croft, W. Bruce},
title = {Improving Patent Search by Search Result Diversification},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809455},
doi = {10.1145/2808194.2809455},
abstract = {Patent retrieval has some unique features relative to web search. One major task in this domain is finding existing patents that may invalidate new patents, known as prior-art or invalidity search, where search queries can be formulated from query patents (i.e., new patents). Since a patent document generally contains long and complex descriptions, generating effective search queries can be complex and difficult. Typically, these queries must cover diverse aspects of the new patent application in order to retrieve relevant documents that cover the full scope of the patent. Given this context, search diversification techniques can potentially improve the retrieval performance of patent search by introducing diversity into the document ranking. In this paper, we examine the effectiveness for patent search of a recent term-based diversification framework. Using this framework involves developing methods to identify effective phrases related to the topics mentioned in the query patent. In our experiments, we evaluate our diversification approach using standard measures of retrieval effectiveness and diversity, and show significant improvements relative to state-of-the-art baselines.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {201–210},
numpages = {10},
keywords = {search result diversification, patent retrieval, prior-art search},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809466,
author = {Rodriguez Perez, Jesus Alberto and Jose, Joemon M.},
title = {On Microblog Dimensionality and Informativeness: Exploiting Microblogs' Structure and Dimensions for Ad-Hoc Retrieval},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809466},
doi = {10.1145/2808194.2809466},
abstract = {In recent years, microblog services such as Twitter have gained increasing popularity, leading to active research on how to effectively exploit its content. Microblog documents such as tweets differ in morphology with respect to more traditional documents such as web pages. Particularly, tweets are considerably shorter (140 characters) than web documents and contain contextual tags regarding the topic (hashtags), intended audience (mentions) of the document as well as links to external content(URLs).Traditional and state of the art retrieval models perform rather poorly in capturing the relevance of tweets, since they have been designed under very different conditions. In this work, we define a microblog document as a high-dimensional entity and study the structural differences between those documents deemed relevant and those non-relevant. Secondly we experiment with enhancing the behaviour of the best observed performing retrieval model by means of a re-ranking approach that accounts for the relative differences in these dimensions amongst tweets. Additionally we study the interactions between the different dimensions in terms of their order within the documents by modelling relevant and non-relevant tweets as state machines. These state machines are then utilised to produce scores which in turn are used for re-ranking.Our evaluation results show statistically significant improvements over the baseline in terms of precision at different cut-off points for both approaches. These results confirm that the relative presence of the different dimensions within a document and their ordering are connected with the relevance of microblogs.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {211–220},
numpages = {10},
keywords = {microblog, modelling, ad-hoc retrieval, dimensions, ranking, state machine},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809474,
author = {Vuurens, Jeroen B.P. and de Vries, Arjen P. and Blanco, Roi and Mika, Peter},
title = {Online News Tracking for Ad-Hoc Information Needs},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809474},
doi = {10.1145/2808194.2809474},
abstract = {Following online news about a specific event can be a difficult task as new information is often scattered across web pages. In such cases, an up-to-date summary of the event would help to inform users and allow them to navigate to articles that are likely to contain relevant and novel details. We propose a three-step approach to online news tracking for ad-hoc information needs. First, we continuously cluster the titles of all incoming news articles. Then, we select the clusters that best fit a user's ad-hoc information need and identify salient sentences. Finally, we select sentences for the summary based on novelty and relevance to the information seen, without requiring an a-priori model of events of interest. We evaluate this approach using the 2013 TREC Temporal Summarization test set and show that compared to existing systems our approach retrieves news facts with significantly higher F-measure and Latency-Discounted Expected Gain.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {221–230},
numpages = {10},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809448,
author = {Milajevs, Dmitrijs and Sadrzadeh, Mehrnoosh and Roelleke, Thomas},
title = {IR Meets NLP: On the Semantic Similarity between Subject-Verb-Object Phrases},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809448},
doi = {10.1145/2808194.2809448},
abstract = {Measuring the semantic similarity between phrases and sentences is an important task in natural language processing (NLP) and information retrieval (IR). We compare the quality of the distributional semantic NLP models against phrase-based semantic IR. The evaluation is based on the correlation between human judgements and model scores on a distributional phrase similarity task. We experiment with four NLP and two IR model variants. On the NLP side, models vary over normalization schemes and composition operators. On the IR side, models vary with respect to estimation of the probability of a term being in a document, namely P(t|d) where only term co-occurrence information is used and P(t|d, sim) which incorporates term distributional similarity. A mixture of the two methods is presented and evaluated. For both methods, word meanings are derived from large corpora of data: the BNC and ukWaC. One of the main findings is that grammatical distributional models give better scores than the IR models. This suggests that an IR model enriched with distributional linguistic information performs better in the long standing problem in IR of document retrieval where there is no direct symbolic relationship between query and document concepts.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {231–240},
numpages = {10},
keywords = {subject-verb-object (svo) phrases, nlp, distributional semantics, semantic ir},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809450,
author = {Zhang, Lei and Tran, Thanh and Rettinger, Achim},
title = {A Theoretical Analysis of Cross-Lingual Semantic Relatedness in Vector Space Models},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809450},
doi = {10.1145/2808194.2809450},
abstract = {Semantic relatedness is essential for different text processing tasks, especially in the cross-lingual setting due to the vocabulary mismatch problem. Many concept-based solutions to semantic relatedness have been proposed, which vary in the notions of concept and document representation. In our contribution, we provide a unified model that generalizes over the existing approaches to cross-lingual semantic relatedness. It shows that the main existing solutions represent different ways for constructing the concept space, which result in different document representations and implications for semantic relatedness computation. In particular, it al- lows us to provide theoretical justifications of existing solutions. Through the experimental evaluation, we show that the results support our theoretical findings.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {241–250},
numpages = {10},
keywords = {semantic relatedness, cross-lingual, vector space models},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809453,
author = {Wang, Hong and Liu, Anqi and Wang, Jing and Ziebart, Brian D. and Yu, Clement T. and Shen, Warren},
title = {Context Retrieval for Web Tables},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809453},
doi = {10.1145/2808194.2809453},
abstract = {Many modern knowledge bases are built by extracting information from millions of web pages. Though existing extraction methods primarily focus on web pages' main text, a huge amount of information is embedded within other web structures, such as web tables. Previous studies have shown that linking web page tables and textual context is beneficial for extracting more information from web pages. However, using the text surrounding each table without carefully assessing its relevance introduces noise in the extracted information, degrading its accuracy. To the best of our knowledge, we provide the first systematic study of the problem of table-related context retrieval: given a table and the sentences within the same web page, determine for each sentence whether it is relevant to the table. We define the concept of relevance and introduce a Table-Related Context Retrieval system (TRCR) in this paper. We experiment with different machine learning algorithms, including a recently developed algorithm that is robust to biases in the training data, and show that our system retrieves table-related context with F1=0.735.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {251–260},
numpages = {10},
keywords = {covariate shift, web tables, context retrieval},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809461,
author = {Luo, Jiyun and Dong, Xuchu and Yang, Hui},
title = {Session Search by Direct Policy Learning},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809461},
doi = {10.1145/2808194.2809461},
abstract = {This paper proposes a novel retrieval model for session search. Through gradient descent, the model finds optimal policies for the best search engine actions from what is observed in the user and search engine interactions. The proposed framework applies direct policy learning to session search such that it greatly reduce the model complexity than prior work. It is also a flexible design, which includes a wide range of features describing the rich interactions in session search. The framework is shown to be highly effective evaluated on the recent TREC Session Tracks. As part of the efforts to bring reinforcement learning to information retrieval, this paper makes a novel contribution in theoretical modeling for session search.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {261–270},
numpages = {10},
keywords = {dynamic information retrieval, policy learning, session search},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809468,
author = {Luo, Jiyun and Dong, Xuchu and Yang, Hui},
title = {Learning to Reinforce Search Effectiveness},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809468},
doi = {10.1145/2808194.2809468},
abstract = {Session search is an Information Retrieval (IR) task which handles a series of queries issued for a search task. In this paper, we propose a novel reinforcement learning style information retrieval framework and develop a new feedback learning algorithm to model user feedback, including clicks and query reformulations, as reinforcement signals and to generate rewards in the RL framework. From a new perspective, we view session search as a cooperative game played between two agents, the user and the search engine. We study the communications between the two agents; they always exchange opinions on "whether the current stage of search is relevant" and "whether we should explore now." The algorithm infers user feedback models by an EM algorithm from the query logs. We compare to several state-of-the-art session search algorithms and evaluate our algorithm on the most recent TREC 2012 to 2014 Session Tracks. The experimental results demonstrates that our approach is highly effective for improving session search accuracy.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {271–280},
numpages = {10},
keywords = {reinforcement learning, dynamic information retrieval modeling, session search, stochastic game},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809454,
author = {Raiber, Fiana and Kurland, Oren and Radlinski, Filip and Shokouhi, Milad},
title = {Learning Asymmetric Co-Relevance},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809454},
doi = {10.1145/2808194.2809454},
abstract = {Several applications in information retrieval rely on asymmetric co-relevance estimation; that is, estimating the relevance of a document to a query under the assumption that another document is relevant. We present a supervised model for learning an asymmetric co-relevance estimate. The model uses different types of similarities with the assumed relevant document and the query, as well as document-quality measures. Empirical evaluation demonstrates the merits of using the co-relevance estimate in various applications, including cluster-based and graph-based document retrieval. Specifically, the resultant performance transcends that of using a wide variety of alternative estimates, mostly symmetric inter-document similarity measures that dominate past work.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {281–290},
numpages = {10},
keywords = {asymmetric co-relevance},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809464,
author = {Bennett, Paul N. and Shokouhi, Milad and Caruana, Rich},
title = {Implicit Preference Labels for Learning Highly Selective Personalized Rankers},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809464},
doi = {10.1145/2808194.2809464},
abstract = {Interaction data such as clicks and dwells provide valuable signals for learning and evaluating personalized models. However, while models of personalization typically distinguish between clicked and non-clicked results, no preference distinctions within the non-clicked results are made and all are treated as equally non-relevant.In this paper, we demonstrate that failing to enforce a prior on preferences among non-clicked results leads to learning models that often personalize with no measurable gain at the risk that the personalized ranking is worse than the non-personalized ranking. To address this, we develop an implicit preference-based framework that enables learning highly selective rankers that yield large reductions in risk such as the percentage of queries personalized. We demonstrate theoretically how our framework can be derived from a small number of basic axioms that give rise to well-founded target rankings which combine a weight on prior preferences with the implicit preferences inferred from behavioral data.Additionally, we conduct an empirical analysis to demonstrate that models learned with this approach yield comparable gains on click-based performance measures to standard methods with far fewer queries personalized. On three real-world commercial search engine logs, the method leads to substantial reductions in the number of queries re-ranked (2x - 7x fewer queries re-ranked) while maintaining 85-95% of the total gain achieved by the standard approach.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {291–300},
numpages = {10},
keywords = {personalization risk, robust algorithms, re-ranking},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809477,
author = {Lin, Jimmy and Trotman, Andrew},
title = {Anytime Ranking for Impact-Ordered Indexes},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809477},
doi = {10.1145/2808194.2809477},
abstract = {The ability for a ranking function to control its own execution time is useful for managing load, reigning in outliers, and adapting to different types of queries. We propose a simple yet effective anytime algorithm for impact-ordered indexes that builds on a score-at-a-time query evaluation strategy. In our approach, postings segments are processed in decreasing order of their impact scores, and the algorithm early terminates when a specified number of postings have been processed. With a simple linear model and a few training topics, we can determine this threshold given a time budget in milliseconds. Experiments on two web test collections show that our approach can accurately control query evaluation latency and that aggressive limits on execution time lead to minimal decreases in effectiveness.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {301–304},
numpages = {4},
keywords = {impact scores, score-at-a-time query evaluation},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809492,
author = {Wemhoener, David and Allan, James},
title = {Balancing Aspects in Retrieved Search Results},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809492},
doi = {10.1145/2808194.2809492},
abstract = {Many queries contain explicit aspects which must be balanced in any retrieved result in order to meet a user's information need: if aspects of the query are missing or disproportionately represented in documents, the results will be of lower quality than desired. This balancing thus needs to occur both within the retrieved documents individually and across the entire set. We introduce the concept of query-aspect balance and describe a new evaluation measure, β-NDCG, that allows the evaluation of query-aspect balance on multivalued query-aspect judgments. We apply β-NDCG to a small test collection and explore its utility. We show that β-NDCG-NDCG captures problems of query aspect balance within and across documents in the ranked list.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {305–308},
numpages = {4},
keywords = {effectiveness, models, aspects},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809478,
author = {Lin, Jimmy},
title = {Building a Self-Contained Search Engine in the Browser},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809478},
doi = {10.1145/2808194.2809478},
abstract = {JavaScript engines inside modern web browsers are capable of running sophisticated multi-player games, rendering impressive 3D scenes, and supporting complex, interactive visualizations. Can this processing power be harnessed for information retrieval? This paper explores the feasibility of building a JavaScript search engine that runs completely self-contained on the client side within the browser - this includes building the inverted index, gathering terms statistics for scoring, and performing query evaluation. The design takes advantage of the IndexDB API, which is implemented by the LevelDB key-value store inside Google's Chrome browser. Experiments show that although the performance of the JavaScript prototype falls far short of the open-source Lucene search engine, it is sufficiently responsive for interactive applications. This feasibility demonstration opens the door to interesting applications and architectures.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {309–312},
numpages = {4},
keywords = {javascript, indexeddb, leveldb},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809491,
author = {Diaz, Fernando},
title = {Condensed List Relevance Models},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809491},
doi = {10.1145/2808194.2809491},
abstract = {Pseudo-relevance feedback has traditionally been implemented as an expensive re-retrieval of documents from the target corpus. In this work, we demonstrate that, for high precision metrics, re-ranking the original feedback set provides nearly identical performance to re-retrieval with significantly lower latency.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {313–316},
numpages = {4},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809488,
author = {Zhang, Dell and Wang, Jun and Zhao, Xiaoxue},
title = {Estimating the Uncertainty of Average F1 Scores},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809488},
doi = {10.1145/2808194.2809488},
abstract = {In multi-class text classification, the performance (effectiveness) of a classifier is usually measured by micro-averaged and macro-averaged F1 scores. However, the scores themselves do not tell us how reliable they are in terms of forecasting the classifier's future performance on unseen data. In this paper, we propose a novel approach to explicitly modelling the uncertainty of average F1 scores through Bayesian reasoning.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {317–320},
numpages = {4},
keywords = {performance evaluation, bayesian inference, text classification},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809489,
author = {Wang, Yulu and Lin, Jimmy},
title = {The Feasibility of Brute Force Scans for Real-Time Tweet Search},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809489},
doi = {10.1145/2808194.2809489},
abstract = {The real-time search problem requires making ingested documents immediately searchable, which presents architectural challenges for systems built around inverted indexing. In this paper, we explore a radical proposition: What if we abandon document inversion and instead adopt an architecture based on brute force scans of document representations? In such a design, "indexing" simply involves appending the parsed representation of an ingested document to an existing buffer, which is simple and fast. Quite surprisingly, experiments with TREC Microblog test collections show that query evaluation with brute force scans is feasible and performance compares favorably to a traditional search architecture based on an inverted index, especially if we take advantage of vectorized SIMD instructions and multiple cores in modern processor architectures. We believe that such a novel design is worth further exploration by IR researchers and practitioners.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {321–324},
numpages = {4},
keywords = {search architectures, multi-core processors},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809494,
author = {Pal, Dipasree and Mitra, Mandar and Bhattacharya, Samar},
title = {Improving Pseudo Relevance Feedback in the Divergence from Randomness Model},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809494},
doi = {10.1145/2808194.2809494},
abstract = {In an earlier analysis of Pseudo Relevance Feedback (PRF) models by Clinchant and Gaussier (2013), five desirable properties that PRF models should satisfy were formalised. Also, modifications to two PRF models were proposed in order to improve compliance with the desirable properties. These resulted in improved retrieval effectiveness. In this study, we introduce a sixth property that we believe PRF models should satisfy. We also extend the earlier exercise to Bo1, a standard PRF model. Experimental results on the robust, wt10g and gov2 datasets show that the proposed modifications yield improvements in effectiveness.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {325–328},
numpages = {4},
keywords = {divergence from randomness, pseudo relevance feedback},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809495,
author = {Lipani, Aldo and Lupu, Mihai and Aizawa, Akiko and Hanbury, Allan},
title = {An Initial Analytical Exploration of Retrievability},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809495},
doi = {10.1145/2808194.2809495},
abstract = {We approach the problem of retrievability from an analytical perspective, starting with modeling conjunctive and disjunctive queries in a boolean model. We show that this represents an upper bound on retrievability for all other best match algorithms. We follow this with an observation of imbalance in the distribution of retrievability, using the Gini coefficient. Simulation-based experiments show the behavior of the Gini coefficient for retrievability under different types and lengths of queries, as well as different assumptions about the document length distribution in a collection.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {329–332},
numpages = {4},
keywords = {boolean model, retrievability, accessibility, gini coefficient},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809484,
author = {Voorhees, Ellen M.},
title = {On the Behavior of PRES Using Incomplete Judgment Sets},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809484},
doi = {10.1145/2808194.2809484},
abstract = {PRES, the Patent Retrieval Evaluation Score, is a family of retrieval evaluation measures that combines recall and user effort to reflect the quality of a retrieval run with respect to recall-oriented search tasks. Previous analysis of the measure was done using the test collection for the CLEF-IP 2009 track, a collection that contains a limited range of number of relevant documents, making it difficult to assess the behavior of PRES for varying recall contexts. This paper examines the effect of incomplete judgments on PRES scores using the well-studied TREC-8 ad hoc test collection, a collection with a much more varied number-of-relevants profile. Experiments with small judgment sets created through a typical collection-building process show the PRES measures are resilient to incomplete judgment sets.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {333–336},
numpages = {4},
keywords = {incomplete judgments, test collections, pres measure},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809483,
author = {Trotman, Andrew and Albert, Michael and Burgess, Blake},
title = {Optimal Packing in Simple-Family Codecs},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809483},
doi = {10.1145/2808194.2809483},
abstract = {The Simple family of codecs is popular for encoding postings lists for a search engine because they are both space effective and time efficient at decoding. These algorithms pack as many integers into a codeword as possible before moving on to the next codeword. This technique is known as left-greedy. This contribution proves that left-greedy is not optimal and then goes on to introduce a dynamic programming solution to find the optimal packing. Experiments on .gov2 and INEX Wikipedia 2009 show that although this is an interesting theoretical result, left-greedy is empirically near optimal in effectiveness and efficiency.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {337–340},
numpages = {4},
keywords = {procrastination, inverted files, compression},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809493,
author = {Baruah, Gaurav and Roegiest, Adam and Smucker, Mark D.},
title = {Pooling for User-Oriented Evaluation Measures},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809493},
doi = {10.1145/2808194.2809493},
abstract = {Traditional TREC-style pooling methodology relies on using predicted relevance by systems to select documents for judgment. This coincides with typical search behaviour (e.g., web search). In the case of temporally ordered streams of documents, the order that users encounter documents is in this temporal order and not some predetermined rank order. We investigate a user oriented pooling methodology focusing on the documents that simulated users would likely read in such temporally ordered streams. Under this user model, many of the relevant documents found in the TREC 2013 Temporal Summarization Track's pooling effort would never be read. Not only does our pooling strategy focus on pooling documents that will be read by (simulated) users, the resultant pools are different from the standard TREC pools.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {341–344},
numpages = {4},
keywords = {user models, evaluation, pooling},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809485,
author = {Zellh\"{o}fer, David},
title = {Predicting Relevance Feedback Effectiveness with the Help of the Principle of Polyrepresentation in MIR},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809485},
doi = {10.1145/2808194.2809485},
abstract = {The principle of polyrepresentation - a representative of the cognitive viewpoint on IR, takes a holistic perspective on interactive IR research.One of the principle's core hypotheses is that a document is described by different representations such as visual low-level features, textual content, or relational metadata.The conjunctive combination of these representations, the so-called cognitive overlap, is assumed to compensate the inherent insecurity in relevance assessments of documents w.r.t. an information need.Recently, the cognitively motivated principle of polyrepresentation has been shown to correlate with quantum mechanics-inspired IR models. However, the principle's effectiveness has not been examined in relevance feedback-based interactive MIR. In this work, the principle's utility is studied in interactive MIR in order to investigate whether its main hypothesis can serve as a predictor of retrieval performance during relevance feedback.In order to obtain resilient results all experiments have been carried out with 6 different standard test sets that provide evidence of the utility of the presented approach and the underlying polyrepresentative hypothesis.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {345–348},
numpages = {4},
keywords = {polyrepresentation, cognitive ir model, user simulation},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809481,
author = {Su, Wanhua and Yuan, Yan and Zhu, Mu},
title = {A Relationship between the Average Precision and the Area Under the ROC Curve},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809481},
doi = {10.1145/2808194.2809481},
abstract = {For similar evaluation tasks, the area under the receiver operating characteristic curve (AUC) is often used by researchers in machine learning, whereas the average precision (AP) is used more often by the information retrieval community. We establish some results to explain why this is the case. Specifically, we show that, when both the AUC and the AP are rescaled to lie in [0,1], the AP is approximately the AUC times the initial precision of the system.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {349–352},
numpages = {4},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809490,
author = {Tabrizi, Shayan A. and Dadashkarimi, Javid and Dehghani, Mostafa and Nasr Esfahani, Hassan and Shakery, Azadeh},
title = {Revisiting Optimal Rank Aggregation: A Dynamic Programming Approach},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809490},
doi = {10.1145/2808194.2809490},
abstract = {Rank aggregation, that is merging multiple ranked lists, is a pivotal challenge in many information retrieval (IR) systems, especially in distributed IR and multilingual IR. From the evaluation point of view, being able to calculate the upper-bound of performance of the final aggregated list lays the ground for evaluating different aggregation strategies, independently. In this paper, we propose an algorithm based on dynamic programming which, using relevancy information, obtains the aggregated list with the maximum performance that could be possibly achieved by any aggregation strategy. We also provide a detailed proof for the optimality of the result of the algorithm. Furthermore, we demonstrate that the previous proposed algorithm fails to reach the optimal result in many circumstances, due to its greedy essence.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {353–356},
numpages = {4},
keywords = {evaluation, rank aggregation, information retrieval},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809480,
author = {Minot, Ariana S. and Heier, Andrew and King, Davis and Simek, Olga and Stanisha, Nicholas},
title = {Searching for Twitter Posts by Location},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809480},
doi = {10.1145/2808194.2809480},
abstract = {The microblogging service Twitter is an increasingly popular platform for sharing information worldwide. This motivates the potential to mine information from Twitter, which can serve as a valuable resource for applications such as event localization and location-specific recommendation systems. Geolocation of Twitter messages is integral to such applications. However, only a a small percentage of Twitter posts are accompanied by a GPS location. Recent works have begun exploring ways to estimate the unknown location of Twitter users based on the content of their posts and various available metadata. This presents interesting challenges for natural language processing and multi-objective optimization. We propose a new method for estimating the home location of users based on both the content of their posts and their social connections on Twitter. Our method achieves an accuracy of 77% within 10 km in exchange for a reduction in coverage of 76% with respect to techniques which only use social connections.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {357–360},
numpages = {4},
keywords = {geolocation, twitter, social media search},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809487,
author = {Termehchy, Arash and Touri, Behrouz},
title = {A Signaling Game Approach to Databases Querying and Interaction},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809487},
doi = {10.1145/2808194.2809487},
abstract = {As most database users cannot precisely express their information needs, it is challenging for database querying and exploration interfaces to understand them. We propose a novel formal framework for representing and understanding information needs in database querying and exploration. Our framework considers querying as a collaboration between the user and the database system to establish a mutual language for representing information needs. We formalize this collaboration as a signaling game, where each mutual language is an equilibrium for the game. A query interface is more effective if it establishes a less ambiguous mutual language faster. We discuss some equilibria, strategies, and the convergence rates in this game. In particular, we propose a reinforcement learning mechanism and analyze it within our framework. We prove that this adaptation mechanism for the query interface improves the effectiveness of answering queries stochastically speaking, and converges almost surely.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {361–364},
numpages = {4},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809479,
author = {Goswami, Parantapa and Amini, Massih-Reza and Gaussier, Eric},
title = {Study of Heuristic IR Constraints Under Function Discovery Framework},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809479},
doi = {10.1145/2808194.2809479},
abstract = {In this paper we investigate the effect of the heuristic IR constraints on IR term-document scoring functions within the recently proposed function discovery framework. In the earlier study the constraints were empirically validated as a whole. Moreover, only the group of form constraints was utilized and the other prominent group, the adjustment constraints, was not considered. In this work we will investigate all the constraints individually and study them with two different term frequency normalization, namely normalization scheme used in DFR models and relative term count normalization used in language models.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {365–368},
numpages = {4},
keywords = {function discovery, ir theory, heuristic ir constraints},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809475,
author = {Makarenkov, Victor and Shapira, Bracha and Rokach, Lior},
title = {Theoretical Categorization of Query Performance Predictors},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809475},
doi = {10.1145/2808194.2809475},
abstract = {The query-performance prediction task aims at estimating the retrieval effectiveness of queries without obtaining relevance feedback from users. Most of the recently proposed predictors were empirically evaluated with various datasets to demonstrate their merits. We propose a framework for theoretical categorization and estimation of the value of query performance predictors (QPP) without empirical evaluation. We demonstrate the application of the proposed framework on four representative selected predictors and show how it emphasizes their strengths and weaknesses. The main contribution of this work is the theoretical grounded categorization of representative QPP.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {369–372},
numpages = {4},
keywords = {theoretical categoeization, query performance prediction, categorization},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809476,
author = {Liu, Xitong and Fang, Hui and Cai, Deng},
title = {Towards Less Biased Web Search},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809476},
doi = {10.1145/2808194.2809476},
abstract = {Web search engines now serve as essential assistant to help users make decisions in different aspects. Delivering correct and impartial information is a crucial functionality for search engines as any false information may lead to unwise decision and thus undesirable consequences. Unfortunately, a recent study revealed that Web search engines tend to provide biased information with most results supporting users' beliefs conveyed in queries regardless of the truth.In this paper we propose to alleviate bias in Web search through predicting the topical polarity of documents, which is the overall tendency of one document regarding whether it supports or disapproves the belief in query. By applying the prediction to balance search results, users would receive less biased information and therefore make wiser decision. To achieve this goal, we propose a novel textual segment extraction method to distill and generate document feature representation, and leverage convolution neural network, an effective deep learning approach, to predict topical polarity of documents. We conduct extensive experiments on a set of queries with medical indents and demonstrate that our model performs empirically well on identifying topical polarity with satisfying accuracy. To our best knowledge, our work is the first on investigating the mitigation of bias in Web search and could provide directions on future research.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {373–376},
numpages = {4},
keywords = {topical polarity, search bias},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809482,
author = {Di Buccio, Emanuele and Melucci, Massimo},
title = {Two Operators to Define and Manipulate Themes of a Document Collection},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809482},
doi = {10.1145/2808194.2809482},
abstract = {In this paper, we propose the theme model, which will provide the end user with join and meet operators to define and manipulate themes. These operators have properties that cannot be reduced to the classical logic operators, thus allowing the researchers to model the informative content of documents in a novel way and to rank documents in ways other than those provided by the classical logic. To this end, we introduce the main definitions and properties of the theme model and we link the model to a number of related techniques, thus suggesting how the model can be implemented and applied.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {377–380},
numpages = {4},
keywords = {multi-modal search, quantum mechanics, query language, exploratory search, abstract vector spaces},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809496,
author = {McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
title = {Using Part-of-Speech N-Grams for Sensitive-Text Classification},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809496},
doi = {10.1145/2808194.2809496},
abstract = {Freedom of Information legislations in many western democracies, including the United Kingdom (UK) and the United States of America (USA), state that citizens have typically the right to access government documents. However, certain sensitive information is exempt from release into the public domain. For example, in the UK, FOIA Exemption 27 (International Relations) excludes the release of Information that might damage the interests of the UK abroad. Therefore, the process of reviewing government documents for sensitivity is essential to determine if a document must be redacted before it is archived, or closed until the information is no longer sensitive. With the increased volume of digital government documents in recent years, there is a need for new tools to assist the digital sensitivity review process. Therefore, in this paper we propose an automatic approach for identifying sensitive text in documents by measuring the amount of sensitivity in sequences of text. Using government documents reviewed by trained sensitivity reviewers, we focus on an aspect of FOIA Exemption 27 which can have a major impact on international relations, namely, information supplied in confidence. We show that our approach leads to markedly increased recall of sensitive text, while achieving a very high level of precision, when compared to a baseline that has been shown to be effective at identifying sensitive text in other domains.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {381–384},
numpages = {4},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

@inproceedings{10.1145/2808194.2809486,
author = {Lipani, Aldo and Lupu, Mihai and Hanbury, Allan and Aizawa, Akiko},
title = {Verboseness Fission for BM25 Document Length Normalization},
year = {2015},
isbn = {9781450338332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808194.2809486},
doi = {10.1145/2808194.2809486},
abstract = {BM25 is probably the most well known term weighting model in Information Retrieval. It has, depending on the formula variant at hand, 2 or 3 parameters (k1, b, and k3). This paper addresses b - the document length normalization parameter. Based on the observation that the two cases previously discussed for length normalization (multi-topicality and verboseness) are actually three: multi-topicality, verboseness with word repetition (repetitiveness) and verboseness with synonyms, we propose and test a new length normalization method that removes the need for a b parameter in BM25. Testing the new method on a set of purposefully varied test collections, we observe that we can obtain results statistically indistinguishable from the optimal results, therefore removing the need for ground-truth based optimization.},
booktitle = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
pages = {385–388},
numpages = {4},
location = {Northampton, Massachusetts, USA},
series = {ICTIR '15}
}

