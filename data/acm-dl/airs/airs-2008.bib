@inproceedings{10.5555/1786374.1786376,
author = {Ru, Zhao and Guo, Jun and Xu, Weiran},
title = {Improving Expertise Recommender Systems by Odds Ratio},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Expertise recommenders that help in tracing expertise rather than documents start to apply some advanced information retrieval techniques. This paper introduces an odds ratio model to model expert entities for expert finding. This model applies odds ratio instead of raw probability to use language modeling techniques. A raw language model that uses prior probability for smoothing has a tendency to boost up "common" experts. In such a model the score of a candidate expert increases as its prior probability increases. Therefore, the system would trend to suggest people who have relatively large prior probabilities but not the real experts. While in the odds ratio model, such a tendency is avoided by applying an inverse ratio of the prior probability to accommodate "common" experts. The experiments on TREC test collections shows the odds ratio model improves the performance remarkably.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {1–9},
numpages = {9},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786377,
author = {Fu, Xin and Chen, Miao},
title = {Exploring the Stability of IDF Term Weighting},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {TF IDF has been widely used as a term weighting schemes in today's information retrieval systems. However, computation time and cost have become major concerns for its application. This study investigated the similarities and differences between IDF distributions based on the global collection and on different samples and tested the stability of the IDF measure across collections. A more efficient algorithm based on random samples generated a good approximation to the IDF computed over the entire collection, but with less computation overhead. This practice may be particularly informative and helpful for analysis on large database or dynamic environment like the Web.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {10–21},
numpages = {12},
keywords = {stability, term weighting, term frequency, feature oriented samples, random samples, inverse document frequency},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786378,
author = {Na, Seung-Hoon and Kang, In-Su and Lee, Ye-Ha and Lee, Jong-Hyeok},
title = {Completely-Arbitrary Passage Retrieval in Language Modeling Approach},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Passage retrieval has been expected to be an alternative method to re-solve length-normalization problem, since passages have more uniform lengths and topics, than documents. An important issue in the passage retrieval is to determine the type of the passage. Among several different passage types, the arbitrary passage type which dynamically varies according to query has shown the best performance. However, the previous arbitrary passage type is not fully examined, since it still uses the fixed-length restriction such as n consequent words. This paper proposes a new type of passage, namely completely-arbitrary passages by eliminating all possible restrictions of passage on both lengths and starting positions, and by extremely relaxing the type of the original arbitrary passage. The main advantage using completely-arbitrary passages is that the proximity feature of query terms can be well-supported in the passage retrieval, while the non-completely arbitrary passage cannot clearly support. Experimental result extensively shows that the passage retrieval using the completely-arbitrary passage significantly improves the document retrieval, as well as the passage retrieval using previous non-completely arbitrary passages, on six standard TREC test collections, in the context of language modeling approaches.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {22–33},
numpages = {12},
keywords = {passage retrieval, language modeling approach, complete-arbitrary passage},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786380,
author = {Song, He-Ping and Yang, Qun-Sheng and Zhan, Yin-Wei},
title = {Semantic Discriminative Projections for Image Retrieval},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Subspace learning has attracted much attention in image retrieval. In this paper, we present a subspace learning approach called Semantic Discriminative Projections (SDP), which learns the semantic subspace through integrating the descriptive information and discriminative information. We first construct one graph to characterize the similarity of contented-based features, another to describe the semantic dissimilarity. Then we formulate constrained optimization problem with a penalized difference form. Therefore, we can avoid the singularity problem and get the optimal dimensionality while learning a semantic subspace. Furthermore, SDP may be conducted in the original space or in the reproducing kernel Hilbert space into which images are mapped. This gives rise to kernel SDP. We investigate extensive experiments to verify the effectiveness of our approach. Experimental results show that our approach achieves better retrieval performance than state-of-art methods.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {34–43},
numpages = {10},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786381,
author = {Liu, Haiming and Song, Dawei and R\"{u}ger, Stefan and Hu, Rui and Uren, Victoria},
title = {Comparing Dissimilarity Measures for Content-Based Image Retrieval},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Dissimilarity measurement plays a crucial role in content-based image retrieval, where data objects and queries are represented as vectors in high-dimensional content feature spaces. Given the large number of dissimilarity measures that exist in many fields, a crucial research question arises: Is there a dependency, if yes, what is the dependency, of a dissimilarity measure's retrieval performance, on different feature spaces? In this paper, we summarize fourteen core dissimilarity measures and classify them into three categories. A systematic performance comparison is carried out to test the effectiveness of these dissimilarity measures with six different feature spaces and some of their combinations on the Corel image collection. From our experimental results, we have drawn a number of observations and insights on dissimilarity measurement in content-based image retrieval, which will lay a foundation for developing more effective image search technologies.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {44–50},
numpages = {7},
keywords = {content-based image retrieval, feature space, dissimilarity measure},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786382,
author = {Caicedo, Juan C. and Gonzalez, Fabio A. and Romero, Eduardo},
title = {A Semantic Content-Based Retrieval Method for Histopathology Images},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper proposes a model for content-based retrieval of histopathology images. The most remarkable characteristic of the proposed model is that it is able to extract high-level features that reflect the semantic content of the images. This is accomplished by a semantic mapper that maps conventional low-level features to high-level features using state-of-the-art machine-learning techniques. The semantic mapper is trained using images labeled by a pathologist. The system was tested on a collection of 1502 histopathology images and the performance assessed using standard measures. The results show an improvement from a 67% of average precision for the first result, using low-level features, to 80% of precision using high-level features.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {51–60},
numpages = {10},
keywords = {medical imaging, content-based image retrieval, image databases},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786384,
author = {Jiang, Eric P.},
title = {Integrating Background Knowledge into RBF Networks for Text Classification},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Text classification is a problem applied to natural language texts that assigns a document into one or more predefined categories, based on its content. In this paper, we present an automatic text classification model that is based on the Radial Basis Function (RBF) networks. It utilizes valuable discriminative information in training data and incorporates background knowledge in model learning. This approach can be particularly advantageous for applications where labeled training data are in short supply. The proposed model has been applied for classifying spam email, and the experiments on some benchmark spam testing corpus have shown that the model is effective in learning to classify documents based on content and represents a competitive alternative to the well-known text classifiers such as na\"{\i}ve Bayes and SVM.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {61–70},
numpages = {10},
keywords = {text classification, clustering, information retrieval, radial basis function networks},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786385,
author = {Xu, Yan and Wang, Bin and Li, JinTao and Jing, Hongfang},
title = {An Extended Document Frequency Metric for Feature Selection in Text Categorization},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature selection plays an important role in text categorization. Many sophisticated feature selection methods such as Information Gain (IG), Mutual Information (MI) and χ2 statistic measure (CHI) have been proposed. However, when compared to these above methods, a very simple technique called Document Frequency thresholding (DF) has shown to be one of the best methods either on Chinese or English text data. A problem is that DF method is usually considered as an empirical approach and it does not consider Term Frequency (TF) factor. In this paper, we put forward an extended DF method called TFDF which combines the Term Frequency (TF) factor. Experimental results on Reuters-21578 and OHSUMED corpora show that TFDF performs much better than the original DF method.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {71–82},
numpages = {12},
keywords = {rough set, text categorization, document frequency, feature selection},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786386,
author = {Li, Wenbo and Sun, Le and Feng, Yuanyong and Zhang, Dakun},
title = {Smoothing LDA Model for Text Categorization},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Latent Dirichlet Allocation (LDA) is a document level language model. In general, LDA employ the symmetry Dirichlet distribution as prior of the topic-words' distributions to implement model smoothing. In this paper, we propose a data-driven smoothing strategy in which probability mass is allocated from smoothing-data to latent variables by the intrinsic inference procedure of LDA. In such a way, the arbitrariness of choosing latent variables' priors for the multi-level graphical model is overcome. Following this data-driven strategy, two concrete methods, Laplacian smoothing and Jelinek-Mercer smoothing, are employed to LDA model. Evaluations on different text categorization collections show data-driven smoothing can significantly improve the performance in balanced and unbalanced corpora.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {83–94},
numpages = {12},
keywords = {latent dirichlet allocation, smoothing, text categorization, graphical model},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786388,
author = {Zhang, Yuejie and Xu, Zhiting and Zhang, Tao},
title = {Fusion of Multiple Features for Chinese Named Entity Recognition Based on CRF Model},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents the ability of Conditional Random Field (CRF) combining with multiple features to perform robust and accurate Chinese Named Entity Recognition. We describe the multiple feature templates including local feature templates and global feature templates used to extract multiple features with the help of human knowledge. Besides, we show that human knowledge can reasonably smooth the model and thus the need of training data for CRF might be reduced. From the experimental results on People's Daily corpus, we can conclude that our model is an effective pattern to combine statistical model and human knowledge. And the experiments on another data set also confirm the above conclusion, which shows that our features have consistence on different testing data.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {95–106},
numpages = {12},
keywords = {multiple features, named entity recognition, conditional random field},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786389,
author = {Wu, Chia-Wei and Tsai, Richard Tzong-Han and Hsu, Wen-Lian},
title = {Semi-Joint Labeling for Chinese Named Entity Recognition},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Named entity recognition (NER) is an essential component of text mining applications. In Chinese sentences, words do not have delimiters; thus, incorporating word segmentation information into an NER model can improve its performance. Based on the framework of dynamic conditional random fields, we propose a novel labeling format, called semi-joint labeling which partially integrates word segmentation information and named entity tags for NER. The model enhances the interaction of segmentation tags and NER achieved by traditional approaches. Moreover, it allows us to consider interactions between multiple chains in a linear-chain model. We use data from the SIGHAN 2006 NER bakeoff to evaluate the proposed model. The experimental results demonstrate that our approach outperforms state-of-the-art systems.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {107–116},
numpages = {10},
keywords = {named entity recognition, chinese word segmentation},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786390,
author = {Yan, Hongfei and Chen, Chong and Peng, Bo and Li, Xiaoming},
title = {On the Construction of a Large Scale Chinese Web Test Collection},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The lack of a large scale Chinese test collection is an obstacle to the Chinese information retrieval development. In order to address this issue, we built such a collection composed of millions of Chinese web pages, known as the Chinese Web Test collection with 100 gigabyte (CWT100g) in data volume, which is the largest Chinese web test collection as of this writing, and has been used by several dozen research groups besides being adopted in the evaluation of the SEWM-2004 Chinese Web Track[1] and the HTRDPE-2004[2]. We present the total solution for constructing a large scale test collection like the CWT100g. Further, we found that: 1) the distribution of the number of pages within sites obeys a Zipf-like law instead of a power law proposed by Adamic and Huberman [3, 4]; 2) and an appropriate filtering method on host alias will economize resources for about 25% while crawling pages. The Zipf-like law and the method of filtering host alias proposed in the paper will facilitate both to model the Web and to perfect a search engine. Finally, we report on the results of the SEWM-2004 Chinese Web Track.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {117–128},
numpages = {12},
keywords = {zipf-like law, documents, test collection},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786392,
author = {Zheng, Wei and Zhang, Yu and Hong, Yu and Fan, Jili and Liu, Ting},
title = {Topic Tracking Based on Keywords Dependency Profile},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Topic tracking is an important task of Topic Detection and Tracking (TDT). Its purpose is to detect stories, from a stream of news, related to known topics. Each topic is "known" by its association with several sample stories that discuss it. In this paper, we propose a new method to build the keywords dependency profile (KDP) of each story and track topic basing on similarity between the profiles of topic and story. In this method, keywords of a story are selected by document summarization technology. The KDP is built by keywords co-occurrence frequency in the same sentences of the story. We demonstrate this profile can describe the core events in a story accurately. Experiments on the mandarin resource of TDT4 and TDT5 show topic tracking system basing on KDP improves the performance by 13.25% on training dataset and 7.49% on testing dataset comparing to baseline.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {129–140},
numpages = {12},
keywords = {topic tracking, topic detection and tracking, keywords dependency profile, word co-occurrence},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786393,
author = {Ye, Na and Zhu, Jingbo and Zheng, Yan and Ma, Matthew Y. and Wang, Huizhen and Zhang, Bin},
title = {A Dynamic Programming Model for Text Segmentation Based on Min-Max Similarity},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Text segmentation has a wide range of applications such as information retrieval, question answering and text summarization. In recent years, the use of semantics has been proven to be effective in improving the performance of text segmentation. Particularly, in finding the subtopic boundaries, there have been efforts in focusing on either maximizing the lexical similarity within a segment or minimizing the similarity between adjacent segments. However, no optimal solutions have been attempted to simultaneously achieve maximum within-segment similarity and minimum between-segment similarity. In this paper, a domain independent model based on min-max similarity (MMS) is proposed in order to fill the void. Dynamic programming is adopted to achieve global optimization of the segmentation criterion function. Comparative experimental results on real corpus have shown that MMS model outperforms previous segmentation approaches.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {141–152},
numpages = {12},
keywords = {text segmentation, within-segment similarity, segment lengths, between-segment similarity, similarity weighting, dynamic programming},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786394,
author = {Chan, Ki and Lam, Wai},
title = {Pronoun Resolution with Markov Logic Networks},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Pronoun resolution refers to the problem of determining the coreference linkages between the antecedents and the pronouns. We propose to employ a combined model of statistical learning and first-order logic, the Markov logic network (MLN). Our proposed model can more effectively characterize the pronoun coreference resolution process that requires conducting inference upon a variety of conditions. The influence of different types of constraints are also investigated.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {153–164},
numpages = {12},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786396,
author = {Wang, Jingfan and Xia, Yunqing and Zheng, Thomas Fang and Wu, Xiaojun},
title = {Job Information Retrieval Based on Document Similarity},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Job information retrieval (IR) exhibits unique characteristics compared to common IR task. First, searching precision on job posting full text is low because job descriptions cannot be properly used in common IR methods. Second, job names semantically similar to the one mentioned in the searching query cannot be detected by common IR methods. In this paper, job descriptions are handled under a two-step job IR framework to find job postings semantically similar to seeds job posting retrieved by the common IR methods. Preliminary experiments prove that this method is effective.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {165–175},
numpages = {11},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786397,
author = {Zhang, Zhen-Xing and Lee, Sang-Hong and Lim, Joon S.},
title = {Discrimination of Ventricular Arrhythmias Using NEWFM},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The ventricular arrhythmias including ventricular tachycardia (VT) and ventricular fibrillation (VF) are life-threatening heart diseases. This paper presents a novel method for detecting normal sinus rhythm (NSR), VF, and VT from the MIT/BIH Malignant Ventricular Arrhythmia Database using the neural network with weighted fuzzy membership functions (NEWFM). This paper separates pre-processing into 2 steps. In the first step, ECG beasts are transformed by using Filtering Function [1]. In the second step, transformed ECG beasts produce 240 numbers of probability density curves and 100 points in each probability density curve using the probability density function (PDF) processing. By using three statistical methods, 19 features can be generated from these 100 points of probability density curve, which are the input data of NEWFM. The 15 generalized features from 19 PDF features are selected by non-overlap area measurement method [4]. The BSWFMs of the 15 features trained by NEWFM are shown visually. Since each BSWFM combines multiple weighted fuzzy membership functions into one using bounded sum, the 15 small-sized BSWFMs can realize NSR, VF, and VT detection in mobile environment. The accuracy rates of NSR, VF, and VT is 98.75%, 76.25%, and 63.75%, respectively.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {176–183},
numpages = {8},
keywords = {VF, weighted fuzzy membership function, fuzzy neural networks, PDF, NSR, filtering transform, feature selection, VT},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786399,
author = {Yang, Shuang-Hong and Hu, Bao-Gang},
title = {Efficient Feature Selection in the Presence of Outliers and Noises},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Although regarded as one of the most successful algorithm to identify predictive features, Relief is quite vulnerable to outliers and noisy features. The recently proposed I-Relief algorithm addresses such deficiencies by using an iterative optimization scheme. Effective as it is, I-Relief is rather time-consuming. This paper presents an efficient alternative that significantly enhances the ability of Relief to handle outliers and strongly redundant noisy features. Our method can achieve comparable performance as I-Relief and has a close-form solution, hence requires much less running time. Results on benchmark information retrieval tasks confirm the effectiveness and efficiency of the proposed method.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {184–191},
numpages = {8},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786400,
author = {Zhang, Qi and Qiu, Xipeng and Huang, Xuanjing and Wu, Lide},
title = {Domain Adaptation for Conditional Random Fields},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Conditional Random Fields (CRFs) have received a great amount of attentions in many fields and achieved good results. However, a case frequently encountered in practice is that the test data's domain is different with the training data's. It would affect negatively the performance of CRFs. This paper presents a novel technique for maximum a posteriori (MAP) adaptation of Conditional Random Fields model. The background model, which is trained on data from a domain, could be well adapted to a new domain with a small number of labeled domain specific data. Experimental results on tasks of chunking and capitalizing show that this technique can significantly improve performance on out-of-domain data. In chunking task, the relative improvement given by the adaptation technique is 56.9%. With two in-domain sentences, it also can achieve 30.2% relative improvement.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {192–202},
numpages = {11},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786401,
author = {Zhang, Qi and Zhou, Yaqian and Huang, Xuanjing and Wu, Lide},
title = {Graph Mutual Reinforcement Based Bootstrapping},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we present a new bootstrapping method based on Graph Mutual Reinforcement (GMR-Bootstrapping) to learn semantic lexicons. The novelties of this work include 1) We integrate Graph Mutual Reinforcement method with the Bootstrapping structure to sort the candidate words and patterns; 2) Pattern's uncertainty is defined and used to enhance GMR-Bootstrapping to learn multiple categories simultaneously. Experimental results on MUC4 corpus show that GMR-Bootstrapping outperforms the state-of-the-art algorithms. We also use it to extract names of automobile manufactures and models from Chinese corpus. It achieves good results too.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {203–212},
numpages = {10},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786403,
author = {Hsu, Ming-Hung and Tsai, Ming-Feng and Chen, Hsin-Hsi},
title = {Combining WordNet and ConceptNet for Automatic Query Expansion: A Learning Approach},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present a novel approach that transforms the weighting task to a typical coarse-grained classification problem, aiming to assign appropriate weights for candidate expansion terms, which are selected from WordNet and ConceptNet by performing spreading activation. This transformation benefits us to automatically combine various features. The experimental results show that our approach successfully combines WordNet and ConceptNet and improves retrieval performance. We also investigated the relationship between query difficulty and effectiveness of our approach. The results show that query expansion utilizing the two resources obtains the largest improving effect upon queries of "medium" difficulty.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {213–224},
numpages = {12},
keywords = {WordNet, query expansion, query difficulty, ConceptNet},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786404,
author = {Yang, Cheng-Zen and Chen, Ing-Xiang and Hung, Cheng-Tse and Wu, Ping-Jung},
title = {Improving Hierarchical Taxonomy Integration with Semantic Feature Expansion on Category-Specific Terms},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In recent years, the taxonomy integration problem has obtained much attention in many research studies. Many sorts of implicit information embedded in the source taxonomy are explored to improve the integration performance. However, the semantic information embedded in the source taxonomy has not been discussed in the past research. In this paper, an enhanced integration approach called SFE (Semantic Feature Expansion) is proposed to exploit the semantic information of the category-specific terms. From our experiments on two hierarchical Web taxonomies, the results are positive to show that the integration performance can be further improved with the SFE scheme.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {225–236},
numpages = {12},
keywords = {semantic feature expansion, hierarchical thesauri information, hierarchical taxonomy integration, category-specific terms},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786405,
author = {Liu, Zhizhong and Wang, Huaimin and Zhou, Bin},
title = {HOM: An Approach to Calculating Semantic Similarity Utilizing Relations between Ontologies},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In the Internet environment, ontology heterogeneity is inevitable due to many coexistent ontologies. Ontology alignment is a popular approach to resolve ontology heterogeneity. Ontology alignment establishes the relation between entities by computing their semantic similarities using local or/and non-local contexts of entities. Besides local and non-local context of entities, the relations between two ontologies are helpful for computing their semantic similarity in many situations. The aim of this article is to improve the performance of ontology alignment by using these relations in similarity computing. A hierarchical Ontology Model (HOM) which describes these relations formally is proposed followed by HOM-Matching, an algorithm based on HOM. It makes use of the relations between ontologies to compute semantic similarity. Two groups of experiments are conducted for algorithm validation and parameters optimization.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {237–245},
numpages = {9},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786407,
author = {Yuan, Song An and Yu, Song Nian},
title = {A Progressive Algorithm for Cross-Language Information Retrieval Based on Dictionary Translation},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Query translation is the mainstream in cross-language information retrieval, but ambiguity must be resolved by methods based on dictionary translation. In this paper, we propose a progressive algorithm for disambiguation which is derived from another algorithm we propose called the max-sum model. The new algorithm take a strategy called weighted-average probability distribution to redistribute the probabilities. Moreover, the new algorithm can be computed in a more direct way by solving an equation system. All the resource our method requires is a bilingual dictionary and a monolingual corpus. Experiments show it outperforms four other methods.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {246–255},
numpages = {10},
keywords = {weighted-average distribution, co-occurrence measures, max-sum model, cross-language information retrieval},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786408,
author = {Xie, Maoqiang and Liu, Jinli and Zheng, Nan and Li, Dong and Huang, Yalou and Wang, Yang},
title = {Semi-Supervised Graph-Ranking for Text Retrieval},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Much work has been done on supervised ranking for information retrieval, where the goal is to rank all searched documents in a known repository with many labeled query-document pairs. Unfortunately, the labeled pairs are lack because human labeling is often expensive, difficult and time consuming. To address this issue, we employ graph to represent pairwise relationships among the labeled and unlabeled documents, in order that the ranking score can be propagated to their neighbors. Our main contribution in this paper is to propose a semi-supervised ranking method based on graph-ranking and different weighting schemas. Experimental results show that our method called SSG-Rank on 20-newsgroups dataset outperforms supervised ranking (Ranking SVM and PRank) and unsupervised graph ranking significantly.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {256–263},
numpages = {8},
keywords = {text retrieval, graph ranking, semi-supervised ranking},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786409,
author = {Zheng, Hai-Tao and Kang, Bo-Yeong and Kim, Hong-Gee},
title = {Learnable Focused Crawling Based on Ontology},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Focused crawling is proposed to selectively seek out pages that are relevant to a predefined set of topics. Since an ontology is a well-formed knowledge representation, ontology-based focused crawling approaches have come into research. However, since these approaches apply manually predefined concept weights to calculate the relevance scores of web pages, it is difficult to acquire the optimal concept weights to maintain a stable harvest rate during the crawling process. To address this issue, we propose a learnable focused crawling approach based on ontology. An ANN (Artificial Neural Network) is constructed by using a domain-specific ontology and applied to the classification of web pages. Experiments have been performed, and the results show that our approach outperforms the breadth-first search crawling approach, the simple keyword-based crawling approach, and the focused crawling approach using only the domain-specific ontology.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {264–275},
numpages = {12},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786411,
author = {Yoshida, Minoru and Nakagawa, Hiroshi and Terada, Akira},
title = {Gram-Free Synonym Extraction via Suffix Arrays},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper proposes a method for implementing real-time synonym search systems. Our final aim is to provide users with an interface with which they can query the system for any length strings and the system returns a list of synonyms of the input string. We propose an efficient algorithm for this operation. The strategy involves indexing documents by suffix arrays and finding adjacent strings of the query by dynamically retrieving its contexts (i.e., strings around the query). The extracted contexts are in turn sent to the suffix arrays to retrieve the strings around the contexts, which are likely to contain the synonyms of the query string.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {276–285},
numpages = {10},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786412,
author = {Chen, Chien-Hsing and Hsu, Chung-Chian},
title = {Synonyms Extraction Using Web Content Focused Crawling},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Documents or Web pages collected from the World Wide Web have been considered one of the most important sources for information. Using search engines to retrieve the documents can harvest lots of information, facilitating information exchange and knowledge sharing, including foreign information. However, to better understand by local readers, foreign words, like English, are often translated to local language such as Chinese. Due to different translators and the lack of translation standard, translating foreign words may pose a notorious headache and result in different transliterations, particularly in proper nouns like person names and geographical names. For example, "Bin Laden" is translated into terms "???"(binladeng) or "???"(benladeng). Both are valid synonymous transliterations. In this research, we propose an approach to determining synonymous transliterations via mining Web pages retrieved by a search engine. Experiments show that the proposed approach can effectively extract synonymous transliterations given an input transliteration.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {286–297},
numpages = {12},
keywords = {focused crawling, speech sound comparison, associated word, transliteration, unknown words},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786413,
author = {Cao, Donglin and Liao, Xiangwen and Xu, Hongbo and Bai, Shuo},
title = {Blog Post and Comment Extraction Using Information Quantity of Web Format},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {With the development of the research on blogosphere, acquiring the post and comment from blog page becomes more important in improving the search performance. In this paper, we present a two-stage method. First, we combine the advantage of the vision information and the effective text information to locate the main text which represents the theme of blog page. Second, we use the information quantity of separator to detect the boundary between the post and comment. According to our experiments, this method achieves a good performance in extraction and improves the performance of blog search.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {298–309},
numpages = {12},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786415,
author = {Li, Jing and Sun, Le},
title = {A Lexical Chain Approach for Update-Style Query-Focused Multi-Document Summarization},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we propose a novel chain scoring (chain selecting) method to enhance the performance of Lexical Chain algorithm in query-focused summarization and present an information filtering strategy to adapt Lexical Chain method to update-style summarization. Experiments on DUC2007 datasets demonstrate the encouraging performance.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {310–320},
numpages = {11},
keywords = {update-style, lexical chain, multi-document summarization, query-focused},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786416,
author = {Zhang, Jin and Cheng, Xueqi and Xu, Hongbo},
title = {GSPSummary: A Graph-Based Sub-Topic Partition Algorithm for Summarization},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Multi-document summarization (MDS) is a challenging research topic in natural language processing. In order to obtain an effective summary, this paper presents a novel extractive approach based on graph-based sub-topic partition algorithm (GSPSummary). In particular, a sub-topic model based on graph representation is presented with emphasis on the implicit logic structure of the topic covered in the document collection. Then, a new framework of MDS with sub-topic partition is proposed. Furthermore, a novel scalable ranking criterion is adopted, in which both word based features and global features are integrated together. Experimental results on DUC2005 show that the proposed approach can significantly outperform existing approaches of the top performing systems in DUC tasks.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {321–334},
numpages = {14},
keywords = {sub-topic, graph representation, multi-document summarization},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786418,
author = {Wang, Xinying and Lv, Tianyang and Wang, Shengsheng and Wang, Zhengxuan},
title = {An Ontology and SWRL Based 3D Model Retrieval System},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {3D model retrieval is an important part of multimedia retrieval. However, the performance of traditional retrieval method of text-based and content-based is not satisfying because the keyword and the shape feature do not include the semantic information of the 3D models. The paper explores an ontology and SWRL-based 3D model retrieval system Onto3D. It can infer 3D models' semantic property by rule engine and retrieve the target models by ontology. The experiments on Princeton Shape Benchmark show that Onto3D achieves good performance not only in text retrieval but also in shape retrieval.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {335–344},
numpages = {10},
keywords = {ontology, 3D model retrieval, SWRL, clustering},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786419,
author = {Xie, Lei and Zeng, Jia and Feng, Wei},
title = {Multi-Scale TextTiling for Automatic Story Segmentation in Chinese Broadcast News},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper applies Chinese subword representations, namely character and syllable n-grams, into the TextTiling-based automatic story segmentation of Chinese broadcast news. We show the robustness of Chinese subwords against speech recognition errors, out-of-vocabulary (OOV) words and versatility in word segmentation in lexical matching on errorful Chinese speech recognition transcripts. We propose a multi-scale TextTiling approach that integrates both the specificity of words and the robustness of subwords in lexical similarity measure for story boundary identification. Experiments on the TDT2 Mandarin corpus show that subword bigrams achieve the best performance among all scales with relative f -measure improvement of 8.84% (character bigram) and 7.11% (syllable bigram) over words. Multi-scale fusion of subword bigrams with words can bring further improvement. It is promising that the integration of syllable bigram with syllable sequence of word achieves an f -measure gain of 2.66% over the syllable bigram alone.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {345–355},
numpages = {11},
keywords = {TextTiling, story segmentation, spoken document segmentation, spoken document retrieval, topic segmentation, multi-scale fusion, multimedia retrieval},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786421,
author = {Geng, Guang-Gang and Wang, Chun-Heng and Li, Qiu-Dan},
title = {Improving Spamdexing Detection via a Two-Stage Classification Strategy},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Spamdexing is any of various methods to manipulate the relevancy or prominence of resources indexed by a search engine, usually in a manner inconsistent with the purpose of the indexing system. Combating Spamdexing has become one of the top challenges for web search. Machine learning based methods have shown their superiority for being easy to adapt to newly developed spam techniques. In this paper, we propose a two-stage classification strategy to detect web spam, which is based on the predicted spamicity of learning algorithms and hyperlink propagation. Preliminary experiments on standard WEBSPAM- UK2006 benchmark show that the two-stage strategy is reasonable and effective.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {356–364},
numpages = {9},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786422,
author = {Song, Ling and Ma, Jun and Yan, Po and Lian, Li and Zhang, Dongmei},
title = {Clustering Deep Web Databases Semantically},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Deep Web database clustering is a key operation in organizing Deep Web resources. Cosine similarity in Vector Space Model (VSM) is used as the similarity computation in traditional ways. However it cannot denote the semantic similarity between the contents of two databases. In this paper how to cluster Deep Web databases semantically is discussed. Firstly, a fuzzy semantic measure, which integrates ontology and fuzzy set theory to compute semantic similarity between the visible features of two Deep Web forms, is proposed, and then a hybrid Particle Swarm Optimization (PSO) algorithm is provided for Deep Web databases clustering. Finally the clustering results are evaluated according to Average Similarity of Document to the Cluster Centroid (ASDC) and Rand Index (RI). Experiments show that: 1) the hybrid PSO approach has the higher ASDC values than those based on PSO and K-Means approaches. It means the hybrid PSO approach has the higher intra cluster similarity and lowest inter cluster similarity; 2) the clustering results based on fuzzy semantic similarity have higher ASDC values and higher RI values than those based on cosine similarity. It reflects the conclusion that the fuzzy semantic similarity approach can explore latent semantics.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {365–376},
numpages = {12},
keywords = {PSO, semantic deep web clustering, fuzzy set, K-Means, ontology},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786423,
author = {Chen, Zhi and Zhang, Li and Wang, Weihua},
title = {PostingRank: Bringing Order to Web Forum Postings},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Web forum is an important information resource. Each day innumerable postings on various topics are created in thousands of web forums in internet. However, only a small part of them are utilized for the reason that it is difficult to rank the postings importance. Unlike general web sites with hyperlinks in web pages created by editors, links in web forums are automatically generated, therefore, traditional link-based methods, such as famous PageRank are useless to rank postings. In this paper, we propose a novel algorithm named PostingRank to rank postings. The main idea of our method is to exploit the common repliers between postings and leverage the relationship between these common repliers. We build implicit links based on that co-repliers relation and construct a link graph. In the way of iterative calculation, each posting's importance score can be obtained. The experimental results demonstrate that our method can improve retrieval performance and outperforms traditional link-based methods.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {377–384},
numpages = {8},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786425,
author = {Zhang, Bangzuo and Zuo, Wanli},
title = {A Novel Reliable Negative Method Based on Clustering for Learning from Positive and Unlabeled Examples},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper investigates a new approach for training text classifiers when only a small set of positive examples is available together with a large set of unlabeled examples. The key feature of this problem is that there are no negative examples for learning. Recently, a few techniques have been reported are based on building a classifier in two steps. In this paper, we introduce a novel method for the first step, which cluster the unlabeled and positive examples to identify the reliable negative document, and then run SVM iteratively. We perform a comprehensive evaluation with other two methods, and show experimentally that it is efficient and effective.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {385–392},
numpages = {8},
keywords = {semi-supervised learning, learning from positive and unlabeled examples (LPU), bisecting k-means clustering, text classification},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786426,
author = {Qu, Chao and Li, Yong and Zhu, Jun and Huang, Peican and Yuan, Ruifen and Hu, Tianming},
title = {Term Weighting Evaluation in Bipartite Partitioning for Text Clustering},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {To alleviate the problem of high dimensions in text clustering, an alternative to conventional methods is bipartite partitioning, where terms and documents are modeled as vertices on two sides respectively. Term weighting schemes, which assign weights to the edges linking terms and documents, are vital for the final clustering performance. In this paper, we conducted an comprehensive evaluation of six variants of tf/idf factor as term weighting schemes in bipartite partitioning. With various external validation measures, we found tfidf most effective in our experiments. Besides, our experimental results also indicated that df factor generally leads to better performance than tf factor at moderate partitioning size.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {393–400},
numpages = {8},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786427,
author = {Wu, Ke and Lu, Bao-Liang},
title = {A Refinement Framework for Cross Language Text Categorization},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Cross language text categorization is the task of exploiting labelled documents in a source language (e.g. English) to classify documents in a target language (e.g. Chinese). In this paper, we focus on investigating the use of a bilingual lexicon for cross language text categorization. To this end, we propose a novel refinement framework for cross language text categorization. The framework consists of two stages. In the first stage, a cross language model transfer is proposed to generate initial labels of documents in target language. In the second stage, expectation maximization algorithm based on naive Bayes model is introduced to yield resulting labels of documents. Preliminary experimental results on collected corpora show that the proposed framework is effective.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {401–411},
numpages = {11},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786429,
author = {Fei, Yulian and Wang, Min and Chen, Wenjuan},
title = {Research on Asynchronous Communication-Oriented Page Searching},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Researches on asynchronous communication-oriented page searching aim at solving the new problems for search engine brought about by the adoption of asynchronous communication technology. At present, a full text search engine crawler mostly adopts the algorithm based on a hyperlink analysis. The crawler searches only the contents of the HTML page and ignores the codes in the script region. But it is through the script codes that asynchronous communication is realized. Since a great number of hyperlinks are hidden in the script region, it is necessary to improve the present search engine crawler to search the codes in the script region and extract the hyperlinks hidden in the script region. This paper proposes an approach, which, with the help of script code operation environment, takes advantage of the Windows message mechanism, and employs simulation clicking script function to extract hyperlinks. Meanwhile, in view of the problem that a feedback webpage is not integrated resulting from the asynchronous communication technology, this paper adopts a method that loads in the source page where hyperlinks locate and uses partial refreshing mechanism to save the refreshed page to solve the problem that information cannot be directly stored.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {412–417},
numpages = {6},
keywords = {crawler, asynchronous communication, search engine},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786430,
author = {Yin, Yingshun and Zhang, Xiaobin and Miao, Baojun and Gao, Lili},
title = {A Novel Fuzzy Kernel C-Means Algorithm for Document Clustering},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Fuzzy Kernel C-Means (FKCM) algorithm can improve accuracy significantly compared with classical Fuzzy C-Means algorithms for nonlinear separability, high dimension and clusters with overlaps in input space. Despite of these advantages, several features are subjected to the applications in real world such as local optimal, outliers, the c parameter must be assigned in advance and slow convergence speed. To overcome these disadvantages, Semi-Supervised learning and validity index are employed. Semi-Supervised learning uses limited labeled data to assistant a bulk of unlabeled data. It makes the FKCM avoid drawbacks proposed. The number of cluster will great affect clustering performance. It isn't possible to assume the optimal number of clusters especially to large text corps. Validity function makes it possible to determine the suitable number of cluster in clustering process. Sparse format, scatter and gathering strategy save considerable store space and computation time. Experimental results on the Reuters-21578 benchmark dataset demonstrate that the algorithm proposed is more flexibility and accuracy than the state-of-art FKCM.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {418–423},
numpages = {6},
keywords = {text clustering, semi-supervised learning, fuzzy kernel c-means, kernel validity index},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786431,
author = {Song, Sanming and Yang, Qunsheng and Zhan, Yinwei},
title = {Cov-HGMEM: An Improved Hierarchical Clustering Algorithm},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we present an improved method for hierarchical clustering of Gaussian mixture components derived from Hierarchical Gaussian Mixture Expectation Maximization (HGMEM) algorithm. As HGMEM performs, it is efficient in reducing a large mixture of Gaussians into a smaller mixture while still preserving the component structure of the original mode. Compared with HGMEM algorithm, it takes covariance into account in Expectation-Step without affecting the Maximization-Step, avoiding excessive expansion of some components, and we simply call it Cov-HGMEM. Image retrieval experiments indicate that our proposed algorithm outperforms previously suggested method.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {424–429},
numpages = {6},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786432,
author = {Huang, Peng and Bu, Jiajun and Chen, Chun and Liu, Kangmiao and Qiu, Guang},
title = {Improve Web Image Retrieval by Refining Image Annotations},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Automatic image annotation techniques are proposed for overcoming the so-called semantic-gap between image low-level feature and high-level concept in content-based image retrieval systems. Due to the limitations of techniques, current state-of-the-art automatic image annotation models still produce some irrelevant concepts to image semantics, which are an obstacle to getting high-quality image retrieval. In this paper we focus on improving image annotation to facilitate web image retrieval. The novelty of our work is to use both WordNet and textual information in web documents to refine original coarse annotations produced by the classic Continuous Relevance Model (CRM). Each keyword in annotations is associated with a certain weight, and larger the weight is, more related to image semantics the corresponding concept is. The experimental results show that the refined annotations improve image retrieval to some extent, compared to the original coarse annotations.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {430–435},
numpages = {6},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786433,
author = {Zhang, Xiaoyan and Wang, Ting and Chen, Huowang},
title = {Story Link Detection Based on Event Model with Uneven SVM},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Topic Detection and Tracking refers to automatic techniques for locating topically related materials in streams of data. As a core of it, story link detection is to determine whether two stories are about the same topic. Up to now, many representation models have been used in story link detection. But few of them are specific to stories. This paper proposes an event model based on the characters of stories. This model is used for story link detection and evaluated on the TDT4 Chinese corpus. The experimental results indicate that the system using the event model achieves a better performance than that using the baseline model. Furthermore, it shows a larger improvement to the former, especially when using uneven SVM as the multi-similarity integration strategy.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {436–441},
numpages = {6},
keywords = {uneven SVM, story link detection, event model},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786434,
author = {Teng, Shaohua and Tan, Wenwei},
title = {Video Temporal Segmentation Using Support Vector Machine},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A first step required to allow video indexing and retrieval of visual data is to perform a temporal segmentation, that is, to find the location of camera-shot transitions, which can be either abrupt or gradual. We adopt SVM technique to decide whether a shot transition exists or not within a given video sequence. Active learning strategy is used to accelerate training of SVM-classifiers. We also introduce a new feature description of video frame based on Local Binary Pattern (LBP). Cosine Distance is used to qualify the difference between frames in our works. The proposed method is evaluated on the TRECVID-2005 benchmarking platform and the experimental results reveal the effectiveness of the method.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {442–447},
numpages = {6},
keywords = {support vector machine, video retrieval, temporal video segmentation, shot boundary detection},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786435,
author = {Cao, Junkuo and Wu, Lide and Huang, Xuanjing and Zhou, Yaqian and Liu, Fei},
title = {Using Multiple Combined Ranker for Answering Definitional Questions},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a Multiple Combined Ranker (MCR) approach for answering definitional questions. Generally, our MCR approach first extracts question target-related knowledge as much as possible, then using this knowledge to pick up appropriate question answers. The knowledge includes both online definitions and related terms (RT). In our system, extraction of related terms is different from traditional methods which are largely based on calculating the co-occurred frequency of target words. We adopted the significance of sentences and documents, from which RT were extracted. The MCR approach shows state-in-art performance in handling with increasingly complex definitional questions},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {448–453},
numpages = {6},
keywords = {multiple combined ranker, definitional question answering, related terms extraction},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786436,
author = {Zhang, XueYing},
title = {Route Description Using Natural Language Generation Technology},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper aims to solve the problems of generating natural language route description in Chinese way-finding systems, on the basis of datasets of geographical information systems and natural language generation technology. The techniques of deriving important information e.g. paths, roads, directions and landmarks from geographical information systems are discussed in detail. Through examples we describe the construction of linguistic knowledge base for route description including the categories of Chinese terms, grammar rules and syntax schemata. The experimental output indicates that there are no more distinguishable from human route description.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {454–459},
numpages = {6},
keywords = {electronic map, route description, text planning, natural language},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786437,
author = {Song, Sanming and Yang, Qunsheng and Zhan, Yinwei},
title = {Some Question to Monte-Carlo Simulation in AIB Algorithm},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Hierarchical clustering algorithm is efficient in reducing the bytes needed to describe the original information while preserving the original information structure. Information Bottleneck (IB) theory is a hierarchical clustering framework derivative from the information theory. Agglomerative Information Bottleneck (AIB) algorithm is a suboptimal agglomerative clustering procedure designed for optimizing the original computation-exhausted IB algorithm. But the Monte-Carlo simulation formula which is widely adopted for distortion measures in AIB algorithm is problematic. This paper testified that there being a contradiction between the adopted Monte-Carlo formula and IB principle. Extending special distortion measures to common distances, the paper also present several proposals. And Experiments show their efficiency and availability.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {460–465},
numpages = {6},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786438,
author = {Kim, Youngho and Jung, Yuchul and Myaeng, Sung-Hyon},
title = {An Opinion Analysis System Using Domain-Specific Lexical Knowledge},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we describe an opinion analysis system using domain-specific lexical knowledge in Korean economic news. We tested our hypothesis that such domain-specific knowledge helps enhancing the performance of statistically based approaches and obtained a promising result.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {466–471},
numpages = {6},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786439,
author = {Du, ZhiHua and Ji, Zhen},
title = {A New Algorithm for Reconstruction of Phylogenetic Tree},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The abstract should summarize the contents of the paper and should contain at least 70 and at most 150 words. It should be set in 9-point font size and should be inset 1.0 cm from the right and left margins. There should be two blank (10-point) lines before and after the abstract. This document is in the required format. In this paper, we present a new algorithm for reconstructing large phylogenetic tree. This algorithm is based on a family of Disk-Covering Methods (DCMs) which are divide-and-conquer techniques by dividing input dataset into smaller overlapping subset, constructing phylogenetic trees separately using some base methods and merging these subtrees into a single one. Provided the high memory efficiency of RAxML (which the program inherited from fastDNAml) compared to other programs and the good performance on largereal-world data it appears to be best-suited for use as the base method. The experiments clearly show that the proposed algorithm improves over stand-alone RAxML on all datasets, i.e. yields better likelihood values than RAxML in the same amount of time. This results serve as an argument for the choice of the proposed algorithm instead of stand-alone RAxML.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {472–477},
numpages = {6},
keywords = {phylogenetic tree, DCM, divide-and-conquer},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786440,
author = {Zhu, Kunpeng and Xu, Zhiming and Wang, Xiaolong and Zhao, Yuming},
title = {A Full Distributed Web Crawler Based on Structured Network},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Distributed Web crawlers have recently received more and more attention from researchers. Full decentralized crawler without a centralized managing server seems to be an interesting architectural paradigm for realizing large scale information collecting systems for its scalability, failure resilience and increased autonomy of nodes. This paper provides a novel full distributed Web crawler system which is based on structured network, and a distributed crawling model is developed and applied in it which improves the performance of the system. Some important issues such as assignment of tasks, solution of scalability have been discussed. Finally, an experimental study is used to verify the advantages of system, and the results are comparatively satisfying.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {478–483},
numpages = {6},
keywords = {structured network, full distributed, web crawling},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786441,
author = {Kang, Zhiming and Chen, Chun and Bu, Jiajun and Huang, Peng and Qiu, Guang},
title = {A Simulated Shallow Dependency Parser Based on Weighted Hierarchical Structure Learning},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In the past years much research has been done on data-driven dependency parsing and performance has increased steadily. Dependency grammar has an important inherent characteristic, that is, the nodes closer to root usually make more contribution to audiences than the others. However, that is ignored in previous research in which every node in a dependency structure is considered to play the same role. In this paper a parser based on weighted hierarchical structure learning is proposed to simulate shallow dependency parsing, which has the preference for nodes closer to root during learning. The experimental results show that the accuracies of nodes closer to root are improved at the cost of a little decrease of accuracies of nodes far from root.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {484–489},
numpages = {6},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786442,
author = {Suo, Hongguang and Nie, Kunming and Sun, Xin and Wang, Yuwei},
title = {One Optimized Choosing Method of K-Means Document Clustering Center},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A center choice method based on sub-graph division is presented. After constructing the similarity matrix, the disconnected graphs can be established taking the text node as the vertex of the graph and then it will be analyzed. The number of the clustering center and the clustering center can be confirmed automatically on the error allowable range by this method. The noise data can be eliminated effectively in the process of finding clustering center. The experiment results of the two documents show that this method is effective. Compared with the tradition methods, F-Measure is increased by 8%.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {490–495},
numpages = {6},
keywords = {sub-graph division, document clustering, initial center, k-means},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786443,
author = {Hoang, Linh and Lee, Jung-Tae and Song, Young-In and Rim, Hae-Chang},
title = {A Model for Evaluating the Quality of User-Created Documents},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we propose a model for evaluating the quality of general user-created documents. The model is based on supervised classification approach, in which output scores are considered as quality of given document. In order to utilize both textual and nontextual attributes of documents, we incorporated a number of objectively measurable, real-valued features selected upon predefined criteria for quality. Experiments on two datasets of real world documents show that textual features are stable indicators for evaluating documents' quality. Some features are inferred to be effective for general kinds of documents.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {496–501},
numpages = {6},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786444,
author = {Wang, Min and Fei, Yulian},
title = {Filter Technology of Commerce-Oriented Network Information},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {With the network information growing day by day, people engaging in commercial affairs are crying for a commerce-oriented search engine. The primary step of building up the search engine is to get commercial information efficiently from Internet. This paper introduces a method used to filter commerce-oriented information from Internet. By this method, Spider decides the passing orientation by judging whether the hyperlink is relevant to commercial affairs. In the experiments, we used word-filtering technology to optimize the program and use the thread pool to improve the performance.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {502–507},
numpages = {6},
keywords = {LSA theory, vertical search engine, commerce-oriented spider},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786445,
author = {Yoshioka, Masaharu},
title = {IR Interface for Contrasting Multiple News Sites},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In order to utilize news articles from multiple news sites, it is better to understand the characteristics of each news site. In this paper, a concept of contrast set mining is applied for analyzing the characteristic difference between each news site and all others. The News Site Contrast (NSContrast) system is also proposed based on this mining technique. This system is applied to a news article database constructed from multiple news sites from different countries in order to evaluate its effectiveness.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {508–513},
numpages = {6},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786446,
author = {Mortensen, Magnus and Gurrin, Cathal and Johansen, Dag},
title = {Real-World Mood-Based Music Recommendation},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present a music recommendation system that incorporates both collaborative filtering and mood-based recommendations. The benefits of incorporating mood-based recommendations over both content/genre-based and collaborative filtering-based recommendation are illustrated by means of a real-world user evaluation in which 54 users took part in a one month long evaluation.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {514–519},
numpages = {6},
keywords = {collaborative filtering, recommendation, music recommendation, mood, content filtering},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786447,
author = {Wang, Yong and Liu, Yiqun and Zhang, Min and Ma, Shaoping},
title = {News Page Discovery Policy for Instant Crawlers},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Many news pages which are of high freshness requirements are published on the internet every day. They should be downloaded immediately by instant crawlers. Otherwise, they will become outdated soon. In the past, instant crawlers only downloaded pages from a manually generated news website list. Bandwidth is wasted in downloading non-news pages because news websites do not publish news pages exclusively. In this paper, a novel approach is proposed to discover news pages. This approach includes seed selection and news URL prediction based on user behavior analysis. Empirical studies in a user access log for two months show that our approach outperforms the traditional approach in both precision and recall.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {520–525},
numpages = {6},
keywords = {user behavior analysis, web log, news page discovery},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786448,
author = {Kim, Seokhwan and Jeong, Minwoo and Lee, Gary Geunbae and Ko, Kwangil and Lee, Zino},
title = {An Alignment-Based Approach to Semi-Supervised Relation Extraction Including Multiple Arguments},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present an alignment-based approach to semi-supervised relation extraction task including more than two arguments. We concentrate on improving not only the precision of the extracted result, but also on the coverage of the method. Our relation extraction method is based on an alignment-based pattern matching approach which provides more flexibility of the method. In addition, we extract all relationships including two or more arguments at once in order to obtain the integrated result with high quality. We present experimental results which indicate the effectiveness of our method.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {526–536},
numpages = {11},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786449,
author = {Gurrin, Cathal and Smeaton, Alan F. and Byrne, Daragh and O'Hare, Neil and Jones, Gareth J. F. and O'Connor, Noel},
title = {An Examination of a Large Visual Lifelog},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Lifelogging is the act of recording some aspect of your life in digital format. A basic and common form of lifelogging is the creation and maintenance of blogs, which are typically textual in nature, though often with multi-media elements. In this paper we are concerned with visual lifelogging, a new form of lifelogging based on the passive capture of photos of a person's experiences. We examine the nature of visual lifelogs, and the differences between visual lifelog photos and explicitly captured digital photos. This is done by examining a million lifelog photos encompassing a year of a visual lifelog from the life of one individual.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {537–542},
numpages = {6},
keywords = {passive capture, lifelogging, visual lifelog, photograph, SenseCam},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786450,
author = {Zheng, Xu-Ling and Zhou, Chang-Le and Shi, Xiao-Dong and Li, Tang-Qiu and Chen, Yi-Dong},
title = {Automatic Acquisition of Phrase Semantic Rule for Chinese},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The semantic collocations play important roles in parsing Chinese phrases. They are useful for both semantic disambiguation and structural disambiguation. In this paper, a representation of phrase semantic rules for Chinese is presented to formulate such knowledge, and a corpus-based method was proposed to acquire phrase semantic rules from a Chinese phrase corpus annotated with syntactic and semantic information. The method includes a metarule-guided algorithm for mining cross-level association rules to acquire phrase semantic rules and an optimization algorithm to filter these rules. The experiment results showed the effectiveness of the method. Disambiguation performance of these resulting rules was quiet well.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {543–548},
numpages = {6},
keywords = {semantic rules, association rules, corpus, HowNet},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786451,
author = {Cai, Jihong and Song, Fei},
title = {Maximum Entropy Modeling with Feature Selection for Text Categorization},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Maximum entropy provides a reasonable way of estimating probability distributions and has been widely used for a number of language processing tasks. In this paper, we explore the use of different feature selection methods for text categorization using maximum entropy modeling. We also propose a new feature selection method based on the difference between the relative document frequencies of a feature for both relevant and irrelevant classes. Our experiments on the Reuters RCV1 data set show that our own feature selection performs better than the other feature selection methods and maximum entropy modeling is a competitive method for text categorization.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {549–554},
numpages = {6},
keywords = {text categorization, feature selection, maximum entropy modeling},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786452,
author = {Liu, Wuying and Wang, Ting},
title = {Active Learning for Online Spam Filtering},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Spam filtering is defined as a task trying to label emails with spam or ham in an online situation. The online feature requires the spam filter has a strong timely generalization and has a high processing speed. Machine learning can be employed to fulfill the two requirements. In this paper, we propose a SVMEL (SVM Ensemble Learning) method to combine five simple filters for higher accuracy and an active learning method to choose training emails for less training time. The experiments results show the filter applying active learning method can reduce requirements of labeled training emails and reach steady-state performance more quickly.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {555–560},
numpages = {6},
keywords = {spam filtering, active learning, SVM, ensemble learning, machine learning},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786453,
author = {Li, Junhui and Zhou, Guodong and Zhu, Qiaoming and Qian, Peide},
title = {Syntactic Parsing with Hierarchical Modeling},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper proposes a hierarchical model to parse both English and Chinese sentences. This is done by iteratively constructing simple constituents first, so that complex ones could be detected reliably with richer contextual information in the following processes. Evaluation on the Penn WSJ Treebank and the Penn Chinese Treebank using maximum entropy models shows that our method can achieve a good performance with more flexibility for future improvement.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {561–566},
numpages = {6},
keywords = {POStagging, syntactic parsing, hierarchical modeling},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786454,
author = {Hu, Yongwei and Sui, Zhifang},
title = {Extracting Hyponymy Relation between Chinese Terms},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper studies the problem of the automatic acquisition of the hyponymy (is-a) relation in sentences and develops a new method for it. In this paper, we treat the task of identifying hyponymy relation as two separate problems and solve them based on the following three techniques: term type's commonality, sequential patterns, property nouns and domain verbs.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {567–572},
numpages = {6},
keywords = {hyponymy, pattern-based, domain verb, property noun, relation extraction, sequential pattern},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786455,
author = {Zhang, Hui and Zhao, Liping and Liu, Rui and Wang, Deqing},
title = {A No-Word-Segmentation Hierarchical Clustering Approach to Chinese Web Search Results},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we present a No-Word-Segmentation Hierarchical Clustering Approach (NWSHCA) to Chinese Web search results. The approach uses a new similarity measure between two documents based on a variation of the Edit Distance, and then it generates preliminary clusters using a partitioning clustering method. Next it ranks all common substring in a cluster using a cluster-discriminative metric with the top K as cluster description labels. Finally it uses HAC to cluster the top K cluster labels to form a navigational tree. NWSHCA can generate overlapping clusters contrast to most clustering algorithms. Experimental results show that the approach is feasible and effective.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {573–577},
numpages = {5},
keywords = {edit distance, no-word segmentation, hierarchical clustering, Chinese web search results},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786456,
author = {Chao, Wen-Han and Li, Zhou-Jun},
title = {Similar Sentence Retrieval for Machine Translation Based on Word-Aligned Bilingual Corpus},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we present a novel method to retrieve the similar examples from the corpus when given an input which should be translated, in which we use the word alignment between the bilingual sentence pair to measure the similarity of the input and the example.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {578–585},
numpages = {8},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786457,
author = {Ding, Fan and Wang, Bin},
title = {An Axiomatic Approach to Exploit Term Dependencies in Language Model},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {One of important problems of dependence retrieval model is the challenge to integrate both single words and dependencies in one weighting schema. Although there are many retrieval models that exploit term dependencies in language modeling framework, seldom of them simultaneously study the problem on different query types, e.g., short queries and verbose queries. In this paper, we derive an axiomatic dependence model by defining several basic desirable constraints that a retrieval model should meet. The experiment results show that our model significantly and robustly improves retrieval accuracy over the baseline (unigram model) in verbose queries and achieves better performance than some state-of-art dependence models.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {586–591},
numpages = {6},
keywords = {language model, term dependency, axiomatic approach, retrieval heuristics},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786458,
author = {Wang, Xiuhong and Ju, Shiguang and Wu, Shengli},
title = {A Survey of Chinese Text Similarity Computation},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {There is not a natural delimiter between words in Chinese texts. Moreover, Chinese is a semotactic language with complicated structures focusing on semantics. Its differences from Western languages bring more difficulties in Chinese word segmentation and more challenges in Chinese natural language understanding. How to compute the Chinese text similarity with high precision, recall and low cost is a very important but challenging task. Many researchers have studied it for long time. In this paper, we examine existing Chinese text similarity measures, including measures based on statistics and semantics. Our work provides insights into the advantages and disadvantages of each method, including tradeoffs between effectiveness and efficiency. New directions of the future work are discussed.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {592–597},
numpages = {6},
keywords = {similarity algorithm, Chinese text similarity, Chinese information processing},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786459,
author = {Huang, Ruihong and Sun, Le and Feng, Yuanyong},
title = {Study of Kernel-Based Methods for Chinese Relation Extraction},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we mainly explore the effectiveness of two kernel-based methods, the convolution tree kernel and the shortest path dependency kernel, in which parsing information is directly applied to Chinese relation extraction on ACE 2007 corpus. Specifically, we explore the effect of different parse tree spans involved in convolution kernel for relation extraction. Besides, we experiment with composite kernels by combining the convolution kernel with feature-based kernels to study the complementary effects between tree kernel and flat kernels. For the shortest path dependency kernel, we improve it by replacing the strict same length requirement with finding the longest common subsequences between two shortest dependency paths. Experiments show kernel-based methods are effective for Chinese relation extraction.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {598–604},
numpages = {7},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786460,
author = {Li, Yanpeng and Lin, Hongfei and Yang, Zhihao},
title = {Enhancing Biomedical Named Entity Classification Using Terabyte Unlabeled Data},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a semi-supervised learning method to enhance biomedical named entity classification using features generated from labeled and terabyte unlabeled data, called Feature Coupling Degree (FCD) features. Highly discriminative context words are obtained from labeled free text using Chi-square method and queries formed by combining the named entity and context words are retrieved by search engine. Then the retrieved web page counts are converted into binary features by discretization. We investigate the effect of this type of feature in a biomedical corpus generated from several online resources. Support Vector Machine (SVM) is used as classifier and the performances of different features with various kernels and discretization methods are compared. The results show that the method enhances the classification performance especially for Out-of-Vocabulary (OOV) terms and relative small size of training data. In addition, only using FCD features with polynomial kernels, the performance is competitive to classical features.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {605–612},
numpages = {8},
keywords = {classification, SVM, discretization, semi-supervised learning, polynomial kernel, biomedical named entity},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786461,
author = {Chen, Zhumin and Ma, Jun and Han, Xiaohui and Zhang, Dongmei},
title = {An Effective Relevance Prediction Algorithm Based on Hierarchical Taxonomy for Focused Crawling},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {How to give a formal description for a user's interested topic and predict the relevance of unvisited pages to the given topic effectively is a key issue in the design of focused crawlers. However, almost all previous known focused crawlers do the Relevance Predication based on the Flat Information (RPFI) of topic only, i.e. regardless of the context between keywords or topics. In this paper, we first introduce an algorithm to map the topic described in a keyword set or a document written in natural language text to those described in hierarchical topic taxonomy. Then, we propose a novel approach to do the Relevance Predication based on the Hierarchical Context Information (RPHCI) of the taxonomy. Experiments show that the focused crawler based on RPHCI can obtain significantly higher efficiency than those based on RPFI.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {613–619},
numpages = {7},
keywords = {topic description, hierarchical topic taxonomy, focused crawling, relevance prediction},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786462,
author = {Sun, Bin and Liu, Pei and Zheng, Ying},
title = {Short Query Refinement with Query Derivation},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we present a method for short query refinement. The method includes a query retrieval model that constructs multiple derived queries for the user's query, where derived queries denote a set of queries that are closely related to the query submitted by the user. Derived queries can be efficiently obtained using the indexing and retrieval of a small-unit index, which has index terms that are commonly used words and word senses. Each of the derived queries can be associated with a rank value according to its similarity to the user's search query. Derived queries are useful for improving the current query refinement method and for constructing the final results.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {620–625},
numpages = {6},
keywords = {derived query, word sense, short query problem},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786463,
author = {Na, Seung-Hoon and Kang, In-Su and Lee, Ye-Ha and Lee, Jong-Hyeok},
title = {Applying Completely-Arbitrary Passage for Pseudo-Relevance Feedback in Language Modeling Approach},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Different from the traditional document-level feedback, passage-level feedback restricts the context of selecting relevant terms to a passage in a document, rather than to the entire document. It can thus avoid the selection of nonrelevant terms from non-relevant parts in a document. The most recent work of passage-level feedback has been investigated from the viewpoint of the fixed-window type of passage. However, the fixed-window type of passage has limitation in optimizing the passage-level feedback, since it includes a query-independent portion. To minimize the query-independence of the passage, this paper proposes a new type of passage, called completely-arbitrary passage. Based on this, we devise a novel two-stage passage feedback - which consists of passage-retrieval and passage-extension as sub-steps, unlike previous single-stage passage feedback relying only on passage retrieval. Experimental results show that the proposed two-stage passage-level feedback much significantly improves the document-level feedback than the single-stage passage feedback that uses the fixed-window type of passage.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {626–631},
numpages = {6},
keywords = {language modeling approach, completely-arbitrary passage, pseudo-relevance feedback, passage-level feedback},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786464,
author = {Hao, Tianyong and Song, Wanpeng and Hu, Dawei and Liu, Wenyin},
title = {Automatic Generation of Semantic Patterns for User-Interactive Question Answering},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {An automatic method for generation of semantic patterns from free-text questions is proposed in this paper. An evaluation method is also proposed to estimate the suitability of the generated patterns and implemented in our user-interactive question answering (QA) system. Experiments with 5500 questions show that 63.9% generated patterns are satisfactory in the average.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {632–637},
numpages = {6},
keywords = {semantic pattern, question answering, tagger ontology},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786465,
author = {K\"{o}se, Cemal and \"{O}zyurt, \"{O}zcan and undefinedkibaundefined, Cevat},
title = {A Comparison of Textual Data Mining Methods for Sex Identification in Chat Conversations},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Mining textual data in chat mediums is becoming more important because these mediums contain a vast amount of information, which is potentially relevant to a society's current interests, habits, social behaviors, crime tendency and other tendencies. Here, sex identification is taken as a base study in information mining in chat mediums. In order to do this, a simple discrimination function and semantic analysis method are proposed for sex identification in Turkish chat mediums. Then, the proposed sex identification method is compared with the Support Vector Machine (SVM) and Naive Bayes (NB) methods. Finally, results show that the proposed system has achieved accuracy over 90% in sex identification.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {638–643},
numpages = {6},
keywords = {information extraction, sex identification, mining chat conversations, text mining, machine learning},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786466,
author = {Lee, Sang H. and Park, Hyunjeong and Park, Wook Je},
title = {Similarity Computation between Fuzzy Set and Crisp Set with Similarity Measure Based on Distance},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The computation procedure of similarity between fuzzy set and crisp set is derived. The proposed similarity measure is constructed through distance measure. And our results are compared with those of previous similarity which is based on fuzzy number.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {644–649},
numpages = {6},
keywords = {similarity measure, fuzzy entropy, distance measure},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786467,
author = {Liu, Qian and Jiao, Hui and Jia, Hui-bo},
title = {Experimental Study of Chinese Free-Text IE Algorithm Based on W<sub>CA</sub>-Selection Using Hidden Markov Model},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper proposes the extraction task of the Chinese Sci-tech journal text and presents a WCA-Selection Chinese free-text HMM IE algorithm. The HMM IE algorithm takes the Chinese Sci-tech journal abstract text as the extraction text. According to the features of WCA, an idea of WCA selection model re-optimization is proposed. And a WCA selection optimization strategy is concreted. Then the experimental verification is conducted with a satisfied result. The experiment results show that the designed extraction algorithm and WCA selection optimization strategy have good performance in the the Chinese Sci-tech journal abstract text.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {650–655},
numpages = {6},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786468,
author = {Ma, Jun and Chen, Zhumin and Lian, Li and Li, Lianxia},
title = {Finding and Using the Content Texts of HTML Pages},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A novel algorithm to find the content text in an HTML page is proposed based on a number of features of textual blocks in the page. Experiments show the new algorithm is better than known ones in terms of the ratios of the correctly removed noise blocks and the correctly found content blocks respectively. The application of the algorithm in hidden web classification is demonstrated as well.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {656–662},
numpages = {7},
keywords = {content extraction, page segmentation, page clearning},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786469,
author = {Zhang, Chunxia and Cao, Cungen and Niu, Zhendong and Yang, Qing},
title = {A Transformation-Based Error-Driven Learning Approach for Chinese Temporal Information Extraction},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Temporal information processing plays an important role in many application areas such as information retrieval, question answering, machine translation, and text summarization. This paper proposes a transformation-based error-driven learning approach to extracting temporal expressions from Chinese unstructured texts. The temporal expression annotator used in the approach is developed based on a Chinese time ontology, which includes concepts of temporal expressions and their taxonomical relations. Experiments in three domains show that our algorithm obtained promising results.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {663–669},
numpages = {7},
keywords = {Chinese temporal expressions, temporal information extraction, transformation-based error-driven learning},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786470,
author = {Hung-Yu, Kao and Hsin-Wei, Hsiao and Chih-Lu, Lin and Chia-Chun, Shih and Tse-Ming, Tsai},
title = {An Entropy-Based Hierarchical Search Result Clustering Method by Utilizing Augmented Information},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Because of the improvement of the technology of search engines, and the massively increase of the number of web pages, the results returned by the search engines are always mixed and disordered. Especially for the queries with multiple topics, the mixed and disorderly situation of the search results would be more obvious. The search engines can return information of several hundred to thousand of the pages' titles, snippets and URLs. Almost all of the technologies about search result clustering must attain further information from the contents of the returned lists. However, long execution time is not permitted for a real-time clustering system.In this paper we propose some methods with better efficiency to improve the previous technologies. We utilize and augment information that search engines returned and use entropy calculation to attain the term distribution in snippets. We also propose several new methods to attain better clustered search results and reduce execution time. Our experiments indicate that these proposed methods obtain the better clustered results.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {670–675},
numpages = {6},
keywords = {clustering, snippet, search engine, augmented information, entropy},
location = {Harbin, China},
series = {AIRS'08}
}

@inproceedings{10.5555/1786374.1786471,
author = {Khoo, Christopher S. G. and Na, Jin-Cheon and Wang, Wei},
title = {Pattern Mining for Information Extraction Using Lexical, Syntactic and Semantic Information: Preliminary Results},
year = {2008},
isbn = {3540686339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A method is being developed to mine a text corpus for candidate linguistic patterns for information extraction. The candidate patterns can be used to improve the quality of extraction patterns constructed by a pseudosupervised learning method--an automated method in which the system is provided with a high quality seed pattern or clue, which is used to generate a training set automatically. The study is carried out in the context of developing a system to extract disease-treatment information from medical abstracts retrieved from the Medline database. In an earlier study, the Apriori algorithm had been used to mine a sample of sentences containing a disease concept and a drug concept, to identify frequently occurring word patterns to see if these patterns could be used to identify treatment relations in text. Word patterns and statistical association measures alone were found to be insufficient for generating good extraction patterns, and need to be combined with syntactic and semantic constraints. In this study, we explore the use of syntactic, semantic and lexical constraints to improve the quality of extraction patterns.},
booktitle = {Proceedings of the 4th Asia Information Retrieval Conference on Information Retrieval Technology},
pages = {676–681},
numpages = {6},
keywords = {apriori algorithm, information extraction, pattern mining},
location = {Harbin, China},
series = {AIRS'08}
}

