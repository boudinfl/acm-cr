@inproceedings{10.1145/2810133.2810139,
author = {Van Gysel, Christophe and de Rijke, Maarten and Worring, Marcel},
title = {Semantic Entities},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810139},
doi = {10.1145/2810133.2810139},
abstract = {Entity retrieval has seen a lot of interest from the research community over the past decade. Ten years ago, the expertise retrieval task gained popularity in the research community during the TREC Enterprise Track [10]. It has remained relevant ever since, while broadening to social media, to tracking the dynamics of expertise [1-5, 8, 11], and, more generally, to a range of entity retrieval tasks.In the talk, which will be given by the second author, we will point out that existing methods to entity or expert retrieval fail to address key challenges: (1) Queries and expert documents use different representations to describe the same concepts [6, 7]. Term mismatches between entities and experts [7] occur due to the inability of widely used maximum-likelihood language models to make use of semantic similarities between words [9]. (2) As the amount of available data increases, the need for more powerful approaches with greater learning capabilities than smoothed maximum-likelihood language models is obvious [13]. (3) Supervised methods for entity or expertise retrieval [5, 8] were introduced at the turn of the last decade. However, the acceleration of data availability has the major disadvantage that, in the case of supervised methods, manual annotation efforts need to sustain a similar order of growth. This calls for the further development of unsupervised methods. (4) According to some entity or expertise retrieval methods, a language model is constructed for every document in the collection. These methods lack efficient query capabilities for large document collections, as each query term needs to be matched against every document [2].In the talk we will discuss a recently proposed solution [12] that has a strong emphasis on unsupervised model construction, efficient query capabilities and, most importantly, semantic matching between query terms and candidate entities. We show that the proposed approach improves retrieval performance compared to generative language models mainly due to its ability to perform semantic matching [7]. The proposed method does not require any annotations or supervised relevance judgments and is able to learn from raw textual evidence and document-candidate associations alone.The purpose of the proposal is to provide insight in how we avoid explicit annotations and feature engineering and still obtain semantically meaningful retrieval results. In the talk we will provide a comparative error analysis between the proposed semantic entity retrieval model and traditional generative language models that perform exact matching, which yields important insights in the relative strengths of semantic matching and exact matching for the expert retrieval task in particular and entity retrieval in general.We will also discuss extensions of the proposed model that are meant to deal with scalability and dynamic aspects of entity and expert retrieval.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {1–2},
numpages = {2},
keywords = {distributional semantics, entity search},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810140,
author = {Kelcey, Matthew},
title = {Open and Closed Schema for Aligning Knowledge and Text Collections},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810140},
doi = {10.1145/2810133.2810140},
abstract = {When it comes to knowledge bases most people's first thought are structured sources such as Freebase/Wikidata and their relationship to similarly structured web sources such as Wikipedia. A lot of additional and interesting "knowledge" though is captured in unstructured databases constructed in a less supervised manner using open information extraction techniques. In this talk we'll discuss some of the differences between open/closed schema knowledge bases including the ideas of objective vs subjective content as well as freshness and trust. We'll give an overview on approaches to aligning such data sources in a way that their relative strengths can be combined and finish with applications of such alignments; particularly around open question and answer systems.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {3},
numpages = {1},
keywords = {knowledge bases, semantic annotation, information extraction, question answering},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810137,
author = {Soualah-Alila, Fayrouz and Faucher, Cyril and Bertrand, Fr\'{e}d\'{e}ric and Coustaty, Micka\"{e}l and Doucet, Antoine},
title = {Applying Semantic Web Technologies for Improving the Visibility of Tourism Data},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810137},
doi = {10.1145/2810133.2810137},
abstract = {Tourism industry is an extremely information-intensive, complex and dynamic activity. It can benefit from semantic Web technologies, due to the significant heterogeneity of information sources and the high volume of on-line data. The management of semantically diverse annotated tourism data is facilitated by ontologies that provide methods and standards, which allow flexibility and more intelligent access to on-line data. This paper provides a description of some of the early results of the Tourinflux project which aims to apply semantic Web technologies to support tourist actors in effectively finding and publishing information on the Web.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {5–10},
numpages = {6},
keywords = {mapping, schema.org, tourinfrance, ontology, tourinflux},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810134,
author = {Moreira, C\'{a}tia and Taborda, Jo\~{a}o and Del Gaudio, Rosa and dos Santos, Lara and Pereira, Paulo},
title = {Contextualizing Data on a Content Management System},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810134},
doi = {10.1145/2810133.2810134},
abstract = {Content Management Systems (CMSs) are known for their ability for storing data, both structured and non-structured data. However they are not able to associate meaning and context to the stored information. Furthermore, these systems do not meet the needs and expectations of their users, because as the size of data increases, the system loses its capacity of retrieving meaningful results.In order to overcome this issue, we propose a method to implement data contextualization on a CMS. The proposed method consists of enriching the data with semantic information, allowing a more accurate retrieval of results. The implementation of this approach was validated by applying this contextualization method to a currently used CMS with real information. With this improved CMS, it is expected that the users will be able to retrieve data related to their initial search.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {11–19},
numpages = {9},
keywords = {cms, language detection, query expansion, content management system, text extraction, contextualization},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810136,
author = {Chen, Ruey-Cheng and Spina, Damiano and Croft, W. Bruce and Sanderson, Mark and Scholer, Falk},
title = {Harnessing Semantics for Answer Sentence Retrieval},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810136},
doi = {10.1145/2810133.2810136},
abstract = {Finding answer passages from the Web is a challenging task. One major difficulty is to retrieve sentences that may not have many terms in common with the question. In this paper, we experiment with two semantic approaches for finding non-factoid answers using a learning-to-rank retrieval setting. We show that using semantic representations learned from external resources such as Wikipedia or Google News may substantially improve the quality of top-ranked retrieved answers.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {21–27},
numpages = {7},
keywords = {answer finding, sentence retrieval, semantic representations, non-factoid question answering},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810138,
author = {Gad-Elrab, Mohamed H. and Yosef, Mohamed Amir and Weikum, Gerhard},
title = {Named Entity Disambiguation for Resource-Poor Languages},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810138},
doi = {10.1145/2810133.2810138},
abstract = {Named entity disambiguation (NED) is the task of linking ambiguous names in natural language text to canonical entities like people, organizations or places, registered in a knowledge base. The problem is well-studied for English text, but few systems have considered resource-poor languages that lack comprehensive name-entity dictionaries, entity descriptions, and large annotated training corpora.In this paper we address the NED problem for languages with limited amount of annotated corpora as well as structured resource such as Arabic. We present a method that leverages structured English resources to enrich the components of a language-agnostic NED system and enable effective NED for other languages. We achieve this by fusing data from several multilingual resources and the output of automatic translation/transliteration systems. We show the viability and quality of our approach by synthesizing NED systems for Arabic, Spanish and Italian.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {29–34},
numpages = {6},
keywords = {named entity translation, semantic annotation, information extraction, multilingual named entity linking},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810141,
author = {Sikos, Leslie F. and Powers, David M.W.},
title = {Knowledge-Driven Video Information Retrieval with LOD: From Semi-Structured to Structured Video Metadata},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810141},
doi = {10.1145/2810133.2810141},
abstract = {In parallel with the tremendously increasing number of video contents on the Web, many technical specifications and standards have been introduced to store technical details and describe the content of, and add subtitles to, online videos. Some of these specifications are based on unstructured data with limited machine-processability, data reuse, and interoperability, while others are XML-based, representing semi-structured data. While low-level video features can be derived automatically, high-level features are mainly related to a particular knowledge domain and heavily rely on human experience, judgment, and background. One of the approaches to solve this problem is to map standard, often semi-structured, vocabularies, such as that of MPEG-7, to machine-interpretable ontologies. Another approach is to introduce new multimedia ontologies. While video contents can be annotated efficiently with terms defined by structured LOD datasets, such as DBpedia, ontology standardization would be desired in the video production and distribution domains. This paper compares the state-of-the-art video annotations in terms of descriptor level and machine-readability, highlights the limitations of the different approaches, and makes suggestions towards standard video annotations.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {35–37},
numpages = {3},
keywords = {mpeg-7, ontology, video annotation, linked open data},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810142,
author = {Martin, Paul and Spaniol, Marc and Doucet, Antoine},
title = {Temporal Reconciliation for Dating Photographs Using Entity Information},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810142},
doi = {10.1145/2810133.2810142},
abstract = {Temporal classification of Web contents requires a "notion" about them. This is particularly relevant when contents contain several dates and a human "interpretation" is required in order to chose the appropriate time point. The dating challenge becomes even more complex, when images have to be dated based on the content describing them. In this paper, we present a novel time-stamping approach based on semantics derived from the document. To this end, we will first introduce our experimental dataset and then explain our temporal reconciliation pipeline. In particular, we will explain the process of temporal reconciliation by incorporating information derived from named entities.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {39–41},
numpages = {3},
keywords = {temporal web analytics, entity analytics},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810145,
author = {Dietz, Laura and Schuhmacher, Michael},
title = {An Interface Sketch for Queripidia: Query-Driven Knowledge Portfolios from the Web},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810145},
doi = {10.1145/2810133.2810145},
abstract = {We aim to augment textual knowledge resources such as Wikipedia with information from the World Wide Web and at the same time focus on a given information need. We demonstrate a solution based on what we call knowledge portfolios. A knowledge portfolio is a query-specific collection of relevant entities together with associated passages from the Web that explain how the entity is relevant for the query. Knowledge portfolios are extracted through a combination of retrieval from World Wide Web and Wikipedia with a reasoning process on mutual relevance. A key ingredient are entity link annotations that tie abstract entities from the knowledge base into their context on the Web. We demonstrate the results of our fully automated system Queripidia, which is capable to create a knowledge portfolios for any web-style query, on data from the TREC Web track. The online demo is available via http://smart-cactus.org/~dietz/knowport/.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {43–46},
numpages = {4},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810143,
author = {Karimi, Sarvnaz and Metke-Jimenez, Alejandro and Nguyen, Anthony},
title = {CADEminer: A System for Mining Consumer Reports on Adverse Drug Side Effects},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810143},
doi = {10.1145/2810133.2810143},
abstract = {We introduce CADEminer, a system that mines consumer reviews on medications in order to facilitate discovery of drug side effects that may not have been identified in clinical trials. CADEminer utilises search and natural language processing techniques to (a) extract mentions of side effects, and other relevant concepts such as drug names and diseases in reviews; (b) normalise the extracted mentions to their unified representation in ontologies such as SNOMED CT and MedDRA; (c) identify relationships between extracted concepts, such as a drug caused a side effect; (d) search in authoritative lists of known drug side effects to identify whether or not the extracted side effects are new and therefore require further investigation; and finally (e) provide statistics and visualisation of the data.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {47–50},
numpages = {4},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

@inproceedings{10.1145/2810133.2810144,
author = {Cadilhac, Ana\"{\i}s and Chisholm, Andrew and Hachey, Ben and Kharazmi, Sadegh},
title = {Hugo: Entity-Based News Search and Summarisation},
year = {2015},
isbn = {9781450337908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810133.2810144},
doi = {10.1145/2810133.2810144},
abstract = {We describe Hugo -- a service initially available on iOS that solicits a structured, semantic query and returns entity-specific news articles. Retrieval is powered by a semantic annotation pipeline that includes named entity linking and automatic summarisation. Search and entity linking use an in-house knowledge base initialised with Wikipedia data and continually curated to include new entities. Hugo delivers timely knowledge about a user's professional network, in particular new people they want to know more about.},
booktitle = {Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval},
pages = {51–54},
numpages = {4},
keywords = {semantic search, entity linking, entity retrieval},
location = {Melbourne, Australia},
series = {ESAIR '15}
}

