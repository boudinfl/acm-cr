@article{10.1109/TASLP.2019.2955290,
author = {Gu, Jia-Chen and Ling, Zhen-Hua and Liu, Quan},
title = {Utterance-to-Utterance Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955290},
doi = {10.1109/TASLP.2019.2955290},
abstract = {This article proposes an utterance-to-utterance interactive matching network (U2U-IMN) for multi-turn response selection in retrieval-based chatbots. Different from previous methods following context-to-response matching or utterance-to-response matching frameworks, this model treats both contexts and responses as sequences of utterances when calculating the matching degrees between them. For a context-response pair, the U2U-IMN model first encodes each utterance separately using recurrent and self-attention layers. Then, a global and bidirectional interaction between the context and the response is conducted using the attention mechanism to collect the matching information between them. The distances between context and response utterances are employed as a prior component when calculating the attention weights. Finally, sentence-level aggregation and context-response-level aggregation are executed in turn to obtain the feature vector for matching degree prediction. Experiments on four public datasets showed that our proposed method outperformed baseline methods on all metrics, achieving a new state-of-the-art performance and demonstrating compatibility across domains for multi-turn response selection.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {369–379},
numpages = {11}
}

@article{10.1109/TASLP.2020.2973795,
author = {Huang, Gongping and Benesty, Jacob and Cohen, Israel and Chen, Jingdong},
title = {Differential Beamforming on Graphs},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2973795},
doi = {10.1109/TASLP.2020.2973795},
abstract = {In this article, we study differential beamforming from a graph perspective. The microphone array used for differential beamforming is viewed as a graph, where its sensors correspond to the nodes, the number of microphones corresponds to the order of the graph, and linear spatial difference equations among microphones are related to graph edges. Specifically, for the first-order differential beamforming with an array of <inline-formula><tex-math notation="LaTeX">$M$</tex-math></inline-formula> microphones, each pair of adjacent microphones are directly connected, resulting in <inline-formula><tex-math notation="LaTeX">$M-1$</tex-math></inline-formula> spatial difference equations. On a graph, each of these equations corresponds to a 2-clique. For the second-order differential beamforming, each three adjacent microphones are directly connected, resulting in <inline-formula><tex-math notation="LaTeX">$M-2$</tex-math></inline-formula> second-order spatial difference equations, and each of these equations corresponds to a 3-clique. In an analogous manner, the differential microphone array for any order of differential beamforming can be viewed as a graph. From this perspective, we then derive a class of differential beamformers, including the maximum white noise gain beamformer, the maximum directivity factor one, and optimal compromising beamformers. Simulations are presented to demonstrate the performance of the derived differential beamformers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {901–913},
numpages = {13}
}

@article{10.1109/TASLP.2019.2962684,
author = {Maeno, Yu and Mitsufuji, Yuki and Samarasinghe, Prasanga N. and Murata, Naoki and Abhayapala, Thushara D.},
title = {Spherical-Harmonic-Domain Feedforward Active Noise Control Using Sparse Decomposition of Reference Signals from Distributed Sensor Arrays},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2962684},
doi = {10.1109/TASLP.2019.2962684},
abstract = {Active acoustic noise attenuation over a sizable space is a challenging problem in signal processing. The noise attenuation performance of feedforward active noise control (ANC) relies on the preciseness of a reference signal of a primary noise field. To capture the precise reference signal for controlling a sizable space, a large number of reference microphones are required, which reduces system viability. In this study, we exploit an efficient representation of the reference signal in spherical harmonic (SH) domain by utilizing the inherent sparseness of the noise field. The main contributions of this work are as follows. (1)&nbsp;A general reference microphone geometry can be used. The implementation difficulty in the array structure, which is recognized as the common issue of SH-domain signal processing, e.g., use of a fully surrounding spherical array, is reduced by using the fields translation based on the addition theorem. (2)&nbsp;The accuracy of low-frequency signal decomposition is improved. The low accuracy of low-frequency signal decomposition in compressive sensing (CS), which is commonly reported in the literature, is improved by applying signal representation in SH domain. (3)&nbsp;System robustness is increased. The robustness of the system is increased by considering a noise source spatial distribution of both the interior and exterior sound fields, which is not possible in the case of a general signal representation in SH domain. Experimental results indicate that the noise attenuation performance of our proposed method exceeds that of existing solutions. The flexibility of the array structure is also increased, which leads to a more feasible practical system setup.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {656–670},
numpages = {15}
}

@article{10.1109/TASLP.2019.2948765,
author = {Desiraju, Naveen Kumar and Doclo, Simon and Buck, Markus and Wolff, Tobias},
title = {Online Estimation of Reverberation Parameters For Late Residual Echo Suppression},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2948765},
doi = {10.1109/TASLP.2019.2948765},
abstract = {In hands-free telephony and other distant-talk applications, often a short AEC filter is used to achieve fast convergence at low computational cost. As a result, a significant amount of late residual echo (LRE) may remain, especially in highly reverberant environments. This LRE can be suppressed using a postfilter in the subband domain, which requires an estimate of the power spectral density (PSD) of the LRE. To estimate the LRE PSD, an exponentially decaying model with frequency-dependent reverberation scaling and decay parameters has frequently been assumed. State-of-the-art methods estimate both reverberation parameters independently of each other, either in offline or in online mode. In this article, we propose two signal-based methods (i.e. output error and equation error) to jointly estimate both reverberation parameters in online mode. The estimated parameters are then used to generate an estimate for the LRE PSD, which is fed into a postfilter for the purpose of late residual echo suppression. We derive several gradient-descent-based algorithms to simultaneously update both reverberation parameters, minimizing either the mean squared error or the mean squared log error cost function. The proposed methods are compared with state-of-the-art methods in terms of the accuracy of the estimated reverberation parameters and the corresponding LRE PSD estimate. Extensive simulation results using both artificial as well as measured room impulse responses show that the proposed output error method with mean squared log error minimization outperforms state-of-the-art methods in all considered scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {77–91},
numpages = {15}
}

@article{10.1109/TASLP.2019.2949219,
author = {Huang, Gongping and Chen, Jingdong and Benesty, Jacob},
title = {Design of Planar Differential Microphone Arrays With Fractional Orders},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2949219},
doi = {10.1109/TASLP.2019.2949219},
abstract = {Differential microphone arrays (DMAs) often encounter white noise amplification, especially at low frequencies. If the array geometry and the number of microphones are fixed, one can improve the white noise amplification problem by reducing the DMA order. With the existing differential beamforming methods, the DMA order can only be a positive integer number. Consequently, with a specified beampattern (or a kind of beampattern), reducing this order may easily lead to over compensation of the white noise gain (WNG) and too much reduction of the directivity factor (DF), which is not optimal. To deal with this problem, we present in this article a general approach to the design of DMAs with fractional orders. The major contributions of this article include but are not limited to: 1) we first define a directivity pattern that can achieve a continuous compromise between the pattern corresponding to the maximum DMA order and the omnidirectional pattern; 2) by approximating the beamformer's beampattern with the Jacobi-Anger expansion, we present a method to find the proper differential beamforming filter so that its beampattern matches closely the target directivity pattern of fractional orders; and 3) we show how to determine analytically the proper fractional order of the DMA with a given target beampattern when either the value of the DF or WNG is specified, which is useful in practice to achieve the desired beampattern and spatial gain while maintaining the robustness of the DMA system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {116–130},
numpages = {15}
}

@article{10.1109/TASLP.2019.2949925,
author = {Dai, Tao and Zhu, Li and Wang, Yaxiong and Carley, Kathleen M.},
title = {Attentive Stacked Denoising Autoencoder With Bi-LSTM for Personalized Context-Aware Citation Recommendation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2949925},
doi = {10.1109/TASLP.2019.2949925},
abstract = {The rapid growth of scientific publications brings the problem of finding appropriate citations for authors. Context-aware citation recommendation is an essential technology to overcome this obstacle when given a fragment of manuscript. In this article, we propose a novel neural network model for context-aware citation recommendation by combining stacked denoising autoencoders (SDAE) and Bi-LSTM. To obtain effective embedding for cited paper, we extend SDAE into attentive SDAE (ASDAE) by utilizing the attentive information from citation context, which essentially enhance the learning ability of original SDAE. For citation context, we devise an attentive Bi-LSTM to obtain effective embedding. Specifically, the attentive Bi-LSTM is able to extract suitable citation context and recommend citations simultaneously when given a long text, which is a issue that few papers addressed before. We also integrate personalized author information to improve the performance of recommendation. Our model is essentially a seemly integration of different types of neural network with latent variables. We derive the generative process of our model, and develop a learning algorithm based on maximum a posteriori (MAP) estimation. Experimental results on the RefSeer, ANN and DBLP datasets show that our model outperforms baseline methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {553–568},
numpages = {16}
}

@article{10.1109/TASLP.2019.2953000,
author = {Birnie, Lachlan I. and Abhayapala, Thushara D. and Samarasinghe, Prasanga N.},
title = {Reflection Assisted Sound Source Localization Through a Harmonic Domain MUSIC Framework},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2953000},
doi = {10.1109/TASLP.2019.2953000},
abstract = {This work presents a method that persuades acoustic reflections to be a favorable property for sound source localization. Whilst most real world spatial audio applications utilize prior knowledge of sound source position, estimating such positions in reverberant environments is still considered to be a difficult problem due to acoustic reflections. This article presents a novel MUSIC framework for multiple sound source localization (range, elevation, azimuth) in reverberant rooms by incorporating a recently proposed region-to-region room transfer model. The method is built upon the received signals of a higher order microphone and a spherical harmonic representation of the room transfer function. We demonstrate the method's general applicability and multiple source localization performance through a simulation study across an assortment of reverberant conditions. Additionally, we investigate robustness against various system modeling errors to gauge implementation viability. Finally, we prove the method in a practical experiment inside a real-world room with measured region-to-region transfer function parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {279–293},
numpages = {15}
}

@article{10.1109/TASLP.2019.2953350,
author = {Ding, Wenhao and He, Liang},
title = {Adaptive Multi-Scale Detection of Acoustic Events},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2953350},
doi = {10.1109/TASLP.2019.2953350},
abstract = {The goal of acoustic (or sound) events detection (AED or SED) is to predict the temporal position of target events in given audio segments. This task plays a significant role in safety monitoring, acoustic early warning and other scenarios. However, the deficiency of data and diversity of acoustic event sources make the AED task a tough issue, especially for prevalent data-driven methods. In this article, we start from analyzing acoustic events according to their time-frequency domain properties, showing that different acoustic events have different time-frequency scale characteristics. Inspired by the analysis, we propose an adaptive multi-scale detection (AdaMD) method. By taking advantage of hourglass neural network and gated recurrent unit (GRU) module, our AdaMD produces multiple predictions at different temporal and frequency resolutions. An adaptive training algorithm is subsequently adopted to combine multi-scale predictions to enhance the overall capability. Experimental results on Detection and Classification of Acoustic Scenes and Events 2017 (DCASE 2017) Task 2, DCASE 2016 Task 3 and DCASE 2017 Task 3 demonstrate that the AdaMD outperforms published state-of-the-art competitors in terms of the metrics of event error rate (ER) and F1-score. The verification experiment on our collected factory mechanical dataset also proves the noise-resistant capability of the AdaMD, providing the possibility for it to be deployed in the complex environment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {294–306},
numpages = {13}
}

@article{10.1109/TASLP.2019.2947741,
author = {Eskimez, Sefik Emre and Maddox, Ross K. and Xu, Chenliang and Duan, Zhiyao},
title = {Noise-Resilient Training Method for Face Landmark Generation From Speech},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2947741},
doi = {10.1109/TASLP.2019.2947741},
abstract = {Visual cues such as lip movements, when available, play an important role in speech communication. They are especially helpful for the hearing impaired population or in noisy environments. When not available, having a system to automatically generate talking faces in sync with input speech would enhance speech communication and enable many novel applications. In this article, we present a new system that can generate 3D talking face landmarks from speech in an online fashion. We employ a neural network that accepts the raw waveform as an input. The network contains convolutional layers with 1D kernels and outputs the active shape model (ASM) coefficients of face landmarks. To promote smoother transitions between video frames, we present a variant of the model that has the same architecture but also accepts the previous frame's ASM coefficients as an additional input. To cope with background noise, we propose a new training method to incorporate speech enhancement ideas at the feature level. Objective evaluations on landmark prediction show that the proposed system yields statistically significantly smaller errors than two state-of-the-art baseline methods on both a single-speaker dataset and a multi-speaker dataset. Experiments on noisy speech input with five types of non-stationary unseen noise show statistically significant improvements of the system performance thanks to the noise-resilient training method. Finally, subjective evaluations show that the generated talking faces have a significantly more convincing match with the input audio, achieving a similarly convincing level of realism as the ground-truth landmarks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {27–38},
numpages = {12}
}

@article{10.1109/TASLP.2020.2986896,
author = {Taherian, Hassan and Wang, Zhong-Qiu and Chang, Jorge and Wang, DeLiang},
title = {Robust Speaker Recognition Based on Single-Channel and Multi-Channel Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2986896},
doi = {10.1109/TASLP.2020.2986896},
abstract = {Deep neural network (DNN) embeddings for speaker recognition have recently attracted much attention. Compared to i-vectors, they are more robust to noise and room reverberation as DNNs leverage large-scale training. This article addresses the question of whether speech enhancement approaches are still useful when DNN embeddings are used for speaker recognition. We investigate single- and multi-channel speech enhancement for text-independent speaker verification based on x-vectors in conditions where strong diffuse noise and reverberation are both present. Single-channel (monaural) speech enhancement is based on complex spectral mapping and is applied to individual microphones. We use masking-based minimum variance distortion-less response (MVDR) beamformer and its rank-1 approximation for multi-channel speech enhancement. We propose a novel method of deriving time-frequency masks from the estimated complex spectrogram. In addition, we investigate gammatone frequency cepstral coefficients (GFCCs) as robust speaker features. Systematic evaluations and comparisons on the NIST SRE 2010 retransmitted corpus show that both monaural and multi-channel speech enhancement significantly outperform x-vector's performance, and our covariance matrix estimate is effective for the MVDR beamformer.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1293–1302},
numpages = {10}
}

@article{10.1109/TASLP.2020.2983580,
author = {Padi, Bharat and Mohan, Anand and Ganapathy, Sriram},
title = {Towards Relevance and Sequence Modeling in Language Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2983580},
doi = {10.1109/TASLP.2020.2983580},
abstract = {The task of automatic language identification (LID) involving multiple dialects of the same language family in the presence of noise is a challenging problem. In these scenarios, the identity of the language/dialect may be reliably present only in parts of the temporal sequence of the speech signal. The conventional approaches to LID (and for speaker recognition) ignore the sequence information by extracting long-term statistical summary of the recording assuming an independence of the feature frames. In this paper, we propose a neural network framework utilizing short-sequence information in language recognition. In particular, a new model is proposed for incorporating relevance in language recognition, where parts of speech data are weighted more based on their relevance for the language recognition task. This relevance weighting is achieved using the bidirectional long short-term memory (BLSTM) network with attention modeling. We explore two approaches, the first approach uses segment level i-vector/x-vector representations that are aggregated in the neural model and the second approach where the acoustic features are directly modeled in an end-to-end neural model. Experiments are performed using the language recognition task in NIST LRE 2017 Challenge using clean, noisy and multi-speaker speech data as well as in the RATS language recognition corpus. In these experiments on noisy LRE tasks as well as the RATS dataset, the proposed approach yields significant improvements over the conventional i-vector/x-vector based language recognition approaches as well as with other previous models incorporating sequence information.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1223–1232},
numpages = {10}
}

@article{10.1109/TASLP.2019.2957889,
author = {Parekh, Sanjeel and Essid, Slim and Ozerov, Alexey and Duong, Ngoc Q. K. and P\'{e}rez, Patrick and Richard, Ga\"{e}l},
title = {Weakly Supervised Representation Learning for Audio-Visual Scene Analysis},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2957889},
doi = {10.1109/TASLP.2019.2957889},
abstract = {Audio-visual (AV) representation learning is an important task from the perspective of designing machines with the ability to understand complex events. To this end, we propose a novel multimodal framework that instantiates multiple instance learning. Specifically, we develop methods that identify events and localize corresponding AV cues in unconstrained videos. Importantly, this is done using weak labels where only video-level event labels are known without any information about their location in time. We show that the learnt representations are useful for performing several tasks such as event/object classification, audio event detection, audio source separation and visual object localization. An important feature of our method is its capacity to learn from unsynchronized audio-visual events. We also demonstrate our framework's ability to separate out the audio source of interest through a novel use of nonnegative matrix factorization. State-of-the-art classification results, with a F1-score of 65.0, are achieved on DCASE 2017 smart cars challenge data with promising generalization to diverse object types such as musical instruments. Visualizations of localized visual regions and audio segments substantiate our system's efficacy, especially when dealing with noisy situations where modality-specific cues appear asynchronously.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {416–428},
numpages = {13}
}

@article{10.1109/TASLP.2019.2955289,
author = {Ding, Shaojin and Zhao, Guanlong and Liberatore, Christopher and Gutierrez-Osuna, Ricardo},
title = {Learning Structured Sparse Representations for Voice Conversion},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955289},
doi = {10.1109/TASLP.2019.2955289},
abstract = {Sparse-coding techniques for voice conversion assume that an utterance can be decomposed into a sparse code that only carries linguistic contents, and a dictionary of atoms that captures the speakers’ characteristics. However, conventional dictionary-construction and sparse-coding algorithms rarely meet this assumption. The result is that the sparse code is no longer speaker-independent, which leads to lower voice-conversion performance. In this paper, we propose a Cluster-Structured Sparse Representation (CSSR) that improves the speaker independence of the representations. CSSR consists of two complementary components: a Cluster-Structured Dictionary Learning module that groups atoms in the dictionary into clusters, and a Cluster-Selective Objective Function that encourages each speech frame to be represented by atoms from a small number of clusters. We conducted four experiments on the CMU ARCTIC corpus to evaluate the proposed method. In a first ablation study, results show that each of the two CSSR components enhances speaker independence, and that combining both components leads to further improvements. In a second experiment, we find that CSSR uses increasingly larger dictionaries more efficiently than phoneme-based representations by allowing finer-grained decompositions of speech sounds. In a third experiment, results from objective and subjective measurements show that CSSR outperforms prior voice-conversion methods, improving the acoustic quality of the synthesized speech while retaining the target speaker's voice identity. Finally, we show that the CSSR captures latent (i.e., phonetic) information in the speech signal.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {343–354},
numpages = {12}
}

@article{10.1109/TASLP.2019.2955253,
author = {Sharma, Bidisha and Wang, Ye},
title = {Automatic Evaluation of Song Intelligibility Using Singing Adapted STOI and Vocal-Specific Features},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955253},
doi = {10.1109/TASLP.2019.2955253},
abstract = {An objective machine-driven measure of song intelligibility would be of great utility for various music information retrieval tasks. Song intelligibility mostly depends on two factors, the amount of interference caused by background accompaniment, and the quality of singing vocal. We leverage these two factors to determine the intelligibility of a song. For the first factor, we adapt a well known method for intelligibility prediction of noisy speech, short term objective intelligibility (STOI), to singing. The singing-adapted STOI considers the polyphonic song as a time-frequency weighted noisy version of the extracted singing vocal. We use U-net based audio source separation method to extract singing vocal from a polyphonic song. The singing vocal shares the same underlying physiological mechanism for production as that of speech, with some differences in the pronunciation and prosody of the phonemes. Therefore, for the second factor, we have introduced vocal-specific features to measure the intelligibility of the singing vocal, which are excitation source, spectral, and prosodic singing characteristics. We perform detailed analysis on each of these features to establish their efficacy for quantifying song intelligibility. We train a regression model to derive the intelligibility scores using a combination of the vocal-specific features and singing adapted STOI, obtaining a significant improvement in performance. The correlation between the intelligibility score obtained using proposed framework and human-rated intelligibility score is 0.81, which shows the efficacy of the proposed approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {319–331},
numpages = {13}
}

@article{10.1109/TASLP.2020.2967539,
author = {Zhang, Mengfan and Ge, Zhongshu and Liu, Tiejun and Wu, Xihong and Qu, Tianshu},
title = {Modeling of Individual HRTFs Based on Spatial Principal Component Analysis},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2967539},
doi = {10.1109/TASLP.2020.2967539},
abstract = {Head-related transfer function (HRTF) plays an important role in the construction of 3D auditory display. This article presents an individual HRTF modeling method using deep neural networks based on spatial principal component analysis. The HRTFs are represented by a small set of spatial principal components combined with frequency and individual-dependent weights. By estimating the spatial principal components using deep neural networks and mapping the corresponding weights to a quantity of anthropometric parameters, we predict individual HRTFs in arbitrary spatial directions. The objective and subjective experiments evaluate the HRTFs generated by the proposed method, the principal component analysis (PCA) method, and the generic method. The results show that the HRTFs generated by the proposed method and PCA method perform better than the generic method. For most frequencies the spectral distortion of the proposed method is significantly smaller than the PCA method in the high frequencies but significantly larger in the low frequencies. The evaluation of the localization model shows the PCA method is better than the proposed method. The subjective localization experiments show that the PCA and the proposed methods have similar performances in most conditions. Both the objective and subjective experiments show that the proposed method can predict HRTFs in arbitrary spatial directions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {785–797},
numpages = {13}
}

@article{10.1109/TASLP.2019.2958408,
author = {Trowitzsch, Ivo and Schymura, Christopher and Kolossa, Dorothea and Obermayer, Klaus},
title = {Joining Sound Event Detection and Localization Through Spatial Segregation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2958408},
doi = {10.1109/TASLP.2019.2958408},
abstract = {Identification and localization of sounds are both integral parts of computational auditory scene analysis. Although each can be solved separately, the goal of forming coherent auditory objects and achieving a comprehensive spatial scene understanding suggests pursuing a joint solution of the two problems. This article presents an approach that robustly binds localization with the detection of sound events in a binaural robotic system. Both tasks are joined through the use of spatial stream segregation which produces probabilistic time-frequency masks for individual sources attributable to separate locations, enabling segregated sound event detection operating on these streams. We use simulations of a comprehensive suite of test scenes with multiple co-occurring sound sources, and propose performance measures for systematic investigation of the impact of scene complexity on this segregated detection of sound types. Analyzing the effect of spatial scene arrangement, we show how a robot could facilitate high performance through optimal head rotation. Furthermore, we investigate the performance of segregated detection given possible localization error as well as error in the estimation of number of active sources. Our analysis demonstrates that the proposed approach is an effective method to obtain joint sound event location and type information under a wide range of conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {487–502},
numpages = {16}
}

@article{10.1109/TASLP.2019.2949709,
author = {Emura, Satoru},
title = {Wave-Domain Residual Echo Reduction Using Subspace Tracking},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2949709},
doi = {10.1109/TASLP.2019.2949709},
abstract = {This article proposes a wave-domain residual echo reduction method for two-way immersive sound communication, which is based on wave field synthesis and uses uniform linear arrays of loudspeakers and microphones. This method is intended to improve the echo reduction performance in a mildly reverberant room with moderate increase in computational complexity. For improving such performance, this method takes residual echo due to reflected waves into account and uses a multi-tap echo-path model. To mitigate the increase in the method's computational complexity, it uses a size-reduced transfer function (TF) matrix instead of the full TF matrix. Specifically, this method extracts the dominant components of the loudspeaker signals adaptively by subspace tracking and estimates a size-reduced TF matrix that relates these dominant components to the residual echo. Simulation showed that the proposed method plus a wave-domain acoustic echo canceller (AEC) improved echo return loss enhancement by 10&nbsp;dB compared to the conventional wave-domain echo reduction method plus the wave-domain AEC in a room with a reverberation time 180&nbsp;ms. Its total computational complexity is twice that of the conventional echo reduction method plus a wave-domain AEC.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {144–156},
numpages = {13}
}

@article{10.1109/TASLP.2020.2964427,
author = {Zhou, Qingyu and Yang, Nan and Wei, Furu and Huang, Shaohan and Zhou, Ming and Zhao, Tiejun},
title = {A Joint Sentence Scoring and Selection Framework for Neural Extractive Document Summarization},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2964427},
doi = {10.1109/TASLP.2020.2964427},
abstract = {Extractive document summarization methods aim to extract important sentences to form a summary. Previous works perform this task by first scoring all sentences in the document then selecting most informative ones; while we propose to jointly learn the two steps with a novel end-to-end neural network framework. Specifically, the sentences in the input document are represented as real-valued vectors through a neural document encoder. Then the method builds the output summary by extracting important sentences one by one. Different from previous works, the proposed joint sentence scoring and selection framework directly predicts the relative sentence importance score according to both sentence content and previously selected sentences. We evaluate the proposed framework with two realizations: a hierarchical recurrent neural network based model; and a pre-training based model that uses BERT as the document encoder. Experiments on two datasets show that the proposed joint framework outperforms the state-of-the-art extractive summarization models which treat sentence scoring and selection as two subtasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {671–681},
numpages = {11}
}

@article{10.1109/TASLP.2020.2982287,
author = {Buchris, Yaakov and Cohen, Israel and Benesty, Jacob and Amar, Alon},
title = {Joint Sparse Concentric Array Design for Frequency and Rotationally Invariant Beampattern},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982287},
doi = {10.1109/TASLP.2020.2982287},
abstract = {Frequency-invariant concentric arrays are fundamental components in some real-world applications, like teleconferencing, voice service devices, underwater acoustics, and others, where the azimuthal arrival direction of the desired signal is varying. The fact that the demand for limited hardware and computational resources in such applications is essential, motivates the use of a sparse design which can optimize both the number of the required sensors and the complex weights of the beamformer. Herein, we propose a new greedy based joint-sparse design of frequency and rotationally invariant concentric arrays which preserves the properties of the designed directivity pattern for different azimuthal directions of steering. Simulation results show that the greedy sparse design, compared to uniform and random designs, gives superior performance in terms of array gain, and frequency and rotationally invariant beampattern, with a reasonable computational and hardware resources.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1143–1158},
numpages = {16}
}

@article{10.1109/TASLP.2020.2977776,
author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
title = {Machine Speech Chain},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2977776},
doi = {10.1109/TASLP.2020.2977776},
abstract = {Despite the close relationship between speech perception and production, research in automatic speech recognition (ASR) and text-to-speech synthesis (TTS) has progressed more or less independently without exerting much mutual influence. In human communication, on the other hand, a closed-loop speech chain mechanism with auditory feedback from the speaker's mouth to her ear is crucial. In this paper, we take a step further and develop a closed-loop machine speech chain model based on deep learning. The sequence-to-sequence model in closed-loop architecture allows us to train our model on the concatenation of both labeled and unlabeled data. While ASR transcribes the unlabeled speech features, TTS attempts to reconstruct the original speech waveform based on the text from ASR. In the opposite direction, ASR also attempts to reconstruct the original text transcription given the synthesized speech. To the best of our knowledge, this is the first deep learning framework that integrates human speech perception and production behaviors. Our experimental results show that the proposed approach significantly improved performance over that from separate systems that were only trained with labeled data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {976–989},
numpages = {14}
}

@article{10.1109/TASLP.2019.2950602,
author = {Su, Rongfeng and Liu, Xunying and Wang, Lan and Yang, Jingzhou},
title = {Cross-Domain Deep Visual Feature Generation for Mandarin Audio–Visual Speech Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2950602},
doi = {10.1109/TASLP.2019.2950602},
abstract = {There has been a long term interest in using visual information to improve automatic speech recognition (ASR) system performance. Both audio and visual information are required in conventional audio visual speech recognition (AVSR) systems. This limits their wider applications when visual modality is not present. To this end, one possible solution is to use acoustic-to-visual (A2V) inversion techniques to generate visual features. Previous research in this direction used synthetic acoustic-articulatory parallel data in inversion model training. The acoustic mismatch between the audio-visual (AV) parallel data and target data was not considered. In addition, the target language to apply these technologies has been focused on English. In this article, a real 3D Audio-Visual Mandarin Continuous Speech (3DAV-MCS) corpus was used to train deep neural network based A2V inversion models. Cross-domain adaptation of the inversion models allows suitable visual features to be generated from acoustic data of mismatched domains. The proposed cross-domain deep visual feature generation techniques were evaluated on two state-of-the-art Mandarin speech recognition tasks: DAPRA GALE broadcast transcription and BOLT conversational telephone speech recognition. The AVSR systems constructed using the cross-domain generated visual features consistently outperformed the baseline convolutional neural network (CNN) ASR systems by up to 3.3% absolute (9.1% relative) character error rate (CER) reductions after both speaker adaptive training and sequence discriminative training were performed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {185–197},
numpages = {13}
}

@article{10.1109/TASLP.2019.2948794,
author = {Laufer, Yaron and Gannot, Sharon},
title = {Scoring-Based ML Estimation and CRBs for Reverberation, Speech, and Noise PSDs in a Spatially Homogeneous Noise Field},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2948794},
doi = {10.1109/TASLP.2019.2948794},
abstract = {Hands-free speech systems are subject to performance degradation due to reverberation and noise. Common methods for enhancing reverberant and noisy speech require the knowledge of the speech, reverberation and noise power spectral densities (PSDs). Most literature on this topic assumes that the noise power spectral density (PSD) matrix is known. However, in many practical acoustic scenarios, the noise PSD is unknown and should be estimated along with the speech and the reverberation PSDs. In this article, the noise is modeled as a spatially homogeneous sound field, with an unknown time-varying PSD multiplied by a known time-invariant spatial coherence matrix. We derive two maximum likelihood estimators (MLEs) for the various PSDs, including the noise: The first is a non-blocking-based estimator, that jointly estimates the PSDs of the speech, reverberation and noise components. The second MLE is a blocking-based estimator, that blocks the speech signal and estimates the reverberation and noise PSDs. Since a closed-form solution does not exist, both estimators iteratively maximize the likelihood using the Fisher scoring method. In order to compare both methods, the corresponding Cramér-Rao Bounds (CRBs) are derived. For both the reverberation and the noise PSDs, it is shown that the non-blocking-based CRB is lower than the blocking-based CRB. Performance evaluation using both simulated and real reverberant and noisy signals, shows that the proposed estimators outperform competing estimators, and greatly reduce the effect of reverberation and noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {61–76},
numpages = {16}
}

@article{10.1109/TASLP.2019.2948730,
author = {Zohourian, Mehdi and Martin, Rainer},
title = {Binaural Direct-to-Reverberant Energy Ratio and Speaker Distance Estimation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2948730},
doi = {10.1109/TASLP.2019.2948730},
abstract = {This article addresses the problem of distance estimation using binaural hearing aid microphones in reverberant rooms. Among several distance indicators, the direct-to-reverberant energy ratio (DRR) has been shown to be more effective than other features. Therefore, we present two novel approaches to estimate the DRR of binaural signals. The first method is based on the interaural magnitude-squared coherence whereas the second approach uses stochastic maximum likelihood beamforming to estimate the power of the direct and reverberant components. The proposed DRR estimation algorithms are integrated into a distance estimation technique. When based solely on DRR, the distance estimation algorithm requires calibration where naturally the critical distance is a good calibration point. We thus propose two approaches for the calibration of the distance estimation algorithm: Informed calibration using the critical distance of the reverberant room and blind calibration using the listener's own voice. Results across various acoustical environments show the benefit of the proposed algorithms for the estimation of sound source distances up to 3 m with an estimation error of about 35 cm using informed calibration and about 1 m using the fully blind calibration strategy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {92–104},
numpages = {13}
}

@article{10.1109/TASLP.2020.2966869,
author = {Dietzen, Thomas and Doclo, Simon and Moonen, Marc and van Waterschoot, Toon},
title = {Integrated Sidelobe Cancellation and Linear Prediction Kalman Filter for Joint Multi-Microphone Speech Dereverberation, Interfering Speech Cancellation, and Noise Reduction},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2966869},
doi = {10.1109/TASLP.2020.2966869},
abstract = {In multi-microphone speech enhancement, reverberation as well as additive noise and/or interfering speech are commonly suppressed by deconvolution and spatial filtering, e.g., using multi-channel linear prediction (MCLP) on the one hand and beamforming, e.g., a generalized sidelobe canceler (GSC), on the other hand. In this article, we consider several reverberant speech components, whereof some are to be dereverberated and others to be canceled, as well as a diffuse (e.g., babble) noise component to be suppressed. In order to perform both deconvolution and spatial filtering, we integrate MCLP and the GSC into a novel architecture referred to as integrated sidelobe cancellation and linear prediction (ISCLP), where the sidelobe-cancellation (SC) filter and the linear prediction (LP) filter operate in parallel, but on different microphone signal frames. Within ISCLP, we estimate both filters jointly by means of a single Kalman filter. We further propose a spectral Wiener gain post-processor, which is shown to relate to the Kalman filter's posterior state estimate. The presented ISCLP Kalman filter is benchmarked against two state-of-the-art approaches, namely first a pair of alternating Kalman filters respectively performing dereverberation and noise reduction, and second an MCLP+GSC Kalman filter cascade. While the ISCLP Kalman filter is roughly <inline-formula><tex-math notation="LaTeX">$M^2$</tex-math></inline-formula> times less expensive than both reference algorithms, where <inline-formula><tex-math notation="LaTeX">$M$</tex-math></inline-formula> denotes the number of microphones, it is shown to perform at least similarly as compared to the former, and to outperform the latter. A MATLAB implementation is available.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {740–754},
numpages = {15}
}

@article{10.1109/TASLP.2020.2982030,
author = {Zhu, Yingying and Zhao, Haiquan and Zeng, Xiangping and Chen, Badong},
title = {Robust Generalized Maximum Correntropy Criterion Algorithms for Active Noise Control},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982030},
doi = {10.1109/TASLP.2020.2982030},
abstract = {As a robust nonlinear similarity measure, the maximum correntropy criterion (MCC) has been successfully applied to active noise control (ANC) for impulsive noise. The default kernel function of the filtered-x maximum correntropy criterion (FxMCC) algorithm is the Gaussian kernel, which is desirable in many cases for its smooth and strict positive-definite. However, it is not always the best choice. In this study, a filtered-x generalized maximum correntropy criterion (FxGMCC) algorithm is proposed, which adopts the generalized Gaussian density (GGD) function as its kernel. The FxGMCC algorithm has greater robust ability against non-Gaussian environments, but, it still adopts a single error norm which exhibits poor convergence rate and noise reduction performance. To surmount this problem, an improved FxGMCC (IFxGMCC) algorithm with continuous mixed <italic>L<sub>p</sub>-</italic>norm is proposed. Moreover, to make a trade-off between fast convergence rate and low steady-state misalignment, a convexly combined IFxGMCC (C-IFxGMCC) algorithm is further developed. The stability mechanism and computational complexity of the proposed algorithms are analyzed. Simulation results in the context of different impulsive noises as well as the real noise signals verify that the proposed algorithms are superior to most of the existing robust adaptive algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1282–1292},
numpages = {11}
}

@article{10.1109/TASLP.2019.2947737,
author = {Gupta, Chitralekha and Li, Haizhou and Wang, Ye},
title = {Automatic Leaderboard: Evaluation of Singing Quality Without a Standard Reference},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2947737},
doi = {10.1109/TASLP.2019.2947737},
abstract = {Automatic evaluation of singing quality can be done with the help of a reference singing or the digital sheet music of the song. However, such a standard reference is not always available. In this article, we propose a framework to rank a large pool of singers according to their singing quality without any standard reference. We define musically motivated absolute measures based on pitch histogram, and relative measures based on inter-singer statistics to evaluate the quality of singing attributes such as intonation, and rhythm. The absolute measures evaluate the goodness of pitch histogram specific to a singer, while the relative measures use the similarity between singers in terms of pitch, rhythm, and timbre as an indicator of singing quality. With the relative measures, we formulate the concept of <italic>veracity</italic> or <italic>truth-finding</italic> for the ranking of singing quality. We successfully validate a self-organizing approach to rank-ordering a large pool of singers. The fusion of absolute and relative measures results in an average Spearman's rank correlation of 0.71 with human judgments in a 10-fold cross-validation experiment, which is close to the inter-judge correlation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {13–26},
numpages = {14}
}

@article{10.1109/TASLP.2020.2982029,
author = {Fan, Cunhang and Tao, Jianhua and Liu, Bin and Yi, Jiangyan and Wen, Zhengqi and Liu, Xuefei},
title = {End-to-End Post-Filter for Speech Separation With Deep Attention Fusion Features},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982029},
doi = {10.1109/TASLP.2020.2982029},
abstract = {In this article, we propose an end-to-end post-filter method with deep attention fusion features for monaural speaker-independent speech separation. At first, a time-frequency domain speech separation method is applied as the pre-separation stage. The aim of pre-separation stage is to separate the mixture preliminarily. Although this stage can separate the mixture, it still contains the residual interference. In order to enhance the pre-separated speech and improve the separation performance further, the end-to-end post-filter (E2EPF) with deep attention fusion features is proposed. The E2EPF can make full use of the prior knowledge of the pre-separated speech, which contributes to speech separation. It is a fully convolutional speech separation network and uses the waveform as the input features. Firstly, the 1-D convolutional layer is utilized to extract the deep representation features for the mixture and pre-separated signals in the time domain. Secondly, to pay more attention to the outputs of the pre-separation stage, an attention module is applied to acquire deep attention fusion features, which are extracted by computing the similarity between the mixture and the pre-separated speech. These deep attention fusion features are conducive to reduce the interference and enhance the pre-separated speech. Finally, these features are sent to the post-filter to estimate each target signals. Experimental results on the WSJ0-2mix dataset show that the proposed method outperforms the state-of-the-art speech separation method. Compared with the pre-separation method, our proposed method can acquire 64.1%, 60.2%, 25.6% and 7.5% relative improvements in scale-invariant source-to-noise ratio (SI-SNR), the signal-to-distortion ratio (SDR), the perceptual evaluation of speech quality (PESQ) and the short-time objective intelligibility (STOI) measures, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1303–1314},
numpages = {12}
}

@article{10.1109/TASLP.2019.2952013,
author = {Mimilakis, Stylianos Ioannis and Drossos, Konstantinos and Cano, Estefan\'{\i}a and Schuller, Gerald},
title = {Examining the Mapping Functions of Denoising Autoencoders in Singing Voice Separation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2952013},
doi = {10.1109/TASLP.2019.2952013},
abstract = {The goal of this article is to investigate what singing voice separation approaches based on neural networks learn from the data. We examine the mapping functions of neural networks based on the denoising autoencoder (DAE) model that are conditioned on the mixture magnitude spectra. To approximate the mapping functions, we propose an algorithm inspired by the knowledge distillation, denoted the neural couplings algorithm (NCA). The NCA yields a matrix that expresses the mapping of the mixture to the target source magnitude information. Using the NCA, we examine the mapping functions of three fundamental DAE-based models in music source separation; one with single-layer encoder and decoder, one with multi-layer encoder and single-layer decoder, and one using skip-filtering connections (SF) with a single-layer encoding and decoding. We first train these models with realistic data to estimate the singing voice magnitude spectra from the corresponding mixture. We then use the optimized models and test spectral data as input to the NCA. Our experimental findings show that approaches based on the DAE model learn scalar filtering operators, exhibiting a predominant diagonal structure in their corresponding mapping functions, limiting the exploitation of inter-frequency structure of music data. In contrast, skip-filtering connections are shown to assist the DAE model in learning filtering operators that exploit richer inter-frequency structures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {266–278},
numpages = {13}
}

@article{10.1109/TASLP.2019.2950597,
author = {Yu, Yi and He, Hongsen and Chen, Badong and Li, Jianghui and Zhang, Youwen and Lu, Lu},
title = {M-Estimate Based Normalized Subband Adaptive Filter Algorithm: Performance Analysis and Improvements},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2950597},
doi = {10.1109/TASLP.2019.2950597},
abstract = {This article studies the mean and mean-square behaviors of the M-estimate based normalized subband adaptive filter algorithm (M-NSAF) with robustness against impulsive noise. Based on the contaminated-Gaussian noise model, the stability condition, transient and steady-state results of the algorithm are formulated analytically. These analysis results help us to better understand the M-NSAF performance in impulsive noise. To further obtain fast convergence and low steady-state estimation error, we derive a variable step size (VSS) M-NSAF algorithm. This VSS scheme is also generalized to the proportionate M-NSAF variant for sparse systems. Computer simulations on the system identification in impulsive noise and the acoustic echo cancellation with double-talk are performed to demonstrate our theoretical analysis and the effectiveness of the proposed algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {225–239},
numpages = {15}
}

@article{10.1109/TASLP.2020.2982297,
author = {Fernando, Tharindu and Sridharan, Sridha and McLaren, Mitchell and Priyasad, Darshana and Denman, Simon and Fookes, Clinton},
title = {Temporarily-Aware Context Modeling Using Generative Adversarial Networks for Speech Activity Detection},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982297},
doi = {10.1109/TASLP.2020.2982297},
abstract = {This paper presents a novel framework for Speech Activity Detection (SAD). Inspired by the recent success of multi-task learning approaches in the speech processing domain, we propose a novel joint learning framework for SAD. We utilise generative adversarial networks to automatically learn a loss function for joint prediction of the frame-wise speech/ non-speech classifications together with the next audio segment. In order to exploit the temporal relationships within the input signal, we propose a temporal discriminator which aims to ensure that the predicted signal is temporally consistent. We evaluate the proposed framework on multiple public benchmarks, including NIST OpenSAT’ 17, AMI Meeting and HAVIC, where we demonstrate its capability to outperform state-of-the-art SAD approaches. Furthermore, our cross-database evaluations demonstrate the robustness of the proposed approach across different languages, accents, and acoustic environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1159–1169},
numpages = {11}
}

@article{10.1109/TASLP.2020.2975390,
author = {Schepker, Henning and Nordholm, Sven and Doclo, Simon},
title = {Acoustic Feedback Suppression for Multi-Microphone Hearing Devices Using a Soft-Constrained Null-Steering Beamformer},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2975390},
doi = {10.1109/TASLP.2020.2975390},
abstract = {Acoustic feedback occurs in hearing aids due to the coupling between the hearing aid loudspeaker and microphone(s). In order to reduce the acoustic feedback, adaptive filters are commonly used to estimate the feedback contribution in the microphone(s). While theoretically allowing for perfect feedback cancellation, in practice the adaptive filter typically converges to a biased optimal solution due to the closed-loop acoustical system of the hearing aid. Previously it has therefore been proposed to suppress the acoustic feedback contribution for an earpiece with multiple integrated microphones and loudspeakers using a fixed null-steering beamformer and hence avoiding a biased adaption. While previous null-steering beamforming approaches aimed at perfect preservation of the incoming signal using its relative transfer function (RTF), in this article we propose to use a soft constraint that allows to trade off between incoming signal preservation and feedback suppression. We formulate the computation of the beamformer coefficients both as a least-squares optimization procedure, aiming to minimize the residual feedback power, and as a min-max optimization procedure, aiming to directly maximize the maximum stable gain of the hearing aid. Experimental evaluations were performed using measured acoustic feedback paths from a custom earpiece with two microphones in the vent and a third microphone in the concha. Results show that the proposed fixed null-steering beamformer using the RTF-based soft constraint provides a reduction of the acoustic feedback by 7–8&nbsp;dB compared to the previously proposed RTF-based hard constraint while limiting the distortions of the incoming signal in the beamformer output.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {929–940},
numpages = {12}
}

@article{10.1109/TASLP.2019.2960721,
author = {Zhang, Jing-Xuan and Ling, Zhen-Hua and Dai, Li-Rong},
title = {Non-Parallel Sequence-to-Sequence Voice Conversion With Disentangled Linguistic and Speaker Representations},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2960721},
doi = {10.1109/TASLP.2019.2960721},
abstract = {This article presents a method of sequence-to-sequence (seq2seq) voice conversion using non-parallel training data. In this method, disentangled linguistic and speaker representations are extracted from acoustic features, and voice conversion is achieved by preserving the linguistic representations of source utterances while replacing the speaker representations with the target ones. Our model is built under the framework of encoder-decoder neural networks. A recognition encoder is designed to learn the disentangled linguistic representations with two strategies. First, phoneme transcriptions of training data are introduced to provide the references for leaning linguistic representations of audio signals. Second, an adversarial training strategy is employed to further wipe out speaker information from the linguistic representations. Meanwhile, speaker representations are extracted from audio signals by a speaker encoder. The model parameters are estimated by two-stage training, including a pre-training stage using a multi-speaker dataset and a fine-tuning stage using the dataset of a specific conversion pair. Since both the recognition encoder and the decoder for recovering acoustic features are seq2seq neural networks, there are no constrains of frame alignment and frame-by-frame conversion in our proposed method. Experimental results showed that our method obtained higher similarity and naturalness than the best non-parallel voice conversion method in Voice Conversion Challenge 2018. Besides, the performance of our proposed method was closed to the state-of-the-art parallel seq2seq voice conversion method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {540–552},
numpages = {13}
}

@article{10.1109/TASLP.2020.2982578,
author = {Ho, Chung-Ying and Shyu, Kuo-Kai and Chang, Cheng-Yuan and Kuo, Sen M.},
title = {Efficient Narrowband Noise Cancellation System Using Adaptive Line Enhancer},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982578},
doi = {10.1109/TASLP.2020.2982578},
abstract = {Rotating machines such as motors, generate noise at the fundamental frequency and its harmonics. The narrowband active noise control (NANC) algorithm is widely used to cancel such noise. Traditional feedforward NANC systems use non-acoustic sensors to measure rotation speeds, and then a bank of signal generators produce synchronized tonal signals as the reference signals according to the fundamental frequency of the undesired noise. However, the non-acoustic sensors such as encoders usually cost a lot and are not reliable. This study proposes using the feedback ANC structure and incorporates with several adaptive line enhancers (ALEs) to generate the reference signals. Without using non-acoustic sensors, the proposed system only updates the center frequency of the first ALE to track the fundamental frequency change, which greatly reduces complexity of the proposed ANC system. The frequency mismatch (FM) problem is overcome by properly setting the bandwidths of the ALE. Performance were verified by using both simulations and real-time experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1094–1103},
numpages = {10}
}

@article{10.1109/TASLP.2019.2960734,
author = {Fahim, Abdullah and Samarasinghe, Prasanga N. and Abhayapala, Thushara D.},
title = {Multi-Source DOA Estimation Through Pattern Recognition of the Modal Coherence of a Reverberant Soundfield},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2960734},
doi = {10.1109/TASLP.2019.2960734},
abstract = {We propose a novel multi-source direction of arrival (DOA) estimation technique using a convolutional neural network algorithm which learns the modal coherence patterns of an incident soundfield through measured spherical harmonic coefficients. We train our model for individual time-frequency bins in the short-time Fourier transform spectrum by analyzing the unique snapshot of modal coherence for each desired direction. The proposed method is capable of estimating simultaneously active multiple sound sources on a 3D space using a single-source training scheme. This single-source training scheme reduces the training time and resource requirements as well as allows the reuse of the same trained model for different multi-source combinations. The method is evaluated against various simulated and practical noisy and reverberant environments with varying acoustic criteria and found to outperform the baseline methods in terms of DOA estimation accuracy. Furthermore, the proposed algorithm allows independent training of azimuth and elevation during a full DOA estimation over 3D space which significantly improves its training efficiency without affecting the overall estimation accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {605–618},
numpages = {14}
}

@article{10.1109/TASLP.2019.2949687,
author = {Su, Ming-Hsiang and Wu, Chung-Hsien and Chen, Liang-Yu},
title = {Attention-Based Response Generation Using Parallel Double Q-Learning for Dialog Policy Decision in a Conversational System},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2949687},
doi = {10.1109/TASLP.2019.2949687},
abstract = {This article proposes an approach to response generation using a Parallel Double Q-learning algorithm for dialog policy decision in a conversational system. First, a new semantic representation of the user's input sentence is presented by using the CKIP parser to derive the semantic dependency sequence of the input sentence. Then, a Gated Recurrent Unit-based Autoencoder is used to obtain the user's turn representation as well as context representation. A Parallel Double Q-learning algorithm with a Deep Neural Network (PD-DQN), combining two Double DQNs in parallel for the contextual and semantic information in the user's message, respectively, are proposed to determine the dialog act. Finally, the user's input and the determined dialog act are fed to an attention-based Transformer model to generate the response template. With the generated response template, the semantic slots are filled with their corresponding values to obtain the final sentence response. This article collects a multi-turn conversation database consisting of 4186 turns in the travel domain and 447 chitchat question-answer pairs as the evaluation corpus. Five-fold cross validation is employed for performance evaluation. Experimental results show that the proposed approach based on semantic dependency for intent detection increases the accuracy by 4.3%. For dialog policy decision, the PD-DQN achieves 87.57% task success rate, which is 13.9% higher than the baseline Double DQN (73.67%). Finally, using the attention-based Transformer for response template generation obtains a Bleu score of 13.6, improved by 1.5 compared to the Sequence-to-Sequence model. In subjective evaluation, both the dialog policy and sentence generation model achieve a higher appropriateness and grammatical correctness scores than the baseline system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {131–143},
numpages = {13}
}

@article{10.1109/TASLP.2020.2986877,
author = {T, Lavanya and T, Nagarajan and P, Vijayalakshmi},
title = {Multi-Level Single-Channel Speech Enhancement Using a Unified Framework for Estimating Magnitude and Phase Spectra},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2986877},
doi = {10.1109/TASLP.2020.2986877},
abstract = {Speech enhancement algorithms aim to improve the quality and intelligibility of noise corrupted speech, through spectral or temporal modifications. Most of the existing speech enhancement algorithms achieve this by modifying the magnitude spectrum alone, while keeping the phase spectrum intact. In the current work, both phase and magnitude spectra are modified to enhance noisy speech using a multi-level speech enhancement technique. Proposed phase compensation (PC) function achieves first-level enhancement by modifying the phase spectrum alone. Second-level enhancement performs energy redistribution in the phase compensated speech signal to make weak speech and non-speech regions highly contrastive. Energy redistribution from the energy-rich voiced to the weak unvoiced regions is carried out using adaptive power law transformation (APLT) technique by optimizing the parameters with a total energy constraint employing particle swarm optimization algorithm. Log MMSE technique with a novel speech presence uncertainty (SPU) estimation method is proposed for third-level enhancement. The compensated phase spectrum and the magnitude spectrum estimated using log MMSE, with proposed SPU estimation (log MMSE + proposed SPU), are used to reconstruct the enhanced speech signal. The proposed speech enhancement technique is compared with recent speech enhancement techniques that estimate both magnitude and phase, for various noise levels (−5 to +5&nbsp;dB), in terms of objective and subjective measures. It is observed that the proposed technique improves signal quality and maintains or improves intelligibility under stationary, and non-stationary noise conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1315–1327},
numpages = {13}
}

@article{10.1109/TASLP.2019.2959721,
author = {Li, Ruizhi and Wang, Xiaofei and Mallidi, Sri Harish and Watanabe, Shinji and Hori, Takaaki and Hermansky, Hynek},
title = {Multi-Stream End-to-End Speech Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2959721},
doi = {10.1109/TASLP.2019.2959721},
abstract = {Attention-based methods and Connectionist Temporal Classification (CTC) network have been promising research directions for end-to-end (E2E) Automatic Speech Recognition (ASR). The joint CTC/Attention model has achieved great success by utilizing both architectures during multi-task training and joint decoding. In this article, we present a multi-stream framework based on joint CTC/Attention E2E ASR with parallel streams represented by separate encoders aiming to capture diverse information. On top of the regular attention networks, the Hierarchical Attention Network (HAN) is introduced to steer the decoder toward the most informative encoders. A separate CTC network is assigned to each stream to force monotonic alignments. Two representative framework have been proposed and discussed, which are Multi-Encoder Multi-Resolution (MEM-Res) framework and Multi-Encoder Multi-Array (MEM-Array) framework, respectively. In MEM-Res framework, two heterogeneous encoders with different architectures, temporal resolutions and separate CTC networks work in parallel to extract complementary information from same acoustics. Experiments are conducted on Wall Street Journal (WSJ) and CHiME-4, resulting in relative Word Error Rate (WER) reduction of <inline-formula><tex-math notation="LaTeX">$text{18.0}!-!text{32.1}%$</tex-math></inline-formula> and the best WER of <inline-formula><tex-math notation="LaTeX">$text{3.6}%$</tex-math></inline-formula> in the WSJ eval92 test set. The MEM-Array framework aims at improving the far-field ASR robustness using multiple microphone arrays which are activated by separate encoders. Compared with the best single-array results, the proposed framework has achieved relative WER reduction of <inline-formula><tex-math notation="LaTeX">$text{3.7}%$</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX">$text{9.7}%$</tex-math></inline-formula> in AMI and DIRHA multi-array corpora, respectively, which also outperforms conventional fusion strategies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {646–655},
numpages = {10}
}

@article{10.1109/TASLP.2020.2987748,
author = {Zuo, Huanyu and Samarasinghe, Prasanga N. and Abhayapala, Thushara D.},
title = {Intensity Based Spatial Soundfield Reproduction Using an Irregular Loudspeaker Array},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2987748},
doi = {10.1109/TASLP.2020.2987748},
abstract = {Sound intensity is an acoustic quantity closely linked with human perception of sound location, and it can be controlled to create a high level of realism to humans in soundfield reproduction systems. In this article, we present an intensity matching technique to optimally reproduce sound intensity over a continuous spatial region using an irregular loudspeaker array. This avoids several known limitations in the previous works on intensity based soundfield reproduction, such as a single sweet spot for the listener and a regular loudspeaker geometry that is difficult to implement in real-world applications. In contrast to the previous works, the new technique uses a cost function we built to optimize sound intensity over space by exploiting spatial sound intensity distributions. The spatial sound intensity distribution is represented by spherical harmonic coefficients of sound pressure, which are widely used to describe a spatial soundfield. Compared to the conventional spatial soundfield reproduction method of pressure matching in the spherical harmonic domain and the HOA <inline-formula><tex-math notation="LaTeX">$max$</tex-math></inline-formula>-<inline-formula><tex-math notation="LaTeX">$r_E$</tex-math></inline-formula> decoding method optimizing sound intensity at a single position, we show that the intensity matching technique has better overall performance with two different irregular loudspeaker layouts through simulations. The impact of microphone noise on reproduction performance is also assessed. Finally, we carry out perceptual localization experiments to validate the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1356–1369},
numpages = {14}
}

@article{10.1109/TASLP.2019.2947777,
author = {Amini, Jamal and Hendriks, Richard Christian and Heusdens, Richard and Guo, Meng and Jensen, Jesper},
title = {Rate-Constrained Noise Reduction in Wireless Acoustic Sensor Networks},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2947777},
doi = {10.1109/TASLP.2019.2947777},
abstract = {Wireless acoustic sensor networks (WASNs) can be used for centralized multi-microphone noise reduction, where the processing is done in a fusion center (FC). To perform the noise reduction, the data needs to be transmitted to the FC. Considering the limited battery life of the devices in a WASN, the total data rate at which the FC can communicate with the different network devices should be constrained. In this article, we propose a rate-constrained multi-microphone noise reduction algorithm, which jointly finds the best rate allocation and estimation weights for the microphones across all frequencies. The optimal linear estimators are found to be the quantized Wiener filters, and the rates are the solutions to a filter-dependent reverse water-filling problem. The performance of the proposed framework is evaluated using simulations in terms of mean square error and predicted speech intelligibility. The results show that the proposed method is very close in performance to that of the existing optimal method based on discrete optimization. However, the proposed approach can do this at a much lower complexity, while the existing optimal reference method needs a non-tractable exhaustive search to find the best rate allocation across microphones.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1–12},
numpages = {12}
}

@article{10.1109/TASLP.2019.2955252,
author = {Zhang, Weijian and Song, Peng},
title = {Transfer Sparse Discriminant Subspace Learning for Cross-Corpus Speech Emotion Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955252},
doi = {10.1109/TASLP.2019.2955252},
abstract = {Cross-corpus speech emotion recognition has attracted much attention due to the widespread existence of various emotional speech in life. It takes one corpus for training and another corpus for testing, and generally involves the following two basic problems: the corpus-invariant feature representation and relevance across different corpora. To deal with these two problems, we propose a novel transfer learning method called transfer sparse discriminant subspace learning (TSDSL) in this article. Specifically, to solve the first problem, we learn a common feature subspace of different corpora by introducing the discriminative learning and <inline-formula><tex-math notation="LaTeX">$ell _{2,1}-$</tex-math></inline-formula>norm penalty, which can learn the most discriminative features across different corpora. To address the second problem, we construct a novel nearest neighbor graph as the distance metric, in which the similarity between different corpora can be measured simultaneously. Extensive experiments are carried out on cross-corpus speech emotion recognition tasks, and the results show that our method can achieve competitive performance compared with state-of-the-art algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {307–318},
numpages = {12}
}

@article{10.1109/TASLP.2020.2982282,
author = {Sun, Haipeng and Wang, Rui and Chen, Kehai and Utiyama, Masao and Sumita, Eiichiro and Zhao, Tiejun},
title = {Unsupervised Neural Machine Translation With Cross-Lingual Language Representation Agreement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982282},
doi = {10.1109/TASLP.2020.2982282},
abstract = {Unsupervised cross-lingual language representation initialization methods such as unsupervised bilingual word embedding (UBWE) pre-training and cross-lingual masked language model (CMLM) pre-training, together with mechanisms such as denoising and back-translation, have advanced unsupervised neural machine translation (UNMT), which has achieved impressive results on several language pairs, particularly French-English and German-English. Typically, UBWE focuses on initializing the word embedding layer in the encoder and decoder of UNMT, whereas the CMLM focuses on initializing the entire encoder and decoder of UNMT. However, UBWE/CMLM training and UNMT training are independent, which makes it difficult to assess how the quality of UBWE/CMLM affects the performance of UNMT during UNMT training. In this paper, we first empirically explore relationships between UNMT and UBWE/CMLM. The empirical results demonstrate that the performance of UBWE and CMLM has a significant influence on the performance of UNMT. Motivated by this, we propose a novel UNMT structure with cross-lingual language representation agreement to capture the interaction between UBWE/CMLM and UNMT during UNMT training. Experimental results on several language pairs demonstrate that the proposed UNMT models improve significantly over the corresponding state-of-the-art UNMT baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1170–1182},
numpages = {13}
}

@article{10.1109/TASLP.2019.2955858,
author = {Duan, Richeng and Kawahara, Tatsuya and Dantsuji, Masatake and Nanjo, Hiroaki},
title = {Cross-Lingual Transfer Learning of Non-Native Acoustic Modeling for Pronunciation Error Detection and Diagnosis},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955858},
doi = {10.1109/TASLP.2019.2955858},
abstract = {In computer-assisted pronunciation training (CAPT), the scarcity of large-scale non-native corpora and human expert annotations are two fundamental challenges to non-native acoustic modeling. Most existing approaches of acoustic modeling in CAPT are based on non-native corpora while there are so many living languages in the world. It is impractical to collect and annotate every non-native speech corpus considering different language pairs. In this work, we address non-native acoustic modeling (both on phonetic and articulatory level) based on transfer learning. In order to effectively train acoustic models of non-native speech without using such data, we propose to exploit two large native speech corpora of learner's native language (L1) and target language (L2) to model cross-lingual phenomena. This kind of transfer learning can provide a better feature representation of non-native speech. Experimental evaluations are carried out for Japanese speakers learning English. We first demonstrate the proposed acoustic-phone model achieves a lower word error rate in non-native speech recognition. It also improves the pronunciation error detection based on goodness of pronunciation (GOP) score. For diagnosis of pronunciation errors, the proposed acoustic-articulatory modeling method is effective for providing detailed feedback at the articulation level.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {391–401},
numpages = {11}
}

@article{10.1109/TASLP.2019.2948770,
author = {Mitsufuji, Yuki and Uhlich, Stefan and Takamune, Norihiro and Kitamura, Daichi and Koyama, Shoichi and Saruwatari, Hiroshi},
title = {Multichannel Non-Negative Matrix Factorization Using Banded Spatial Covariance Matrices in Wavenumber Domain},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2948770},
doi = {10.1109/TASLP.2019.2948770},
abstract = {Blind source separation exploiting multichannel information has long been a popular topic, and recently proposed methods based on the local Gaussian model have shown promising results despite its high computational cost for the case of many microphone signals. The low updating speed for such a model is mainly due to the inversion of a spatial covariance matrix, for which the complexity increases with the number of microphones, <inline-formula><tex-math notation="LaTeX">$M$</tex-math></inline-formula>, and is generally of order <inline-formula><tex-math notation="LaTeX">$O(M^3)$</tex-math></inline-formula>. Several projection-based approaches that attempt to concentrate energy on the diagonal part of the spatial covariance matrix have been introduced to circumvent the matrix inversion, which can reduce the complexity to <inline-formula><tex-math notation="LaTeX">$O(M)$</tex-math></inline-formula>. In this article, we focus on the fast Fourier transform as a projection method because the energy concentration on the diagonal can be efficiently achieved compared with other projection-based methods. For the case where the diagonalization is imperfect, for example, owing to discontinuities at the edge of a linear array, we also developed a more robust algorithm approximating the tri-diagonal part of the spatial covariance matrix, which requires a complexity of <inline-formula><tex-math notation="LaTeX">$O(M^2)$</tex-math></inline-formula> for the inversion by applying the Thomas algorithm. To remove the ad-hoc integration of post clustering after the decomposition, we also examine a self-clustering algorithm. Our evaluation shows better results than other previously proposed methods in terms of the separation quality under reverberant conditions as well as higher efficiency than multichannel non-negative matrix factorization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {49–60},
numpages = {12}
}

@article{10.1109/TASLP.2020.2964958,
author = {Koyama, Shoichi and Chardon, Gilles and Daudet, Laurent},
title = {Optimizing Source and Sensor Placement for Sound Field Control: An Overview},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2964958},
doi = {10.1109/TASLP.2020.2964958},
abstract = {In order to control an acoustic field inside a target region, it is important to choose suitable positions of secondary sources (loudspeakers) and sensors (control points/microphones). This article provides an overview of state-of-the-art source and sensor placement methods in sound field control. Although the placement of both sources and sensors greatly affects control accuracy and filter stability, their joint optimization has not been thoroughly investigated in the acoustics literature. In this context, we reformulate five general source and/or sensor placement methods that can be applied for sound field control. We compare the performance of these methods through extensive numerical simulations in both narrowband and broadband scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {696–714},
numpages = {19}
}

@article{10.1109/TASLP.2020.2970241,
author = {Ai, Yang and Ling, Zhen-Hua},
title = {A Neural Vocoder With Hierarchical Generation of Amplitude and Phase Spectra for Statistical Parametric Speech Synthesis},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2970241},
doi = {10.1109/TASLP.2020.2970241},
abstract = {This article presents a neural vocoder named HiNet which reconstructs speech waveforms from acoustic features by predicting amplitude and phase spectra hierarchically. Different from existing neural vocoders such as WaveNet, SampleRNN and WaveRNN which directly generate waveform samples using single neural networks, the HiNet vocoder is composed of an amplitude spectrum predictor (ASP) and a phase spectrum predictor (PSP). The ASP is a simple DNN model which predicts log amplitude spectra (LAS) from acoustic features. The predicted LAS are sent into the PSP for phase recovery. Considering the issue of phase warping and the difficulty of phase modeling, the PSP is constructed by concatenating a neural source-filter (NSF) waveform generator with a phase extractor. We also introduce generative adversarial networks (GANs) into both ASP and PSP. Finally, the outputs of ASP and PSP are combined to reconstruct speech waveforms by short-time Fourier synthesis. Since there are no autoregressive structures in both predictors, the HiNet vocoder can generate speech waveforms with high efficiency. Objective and subjective experimental results show that our proposed HiNet vocoder achieves better naturalness of reconstructed speech than the conventional STRAIGHT vocoder, a 16-bit WaveNet vocoder using open source implementation and an NSF vocoder with similar complexity to the PSP and obtains similar performance with a 16-bit WaveRNN vocoder. We also find that the performance of HiNet is insensitive to the complexity of the neural waveform generator in PSP to some extend. After simplifying its model structure, the time consumed for generating 1&nbsp;s waveforms of 16&nbsp;kHz speech using a GPU can be further reduced from 0.34&nbsp;s to 0.19&nbsp;s without significant quality degradation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {839–851},
numpages = {13}
}

@article{10.1109/TASLP.2019.2946789,
author = {Wang, Peidong and Tan, Ke and Wang, De Liang},
title = {Bridging the Gap Between Monaural Speech Enhancement and Recognition With Distortion-Independent Acoustic Modeling},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2946789},
doi = {10.1109/TASLP.2019.2946789},
abstract = {Monaural speech enhancement has made dramatic advances since the introduction of deep learning a few years ago. Although enhanced speech has been demonstrated to have better intelligibility and quality for human listeners, feeding it directly to automatic speech recognition (ASR) systems trained with noisy speech has not produced expected improvements in ASR performance. The lack of an enhancement benefit on recognition, or the gap between monaural speech enhancement and recognition, is often attributed to speech distortions introduced in the enhancement process. In this article, we analyze the distortion problem, compare different acoustic models, and investigate a distortion-independent training scheme for monaural speech recognition. Experimental results suggest that distortion-independent acoustic modeling is able to overcome the distortion problem. Such an acoustic model can also work with speech enhancement models different from the one used during training. Moreover, the models investigated in this paper outperform the previous best system on the CHiME-2 corpus.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {39–48},
numpages = {10}
}

@article{10.1109/TASLP.2020.2966891,
author = {Dietzen, Thomas and Doclo, Simon and Moonen, Marc and van Waterschoot, Toon},
title = {Square Root-Based Multi-Source Early PSD Estimation and Recursive RETF Update in Reverberant Environments by Means of the Orthogonal Procrustes Problem},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2966891},
doi = {10.1109/TASLP.2020.2966891},
abstract = {Multi-channel short-time Fourier transform (STFT) domain-based processing of reverberant microphone signals commonly relies on power-spectral-density (PSD) estimates of early source images, where early refers to reflections contained within the same STFT frame. State-of-the-art approaches to multi-source early PSD estimation, given an estimate of the associated relative early transfer functions (RETFs), conventionally minimize the approximation error defined with respect to the early correlation matrix, requiring non-negative inequality constraints on the PSDs. Instead, we here propose to factorize the early correlation matrix and minimize the approximation error defined with respect to the early-correlation-matrix square root. The proposed minimization problem—constituting a generalization of the so-called orthogonal Procrustes problem—seeks a unitary matrix and the square roots of the early PSDs up to an arbitrary complex argument, whereby non-negative inequality constraints become redundant. A solution is obtained iteratively, requiring one singular value decomposition (SVD) per iteration. The estimated unitary matrix and early PSD square roots further allow to recursively update the RETF estimate, which is not inherently possible in the conventional approach. An estimate of the said early-correlation-matrix square root itself is obtained by means of the generalized eigenvalue decomposition (GEVD), where we further propose to restore non-stationarities by desmoothing the generalized eigenvalues in order to compensate for inevitable recursive averaging. Simulation results indicate fast convergence of the proposed multi-source early PSD estimation approach in only one iteration if initialized appropriately, and better performance as compared to the conventional approach. A MATLAB implementation is available.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {755–769},
numpages = {15}
}

@article{10.1109/TASLP.2019.2950099,
author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi and King, Simon and Tokuda, Keiichi},
title = {A Vector Quantized Variational Autoencoder (VQ-VAE) Autoregressive Neural <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> Model for Statistical Parametric Speech Synthesis},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2950099},
doi = {10.1109/TASLP.2019.2950099},
abstract = {Recurrent neural networks (RNNs) can predict fundamental frequency (F0) for statistical parametric speech synthesis systems, given linguistic features as input. However, these models assume conditional independence between consecutive <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> values, given the RNN state. In a previous study, we proposed autoregressive (AR) neural <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> models to capture the causal dependency of successive <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> values. In subjective evaluations, a deep AR model (DAR) outperformed an RNN. Here, we propose a Vector Quantized Variational Autoencoder (VQ-VAE) neural <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> model that is both more efficient and more interpretable than the DAR. This model has two stages: one uses the VQ-VAE framework to learn a latent code for the <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> contour of each linguistic unit, and other learns to map from linguistic features to latent codes. In contrast to the DAR and RNN, which process the input linguistic features frame-by-frame, the new model converts one linguistic feature vector into one latent code for each linguistic unit. The new model achieves better objective scores than the DAR, has a smaller memory footprint and is computationally faster. Visualization of the latent codes for phones and moras reveals that each latent code represents an <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> shape for a linguistic unit.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {157–170},
numpages = {14}
}

@article{10.1109/TASLP.2020.2973896,
author = {Scharenborg, Odette and Ondel, Lucas and Palaskar, Shruti and Arthur, Philip and Ciannella, Francesco and Du, Mingxing and Larsen, Elin and Merkx, Danny and Riad, Rachid and Wang, Liming and Dupoux, Emmanuel and Besacier, Laurent and Black, Alan and Hasegawa-Johnson, Mark and Metze, Florian and Neubig, Graham and St\"{u}ker, Sebastian and Godard, Pierre and M\"{u}ller, Markus},
title = {Speech Technology for Unwritten Languages},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2973896},
doi = {10.1109/TASLP.2020.2973896},
abstract = {Speech technology plays an important role in our everyday life. Among others, speech is used for human-computer interaction, for instance for information retrieval and on-line shopping. In the case of an unwritten language, however, speech technology is unfortunately difficult to create, because it cannot be created by the standard combination of pre-trained speech-to-text and text-to-speech subsystems. The research presented in this article takes the first steps towards speech technology for unwritten languages. Specifically, the aim of this work was 1) to learn speech-to-meaning representations without using text as an intermediate representation, and 2) to test the sufficiency of the learned representations to regenerate speech or translated text, or to retrieve images that depict the meaning of an utterance in an unwritten language. The results suggest that building systems that go directly from speech-to-meaning and from meaning-to-speech, bypassing the need for text, is possible.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {964–975},
numpages = {12}
}

@article{10.1109/TASLP.2020.2969845,
author = {Gribben, Christopher and Lee, Hyunkook},
title = {The Perception of Band-Limited Decorrelation Between Vertically Oriented Loudspeakers},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2969845},
doi = {10.1109/TASLP.2020.2969845},
abstract = {Two experiments have been conducted to investigate the perceptual effect of band-limited interchannel decorrelation between vertically oriented loudspeakers. The perceived vertical image spread (VIS) and tonal quality (TQ) of phantom auditory images have been subjectively assessed in multiple comparison trials. The aim of the article was to find a lower decorrelation boundary that provides a significant increase of VIS, whilst maintaining TQ close to that of the original source. For test stimuli, decorrelation was applied to natural sound sources and pink noise in groups of octave-bands, where the lowest band was varied between 63&nbsp;Hz and 8&nbsp;kHz and the upper band was fixed at 16&nbsp;kHz, resulting in eight decorrelated conditions for each source. Unprocessed octave-bands below the lower boundary were reproduced simultaneously through the lower main-layer loudspeaker only, and a monophonic main-layer only condition was also included in the comparison alongside the decorrelated stimuli. Results reveal that vertical decorrelation of the 500&nbsp;Hz octave-band and above tends to significantly increase VIS, similar to that of broadband decorrelation, with little impact on TQ. In some cases, decorrelation of higher octave-bands and above can also produce similar increases of VIS with less impact on TQ, however, this is shown to be largely source-dependent. These results suggest that vertical decorrelation of lower frequencies has little perceptual benefit, and band-limiting vertical decorrelation to higher frequencies is likely to reduce low frequency phase cancellation. Applications of such an approach include 2D-to-3D upmixing and binaural audio rendering, with additional implications for 3D audio recording.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {876–888},
numpages = {13}
}

@article{10.1109/TASLP.2020.2964960,
author = {Liu, Yijia and Che, Wanxiang and Qin, Bing and Liu, Ting},
title = {Exploring Segment Representations for Neural Semi-Markov Conditional Random Fields},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2964960},
doi = {10.1109/TASLP.2020.2964960},
abstract = {Many problems in natural language processing (NLP) can be cast as the problem of segmenting a sequence. In this article, we combine the semi-Markov conditional random fields (semi-CRF) with neural networks to solve NLP segmentation problems. We focus on the segment representation in neural semi-CRF which is important to the performance. Based on our preliminary work in Liu <italic>et&nbsp;al.</italic> <xref ref-type="bibr" rid="ref1">[1]</xref>, we represent a segment by both encoding the subsequence and embedding the segment string. We conduct a systematic study of the utility of various components in subsequence encoding and propose a method of constructing and deriving segment string embeddings. Extensive experiments on three typical segmentation problems, namely, shallow syntax parsing, named entity recognition, and Chinese word segmentation are conducted. The results show that we can achieve equally-performed subsequence encoding with a three times faster concatenation network compared to previous work. The results also show that the segment string embeddings help our neural semi-CRF model to achieve a macro-averaged error reduction of 13.15% over a strong baseline using deep contextualized embeddings and bidirectional long-short-term memory CRF, which also show the usefulness of semi-CRF even with contextualized embeddings. These results are competitive with the state-of-the-art segmentation systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {813–824},
numpages = {12}
}

@article{10.1109/TASLP.2019.2962689,
author = {Laufer, Yaron and Laufer-Goldshtein, Bracha and Gannot, Sharon},
title = {ML Estimation and CRBs for Reverberation, Speech, and Noise PSDs in Rank-Deficient Noise Field},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2962689},
doi = {10.1109/TASLP.2019.2962689},
abstract = {Speech communication systems are prone to performance degradation in reverberant and noisy acoustic environments. Dereverberation and noise reduction algorithms typically require several model parameters, e.g.&nbsp;the speech, reverberation and noise power spectral densities (PSDs). A commonly used assumption is that the noise PSD matrix is known. However, in practical acoustic scenarios, the noise PSD matrix is unknown and should be estimated along with the speech and reverberation PSDs. In this article, we consider the case of rank-deficient noise PSD matrix, which arises when the noise signal consists of multiple directional noise sources, whose number is less than the number of microphones. We derive two closed-form maximum likelihood estimators (MLEs). The first is a non-blocking-based estimator which jointly estimates the speech, reverberation and noise PSDs, and the second is a blocking-based estimator, which first blocks the speech signal and then jointly estimates the reverberation and noise PSDs. Both estimators are analytically compared and analyzed, and mean square errors (MSEs) expressions are derived. Furthermore, Cramér-Rao Bounds (CRBs) on the estimated PSDs are derived. The proposed estimators are examined using both simulation and real reverberant and noisy signals, demonstrating the advantage of the proposed method compared to competing estimators.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {619–634},
numpages = {16}
}

@article{10.1109/TASLP.2020.2980974,
author = {Schymura, Christopher and Kolossa, Dorothea},
title = {Audiovisual Speaker Tracking Using Nonlinear Dynamical Systems With Dynamic Stream Weights},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2980974},
doi = {10.1109/TASLP.2020.2980974},
abstract = {Data fusion plays an important role in many technical applications that require efficient processing of multimodal sensory observations. A prominent example is audiovisual signal processing, which has gained increasing attention in automatic speech recognition, speaker localization and related tasks. If appropriately combined with acoustic information, additional visual cues can help to improve the performance in these applications, especially under adverse acoustic conditions. A dynamic weighting of acoustic and visual streams based on instantaneous sensor reliability measures is an efficient approach to data fusion in this context. This article presents a framework that extends the well-established theory of nonlinear dynamical systems with the notion of dynamic stream weights for an arbitrary number of sensory observations. It comprises a recursive state estimator based on the Gaussian filtering paradigm, which incorporates dynamic stream weights into a framework closely related to the extended Kalman filter. Additionally, a convex optimization approach to estimate oracle dynamic stream weights in fully observed dynamical systems utilizing a Dirichlet prior is presented. This serves as a basis for a generic parameter learning framework of dynamic stream weight estimators. The proposed system is application-independent and can be easily adapted to specific tasks and requirements. A study using audiovisual speaker tracking tasks is considered as an exemplary application in this work. An improved tracking performance of the dynamic stream-weight-based estimation framework over state-of-the-art methods is demonstrated in the experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1065–1078},
numpages = {14}
}

@article{10.1109/TASLP.2020.2987130,
author = {Ycart, Adrien and Benetos, Emmanouil},
title = {Learning and Evaluation Methodologies for Polyphonic Music Sequence Prediction With LSTMs},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2987130},
doi = {10.1109/TASLP.2020.2987130},
abstract = {Music language models play an important role for various music signal and symbolic music processing tasks, such as music generation, symbolic music classification, or automatic music transcription (AMT). In this article, we investigate Long Short-Term Memory (LSTM) networks for polyphonic music prediction, in the form of binary piano rolls. A preliminary experiment, assessing the influence of the timestep of piano rolls on system performance, highlights the need for more musical evaluation metrics. We introduce a range of metrics, focusing on temporal and harmonic aspects. We propose to combine them into a parametrisable loss to train our network. We then conduct a range of experiments with this new loss, both for polyphonic music prediction (intrinsic evaluation) and using our predictive model as a language model for AMT (extrinsic evaluation). Intrinsic evaluation shows that tuning the behaviour of a model is possible by adjusting loss parameters, with consistent results across timesteps. Extrinsic evaluation shows consistent behaviour across timesteps in terms of precision and recall with respect to the loss parameters, leading to an improvement in AMT performance without changing the complexity of the model. In particular, we show that intrinsic performance (in terms of cross entropy) is not related to extrinsic performance, highlighting the importance of using custom training losses for each specific application. Our model also compares favourably with previously proposed MLMs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1328–1341},
numpages = {14}
}

@article{10.1109/TASLP.2020.2985066,
author = {Kodrasi, Ina and Bourlard, Herv\'{e}},
title = {Spectro-Temporal Sparsity Characterization for Dysarthric Speech Detection},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2985066},
doi = {10.1109/TASLP.2020.2985066},
abstract = {To assist the clinical diagnosis and treatment of neurological diseases that cause speech dysarthria such as Parkinson's disease&nbsp;(PD), it is of paramount importance to craft robust features which can be used to automatically discriminate between healthy and dysarthric speech. Since dysarthric speech of patients suffering from PD is breathy, semi-whispery, and is characterized by abnormal pauses and imprecise articulation, it can be expected that its spectro-temporal sparsity differs from the spectro-temporal sparsity of healthy speech. While we have recently successfully used temporal sparsity characterization for dysarthric speech detection, characterizing spectral sparsity poses the challenge of constructing a valid feature vector from signals with a different number of unaligned time frames. Further, although several non-parametric and parametric measures of sparsity exist, it is unknown which sparsity measure yields the best performance in the context of dysarthric speech detection. The objective of this paper is to demonstrate the advantages of spectro-temporal sparsity characterization for automatic dysarthric speech detection. To this end, we first provide a numerical analysis of the suitability of different non-parametric and parametric measures (i.e., <inline-formula><tex-math notation="LaTeX">$l_1$</tex-math></inline-formula>-norm, kurtosis, Shannon entropy, Gini index, shape parameter of a Chi distribution, and shape parameter of a Weibull distribution) for sparsity characterization. It is shown that kurtosis, the Gini index, and the parametric sparsity measures are advantageous sparsity measures, whereas the <inline-formula><tex-math notation="LaTeX">$l_1$</tex-math></inline-formula>-norm and entropy measures fail to robustly characterize the temporal sparsity of signals with a different number of time frames. Second, we propose to characterize the spectral sparsity of an utterance by initially time-aligning it to the same utterance uttered by a (arbitrarily selected) reference speaker using dynamic time warping. Experimental results on a Spanish database of healthy and dysarthric speech show that estimating the spectro-temporal sparsity using the Gini index or the parametric sparsity measures and using it as a feature in a support vector machine results in a high classification accuracy of 83.3%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1210–1222},
numpages = {13}
}

@article{10.1109/TASLP.2020.2975902,
author = {Wang, Zhong-Qiu and Wang, DeLiang},
title = {Deep Learning Based Target Cancellation for Speech Dereverberation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2975902},
doi = {10.1109/TASLP.2020.2975902},
abstract = {This article investigates deep learning based single- and multi-channel speech dereverberation. For single-channel processing, we extend magnitude-domain masking and mapping based dereverberation to complex-domain mapping, where deep neural networks (DNNs) are trained to predict the real and imaginary (RI) components of the direct-path signal from reverberant (and noisy) ones. For multi-channel processing, we first compute a minimum variance distortionless response (MVDR) beamformer to cancel the direct-path signal, and then feed the RI components of the cancelled signal, which is expected to be a filtered version of non-target signals, as additional features to perform dereverberation. Trained on a large dataset of simulated room impulse responses, our models show excellent speech dereverberation and recognition performance on the test set of the REVERB challenge, consistently better than single- and multi-channel weighted prediction error (WPE) algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {941–950},
numpages = {10}
}

@article{10.1109/TASLP.2019.2959251,
author = {Wang, Jin and Yu, Liang-Chih and Lai, K. Robert and Zhang, Xuejie},
title = {Tree-Structured Regional CNN-LSTM Model for Dimensional Sentiment Analysis},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2959251},
doi = {10.1109/TASLP.2019.2959251},
abstract = {Dimensional sentiment analysis aims to recognize continuous numerical values in multiple dimensions such as the valence-arousal (VA) space. Compared to the categorical approach that focuses on sentiment classification such as binary classification (i.e., positive and negative), the dimensional approach can provide a more fine-grained sentiment analysis. This article proposes a tree-structured regional CNN-LSTM model consisting of two parts: regional CNN and LSTM to predict the VA ratings of texts. Unlike a conventional CNN which considers a whole text as input, the proposed regional CNN uses a part of the text as a region, dividing an input text into several regions such that the useful affective information in each region can be extracted and weighted according to their contribution to the VA prediction. Such regional information is sequentially integrated across regions using LSTM for VA prediction. By combining the regional CNN and LSTM, both local (regional) information within sentences and long-distance dependencies across sentences can be considered in the prediction process. To further improve performance, a region division strategy is proposed to discover task-relevant phrases and clauses to incorporate structured information into VA prediction. Experimental results on different corpora show that the proposed method outperforms lexicon-, regression-, conventional NN and other structured NN methods proposed in previous studies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {581–591},
numpages = {11}
}

@article{10.1109/TASLP.2019.2957872,
author = {Yu, Jianfei and Jiang, Jing and Xia, Rui},
title = {Entity-Sensitive Attention and Fusion Network for Entity-Level Multimodal Sentiment Classification},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2957872},
doi = {10.1109/TASLP.2019.2957872},
abstract = {Entity-level (aka target-dependent) sentiment analysis of social media posts has recently attracted increasing attention, and its goal is to predict the sentiment orientations over individual target entities mentioned in users’ posts. Most existing approaches to this task primarily rely on the textual content, but fail to consider the other important data sources (e.g., images, videos, and user profiles), which can potentially enhance these text-based approaches. Motivated by the observation, we study <italic>entity-level multimodal sentiment classification</italic> in this article, and aim to explore the usefulness of images for entity-level sentiment detection in social media posts. Specifically, we propose an Entity-Sensitive Attention and Fusion Network (ESAFN) for this task. First, to capture the intra-modality dynamics, ESAFN leverages an effective attention mechanism to generate entity-sensitive textual representations, followed by aggregating them with a textual fusion layer. Next, ESAFN learns the entity-sensitive visual representation with an entity-oriented visual attention mechanism, followed by a gated mechanism to eliminate the noisy visual context. Moreover, to capture the inter-modality dynamics, ESAFN further fuses the textual and visual representations with a bilinear interaction layer. To evaluate the effectiveness of ESAFN, we manually annotate the sentiment orientation over each given entity based on two recently released multimodal NER datasets, and show that ESAFN can significantly outperform several highly competitive unimodal and multimodal methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {429–439},
numpages = {11}
}

@article{10.1109/TASLP.2020.2971417,
author = {Perrotin, Olivier and McLoughlin, Ian V.},
title = {Glottal Flow Synthesis for Whisper-to-Speech Conversion},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2971417},
doi = {10.1109/TASLP.2020.2971417},
abstract = {Whisper-to-speech conversion is motivated by laryngeal disorders, in which malfunction of the vocal folds leads to loss of voicing. Many patients with laryngeal disorders can still produce functional whispers, since these are characterised by the absence of vocal fold vibration. Whispers therefore constitute a common ground for speech rehabilitation across many kinds of laryngeal disorder. Whisper-to-speech conversion involves recreating natural-sounding speech from recorded whispers, and is a non-invasive and non-surgical rehabilitation that can maintain a natural method of speaking, unlike the existing methods of rehabilitation. This article proposes a new rule-based method for whisper-to-speech conversion that replaces the noisy whisper sound source with a synthesised speech-like harmonic source, while maintaining the vocal tract component unaltered. In particular, a novel glottal source generator is developed in which whisper information is used to parameterise the excitation through a high-quality glottis model. Evaluation of the system against the standard pulse train excitation method reveals significantly improved performance. Since our method is glottis-based, it is potentially compatible with the many existing vocal tract component adaptation systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {889–900},
numpages = {12}
}

@article{10.1109/TASLP.2019.2959224,
author = {Nishimura, Yuta and Sudoh, Katsuhito and Neubig, Graham and Nakamura, Satoshi},
title = {Multi-Source Neural Machine Translation With Missing Data},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2959224},
doi = {10.1109/TASLP.2019.2959224},
abstract = {Machine translation is rife with ambiguities in word ordering and word choice, and even with the advent of machine-learning methods that learn to resolve this ambiguity based on statistics from large corpora, mistakes are frequent. <italic>Multi-source translation</italic> is an approach that attempts to resolve these ambiguities by exploiting multiple inputs (e.g. sentences in three different languages) to increase translation accuracy. These methods are trained on multilingual corpora, which include the multiple source languages and the target language, and then at test time uses information from both source languages while generating the target. While there are many of these multilingual corpora, such as multilingual translations of TED talks or European parliament proceedings, in practice, many multilingual corpora are not complete due to the difficulty to provide translations in <italic>all</italic> of the relevant languages. Existing studies on multi-source translation did not explicitly handle such situations, and thus are only applicable to complete corpora that have all of the languages of interest, severely limiting their practical applicability. In this article, we examine approaches for multi-source neural machine translation (NMT) that can learn from and translate such incomplete corpora. Specifically, we propose methods to deal with incomplete corpora at both training time and test time. For training time, we examine two methods: (1) a simple method that simply replaces missing source translations with a special NULL symbol, and (2) a data augmentation approach that fills in incomplete parts with source translations created from multi-source NMT. For test-time, we examine methods that use multi-source translation even when only a single source is provided by first translating into an additional auxiliary language using standard NMT, then using multi-source translation on the original source and this generated auxiliary language sentence. Extensive experiments demonstrate that the proposed training-time and test-time methods both significantly improve translation performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {569–580},
numpages = {12}
}

@article{10.1109/TASLP.2019.2950596,
author = {Parcollet, Titouan and Morchid, Mohamed and Bost, Xavier and Linar\`{e}s, Georges and De Mori, Renato},
title = {Real to H-Space Autoencoders for Theme Identification in Telephone Conversations},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2950596},
doi = {10.1109/TASLP.2019.2950596},
abstract = {Machine learning (ML) and deep learning with deep neural networks (DNN), have drastically improved the performances of modern systems on numerous spoken language understanding (SLU) related tasks. Since most of current researches focus on new neural architectures to enhance the performances in realistic conditions, few recent works investigated the use of different algebras with neural networks (NN), to better represent the nature of the data being processed. To this extent, quaternion-valued neural networks (QNN) have shown better performances, and an important reduction of the number of neural parameters compared to traditional real-valued neural networks, when dealing with multidimensional signal. Nonetheless, the use of QNNs is strictly limited to quaternion input or output features. This article introduces a new unsupervised method based on a hybrid autoencoder (AE) called real-to-quaternion autoencoder (R2H), to extract a quaternion-valued input signal from any real-valued data, to be processed by QNNs. The experiments performed to identify the most related theme of a given telephone conversation from a customer care service (CCS), demonstrate that the R2H approach outperforms all the previously established models, either real- or quaternion-valued ones, in term of accuracy and with up to four times fewer neural parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {198–210},
numpages = {13}
}

@article{10.1109/TASLP.2020.2982799,
author = {Zhang, Qiaoling and Xu, Weiqiang and Zhang, Weiwei and Feng, Jie and Chen, Zhiyong},
title = {Multi-Hypothesis Square-Root Cubature Kalman Particle Filter for Speaker Tracking in Noisy and Reverberant Environments},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982799},
doi = {10.1109/TASLP.2020.2982799},
abstract = {In this paper, a multi-hypothesis square-root cubature Kalman particle filter (MH-SRCKPF) is proposed for speaker tracking in noisy and reverberant environments with distributed microphone arrays. The conventional cubature Kalman particle filter (CKPF) uses the cubature Kalman filter (CKF) to generate its proposal for particle sampling. Such a proposal incorporates only one observation from a certain localization function for the state estimation, which is vulnerable to noise or reverberation, yielding the degraded tracking performance. To tackle the problem, by incorporating multiple possible observations into CKF for the proposal, a multi-hypothesis CKPF (MH-CKPF) algorithm is first developed. Furthermore, to improve the numerical stability, an MH-SRCKPF algorithm is developed, where the state estimate and the square root of the error covariance are propagated at each time. Finally, the MH-SRCKPF is applied to the speaker tracking problems in distributed microphone arrays. Experimental results demonstrate that the proposed MH-SRCKPF outperforms the competing methods in the presence of noise and reverberation. Meanwhile, by propagating the square root of the state covariance, the proposed method exhibits attractive numerical characteristics.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1183–1197},
numpages = {15}
}

@article{10.1109/TASLP.2020.2969779,
author = {Aroudi, Ali and Doclo, Simon},
title = {Cognitive-Driven Binaural Beamforming Using EEG-Based Auditory Attention Decoding},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2969779},
doi = {10.1109/TASLP.2020.2969779},
abstract = {Identifying the target speaker in hearing aid applications is an essential ingredient to improve speech intelligibility. Recently, a least-squares-based auditory attention decoding (AAD) method has been proposed to identify the target speaker from single-trial EEG recordings in an acoustic scenario with two competing speakers. Aiming at enhancing the target speaker and suppressing the interfering speaker and ambient noise, in this article, we propose a cognitive-driven speech enhancement system, consisting of a binaural beamformer which is steered based on AAD and estimated relative transfer function (RTF) vectors, which require estimates of the direction-of-arrivals (DOAs) of both speakers. For binaural beamforming and to generate reference signals for AAD, we consider either minimum-variance-distortionless-response (MVDR) beamformers or linearly-constrained-minimum-variance (LCMV) beamformers. Contrary to the binaural MVDR beamformer, the binaural LCMV beamformer allows to preserve the spatial impression of the acoustic scene and to control the suppression of the interfering speaker, which is important when intending to switch attention between speakers. The speech enhancement performance of the proposed system is evaluated in terms of the binaural signal-to-interference-plus-noise ratio (<inline-formula><tex-math notation="LaTeX">$text {SINR}$</tex-math></inline-formula>) improvement in anechoic and reverberant conditions. Furthermore, we investigate the impact of RTF and DOA estimation errors and AAD errors on the speech enhancement performance. The experimental results show that the proposed system using LCMV beamformers yields a larger decoding performance and binaural <inline-formula><tex-math notation="LaTeX">$text {SINR}$</tex-math></inline-formula> improvement compared to using MVDR beamformers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {862–875},
numpages = {14}
}

@article{10.1109/TASLP.2019.2948773,
author = {Shin, Youhyun and Lee, Sang-goo},
title = {Learning Context Using Segment-Level LSTM for Neural Sequence Labeling},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2948773},
doi = {10.1109/TASLP.2019.2948773},
abstract = {This article introduces an approach that learns segment-level context for sequence labeling in natural language processing (NLP). Previous approaches limit their basic unit to a word for feature extraction because sequence labeling is a token-level task in which labels are annotated word-by-word. However, the text segment is an ultimate unit for labeling, and we are easily able to obtain segment information from annotated labels in a IOB/IOBES format. Most neural sequence labeling models expand their learning capacity by employing additional layers, such as a character-level layer, or jointly training NLP tasks with common knowledge. The architecture of our model is based on the charLSTM-BiLSTM-CRF model, and we extend the model with an additional segment-level layer called segLSTM. We therefore suggest a sequence labeling algorithm called charLSTM-BiLSTM-CRF-segLSTM<inline-formula><tex-math notation="LaTeX">$^{sLM}$</tex-math></inline-formula> which employs an additional segment-level long short-term memory (LSTM) that trains features by learning adjacent context in a segment. We demonstrate the performance of our model on four sequence labeling datasets, namely, Peen Tree Bank, CoNLL 2000, CoNLL 2003, and OntoNotes 5.0. Experimental results show that our model performs better than state-of-the-art variants of BiLSTM-CRF. In particular, the proposed model enhances the performance of tasks for finding appropriate labels of multiple token segments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {105–115},
numpages = {11}
}

@article{10.1109/TASLP.2019.2959222,
author = {Ghasemzadeh, Hamzeh and Arjmandi, Meisam K.},
title = {Toward Optimum Quantification of Pathology-Induced Noises: An Investigation of Information Missed by Human Auditory System},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2959222},
doi = {10.1109/TASLP.2019.2959222},
abstract = {Clinical diagnosis of voice disorder and evaluation of therapy outcome heavily rely on accurate quantification of voice quality, which is closely tied to the physiology and function of the laryngeal mechanism. Considering the evaluation methodology of the voice, two main categories of auditory-perceptual assessment and acoustic analysis can be identified. This article presents a new approach for acoustic analysis of voice quality, which brings several advantages to the field. The proposed approach is non-parametric in the sense that it does not require the estimation of the fundamental frequency or spectral response of the vocal tract. This reduces the computational complexity of the measurement and reduces the possible errors due to inaccurate estimation of those parameters. Additionally, the method does not make any assumption about the phonetic context and hence has the potential to be applied to connected speech. The proposed method benefits from the multiresolution structure of the wavelet analysis for estimating the noisy component of a voice in the spectro-temporal domain. The informativeness of the estimated noise for voice quality distinction is examined based on different noise-quantification approaches. It is shown that deviation from the model of the human auditory system (HAS) leads to performance improvement. Through several analyses, it is argued that using models of HAS for quantification of the noise leads to significant loss of information relevant to voice quality. Findings from this article suggest that perception-based measures of voice quality are highly restricted in capturing important aspects of acoustic that could assist with voice quality distinctions. This characteristic is inherent to HAS and cannot be alleviated, highlighting a significant limitation of perception-based measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {519–528},
numpages = {10}
}

@article{10.1109/TASLP.2019.2955286,
author = {Morgenstern, Hai and Rafaely, Boaz},
title = {Perceptually-Transparent Online Estimation of Two-Channel Room Transfer Function for Sound Calibration},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955286},
doi = {10.1109/TASLP.2019.2955286},
abstract = {Sound calibration is employed in many commercial audio systems for improving sound quality. This process includes the estimation of the room transfer function (RTF) between each loudspeaker and a microphone located at the listeners’ position. Current methods for RTF estimation employ calibration signals, such as noise or tones, in a dedicated process applied to each loudspeaker separately. Such an estimation disrupts normal playback, is time consuming, and requires user intervention. A perceptually-transparent online RTF estimation method for a two-channel system, which employs calibration signals generated using the original audio signals and complementary filters, is proposed in this article. These calibration signals are uncorrelated across the two channels, which facilitates the online estimation of both channels using a single microphone. Estimation performance is investigated for an experimental system and displays low estimation errors. Finally, a subjective evaluation via a listening test shows that playback of calibration signals is perceptually-transparent under some of the conditions investigated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {332–342},
numpages = {11}
}

@article{10.1109/TASLP.2020.2966868,
author = {Zhang, Liwen and Shi, Ziqiang and Han, Jiqing},
title = {Pyramidal Temporal Pooling With Discriminative Mapping for Audio Classification},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2966868},
doi = {10.1109/TASLP.2020.2966868},
abstract = {Audio signals are temporally-structured data, and learning their discriminative representations containing temporal information is crucial for the audio classification. In this article, we propose an audio representation learning method with a hierarchical pyramid structure called pyramidal temporal pooling (PTP) which aims to capture the temporal information of an entire audio sample. By stacking a global temporal pooling layer on multiple local temporal pooling layers, the PTP can capture the high-level temporal dynamics of the input feature sequence in an unsupervised way. Furthermore, in the top global temporal pooling layer, we jointly optimize a learnable discriminative mapping (DM) and a softmax classifier. Such that, a joint learning method for the discriminative audio representations and the classifier called DM-PTP is also presented. By treating the temporal encoding as a low-level constraint of a bi-level optimization problem, the DM-PTP can produce the discriminative representation while maintaining the temporal information of the whole sequence. For an audio sample with an arbitrary time duration, both our PTP and DM-PTP can encode the input feature sequence with arbitrary length into a fixed-length representation. Without using any data augmentation and ensemble learning methods, both PTP and DM-PTP outperform the state-of-the-art CNNs on the audio event recognition (AER) dataset, and can achieve comparable performance on the DCASE 2018 acoustic scene classification (ASC) dataset compared with other best models in the challenge.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {770–784},
numpages = {15}
}

@article{10.1109/TASLP.2020.2978409,
author = {Hashemgeloogerdi, Sahar and Bocko, Mark F.},
title = {Adaptive Feedback Cancellation in Hearing Aids Based on Orthonormal Basis Functions With Prediction-Error Method Based Prewhitening},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2978409},
doi = {10.1109/TASLP.2020.2978409},
abstract = {Acoustic feedback is a persistent problem in hearing aids, which limits the achievable amplification and may severely degrade the sound quality by producing howling artifacts. A potential approach to feedback cancellation is to estimate the feedback path utilizing an adaptive filter. However, estimation of the feedback path suffers a large model error, known as the bias, due to the correlation between the loudspeaker and source signals. A prediction-error method (PEM) based prewhitening filter has been widely utilized to reduce the bias. This approach, however, requires a large number of adaptive parameters, thus increasing the computational complexity, reducing the convergence rate, and limiting the added stable gain. We introduce an adaptive feedback cancellation (AFC) algorithm derived based on the orthonormal basis functions (OBFs) for closed-loop identification of the feedback path by minimizing the prediction error. The OBFs are defined by a set of fixed poles and a small number of adaptive tap-output weights. We study two methods for obtaining the fixed poles, an inherently stable least-squares method and a log-scale frequency resolution method. The poles are then embedded as the <italic>a priori</italic> information into the algorithm. The proposed algorithm is extensively evaluated with speech and music source signals and with sudden changes in the feedback path. The experimental results show that the proposed method significantly increases the added stable gain, accelerates the convergence rate, and enhances the sound quality compared to state-of-the-art, while requiring far fewer adaptive parameters which leads to reduced computational complexity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1260–1269},
numpages = {10}
}

@article{10.1109/TASLP.2019.2957883,
author = {Cherkassky, Dani and Gannot, Sharon},
title = {Successive Relative Transfer Function Identification Using Blind Oblique Projection},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2957883},
doi = {10.1109/TASLP.2019.2957883},
abstract = {Distortionless speech extraction in a reverberant environment can be achieved by applying a beamforming algorithm, provided that the relative transfer functions (RTFs) of the sources and the covariance matrix of the noise are known. In this paper, the challenge of RTF identification in a multi-speaker scenario is addressed. We propose a successive RTF identification (SRI) technique, based on the sole assumption that sources do not become simultaneously active. That is, we address the challenge of estimating the RTF of a specific speech source while assuming that the RTFs of all other active sources in the environment were previously estimated in an earlier stage. The RTF of interest is identified by applying the blind oblique projection (BOP)-SRI technique. When a new speech source is identified, the BOP algorithm is applied. BOP results in a null steering toward the RTF of interest, by means of applying an oblique projection to the microphone measurements. We prove that by artificially increasing the rank of the range of the projection matrix, the RTF of interest can be identified. An experimental study is carried out to evaluate the performance of the BOP-SRI algorithm in various signal to noise ratio (SNR) and signal to interference ratio (SIR) conditions and to demonstrate its effectiveness in speech extraction tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {474–486},
numpages = {13}
}

@article{10.1109/TASLP.2019.2949928,
author = {Wen, Hao-Xiang and Yang, Sen-Quan and Hong, Yuan-Quan and Luo, Huan},
title = {A Partial Update Adaptive Algorithm for Sparse System Identification},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2949928},
doi = {10.1109/TASLP.2019.2949928},
abstract = {A sparse partial update (SPU) algorithm and its improved version improved SPU (ISPU) algorithm, are proposed in this paper for sparse system identification. The SPU first categorizes its filter coefficients into active and inactive coefficients. Then all the active coefficients are included in each adaptation, while only a small portion of the inactive coefficients is periodically chosen to be included in the adaptation. The SPU emphasizes convergence of the active coefficients by updating them at every adaptation, while the periodical adaptation of the inactive coefficients ensures its robustness and tracking capability. By eliminating most of the inactive coefficients from adaptation, the SPU significantly reduces the number of adapting coefficients in each adaptation. The decline in the number of adapting coefficients eventually leads to improvement in both computation and convergence speed. To avoid performance degradation in the case of identifying a dispersive system, an ISPU is further proposed by making modifications to the SPU. The simulation results demonstrate that the ISPU not only outperforms other sparse adaptive algorithms in identifying a sparse system but also performs at least as well as the NLMS algorithm in identifying a dispersive system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {240–255},
numpages = {16}
}

@article{10.1109/TASLP.2020.2984852,
author = {Varanasi, Vishnuvardhan and Gupta, Harshit and Hegde, Rajesh M.},
title = {A Deep Learning Framework for Robust DOA Estimation Using Spherical Harmonic Decomposition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2984852},
doi = {10.1109/TASLP.2020.2984852},
abstract = {Spherical harmonic decomposition facilitates decomposing the sound pressure at different microphones into independent functions of frequency, azimuth and elevation of the source and microphone locations. This decomposition facilitates the extraction of two sets of features containing different information about elevation and azimuth of the source for the direction of arrival (DOA) estimation. These features can be given as input to a learning approach for the estimation of azimuth and elevation separately. This approach aims at breaking down the problem of DOA estimation into azimuth and elevation estimation separately. An advantage of this is the reduction in computational complexity when compared with the joint DOA estimation. This facilitates a straightforward extension of this approach to denser DOA search grids. The contribution of this paper is threefold. First, we propose spherical harmonic magnitude and phase features and discuss the information present in these features regarding the azimuth and elevation of the source. Second, we propose the convolutional neural network architectures for DOA estimation. Third, we analyse the training, run-time computational complexities and propose to extend the DOA estimation approach to dense DOA search grid rather than restricting to a sparse DOA search grid. The performance of conventional DOA estimation approaches degrades in case of a noisy and reverberant environment. Several advancements to the existing DOA estimation approaches have been recently proposed. However, to the best of the authors’ knowledge, learning approaches to DOA estimation with dense DOA search grids with few frames in the context of spherical arrays have not been proposed. Performance evaluation is carried out using simulated as well as real datasets. The proposed approach is also evaluated on LOCATA dataset in the context of a moving source. The results are motivating enough to consider the application of the proposed method in practical scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1248–1259},
numpages = {12}
}

@article{10.1109/TASLP.2020.2980989,
author = {Huang, Gongping and Benesty, Jacob and Cohen, Israel and Chen, Jingdong},
title = {A Simple Theory and New Method of Differential Beamforming With Uniform Linear Microphone Arrays},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2980989},
doi = {10.1109/TASLP.2020.2980989},
abstract = {This article presents a theoretical study of differential beamforming with uniform linear arrays. By defining a forward spatial difference operator, any order of the spatial difference of the observed signals can be represented as a product of a difference operator matrix and the microphone array observations. Consequently, differential beamforming is implemented in two stages, where the first one obtains spatial difference of the observations and the second stage optimizes the beamformer. The major contributions of this article are as follows. First, we propose a new theory of differential beamforming with uniform linear arrays, which shows clearly the connection between the conventional differential beamforming and the null-constrained differential beamforming methods. This provides some new insight into the design of differential beamformers. Second, we deduce some new differential beamformers, where conventional beamforming may be seen as a particular case. Specifically, we derive the maximum white noise gain (MWNG), maximum directivity factor (MDF), parameterized MDF, and parameterized maximum front-to-back ratio differential beamformers. Third, we further extend the idea of how to design optimal differential beamformers by combining both the observed signals and their spatial differences.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1079–1093},
numpages = {15}
}

@article{10.1109/TASLP.2020.2968738,
author = {Kolb\ae{}k, Morten and Tan, Zheng-Hua and Jensen, S\o{}ren Holdt and Jensen, Jesper},
title = {On Loss Functions for Supervised Monaural Time-Domain Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2968738},
doi = {10.1109/TASLP.2020.2968738},
abstract = {Many deep learning-based speech enhancement algorithms are designed to minimize the mean-square error&nbsp;(MSE) in some transform domain between a predicted and a target speech signal. However, optimizing for MSE does not necessarily guarantee high speech quality or intelligibility, which is the ultimate goal of many speech enhancement algorithms. Additionally, only little is known about the impact of the loss function on the emerging class of time-domain deep learning-based speech enhancement systems. We study how popular loss functions influence the performance of time-domain deep learning-based speech enhancement systems. First, we demonstrate that perceptually inspired loss functions might be advantageous over classical loss functions like MSE. Furthermore, we show that the learning rate is a crucial design parameter even for adaptive gradient-based optimizers, which has been generally overlooked in the literature. Also, we found that waveform matching performance metrics must be used with caution as they in certain situations can fail completely. Finally, we show that a loss function based on scale-invariant signal-to-distortion ratio (SI-SDR) achieves good general performance across a range of popular speech enhancement evaluation metrics, which suggests that SI-SDR is a good candidate as a general-purpose loss function for speech enhancement systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {825–838},
numpages = {14}
}

@article{10.1109/TASLP.2019.2951995,
author = {M\o{}ller, Martin Bo and \O{}stergaard, Jan},
title = {A Moving Horizon Framework for Sound Zones},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2951995},
doi = {10.1109/TASLP.2019.2951995},
abstract = {Sound zones are generated to provide independent audio reproduction to multiple people in the same room using loudspeakers. In this article, sound zones are formulated in terms of a moving horizon framework. This framework allows the reproduction scenario to be time-varying and adapt to changes e.g. in the location of the zones or in the audio signal. The framework is tested using both simulated and measured room impulse responses from eight loudspeakers in a rectangular room. The performance is investigated using signals limited between 35–500&nbsp;Hz, but the framework is not limited to a particular frequency range. The experimental results show that it is possible to gain on the order of 4&nbsp;dB higher separation between the zones using the proposed framework, relative to a conventional time-invariant solution. This gain arises from knowledge about the audio content currently being reproduced in the zones, and it is obtained without deteriorating the reproduction accuracy or increasing the signal energy injected into the loudspeakers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {256–265},
numpages = {10}
}

@article{10.1109/TASLP.2019.2961556,
author = {Azad, Abul and Mili, Lamine},
title = {Robust Speech Filter and Voice Encoder Parameter Estimation Using the Phase–Phase Correlator},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2961556},
doi = {10.1109/TASLP.2019.2961556},
abstract = {In recent years, linear prediction voice encoders have become very efficient in terms of computing execution time and channel bandwidth usage while providing, in the absence of impulsive noise, natural sounding synthetic speech signals. This good performance has been achieved via the use of a maximum likelihood parameter estimation of an auto-regressive model of order ten that best fits the speech signal under the assumption that the signal and the noise are Gaussian stochastic processes. However, this method breaks down in the presence of impulse noise, which is common in practice, resulting in harsh or non-intelligible audio signals. In this paper, we propose a robust estimator of correlation, the Phase-Phase correlator that is able to cope with impulsive noise. Utilizing this correlator, we develop a Robust Mixed Excitation Linear Prediction encoder that provides improved audio quality for voiced, unvoiced, and transition speech segments. This is achieved by applying a statistical test to robust Mahalanobis distances for identifying the outliers in the corrupted speech signal, which are then replaced with filtered signals. Simulation results reveal that the proposed estimator of correlator outperforms in variance, bias, and breakdown point compared to three other robust approaches based on the arcsin law, the polarity coincidence correlator, and the median-of-ratio estimator without sacrificing the encoder bandwidth efficiency and the compression gain while remaining compatible with real-time applications. Furthermore, in the presence of impulsive noise, the proposed speech encoder speech subjective quality outperforms the state-of-the-art in terms of mean opinion score.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {592–604},
numpages = {13}
}

@article{10.1109/TASLP.2020.2964953,
author = {Kukanov, Ivan and Trong, Trung Ngo and Hautam\"{a}ki, Ville and Siniscalchi, Sabato Marco and Salerno, Valerio Mario and Lee, Kong Aik},
title = {Maximal Figure-of-Merit Framework to Detect Multi-Label Phonetic Features for Spoken Language Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2964953},
doi = {10.1109/TASLP.2020.2964953},
abstract = {Bottleneck features (BNFs) generated with a deep neural network (DNN) have proven to boost spoken language recognition accuracy over basic spectral features significantly. However, BNFs are commonly extracted using language-dependent tied-context phone states as learning targets. Moreover, BNFs are less phonetically expressive than the output layer in a DNN, which is usually not used as a speech feature because of its very high dimensionality hindering further post-processing. In this article, we put forth a novel deep learning framework to overcome all of the above issues and evaluate it on the 2017 NIST Language Recognition Evaluation (LRE) challenge. We use manner and place of articulation as speech attributes, which lead to low-dimensional “universal” phonetic features that can be defined across all spoken languages. To model the asynchronous nature of the speech attributes while capturing their intrinsic relationships in a given speech segment, we introduce a new training scheme for deep architectures based on a Maximal Figure of Merit (MFoM) objective. MFoM introduces non-differentiable metrics into the backpropagation-based approach, which is elegantly solved in the proposed framework. The experimental evidence collected on the recent NIST LRE 2017 challenge demonstrates the effectiveness of our solution. In fact, the performance of speech language recognition (SLR) systems based on spectral features is improved for more than 5% absolute Cavg. Finally, the F1 metric can be brought from 77.6% up to 78.1% by combining the conventional baseline phonetic BNFs with the proposed articulatory attribute features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {682–695},
numpages = {14}
}

@article{10.1109/TASLP.2019.2957871,
author = {Beerends, John G. and Neumann, Niels M. P. and van den Broek, Egon L. and Llagostera Casanovas, Anna and Menendez, Jovana Torres and Schmidmer, Christian and Berger, Jens},
title = {Subjective and Objective Assessment of Full Bandwidth Speech Quality},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2957871},
doi = {10.1109/TASLP.2019.2957871},
abstract = {With the introduction of fullband speech coding the question arises what role frequency components above 14&nbsp;kHz play in speech quality assessment. On the one hand, our results show that bandwidth limitation from 24&nbsp;kHz down to 14&nbsp;kHz is not audible to even the most critical subject. On the other hand, 14–24&nbsp;kHz band limited, audible levels of noise clearly decrease the perceived quality, especially for young subjects with healthy ears. Furthermore, modern high-quality voice links, using the latest speech codecs, often apply advanced buffering schemes that introduce a new type of audible degradation: micropauses. We investigated the impact of i) bandwidth limitation, ii) coding schemes, iii) micropause, and iv) noise on the perceived quality. Subjective results and objective predictions based on ITU-T recommendation P.863 POLQA are compared. For accurate prediction of the impact of micropauses and noise degradations small model adaptations are suggested. In contrast codec degradations and bandwidth limitation are already predicted with very high accuracy by POLQA: <italic>r</italic> = 0.98, RMSE<sup>*</sup> = 0.05 Mean Opinion Score (MOS).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {440–449},
numpages = {10}
}

@article{10.1109/TASLP.2020.2982291,
author = {Urbanietz, Christoph and Enzner, Gerald},
title = {Direct Spatial-Fourier Regression of HRIRs from Multi-Elevation Continuous-Azimuth Recordings},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982291},
doi = {10.1109/TASLP.2020.2982291},
abstract = {Individual head-related impulse responses (HRIRs) have been recognized as a key to creating high-fidelity virtual auditory spaces. Thus, fast and comprehensive acquisition of individual HRIRs has been a subject of continued research. Traditional stop-and-go measurement at discrete angles is time consuming and additionally requires spatial interpolation that has been tackled by mapping discrete HRIR tables to a spatial Fourier format. Uniformly continuous-azimuth recording with a moving apparatus, on the other hand, reduces acquisition time, but is noise-limited due to a very short observation time per angle. In the interest of both fast acquisition and high accuracy, in this paper, we propose direct retrieval of a spatial Fourier format from continuous-azimuth recordings at multiple simultaneous discrete elevations. Specifically, we fit a generative continuous-azimuth model of the recorded signal, based on the spatial Fourier representation, to the continuous recordings of individuals by least-squares. In this approach, the model is meant to entirely capture the spatial variation of the HRIR in azimuth, while the duration of the recording then systematically controls the noise rejection. The proposed time-domain treatment is free of block artifacts, but is numerically demanding. We outline how to take the special structure of the involved covariance matrices into account. Experimental results with simulated data and real recordings demonstrate that the HRIR performance in terms of binaural cues and reproducibility benefits from the proposed algorithm. Our method is hence practical in terms of low measurement time and high performance, while benefiting from increased computational power of current computers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1129–1142},
numpages = {14}
}

@article{10.1109/TASLP.2020.2982285,
author = {Gfeller, Beat and Frank, Christian and Roblek, Dominik and Sharifi, Matt and Tagliasacchi, Marco and Velimirovi\'{c}, Mihajlo},
title = {SPICE: Self-Supervised Pitch Estimation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982285},
doi = {10.1109/TASLP.2020.2982285},
abstract = {We propose a model to estimate the fundamental frequency in monophonic audio, often referred to as pitch estimation. We acknowledge the fact that obtaining ground truth annotations at the required temporal and frequency resolution is a particularly daunting task. Therefore, we propose to adopt a self-supervised learning technique, which is able to estimate pitch without any form of supervision. The key observation is that pitch shift maps to a simple translation when the audio signal is analysed through the lens of the constant-Q transform (CQT). We design a self-supervised task by feeding two shifted slices of the CQT to the same convolutional encoder, and require that the difference in the outputs is proportional to the corresponding difference in pitch. In addition, we introduce a small model head on top of the encoder, which is able to determine the confidence of the pitch estimate, so as to distinguish between voiced and unvoiced audio. Our results show that the proposed method is able to estimate pitch at a level of accuracy comparable to fully supervised models, both on clean and noisy audio samples, although it does not require access to large labeled datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1118–1128},
numpages = {11}
}

@article{10.1109/TASLP.2019.2955276,
author = {Tan, Ke and Wang, DeLiang},
title = {Learning Complex Spectral Mapping With Gated Convolutional Recurrent Networks for Monaural Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955276},
doi = {10.1109/TASLP.2019.2955276},
abstract = {Phase is important for perceptual quality of speech. However, it seems intractable to directly estimate phase spectra through supervised learning due to their lack of spectrotemporal structure in it. Complex spectral mapping aims to estimate the real and imaginary spectrograms of clean speech from those of noisy speech, which simultaneously enhances magnitude and phase responses of speech. Inspired by multi-task learning, we propose a gated convolutional recurrent network (GCRN) for complex spectral mapping, which amounts to a causal system for monaural speech enhancement. Our experimental results suggest that the proposed GCRN substantially outperforms an existing convolutional neural network (CNN) for complex spectral mapping in terms of both objective speech intelligibility and quality. Moreover, the proposed approach yields significantly higher STOI and PESQ than magnitude spectral mapping and complex ratio masking. We also find that complex spectral mapping with the proposed GCRN provides an effective phase estimate.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {380–390},
numpages = {11}
}

@article{10.1109/TASLP.2019.2959257,
author = {Mogami, Shinichi and Takamune, Norihiro and Kitamura, Daichi and Saruwatari, Hiroshi and Takahashi, Yu and Kondo, Kazunobu and Ono, Nobutaka},
title = {Independent Low-Rank Matrix Analysis Based on Time-Variant Sub-Gaussian Source Model for Determined Blind Source Separation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2959257},
doi = {10.1109/TASLP.2019.2959257},
abstract = {Independent low-rank matrix analysis (ILRMA) is a fast and stable method of blind audio source separation. Conventional ILRMAs assume time-variant (super-)Gaussian source models, which can only represent signals that follow a super-Gaussian distribution. In this article, we focus on ILRMA based on a generalized Gaussian distribution (GGD-ILRMA) and propose a new type of GGD-ILRMA that adopts a time-variant sub-Gaussian distribution for the source model. We propose a new update scheme called generalized iterative projection for homogeneous source models (GIP-HSM) and obtain a convergence-guaranteed update rule for demixing spatial parameters by combining the GIP-HSM scheme and the majorization-minimization (MM) algorithm. Furthermore, a new extension of the MM algorithm is proposed for the convergence acceleration by applying the majorization-equalization algorithm to a multivariate case. In the experimental evaluation, we show the versatility of the proposed method, i.e., the proposed time-variant sub-Gaussian source model can be applied to various types of source signal.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {503–518},
numpages = {16}
}

@article{10.1109/TASLP.2020.2967567,
author = {Moro-Vel\'{a}quez, Laureano and Hern\'{a}ndez-Garc\'{\i}a, Estefan\'{\i}a and G\'{o}mez-Garc\'{\i}a, Jorge A. and Godino-Llorente, Juan I. and Dehak, Najim},
title = {Analysis of the Effects of Supraglottal Tract Surgical Procedures in Automatic Speaker Recognition Performance},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2967567},
doi = {10.1109/TASLP.2020.2967567},
abstract = {This article evaluates the impact in the performance of state-of-the-art automatic speaker recognition schemes of three surgical procedures modifying the supraglottal tract structures of speakers. To do so, a new corpus (Cuco) was recorded, containing the speech of 107 speakers before and after surgery. Speakers were divided into four groups depending on the type of surgery: tonsillectomy, functional endoscopy sinus surgery (FESS), septoplasty, and controls. The analyzed speaker recognition schemes were i-vectors, i-vectors with supervised Universal Background Model, i-vectors employing Time-delay Deep Neural Networks and x-vectors. In all cases, probabilistic linear discriminant analysis was employed in the back-end. Results show changes in the speech of patients who underwent tonsillectomy or FESS after surgery in contrast to controls or patients who had a septoplasty, where not significant variations are observed. These changes increase the Equal Error Rate (EER) of the analyzed speaker recognition schemes for the septoplasty and FESS groups when employing enrollment data recorded before the surgery. Moreover, surgery has a similar influence in the speech of female and male speakers with respect to the analyzed schemes. In consequence, results suggest that it is advisable to update the speaker's enrollment speech after three months following supraglottal tract surgery to ensure that the effects of the operation and post-operative recovery period do not influence the performance of the automatic speaker recognition systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {798–812},
numpages = {15}
}

@article{10.1109/TASLP.2020.2963954,
author = {Wang, Zhongqing and Sun, Qingying and Li, Shoushan and Zhu, Qiaoming and Zhou, Guodong},
title = {Neural Stance Detection With Hierarchical Linguistic Representations},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2963954},
doi = {10.1109/TASLP.2020.2963954},
abstract = {Stance detection aims to assign a stance label (i.e., <italic>favor</italic> or <italic>against</italic>) to a post towards a specific target. Recently, there is a growing interest in adopting neural models to detect stance of a document. However, most of these works focus on modeling the sequence of words to learn document representation, though other linguistic information, such as sentiment and arguments, are correlated with the stance of document, and may inspire us to explore the stance. In this article, we propose a <italic>hierarchical attention neural model</italic> to well study various linguistic information to better represent a document via hierarchical linguistic representations. In addition, we propose a hierarchical network with attention mechanism to weight the importance of various kinds of linguistic information, and learn the mutual attention between document and linguistic information. Detail evaluation on two benchmark datasets demonstrates the effectiveness of proposed hierarchical network with attention mechanism.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {635–645},
numpages = {11}
}

@article{10.1109/TASLP.2020.2984089,
author = {L\'{o}pez-Espejo, Iv\'{a}n and Tan, Zheng-Hua and Jensen, Jesper},
title = {Improved External Speaker-Robust Keyword Spotting for Hearing Assistive Devices},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2984089},
doi = {10.1109/TASLP.2020.2984089},
abstract = {For certain applications, keyword spotting (KWS) requires some degree of personalization. This is the case for KWS for hearing assistive devices, e.g., hearing aids, where only the device user should be allowed to trigger the KWS system. In this paper, we first develop a new realistic hearing aid experimental framework. Next, using this framework we show that the performance of a state-of-the-art multi-task deep learning architecture exploiting cepstral features for joint KWS and users’ own-voice/external speaker detection drops significantly. To overcome this problem, we use phase difference information through GCC-PHAT (Generalized Cross-Correlation with PHAse Transform)-based coefficients along with log-spectral magnitude features. In addition, we demonstrate that working in the perceptually-motivated constant-Q transform (CQT) domain instead of in the short-time Fourier transform (STFT) domain allows for the generation of compact and coherent features which provide superior KWS performance. Our experimental results show that our CQT-based proposal achieves a relative KWS accuracy improvement of around 18% compared to using cepstral features while dramatically decreasing the number of multiplications in the multi-task architecture, which is key in the context of low-resource devices like hearing assistive devices.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1233–1247},
numpages = {15}
}

@article{10.1109/TASLP.2020.2986886,
author = {Kano, Takatomo and Sakti, Sakriani and Nakamura, Satoshi},
title = {End-to-End Speech Translation With Transcoding by Multi-Task Learning for Distant Language Pairs},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2986886},
doi = {10.1109/TASLP.2020.2986886},
abstract = {Directly translating spoken utterances from a source language to a target language is challenging because it requires a fundamental transformation in both linguistic and para/non-linguistic features. Traditional speech-to-speech translation approaches concatenate automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech synthesizer (TTS) by text information. The current state-of-the-art models for ASR, MT, and TTS have mainly been built using deep neural networks, in particular, an attention-based encoder-decoder neural network with an attention mechanism. Recently, several works have constructed end-to-end direct speech-to-text translation by combining ASR and MT into a single model. However, the usefulness of these models has only been investigated on language pairs of similar syntax and word order (e.g., English-French or English-Spanish). For syntactically distant language pairs (e.g., English-Japanese), speech translation requires distant word reordering. Furthermore, parallel texts with corresponding speech utterances that are suitable for training end-to-end speech translation are generally unavailable. Collecting such corpora is usually time-consuming and expensive. This article proposes the first attempt to build an end-to-end direct speech-to-text translation system on syntactically distant language pairs that suffer from long-distance reordering. We train the model on English (subject-verb-object (SVO) word order) and Japanese (SOV word order) language pairs. To guide the attention-based encoder-decoder model on this difficult problem, we construct end-to-end speech translation with transcoding and utilize curriculum learning (CL) strategies that gradually train the network for end-to-end speech translation tasks by adapting the decoder or encoder parts. We use TTS for data augmentation to generate corresponding speech utterances from the existing parallel text data. Our experiment results show that the proposed approach provides significant improvements compared with conventional cascade models and the direct speech translation approach that uses a single model without transcoding and CL strategies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1342–1355},
numpages = {14}
}

@article{10.1109/TASLP.2020.2971828,
author = {Kim, Yeongseok and Park, Youngjin},
title = {Blockwise Weighted Least Square Active Noise Control for CPU-GPU Architecture},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2971828},
doi = {10.1109/TASLP.2020.2971828},
abstract = {Active noise control (ANC) is a technology which lowers the noise level by using the principle of destructive interference of sound wave. Even though recent developments in digital signal processing (DSP) made it possible to implement ANC algorithms in real-time, insufficient computational power is still one of the challenges to solve. In the previous research, as a way of overcoming the lack of computational power, CPU-GPU architecture was proposed so that ANC algorithms utilize the massive computing power of GPU without suffering from the block data transfer between CPU and GPU memories. However, for the feasibility test of the proposed CPU-GPU architecture in the previous research, a conventional block ANC algorithm was used, and ANC algorithm which can fully utilize the massive computing power of GPU has not been developed. In this article, ANC algorithm, which directly derives blockwise least square solution through GPU computation while generating control signals through CPU computation, is proposed. Based on the observation about speaker saturation and increase of noise level after applying the conventional blockwise least square solution, a new cost function for preventing such problems is also proposed. Therefore, blockwise weighted least square ANC (BWLS-ANC) algorithm, which derives blockwise least square solution minimizing the proposed cost function through GPU computation while generating control signals through CPU computation, is proposed throughout this research. Problems of conventional blockwise least square solution upon ANC applications are observed through simulations and experiments. The feasibility of the proposed BWLS-ANC algorithm is verified through experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {951–963},
numpages = {13}
}

@article{10.1109/TASLP.2019.2955246,
author = {Nguyen, Minh and Ngo, Gia H. and Chen, Nancy F.},
title = {Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin Using Recursive Neural Networks},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955246},
doi = {10.1109/TASLP.2019.2955246},
abstract = {Logographs (Chinese characters) have recursive structures (i.e. hierarchies of sub-units in logographs) that contain phonological and semantic information, as developmental psychology literature suggests that native speakers leverage on the structures to learn how to read. Exploiting these structures could potentially lead to better embeddings that can benefit many downstream tasks. We propose building hierarchical logograph (character) embeddings from logograph recursive structures using treeLSTM, a recursive neural network. Using recursive neural network imposes a prior on the mapping from logographs to embeddings since the network must read in the sub-units in logographs according to the order specified by the recursive structures. Based on human behavior in language learning and reading, we hypothesize that modeling logographs’ structures using recursive neural network should be beneficial. To verify this claim, we consider two tasks (1) predicting logographs’ Cantonese pronunciation from logographic structures and (2) language modeling. Empirical results show that the proposed hierarchical embeddings outperform baseline approaches. Diagnostic analysis suggests that hierarchical embeddings constructed using treeLSTM is less sensitive to distractors, thus is more robust, especially on complex logographs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {461–473},
numpages = {13}
}

@article{10.1109/TASLP.2020.2983589,
author = {Cobos, Maximo and Antonacci, Fabio and Comanducci, Luca and Sarti, Augusto},
title = {Frequency-Sliding Generalized Cross-Correlation: A Sub-Band Time Delay Estimation Approach},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2983589},
doi = {10.1109/TASLP.2020.2983589},
abstract = {The generalized cross-correlation(GCC) is regarded as the most popular approach for estimating the time difference of arrival (TDOA) between the signals received at two sensors. Time delay estimates are obtained by maximizing the GCC output, where the direct-path delay is usually observed as a prominent peak. Moreover, GCCs play also an important role in steered response power (SRP) localization algorithms, where the SRP functional can be written as an accumulation of the GCCs computed from multiple sensor pairs. Unfortunately, the accuracy of TDOA estimates is affected by multiple factors, including noise, reverberation and signal bandwidth. In this paper, a sub-band approach for time delay estimation aimed at improving the performance of the conventional GCC is presented. The proposed method is based on the extraction of multiple GCCs corresponding to different frequency bands of the cross-power spectrum phase in a sliding-window fashion. The major contributions of this paper include: 1) a sub-band GCC representation of the cross-power spectrum phase that, despite having a reduced temporal resolution, provides a more suitable representation for estimating the true TDOA; 2) such matrix representation is shown to be rank one in the ideal noiseless case, a property that is exploited in more adverse scenarios to obtain a more robust and accurate GCC; 3) we propose a set of low-rank approximation alternatives for processing the sub-band GCC matrix, leading to better TDOA estimates and source localization performance. An extensive set of experiments is presented to demonstrate the validity of the proposed approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1270–1281},
numpages = {12}
}

@article{10.1109/TASLP.2020.2975423,
author = {Laufer-Goldshtein, Bracha and Talmon, Ronen and Gannot, Sharon},
title = {Global and Local Simplex Representations for Multichannel Source Separation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2975423},
doi = {10.1109/TASLP.2020.2975423},
abstract = {The problem of blind audio source separation (BASS) in noisy and reverberant conditions is addressed by a novel approach, termed Global and LOcal Simplex Separation (GLOSS), which integrates full- and narrow-band simplex representations. We show that the eigenvectors of the correlation matrix between time frames in a certain frequency band form a simplex that organizes the frames according to the speaker activities in the corresponding band. We propose to build two simplex representations: one global based on a broad frequency band and one local based on a narrow band. In turn, the two representations are combined to determine the dominant speaker in each time-frequency (TF) bin. Using the identified dominating speakers, a spectral mask is computed and is utilized for extracting each of the speakers using spatial beamforming followed by spectral postfiltering. The performance of the proposed algorithm is demonstrated using real-life recordings in various noisy and reverberant conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {914–928},
numpages = {15}
}

@article{10.1109/TASLP.2020.2966857,
author = {Ando, Atsushi and Masumura, Ryo and Kamiyama, Hosana and Kobashikawa, Satoshi and Aono, Yushi and Toda, Tomoki},
title = {Customer Satisfaction Estimation in Contact Center Calls Based on a Hierarchical Multi-Task Model},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2966857},
doi = {10.1109/TASLP.2020.2966857},
abstract = {This article presents a novel customer satisfaction&nbsp;(CS) estimation method that outputs both turn-level and call-level estimations simultaneously. Our key idea is to directly apply turn-level estimation results to call-level estimation and optimize them jointly; previous works treat both as being independent. Our proposal applies long short-term memory recurrent neural networks&nbsp;(LSTM-RNNs) to turn-level and call-level CS estimation to capture long-range sequential context in contact center calls. In addition, both networks are hierarchically stacked so as to use turn-level estimation results for call-level estimation directly. In order to learn the relationship between the two tasks, we also introduce joint optimization training to the stacked model. Several analyses of turn-level and call-level CS are provided on acted and real calls to support the proposed method. Experiments show that the proposed framework outperforms the conventional methods in both turn-level and call-level estimations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {715–728},
numpages = {14}
}

@article{10.1109/TASLP.2019.2956145,
author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
title = {Neural Source-Filter Waveform Models for Statistical Parametric Speech Synthesis},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2956145},
doi = {10.1109/TASLP.2019.2956145},
abstract = {Neural waveform models have demonstrated better performance than conventional vocoders for statistical parametric speech synthesis. One of the best models, called WaveNet, uses an autoregressive (AR) approach to model the distribution of waveform sampling points, but it has to generate a waveform in a time-consuming sequential manner. Some new models that use inverse-autoregressive flow (IAF) can generate a whole waveform in a one-shot manner but require either a larger amount of training time or a complicated model architecture plus a blend of training criteria. As an alternative to AR and IAF-based frameworks, we propose a neural source-filter (NSF) waveform modeling framework that is straightforward to train and fast to generate waveforms. This framework requires three components to generate waveforms: a source module that generates a sine-based signal as excitation, a non-AR dilated-convolution-based filter module that transforms the excitation into a waveform, and a conditional module that pre-processes the input acoustic features for the source and filter modules. This framework minimizes spectral-amplitude distances for model training, which can be efficiently implemented using short-time Fourier transform routines. As an initial NSF study, we designed three NSF models under the proposed framework and compared them with WaveNet using our deep learning toolkit. It was demonstrated that the NSF models generated waveforms at least 100 times faster than our WaveNet-vocoder, and the quality of the synthetic speech from the best NSF model was comparable to that from WaveNet on a large single-speaker Japanese speech corpus.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {402–415},
numpages = {14}
}

@article{10.1109/TASLP.2020.2975419,
author = {De Sena, Enzo and Cvetkovi\'{c}, Zoran and Hac\i{}habibo\u{g}lu, H\"{u}seyin and Moonen, Marc and van Waterschoot, Toon},
title = {Localization Uncertainty in Time-Amplitude Stereophonic Reproduction},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2975419},
doi = {10.1109/TASLP.2020.2975419},
abstract = {This article studies the effects of inter-channel time and level differences in stereophonic reproduction on perceived localization uncertainty, which is defined as how difficult it is for a listener to tell where a sound source is located. Towards this end, a computational model of localization uncertainty is proposed first. The model calculates inter-aural time and level difference cues, and compares them to those associated to free-field point-like sources. The comparison is carried out using a particular distance functional that replicates the increased uncertainty observed experimentally with inconsistent inter-aural time and level difference cues. The model is validated by formal listening tests, achieving a Pearson correlation of 0.99. The model is then used to predict localization uncertainty for stereophonic setups and a listener in central and off-central positions. Results show that amplitude methods achieve a slightly lower localization uncertainty for a listener positioned exactly in the center of the sweet spot. As soon as the listener moves away from that position, the situation reverses, with time-amplitude methods achieving a lower localization uncertainty.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1000–1015},
numpages = {16}
}

@article{10.1109/TASLP.2019.2950774,
author = {Hoffmann, Falk-Martin and Nelson, Philip Arthur and Fazi, Filippo Maria},
title = {DOA Estimation Performance With Circular Arrays in Sound Fields With Finite Rate of Innovation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2950774},
doi = {10.1109/TASLP.2019.2950774},
abstract = {A novel Direction-of-Arrival (DOA) estimation method based on a plane wave sound field model was recently proposed for circular microphone arrays&nbsp;<xref ref-type="bibr" rid="ref1">[1]</xref>. This article presents a detailed theoretical analysis of the method that relies on a Finite Rate of Innovation (FRI) assumption, investigating the impact of different theoretical acoustic source models. The method's estimation accuracy, and robustness against noisy measurement data and deviations from the model are investigated. The estimation performance is validated and assessed on the basis of results obtained from both simulations and experimental data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {171–184},
numpages = {14}
}

@article{10.1109/TASLP.2020.2980436,
author = {Sterpu, George and Saam, Christian and Harte, Naomi},
title = {How to Teach DNNs to Pay Attention to the Visual Modality in Speech Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2980436},
doi = {10.1109/TASLP.2020.2980436},
abstract = {Audio-Visual Speech Recognition (AVSR) seeks to model, and thereby exploit, the dynamic relationship between a human voice and the corresponding mouth movements. A recently proposed multimodal fusion strategy, <italic>AV Align</italic>, based on state-of-the-art sequence to sequence neural networks, attempts to model this relationship by explicitly aligning the acoustic and visual representations of speech. This study investigates the inner workings of <italic>AV Align</italic> and visualises the audio-visual alignment patterns. Our experiments are performed on two of the largest publicly available AVSR datasets, TCD-TIMIT and LRS2. We find that <italic>AV Align</italic> learns to align acoustic and visual representations of speech at the frame level on TCD-TIMIT in a generally monotonic pattern. We also determine the cause of initially seeing no improvement over audio-only speech recognition on the more challenging LRS2. We propose a regularisation method which involves predicting lip-related Action Units from visual representations. Our regularisation method leads to better exploitation of the visual modality, with performance improvements between 7% and 30% depending on the noise level. Furthermore, we show that the alternative <italic>Watch, Listen, Attend, and Spell</italic> network is affected by the same problem as <italic>AV Align</italic>, and that our proposed approach can effectively help it learn visual representations. Our findings validate the suitability of the regularisation method to AVSR and encourage researchers to rethink the multimodal convergence problem when having one dominant modality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1052–1064},
numpages = {13}
}

@article{10.1109/TASLP.2020.2971419,
author = {Yu, Dongyan and Duan, Huiping and Fang, Jun and Zeng, Bing},
title = {Predominant Instrument Recognition Based on Deep Neural Network With Auxiliary Classification},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2971419},
doi = {10.1109/TASLP.2020.2971419},
abstract = {Instrument recognition plays very important roles in music information retrieval, sound source separation and automatic music transcription. However, due to different playing styles and audio qualities, this task cannot be accomplished easily. Simultaneous existence of multiple instruments in polyphonic music increases the challenge to a greater extent. This article mainly focus on the identification of the predominant instruments in polyphonic music. We propose to construct a network with an auxiliary classification designed based on the onset groups and instrument families. The principal classification and the auxiliary classification enable the network to learn the instrument categories and groups jointly in a pattern of multitask learning. The IRMAS dataset is adopted in the experiment to extract the mel-spectrogram and six other types of features. The micro and macro average of precisions, recalls and F1 measures are used to evaluate the classification results. The effect of multitask learning, batch normalization and center loss in the predominant instrument recognition are demonstrated by various experiments. By selecting the loss ratios through a development set, the micro and macro F1 measures of our proposed network can reach 0.685 and 0.597, which are 10.7% and 16.4% higher than those obtained by the baseline, the ConvNet presented in&nbsp;<xref ref-type="bibr" rid="ref1">[1]</xref>.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {852–861},
numpages = {10}
}

@article{10.1109/TASLP.2019.2955293,
author = {Diez, Mireia and Burget, Luk\'{a}\v{s} and Landini, Federico and \v{C}ernock\'{y}, Jan},
title = {Analysis of Speaker Diarization Based on Bayesian HMM With Eigenvoice Priors},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955293},
doi = {10.1109/TASLP.2019.2955293},
abstract = {In our previous work, we introduced our Bayesian Hidden Markov Model with eigenvoice priors, which has been recently recognized as the state-of-the-art model for Speaker Diarization. In this article we present a more complete analysis of the Diarization system. The inference of the model is fully described and derivations of all update formulas are provided for a complete understanding of the algorithm. An extensive analysis on the effect, sensitivity and interactions of all model parameters is provided, which might be used as a guide for their optimal setting. The newly introduced speaker regularization coefficient allows us to control the number of speakers inferred in an utterance. A naive speaker model merging strategy is also presented, which allows to drive the variational inference out of local optima. Experiments for the different diarization scenarios are presented on CALLHOME and DIHARD datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {355–368},
numpages = {14}
}

@article{10.1109/TASLP.2020.2979603,
author = {Nugraha, Aditya Arie and Sekiguchi, Kouhei and Yoshii, Kazuyoshi},
title = {A Flow-Based Deep Latent Variable Model for Speech Spectrogram Modeling and Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2979603},
doi = {10.1109/TASLP.2020.2979603},
abstract = {This article describes a deep latent variable model of speech power spectrograms and its application to semi-supervised speech enhancement with a deep speech prior. By integrating two major deep generative models, a variational autoencoder (VAE) and a normalizing flow (NF), in a mutually-beneficial manner, we formulate a flexible latent variable model called the NF-VAE that can extract low-dimensional latent representations from high-dimensional observations, akin to the VAE, and does not need to explicitly represent the distribution of the observations, akin to the NF. In this article, we consider a variant of NF called the generative flow (GF a.k.a. Glow) and formulate a latent variable model called the GF-VAE. We experimentally show that the proposed GF-VAE is better than the standard VAE at capturing fine-structured harmonics of speech spectrograms, especially in the high-frequency range. A similar finding is also obtained when the GF-VAE and the VAE are used to generate speech spectrograms from latent variables randomly sampled from the standard Gaussian distribution. Lastly, when these models are used as speech priors for statistical multichannel speech enhancement, the GF-VAE outperforms the VAE and the GF.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1104–1117},
numpages = {14}
}

@article{10.1109/TASLP.2019.2960716,
author = {Ma, Fei and Zhang, Wen and Abhayapala, Thushara Dheemantha},
title = {Active Control of Outgoing Broadband Noise Fields in Rooms},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2960716},
doi = {10.1109/TASLP.2019.2960716},
abstract = {Active noise control system has been actively researched over the past half century, and implemented to reduce noises in ducts, headsets, and inside several automobile models. However, active control of noise fields, and specifically broadband noise fields, in rooms is still a barely explored topic due to the difficult of obtaining the reference signals, the challenge of online secondary path estimation, and the causal control constraint. In this paper, an active noise control system is developed that can cancel outgoing broadband noise fields in rooms. The proposed system decomposes the noise field on a sphere surrounding the noise sources into spherical harmonic modes, exploiting their directionality to generate the reference signals and remove the need for online secondary path estimation. A time-domain sound field separation algorithm and a time-wave domain adaptive algorithm allow the proposed system to meet the causal control constraint. Simulation results demonstrate that the proposed system can cancel broadband noise field globally in a room without suffering from the secondary source feedback problem.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {529–539},
numpages = {11}
}

@article{10.1109/TASLP.2020.2983593,
author = {Zheng, Yinhe and Chen, Guanyi and Huang, Minlie},
title = {Out-of-Domain Detection for Natural Language Understanding in Dialog Systems},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2983593},
doi = {10.1109/TASLP.2020.2983593},
abstract = {Natural Language Understanding (NLU) is a vital component of dialogue systems, and its ability to detect Out-of-Domain (OOD) inputs is critical in practical applications, since the acceptance of the OOD input that is unsupported by the current system may lead to catastrophic failure. However, most existing OOD detection methods rely heavily on manually labeled OOD samples and cannot take full advantage of unlabeled data. This limits the feasibility of these models in practical applications. In this paper, we propose a novel model to generate high-quality pseudo OOD samples that are akin to IN-Domain (IND) input utterances and thereby improves the performance of OOD detection. To this end, an autoencoder is trained to map an input utterance into a latent code. Moreover, the codes of IND and OOD samples are trained to be indistinguishable by utilizing a generative adversarial network. To provide more supervision signals, an auxiliary classifier is introduced to regularize the generated OOD samples to have indistinguishable intent labels. Experiments show that these pseudo OOD samples generated by our model can be used to effectively improve OOD detection in NLU. Besides, we also demonstrate that the effectiveness of these pseudo OOD data can be further improved by efficiently utilizing unlabeled data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1198–1209},
numpages = {12}
}

@article{10.1109/TASLP.2019.2950767,
author = {Canclini, Antonio and Antonacci, Fabio and Tubaro, Stefano and Sarti, Augusto},
title = {A Methodology for the Robust Estimation of the Radiation Pattern of Acoustic Sources},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2950767},
doi = {10.1109/TASLP.2019.2950767},
abstract = {We propose a novel methodology for estimating the radiation pattern of acoustic sources, which is general enough as to be suitable for a wide variety of sources without the need of anechoic conditions of operation. Multiple plenacoustic cameras (which can be thought of as arrays of acoustic cameras) scan the source while keeping reflections and interferers at bay through deconvolution and windowing of the measured response. In the case of a moving source (e.g. a musical instrument while it is being played), the plenacoustic cameras are also used for tracking the position of the source. As for its orientation, we propose practical solutions for tracking that as well, whenever such information is not known in advance. Two experiments are conducted in order to validate the proposed solution. The former focuses on a commercial loudspeaker cabinet, whose radiation pattern is known in advance and can be used as groundtruth. The latter concerns violins, which exhibit an extremely rich and hard to predict acoustic behavior, due to their inherent structural and constructional complexity. Our method allows us to capture the radiation pattern of the instrument while it is being played, thus returning data corresponding to the natural timbre of the instrument, including the unavoidable acoustic shadow of the violinist's head. Experimental results confirm a relevant improvement in accuracy and robustness afforded by the adoption of dynamic plenacoustic solutions with respect to state-of-the-art techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {211–224},
numpages = {14}
}

@article{10.1109/TASLP.2020.2977472,
author = {Khadem-hosseini, M. and Ghaemmaghami, S. and Abtahi, A. and Gazor, S. and Marvasti, F.},
title = {Error Correction in Pitch Detection Using a Deep Learning Based Classification},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2977472},
doi = {10.1109/TASLP.2020.2977472},
abstract = {While pitch detection has been the focal subject of numerous research efforts for several decades, it is still a challenging task in noisy conditions. In this article, we propose a method to improve the pitch detection accuracy of conventional pitch detection methods. The proposed pitch detection process starts with using the pitch value estimated by a conventional pitch detection method. Then, it extracts pitch candidates according to the most probable types of errors in the initial estimation of high-pitch and low-pitch frames classified by a Deep Convolutional Neural Network (DCNN). Next, a restrained selection procedure is run to find the true pitch value from the set of pitch candidates. In this procedure, we employ two features (harmonic summation and Euclidian deviation), the soft decision of the DCNN, the pitch smoothness feature in successive frames, and the effect of the initial estimation in a cost function. The pitch value which leads to the lowest cost value is chosen as the estimated pitch value. The simulations on CSTR and KEELE databases, in noisy environments with twelve types of noise, were performed. The results show the superiority of the proposed method over the state-of-the-art methods under different SNR conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {990–999},
numpages = {10}
}

@article{10.1109/TASLP.2019.2957887,
author = {Mathad, Vikram C. and Prasanna, S. R. Mahadeva},
title = {Vowel Onset Point Based Screening of Misarticulated Stops in Cleft Lip and Palate Speech},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2957887},
doi = {10.1109/TASLP.2019.2957887},
abstract = {The presence of velopharyngeal dysfunction, dental occlusion, and mislearned articulation in individuals with cleft lip and palate (CLP) results in the production of misarticulated stop consonants. The present work considers vowel onset points (VOPs) as the anchor points, around which the consonant-vowel (CV) transition regions are segmented to analyze the difference between normal and misarticulated stops. VOPs are located using an epoch-synchronously computed feature called maximum weighted inner product. Spectro-temporal dynamics of CV transitions anchored around VOP are analyzed using two-dimensional discrete cosine transform (2D-DCT) coefficients, where 2D-DCT coefficients are derived from single pole filter (SPF) based time-frequency representation. The SPF-based 2D-DCT coefficients are used to train a support vector machine for the classification of normal and misarticulated stops, where the class of misarticulated stops includes weak, nasalized, palatal, velar, pharyngeal, glottal, and devoicing errors produced by CLP speakers. The performance of the proposed VOP detection algorithm is evaluated on a database containing CV units of normal and misarticulated stops, and the results are compared with the state-of-the-art VOP detection methods. The classification results obtained for the proposed SPF-based 2D-DCT coefficients are compared with the short-time Fourier transform-based 2D-DCT coefficients and Mel-frequency cepstral coefficients. Further, the performance of the proposed system is compared with the hidden Markov model-based goodness of pronunciation approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {450–460},
numpages = {11}
}

@article{10.1109/TASLP.2019.2935843,
author = {Yu, Jun and Ling, Qiang and Luo, Changwei and Chen, Chang Wen},
title = {Synthesizing 3D Trump: Predicting and Visualizing the Relationship Between Text, Speech, and Articulatory Movements},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2935843},
doi = {10.1109/TASLP.2019.2935843},
abstract = {The movements of articulators, such as lips, tongue and teeth, play an important role in increasing the language expression capability by unmasking the information hid in text or speech. Hence, it is necessary to deeply mine and visualize the relationship between text, speech and articulatory movements for understanding language in multi-modality and multi-level. As a case study, given text and audio of President Donald John Trump, this paper synthesizes a high quality 3D animation of him speaking with accurate synchronicity between speech and articulators. First, visual co-articulation is modeled by predicting the mapping from text/speech to articulatory movements. Then, based on a reconstructed 3D head model, physiological characteristics and statistical learning are combined to visualize each phoneme. Finally, the visualization results of consecutive phonemes are fused by visual co-articulation model to generate synchronized articulatory animations. Experiments show that the system can not only produce photo-realistic results in front but also distinguish the visual differences among phonemes from unconstrained views.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2223–2233},
numpages = {11}
}

@article{10.1109/TASLP.2019.2942140,
author = {Prathosh, A. P. and Srivastava, Varun and Mishra, Mayank},
title = {Adversarial Approximate Inference for Speech to Electroglottograph Conversion},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2942140},
doi = {10.1109/TASLP.2019.2942140},
abstract = {Speech produced by human vocal apparatus conveys substantial non-semantic information including the gender of the speaker, voice quality, affective state, abnormalities in the vocal apparatus etc. Such information is attributed to the properties of the voice source signal, which is usually estimated from the speech signal. However, most of the source estimation techniques depend heavily on the goodness of the model assumptions and are prone to noise. A popular alternative is to indirectly obtain the source information through the Electroglottographic (EGG) signal that measures the electrical admittance around the vocal folds using dedicated hardware. In this paper, we address the problem of estimating the EGG signal directly from the speech signal, devoid of any hardware. Sampling from the intractable conditional distribution of the EGG signal given the speech signal is accomplished through optimization of an evidence lower bound. This is constructed via minimization of the KL-divergence between the true and the approximated posteriors of a latent variable learned using a deep neural auto-encoder that serves an informative prior. We demonstrate the efficacy of the method at generating the EGG signal by conducting several experiments on datasets comprising multiple speakers, voice qualities, noise settings and speech pathologies. The proposed method is evaluated on many benchmark metrics and is found to agree with the gold standard while proving better than the state-of-the-art algorithms on a few tasks such as epoch extraction.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2183–2196},
numpages = {14}
}

@article{10.1109/TASLP.2019.2945489,
author = {Kato, Akihiro and Kinnunen, Tomi H.},
title = {Statistical Regression Models for Noise Robust F0 Estimation Using Recurrent Deep Neural Networks},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2945489},
doi = {10.1109/TASLP.2019.2945489},
abstract = {The fundamental frequency (F0) in a speech signal, which corresponds to pitch, is one of the key features involved in a variety of speech processing tasks. Therefore, accurate F0 estimation has remained an important problem to be solved over decades. However, this problem is difficult, especially in low signal-to-noise ratio (SNR) conditions with unknown noise. In this work, we propose new approaches to noise-robust F0 estimation using recurrent neural networks (RNNs). Recent F0 estimation studies exploit deep neural networks (DNNs), including RNNs, to classify acoustic features into quantized frequency states. In contrast to these classification approaches, we put forward a <italic>regression</italic> method for F0 tracking, which is accomplished with RNNs. To this end, we propose two variants. Our first model predicts the (scalar) F0 value directly from a spectrum, while our second model predicts a target sinusoidal waveform (with the desired F0) from the raw speech waveform. Our experiments with the <italic>pitch tracking database from Graz University of Technology</italic> (PTDB-TUG), contaminated by additive noise (NOISEX-92), demonstrate the improvement of the proposed approaches in terms of the gross pitch error (GPE) and fine pitch error (FPE) rates by more than 35% at SNRs between −10&nbsp;dB and +10&nbsp;dB against a well-known, noise-robust F0 tracker, PEFAC. Furthermore, our methods outperform state-of-the-art neural network-based approaches by more than 15% in terms of both the FPE and GPE rates over the abovementioned SNR range.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2336–2349},
numpages = {14}
}

@article{10.1109/TASLP.2019.2945843,
author = {Sugiura, Ryosuke and Kamamoto, Yutaka and Moriya, Takehiro},
title = {Shape Control of Discrete Generalized Gaussian Distributions for Frequency-Domain Audio Coding},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2945843},
doi = {10.1109/TASLP.2019.2945843},
abstract = {Entropy coding, which is an essential part of audio compression, is always required to manage the tradeoffs between compression efficiency and computational complexity, and the strategy to achieve them highly depends on the distributions of inputs. In this paper, we present a method of controlling them for enhancing the compression efficiency of Golomb-Rice (GR) encoding, one of the simplest entropy coding methods optimal for Laplacian distributions. We will show that the proposed invertible and low-complexity mapping of integers enables the GR encoding to assign nearly the optimal code length for a wider range of distributions, generalized Gaussian distributions, maintaining low computational cost. A simulation by random numbers reveals that the proposed coder based on this scheme works about 6 times faster than the state-of-the-art arithmetic coder for Gaussian-distributed integers maintaining the increase in relative redundancy around <inline-formula><tex-math notation="LaTeX">$text{2.6}{%}$</tex-math></inline-formula>, which is much lower than that of a conventional GR coder. Additionally, an application to a practical speech and audio coding scheme is presented, and an objective evaluation for real speech and audio signals confirms the advantages of the proposed method in compression. The method is expected to widen the capability of low-complexity entropy coding, providing us with more flexible codec designs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2234–2248},
numpages = {15}
}

@article{10.1109/TASLP.2019.2946480,
author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Luo, Jiebo},
title = {Future-Aware Knowledge Distillation for Neural Machine Translation},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2946480},
doi = {10.1109/TASLP.2019.2946480},
abstract = {Although future context is widely regarded useful for word prediction in machine translation, it is quite difficult in practice to incorporate it into neural machine translation. In this paper, we propose a future-aware knowledge distillation framework (FKD) to address this issue. In the FKD framework, we learn to distill future knowledge from a backward neural language model (teacher) to future-aware vectors (student) during the training phase. The future-aware vector for each word position is computed in a bridge network and optimized towards the corresponding hidden state in the backward neural language model via a knowledge distillation mechanism. We further propose an algorithm to jointly train the neural machine translation model, neural language model and knowledge distillation module end-to-end. The learned future-aware vectors are incorporated into the attention layer of the decoder to provide full-range context information during the decoding phase. Experiments on the NIST Chinese-English and WMT English-German translation tasks show that the proposed method significantly improves translation quality and word alignment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2278–2287},
numpages = {10}
}

@article{10.1109/TASLP.2019.2933047,
author = {Antonello, Niccolo and De Sena, Enzo and Moonen, Marc and Naylor, Patrick A. and van Waterschoot, Toon},
title = {Joint Acoustic Localization and Dereverberation Through Plane Wave Decomposition and Sparse Regularization},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2933047},
doi = {10.1109/TASLP.2019.2933047},
abstract = {Acoustic source localization and dereverberation are formulated jointly as an inverse problem. The inverse problem consists of the approximation of the sound field measured by a set of microphones. The recorded sound pressure is matched with that of a particular acoustic model based on a collection of plane waves arriving from different directions at the microphone positions. In order to achieve meaningful results, spatial and spatio-spectral sparsity can be promoted in the weight signals controlling the plane waves. The large-scale optimization problem resulting from the inverse problem formulation is solved using a first order optimization algorithm combined with a weighted overlap-add procedure. It is shown that once the weight signals capable of effectively approximating the sound field are obtained, they can be readily used to localize a moving sound source in terms of direction of arrival DOA and to perform dereverberation in a highly reverberant environment. Results from simulation experiments and from real measurements show that the proposed algorithm is robust against both localized and diffuse noise exhibiting a noise reduction in the dereverberated signals.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1893–1905},
numpages = {13}
}

@article{10.1109/TASLP.2019.2942439,
author = {Arnela, Marc and Dabbaghchian, Saeed and Guasch, Oriol and Engwall, Olov},
title = {MRI-Based Vocal Tract Representations for the Three-Dimensional Finite Element Synthesis of Diphthongs},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2942439},
doi = {10.1109/TASLP.2019.2942439},
abstract = {The synthesis of diphthongs in three-dimensions 3D involves the simulation of acoustic waves propagating through a complex 3D vocal tract geometry that deforms over time. Accurate 3D vocal tract geometries can be extracted from Magnetic Resonance Imaging MRI, but due to long acquisition times, only static sounds can be currently studied with an adequate spatial resolution. In this work, 3D dynamic vocal tract representations are built to generate diphthongs, based on a set of cross-sections extracted from MRI-based vocal tract geometries of static vowel sounds. A diphthong can then be easily generated by interpolating the location, orientation and shape of these cross-sections, thus avoiding the interpolation of full 3D geometries. Two options are explored to extract the cross-sections. The first one is based on an adaptive grid AG, which extracts the cross-sections perpendicular to the vocal tract midline, whereas the second one resorts to a semi-polar grid SPG strategy, which fixes the cross-section orientations. The finite element method FEM has been used to solve the mixed wave equation and synthesize diphthongs [${alpha i}$] and [${alpha u}$] in the dynamic 3D vocal tracts. The outputs from a 1D acoustic model based on the Transfer Matrix Method have also been included for comparison. The results show that the SPG and AG provide very close solutions in 3D, whereas significant differences are observed when using them in 1D. The SPG dynamic vocal tract representation is recommended for 3D simulations because it helps to prevent the collision of adjacent cross-sections.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2173–2182},
numpages = {10}
}

@article{10.1109/TASLP.2019.2947232,
author = {Marafioti, Andr\'{e}s and Perraudin, Nathana\"{e}l and Holighaus, Nicki and Majdak, Piotr},
title = {A Context Encoder For Audio Inpainting},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2947232},
doi = {10.1109/TASLP.2019.2947232},
abstract = {In this article, we study the ability of deep neural networks (DNNs) to restore missing audio content based on its context, i.e., inpaint audio gaps. We focus on a condition which has not received much attention yet: gaps in the range of tens of milliseconds. We propose a DNN structure that is provided with the signal surrounding the gap in the form of time-frequency (TF) coefficients. Two DNNs with either complex-valued TF coefficient output or magnitude TF coefficient output were studied by separately training them on inpainting two types of audio signals (music and musical instruments) having 64-ms long gaps. The magnitude DNN outperformed the complex-valued DNN in terms of signal-to-noise ratios and objective difference grades. Although, for instruments, a reference inpainting obtained through linear predictive coding performed better in both metrics, it performed worse than the magnitude DNN for music. This demonstrates the potential of the magnitude DNN, in particular for inpainting signals that are more complex than single instrument sounds.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2362–2372},
numpages = {11}
}

@article{10.1109/TASLP.2019.2936755,
author = {Li, Wei and Chen, Nancy F. and Siniscalchi, Sabato Marco and Lee, Chin-Hui},
title = {Improving Mispronunciation Detection of Mandarin Tones for Non-Native Learners With Soft-Target Tone Labels and BLSTM-Based Deep Tone Models},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2936755},
doi = {10.1109/TASLP.2019.2936755},
abstract = {We investigate the effectiveness of soft-target tone labels and sequential context information for mispronunciation detection of Mandarin lexical tones pronounced by second language L2 learners whose first language L1 is of European origin. In conventional approaches, prosodic information e.g., F0 and tone posteriors extracted from trained tone models is used to calculate goodness of pronunciation GOP scores or train binary classifiers to verify pronunciation correctness. We propose three techniques to improve detection of mispronunciation of Mandarin tones for non-native learners. First, we extend our tone model from a deep neural network DNN to a bidirectional long short-term memory BLSTM network in order to more accurately model the high variability of non-native tone productions and the contextual information expressed in tone-level co-articulation. Second, we characterize ambiguous pronunciations where L2 learners’ tone realizations are between two canonical tone categories by relaxing hard target labels to soft targets with probabilistic transcriptions. Third, segmental tone features fed into verifiers are extracted by a BLSTM to exploit sequential context information to improve mispronunciation detection. Compared to DNN-GOP trained with hard targets, the proposed BLSTM-GOP framework trained with soft targets reduces the tones’ averaged equal error rate ERR from 7.58% to 5.83% and the averaged area under ROC curve AUC is increased from 97.85% to 98.31%. By utilizing BLSTM-based verifiers the EER further decreases to 5.16%, and the AUC is increased to 98.47%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2012–2024},
numpages = {13}
}

@article{10.1109/TASLP.2019.2934834,
author = {Ueno, Natsuki and Koyama, Shoichi and Saruwatari, Hiroshi},
title = {Three-Dimensional Sound Field Reproduction Based on Weighted Mode-Matching Method},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2934834},
doi = {10.1109/TASLP.2019.2934834},
abstract = {A sound field reproduction method based on the spherical wavefunction expansion of sound fields is proposed, which can be flexibly applied to various array geometries and directivities. First, we formulate sound field synthesis as a minimization problem of some norm on the difference between the desired and synthesized sound fields, and then the optimal driving signals are derived by using the spherical wavefunction expansion of the sound fields. This formulation is closely related to the mode-matching method; a major advantage of the proposed method is the optimal weight on the mode determined according to the norm to be minimized instead of the empirical truncation in the mode-matching method. We also provide some examples of norms and their corresponding weights in analytical forms. Both interior and exterior sound field reproduction are considered in the proposed method, and some applications, such as multizone reproduction and interior reproduction with exterior cancellation, are also discussed. Numerical simulation results indicated that higher reproduction accuracy can be achieved by the proposed method than by the current pressure-matching and mode-matching methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1852–1867},
numpages = {16}
}

@article{10.1109/TASLP.2019.2935891,
author = {Qi, Jun and Du, Jun and Siniscalchi, Sabato Marco and Lee, Chin-Hui},
title = {A Theory on Deep Neural Network Based Vector-to-Vector Regression With an Illustration of Its Expressive Power in Speech Enhancement},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2935891},
doi = {10.1109/TASLP.2019.2935891},
abstract = {This paper focuses on a theoretical analysis of deep neural network DNN based functional approximation. Leveraging upon two classical theorems on universal approximation, an artificial neural network ANN with a single hidden layer of neurons is used. With modified ReLU and Sigmoid activation functions, we first generalize the related concepts to vector-to-vector regression. Then, we show that the width of the hidden layer of ANN is numerically related to the approximation of the regression function. Furthermore, we increase the number of hidden layers and show that the depth of the ANN-based regression function can enhance its expressive power. We illustrate this representation with recently-emerged DNN based speech enhancement. We first compare the expressive power by varying ANN structures and then test its related regression performance under different noisy conditions in various noise types and signal-to-noise-ratio levels. Experimental results verify our theoretical prediction that an ANN of a broader hidden layer and a deeper architecture can jointly ensure a closer approximation of the vector-to-vector regression functions in terms of the Euclidean distance between the log power spectra of noisy and expected clean speech. Moreover, a DNN with a broader width at the top hidden layer can improve the regression performance relative to those with a narrower width at the top hidden layers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1932–1943},
numpages = {12}
}

@article{10.1109/TASLP.2019.2935807,
author = {Zheng, Yibin and Tao, Jianhua and Wen, Zhengqi and Yi, Jiangyan},
title = {Forward–Backward Decoding Sequence for Regularizing End-to-End TTS},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2935807},
doi = {10.1109/TASLP.2019.2935807},
abstract = {Neural end-to-end TTS such as Tacotron like network can generate very high-quality synthesized speech, and even close to human recording for similar domain text. However, it performs unsatisfactory when scaling it to some challenging test sets. One concern is that the encoder-decoder with attention-based network adopts autoregressive generative sequence model with the limitation of “exposure bias”: errors made early could be quickly amplified, harming subsequent sequence generation. To address this issue, we propose two novel methods, which aim at predicting future by improving the agreement between forward and backward decoding sequence. The first one denoted as MRBA is achieved by adding divergence regularization terms to model training objective to maximize the agreement between two directional models, namely L2R which generates targets from left-to-right and R2L which generates targets from right-to-left. While the second one denoted as BDR operates on decoder-level and exploits the future information during decoding. By introducing regularization term into the training objective of forward-backward decoders, the forward-decoder's hidden states are forced to be close to the backward-decoder's. Thus, the hidden representations of a unidirectional decoder are encouraged to embed some useful information about the future. Moreover, in order to make forward and backward decoding to improve each other in an interactive process, a joint training method is designed. Experimental results on both English and Mandarin dataset show that our proposed methods especially the second one BDR, lead to a significantly improvement on both robustness and overall naturalness, as achieving obvious preference advantages in a challenging test, and achieving state-of-the-art performance outperforming baseline “the revised version of Tacotron2” with a gap of 0.13 and 0.12 for English and Mandarin in MOS, respectively on a general test.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2067–2079},
numpages = {13}
}

@article{10.1109/TASLP.2019.2946086,
author = {Ali, Randall and Van Waterschoot, Toon and Moonen, Marc},
title = {Integration of <italic>a Priori</italic> and Estimated Constraints Into an MVDR Beamformer for Speech Enhancement},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2946086},
doi = {10.1109/TASLP.2019.2946086},
abstract = {Conventionally, the single constraint of the minimum variance distortionless response (MVDR) beamformer for speech enhancement has been defined using one of two approaches. Either it is based on a priori assumptions such as microphone characteristics, position, speech source location, and room acoustics, or on a relative transfer function (RTF) vector estimate using a data dependent method. Each approach has its respective merits and drawbacks and a decision usually has to be made between one of the approaches. In this paper, an alternative approach of using an integrated MVDR beamformer is investigated, where both the hard constraints from the two conventional approaches are softened to yield two tuning parameters. It will be shown that this integrated MVDR beamformer can be expressed as a convex combination of the conventional MVDR beamformers, a linearly constrained minimum variance (LCMV) beamformer, and an all-zero vector, with real, positive-valued coefficients. By analysing how the tuning parameters affect these coefficients, two tuning rules for a practical implementation of the integrated MVDR are subsequently proposed. An evaluation with simulated and recorded data demonstrates that the integrated MVDR beamformer can be beneficial as opposed to relying on either of the conventional MVDR beamformers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2288–2300},
numpages = {13}
}

@article{10.1109/TASLP.2019.2942157,
author = {Hu, Jingyi and Chen, Ning},
title = {Enhanced Feature Summarizing for Effective Cover Song Identification},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2942157},
doi = {10.1109/TASLP.2019.2942157},
abstract = {Self-similarity analysis-based feature summarizing technique SuCo was proposed recently to improve the time and memory efficiency of Cover Song Identification CSI. In this paper, both the feature summarizing and the cross-similarity calculating strategies of the SuCo model are modified as follows to enhance its identification accuracy. At the feature summarizing stage, first, the Hubness Reduction HR strategy is adopted to reduce the possible ‘Hubness’ phenomenon existing in the feature subsequence community, which may affect the retrieval effectiveness. Then, the Network Enhancement NE technique, which was originally proposed in biology to improve gene-function prediction accuracy, is introduced to reduce the noise in the self-similarity network caused by the limitation of feature extraction and similarity measuring, and the inherent musical and acoustic variations. At the cross-similarity calculating stage, first, the summarized representative feature subsequences of the reference are concatenated to obtain its combined representative feature. Then, considering that the nonlinear recurrence property is important for describing the melody perception-based similarity, Qmax is adopted to measure the similarity between the combined representative feature of the reference and the unsummarized feature sequence of the query. Extensive experiments carried out on four open CSI datasets with 5 types of features and 2 kinds of representative feature subsequence choosing methods verify that: i The proposed scheme outperforms the SuCo model in retrieval effectiveness. ii Each of the above modifications contributes to the performance enhancement of the proposed scheme. iii The proposed scheme achieves high generalization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2113–2126},
numpages = {14}
}

@article{10.1109/TASLP.2019.2935809,
author = {Valimaki, Vesa and Ramo, Jussi},
title = {Neurally Controlled Graphic Equalizer},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2935809},
doi = {10.1109/TASLP.2019.2935809},
abstract = {This paper describes a neural network based method to simplify the design of a graphic equalizer without sacrificing the accuracy of approximation. The key idea is to train a neural network to predict the mapping from target gains to the optimized band filter gains at specified center frequencies. The prediction is implemented with a feedforward neural network having a hidden layer with 20 neurons in the case of the ten-octave graphic equalizer. The band filter coefficients can then be quickly and easily computed using closed-form formulas. This work turns, for the first time, the accurate graphic equalization design into a feedforward calculation without matrix inversion or iterations. The filter gain control using the neural network reduces the computing time by 99.6% in comparison to the least-squares design method it is imitating and contributes an approximation error of less than 0.1&nbsp;dB. The resulting neurally controlled graphic equalizer will be highly useful in various audio and music processing applications, which require time-varying equalization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2140–2149},
numpages = {10}
}

@article{10.1109/TASLP.2019.2937190,
author = {Chen, Kehai and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro and Zhao, Tiejun},
title = {Neural Machine Translation With Sentence-Level Topic Context},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2937190},
doi = {10.1109/TASLP.2019.2937190},
abstract = {Traditional neural machine translation NMT methods use the word-level context to predict target language translation while neglecting the sentence-level context, which has been shown to be beneficial for translation prediction in statistical machine translation. This paper represents the sentence-level context as latent topic representations by using a convolution neural network, and designs a topic attention to integrate source sentence-level topic context information into both attention-based and Transformer-based NMT. In particular, our method can improve the performance of NMT by modeling source topics and translations jointly. Experiments on the large-scale LDC Chinese-to-English translation tasks and WMT’14 English-to-German translation tasks show that the proposed approach can achieve significant improvements compared with baseline systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1970–1984},
numpages = {15}
}

@article{10.1109/TASLP.2019.2935837,
author = {Dang, Xudong and Cheng, Qi and Zhu, Hongyan},
title = {Indoor Multiple Sound Source Localization via Multi-Dimensional Assignment Data Association},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2935837},
doi = {10.1109/TASLP.2019.2935837},
abstract = {In this paper, we address the multiple sound source localization problem by associating and fusing the direction of arrival DOA estimates from multiple microphone arrays. For multi-source scenarios especially in indoor environments, a critical issue is to tell the correspondence among DOA estimates across different arrays, which is known as the data association problem. We propose a multi-dimensional assignment-based data association approach to find the optimal associations of DOA estimates from the same source. First, in the sense of maximum likelihood, the data association problem is formulated by finding the most likely partition of the measurement set into the source-originated and false alarm-originated subsets. Next, by defining the association costs appropriately, the problem of finding the most likely measurement partition is transformed into a generalized multi-dimensional assignment problem which can be solved efficiently by a Lagrangian relaxation algorithm. After the optimal associations of DOA estimates across different arrays are obtained, the locations of sources can be estimated by fusing the same source-originated DOA estimates. In the presence of missed detections, false alarms and the unknown number of sources, the proposed method achieves high accuracy in data association and localization, and outperforms the competing method in reverberant and noisy environments. In addition, since our method does not require additional features and uses DOA estimates only, it is more computationally efficient than the competing method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1944–1956},
numpages = {13}
}

@article{10.1109/TASLP.2019.2934567,
author = {Borra, Federico and Bernardini, Alberto and Antonacci, Fabio and Sarti, Augusto},
title = {Uniform Linear Arrays of First-Order Steerable Differential Microphones},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2934567},
doi = {10.1109/TASLP.2019.2934567},
abstract = {We propose a spatial filtering method for linear arrays of First-Order Steerable Differential Microphones FOSDMs, which operates in two layers. In the former, signals acquired by individual microphones are locally filtered to produce the outputs of the FOSDMs. In the latter, the outputs of the FOSDMs are processed by another filter. We analyse different design methodologies and study the conditions under which the two filtering layers can be decoupled. The proposed two-layer spatial filter can be flexibly controlled with a single scalar parameter, which can be chosen, for example, to maximize the White Noise Gain like in a Delay-and-Sum beamformer; or to maximize the Directivity Factor like in a Super-Directive beamformer; without needing any matrix inversion. The effectiveness of the proposed beamforming method is compared with traditional spatial filtering techniques using different metrics.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1906–1918},
numpages = {13}
}

@article{10.1109/TASLP.2019.2941592,
author = {Pfeifenberger, Lukas and Zohrer, Matthias and Pernkopf, Franz},
title = {Eigenvector-Based Speech Mask Estimation for Multi-Channel Speech Enhancement},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2941592},
doi = {10.1109/TASLP.2019.2941592},
abstract = {We present the Eigennet architecture for estimating a gain mask from noisy, multi-channel microphone observations. While existing mask estimators use magnitude features, our system also exploits the spatial information embedded in the phase of the data. The mask is used to obtain the Minimum Variance Distortionless Response MVDR and Generalized Eigenvalue GEV beamformers. We also derive the Phase Aware Normalization PAN postfilter, which corrects both magnitude and phase distortions caused by the GEV. Further, we demonstrate the properties of our eigenvector features, and compare their performance with three state-of-the-art reference systems. We report their performance in terms of SNR improvement and Word Error Rate WER using Google and Kaldi Speech-to-Text API. Experiments are performed on the WSJ0 and CHiME4 corpora, where competitive performance in both WER and SNR is achieved.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2162–2172},
numpages = {11}
}

@article{10.1109/TASLP.2019.2935803,
author = {Chai, Li and Du, Jun and Liu, Qing-Feng and Lee, Chin-Hui},
title = {Using Generalized Gaussian Distributions to Improve Regression Error Modeling for Deep Learning-Based Speech Enhancement},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2935803},
doi = {10.1109/TASLP.2019.2935803},
abstract = {From a statistical perspective, the conventional minimum mean squared error MMSE criterion can be considered as the maximum likelihood ML solution under an assumed homoscedastic Gaussian error model. However, in this paper, a statistical analysis reveals the super-Gaussian and heteroscedastic properties of the prediction errors in nonlinear regression deep neural network DNN-based speech enhancement when estimating clean log-power spectral LPS components at DNN outputs with noisy LPS features in DNN input vectors. Accordingly, we propose treating all dimensions of the prediction error vector as statistically independent random variables and model them with generalized Gaussian distributions GGDs. Then, the objective function with the GGD error model is derived according to the ML criterion. Experiments on the TIMIT corpus corrupted by simulated additive noises show consistent improvements of our proposed DNN framework over the conventional DNN framework in terms of various objective quality measures under 14 unseen noise types evaluated and at various signal-to-noise ratio levels. Furthermore, the ML optimization objective with GGD outperforms the conventional MMSE criterion, achieving improved generalization and robustness.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1919–1931},
numpages = {13}
}

@article{10.1109/TASLP.2019.2937174,
author = {Wood, Sean U. N. and Stahl, Johannes K. W. and Mowlaee, Pejman},
title = {Binaural Codebook-Based Speech Enhancement With Atomic Speech Presence Probability},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2937174},
doi = {10.1109/TASLP.2019.2937174},
abstract = {In this work, we present a universal codebook-based speech enhancement framework that relies on a single codebook to encode both speech and noise components. The atomic speech presence probability ASPP is defined as the probability that a given codebook atom encodes speech at a given point in time. We develop ASPP estimators based on binaural cues including the interaural phase and level difference IPD and ILD, the interaural coherence magnitude ICM, as well as a combined version leveraging the full interaural transfer function ITF. We evaluate the performance of the resulting ASPP-based speech enhancement algorithms on binaural mixtures of reverberant speech and real-world noise. The proposed approach improves both objective speech quality and intelligibility over a wide range of input SNR, as measured with PESQ and binaural STOI metrics, outperforming two binaural speech enhancement benchmark methods. We show that the proposed ITF-based ASPP approach achieves a good balance of the trade-off between binaural noise reduction and binaural cue preservation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2150–2161},
numpages = {12}
}

@article{10.1109/TASLP.2019.2937413,
author = {Gomez-Alanis, Alejandro and Peinado, Antonio M. and Gonzalez, Jose A. and Gomez, Angel M.},
title = {A Gated Recurrent Convolutional Neural Network for Robust Spoofing Detection},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2937413},
doi = {10.1109/TASLP.2019.2937413},
abstract = {Automatic speaker verification ASV systems are exposed to spoofing attacks which may compromise their security. While anti-spoofing techniques have been mainly studied for clean scenarios, it has also been shown that they perform poorly in noisy environments. In this work, we aim at improving the performance of spoofing detection for ASV in clean and noisy scenarios. To achieve this, we first propose the use of Gated Recurrent Convolutional Neural Networks GRCNNs as a deep feature extractor to robustly represent speech signals as utterance-level embeddings, which are later used by a back-end recognizer for the final genuine/spoofed classification. Then, to enhance the robustness of the system in noisy conditions, we propose the use of signal-to-noise masks SNMs as new input features to inform the anti-spoofing system about the time-frequency regions of the input spectral features that are mostly affected by noise and, hence, should be neglected when computing the embeddings. To evaluate our proposals, experiments were carried out on the clean and noisy versions of the ASVspoof 2015 corpus for detecting logical access attacks, as well as on the ASVspoof 2017 database to detect replay attacks. Additional results are provided for the ASVspoof 2019 corpus, including both logical and physical scenarios. The experimental results show that our proposal clearly outperforms some well-known methods based on classical features and other similar deep feature based systems for both clean and noisy conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1985–1999},
numpages = {15}
}

@article{10.1109/TASLP.2019.2937192,
author = {Tu, Quansheng and Chen, Huawei},
title = {On Mainlobe Orientation of the First- and Second-Order Differential Microphone Arrays},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2937192},
doi = {10.1109/TASLP.2019.2937192},
abstract = {Due to the increased sensitivity to microphone mismatches, lower-order differential microphone arrays DMAs are usually employed in practice, especially the first and second-order DMAs. It is known that the mainlobe orientation of the typical first- and second-order DMAs is both along the fixed endfire direction. This may be no longer true, however, in the presence of microphone mismatchs. This paper studies the fundamental problem of how microphone mismatches affect mainlobe orientation of the first- and second-order DMAs. Some insights into the effects of microphone mismatches on the mainlobe orientation of the two types of DMAs are revealed. In addition, the property on mainlobe orientation of the second-order DMA under ideal condition, which is yet to be known, is also studied. Moreover, tolerance analysis of microphone mismatches to ensure correct mainlobe orientation of the first- and second-order DMAs are performed in order to offer a useful guidance for practical design. Numerical examples are shown to verify the theoretical findings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2025–2040},
numpages = {16}
}

@article{10.1109/TASLP.2019.2936385,
author = {Schneider, Martin and Habets, Emanuel A. P.},
title = {Iterative DFT-Domain Inverse Filter Optimization Using a Weighted Least-Squares Criterion},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2936385},
doi = {10.1109/TASLP.2019.2936385},
abstract = {For many inverse filtering problems, finite impulse response filters are designed according to least-squares criteria, where time-domain and frequency-domain weights are often applied to achieve optimal results for the considered application. While least-squares-optimal filter coefficients are given by an explicit formula, the computation cost to compute its solution is proportional up to the third power of the number of jointly optimized filter coefficients. A joint optimization of all filter coefficients is necessary whenever a time-domain or a frequency-domain weight is introduced. This imposes limits for filter lengths and numbers of channels in many real-world scenarios. In this contribution, an algorithm is presented that yields time-domain filter coefficients optimized to meet such a weighted least-squares criterion, while performing the most expensive computation steps efficiently in the discrete Fourier transform domain. As a consequence, the demands on computational power and memory are kept on a moderate level, even for large-scale problems. A rigorous mathematical derivation is provided that identifies all approximations used in the algorithm. Additionally, an effective regularization method is proposed that does not depend much on the regularization parameters. Furthermore, the proposed approach is experimentally evaluated considering a sound-zones scenario, which is one of many possible application areas. In that way, the applicability of the proposed approach is verified.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1957–1969},
numpages = {13}
}

@article{10.1109/TASLP.2019.2938863,
author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and van den Oord, Aaron},
title = {Unsupervised Speech Representation Learning Using WaveNet Autoencoders},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2938863},
doi = {10.1109/TASLP.2019.2938863},
abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g. phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder VAE, and a discrete Vector Quantized VAE VQ-VAE. We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2041–2053},
numpages = {13}
}

@article{10.1109/TASLP.2019.2941148,
author = {Liu, Yuzhou and Wang, DeLiang},
title = {Divide and Conquer: A Deep CASA Approach to Talker-Independent Monaural Speaker Separation},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2941148},
doi = {10.1109/TASLP.2019.2941148},
abstract = {We address talker-independent monaural speaker separation from the perspectives of deep learning and computational auditory scene analysis CASA. Specifically, we decompose the multi-speaker separation task into the stages of simultaneous grouping and sequential grouping. Simultaneous grouping is first performed in each time frame by separating the spectra of different speakers with a permutation-invariantly trained neural network. In the second stage, the frame-level separated spectra are sequentially grouped to different speakers by a clustering network. The proposed deep CASA approach optimizes frame-level separation and speaker tracking in turn, and produces excellent results for both objectives. Experimental results on the benchmark WSJ0-2mix database show that the new approach achieves the state-of-the-art results with a modest model size.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2092–2102},
numpages = {11}
}

@article{10.1109/TASLP.2019.2944348,
author = {Sekiguchi, Kouhei and Bando, Yoshiaki and Nugraha, Aditya Arie and Yoshii, Kazuyoshi and Kawahara, Tatsuya},
title = {Semi-Supervised Multichannel Speech Enhancement With a Deep Speech Prior},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2944348},
doi = {10.1109/TASLP.2019.2944348},
abstract = {This paper describes a semi-supervised multichannel speech enhancement method that uses clean speech data for prior training. Although multichannel nonnegative matrix factorization (MNMF) and its constrained variant called independent low-rank matrix analysis (ILRMA) have successfully been used for unsupervised speech enhancement, the low-rank assumption on the power spectral densities (PSDs) of all sources (speech and noise) does not hold in reality. To solve this problem, we replace a low-rank speech model with a deep generative speech model, i.e., formulate a probabilistic model of noisy speech by integrating a deep speech model, a low-rank noise model, and a full-rank or rank-1 model of spatial characteristics of speech and noise. The deep speech model is trained from clean speech data in an unsupervised auto-encoding variational Bayesian manner. Given multichannel noisy speech spectra, the full-rank or rank-1 spatial covariance matrices and PSDs of speech and noise are estimated in an unsupervised maximum-likelihood manner. Experimental results showed that the full-rank version of the proposed method was significantly better than MNMF, ILRMA, and the rank-1 version. We confirmed that the initialization-sensitivity and local-optimum problems of MNMF with many spatial parameters can be solved by incorporating the precise speech model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2197–2212},
numpages = {16}
}

@article{10.1109/TASLP.2019.2933727,
author = {Wu, Lijun and Tan, Xu and Qin, Tao and Lai, Jianhuang and Liu, Tie-Yan},
title = {Beyond Error Propagation: Language Branching Also Affects the Accuracy of Sequence Generation},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2933727},
doi = {10.1109/TASLP.2019.2933727},
abstract = {Sequence generation tasks, such as neural machine translation NMT and abstractive summarization, usually suffer from exposure bias as well as the error propagation problem due to the autoregressive training and generation. Many previous works have discussed the relationship between error propagation and the accuracy drop problem i.e., the right part of the generated sentence is often worse than its left part in left-to-right decoding models. In this paper, taking NMT as a typical sequence generation task, we measure the accuracy of the generated sentence with various metrics and conduct a series of analyses to deeply understand the accuracy drop problem. We obtain several interesting findings. First, The role of error propagation on accuracy drop is overstated in the literature, although it is indeed a cause to the accuracy drop problem. Second, Characteristics of a language play a more important role in causing the accuracy drop problem: the left part of the generated sentence in a right-branching language e.g., English is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language e.g., Japanese. Our discoveries are also confirmed on other generation tasks e.g., image captioning, abstractive summarization and language modeling with multiple left/right-branching languages, as well as in various model structures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1868–1879},
numpages = {12}
}

@article{10.1109/TASLP.2019.2946897,
author = {Yang, Jichen and Das, Rohan Kumar and Zhou, Nina},
title = {Extraction of Octave Spectra Information for Spoofing Attack Detection},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2946897},
doi = {10.1109/TASLP.2019.2946897},
abstract = {This article focuses on extracting information from the octave power spectra of long-term constant-Q transform (CQT) for spoofing attack detection. A novel framework based on multi-level transform (MLT) is proposed that can capture the relevant information from octave power spectra using level by level in a multi-level manner. We then derive a novel feature referred to as constant-Q multi-level coefficient (CMC) based on proposed MLT. The proposed feature is evaluated on synthetic as well as replay speech detection studies on ASVspoof 2015 and ASVspoof 2017 version 2.0 database, respectively. We find the proposed CMC feature outperforms the conventional constant-Q cepstral coefficient based long-term feature obtained from linear power spectrum after uniform resampling. This depicts the usefulness of MLT to extract salient artifacts from octave power spectrum. Further, the proposed CMC feature performs better than the existing the well known other state-of-the-art systems for spoofing attack detection that showcases its importance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2373–2384},
numpages = {12}
}

@article{10.1109/TASLP.2019.2945485,
author = {Tiwari, Nitya and Pandey, Prem C.},
title = {Speech Enhancement Using Noise Estimation With Dynamic Quantile Tracking},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2945485},
doi = {10.1109/TASLP.2019.2945485},
abstract = {A technique for quantile-based noise estimation is presented for single-input speech enhancement in hearing aids and speech communication devices. The noise spectrum is updated by dynamic tracking of quantiles of the samples of the magnitude spectrum of the noisy speech without sorting of the past samples. Another technique is presented for improved tracking of nonstationary noise using adaptive quantiles, which are calculated by estimation of the quantile functions. The two noise estimation techniques are compared with some of the earlier techniques in terms of computational requirement, error in noise tracking, and speech enhancement using spectral subtraction based on the geometric approach. The technique with fixed quantiles has the lowest computational requirement and its performance in terms of noise tracking and speech enhancement for different SNRs and noise types is found to be better than or comparable to the earlier techniques. The technique with adaptive quantiles, having a higher computational requirement, provides better performance at low SNRs and for nonstationary noises.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2301–2312},
numpages = {12}
}

@article{10.1109/TASLP.2019.2944568,
author = {Barkan, Oren and Tsiris, David and Katz, Ori and Koenigstein, Noam},
title = {InverSynth: Deep Estimation of Synthesizer Parameter Configurations From Audio Signals},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2944568},
doi = {10.1109/TASLP.2019.2944568},
abstract = {Sound synthesis is a complex field that requires domain expertise. Manual tuning of synthesizer parameters to match a specific sound can be an exhaustive task, even for experienced sound engineers. In this paper, we introduce InverSynth - an automatic method for synthesizer parameters tuning to match a given input sound. InverSynth is based on strided convolutional neural networks and is capable of inferring the synthesizer parameters configuration from the input spectrogram and even from the raw audio. The effectiveness InverSynth is demonstrated on a subtractive synthesizer with four frequency modulated oscillators, envelope generator and a gater effect. We present extensive quantitative and qualitative results that showcase the superiority InverSynth over several baselines. Furthermore, we show that the network depth is an important factor that contributes to the prediction accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2385–2396},
numpages = {12}
}

@article{10.1109/TASLP.2019.2933325,
author = {Das, Amit and Li, Jinyu and Ye, Guoli and Zhao, Rui and Gong, Yifan},
title = {Advancing Acoustic-to-Word CTC Model With Attention and Mixed-Units},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2933325},
doi = {10.1109/TASLP.2019.2933325},
abstract = {The acoustic-to-word model based on the Connectionist Temporal Classification CTC criterion is a natural end-to-end E2E system directly targeting word as output unit. Two issues exist in the system: first, the current output of the CTC model relies on the current input and does not account for context weighted inputs. This is the hard alignment issue. Second, the word-based CTC model suffers from the out-of-vocabulary OOV issue. This means it can model only frequently occurring words while tagging the remaining words as OOV. Hence, such a model is limited in its capacity in recognizing only a fixed set of frequent words. In this study, we propose addressing these problems using a combination of attention mechanism and mixed-units. In particular, we introduce Attention CTC, Self-Attention CTC, Hybrid CTC, and Mixed-unit CTC. First, we blend attention modeling capabilities directly into the CTC network using Attention CTC and Self-Attention CTC. Second, to alleviate the OOV issue, we present Hybrid CTC which uses a word and letter CTC with shared hidden layers. The Hybrid CTC consults the letter CTC when the word CTC emits an OOV. Then, we propose a much better solution by training a Mixed-unit CTC which decomposes all the OOV words into sequences of frequent words and multi-letter units. Evaluated on a 3400 hours Microsoft Cortana voice assistant task, our final acoustic-to-word solution using attention and mixed-units achieves a relative reduction in word error rate WER over the vanilla word CTC by 12.09%. Such an E2E model without using any language model LM or complex decoder also outperforms a traditional context-dependent CD phoneme CTC with strong LM and decoder by 6.79% relative.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1880–1892},
numpages = {13}
}

@article{10.1109/TASLP.2019.2941587,
author = {Liu, Xuebo and Wong, Derek F. and Chao, Lidia S. and Liu, Yang},
title = {Latent Attribute Based Hierarchical Decoder for Neural Machine Translation},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2941587},
doi = {10.1109/TASLP.2019.2941587},
abstract = {Neural machine translation NMT has achieved state-of-the-art performance in many translation tasks. However, because the computational cost increases with the size of the search space for predicting the target words, the translation quality of NMT is constrained by the limited vocabulary. To alleviate this problem, we propose a novel dynamic hierarchical decoder for NMT to utilize all of the target words in the training and decoding process. In the proposed model, a target word is represented by two latent attribute vectors rather than a word vector.&nbsp;The model is trained to dynamically put together those words that share similar linguistic attributes. The prediction of a target word is, therefore, turned into the prediction of attribute vectors, where the $mathrm{softmax}$ functions are performed at the attribute level. This greatly reduces the model size and the decoding time.&nbsp;Our experimental results demonstrate that the proposed model significantly outperforms the NMT baselines in both Chinese-English and English-German translation tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2103–2112},
numpages = {10}
}

@article{10.1109/TASLP.2019.2946043,
author = {Remaggi, Luca and Jackson, Philip J. B. and Wang, Wenwu},
title = {Modeling the Comb Filter Effect and Interaural Coherence for Binaural Source Separation},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2946043},
doi = {10.1109/TASLP.2019.2946043},
abstract = {Typical methods for binaural source separation consider only the direct sound as the target signal in a mixture. However, in most scenarios, this assumption limits the source separation performance. It is well known that the early reflections interact with the direct sound, producing acoustic effects at the listening position, e.g. the so-called comb filter effect. In this article, we propose a novel source separation model, that utilizes both the direct sound and the first early reflection information to model the comb filter effect. This is done by observing the interaural phase difference obtained from the time-frequency representation of binaural mixtures. Furthermore, a method is proposed to model the interaural coherence of the signals. Including information related to the sound multipath propagation, the performance of the proposed separation method is improved with respect to the baselines that did not use such information, as illustrated by using binaural recordings made in four rooms, having different sizes and reverberation times.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2263–2277},
numpages = {15}
}

@article{10.1109/TASLP.2019.2944563,
author = {Zhao, Lujun and Qiu, Xipeng and Zhang, Qi and Huang, Xuanjing},
title = {Sequence Labeling With Deep Gated Dual Path CNN},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2944563},
doi = {10.1109/TASLP.2019.2944563},
abstract = {Sequence labeling, such as part-of-speech (POS) tagging, named entity recognition (NER), text chunking, is a classic task in natural language processing. Most existing neural networks models for sequence labeling are based on recurrent neural networks. Recently, convolutional neural networks have been proposed to replace the recurrent components for sequence labeling. However, they are usually shallow compared to deep convolutional networks that achieve start-of-the-art performance in other fields. Due to the vanishing gradient problem, these models usually can not work well when simply increasing the number of layers. In this paper, we propose using deep CNN architecture in sequence labeling, which can capture a large context through stacked convolutions. To reduce the vanishing gradient problem, the proposed method incorporates gated linear units, residual connections, and dense connections. Experimental results on three sequence labeling tasks show that the proposed model can achieve competitive performance to the RNN-based state-of-the-art method while maintaining <inline-formula><tex-math notation="LaTeX">$2.41times$</tex-math></inline-formula> faster speed, even with up to 10 convolutional layers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2326–2335},
numpages = {10}
}

@article{10.1109/TASLP.2019.2944078,
author = {Guo, Qipeng and Qiu, Xipeng and Xue, Xiangyang and Zhang, Zheng},
title = {Low-Rank and Locality Constrained Self-Attention for Sequence Modeling},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2944078},
doi = {10.1109/TASLP.2019.2944078},
abstract = {Self-attention mechanism becomes more and more popular in natural language processing (NLP) applications. Recent studies show the Transformer architecture which relies mainly on the attention mechanism achieves much success on large datasets. But a raised problem is its generalization ability is weaker than CNN and RNN on many moderate-sized datasets. We think the reason can be attributed to its unsuitable inductive bias of the self-attention structure. In this paper, we regard the self-attention as matrix decomposition problem and propose an improved self-attention module by introducing two linguistic constraints: low-rank and locality. We further develop the low-rank attention and band attention to parameterize the self-attention mechanism under the low-rank and locality constraints. Experiments on several real NLP tasks show our model outperforms the vanilla Transformer and other self-attention models on moderate size datasets. Additionally, evaluation on a synthetic task gives us a more detailed understanding of working mechanisms of different architectures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2213–2222},
numpages = {10}
}

@article{10.1109/TASLP.2019.2942160,
author = {Ma, Qianli and Yu, Liuhong and Tian, Shuai and Chen, Enhuan and Ng, Wing W. Y.},
title = {Global-Local Mutual Attention Model for Text Classification},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2942160},
doi = {10.1109/TASLP.2019.2942160},
abstract = {Text classification is a central field of inquiry in natural language processing NLP. Although some models learn local semantic features and global long-term dependencies simultaneously, they simply combine them through concatenation either in a cascade way or in parallel while mutual effects between them are ignored. In this paper, we propose the Global-Local Mutual Attention GLMA model for text classification problems, which introduces a mutual attention mechanism for mutual learning between local semantic features and global long-term dependencies. The mutual attention mechanism consists of a Local-Guided Global-Attention LGGA and a Global-Guided Local-Attention GGLA. The LGGA allows to assign weights and combine global long-term dependencies of word positions that are semantic related. It captures combined semantics and alleviates the gradient vanishing problem. The GGLA automatically assigns more weights to relevant local semantic features, which captures key local semantic information and filters both noises and irrelevant words/phrases. Furthermore, a weighted-over-time pooling operation is developed to aggregate the most informative and discriminative features for classification. Extensive experiments demonstrate that our model obtains the state-of-the-art performance on seven benchmark datasets and sixteen Amazon product reviews datasets. Both the result analysis and the mutual attention weights visualization further demonstrate the effectiveness of the proposed model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2127–2139},
numpages = {13}
}

@article{10.1109/TASLP.2019.2937953,
author = {Feng, Siyuan and Lee, Tan},
title = {Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2937953},
doi = {10.1109/TASLP.2019.2937953},
abstract = {This research addresses the problem of acoustic modeling of low-resource languages for which transcribed training data is absent. The goal is to learn robust frame-level feature representations that can be used to identify and distinguish subword-level speech units. The proposed feature representations comprise various types of multilingual bottleneck features BNFs that are obtained via multi-task learning of deep neural networks MTL-DNN. One of the key problems is how to acquire high-quality frame labels for untranscribed training data to facilitate supervised DNN training. It is shown that learning of robust BNF representations can be achieved by effectively leveraging transcribed speech data and well-trained automatic speech recognition ASR systems from one or more out-of-domain resource-rich languages. Out-of-domain ASR systems can be applied to perform speaker adaptation with untranscribed training data of the target language, and to decode the training speech into frame-level labels for DNN training. It is also found that better frame labels can be generated by considering temporal dependency in speech when performing frame clustering. The proposed methods of feature learning are evaluated on the standard task of unsupervised subword modeling in Track 1 of the ZeroSpeech 2017 Challenge. The best performance achieved by our system is $9.7%$ in terms of across-speaker triphone minimal-pair ABX error rate, which is comparable to the best systems reported recently. Lastly, our investigation reveals that the closeness between target languages and out-of-domain languages and the amount of available training data for individual target languages could have significant impact on the goodness of learned features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2000–2011},
numpages = {12}
}

@article{10.1109/TASLP.2019.2943018,
author = {Liu, Dayiheng and Fu, Jie and Qu, Qian and Lv, Jiancheng},
title = {BFGAN: Backward and Forward Generative Adversarial Networks for Lexically Constrained Sentence Generation},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2943018},
doi = {10.1109/TASLP.2019.2943018},
abstract = {Incorporating prior knowledge like <italic>lexical constraints</italic> into the model’s output to generate meaningful and coherent sentences has many applications in dialogue system, machine translation, image captioning, etc. However, existing auto-regressive models incrementally generate sentences from left to right via beam search, which makes it difficult to directly introduce lexical constraints into the generated sentences. In this paper, we propose a new algorithmic framework, dubbed BFGAN, to address this challenge. Specifically, we employ a backward generator and a forward generator to generate lexically constrained sentences together, and use a discriminator to guide the joint training of two generators by assigning them reward signals. Due to the difficulty of BFGAN training, we propose several training techniques to make the training process more stable and efficient. Our extensive experiments on three large-scale datasets with human evaluation demonstrate that BFGAN has significant improvements over previous methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2350–2361},
numpages = {12}
}

@article{10.1109/TASLP.2019.2945479,
author = {Ben-Hur, Zamir and Alon, David Lou and Mehra, Ravish and Rafaely, Boaz},
title = {Efficient Representation and Sparse Sampling of Head-Related Transfer Functions Using Phase-Correction Based on Ear Alignment},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2945479},
doi = {10.1109/TASLP.2019.2945479},
abstract = {With the proliferation of high quality virtual reality systems, the demand for high fidelity spatial audio reproduction has grown. This requires individual head-related transfer functions (HRTFs) with high spatial resolution. Acquiring such HRTFs is not always possible, which motivates the need for sparsely sampled HRTFs. Additionally, real-time applications require compact representation of HRTFs. Recently, spherical-harmonics (SH) has been suggested for efficient interpolation and representation of HRTFs. However, representation of sparse HRTFs with a limited SH order may introduce spatial aliasing and truncation errors, which have a detrimental effect on the reproduced spatial audio. This is because the HRTF is inherently of a high spatial order. One approach to overcome this limitation is to pre-process the HRTF, with the aim of reducing its effective SH order. A recent study showed that order-reduction can be achieved by time-alignment of HRTFs, through numerical estimation of the time delays of the HRTFs. In this paper, a new method for pre-processing HRTFs in order to reduce their effective order is presented. The method uses phase-correction based on ear alignment, by exploiting the dual-centering nature of HRTF measurements. In contrast to time-alignment, the phase-correction is performed parametrically, making it more robust to measurement noise. The SH order reduction and ensuing interpolation errors due to sparse sampling were analyzed for these two methods. Results indicate significant reduction in the effective SH order, where only 100 measurements and order 6 are required to achieve a normalized mean square error below <inline-formula><tex-math notation="LaTeX">$-$</tex-math></inline-formula>10&nbsp;dB compared to a fully-sampled, high-order HRTF.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2249–2262},
numpages = {14}
}

@article{10.1109/TASLP.2019.2947364,
author = {Duan, Junwen and Ding, Xiao and Zhang, Yue and Liu, Ting},
title = {TEND: A Target-Dependent Representation Learning Framework for News Document},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2947364},
doi = {10.1109/TASLP.2019.2947364},
abstract = {Real-time news documents published on the Internet have global financial and political impacts. Pioneering statistical approaches investigate manually defined features to capture lexical, sentiment, and event information, which suffer from feature sparsity. As a remedy, recent work has considered learning dense vector representations for documents. Such representations are general, which can not model target-dependent scenarios, such as stance detection towards a specific claim. There has been work on target-specific word and sentence representations, but little was done on target-dependent document representation. Moreover, documents contain more potentially helpful information, but also noise compared to events and sentences. To address the above issues, we focus on models that are: 1. <italic>task-driven</italic>, which optimize the neural network representations for the end task; 2. <italic>target-specific</italic>, learning news representations by considering the influence of specific targets. In particular, we propose a novel document-level target-dependent learning framework TEND. The framework employs the information of the target and the news abstract as clues, obtaining relatively informative sentences from the entire document for our objectives. The framework assembles a document representation by integrating the news abstract representation and a weighted sum of sentence representations in the document. To the best of our knowledge, we are among the first to investigate target-dependent document representation. Existing text representation models can be easily integrated into our TEND framework, and it is general enough to be applied to different target-dependent document representation tasks. We empirically evaluate our framework on two target-dependent document-level tasks, including a cumulative abnormal return prediction task and a news stance detection task. Results show that our models give the best performances compared to state-of-the-art document embedding methods, yielding robust and consistent performances across datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2313–2325},
numpages = {13}
}

@article{10.1109/TASLP.2019.2939782,
author = {Varanasi, Vishnuvardhan and Agarwal, Ayushya and Hegde, Rajesh M.},
title = {Near-Field Acoustic Source Localization Using Spherical Harmonic Features},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2939782},
doi = {10.1109/TASLP.2019.2939782},
abstract = {Near-field acoustic source localization and beamforming has hitherto not been investigated extensively in the spherical harmonic domain under reverberant conditions. In this paper, a novel method for the near-field direction of arrival DOA and range estimation using signal invariant and direction independent spherical harmonic features is proposed. A spatial pressure interpolation method that effectively captures the acoustic energy on the surface of the sphere is first developed in the spherical harmonic domain. Near-field DOA estimates are then computed using this pressure distribution. Spherical harmonic features that are signal invariant and direction independent are then extracted using the near field DOA estimates. Signal invariant features are obtained by normalizing spherical harmonic coefficients with a component that is proportional to the source signal strength. Direction independent features are obtained using two methods. Rotation of spherical harmonic functions over a sphere is performed using Wigner-D functions in one method, whereas in the other, the effect of DOA is compensated by spherical harmonic normalization. Using the signal invariant and direction independent features, a learning-based framework which utilizes a convolutional neural network and voicing activity detection is also developed to compute the range of the near-field source. Experiments are conducted both on simulated and real speech data for evaluating the performance of the proposed spherical harmonic features in the context of near-field localization as well as beamforming. Root mean square error of both near-field DOA and source range estimates are obtained. Objective evaluation of near-field beamformed acoustic outputs is also performed. Results obtained are motivating enough for the method to be used in practical near-field beamforming applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2054–2066},
numpages = {13}
}

@article{10.1109/TASLP.2019.2940662,
author = {Tu, Yan-Hui and Du, Jun and Lee, Chin-Hui},
title = {Speech Enhancement Based on Teacher–Student Deep Learning Using Improved Speech Presence Probability for Noise-Robust Speech Recognition},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2940662},
doi = {10.1109/TASLP.2019.2940662},
abstract = {In this paper, we propose a novel teacher-student learning framework for the preprocessing of a speech recognizer, leveraging the online noise tracking capabilities of improved minima controlled recursive averaging IMCRA and deep learning of nonlinear interactions between speech and noise. First, a teacher model with deep architectures is built to learn the target of ideal ratio masks IRMs using simulated training pairs of clean and noisy speech data. Next, a student model is trained to learn an improved speech presence probability by incorporating the estimated IRMs from the teacher model into the IMCRA approach. The student model can be compactly designed in a causal processing mode having no latency with the guidance of a complex and noncausal teacher model. Moreover, the clean speech requirement, which is difficult to meet in real-world adverse environments, can be relaxed for training the student model, implying that noisy speech data can be directly used to adapt the regression-based enhancement model to further improve speech recognition accuracies for noisy speech collected in such conditions. Experiments on the CHiME-4 challenge task show that our best student model with bidirectional gated recurrent units BGRUs can achieve a relative word error rate WER reduction of 18.85% for the real test set when compared to unprocessed system without acoustic model retraining. However, the traditional teacher model degrades the performance of the unprocessed system in this case. In addition, the student model with a deep neural network DNN in causal mode having no latency yields a relative WER reduction of 7.94% over the unprocessed system with 670 times less computing cycles when compared to the BGRU-equipped student model. Finally, the conventional speech enhancement and IRM-based deep learning method destroyed the ASR performance when the recognition system became more powerful. While our proposed approach could still improve the ASR performance even in the more powerful recognition system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2080–2091},
numpages = {12}
}

@article{10.1109/TASLP.2019.2933326,
author = {Deng, Dong and Jing, Liping and Yu, Jian and Sun, Shaolong},
title = {Sparse Self-Attention LSTM for Sentiment Lexicon Construction},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2933326},
doi = {10.1109/TASLP.2019.2933326},
abstract = {Sentiment lexicon is a very important resource for opinion mining. Recently, many state-of-the-art works employ deep learning techniques to construct sentiment lexicons. In general, they firstly learn sentiment-aware word embeddings, and then use it as word features to construct sentiment lexicons. However, these methods do not consider the importance of each word to the distinguish of documents’ sentiment polarities. As we know, most words among a document do not contribute to understand documents’ semantic or sentiment. For example, in the tweet&nbsp;It's a good day, but i can't feel it. I'm really unhappy. The words ‘unhappy’, ‘feel’ and ‘can't’ are much more important than the words ‘good’, ‘day’ in predicting the sentiment polarity of this twitter. Meanwhile, many words, such as ‘the’, ‘in’, ‘it’ and ‘I'm’ are uninformative. In this paper, we propose a novel sparse self-attention LSTM&nbsp;SSALSTM to efficiently capture the above intuitive facts, and then construct a large scale sentiment lexicons in twitter. In SSALSTM, we use a novel self-attention mechanism to capture the importance of each words to the distinguish of documents’ sentiment polarities. In addition, a $L_1$ regularize is applied in the attentions which can ensure the sparsity characters that most words in a document are semantic and sentiment indistinguishable. Once we learn an efficient sentiment-aware word embedding, we train a classifier which uses sentiment-aware word embedding as features to predict the sentiment polarities of words. Extensive experiments on four publicly available datasets, SemEval 2013–2016, indicate that the sentiment lexicon generated by our proposed model achieves state-of-the-art performance on both supervised and unsupervised sentiment classification tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1777–1790},
numpages = {14}
}

@article{10.1109/TASLP.2019.2933698,
author = {Elshamy, Samy and Fingscheidt, Tim},
title = {DNN-Based Cepstral Excitation Manipulation for Speech Enhancement},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2933698},
doi = {10.1109/TASLP.2019.2933698},
abstract = {This contribution aims at speech model-based speech enhancement by exploiting the source-filter model of human speech production. The proposed method enhances the excitation signal in the cepstral domain by making use of a deep neural network DNN. We investigate two types of target representations along with the significant effects of their normalization. The new approach exceeds the performance of a formerly introduced classical signal processing-based cepstral excitation manipulation CEM method in terms of noise attenuation by about 1.5&nbsp;dB. We show that this gain also holds true when comparing serial combinations of envelope and excitation enhancement. In the important low-SNR conditions, no significant trade-off for speech component quality or speech intelligibility is induced, while allowing for substantially higher noise attenuation. In total, a traditional purely statistical state-of-the-art speech enhancement system is outperformed by more than 3&nbsp;dB noise attenuation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1803–1814},
numpages = {12}
}

@article{10.1109/TASLP.2019.2934319,
author = {Delfarah, Masood and Wang, DeLiang},
title = {Deep Learning for Talker-Dependent Reverberant Speaker Separation: An Empirical Study},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2934319},
doi = {10.1109/TASLP.2019.2934319},
abstract = {Speaker separation refers to the problem of separating speech signals from a mixture of simultaneous speakers. Previous studies are limited to addressing the speaker separation problem in anechoic conditions. This paper addresses the problem of talker-dependent speaker separation in reverberant conditions, which are characteristic of real-world environments. We employ recurrent neural networks with bidirectional long short-term memory BLSTM to separate and dereverberate the target speech signal. We propose two-stage networks to effectively deal with both speaker separation and speech dereverberation. In the two-stage model, the first stage separates and dereverberates two-talker mixtures and the second stage further enhances the separated target signal. We have extensively evaluated the two-stage architecture, and our empirical results demonstrate large improvements over unprocessed mixtures and clear performance gain over single-stage networks in a wide range of target-to-interferer ratios and reverberation times in simulated as well as recorded rooms. Moreover, we show that time-frequency masking yields better performance than spectral mapping for reverberant speaker separation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1839–1848},
numpages = {10}
}

@article{10.1109/TASLP.2019.2925934,
author = {Xie, Yue and Liang, Ruiyu and Liang, Zhenlin and Huang, Chengwei and Zou, Cairong and Schuller, Bjorn},
title = {Speech Emotion Classification Using Attention-Based LSTM},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2925934},
doi = {10.1109/TASLP.2019.2925934},
abstract = {Automatic speech emotion recognition has been a research hotspot in the field of human–computer interaction over the past decade. However, due to the lack of research on the inherent temporal relationship of the speech waveform, the current recognition accuracy needs improvement. To make full use of the difference of emotional saturation between time frames, a novel method is proposed for speech recognition using frame-level speech features combined with attention-based long short-term memory LSTM recurrent neural networks. Frame-level speech features were extracted from waveform to replace traditional statistical features, which could preserve the timing relations in the original speech through the sequence of frames. To distinguish emotional saturation in different frames, two improvement strategies are proposed for LSTM based on the attention mechanism: first, the algorithm reduces the computational complexity by modifying the forgetting gate of traditional LSTM without sacrificing performance and second, in the final output of the LSTM, an attention mechanism is applied to both the time and the feature dimension to obtain the information related to the task, rather than using the output from the last iteration of the traditional algorithm. Extensive experiments on the CASIA, eNTERFACE, and GEMEP emotion corpora demonstrate that the performance of the proposed approach is able to outperform the state-of-the-art algorithms reported to date.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1675–1685},
numpages = {11}
}

@article{10.1109/TASLP.2019.2922537,
author = {Zhang, Zhuosheng and Zhao, Hai and Ling, Kangwei and Li, Jiangtong and Li, Zuchao and He, Shexia and Fu, Guohong},
title = {Effective Subword Segmentation for Text Comprehension},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2922537},
doi = {10.1109/TASLP.2019.2922537},
abstract = {Representation learning is the foundation of machine reading comprehension and inference. In state-of-the-art models, character-level representations have been broadly adopted to alleviate the problem of effectively representing rare or complex words. However, character itself is not a natural minimal linguistic unit for representation or word embedding composing due to ignoring the linguistic coherence of consecutive characters inside word. This paper presents a general subword-augmented embedding framework for learning and composing computationally derived subword-level representations. We survey a series of unsupervised segmentation methods for subword acquisition and different subword-augmented strategies for text understanding, showing that subword-augmented embedding significantly improves our baselines in various types of text understanding tasks on both English and Chinese benchmarks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1664–1674},
numpages = {11}
}

@article{10.1109/TASLP.2019.2928140,
author = {Lu, Rui and Duan, Zhiyao and Zhang, Changshui},
title = {Audio–Visual Deep Clustering for Speech Separation},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2928140},
doi = {10.1109/TASLP.2019.2928140},
abstract = {Speech separation aims to separate individual voices from an audio mixture of multiple simultaneous talkers. Audio-only approaches show unsatisfactory performance when the speakers are of the same gender or share similar voice characteristics. This is due to challenges on learning appropriate feature representations for separating voices in single frames and streaming voices across time. Visual signals of speech e.g., lip movements, if available, can be leveraged to learn better feature representations for separation. In this paper, we propose a novel audio–visual deep clustering model AVDC to integrate visual information into the process of learning better feature representations embeddings for Time–Frequency T–F bin clustering. It employs a two-stage audio–visual fusion strategy where speaker-wise audio–visual T–F embeddings are first computed after the first-stage fusion to model the audio–visual correspondence for each speaker. In the second-stage fusion, audio–visual embeddings of all speakers and audio embeddings calculated by deep clustering from the audio mixture are concatenated to form the final T–F embedding for clustering. Through a series of experiments, the proposed AVDC model is shown to outperform the audio-only deep clustering and utterance-level permutation invariant training baselines and three other state-of-the-art audio–visual approaches. Further analyses show that the AVDC model learns a better T–F embedding for alleviating the source permutation problem across frames. Other experiments show that the AVDC model is able to generalize across different numbers of speakers between training and testing and shows some robustness when visual information is partially missing.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1697–1712},
numpages = {16}
}

@article{10.1109/TASLP.2019.2928128,
author = {Wang, Shuai and Huang, Zili and Qian, Yanmin and Yu, Kai},
title = {Discriminative Neural Embedding Learning for Short-Duration Text-Independent Speaker Verification},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2928128},
doi = {10.1109/TASLP.2019.2928128},
abstract = {Short duration text-independent speaker verification remains a hot research topic in recent years, and deep neural network based embeddings have shown impressive results in such conditions. Good speaker embeddings require the property of both small intra-class variation and large inter-class difference, which is critical for the ability of discrimination and generalization. Current embedding learning strategies can be grouped into two frameworks: “Cascade embedding learning” with multiple stages and “direct embedding learning” from spectral feature directly. We propose new approaches to achieve more discriminant speaker embeddings. Within the cascade framework, a neural network based deep discriminant analysis DDA is proposed to project i-vector to more discriminative embeddings. Within the direct embedding framework, a deep model with more advanced center loss and A-softmax loss is used, the focal loss is also investigated in this framework. Moreover, the traditional i-vector and neural embeddings are finally combined with neural network based DDA to achieve further gain. Main experiments are carried out on a short-duration text-independent speaker verification dataset generated from the SRE corpus. The results show that the newly proposed method is promising for short-duration text-independent speaker verification, and it is consistently better than traditional i-vector and neural embedding baselines. The best embeddings achieve roughly 30% relative EER reduction compared to the i-vector baseline, which could be further enhanced when combined with the i-vector system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1686–1696},
numpages = {11}
}

@article{10.1109/TASLP.2019.2931759,
author = {Bernardini, Alberto and Maffezzoni, Paolo and Sarti, Augusto},
title = {Linear Multistep Discretization Methods With Variable Step-Size in Nonlinear Wave Digital Structures for Virtual Analog Modeling},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2931759},
doi = {10.1109/TASLP.2019.2931759},
abstract = {There is a growing interest in Virtual Analog modeling algorithms for musical audio processing designed in the Wave Digital WD domain. Such algorithms typically employ a discretization strategy based on the trapezoidal rule with fixed sampling step, though this is not the only option. In fact, alternative discretization strategies possibly with an adaptive sampling step can be quite advantageous, particularly when dealing with nonlinear systems characterized by stiff equations. In this paper, we propose a unified approach for modeling capacitors and inductors in the WD domain using generic linear multi-step discretization methods with variable time-step size, and provide generalized adaptation conditions. We also show that the proposed approach for implementing dynamic energy-storing elements in the WD domain is particularly suitable to be combined with a recently developed technique for efficiently solving a class of circuits with multiple one-port nonlinearities, called Scattering Iterative Method. Finally, as examples of application, we develop WD models for a Van Der Pol oscillator and a dynamic diode-based ring modulator, which use different discretization methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1763–1776},
numpages = {14}
}

@article{10.1109/TASLP.2019.2930913,
author = {Kong, Qiuqiang and Yu, Changsong and Xu, Yong and Iqbal, Turab and Wang, Wenwu and Plumbley, Mark D.},
title = {Weakly Labelled AudioSet Tagging With Attention Neural Networks},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2930913},
doi = {10.1109/TASLP.2019.2930913},
abstract = {Audio tagging is the task of predicting the presence or absence of sound classes within an audio clip. Previous work in audio tagging focused on relatively small datasets limited to recognizing a small number of sound classes. We investigate audio tagging on AudioSet, which is a dataset consisting of over 2 million audio clips and 527 classes. AudioSet is weakly labelled, in that only the presence or absence of sound classes is known for each clip, whereas the onset and offset times are unknown. To address the weakly labelled audio tagging problem, we propose attention neural networks as a way to attend the most salient parts of an audio clip. We bridge the connection between attention neural networks and multiple instance learning MIL methods, and propose decision-level and feature-level attention neural networks for audio tagging. We investigate attention neural networks modeled by different functions, depths, and widths. Experiments on AudioSet show that the feature-level attention neural network achieves a state-of-the-art mean average precision of 0.369, outperforming the best MIL method of 0.317 and Google's deep neural network baseline of 0.314. In addition, we discover that the audio tagging performance on AudioSet-embedding features has a weak correlation with the number of training samples and the quality of labels of each sound class.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1791–1802},
numpages = {12}
}

@article{10.1109/TASLP.2019.2929647,
author = {Parshakova, Tetiana and Rameau, Francois and Serdega, Andriy and Kweon, In So and Kim, Dae-Shik},
title = {Latent Question Interpretation Through Variational Adaptation},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2929647},
doi = {10.1109/TASLP.2019.2929647},
abstract = {Most artificial neural network models for question-answering rely on complex attention mechanisms. These techniques demonstrate high performance on existing datasets; however, they are limited in their ability to capture natural language variability, and to generate diverse relevant answers. To address this limitation, we propose a model that learns multiple interpretations of a given question. This diversity is ensured by our interpretation policy module which automatically adapts the parameters of a question-answering model with respect to a discrete latent variable. This variable follows the distribution of interpretations learned by the interpretation policy through a semi-supervised variational inference framework. To boost the performance further, the resulting policy is fine-tuned using the rewards from the answer accuracy with a policy gradient. We demonstrate the relevance and efficiency of our model through a large panel of experiments. Qualitative results, in particular, underline the ability of the proposed architecture to discover multiple interpretations of a question. When tested using the Stanford Question Answering Dataset 1.1, our model outperforms the baseline methods in finding multiple and diverse answers. To assess our strategy from a human standpoint, we also conduct a large-scale user study. This study highlights the ability of our network to produce diverse and coherent answers compared to existing approaches. Our Pytorch implementation is available as open source.11github.com/parshakova/APIP.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1713–1724},
numpages = {12}
}

@article{10.1109/TASLP.2019.2933146,
author = {Sun, Sining and Guo, Pengcheng and Xie, Lei and Hwang, Mei-Yuh},
title = {Adversarial Regularization for Attention Based End-to-End Robust Speech Recognition},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2933146},
doi = {10.1109/TASLP.2019.2933146},
abstract = {End-to-end speech recognition, such as attention based approaches, is an emerging and attractive topic in recent years. It has achieved comparable performance with the traditional speech recognition framework. Because end-to-end approaches integrate acoustic and linguistic information into one model, the perturbation in the acoustic level such as acoustic noise, could be easily propagated to the linguistic level. Thus improving model robustness in real application environments for these end-to-end systems is crucial. In this paper, in order to make the attention based end-to-end model more robust against noises, we formulate regulation of the objective function with adversarial training examples. Particularly two adversarial regularization techniques, the fast gradient-sign method and the local distributional smoothness method, are explored to improve noise robustness. Experiments on two publicly available Chinese Mandarin corpora, AISHELL-1 and AISHELL-2, show that adversarial regularization is an effective approach to improve robustness against noises for our attention-based models. Specifically, we obtained 18.4% relative character error rate CER reduction on the AISHELL-1 noisy test set. Even on the clean test set, we showed 16.7% relative improvement. As the training set increases and covers more environmental varieties, our proposed methods remain effective despite that the improvement shrinks. Training on the large AISHELL-2 training corpus and testing on the various AISHELL-2 test sets, we achieved 7.0%–12.2% relative error rate reduction. To our knowledge, this is the first successful application of adversarial regularization to sequence-to-sequence speech recognition systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1826–1838},
numpages = {13}
}

@article{10.1109/TASLP.2019.2928143,
author = {Maghsoodi, Nooshin and Sameti, Hossein and Zeinali, Hossein and Stafylakis, Themos},
title = {Speaker Recognition With Random Digit Strings Using Uncertainty Normalized HMM-Based i-Vectors},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2928143},
doi = {10.1109/TASLP.2019.2928143},
abstract = {In this paper, we combine Hidden Markov Models HMMs with i-vector extractors to address the problem of text-dependent speaker recognition with random digit strings. We employ digit-specific HMMs to segment the utterances into digits, to perform frame alignment to HMM states and to extract Baum-Welch statistics. By making use of the natural partition of input features into digits, we train digit-specific i-vector extractors on top of each HMM and we extract well-localized i-vectors, each modelling merely the phonetic content corresponding to a single digit. We then examine ways to perform channel and uncertainty compensation, and we propose a novel method for using the uncertainty in the i-vector estimates. The experiments on RSR2015 part III show that the proposed method attains 1.52% and 1.77% Equal Error Rate EER for male and female respectively, outperforming state-of-the-art methods such as x-vectors, trained on vast amounts of data. Furthermore, these results are attained by a single system trained entirely on RSR2015, and by a simple score-normalized cosine distance. Moreover, we show that the omission of channel compensation yields only a minor degradation in performance, meaning that the system attains state-of-the-art results even without recordings from multiple handsets per speaker for training or enrolment. Similar conclusions are drawn from our experiments on the RedDots corpus, where the same method is evaluated on phrases. Finally, we report results with bottleneck features and show that further improvement is attained when fusing them with spectral features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1815–1825},
numpages = {11}
}

@article{10.1109/TASLP.2019.2930917,
author = {Shi, Liming and Nielsen, Jesper Kjaer and Jensen, Jesper Rindom and Little, Max A. and Christensen, Mads Graesboll},
title = {Robust Bayesian Pitch Tracking Based on the Harmonic Model},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2930917},
doi = {10.1109/TASLP.2019.2930917},
abstract = {Fundamental frequency is one of the most important characteristics of speech and audio signals. Harmonic model-based fundamental frequency estimators offer a higher estimation accuracy and robustness against noise than the widely used autocorrelation-based methods. However, the traditional harmonic model-based estimators do not take the temporal smoothness of the fundamental frequency, the model order, and the voicing into account as they process each data segment independently. In this paper, a fully Bayesian fundamental frequency tracking algorithm based on the harmonic model and a first-order Markov process model is proposed. Smoothness priors are imposed on the fundamental frequencies, model orders, and voicing using first-order Markov process models. Using these Markov models, fundamental frequency estimation and voicing detection errors can be reduced. Using the harmonic model, the proposed fundamental frequency tracker has an improved robustness to noise. An analytical form of the likelihood function, which can be computed efficiently, is derived. Compared to the state-of-the-art neural network and nonparametric approaches, the proposed fundamental frequency tracking algorithm has superior performance in almost all investigated scenarios, especially in noisy conditions. For example, under 0 dB white Gaussian noise, the proposed algorithm reduces the mean absolute errors and gross errors by 15% and 20% on the Keele pitch database and 36% and 26% on sustained /a/ sounds from a database of Parkinson's disease voices. A MATLAB version of the proposed algorithm is made freely available for reproduction of the results.11An implementation of the proposed algorithm using MATLAB may be found in https://tinyurl.com/yxn4a543.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1737–1751},
numpages = {15}
}

@article{10.1109/TASLP.2019.2930914,
author = {Yang, Yan and Bao, Changchun},
title = {RS-CAE-Based AR-Wiener Filtering and Harmonic Recovery for Speech Enhancement},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2930914},
doi = {10.1109/TASLP.2019.2930914},
abstract = {By taking into account temporal correlation of speech feature. In this paper, a novel structure of convolutional Auto Encoder CAE was proposed. In this structure, the historical output of the CAE was fed into a CAE stack recurrently. We name this structure as Recurrent Stack Convolutional Auto Encoder RS-CAE. In the training stage, the training feature maps of the RS-CAE comprise of log power spectrum LPS of noisy speech and an additional feature map derived from the LPS of the enhanced speech in the history. In this way, the temporal correlation is incorporated as much as possible in the RS-CAE. The training target is a concatenated vector of auto-regressive AR model parameters of speech and noise. At online stage, the LPS of noisy speech and the LPS of the enhanced speech from the history make up input feature maps together. The outputs of the RS-CAE are the AR model parameters of speech and noise, which are used to construct the AR-Wiener filter. Because the estimated AR model parameters are not completely accurate and some harmonics may be lost in the enhanced speech, the codebook-based harmonic recovery technique was proposed to reconstruct harmonic structure of the enhanced speech. The test results confirmed that the proposed method achieved better performance compared with some existing approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1752–1762},
numpages = {11}
}

@article{10.1109/TASLP.2019.2929859,
author = {Wong, Jeremy Heng Meng and Gales, Mark John Francis and Wang, Yu},
title = {General Sequence Teacher–Student Learning},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2929859},
doi = {10.1109/TASLP.2019.2929859},
abstract = {In automatic speech recognition, performance gains can often be obtained by combining an ensemble of multiple models. However, this can be computationally expensive when performing recognition. Teacher–student learning alleviates this cost by training a single student model to emulate the combined ensemble behaviour. Only this student needs to be used for recognition. Previously investigated teacher–student criteria often limit the forms of diversity allowed in the ensemble, and only propagate information from the teachers to the student at the frame level. This paper addresses both of these issues by examining teacher–student learning within a sequence-level framework, and assessing the flexibility that these approaches offer. Various sequence-level teacher–student criteria are examined in this work, to propagate sequence posterior information. A training criterion based on the Kullback–Leibler KL-divergence between context-dependent state sequence posteriors is proposed that allows for a diversity of state cluster sets to be present in the ensemble. This criterion is shown to be an upper bound to a more general KL-divergence between word sequence posteriors, which places even fewer restrictions on the ensemble diversity, but whose gradient can be expensive to compute. These methods are evaluated on the augmented multi-party interaction AMI meeting transcription and MGB-3 television broadcast audio tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1725–1736},
numpages = {12}
}

@article{10.1109/TASLP.2019.2921423,
author = {Wang, Yijun and Xia, Yingce and Zhao, Li and Bian, Jiang and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
title = {Semi-Supervised Neural Machine Translation via Marginal Distribution Estimation},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2921423},
doi = {10.1109/TASLP.2019.2921423},
abstract = {Neural machine translation NMT heavily relies on parallel bilingual corpora for training. Since large-scale, high-quality parallel corpora are usually costly to collect, it is appealing to exploit monolingual corpora to improve NMT. Inspired by the law of total probability, which connects the probability of a given target-side monolingual sentence to the conditional probability of translating from a source sentence to the target one, we propose to explicitly exploit this connection and help the training procedure of NMT models using monolingual data. The key technical challenge of this approach is that there are exponentially many source sentences for a target monolingual sentence while computing the sum of the conditional probability given each possible source sentence. We address this challenge by leveraging the reverse translation model target-to-source translation model to sample several mostly likely source-side sentences and avoid enumerating all possible candidate source sentences. Then we propose two different methods to leverage the law of total probability, including marginal distribution regularization and likelihood maximization of monolingual corpora. Experiment results on English$rightarrow$French and German$rightarrow$English tasks demonstrate that our methods achieve significant improvement over several strong baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1564–1576},
numpages = {13}
}

@article{10.1109/TASLP.2019.2922845,
author = {Li, Pairui and Chen, Chuan and Zheng, Wujie and Deng, Yuetang and Ye, Fanghua and Zheng, Zibin},
title = {STD: An Automatic Evaluation Metric for Machine Translation Based on Word Embeddings},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2922845},
doi = {10.1109/TASLP.2019.2922845},
abstract = {Lexical-based metrics such as BLEU, NIST, and WER have been widely used in machine translation MT evaluation. However, these metrics badly represent semantic relationships and impose strict identity matching, leading to moderate correlation with human judgments. In this paper, we propose a novel MT automatic evaluation metric Semantic Travel Distance STD based on word embeddings. STD incorporates both semantic and lexical features word embeddings and n-gram and word order into one metric. It measures the semantic distance between the hypothesis and reference by calculating the minimum cumulative cost that the embedded n-grams of the hypothesis need to “travel” to reach the embedded n-grams of the reference. Experiment results show that STD has a better and more robust performance than a range of state-of-the-art metrics for both the segment-level and system-level evaluation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1497–1506},
numpages = {10}
}

@article{10.1109/TASLP.2019.2926754,
author = {Zhao, Guanlong and Gutierrez-Osuna, Ricardo},
title = {Using Phonetic Posteriorgram Based Frame Pairing for Segmental Accent Conversion},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2926754},
doi = {10.1109/TASLP.2019.2926754},
abstract = {Accent conversion AC aims to transform non-native utterances to sound as if the speaker had a native accent. This can be achieved by mapping source speech spectra from a native speaker into the acoustic space of the target non-native speaker. In prior work, we proposed an AC approach that matches frames between the two speakers based on their acoustic similarity after compensating for differences in vocal tract length. In this paper, we propose a new approach that matches frames between the two speakers based on their phonetic rather than acoustic similarity. Namely, we map frames from the two speakers into a phonetic posteriorgram using speaker-independent acoustic models trained on native speech. We thoroughly evaluate the approach on a speech corpus containing multiple native and non-native speakers. The proposed algorithm outperforms the prior approach, improving ratings of acoustic quality 22% increase in mean opinion score and native accent 69% preference while retaining the voice quality of the non-native speaker. Furthermore, we show that the approach can be used in the reverse conversion direction, i.e., generating speech with a native speaker's voice quality and a non-native accent. Finally, we show that this approach can be applied to non-parallel training data, achieving the same accent conversion performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1649–1660},
numpages = {12}
}

@article{10.1109/TASLP.2019.2923969,
author = {Park, Jihwan and Chang, Joon-Hyuk},
title = {State-Space Microphone Array Nonlinear Acoustic Echo Cancellation Using Multi-Microphone Near-End Speech Covariance},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2923969},
doi = {10.1109/TASLP.2019.2923969},
abstract = {Nonlinear acoustic echo cancellation AEC is a highly challenging task in a single-microphone; hence, the AEC technique with a microphone array has also been considered to more effectively reduce the residual echo. However, these algorithms track only a linear acoustic path between the loudspeaker and the microphone array. This study proposes a microphone array form of the single-microphone nonlinear AEC NAEC algorithm in the reverberant condition. We extend a single-microphone-based model of the nonlinear acoustic echo to the microphone array case and propose the modeling of the acoustic transfer function ATF vector extended with a power series using a state-space equation. The Kalman filter is also adapted to optimally and recursively estimate the ATF vector. Furthermore, low-rank approximation and multi-microphone Wiener filtering are applied to estimate the multi-microphone near-end speech covariance, which results in the microphone array NAEC algorithm showing a consistently outstanding performance under severe signal-to-echo ratio SER and highly reverberant conditions. Consequently, our approach outperforms conventional methods regarding echo reduction and near-end speech quality for a wide range of SER and reverberation conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1520–1534},
numpages = {15}
}

@article{10.1109/TASLP.2019.2923951,
author = {Luo, Zhaojie and Chen, Jinhui and Takiguchi, Tetsuya and Ariki, Yasuo},
title = {Emotional Voice Conversion Using Dual Supervised Adversarial Networks With Continuous Wavelet Transform F0 Features},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2923951},
doi = {10.1109/TASLP.2019.2923951},
abstract = {In emotional voice conversion VC tasks, it is difficult to deal with a simple representation of fundamental frequency F0, which is the most important feature in emotional voice representation. In order to address this issue, we propose the adaptive scales continuous wavelet transform ADS-CWT method to systematically capture F0 features of different temporal levels, which can represent different prosodic aspects, ranging from micro-prosody to sentences. Moreover, in an emotional VC task, each dataset is paired with the labeled emotional voice and neutral voice, which can be regarded as a dual task. Owing to, first, dual supervised learning's ability to improve the training performances by using the leveraging probabilistic connection between the dual tasks to enhance the learning from labeled data and, second, generative adversarial networks’ GANs’ ability to mitigate the over-smoothing problem caused in the low-level data space when converting the acoustic features, we further present a novel training framework for emotional VC using GANs combined with dual supervised learning, named as dual supervised adversarial networks. In emotional VC experiments, we confirmed the high similarity performance of our method when using limited labeled data for emotional VC. Our method achieves good and consistent performance, in both objective and subjective evaluations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1535–1548},
numpages = {14}
}

@article{10.1109/TASLP.2019.2926125,
author = {Lan, Yunshi and Wang, Shuohang and Jiang, Jing},
title = {Knowledge Base Question Answering With a Matching-Aggregation Model and Question-Specific Contextual Relations},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2926125},
doi = {10.1109/TASLP.2019.2926125},
abstract = {Making use of knowledge bases to answer questions&nbsp;KBQA is a key direction in question answering systems. Researchers have developed a diverse range of methods to address this problem, but there are still some limitations with the existing methods. Specifically, the existing neural network-based methods for KBQA have not taken advantage of the recent “matching-aggregation” framework for the sequence matching, and when representing a candidate answer entity, they may not choose the most useful context of the candidate for matching. In this paper, we explore the use of a “matching-aggregation” framework to match candidate answers with questions. We further make use of question-specific contextual relations to enhance the representations of candidate answer entities. Our complete method is able to achieve state-of-the-art performance on two benchmark datasets: WebQuestions and SimpleQuestions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1629–1638},
numpages = {10}
}

@article{10.1109/TASLP.2019.2924842,
author = {Fontana, Federico and Bozzo, Enrico},
title = {Newton–Raphson Solution of Nonlinear Delay-Free Loop Filter Networks},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2924842},
doi = {10.1109/TASLP.2019.2924842},
abstract = {For their numerical properties and speed of convergence, Newton–Raphson methods are frequently used to compute nonlinear audio electronic circuit models in the digital domain. These methods are traditionally employed regardless of preliminary considerations about their applicability, primarily because of a lack of flexible mathematical tools making the convergence analysis an easy task. We define the basin delimiter, a tool that can be applied to the case when the nonlinear circuit is modeled by a delay-free loop network. This tool is derived from a known convergence theorem providing a sufficient condition for quadratic speed of convergence of the method. After substituting the nonlinear characteristics with equivalent linear filters that compute Newton–Raphson on the existing network, through the basin delimiter, we figure out constraints guaranteeing quadratic convergence speed in the diode clipper. Further application to a ring modulator circuit does not lead to comparably useful constraints for quadratic convergence; however, also in this circuit, the basin delimiter has a magnitude roughly proportional to the number of iterations needed by the solver to find a solution. Together, such case studies foster refinement and generalization of this tool as a speed predictor, with potential application to the design of virtual analogue systems for real-time digital audio effects.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1590–1600},
numpages = {11}
}

@article{10.1109/TASLP.2019.2925450,
author = {Makishima, Naoki and Mogami, Shinichi and Takamune, Norihiro and Kitamura, Daichi and Sumino, Hayato and Takamichi, Shinnosuke and Saruwatari, Hiroshi and Ono, Nobutaka},
title = {Independent Deeply Learned Matrix Analysis for Determined Audio Source Separation},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2925450},
doi = {10.1109/TASLP.2019.2925450},
abstract = {In this paper, we propose a new framework called independent deeply learned matrix analysis IDLMA, which unifies a deep neural network DNN and independence-based multichannel audio source separation. IDLMA utilizes both pretrained DNN source models and statistical independence between sources for the separation, where the time-frequency structures of each source are iteratively optimized by a DNN while enhancing the estimation accuracy of the spatial demixing filters. As the source generative model, we introduce a complex heavy-tailed distribution to improve the separation performance. In addition, we address a semi-supervised situation; namely, a solo-recorded audio dataset can be prepared for only one source in the mixture signal. To solve the limited-data problem, we propose an appropriate data augmentation method to adapt the DNN source models to the observed signal, which enables IDLMA to work even in the semi-supervised situation. Experiments are conducted using music signals with a training dataset in both supervised and semi-supervised situations. The results show the validity of the proposed method in terms of the separation accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1601–1615},
numpages = {15}
}

@article{10.1109/TASLP.2019.2924321,
author = {As'ad, Hala and Bouchard, Martin and Kamkar-Parsi, Homayoun},
title = {A Robust Target Linearly Constrained Minimum Variance Beamformer With Spatial Cues Preservation for Binaural Hearing Aids},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2924321},
doi = {10.1109/TASLP.2019.2924321},
abstract = {In this paper, a binaural beamforming algorithm for hearing aid applications is introduced. The beamforming algorithm is designed to be robust to some error in the estimate of the target speaker direction. The algorithm has two main components: a robust target linearly constrained minimum variance TLCMV algorithm based on imposing two constraints around the estimated direction of the target signal, and a post-processor to help with the preservation of binaural cues. The robust TLCMV provides a good level of noise reduction and low level of target distortion under realistic conditions. The post-processor enhances the beamformer abilities to preserve the binaural cues for both diffuse-like background noise and directional interferers competing speakers, while keeping a good level of noise reduction. The introduced algorithm does not require knowledge or estimation of the directional interferers’ directions nor the second-order statistics of noise-only components. The introduced algorithm requires an estimate of the target speaker direction, but it is designed to be robust to some deviation from the estimated direction. Compared with recently proposed state-of-the-art methods, comprehensive evaluations are performed under complex realistic acoustic scenarios generated in both anechoic and mildly reverberant environments, considering a mismatch between estimated and true sources direction of arrival. Mismatch between the anechoic propagation models used for the design of the beamformers and the mildly reverberant propagation models used to generate the simulated directional signals is also considered. The results illustrate the robustness of the proposed algorithm to such mismatches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1549–1563},
numpages = {15}
}

@article{10.1109/TASLP.2019.2925973,
author = {Bai, Xuefeng and Cao, Hailong and Chen, Kehai and Zhao, Tiejun},
title = {A Bilingual Adversarial Autoencoder for Unsupervised Bilingual Lexicon Induction},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2925973},
doi = {10.1109/TASLP.2019.2925973},
abstract = {Unsupervised bilingual lexicon induction aims to generate bilingual lexicons without any cross-lingual signals. Successfully solving this problem would benefit many downstream tasks, such as unsupervised machine translation and transfer learning. In this work, we propose an unsupervised framework, named bilingual adversarial autoencoder, which automatically generates bilingual lexicon for a pair of languages from their monolingual word embeddings. In contrast to existing frameworks which learn a direct cross-lingual mapping of word embeddings from the source language to the target language, we train two autoencoders jointly to transform the source and the target monolingual word embeddings into a shared embedding space, where a word and its translation are close to each other. In this way, we capture the cross-lingual features of word embeddings from different languages and use them to induce bilingual lexicons. By conducting extensive experiments across eight language pairs, we demonstrate that the proposed method significantly outperforms the existing adversarial methods and even achieves best-published results across most language pairs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1639–1648},
numpages = {10}
}

@article{10.1109/TASLP.2019.2923542,
author = {Zhang, Jie and Heusdens, Richard and Hendriks, Richard Christian},
title = {Relative Acoustic Transfer Function Estimation in Wireless Acoustic Sensor Networks},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2923542},
doi = {10.1109/TASLP.2019.2923542},
abstract = {In this paper, we present an algorithm to estimate the relative acoustic transfer function RTF of a target source in wireless acoustic sensor networks WASNs. Two well-known methods to estimate the RTF are the covariance subtraction CS method and the covariance whitening CW approach, the latter based on the generalized eigenvalue decomposition. Both methods depend on the use of the noisy correlation matrix, which, in practice, has to be estimated using limited and in WASNs quantized data. The bit rate and the fact that we use limited data records therefore directly affect the accuracy of the estimated RTFs. Therefore, we first theoretically analyze the estimation performance of the two approaches in terms of bit rate. Second, we propose a rate-distribution method by minimizing the power usage and constraining the expected estimation error for both RTF estimators. The optimal rate distributions are found by using convex optimization techniques. The model-based methods, however, are impractical due to the dependence on the true RTFs. We therefore further develop two greedy rate-distribution methods for both approaches. Finally, numerical simulations on synthetic data and real audio recordings show the superiority of the proposed approaches in power usage compared to uniform rate allocation. We find that in order to satisfy the same RTF estimation accuracy, the rate-distributed CW methods consume much less transmission energy than the CS-based methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1507–1519},
numpages = {13}
}

@article{10.1109/TASLP.2019.2924534,
author = {Prakash, Jeena J. and Murthy, Hema A.},
title = {Analysis of Inter-Pausal Units in Indian Languages and Its Application to Text-to-Speech Synthesis},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2924534},
doi = {10.1109/TASLP.2019.2924534},
abstract = {Lack of punctuation in Indian language text makes the analysis of phrases difficult. In this paper, inter-pausal units IPUs in read sentences are considered as phrases and are analyzed. A key observation from this analysis is that the length of the IPUs in read sentences follow uniformly across all languages a Gamma distribution. Additionally, an analysis of the scale and shape parameters suggest that these parameters are governed by the location of the IPU in an utterance. This information is used in text-to-speech TTS systems for four Indian languages leading to an improvement in naturalness. A novel IPU-based TTS system is proposed for better prosody modeling as well. A given text is parsed into IPUs, and an appropriate TTS system for different IPUs is used for synthesis. It is observed that there is a significant improvement in the naturalness of synthesized speech compared to that of a single TTS system being used for the entire sentence.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1616–1628},
numpages = {13}
}

@article{10.1109/TASLP.2019.2921890,
author = {Jati, Arindam and Georgiou, Panayiotis},
title = {Neural Predictive Coding Using Convolutional Neural Networks Toward Unsupervised Learning of Speaker Characteristics},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2921890},
doi = {10.1109/TASLP.2019.2921890},
abstract = {Learning speaker-specific features is vital in many applications like speaker recognition, diarization, and speech recognition. This paper provides a novel approach, we term neural predictive coding NPC, to learn speaker-specific characteristics in a completely unsupervised manner from large amounts of unlabeled training data that even contain many non-speech events and multi-speaker audio streams. The NPC framework exploits the proposed short-term active-speaker stationarity hypothesis which assumes two temporally close short speech segments belong to the same speaker, and thus a common representation that can encode the commonalities of both the segments, should capture the vocal characteristics of that speaker. We train a convolutional deep siamese network to produce “speaker embeddings” by learning to separate “same” versus “different” speaker pairs which are generated from an unlabeled data of audio streams. Two sets of experiments are done in different scenarios to evaluate the strength of NPC embeddings and compare with state-of-the-art in-domain supervised methods. First, two speaker identification experiments with different context lengths are performed in a scenario with comparatively limited within-speaker channel variability. NPC embeddings are found to perform the best at short duration experiment, and they provide complementary information to i-vectors for full utterance experiments. Second, a large-scale speaker verification task having a wide range of within-speaker channel variability is adopted as an upper-bound experiment where comparisons are drawn with in-domain supervised methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1577–1589},
numpages = {13}
}

@article{10.1109/TASLP.2019.2917232,
author = {Kameoka, Hirokazu and Kaneko, Takuhiro and Tanaka, Kou and Hojo, Nobukatsu},
title = {ACVAE-VC: Non-Parallel Voice Conversion With Auxiliary Classifier Variational Autoencoder},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2917232},
doi = {10.1109/TASLP.2019.2917232},
abstract = {This paper proposes a non-parallel voice conversion VC method using a variant of the conditional variational autoencoder VAE called an auxiliary classifier VAE. The proposed method has two key features. First, it adopts fully convolutional architectures to construct the encoder and decoder networks so that the networks can learn conversion rules that capture the time dependencies in the acoustic feature sequences of source and target speech. Second, it uses information-theoretic regularization for the model training to ensure that the information in the attribute class label will not be lost in the conversion process. With regular conditional VAEs, the encoder and decoder are free to ignore the attribute class label input. This can be problematic since in such a situation, the attribute class label will have little effect on controlling the voice characteristics of input speech at test time. Such situations can be avoided by introducing an auxiliary classifier and training the encoder and decoder so that the attribute classes of the decoder outputs are correctly predicted by the classifier. We also present several ways to convert the feature sequence of input speech using the trained encoder and decoder and compare them in terms of audio quality through objective and subjective evaluations. We confirmed experimentally that the proposed method outperformed baseline non-parallel VC systems and performed comparably to an open-source parallel VC system trained using a parallel corpus in a speaker identity conversion task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1432–1443},
numpages = {12}
}

@article{10.1109/TASLP.2019.2913499,
author = {Lee, Chia-Hsuan and Lee, Hung-yi and Wu, Szu-Lin and Liu, Chi-Liang and Fang, Wei and Hsu, Juei-Yang and Tseng, Bo-Hsiang},
title = {Machine Comprehension of Spoken Content: TOEFL Listening Test and Spoken SQuAD},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2913499},
doi = {10.1109/TASLP.2019.2913499},
abstract = {A user can scan through a text easily, but it is not the case for spoken content, because they cannot be directly displayed on-screen. As a result, accessing large collections of spoken content is much more difficult and time-consuming than doing so for the text content. It would therefore be helpful to develop machines that understand spoken content. In this paper, we propose two new tasks for machine comprehension of spoken content. The first is a listening comprehension test for TOEFL, a challenging academic English examination for English learners who are not the native English speakers. We show that the proposed model outperforms the naive approaches and other neural network based models by exploiting the hierarchical structures of natural languages and the selective power of attention mechanism. For the second listening comprehension task&nbsp;–&nbsp;spoken SQuAD&nbsp;–&nbsp;we find that speech recognition errors severely impair machine comprehension; we propose the use of subword units to mitigate the impact of these errors.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1469–1480},
numpages = {12}
}

@article{10.1109/TASLP.2019.2921892,
author = {Wang, Rui and Chen, Zhe and Yin, Fuliang Yin},
title = {DOA-Based Three-Dimensional Node Geometry Calibration in Acoustic Sensor Networks and Its Cram\'{e}R–Rao Bound and Sensitivity Analysis},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2921892},
doi = {10.1109/TASLP.2019.2921892},
abstract = {Acoustic sensor networks ASNs are widely applied in scenarios like teleconference, teaching, and theatre. ASNs can be used in tracking speakers, enhancing the speaker's speech and human–machine interactions, etc., but the geometric structure of the ASN has to be calibrated. ASN geometry calibration is a challenging task due to the irregular geometric structures of ASNs. A three-dimensional 3D node geometry calibration approach based on direction of arrival DOA measurements and artificial bee colony ABC algorithm is proposed in this paper. The theoretical DOAs of sound sources relative to nodes are first derived based on 3D rotation matrices and translation vectors, and the corresponding measured DOAs are estimated by the time-difference-of-arrival. Then, the node geometry calibration problem is formulated as the minimization of a cost function measuring the mismatch between theoretical and measured DOAs, and such non-convex minimization is effectively solved by the ABC algorithm. Next, Cram\'{e}r–Rao bound is presented to provide a theoretical lower bound for DOA-based node geometry calibration. Finally, the sensitivity of the proposed method to the sound source position error is discussed. The proposed method can calibrate node geometry positions successfully in both 2D plane and 3D space and requires no information transmission among nodes when the positions of few sound sources and the relative geometry of microphones in each node are known. Experimental results reveal the validity of the proposed node geometry calibration method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1455–1468},
numpages = {14}
}

@article{10.1109/TASLP.2019.2915922,
author = {Xu, Zhen and Sun, Chengjie and Long, Yinong and Liu, Bingquan and Wang, Baoxun and Wang, Mingjiang and Zhang, Min and Wang, Xiaolong},
title = {Dynamic Working Memory for Context-Aware Response Generation},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2915922},
doi = {10.1109/TASLP.2019.2915922},
abstract = {In human-to-human conversations, the context generally provides several backgrounds and strategic points for the following response. Therefore, many response generation approaches have explored the methodologies to incorporate the context into the encoder–decoder architecture, to generate context-aware responses that are remarkably relevant and cohesive to the given context. However, most approaches pay less attention to semantic interactions implicitly existing within contextual utterances, which are of great importance to capture semantic clues of the given dialog context, indeed. This paper proposes a dynamic working memory mechanism to model long-term semantic hints in the conversation context, by performing semantic interactions between utterances and updating context representation dynamically. Then, the outputs of the dynamic working memory are employed to provide helpful clues for the encoder–decoder architecture to generate responses to the given dialog. We have evaluated the proposed approach on Twitter Customer Service Corpus and OpenSubtitles Corpus, with several automatic evaluation metrics and the human evaluation, and the empirical results show the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1419–1431},
numpages = {13}
}

@article{10.1109/TASLP.2019.2919183,
author = {Li, Xiaofei and Girin, Laurent and Gannot, Sharon and Horaud, Radu},
title = {Multichannel Online Dereverberation Based on Spectral Magnitude Inverse Filtering},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2919183},
doi = {10.1109/TASLP.2019.2919183},
abstract = {This paper addresses the problem of multichannel online dereverberation. The proposed method is carried out in the short-time Fourier transform STFT domain, and for each frequency band independently. In the STFT domain, the time-domain room impulse response is approximately represented by the convolutive transfer function CTF. The multichannel CTFs are adaptively identified based on the cross-relation method, and using the recursive least square criterion. Instead of the complex-valued CTF convolution model, we use a nonnegative convolution model between the STFT magnitude of the source signal and the CTF magnitude, which is just a coarse approximation of the former model, but is shown to be more robust against the CTF perturbations. Based on this nonnegative model, we propose an online STFT magnitude inverse filtering method. The inverse filters of the CTF magnitude are formulated based on the multiple-input/output inverse theorem, and adaptively estimated based on the gradient descent criterion. Finally, the inverse filtering is applied to the STFT magnitude of the microphone signals, obtaining an estimate of the STFT magnitude of the source signal. Experiments regarding both speech enhancement and automatic speech recognition are conducted, which demonstrate that the proposed method can effectively suppress reverberation, even for the difficult case of a moving speaker.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1365–1377},
numpages = {13}
}

@article{10.1109/TASLP.2019.2922048,
author = {Chen, Xie and Liu, Xunying and Wang, Yu and Ragni, Anton and Wong, Jeremy H. M. and Gales, Mark J. F.},
title = {Exploiting Future Word Contexts in Neural Network Language Models for Speech Recognition},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2922048},
doi = {10.1109/TASLP.2019.2922048},
abstract = {Language modeling is a crucial component in a wide range of applications including speech recognition. Language models LMs are usually constructed by splitting a sentence into words and computing the probability of a word based on its word history. This sentence probability calculation, making use of conditional probability distributions, assumes that there is little impact from approximations used in the LMs, including the word history representations and finite training data. This motivates examining models that make use of additional information from the sentence. In this paper, future word information, in addition to the history, is used to predict the probability of the current word. For recurrent neural network LMs RNNLMs, this information can be encapsulated in a bi-directional model. However, if used directly, this form of model is computationally expensive when trained on large quantities of data, and can be problematic when used with word lattices. This paper proposes a novel neural network language model structure, the succeeding-word RNNLM, su-RNNLM, to address these issues. Instead of using a recurrent unit to capture the complete future word contexts, a feedforward unit is used to model a fixed finite number of succeeding words. This is more efficient in training than bi-directional models and can be applied to lattice rescoring. The generated lattices can be used for downstream applications, such as confusion network decoding and keyword search. Experimental results on speech recognition and keyword spotting tasks illustrate the empirical usefulness of future word information, and the flexibility of the proposed model to represent this information.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1444–1454},
numpages = {11}
}

@article{10.1109/TASLP.2019.2922832,
author = {Chen, Yi-Chen and Huang, Sung-Feng and Lee, Hung-yi and Wang, Yu-Hsuan and Shen, Chia-Hao},
title = {Audio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised Learning of Audio Segmentation and Representation},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2922832},
doi = {10.1109/TASLP.2019.2922832},
abstract = {In text, word2vec transforms each word into a fixed-size vector used as the basic component in applications of natural language processing. Given a large collection of unannotated audio, audio word2vec can also be trained in an unsupervised way using a sequence-to-sequence autoencoder SA. These vector representations are shown to effectively describe the sequential phonetic structures of the audio segments. In this paper, we further extend this research in the following two directions. First, we disentangle phonetic information and speaker information from the SA vector representations. Second, we extend audio word2vec from the word level to the utterance level by proposing a new segmental audio word2vec in which unsupervised spoken word boundary segmentation and audio word2vec are jointly learned and mutually enhanced, and utterances are directly represented as sequences of vectors carrying phonetic information. This is achieved by means of a segmental sequence-to-sequence autoencoder, in which a segmentation gate trained with reinforcement learning is inserted in the encoder.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1481–1493},
numpages = {13}
}

@article{10.1109/TASLP.2019.2919872,
author = {Chen, Lu and Chen, Zhi and Tan, Bowen and Long, Sishan and Gasic, Milica and Yu, Kai},
title = {AgentGraph: Toward Universal Dialogue Management With Structured Deep Reinforcement Learning},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2919872},
doi = {10.1109/TASLP.2019.2919872},
abstract = {Dialogue policy plays an important role in task-oriented spoken dialogue systems. It determines how to respond to users. The recently proposed deep reinforcement learning DRL approaches have been used for policy optimization. However, these deep models are still challenging for two reasons: first, many DRL-based policies are not sample efficient; and second, most models do not have the capability of policy transfer between different domains. In this paper, we propose a universal framework, AgentGraph, to tackle these two problems. The proposed AgentGraph is the combination of graph neural network GNN based architecture and DRL-based algorithm. It can be regarded as one of the multi-agent reinforcement learning approaches. Each agent corresponds to a node in a graph, which is defined according to the dialogue domain ontology. When making a decision, each agent can communicate with its neighbors on the graph. Under AgentGraph framework, we further propose dual GNN-based dialogue policy, which implicitly decomposes the decision in each turn into a high-level global decision and a low-level local decision. Experiments show that AgentGraph models significantly outperform traditional reinforcement learning approaches on most of the 18 tasks of the PyDial benchmark. Moreover, when transferred from the source task to a target task, these models not only have acceptable initial performance but also converge much faster on the target task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1378–1391},
numpages = {14}
}

@article{10.1109/TASLP.2019.2921151,
author = {Moller, Martin Bo and Nielsen, Jesper Kjaer and Fernandez-Grande, Efren and Olesen, Soren Krarup},
title = {On the Influence of Transfer Function Noise on Sound Zone Control in a Room},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2921151},
doi = {10.1109/TASLP.2019.2921151},
abstract = {Sound zones are valuable in scenarios where multiple people are present in the same room but want to listen to individual audio content without wearing headphones. The purpose of sound zone methods is to minimize the acoustic leakage between the zones by controlling multiple loudspeakers. This requires knowledge of how the loudspeakers interact with the room and radiate sound to the zones. That interaction is characterized by the transfer functions between the loudspeakers and microphones sampling the sound field in the zones. In this paper, the effect on the acoustic separation due to inherent noise in in situ transfer function measurements is investigated. The attainable separation is analyzed in the frequency range 20–300&nbsp;Hz by means of the eigenfunctions of a rectangular room. The concept of observable degrees of freedom is introduced to indicate the number of active eigenfunctions, which are different within the zones at a given frequency. Likewise, controllable degrees of freedom indicate whether each source can excite the active eigenfunctions independently. It is argued that high separation can be achieved when the observable degrees of freedom are fewer than the controllable, and the target sound field can be described by the observable degrees of freedom. However, to attain this high separation it is a requirement that the details in the transfer functions associated with these degrees of freedom can be resolved in the presence of the measurement noise. For both simulated and experimental conditions the transfer functions are estimated using Bayesian inference and the uncertainty in the estimates is used to automatically regularize the sound field control. This regularization is seen to improve the performance when the measurement noise is correlated between the microphones and have little effect when the noise is uncorrelated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1405–1418},
numpages = {14}
}

@article{10.1109/TASLP.2019.2918400,
author = {Ali, Randall and Bernardi, Giuliano and van Waterschoot, Toon and Moonen, Marc},
title = {Methods of Extending a Generalized Sidelobe Canceller With External Microphones},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2918400},
doi = {10.1109/TASLP.2019.2918400},
abstract = {While substantial noise reduction and speech enhancement can be achieved with multiple microphones organized in an array, in some cases, such as when the microphone spacings are quite close, it can also be quite limited. This degradation can, however, be resolved by the introduction of one or more external microphones $text{XM}$s into the same physical space as the local microphone array $text{LMA}$. In this paper, three methods of extending an $text{LMA}$-based generalized sidelobe canceller $text{GSC-LMA}$ with multiple $text{XM}$s are proposed in such a manner that the relative transfer function pertaining to the $text{LMA}$ is treated as a priori knowledge. Two of these methods involve a procedure for completing an extended blocking matrix, whereas the third uses the speech estimate from the $text{GSC-LMA}$ directly with an orthogonalized version of the $text{XM}$ signals to obtain an improved speech estimate via a rank-1 generalized eigenvalue decomposition. All three methods were evaluated with recorded data from an office room and it was found that the third method could offer the most improvement. It was also shown that in using this method, the speech estimate from the $text{GSC-LMA}$ was not compromised and would be available to the listener if so desired, along with the improved speech estimate that uses both the $text{LMA}$ and $text{XM}$s.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1349–1364},
numpages = {16}
}

@article{10.1109/TASLP.2019.2921726,
author = {Li, Luoqin and Wang, Jiabing and Li, Jichang and Ma, Qianli and Wei, Jia},
title = {Relation Classification via Keyword-Attentive Sentence Mechanism and Synthetic Stimulation Loss},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2921726},
doi = {10.1109/TASLP.2019.2921726},
abstract = {Previous studies have shown that attention mechanisms and shortest dependency paths have a positive effect on relation classification. In this paper, a keyword-attentive sentence mechanism is proposed to effectively combine the two methods. Furthermore, to effectively handle the imbalanced classification problem, this paper proposes a new loss function called the synthetic stimulation loss, which uses a modulating factor to allow the model to focus on hard-to-classify samples. The proposed two methods are integrated into a bidirectional gated recurrent unit BiGRU. As a single model is not strong in noise immunity, this paper applies the mutual learning method to our model and forces the networks to teach each other. Therefore, we call the final model SSL-KAS-MuBiGRU. Experiments on the SemEval-2010 Task 8 data set and the TAC40 data set demonstrate that the keyword-attentive sentence mechanism and synthetic stimulation loss are useful for relation classification, and our model achieves state-of-the-art results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1392–1404},
numpages = {13}
}

@article{10.1109/TASLP.2019.2918081,
author = {Pan, Chao and Chen, Jingdong and Benesty, Jacob and Shi, Guangming},
title = {On the Design of Target Beampatterns for Differential Microphone Arrays},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2918081},
doi = {10.1109/TASLP.2019.2918081},
abstract = {Differential microphone arrays DMAs have many interesting properties and have been widely used in acoustic, audio, and speech applications. A critical part of a DMA is the differential beamformer, which is generally designed in two important steps: 1 specifying a target beampattern based on what differential sound pressure field the DMA is expected to respond to and 2 designing the differential beamforming filter so that the resulting beampattern matches the target one. Most efforts in the study of DMAs so far have focused on the second step while choosing one of the limited patterns available in the literature as the target beampattern. Since it governs how the array performs, how to design the target beampattern is an important problem, which this paper addresses. The major contributions of this paper consists of the following four aspects. First, a positive superposition theorem is presented, which shows that the linear combination of effective beampatterns with non-negative coefficients is always an effective beampattern. Second, we propose a general approach to the design of target DMA beampatterns based on the positive superposition theorem. Third, an overview of the classical target beampatterns is provided and discussion is made on how to form effective base patterns. Fourth, we show that the smallest first null of a DMA is $pi /2N$ with $N$ being the DMA order, which provides the rule of setting nulls in practice. Finally, with examples, we show that with the use of the alternating-direction-method-of-multipliers algorithm, the proposed approach is able to generate useful DMA target beampatterns.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1295–1307},
numpages = {13}
}

@article{10.1109/TASLP.2019.2913091,
author = {Zhang, Teng and Wu, Ji},
title = {Constrained Learned Feature Extraction for Acoustic Scene Classification},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2913091},
doi = {10.1109/TASLP.2019.2913091},
abstract = {Deep neural networks DNNs have been proven to be powerful models for acoustic scene classification tasks. State-of-the-art DNNs have millions of connections and are computationally intensive, making them difficult to deploy on systems with limited resources. With a focus on acoustic scene classification, we describe a new learnable module, the simulated Fourier transform module, which allows deep neural networks to implement the discrete Fourier transform operation 8x faster on a graphics processing unit GPU. We frame the signal processing procedure as an adaptive machine learning problem and introduce learnable parameters in the module to facilitate fast adaptation for the complex and variable acoustic signal. This module gives neural networks the ability to model audio signals from raw waveforms, without extra fast Fourier transform and filter bank patches. Then, we use the temporal transformer module, which has been previously published, to alleviate the information loss caused by the simulated Fourier transform module. These techniques can be integrated into an existing fully connected neural network FCNN, convolutional neural network CNN, or recurrent neural network RNN models. We evaluate the proposed strategy using four acoustic scene datasets LITIS Rouen, DCASE2016, DCASE2017, and DCASE2018 as target tasks. We show that the proposed approach significantly outperforms the vanilla FCNN, CNN, and RNN approach on both efficiency and performance. For instance, the proposed approach can reduce inference time by 8x while reducing the classification error on LITIS Rouen dataset from 3.21% to 1.81%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1216–1228},
numpages = {13}
}

@article{10.1109/TASLP.2019.2915167,
author = {Luo, Yi and Mesgarani, Nima},
title = {Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2915167},
doi = {10.1109/TASLP.2019.2915167},
abstract = {Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time–frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time–frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully convolutional time-domain audio separation network Conv-TasNet, a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions masks to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network consisting of stacked one-dimensional dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time–frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time–frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study, therefore, represents a major step toward the realization of speech separation systems for real-world speech processing technologies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1256–1266},
numpages = {11}
}

@article{10.1109/TASLP.2019.2918404,
author = {Azmi, Aqil M. and Almutery, Manal N. and Aboalsamh, Hatim A.},
title = {Real-Word Errors in Arabic Texts: A Better Algorithm for Detection and Correction},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2918404},
doi = {10.1109/TASLP.2019.2918404},
abstract = {Real-word also known as semantic or context-sensitive spelling error is a class of error that escapes the typical spell checker which relies on dictionary look-up. This kind of error occurs when a user types a correctly spelled word–by mistake–when another is intended, e.g., “I want a peace piece of cake.” Further, these errors commonly arise in text written by people with dyslexia. Real-word errors are harder to detect as we need to consider the context. In this paper, we propose a spell checker that detects and corrects real-word errors for the Arabic language. Our method avoids predefined confusion sets–a simple approach used by many works tackling this problem–which limits the list of words that can be detected and corrected. Thus, our system can detect and correct a larger set of real-word errors. For the detection phase, we employ word and stem n-gram n = 1–3 language model along with machine learning, achieving a precision and recall of 83.5% and 99.2%, respectively. And for the correction phase we use n-gram, which results in an accuracy of 98%. Our scheme is robust, with an excellent performance even when the percentage of real-word error words is high. This makes the system suitable for handling errors in post OCR recognition of Arabic text.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1308–1320},
numpages = {13}
}

@article{10.1109/TASLP.2019.2915322,
author = {Sarkar, Achintya Kumar and Tan, Zheng-Hua and Tang, Hao and Shon, Suwon and Glass, James},
title = {Time-Contrastive Learning Based Deep Bottleneck Features for Text-Dependent Speaker Verification},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2915322},
doi = {10.1109/TASLP.2019.2915322},
abstract = {There are a number of studies about extraction of bottleneck BN features from deep neural networks DNNs trained to discriminate speakers, pass-phrases, and triphone states for improving the performance of text-dependent speaker verification TD-SV. However, a moderate success has been achieved. A recent study presented a time contrastive learning TCL concept to explore the non-stationarity of brain signals for classification of brain states. Speech signals have similar non-stationarity property, and TCL further has the advantage of having no need for labeled data. We therefore present a TCL based BN feature extraction method. The method uniformly partitions each speech utterance in a training dataset into a predefined number of multi-frame segments. Each segment in an utterance corresponds to one class, and class labels are shared across utterances. DNNs are then trained to discriminate all speech frames among the classes to exploit the temporal structure of speech. In addition, we propose a segment-based unsupervised clustering algorithm to re-assign class labels to the segments. TD-SV experiments were conducted on the RedDots challenge database. The TCL-DNNs were trained using speech data of fixed pass-phrases that were excluded from the TD-SV evaluation set, so the learned features can be considered phrase-independent. We compare the performance of the proposed TCL BN feature with those of short-time cepstral features and BN features extracted from DNNs discriminating speakers, pass-phrases, speaker+pass-phrase, as well as monophones whose labels and boundaries are generated by three different automatic speech recognition ASR systems. Experimental results show that the proposed TCL-BN outperforms cepstral features and speaker+pass-phrase discriminant BN features, and its performance is on par with those of ASR derived BN features. Moreover, the clustering method improves the TD-SV performance of TCL-BN and ASR derived BN features with respect to their standalone counterparts. We further study the TD-SV performance of fusing cepstral and BN features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1267–1279},
numpages = {13}
}

@article{10.1109/TASLP.2019.2914530,
author = {Gabrielli, Leonardo and Tomassetti, Stefano and Squartini, Stefano and Zinato, Carlo and Guaiana, Stefano},
title = {A Multi-Stage Algorithm for Acoustic Physical Model Parameters Estimation},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2914530},
doi = {10.1109/TASLP.2019.2914530},
abstract = {One of the challenges in computational acoustics is the identification of models that can simulate and predict the physical behavior of a system generating an acoustic signal. Whenever such models are used for commercial applications, an additional constraint is the time to market, making automation of the sound design process desirable. In previous works, a computational sound design approach has been proposed for the parameter estimation problem involving timbre matching by deep learning, which was applied to the synthesis of pipe organ tones. In this paper, we refine previous results by introducing the former approach in a multi-stage algorithm that also adds heuristics and a stochastic optimization method operating on perceptually motivated objective cost functions. The optimization method shows to be able to refine the first estimate given by the deep learning approach and substantially improve the objective metrics, with the additional benefit of reducing the sound design process time. Subjective listening tests are also conducted to gather additional insights on the results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1229–1240},
numpages = {12}
}

@article{10.1109/TASLP.2019.2918618,
author = {Korpusik, Mandy and Glass, James},
title = {Deep Learning for Database Mapping and Asking Clarification Questions in Dialogue Systems},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2918618},
doi = {10.1109/TASLP.2019.2918618},
abstract = {A dialogue system will often ask followup clarification questions when interacting with a user if the agent is unsure how to respond. In this new study, we explore deep reinforcement learning RL for asking followup questions when a user records a meal description, and the system needs to narrow down the options for which foods the person has eaten. We build off of prior work in which we use novel convolutional neural network models to bypass the standard feature engineering used in dialogue systems to handle the text mismatch between natural language user queries and structured database entries, demonstrating that our model learns semantically meaningful embedding representations of natural language. In this new nutrition domain, the followup clarification questions consist of possible attributes for each food that was consumed; for example, if the user drinks a cup of milk, the system should ask about the percent milkfat. We investigate an RL agent to dynamically follow up with the user, which we compare to rule-based and entropy-based methods. On a held-out test set, assuming the followup questions are answered correctly, deep RL significantly boosts top five food recall from 54.9% without followup to 89.0%. We also demonstrate that a hybrid RL model achieves the best perceived naturalness ratings in a human evaluation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1321–1334},
numpages = {14}
}

@article{10.1109/TASLP.2019.2916360,
author = {Chua, Jiawen and Kleijn, W. Bastiaan},
title = {A Low Latency Approach for Blind Source Separation},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2916360},
doi = {10.1109/TASLP.2019.2916360},
abstract = {We present a low latency approach for blind source separation BSS. BSS algorithms generally require a long window to estimate the demixing parameters. In traditional approaches, the long analysis window leads to a long algorithmic delay. Hence, traditional BSS approaches cannot be used in real-time systems. In contrast, our approach reduces the algorithmic delay independently of the window length used for estimation, while retaining separation performance. The new method exploits that the information about the sources provided by additional microphones can be traded against algorithmic delay. The method can be integrated with existing BSS algorithms and can be implemented in the time domain or in the time-frequency domain. Our experimental results confirm the effectiveness of our approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1280–1294},
numpages = {15}
}

@article{10.1109/TASLP.2019.2919378,
author = {Pak, Junhyeong and Shin, Jong Won},
title = {Sound Localization Based on Phase Difference Enhancement Using Deep Neural Networks},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2919378},
doi = {10.1109/TASLP.2019.2919378},
abstract = {The performance of most of the classical sound source localization algorithms degrades seriously in the presence of background noise or reverberation. Recently, deep neural networks DNNs have successfully been applied to sound source localization, which mainly aim to classify the direction-of-arrival DoA into one of the candidate sectors. In this paper, we propose a DNN-based phase difference enhancement for DoA estimation, which turned out to be better than the direct estimation of the DoAs from the input interchannel phase differences IPDs. The sinusoidal functions of the phase differences for “clean and dry” source signals are estimated from the sinusoidal functions of the IPDs for the input signals, which may include directional signals, diffuse noise, and reverberation. The resulted DoA is further refined to compensate for the estimation bias near the end-fire directions. From the enhanced IPDs, we can determine the DoA for each frequency bin and the DoAs for the current frame from the distributions of the DoAs for frequencies. Experimental results with various types and levels of background noise, reverberation times, numbers of sources, room impulse responses, and DoAs showed that the proposed method outperformed conventional approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1335–1345},
numpages = {11}
}

@article{10.1109/TASLP.2019.2915785,
author = {Yang, Bing and Liu, Hong and Pang, Cheng and Li, Xiaofei},
title = {Multiple Sound Source Counting and Localization Based on TF-Wise Spatial Spectrum Clustering},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2915785},
doi = {10.1109/TASLP.2019.2915785},
abstract = {This paper addresses the problem of multiple sound source counting and localization in adverse acoustic environments, using microphone array recordings. The proposed time-frequency TF wise spatial spectrum clustering based method contains two stages. First, given the received sensor signals, the spatial correlation matrix is computed and denoised in the TF domain. The TF-wise spatial spectrum is estimated based on the signal subspace information, and further enhanced by an exponential transform, which can increase the reliability of the source presence possibility reflected by spatial spectrum. Second, to jointly count and localize sound sources, the enhanced TF-wise spatial spectra are divided into several clusters with each cluster corresponding to one source. Sources are successively detected by searching the significant peaks of the remaining global spatial spectrum, which is formed using unassigned spatial spectra. After each new source detection, spatial spectra are reassigned to detected sources according to the dominance association between them. The interaction between sources is reduced by iteratively performing new source detection and spatial spectrum assignment. Experiments on both simulated data and real-world data demonstrate the superiority of the proposed method for multiple sound source counting and localization in the environment with different levels of noise and reverberation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1241–1255},
numpages = {15}
}

@article{10.1109/TASLP.2019.2911167,
author = {Koutrouvelis, Andreas I. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
title = {Robust Joint Estimation of Multimicrophone Signal Model Parameters},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Press},
volume = {27},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2911167},
doi = {10.1109/TASLP.2019.2911167},
abstract = {One of the biggest challenges in multimicrophone applications is the estimation of the parameters of the signal model, such as the power spectral densities PSDs of the sources, the early relative acoustic transfer functions of the sources with respect to the microphones, the PSD of late reverberation, and the PSDs of microphone-self noise. Typically, existing methods estimate subsets of the aforementioned parameters and assume some of the other parameters to be known a priori. This may result in inconsistencies and inaccurately estimated parameters and potential performance degradation in the applications using these estimated parameters. So far, there is no method to jointly estimate all the aforementioned parameters. In this paper, we propose a robust method for jointly estimating all the aforementioned parameters using confirmatory factor analysis. The estimation accuracy of the signal-model parameters thus obtained outperforms existing methods in most cases. We experimentally show significant performance gains in several multimicrophone applications over state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1136–1150},
numpages = {15}
}

@article{10.1109/TASLP.2019.2904850,
author = {Flesner, Jan-Hendrik and Biberger, Thomas and Ewert, Stephan D.},
title = {Subjective and Objective Assessment of Monaural and Binaural Aspects of Audio Quality},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Press},
volume = {27},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2904850},
doi = {10.1109/TASLP.2019.2904850},
abstract = {Recently, the binaural auditory-model-based quality prediction BAM-Q was successfully applied to predict binaural audio quality degradations, while the generalized power-spectrum model for quality GPSM$^text{q}$ has been demonstrated to account for a large variety of monaural signal distortions. For many applications, a combined monaural and binaural model would be advantageous, however, the contribution of monaural and binaural quality aspects to overall spatial quality is not conclusively clarified. Thus, the current study systematically investigated overall audio quality in a listening experiment for monaural and binaural distortions on music, speech, and noise, applied either in isolation or in combination. The resulting database was used for assessing different methods for combining BAM-Q and GPSM$^text{q}$ to joint overall audio predictions for monaural and binaural signal distortions. It was investigated, if monaural or binaural quality aspects contribute stronger to overall audio quality. The results indicate that overall audio quality depends on the lower quality aspect, either monaural or binaural.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1112–1125},
numpages = {14}
}

@article{10.1109/TASLP.2019.2911164,
author = {Yusuf, Bolaji and Gundogdu, Batuhan and Saraclar, Murat},
title = {Low Resource Keyword Search With Synthesized Crosslingual Exemplars},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Press},
volume = {27},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2911164},
doi = {10.1109/TASLP.2019.2911164},
abstract = {The transfer of acoustic data across languages has been shown to improve keyword search KWS performance in data-scarce settings. In this paper, we propose a way of performing this transfer that reduces the impact of the prevalence of out-of-vocabulary OOV terms on KWS in such a setting. We investigate a novel usage of multilingual features for KWS with very little training data in the target languages. The crux of our approach is the use of synthetic phone exemplars to convert the search into a query-by-example task, which we solve with the dynamic time warping algorithm. Using bottleneck features obtained from a network trained multilingually on a set of source languages, we train an extended distance metric learner EDML for four target languages from the IARPA Babel program which are distinct from the source languages. Compared with a baseline system that is based on automatic speech recognition ASR with a multilingual acoustic model, we observe an average term weighted value improvement of ${0.0603}$ absolute $text{74}%$ relative in a setting with only 1 h of training data in the target language. When the data scarcity is relaxed to 10&nbsp;h, we find that phone posteriors obtained by fine-tuning the multilingual network give better EDML systems. In this relaxed setting, the EDML systems still perform better than the baseline on OOV terms. Given their complementary natures, combining the EDML and the ASR-based baseline results in even further performance improvements in all settings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1126–1135},
numpages = {10}
}

@article{10.1109/TASLP.2019.2912123,
author = {Cauchi, Benjamin and Siedenburg, Kai and Santos, Joao F. and Falk, Tiago H. and Doclo, Simon and Goetze, Stefan},
title = {Non-Intrusive Speech Quality Prediction Using Modulation Energies and LSTM-Network},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Press},
volume = {27},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2912123},
doi = {10.1109/TASLP.2019.2912123},
abstract = {Many signal processing algorithms have been proposed to improve the quality of speech recorded in the presence of noise and reverberation. Perceptual measures, i.e., listening tests, are usually considered the most reliable way to evaluate the quality of speech processed by such algorithms but are costly and time-consuming. Consequently, speech enhancement algorithms are often evaluated using signal-based measures, which can be either intrusive or non-intrusive. As the computation of intrusive measures requires a reference signal, only non-intrusive measures can be used in applications for which the clean speech signal is not available. However, many existing non-intrusive measures correlate poorly with the perceived speech quality, particularly when applied over a wide range of algorithms or acoustic conditions. In this paper, we propose a novel non-intrusive measure of the quality of processed speech that combines modulation energy features and a recurrent neural network using long short-term memory cells. We collected a dataset of perceptually evaluated signals representing several acoustic conditions and algorithms and used this dataset to train and evaluate the proposed measure. Results show that the proposed measure yields higher correlation with perceptual speech quality than that of benchmark intrusive and non-intrusive measures when considering various categories of algorithms. Although the proposed measure is sensitive to mismatch between training and testing, results show that it is a useful approach to evaluate specific algorithms over a wide range of acoustic conditions and may, thus, become particularly useful for real-time selection of speech enhancement algorithm settings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1151–1163},
numpages = {13}
}

@article{10.1109/TASLP.2019.2913087,
author = {Zhang, Yike and Zhang, Pengyuan and Yan, Yonghong},
title = {Tailoring an Interpretable Neural Language Model},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Press},
volume = {27},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2913087},
doi = {10.1109/TASLP.2019.2913087},
abstract = {Neural networks have shown great potential in language modeling. Currently, the dominant approach to language modeling is based on recurrent neural networks RNNs and convolutional neural networks CNNs. Nonetheless, it is not clear why RNNs and CNNs are suitable for the language modeling task since these neural models are lack of interpretability. The goal of this paper is to tailor an interpretable neural model as an alternative to RNNs and CNNs for the language modeling task. This paper proposes a unified framework for language modeling, which can partly interpret the rationales behind existing language models LMs. Based on the proposed framework, an interpretable neural language model INLM is proposed, including a tailored architectural structure and a tailored learning method for the language modeling task. The proposed INLM can be approximated as a parameterized auto-regressive moving average model and provides interpretability in two aspects: component interpretability and prediction interpretability. Experiments demonstrate that the proposed INLM outperforms some typical neural LMs on several language modeling datasets and on the switchboard speech recognition task. Further experiments also show that the proposed INLM is competitive with the state-of-the-art long short-term memory LMs on the Penn Treebank and WikiText-2 datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1164–1178},
numpages = {15}
}

@article{10.1109/TASLP.2019.2913094,
author = {Luo, Huaishao and Li, Tianrui and Liu, Bing and Wang, Bin and Unger, Herwig},
title = {Improving Aspect Term Extraction With Bidirectional Dependency Tree Representation},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Press},
volume = {27},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2913094},
doi = {10.1109/TASLP.2019.2913094},
abstract = {Aspect term extraction is one of the important subtasks in aspect-based sentiment analysis. Previous studies have shown that using dependency tree structure representation is promising for this task. However, most dependency tree structures involve only one directional propagation on the dependency tree. In this paper, we first propose a novel bidirectional dependency tree network to extract dependency structure features from the given sentences. The key idea is to explicitly incorporate both representations gained separately from the bottom-up and top-down propagation on the given dependency syntactic tree. An end-to-end framework is then developed to integrate the embedded representations and BiLSTM plus CRF to learn both tree-structured and sequential features to solve the aspect term extraction problem. Experimental results demonstrate that the proposed model outperforms state-of-the-art baseline models on four benchmark SemEval datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1201–1212},
numpages = {12}
}

@article{10.1109/TASLP.2019.2913512,
author = {Pandey, Ashutosh and Wang, DeLiang},
title = {A New Framework for CNN-Based Speech Enhancement in the Time Domain},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Press},
volume = {27},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2913512},
doi = {10.1109/TASLP.2019.2913512},
abstract = {This paper proposes a new learning mechanism for a fully convolutional neural network CNN to address speech enhancement in the time domain. The CNN takes as input the time frames of noisy utterance and outputs the time frames of the enhanced utterance. At the training time, we add an extra operation that converts the time domain to the frequency domain. This conversion corresponds to simple matrix multiplication, and is hence differentiable implying that a frequency domain loss can be used for training in the time domain. We use mean absolute error loss between the enhanced short-time Fourier transform STFT magnitude and the clean STFT magnitude to train the CNN. This way, the model can exploit the domain knowledge of converting a signal to the frequency domain for analysis. Moreover, this approach avoids the well-known invalid STFT problem since the proposed CNN operates in the time domain. Experimental results demonstrate that the proposed method substantially outperforms the other methods of speech enhancement. The proposed method is easy to implement and applicable to related speech processing tasks that require time-frequency masking or spectral mapping.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1179–1188},
numpages = {10}
}

@article{10.1109/TASLP.2019.2913089,
author = {Vikram, C. M. and Adiga, Nagaraj and Prasanna, S. R. Mahadeva},
title = {Detection of Nasalized Voiced Stops in Cleft Palate Speech Using Epoch-Synchronous Features},
year = {2019},
issue_date = {July 2019},
publisher = {IEEE Press},
volume = {27},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2913089},
doi = {10.1109/TASLP.2019.2913089},
abstract = {The presence of velopharyngeal dysfunction in individuals with cleft palate CP nasalizes the voiced stops. Due to this, voiced stops /b/, /d/, /g/ tend to be perceive like nasal consonants /m/, /n/, /ng/. In this work, a novel algorithm is proposed for the detection of nasalized voiced stops in CP speech using epoch-synchronous features. Speech regions corresponding to consonant and consonant-vowel transitions are segmented using the knowledge of glottal activity, syllable nucleus, low-frequency spectral dominance, and vowel onset point. The segmented regions are epoch-synchronously processed to analyze the spectral, spectro-temporal, excitation source, and periodicity characteristics of normal and nasalized voiced stops. Spectral and spectro temporal features are computed using single pole filter based time-frequency representation. The amplitude of Hilbert envelope of linear prediction residual, measured around the epoch is used to analyze the effect of nasalization on excitation source. Comparison of speech frames of successive inter-epoch intervals is carried out to analyze the periodicity characteristics. The proposed features are used to develop a support vector machine classifier for the classification of normal and nasalized voiced stops. Segmentation accuracy for the proposed knowledge based method is found to be better than the hidden Markov model based force-alignment approach. The detection rate of nasalized voiced stops is found to be high for the proposed epoch synchronous features than the conventional Mel-frequency cepstral coefficients.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1189–1200},
numpages = {12}
}

@article{10.1109/TASLP.2019.2910637,
author = {Sisman, Berrak and Zhang, Mingyang and Li, Haizhou},
title = {Group Sparse Representation With WaveNet Vocoder Adaptation for Spectrum and Prosody Conversion},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2910637},
doi = {10.1109/TASLP.2019.2910637},
abstract = {The statistical approach to voice conversion typically consists of a feature conversion module followed by a vocoder. So far, the feature conversion studies are mainly focused on the conversion of spectrum. However, speaker identity is also characterized by prosodic features, such as fundamental frequency F0 and energy contour among others. In this paper, we study the transformation of speaker characteristics both in terms of spectrum and prosody. We propose two novel techniques that effectively use a limited amount of source-target training data and leverage a large general speech corpus to improve the voice conversion quality. First, we study the phonetic sparse representation under the group sparsity mathematical formulation. We use phonetic posteriorgrams PPGs together with spectral and prosody features to form tandem feature in the phonetic dictionary. The tandem feature allow us to estimate an activation matrix that is less dependent on source speakers, thus providing a better voice conversion quality. Second, we study the use of WaveNet vocoder that can be trained on general speech corpus from multiple speakers and adapted on target speaker data to improve the vocoding quality. We benefit from the large general speech databases that are used to train the PPG generator, and the WaveNet vocoder. The experiments show that the proposed conversion framework outperforms the traditional spectrum and prosody conversion techniques in both objective and subjective evaluations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1085–1097},
numpages = {13}
}

@article{10.1109/TASLP.2019.2906484,
author = {Juvela, Lauri and Bollepalli, Bajibabu and Tsiaras, Vassilis and Alku, Paavo},
title = {GlotNet—A Raw Waveform Model for the Glottal Excitation in Statistical Parametric Speech Synthesis},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2906484},
doi = {10.1109/TASLP.2019.2906484},
abstract = {Recently, generative neural network models which operate directly on raw audio, such as WaveNet, have improved the state of the art in text-to-speech synthesis TTS. Moreover, there is increasing interest in using these models as statistical vocoders for generating speech waveforms from various acoustic features. However, there is also a need to reduce the model complexity, without compromising the synthesis quality. Previously, glottal pulseforms i.e., time-domain waveforms corresponding to the source of human voice production mechanism have been successfully synthesized in TTS by glottal vocoders using straightforward deep feedforward neural networks. Therefore, it is natural to extend the glottal waveform modeling domain to use the more powerful WaveNet-like architecture. Furthermore, due to their inherent simplicity, glottal excitation waveforms permit scaling down the waveform generator architecture. In this study, we present a raw waveform glottal excitation model, called GlotNet, and compare its performance with the corresponding direct speech waveform model, WaveNet, using equivalent architectures. The models are evaluated as part of a statistical parametric TTS system. Listening test results show that both approaches are rated highly in voice similarity to the target speaker, and obtain similar quality ratings with large models. Furthermore, when the model size is reduced, the quality degradation is less severe for GlotNet.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1019–1030},
numpages = {12}
}

@article{10.1109/TASLP.2019.2910638,
author = {Lee, Jinkyu and Kang, Hong-Goo},
title = {A Joint Learning Algorithm for Complex-Valued T-F Masks in Deep Learning-Based Single-Channel Speech Enhancement Systems},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2910638},
doi = {10.1109/TASLP.2019.2910638},
abstract = {This paper presents a joint learning algorithm for complex-valued time-frequency T-F masks in single-channel speech enhancement systems. Most speech enhancement algorithms operating in a single-channel microphone environment aim to enhance the magnitude component in a T-F domain, while the input noisy phase component is used directly without any processing. Consequently, the mismatch between the processed magnitude and the unprocessed phase degrades the sound quality. To address this issue, a learning method of targeting a T-F mask that is defined in a complex domain has recently been proposed. However, due to a wide dynamic range and an irregular spectrogram pattern of the complex-valued T-F mask, the learning process is difficult even with a large-scale deep learning network. Moreover, the learning process targeting the T-F mask itself does not directly minimize the distortion in spectra or time domains. In order to address these concerns, we focus on three issues: 1 an effective estimation of complex numbers with a wide dynamic range; 2 a learning method that is directly related to speech enhancement performance; and 3 a way to resolve the mismatch between the estimated magnitude and phase spectra. In this study, we propose objective functions that can solve each of these issues and train the network by minimizing them with a joint learning framework. The evaluation results demonstrate that the proposed learning algorithm achieves significant performance improvement in various objective measures and subjective preference listening test.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1098–1109},
numpages = {12}
}

@article{10.1109/TASLP.2019.2905778,
author = {Liu, Yuanyuan and Lee, Tan and Law, Thomas and Lee, Kathy Yuet-Sheung},
title = {Acoustical Assessment of Voice Disorder With Continuous Speech Using ASR Posterior Features},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2905778},
doi = {10.1109/TASLP.2019.2905778},
abstract = {Traditionally acoustical assessment of voice disorder relies on simple and homogeneous speech samples like sustained vowels. Continuous speech is believed to be more representative of the daily function of voice and more preferable in clinical practice. This paper describes an attempt on automating voice assessment with continuous speech utterances. The proposed system makes use of a novel type of features that are derived from phone posterior probabilities outputted by a deep neural network based automatic speech recognition ASR system. These ASR-based voice features are designed to effectively quantify the mismatch between disordered voice and normal voice. Prediction of voice disorder severity is carried out first at utterance-level and subsequently the prediction scores for individual utterances from a subject are combined to give an overallassessment on the subject. With a low-dimension ASR-based feature vector, the utterance-level prediction accuracy is comparable to that with conventional features with a much higher dimension. By jointly using the ASR features and conventional voice features, a subject-level prediction accuracy of over $text{80}{%}$ on three severity classes can be achieved. Subjects with mild disorder and those with severe disorder could be perfectly distinguished by the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1047–1059},
numpages = {13}
}

@article{10.1109/TASLP.2019.2907016,
author = {Mesaros, Annamaria and Diment, Aleksandr and Elizalde, Benjamin and Heittola, Toni and Vincent, Emmanuel and Raj, Bhiksha and Virtanen, Tuomas},
title = {Sound Event Detection in the DCASE 2017 Challenge},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2907016},
doi = {10.1109/TASLP.2019.2907016},
abstract = {Each edition of the challenge on Detection and Classification of Acoustic Scenes and Events DCASE contained several tasks involving sound event detection in different setups. DCASE 2017 presented participants with three such tasks, each having specific datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly labeled data were available for training. In this paper, we present three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency-based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-specific optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of confidence intervals based on a jackknife resampling procedure to perform statistical analysis of the challenge results. The analysis indicates that while the 95% confidence intervals for many systems overlap, there are significant differences in performance between the top systems and the baseline for all tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {992–1006},
numpages = {15}
}

@article{10.1109/TASLP.2019.2906427,
author = {Chetupalli, Srikanth Raj and Sreenivas, Thippur V.},
title = {Late Reverberation Cancellation Using Bayesian Estimation of Multi-Channel Linear Predictors and Student's t-Source Prior},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2906427},
doi = {10.1109/TASLP.2019.2906427},
abstract = {Multi-channel linear prediction MCLP can model the late reverberation in the short-time Fourier transform domain using a delayed linear predictor and the prediction residual is taken as the desired early reflection component. Traditionally, a Gaussian source model with time-dependent precision inverse of variance is considered for the desired signal. In this paper, we propose a Student's t-distribution model for the desired signal, which is realized as a Gaussian source with a Gamma distributed precision. Further, since the choice of a proper MCLP order is critical, we also incorporate a Gaussian distribution prior for the prediction coefficients and a higher order. We consider a batch estimation scenario and develop variational Bayes expectation maximization VBEM algorithm for joint posterior inference and hyper-parameter estimation. This has lead to more accurate and robust estimation of the late reverb component and hence its cancellation, benefitting the desired residual signal estimation. Along with these stochastic models, we formulate single channel output MISO and multi channel output MIMO schemes using shared priors for the desired signal precision and the estimated MCLP coefficients at each microphone. Experiments using real room impulse responses show improved late reverberation suppression with the proposed VBEM approach over the traditional methods, for different room conditions. Additionally, we achieve a sparse coefficient vector for the MCLP avoiding the criticality of manually choosing the model order. The MIMO formulation is easily extended to include spatial filtering of the enhanced signals, which further improves the estimation of the desired signal.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1007–1018},
numpages = {12}
}

@article{10.1109/TASLP.2019.2892895,
author = {Winter, Fiete and Schultz, Frank and Firtha, Gergely and Spors, Sascha},
title = {A Geometric Model for Prediction of Spatial Aliasing in 2.5D Sound Field Synthesis},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2892895},
doi = {10.1109/TASLP.2019.2892895},
abstract = {The avoidance of spatial aliasing is a major challenge in the practical implementation of sound field synthesis. Such methods aim at a physically accurate reconstruction of a desired sound field inside a target region using a finite ensemble of loudspeakers. In the past, different theoretical treatises of the inherent spatial sampling process led to anti-aliasing criteria for simple loudspeaker array arrangements, e.g., lines and circles, and fundamental sound fields, e.g., plane and spherical waves. Many criteria were independent of the listener's position inside the target region. Within this paper, a geometrical framework based on ray-approximation of the underlying synthesis problem is proposed. Unlike former approaches, this model predicts spatial aliasing artefacts for arbitrary convex loudspeaker arrays and as a function of the listening position and the desired sound field. Anti-aliasing criteria for distinct listening positions and extended listening areas are formulated based on the established predictions. For validation, the model is applied to different analytical sound field synthesis approaches: The predicted spatial structure of the spatial aliasing agrees with numerical simulation of the synthesised sound fields. Moreover, it is shown within this framework, that the active prioritization of a control region using so-called local sound field synthesis approaches does indeed reduce spatial aliasing artefacts. For the scenario under investigation, a method for local wave field synthesis achieves an artefact-free synthesis up to a frequency which is between 2.9 and 17.3 times as high as for conventional wave field synthesis.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1031–1046},
numpages = {16}
}

@article{10.1109/TASLP.2019.2908279,
author = {Payal, Shreyas Srikanth and Mathews, V John and Button, Douglas J. and Iyer, Ajay and Lambert, Russell H. and Hutchings, Jeffrey and Azpicueta-Ruiz, Luis A.},
title = {Equalization of Nonlinear Propagation Distortion in Cylindrical Waveguides},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2908279},
doi = {10.1109/TASLP.2019.2908279},
abstract = {A model-based pre-equalizer for propagation-induced distortion of acoustic waveforms in an air-filled waveguide at high sound pressure levels is developed. The nonlinear distortions introduced by propagation are modeled using Burgers propagation model. This equalizer is stimulus-independent and mitigates nonlinear distortion at some predefined distance in the waveguide. The pre-equalizer digitally propagates the waveform backwards using a sign-inverted propagation model. The output of the pre-equalizer is fed to the waveguide. When the forward model completely characterizes the waveguide, the waveforms arriving at the chosen destination will be identical to a delayed version of the input signal fed to the pre-equalizer. Experimental evaluations employing sinusoidal and multitone stimuli demonstrated approximately a two-fold reduction in the intermodulation distortion and a four-fold reduction in total harmonic distortion at low frequencies. These results represented a two-fold improvement over a model-based method available in the literature.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1072–1084},
numpages = {13}
}

@article{10.1109/TASLP.2019.2908057,
author = {Porschmann, Christoph and Arend, Johannes M. and Brinkmann, Fabian},
title = {Directional Equalization of Sparse Head-Related Transfer Function Sets for Spatial Upsampling},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2908057},
doi = {10.1109/TASLP.2019.2908057},
abstract = {Acquiring decent full-spherical sets of head-related transfer functions HRTFs based on a small number of measurements is highly desirable. For spatial upsampling, HRTF interpolation in the spatially continuous spherical harmonics SH domain is a common approach. However, the number of measured HRTFs limits the assessable SH order, resulting in order-limited HRTFs when transformed to the SH domain. Thus, the SH representation of sparse HRTF sets shows restricted spatial resolution and suffers from order-limitation errors. We present a method that reduces these errors by a directional equalization prior to the SH transform. This is done by a spectral division of each HRTF with a corresponding directional rigid sphere transfer function. The processing removes direction-dependent temporal and spectral components and, therefore, significantly reduces the spatial complexity of the HRTF set, allowing for an enhanced interpolation of HRTFs at reduced SH orders. Spatial upsampling is achieved by an inverse SH transform on an arbitrary dense sampling grid. A subsequent de-equalization by a spectral multiplication with the rigid sphere transfer function recovers the energy in higher spatial orders that was not inherent in the sparse HRTF set. For evaluation, HRTFs were calculated for various limited orders from sparse datasets and compared to a reference. The results show that the proposed method clearly outperforms common SH interpolation of HRTF spectra regarding the overall spectral and temporal structure as well as modeled localization performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1060–1071},
numpages = {12}
}

@article{10.1109/TASLP.2019.2903288,
author = {Kumar, Anurendra and Guha, Tanaya and Ghosh, Prasanta Kumar},
title = {Dirichlet Latent Variable Model: A Dynamic Model Based on Dirichlet Prior for Audio Processing},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2903288},
doi = {10.1109/TASLP.2019.2903288},
abstract = {We propose a dynamic latent variable model for learning latent bases from time varying, non-negative data. We take a probabilistic approach to modeling the temporal dependence in data by introducing a dynamic Dirichlet prior—a Dirichlet distribution with dynamic parameters. This new distribution allows us to assure non-negativity and avoid intractability when sequential updates are performed otherwise encountered in using Dirichlet prior. We refer to the proposed model as the Dirichlet latent variable model DLVM. We develop an expectation maximization algorithm for the proposed model, and also derive a maximum a posteriori estimate of the parameters. Furthermore, we connect the proposed DLVM to two popular latent basis learning methods—probabilistic latent component analysis PLCA and non-negative matrix factorization NMF. We show that 1 PLCA is a special case of our DLVM, and 2 DLVM can be interpreted as a dynamic version of NMF. The usefulness of DLVM is demonstrated for three audio processing applications—speaker source separation, denoising, and bandwidth expansion. To this end, a new algorithm for source separation is also proposed. Through extensive experiments on benchmark databases, we show that the proposed model outperforms several relevant existing methods in all three applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {919–931},
numpages = {13}
}

@article{10.1109/TASLP.2019.2905167,
author = {Koriyama, Tomoki and Kobayashi, Takao},
title = {Statistical Parametric Speech Synthesis Using Deep Gaussian Processes},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2905167},
doi = {10.1109/TASLP.2019.2905167},
abstract = {This paper proposes a framework of speech synthesis based on deep Gaussian processes DGPs, which is a deep architecture model composed of stacked Bayesian kernel regressions. In this method, we train a statistical model of transformation from contextual features to speech parameters in a similar manner to deep neural network DNN-based speech synthesis. To apply DGPs to a statistical parametric speech synthesis framework, our framework uses an approximation method, doubly stochastic variational inference, which is suitable for an arbitrary amount of data. Since the training of DGPs is based on the marginal likelihood that takes into account not only data fitting, but also model complexity, DGPs are less vulnerable to overfitting compared with DNNs. In experimental evaluations, we investigated a performance comparison of the proposed DGP-based framework with a feedforward DNN-based one. Subjective and objective evaluation results showed that our DGP framework yielded a higher mean opinion score and lower acoustic feature distortions than the conventional framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {948–959},
numpages = {12}
}

@article{10.1109/TASLP.2019.2903276,
author = {Elisei-Iliescu, Camelia and Paleologu, Constantin and Benesty, Jacob and Stanciu, Cristian and Anghel, Cristian and Ciochina, Silviu},
title = {Recursive Least-Squares Algorithms for the Identification of Low-Rank Systems},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2903276},
doi = {10.1109/TASLP.2019.2903276},
abstract = {The recursive least-squares RLS adaptive filter is an appealing choice in many system identification problems. The main reason behind its popularity is its fast convergence rate. However, this algorithm is computationally very complex, which may make it useless for the identification of long length impulse responses, like in echo cancellation. Computationally efficient versions of the RLS algorithm, like those based on the dichotomous coordinate descent DCD iterations or QR decomposition techniques, reduce the complexity, but still have to face the challenges related to long length adaptive filters e.g., convergence/tracking capabilities. In this paper, we focus on a different approach to improve the efficiency of the RLS algorithm. The basic idea is to exploit the impulse response decomposition based on the nearest Kronecker product and low-rank approximation. In other words, a high-dimension system identification problem is reformulated in terms of low-dimension problems, which are combined together. This approach was recently addressed in terms of the Wiener filter, showing appealing features for the identification of low-rank systems, like real-world echo paths. In this paper, besides the development of the RLS algorithm based on this approach, we also propose a variable regularized version of this algorithm using the DCD method to reduce the complexity, with improved robustness to double-talk. Simulations are performed in the context of echo cancellation and the results indicate the good performance of these algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {903–918},
numpages = {16}
}

@article{10.1109/TASLP.2019.2895241,
author = {Cohen, Israel and Benesty, Jacob and Chen, Jingdong},
title = {Differential Kronecker Product Beamforming},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2895241},
doi = {10.1109/TASLP.2019.2895241},
abstract = {Differential beamformers have attracted much interest over the past few decades. In this paper, we introduce differential Kronecker product beamformers that exploit the structure of the steering vector to perform beamforming differently from the well-known and studied conventional approach. We consider a class of microphone arrays that enable to decompose the steering vector as a Kronecker product of two steering vectors of smaller virtual arrays. In the proposed approach, instead of directly designing the differential beamformer, we break it down following the decomposition of the steering vector, and show how to derive differential beamformers using the Kronecker product formulation. As demonstrated, the Kronecker product decomposition facilitates further flexibility in the design of differential beamformers and in the tradeoff control between the directivity factor and the white noise gain.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {892–902},
numpages = {11}
}

@article{10.1109/TASLP.2019.2907015,
author = {Shimada, Kazuki and Bando, Yoshiaki and Mimura, Masato and Itoyama, Katsutoshi and Yoshii, Kazuyoshi and Kawahara, Tatsuya},
title = {Unsupervised Speech Enhancement Based on Multichannel NMF-Informed Beamforming for Noise-Robust Automatic Speech Recognition},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2907015},
doi = {10.1109/TASLP.2019.2907015},
abstract = {This paper describes multichannel speech enhancement for improving automatic speech recognition ASR in noisy environments. Recently, the minimum variance distortionless response MVDR beamforming has widely been used because it works well if the steering vector of speech and the spatial covariance matrix SCM of noise are given. To estimating such spatial information, conventional studies take a supervised approach that classifies each time-frequency TF bin into noise or speech by training a deep neural network DNN. The performance of ASR, however, is degraded in an unknown noisy environment. To solve this problem, we take an unsupervised approach that decomposes each TF bin into the sum of speech and noise by using multichannel nonnegative matrix factorization MNMF. This enables us to accurately estimate the SCMs of speech and noise not from observed noisy mixtures but from separated speech and noise components. In this paper, we propose online MVDR beamforming by effectively initializing and incrementally updating the parameters of MNMF. Another main contribution is to comprehensively investigate the performances of ASR obtained by various types of spatial filters, i.e., time-invariant and variant versions of MVDR beamformers and those of rank-1 and full-rank multichannel Wiener filters, in combination with MNMF. The experimental results showed that the proposed method outperformed the state-of-the-art DNN-based beamforming method in unknown environments that did not match training data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {960–971},
numpages = {12}
}

@article{10.1109/TASLP.2019.2901643,
author = {Ibarrola, Francisco Javier and Spies, Ruben Daniel and Persia, Leandro Ezequiel Di},
title = {Switching Divergences for Spectral Learning in Blind Speech Dereverberation},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2901643},
doi = {10.1109/TASLP.2019.2901643},
abstract = {When recorded in an enclosed room, a sound signal will most certainly get affected by reverberation. This not only undermines audio quality, but also poses a problem for many human-machine interaction technologies that use speech as their input. In this paper, a new blind, two-stage dereverberation approach based in a generalized $beta$-divergence as a fidelity term over a non-negative representation is proposed. The first stage consists of learning the spectral structure of the signal solely from the observed spectrogram, while the second stage is devoted to model reverberation. Both steps are taken by minimizing a cost function in which the aim is put either in constructing a dictionary or a good representation by changing the divergence involved. In addition, an approach for finding an optimal fidelity parameter for dictionary learning is proposed. An algorithm for implementing the proposed method is described and tested against state-of-the-art methods. Results show improvements for both artificial reverberation and real recordings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {881–891},
numpages = {11}
}

@article{10.1109/TASLP.2019.2904790,
author = {Jancovic, Peter and Kokuer, Munevver},
title = {Bird Species Recognition Using Unsupervised Modeling of Individual Vocalization Elements},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2904790},
doi = {10.1109/TASLP.2019.2904790},
abstract = {This paper investigates acoustic modeling for recognition of bird species from audio field recordings. First, the acoustic scene is decomposed into isolated segments, corresponding to detected sinusoids. Each segment is represented by a sequence of the frequency and normalized magnitude values of the sinusoid. The temporal evolution of these features is modeled using hidden Markov models HMMs. A novel method for an unsupervised modeling of individual bird vocalization elements is proposed. The element models are initialized using HMM-based clustering and then further trained using an iterative maximum likelihood label re-assignment procedure. State duration modeling, performed in a post-recognition stage, is explored. Finally, we developed a hybrid deep neural network—hidden Markov model. The developed acoustic models are employed for bird species identification, detection of specific species, and recognition of multiple bird species vocalizing in a given recording. The detection system employs score normalization. Recognition of multiple bird species is performed based on maximizing the likelihood of a set of segments on a subset of bird species models, with penalization based on Bayesian information criterion applied. Experimental evaluations are performed on more than 37&nbsp;h of sound field recordings, containing vocalizations of 48 bird species, plus more than 16&nbsp;h of non-bird sound recordings. Using 3&nbsp;s of the detected signal, the best system achieved: identification accuracy of 98.7%, detection with the equal error rate of 2.7%, and recognition accuracy of 97.3% and 95.4% when vocalizations of multiple bird species are present, with the number of bird species known and estimated, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {932–947},
numpages = {16}
}

@article{10.1109/TASLP.2019.2904839,
author = {Widmark, Simon},
title = {Causal MSE-Optimal Filters for Personal Audio Subject to Constrained Contrast},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2904839},
doi = {10.1109/TASLP.2019.2904839},
abstract = {A novel design method that generates causal pre-compensation filters is formulated. The resulting filters are constrained with respect to the amount of acoustic contrast they generate and are intended to be used for personal audio. The proposed method provides a more direct method for trading bright zone behavior against acoustic contrast as compared to other causal methods available. It also provides improved control over the temporal properties of the resulting filters as compared to the pre-existing non-causal methods. The resulting filters are analyzed by means of simulations, based on measured impulse responses of the sound-system–room interactions. The results of the simulations are compared to simulations of a frequency-domain optimal method with comparable objective, as proposed by Cai et&nbsp;al. and the results of the comparison are explained using the design equations. It is shown that the proposed method is viable, but that unattainable contrasts have a detrimental impact on the spectral bright zone behavior. A few different strategies for dealing with this problem are also proposed. It is demonstrated that the detrimental effect of increasingly strict causality constraints mainly concerns the lower frequency bright zone behavior in the system under investigation, but that the very highest attainable contrast levels may also be reduced somewhat.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {972–987},
numpages = {16}
}

@article{10.1109/TASLP.2019.2899517,
author = {Wang, Xianghui and Cohen, Israel and Chen, Jingdong and Benesty, Jacob},
title = {On Robust and High Directive Beamforming With Small-Spacing Microphone Arrays for Scattered Sources},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2899517},
doi = {10.1109/TASLP.2019.2899517},
abstract = {This paper is devoted to beamforming with small-spacing microphone arrays for processing broadband and scattered acoustic sources. It presents a maximum diffuse noise gain MDNG beamformer in this context using the joint diagonalization technique, which is effective in suppressing diffuse and directional noise, but at a price of low white noise gain WNG. We also introduce a maximum WNG MWNG beamformer, which is robust to the array imperfections, but paying a price of sacrificing the diffuse noise gain DNG. To make a tradeoff between WNG and DNG so that the beamformer, on the one hand, can achieve high directivity and, on the other hand, is robust to implement, we propose a generalized MDNG beamformer, which includes both the MDNG and MWNG beamformers as particular cases. Simulations are conducted to illustrate the properties and advantages of the proposed beamformers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {842–852},
numpages = {11}
}

@article{10.1109/TASLP.2019.2895254,
author = {Kong, Qiuqiang and Xu, Yong and Sobieraj, Iwona and Wang, Wenwu and Plumbley, Mark D.},
title = {Sound Event Detection and Time–Frequency Segmentation from Weakly Labelled Data},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2895254},
doi = {10.1109/TASLP.2019.2895254},
abstract = {Sound event detection SED aims to detect when and recognize what sound events happen in an audio clip. Many supervised SED algorithms rely on strongly labelled data that contains the onset and offset annotations of sound events. However, many audio tagging datasets are weakly labelled, that is, only the presence of the sound events is known, without knowing their onset and offset annotations. In this paper, we propose a time–frequency T–F segmentation framework trained on weakly labelled data to tackle the sound event detection and separation problem. In training, a segmentation mapping is applied on a T–F representation, such as log mel spectrogram of an audio clip to obtain T–F segmentation masks of sound events. The T–F segmentation masks can be used for separating the sound events from the background scenes in the T–F domain. Then, a classification mapping is applied on the T–F segmentation masks to estimate the presence probabilities of the sound events. We model the segmentation mapping using a convolutional neural network and the classification mapping using a global weighted rank pooling. In SED, predicted onset and offset times can be obtained from the T–F segmentation masks. As a byproduct, separated waveforms of sound events can be obtained from the T–F segmentation masks. We remixed the DCASE 2018 Task 1 acoustic scene data with the DCASE 2018 Task 2 sound events data. When mixing under 0&nbsp;dB, the proposed method achieved F1 scores of 0.534, 0.398, and 0.167 in audio tagging, frame-wise SED and event-wise SED, outperforming the fully connected deep neural network baseline of 0.331, 0.237, and 0.120, respectively. In T–F segmentation, we achieved an F1 score of 0.218, where previous methods were not able to do T–F segmentation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {777–787},
numpages = {11}
}

@article{10.1109/TASLP.2019.2898818,
author = {Lin, Shoufeng},
title = {Robust Pitch Estimation and Tracking For Speakers Based on Subband Encoding and The Generalized Labeled Multi-Bernoulli Filter},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2898818},
doi = {10.1109/TASLP.2019.2898818},
abstract = {This paper proposes a new pitch estimator and a novel pitch tracker for speakers. We first decompose the sound signal into subbands using an auditory filterbank, assuming time–frequency sparsity of human speech. Instead of directly selecting the number of subbands according to experience, we propose a novel frequency coverage metric to derive the number of subbands and the center frequencies of the filterbank. The subband signals are then encoded inspired by the computational auditory scene analysis approach, and the normalized autocorrelations are calculated for pitch estimation. To suppress spurious errors and track the speaker identity, the temporal continuity constraint is exploited and a generalized labeled multi-Bernoulli filter is adapted for pitch tracking, where we use a novel pitch state transition model based on the Ornstein–Uhlenbeck process, and the measurement-driven birth model for adaptive new births of pitch targets. Experimental evaluations with various additive noises demonstrate that the proposed methods have achieved better accuracy compared with several state-of-the-art pitch estimation methods in most studied scenarios. Tests using real recordings in a reverberant room also show that the proposed method is robust against reverberation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {827–841},
numpages = {15}
}

@article{10.1109/TASLP.2019.2892234,
author = {Schepker, Henning and Nordholm, Sven Erik and Tran, Linh Thi Thuc and Doclo, Simon},
title = {Null-Steering Beamformer-Based Feedback Cancellation for Multi-Microphone Hearing Aids With Incoming Signal Preservation},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2892234},
doi = {10.1109/TASLP.2019.2892234},
abstract = {In hearing aids, acoustic feedback occurs due to the coupling between the hearing aid loudspeaker and microphones. In order to reduce the acoustic feedback, adaptive filters are commonly used to estimate the feedback contribution in the microphones. While theoretically allowing for perfect feedback cancellation, in practice the adaptive filter converges to an optimal solution that is typically biased due to the closed-loop acoustical system of the hearing aid. In order to avoid the adaptation to a biased optimal solution, in this paper we propose to use a fixed beamformer to cancel the acoustic feedback contribution for an earpiece with multiple integrated microphones and loudspeakers. By steering a spatial null in the direction of the hearing aid loudspeaker, we show that theoretically perfect feedback cancellation can be achieved. While previous null-steering beamforming approaches did not control for distortions of the incoming signal, in this paper we propose to incorporate a constraint based on the relative transfer function RTF of the incoming signal, aiming to perfectly preserve this signal. We formulate the computation of the beamformer coefficients both as a least-squares optimization procedure, aiming to minimize the residual feedback power, and as a min–max optimization procedure, aiming to directly maximize the maximum stable gain of the hearing aid. Experimental results using measured acoustic feedback paths from a custom earpiece with two microphones in the vent and a third microphone in the concha show that the proposed fixed null-steering beamformer using the RTF-based constraint provides a reduction of the acoustic feedback and substantially increases the added stable gain while preserving the incoming signal. This can even be achieved for unknown acoustic feedback paths and incoming signal directions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {679–691},
numpages = {13}
}

@article{10.1109/TASLP.2019.2893499,
author = {Zhou, Mantong and Huang, Minlie and Zhu, Xiaoyan},
title = {Story Ending Selection by Finding Hints From Pairwise Candidate Endings},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2893499},
doi = {10.1109/TASLP.2019.2893499},
abstract = {The ability of story comprehension is a strong indicator of natural language understanding. Recently, Story Cloze Test has been introduced as a new task of machine reading comprehension, i.e., selecting a correct ending from two candidate endings given a four-sentence story context. Most existing methods for Story Cloze Test are essentially matching-based that operate by comparing an individual ending with a given context, therefore suffering from the evidence bias issue: both candidate endings can obtain supporting evidence from the story context, which misleads the classifier to choose an incorrect ending. To address this issue, we present a novel idea to improve story comprehension by utilizing the hints that are obtained through comparing two candidate endings. The proposed model firstly anticipates a feature vector for a possible ending solely based on the context, and then refines the feature prediction using the hints which encode the difference between two candidates. The candidate ending whose feature vector is more similar to the predicted ending vector is regarded as correct. Experimental results demonstrate that our approach can alleviate the evidence bias issue and improve story comprehension.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {719–729},
numpages = {11}
}

@article{10.1109/TASLP.2019.2892232,
author = {Deng, Dong and Jing, Liping and Yu, Jian and Sun, Shaolong and Ng, Michael K.},
title = {Sentiment Lexicon Construction With Hierarchical Supervision Topic Model},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2892232},
doi = {10.1109/TASLP.2019.2892232},
abstract = {In this paper, we propose a novel hierarchical supervision topic model to construct a topic-adaptive sentiment lexicon TaSL for higher-level classification tasks. It is widely recognized that sentiment lexicon as a useful prior knowledge is crucial in sentiment analysis or opinion mining. However, many existing sentiment lexicons are constructed ignoring the variability of the sentiment polarities of words in different topics or domains. For example, the word “amazing” can refer to causing great surprise or wonder but can also refer to very impressive and excellent. In TaSL, we solve this issue by jointly considering the topics and sentiments of words. Documents are represented by multiple pairs of topics and sentiments, where each pair is characterized by a multinomial distribution over words. Meanwhile, this generating process is supervised under hierarchical supervision information of documents and words. The main advantage of TaSL is that the sentiment polarity of each word in different topics can be sufficiently captured. This model is beneficial to construct a domain-specific sentiment lexicon and then effectively improve the performance of sentiment classification. Extensive experimental results on four publicly available datasets, MR, OMD, semEval13A, and semEval16B were presented to demonstrate the usefulness of the proposed approach. The results have shown that TaSL performs better than the existing manual sentiment lexicon MPQA, the topic model based domain-specific lexicon ssLDA, the expanded lexiconsWeka-ED, Weka-STS, NRC, Liu's, and deep neural network based lexicons nnLexicon, HIT, HSSWE.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {704–718},
numpages = {15}
}

@article{10.1109/TASLP.2019.2896437,
author = {Tuan, Yi-Lin and Lee, Hung-Yi},
title = {Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2896437},
doi = {10.1109/TASLP.2019.2896437},
abstract = {Sequence generative adversarial networks SeqGAN have been used to improve conditional sequence generation tasks, for example, chit-chat dialogue generation. To stabilize the training of SeqGAN, Monte Carlo tree search MCTS or reward at every generation step REGS is used to evaluate the goodness of a generated subsequence. MCTS is computationally intensive, but the performance of REGS is worse than MCTS. In this paper, we propose stepwise GAN StepGAN, in which the discriminator is modified to automatically assign scores quantifying the goodness of each subsequence at every generation step. StepGAN has significantly less computational costs than MCTS. We demonstrate that StepGAN outperforms previous GAN-based methods on both synthetic experiment and chit-chat dialogue generation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {788–798},
numpages = {11}
}

@article{10.1109/TASLP.2019.2894554,
author = {Yu, Jianguo and Markov, Konstantin and Matsui, Tomoko},
title = {Articulatory and Spectrum Information Fusion Based on Deep Recurrent Neural Networks},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2894554},
doi = {10.1109/TASLP.2019.2894554},
abstract = {Many studies have shown that articulatory features can significantly improve the performance of automatic speech recognition systems. Unfortunately, such features are not available at recognition time. There are two main approaches to solve this problem: a feature-based approach, the most popular example of which is the acoustic-to-articulatory inversion, where the missing articulatory features are generated from the speech signal, and a model-based approach, where articulatory information is embedded in the model structure and parameters in a way that allows recognition using only acoustic features. In this paper, we propose two new methods to integrate articulatory information into a phoneme recognition system. One of them is feature based, and the other is model based. In both cases, the underlying acoustic model AM is a deep neural networks-hidden Markov model DNN-HMM hybrid. In the feature-based method, the articulatory inversion DNN and the acoustic model DNN are trained jointly using a linear combination of their loss functions. In the model-based method, we utilize the generalized distillation framework to train the AM DNN. In this case, first, a teacher DNN is trained on both the acoustic and articulatory features, and then its outputs are used as additional targets during the AM DNN training with acoustic features only. A 7-fold cross-validation experiments using 42 speakers from the XRMB database showed that both the proposed methods provide about 22% to 25% performance improvement with respect to the DNN acoustic model trained with acoustic features only.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {742–752},
numpages = {11}
}

@article{10.1109/TASLP.2019.2899494,
author = {Quan, Zhe and Wang, Zhi-Jie and Le, Yuquan and Yao, Bin and Li, Kenli and Yin, Jian},
title = {An Efficient Framework for Sentence Similarity Modeling},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2899494},
doi = {10.1109/TASLP.2019.2899494},
abstract = {Sentence similarity modeling lies at the core of many natural language processing applications, and thus has received much attention. Owing to the success of word embeddings, recently, popular neural network methods achieved sentence embedding. Most of them focused on learning semantic information and modeling it as a continuous vector, yet the syntactic information of sentences has not been fully exploited. On the other hand, prior works have shown the benefits of structured trees that include syntactic information, while few methods in this branch utilized the advantages of word embeddings and another powerful technique—attention weight mechanism. This paper suggests to absorb their advantages by merging these techniques in a unified structure, dubbed as attention constituency vector tree ACVT. Meanwhile, this paper develops a new tree kernel, known as ACVT kernel, which is tailored for sentence similarity measure based on the proposed structure. The experimental results, based on 19 widely used semantic textual similarity datasets, demonstrate that our model is effective and competitive, when compared against state-of-the-art models. Additionally, the experimental results validate that many attention weight mechanisms and word embedding techniques can be seamlessly integrated into our model, demonstrating the robustness and universality of our model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {853–865},
numpages = {13}
}

@article{10.1109/TASLP.2019.2895969,
author = {Abel, Johannes and Fingscheidt, Tim},
title = {Sinusoidal-Based Lowband Synthesis for Artificial Speech Bandwidth Extension},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2895969},
doi = {10.1109/TASLP.2019.2895969},
abstract = {Conventional narrowband NB telephony suffers from limited acoustic bandwidth at the receiver side, leading to degraded speech quality and intelligibility. In this paper, artificial speech bandwidth extension ABE of NB speech toward missing frequencies below about 300 Hz low-frequency band, LB is proposed to enhance the speech quality. The LB-ABE in this paper is employed together with a preexisting ABE toward high-frequency components to obtain spectrally balanced speech signals. In an instrumental quality assessment, the spectral distance in the LB was improved by more than 5 dB compared to NB speech. In a subjective listening test, the gap of speech quality between wideband and NB speech was significantly reduced when employing the proposed ABE toward low frequencies. The LB extension was found to further improve the preexisting ABE toward higher frequencies by a significant 0.26 CMOS points.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {765–776},
numpages = {12}
}

@article{10.1109/TASLP.2019.2900910,
author = {Lubis, Nurul and Sakti, Sakriani and Yoshino, Koichiro and Nakamura, Satoshi},
title = {Positive Emotion Elicitation in Chat-Based Dialogue Systems},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2900910},
doi = {10.1109/TASLP.2019.2900910},
abstract = {We aim to draw on an important overlooked potential of affective dialogue systems—their application to promote positive emotional states, similar to that of emotional support between humans. This can be achieved by eliciting a more positive emotional valence throughout a dialogue system interaction, i.e., positive emotion elicitation. Existing works on emotion elicitation have not yet paid attention to the emotional benefit for the users. Moreover, a positive emotion elicitation corpus does not yet exist despite the growing number of emotion-rich corpora. Towards this goal, first, we propose a response retrieval approach for positive emotion elicitation by utilizing examples of emotion appraisal from a dialogue corpus. Second, we efficiently construct a corpus using the proposed retrieval method, by replacing responses in a dialogue with those that elicit a more positive emotion. We validate the corpus through crowdsourcing to ensure its quality. Finally, we propose a novel neural network architecture for an emotion-sensitive neural chat-based dialogue system, optimized on the constructed corpus to elicit positive emotion. Objective and subjective evaluations show that the proposed methods result in dialogue responses that are more natural and elicit a more positive emotional response. Further analyses of the results are discussed in this paper.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {866–877},
numpages = {12}
}

@article{10.1109/TASLP.2019.2895973,
author = {Itturriet, Fabio Pires and Costa, Marcio Holsbach},
title = {Perceptually Relevant Preservation of Interaural Time Differences in Binaural Hearing Aids},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2895973},
doi = {10.1109/TASLP.2019.2895973},
abstract = {This paper presents a noise reduction method with perceptually relevant preservation of the interaural time difference ITD of the residual noise in binaural hearing aids. The interaural coherence IC concept, previously applied to the multichannel Wiener filter MWF for preservation of the spatial subjective sensation of diffuse noise fields, is proposed here to both preserve and emphasize the ITD binaural cues of a directional acoustic noise source. It is demonstrated that the previously developed MWF-ITD technique may decrease the original IC magnitude of the processed noise, consequently increasing the variance of the interaural phase difference IPD of the output signals. It is shown that the MWF-IC technique simultaneously minimizes a nonlinear function of the difference between input and output IPD, which is strictly related to ITD, and preserves the natural coherence of directional noise captured by the reference microphones. Objective measures and psychoacoustic experiments corroborate the theoretical findings, showing that the MWF-IC technique provides relevant noise reduction, while preserving the original ITD subjective perception and original lateralization for a directional noise source. These results are especially relevant for hearing aid designers, since they indicate MWF-IC as a noise reduction technique that provides residual noise spatial preservation for both diffuse and directional noise sources in frequencies below 1.5 kHz.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {753–764},
numpages = {12}
}

@article{10.1109/TASLP.2018.2887337,
author = {Zhao, Ziyue and Liu, Huijun and Fingscheidt, Tim},
title = {Convolutional Neural Networks to Enhance Coded Speech},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2887337},
doi = {10.1109/TASLP.2018.2887337},
abstract = {Enhancing coded speech suffering from far-end acoustic background noise, quantization noise, and potentially transmission errors is a challenging task. In this paper, we propose two postprocessing approaches applying convolutional neural networks either in the time domain or the cepstral domain to enhance the coded speech without any modification of the codecs. The time-domain approach follows an end-to-end fashion, whereas the cepstral domain approach uses analysis–synthesis with cepstral domain features. The proposed postprocessors in both domains are evaluated for various narrowband and wideband speech codecs in a wide range of conditions. The proposed postprocessor improves perceptual evaluation of speech quality by up to 0.25 mean opinion score listening quality objective points for G.711, 0.30 points for G.726, 0.82 points for G.722, and 0.26 points for adaptive multirate wideband codec. In a subjective comparison category rating listening test, the proposed postprocessor on G.711-coded speech exceeds the speech quality of an ITU-T-standardized postfilter by 0.36 CMOS points, and obtains a clear preference of 1.77 CMOS points compared to legacy G.711, even better than uncoded speech with statistical significance. The source code for the cepstral domain approach to enhance G.711-coded speech is made available.11https://github.com/ifnspaml/Enhancement-Coded-Speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {663–678},
numpages = {16}
}

@article{10.1109/TASLP.2019.2898816,
author = {Lotfian, Reza and Busso, Carlos},
title = {Curriculum Learning for Speech Emotion Recognition From Crowdsourced Labels},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2898816},
doi = {10.1109/TASLP.2019.2898816},
abstract = {This study introduces a method to design a curriculum for machine-learning to maximize the efficiency during the training process of deep neural networks DNNs for speech emotion recognition. Previous studies in other machine-learning problems have shown the benefits of training a classifier following a curriculum where samples are gradually presented in increasing level of difficulty. For speech emotion recognition, the challenge is to establish a natural order of difficulty in the training set to create the curriculum. We address this problem by assuming that, ambiguous samples for humans are also ambiguous for computers. Speech samples are often annotated by multiple evaluators to account for differences in emotion perception across individuals. While some sentences with clear emotional content are consistently annotated, sentences with more ambiguous emotional content present important disagreement between individual evaluations. We propose to use the disagreement between evaluators as a measure of difficulty for the classification task. We propose metrics that quantify the inter-evaluation agreement to define the curriculum for regression problems and binary and multi-class classification problems. The experimental results consistently show that relying on a curriculum based on agreement between human judgments leads to statistically significant improvements over baselines trained without a curriculum.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {815–826},
numpages = {12}
}

@article{10.1109/TASLP.2019.2894329,
author = {Richter, Jan-Gerrit and Fels, Janina},
title = {On the Influence of Continuous Subject Rotation During High-Resolution Head-Related Transfer Function Measurements},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2894329},
doi = {10.1109/TASLP.2019.2894329},
abstract = {In recent years, the required time for high resolution individual head-related transfer function HRTF measurements has been reduced by the use of microphone or loudspeaker arrays and with parallelization of measurement signals. While a short measurement time is beneficial during HRTF measurement of any resolution, it is essential for the acquisition of high resolution individual HRTF measurements since subjects cannot remain motionless over extended periods of time. This decrease in measurement time can be achieved by using continuous rotation during the measurement, instead of step-wise subject positioning. This paper examines the influence of such continuous measurement rotation with sine sweep measurements and the necessary subsequent post-processing. The goal is to identify a rotation speed, from a predefined set of speeds, that does not have an audible influence on the measured HRTF itself. To this end, a subjective and objective comparison between the two measurement modes is presented in this paper. Five HRTF measurements of an artificial head are acquired using different rotation speeds. They are compared objectively to a reference measurement of the same head performed with step-wise measurement rotation in the same measurement setup. A subjective listening experiment examines whether the found differences between the measurements do result in an audible difference. The objective evaluation showed expected trends regarding the dependency of the error on the measurement speed and is used to validate the used signal post-processing. The subjective evaluation showed that with a rotation of 3.8$^circ$/s no audible differences to the reference are present.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {730–741},
numpages = {12}
}

@article{10.1109/TASLP.2019.2892241,
author = {Li, Zeng-Xi and Song, Yan and Dai, Li-Rong and McLoughlin, Ian},
title = {Listening and Grouping: An Online Autoregressive Approach for Monaural Speech Separation},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2892241},
doi = {10.1109/TASLP.2019.2892241},
abstract = {This paper proposes an autoregressive approach to harness the power of deep learning for multi-speaker monaural speech separation. It exploits a causal temporal context in both mixture and past estimated separated signals and performs online separation that is compatible with real-time applications. The approach adopts a learned listening and grouping architecture motivated by computational auditory scene analysis, with a grouping stage that effectively addresses the label permutation problem at both frame and segment levels. Experimental results on the WSJ0-2mix benchmark show that the new approach can achieve better signal-to-distortion ratio and perceptual evaluation of speech quality scores than most of the state-of-the-art methods for both closed-set and open-set evaluations, even methods that exploit whole-utterance statistics for separation. It achieves this while requiring fewer model parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {692–703},
numpages = {12}
}

@article{10.1109/TASLP.2019.2894909,
author = {Dionelis, Nikolaos and Brookes, Mike},
title = {Modulation-Domain Kalman Filtering for Monaural Blind Speech Denoising and Dereverberation},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2894909},
doi = {10.1109/TASLP.2019.2894909},
abstract = {We describe a monaural speech enhancement algorithm based on modulation-domain Kalman filtering to blindly track the time–frequency log-magnitude spectra of speech and reverberation. We propose an adaptive algorithm that performs blind joint denoising and dereverberation, while accounting for the inter-frame speech dynamics, by estimating the posterior distribution of the speech log-magnitude spectrum given the log-magnitude spectrum of the noisy reverberant speech. The Kalman filter update step models the non-linear relations between the speech, noise, and reverberation log spectra. The Kalman filtering algorithm uses a signal model that takes into account the reverberation parameters of the reverberation time $T_{60}$ and the direct-to-reverberant energy ratio DRR and also estimates and tracks $T_{60}$ and the DRR in every frequency bin to improve the estimation of the speech log spectrum. The proposed algorithm is evaluated in terms of speech quality, speech intelligibility, and dereverberation performance for a range of reverberation parameters and reverberant speech to noise ratios, in different noises, and is also compared to competing denoising and dereverberation techniques. Experimental results using noisy reverberant speech demonstrate the effectiveness of the enhancement algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {799–814},
numpages = {16}
}

@article{10.1109/TASLP.2019.2892235,
author = {Zhang, Jing-Xuan and Ling, Zhen-Hua and Liu, Li-Juan and Jiang, Yuan and Dai, Li-Rong},
title = {Sequence-to-Sequence Acoustic Modeling for Voice Conversion},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2892235},
doi = {10.1109/TASLP.2019.2892235},
abstract = {In this paper, a neural network named sequence-to-sequence ConvErsion NeTwork SCENT is presented for acoustic modeling in voice conversion. At training stage, a SCENT model is estimated by aligning the feature sequences of source and target speakers implicitly using attention mechanism. At the conversion stage, acoustic features and durations of source utterances are converted simultaneously using the unified acoustic model. Mel-scale spectrograms are adopted as acoustic features, which contain both excitation and vocal tract descriptions of speech signals. The bottleneck features extracted from source speech using an automatic speech recognition model are appended as an auxiliary input. A WaveNet vocoder conditioned on Mel-spectrograms is built to reconstruct waveforms from the outputs of the SCENT model. It is worth noting that our proposed method can achieve appropriate duration conversion, which is difficult in conventional methods. Experimental results show that our proposed method obtained better objective and subjective performance than the baseline methods using Gaussian mixture models and deep neural networks as acoustic models. This proposed method also outperformed our previous work, which achieved the top rank in Voice Conversion Challenge 2018. Ablation tests further confirmed the effectiveness of several components in our proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {631–644},
numpages = {14}
}

@article{10.1109/TASLP.2018.2883740,
author = {Zhang, Jiajun and Zhao, Yang and Li, Haoran and Zong, Chengqing},
title = {Attention With Sparsity Regularization for Neural Machine Translation and Summarization},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2883740},
doi = {10.1109/TASLP.2018.2883740},
abstract = {The attention mechanism has become the de facto standard component in neural sequence to sequence tasks, such as machine translation and abstractive summarization. It dynamically determines which parts in the input sentence should be focused on when generating each word in the output sequence. Ideally, only few relevant input words should be attended to at each decoding time step and the attention weight distribution should be sparse and sharp. However, previous methods have no good mechanism to control this attention weight distribution. In this paper, we propose a sparse attention model in which a sparsity regularization term is designed to augment the objective function. We explore two kinds of regularizations: $L_{infty }$-norm regularization and minimum entropy regularization, both of which aim to sharpen the attention weight distribution. Extensive experiments on both neural machine translation and abstractive summarization demonstrate that our proposed sparse attention model can substantially outperform the strong baselines. And the detailed analyses reveal that the final attention distribution indeed becomes sparse and sharp.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {507–518},
numpages = {12}
}

@article{10.1109/TASLP.2018.2889927,
author = {Zhu, Qiaoxi and Coleman, Philip and Qiu, Xiaojun and Wu, Ming and Yang, Jun and Burnett, Ian},
title = {Robust Personal Audio Geometry Optimization in the SVD-Based Modal Domain},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2889927},
doi = {10.1109/TASLP.2018.2889927},
abstract = {Personal audio generates sound zones in a shared space to provide private and personalized listening experiences with minimized interference between consumers. Regularization has been commonly used to increase the robustness of such systems against potential perturbations in the sound reproduction. However, the performance is limited by the system geometry such as the number and location of the loudspeakers and controlled zones. This paper proposes a geometry optimization method to find the most geometrically robust approach for personal audio amongst all available candidate system placements. The proposed method aims to approach the most “natural” sound reproduction so that the solo control of the listening zone coincidently accompanies the preferred quiet zone. Being formulated in the SVD-based modal domain, the method is demonstrated by applications in three typical personal audio optimizations, i.e., the acoustic contrast control, the pressure matching, and the planarity control. Simulation results show that the proposed method can obtain the system geometry with better avoidance of “occlusion,” improved robustness to regularization, and improved broadband equalization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {610–620},
numpages = {11}
}

@article{10.1109/TASLP.2018.2889606,
author = {Yi, Jiangyan and Tao, Jianhua and Wen, Zhengqi and Bai, Ye},
title = {Language-Adversarial Transfer Learning for Low-Resource Speech Recognition},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2889606},
doi = {10.1109/TASLP.2018.2889606},
abstract = {The acoustic model trained using the knowledge from the shared hidden layer SHL model outperforms the model trained only by using the target language, especially under low resource conditions. However, the shared features may contain some unnecessary language dependent information. It will degrade the performance of the target model. Therefore, this paper proposes language-adversarial transfer learning to alleviate this problem. Adversarial learning is used to ensure that the shared layers of the SHL-model can learn more language invariant features. Experiments are conducted on IARPA Babel datasets. The results show that the target model trained using the knowledge transferred from the adversarial SHL-model achieves up to 10.1% relative word error rate reduction when compared with the target model trained using the knowledge transferred from the SHL-model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {621–630},
numpages = {10}
}

@article{10.1109/TASLP.2018.2886743,
author = {Dietzen, Thomas and Spriet, Ann and Tirry, Wouter and Doclo, Simon and Moonen, Marc and van Waterschoot, Toon},
title = {Comparative Analysis of Generalized Sidelobe Cancellation and Multi-Channel Linear Prediction for Speech Dereverberation and Noise Reduction},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2886743},
doi = {10.1109/TASLP.2018.2886743},
abstract = {For blind speech dereverberation, two frameworks are commonly used: on the one hand, the multi-channel linear prediction MCLP framework, and on the other hand, data-dependent beamforming, e.g., the generalized sidelobe canceler GSC framework. The MCLP framework is designed to perform deconvolution and hence has gained increased prominence in blind speech dereverberation. The GSC framework is commonly used for noise reduction, but may be applied for dereverberation as well. In previous work, we have shown that for the noiseless case, MCLP and the GSC yield in theory mathematically equivalent results in terms of dereverberation. In this paper, we assume additional coherent as well as incoherent-noise components and formally analyze and compare both frameworks in terms of dereverberation and noise reduction performance. Both the theoretical analysis and time domain simulation results demonstrate that unlike the GSC, MCLP expectably shows limited performance in terms of noise reduction, while both perform equally well in terms of dereverberation, provided that the GSC blocking matrix achieves complete blocking of the early reverberant-speech component and sufficiently many microphones are available. In case of incomplete blocking, however, the GSC performs inferior to MCLP in terms of dereverberation, as shown in short-time Fourier transform domain simulations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {544–558},
numpages = {15}
}

@article{10.1109/TASLP.2019.2892412,
author = {Li, Xiaofei and Girin, Laurent and Gannot, Sharon and Horaud, Radu},
title = {Multichannel Speech Separation and Enhancement Using the Convolutive Transfer Function},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2892412},
doi = {10.1109/TASLP.2019.2892412},
abstract = {This paper addresses the problem of speech separation and enhancement from multichannel convolutive and noisy mixtures, assuming known mixing filters. We propose to perform speech separation and enhancement in the short-time Fourier transform domain using the convolutive transfer function CTF approximation. Compared to time-domain filters, the CTF has much less taps. Consequently, it requires less computational cost and sometimes is more robust against the filter perturbations. We propose three methods: 1 for the multisource case, the multichannel inverse filtering method, i.e., the multiple input/output inverse theorem MINT, is exploited in the CTF domain; 2 a beamforming-like multichannel inverse filtering method applying the single-source MINT and using power minimization, which is suitable whenever the source CTFs are not all known; and 3 a basis pursuit method, where the sources are recovered by minimizing their $ell _1$-norm to impose spectral sparsity, while the $ell _2$-norm fitting cost between microphone signals and mixing model is constrained to be lower than a tolerance. The noise can be reduced by setting this tolerance at the noise power level. Experiments under various acoustic conditions are carried out to evaluate and compare the three proposed methods. Comparison with four baseline methods—beamforming-based, two time-domain inverse filters, and time-domain Lasso—shows the applicability of the proposed methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {645–659},
numpages = {15}
}

@article{10.1109/TASLP.2018.2881536,
author = {Buchris, Yaakov and Amar, Alon and Benesty, Jacob and Cohen, Israel},
title = {Incoherent Synthesis of Sparse Arrays for Frequency-Invariant Beamforming},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2881536},
doi = {10.1109/TASLP.2018.2881536},
abstract = {Frequency-invariant beamformers are used to prevent signal waveform distortions in real world applications like audio, underwater acoustics, and radar. Most of existing methods assume uniform arrays, and only few consider sparse designs, which may lead to higher performance in terms of robustness and directivity factor. We propose an incoherent approach that first determines for each frequency bin a sparse set of sensors positions. Subsequently, by using tools of dimensionality reduction and clustering, these selections are merged together yielding the optimal sensors on a sparse array layout. We present design examples of sparse linear and planar superdirective array designs. We show that the proposed incoherent sparse design obtains superior performance in terms of white noise gain, directivity factor, and computational load compared to a uniform array design and compared to a coherent sparse approach, where the sensors’ locations and the beamformer coefficients are optimized simultaneously for all frequencies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {482–495},
numpages = {14}
}

@article{10.1109/TASLP.2018.2886739,
author = {Gao, Jianqing and Du, Jun and Chen, Enhong},
title = {Mixed-Bandwidth Cross-Channel Speech Recognition via Joint Optimization of DNN-Based Bandwidth Expansion and Acoustic Modeling},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2886739},
doi = {10.1109/TASLP.2018.2886739},
abstract = {Automatic speech recognition ASR systems are often built using scene related speech data due to large variations of transmission channels and sampling rates in different scenarios. In this study, we propose a general framework that establishes a unified model for diversified speech data with different sampling rates and channels. The framework is a joint optimization of deep neural network DNN-based bandwidth expansion and acoustic modeling to exploit a large amount of diversified training data. First, we design two novel DNN architectures to map the acoustic features from narrowband to wideband speech through direct mapping and progressive mapping. The learning targets of the direct mapping DNN DNN-DM are the acoustic features extracted from speech with the largest bandwidth, while the acoustic features from speech with all the other bandwidths are used as input. A progressive stacking network PSN gradually maps the features from the low sampling rates to the highest sampling rate through the design of intermediate target layers via multitask training. Then, in addition to these bandwidth expansion networks, we investigate several joint training strategies for DNN-based acoustic models. Our experiments conducted on three diversified large-scale Mandarin speech datasets with different recording channels and sampling rates 6, 8, and 16&nbsp;kHz show that the proposed unified model using PSN for bandwidth expansion not only is a more flexible and compact design than conventional multiple acoustic models with each bandwidth for a specific sampling rate, but also yields consistent and significant improvements over bandwidth-dependent models with an average relative word error rate reduction of 6.2%, indicating that the proposed model can fully utilize the diversified cross-channel speech data with multiple bandwidths. Moreover, the proposed methods are verified to be robust on different realistic scenes and can be effectively extended to a long short-term memory framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {559–571},
numpages = {13}
}

@article{10.1109/TASLP.2018.2888814,
author = {Deena, Salil and Hasan, Madina and Doulaty, Mortaza and Saz, Oscar and Hain, Thomas},
title = {Recurrent Neural Network Language Model Adaptation for Multi-Genre Broadcast Speech Recognition and Alignment},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2888814},
doi = {10.1109/TASLP.2018.2888814},
abstract = {Recurrent neural network language models RNNLMs generally outperform $n$-gram language models when used in automatic speech recognition ASR. Adapting RNNLMs to new domains is an open problem and current approaches can be categorised as either feature-based or model based. In feature-based adaptation, the input to the RNNLM is augmented with auxiliary features whilst model-based adaptation includes model fine-tuning and the introduction of adaptation layers in the network. In this paper, the properties of both types of adaptation are investigated on multi-genre broadcast speech recognition. Existing techniques for both types of adaptation are reviewed and the proposed techniques for model-based adaptation, namely the linear hidden network adaptation layer and the $K$-component adaptive the RNNLM, are investigated. Moreover, new features derived from the acoustic domain are investigated for the RNNLM adaptation. The contributions of this paper include two hybrid adaptation techniques: the fine-tuning of feature-based RNNLMs and a feature-based adaptation layer. Moreover, the semi-supervised adaptation of RNNLMs using genre information is also proposed. The ASR systems were trained using 700 h of multi-genre broadcast speech. The gains obtained when using the RNNLM adaptation techniques proposed in this paper are consistent when using RNNLMs trained on an in-domain set of 10M words and on a combination of in-domain and out-of-domain sets of 660 M words, with approx. $text{10}{%}$ perplexity and $text{2}{%}$ relative word error rate improvements on a 28.3 h. test set. The best RNNLM adaptation techniques for ASR are also evaluated on a lightly supervised alignment of subtitles task for the same data, where the use of RNNLM adaptation leads to an absolute increase in the F–measure of $text{0.5}{%}$.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {572–582},
numpages = {11}
}

@article{10.1109/TASLP.2018.2885775,
author = {Yang, Guang and He, Haibo and Chen, Qian},
title = {Emotion-Semantic-Enhanced Neural Network},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2885775},
doi = {10.1109/TASLP.2018.2885775},
abstract = {Although sentiment analysis on microblog posts has been studied in depth, sentiment analysis of posts is still challenging because of the limited contextual information that they normally contain. In microblog environments, emoticons are frequently used and they have clear emotional meanings. They are important emotional signals for microblog sentimental analysis. Existing studies typically use emoticons as noisy sentiment labels or similar sentiment indicators to effectively train classifier but overlook their emotional potentiality. We address this issue by constructing an emotional space as a feature representation matrix and projecting emoticons and words into the emotional space based on the semantic composition. To improve the performance of sentimental analysis, we propose a new emotion-semantic-enhanced convolutional neural network ECNN model. ECNN can use emoticon embedding as an emotional space projection operator. By projecting emoticons and words into an emoticon space, it can help identify subjectivity, polarity, and emotion in microblog environments. It is more capable of capturing emotion semantic than other models, so it can improve the sentiment analysis performance. The experimental results show that this model consistently outperforms other models on the dataset of several sentiment tasks. This paper provides insights on the design of ECNN for sentimental analysis in other natural language processing tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {531–543},
numpages = {13}
}

@article{10.1109/TASLP.2018.2882738,
author = {Gelderblom, Femke B. and Tronstad, Tron V. and Viggen, Erlend Magnus},
title = {Subjective Evaluation of a Noise-Reduced Training Target for Deep Neural Network-Based Speech Enhancement},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2882738},
doi = {10.1109/TASLP.2018.2882738},
abstract = {Speech enhancement systems aim to improve the quality and intelligibility of noisy speech. In this study, we compare two speech enhancement systems based on deep neural networks. The speech intelligibility and quality of both systems were evaluated subjectively by a speech recognition test based on Hagerman sentences and a translation of the ITU-T P.835 recommendation, respectively. Results were compared with the objective measures STOI and POLQA. Neither STOI nor POLQA reliably predicted subjective results. While STOI anticipated improvement, subjective results for both models showed degradation of speech intelligibility. POLQA results were overall hardly affected, while the subjective results showed significant changes in overall quality, both positive and negative, in many of the tests. One of the systems was trained to remove all noise; a strategy that is common in speech enhancement systems found in the literature. The other system was trained to only reduce the noise such that the signal-to-noise ratio increased with 10&nbsp;dB. The latter system subjectively outperformed the system that attempted to remove noise completely. From this, we conclude that objective evaluation cannot replace subjective evaluation until a measure that reliably predicts intelligibility and quality for deep-neural-network-based systems has been identified. Results further indicate that it may be beneficial to move away from more aggressive noise removal strategies toward noise reduction strategies that cause less speech distortion.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {583–594},
numpages = {12}
}

@article{10.1109/TASLP.2018.2882307,
author = {Moore, Alastair H. and Xue, Wei and Naylor, Patrick A. and Brookes, Mike},
title = {Noise Covariance Matrix Estimation for Rotating Microphone Arrays},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2882307},
doi = {10.1109/TASLP.2018.2882307},
abstract = {The noise covariance matrix computed between the signals from a microphone array is used in the design of spatial filters and beamformers with applications in noise suppression and dereverberation. This paper specifically addresses the problem of estimating the covariance matrix associated with a noise field when the array is rotating during desired source activity, as is common in head-mounted arrays. We propose a parametric model that leads to an analytical expression for the microphone signal covariance as a function of the array orientation and array manifold. An algorithm for estimating the model parameters during noise-only segments is proposed and the performance shown to be improved, rather than degraded, by array rotation. The stored model parameters can then be used to update the covariance matrix to account for the effects of any array rotation that occurs when the desired source is active. The proposed method is evaluated in terms of the Frobenius norm of the error in the estimated covariance matrix and of the noise reduction performance of a minimum variance distortionless response beamformer. In simulation experiments the proposed method achieves 18&nbsp;dB lower error in the estimated noise covariance matrix than a conventional recursive averaging approach and results in noise reduction which is within 0.05&nbsp;dB of an oracle beamformer using the ground truth noise covariance matrix.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {519–530},
numpages = {12}
}

@article{10.1109/TASLP.2018.2882913,
author = {Jahromi, Mohsen Zareian and Zahedi, Adel and Jensen, Jesper and Ostergaard, Jan},
title = {Information Loss in the Human Auditory System},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2882913},
doi = {10.1109/TASLP.2018.2882913},
abstract = {From the eardrum to the auditory cortex, where acoustic stimuli are decoded, there are several stages of auditory processing and transmission where information may potentially be lost. In this paper, we aim at quantifying the total information loss in the human auditory system by using information theoretic tools. To do so, we consider a speech communication model, where words are uttered and sent through a noisy channel, and then received and processed by a human listener. We define a notion of information loss that is related to the human word recognition rate. To assess the word recognition rate of humans, we conduct a closed-vocabulary intelligibility test. We derive upper and lower bounds on the information loss. Simulations reveal that the bounds are tight and we observe that the information loss in the human auditory system increases as the signal to noise ratio SNR decreases. Our framework also allows us to study whether humans are optimal in terms of speech perception in a noisy environment. Toward that end, we derive optimal classifiers and compare the human and machine performance in terms of information loss and word recognition rate. We observe a higher information loss and lower word recognition rate for humans compared to the optimal classifiers. In fact, depending on the SNR, the machine classifier may outperform humans by as much as 8 dB. This implies that for the speech-in-stationary-noise setup considered here, the human auditory system is suboptimal for recognizing noisy words.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {472–481},
numpages = {10}
}

@article{10.1109/TASLP.2018.2882731,
author = {Rahulamathavan, Yogachandran and Sutharsini, Kunaraj R and Ray, Indranil Ghosh and Lu, Rongxing and Rajarajan, Muttukrishnan},
title = {Privacy-Preserving IVector-Based Speaker Verification},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2882731},
doi = {10.1109/TASLP.2018.2882731},
abstract = {This paper introduces an efficient algorithm to develop a privacy-preserving voice verification based on iVector and linear discriminant analysis techniques. This research considers a scenario in which users enrol their voice biometric to access different services i.e., banking. Once enrolment is completed, users can verify themselves using their voice print instead of alphanumeric passwords. Since a voice print is unique for everyone, storing it with a third-party server raises several privacy concerns. To address this challenge, this paper proposes a novel technique based on randomization to carry out voice authentication, which allows the user to enrol and verify their voice in the randomized domain. To achieve this, the iVector-based voice verification technique has been redesigned to work on the randomized domain. The proposed algorithm is validated using a well-known speech dataset. The proposed algorithm neither compromises the authentication accuracy nor adds additional complexity due to the randomization operations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {496–506},
numpages = {11}
}

@article{10.1109/TASLP.2018.2885786,
author = {Luis Valero, Maria and Habets, Emanuel A. P.},
title = {Low-Complexity Multi-Microphone Acoustic Echo Control in the Short-Time Fourier Transform Domain},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2885786},
doi = {10.1109/TASLP.2018.2885786},
abstract = {Many modern communication and smart devices are equipped with several microphones, in addition to one or more loudspeakers. Each microphone not only acquires sounds produced in the near-end room, i.e., desired near-end speech, background noise, and other interferences, but also a far-end signal that is reproduced by the loudspeakers. This particular type of acoustic coupling, commonly denoted as acoustic echo, can be reduced in a distortionless manner by employing multi-microphone acoustic echo cancellation MM-AEC techniques. However, under noisy conditions, the performance of AEC is limited by the echo-to-noise ratio, and additional echo reduction is needed. Further, to ensure high-quality end-to-end communication in noisy environments, background noise has to be reduced as well. To achieve the latter, multi-microphone speech enhancement techniques, such as beamforming BF, are often used as they are capable of reducing undesired signal components while causing little distortion to the desired near-end speech. In spite of its high computational cost, the most effective solution to reduce acoustic echoes and background noise is to cascade MM-AEC and BF. In this work, a low-complexity multi-microphone echo controller is introduced, which not only combines low-complexity MM-AEC with BF, but also integrates residual echo reduction into the beamformer design.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {595–609},
numpages = {15}
}

@article{10.1109/TASLP.2018.2879399,
author = {Wu, Yiming and Li, Wei},
title = {Automatic Audio Chord Recognition With MIDI-Trained Deep Feature and BLSTM-CRF Sequence Decoding Model},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2879399},
doi = {10.1109/TASLP.2018.2879399},
abstract = {With the advances of machine learning technologies, data-driven feature extraction and sequence modeling approaches are being widely explored for automatic chord recognition tasks. Currently, there is a bottleneck in the amount of enough annotated data for training robust acoustic models, as hand-annotating time-synchronized chord labels requires professional musical skills and considerable labor. To cope with this limitation, in this paper, we propose a convolutional neural network CNN based deep feature extractor, which is trained on a large set of time, synchronized musical instrument digital interface audio data pairs and can robustly estimate pitch class activations of real-world music audio recordings. The CNN feature extractor plus a bidirectional long short-term memory conditional random field decoding model forms the proposed hybrid system for automatic chord recognition. Experiments show that the proposed model is compatible for both regular major/minor triad chord classification and larger vocabulary chord recognition, and outperforms other state-of-the-art chord recognition systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {355–366},
numpages = {12}
}

@article{10.1109/TASLP.2018.2881912,
author = {Wang, Zhong-Qiu and Wang, DeLiang},
title = {Combining Spectral and Spatial Features for Deep Learning Based Blind Speaker Separation},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2881912},
doi = {10.1109/TASLP.2018.2881912},
abstract = {This study tightly integrates complementary spectral and spatial features for deep learning based multi-channel speaker separation in reverberant environments. The key idea is to localize individual speakers so that an enhancement network can be trained on spatial as well as spectral features to extract the speaker from an estimated direction and with specific spectral structures. The spatial and spectral features are designed in a way such that the trained models are blind to the number of microphones and microphone geometry. To determine the direction of the speaker of interest, we identify time-frequency T-F units dominated by that speaker and only use them for direction estimation. The T-F unit level speaker dominance is determined by a two-channel chimera++ network, which combines deep clustering and permutation invariant training at the objective function level, and integrates spectral and interchannel phase patterns at the input feature level. In addition, T-F masking based beamforming is tightly integrated in the system by leveraging the magnitudes and phases produced by beamforming. Strong separation performance has been observed on reverberant talker-independent speaker separation, which separates reverberant speaker mixtures based on a random number of microphones arranged in arbitrary linear-array geometry.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {457–468},
numpages = {12}
}

@article{10.1109/TASLP.2018.2877909,
author = {Kolbaek, Morten and Tan, Zheng-Hua and Jensen, Jesper},
title = {On the Relationship Between Short-Time Objective Intelligibility and Short-Time Spectral-Amplitude Mean-Square Error for Speech Enhancement},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2877909},
doi = {10.1109/TASLP.2018.2877909},
abstract = {The majority of deep neural network&nbsp;DNN based speech enhancement algorithms rely on the mean-square error&nbsp;MSE criterion of short-time spectral amplitudes&nbsp;STSA, which has no apparent link to human perception, e.g., speech intelligibility. Short-time objective intelligibility&nbsp;STOI, a popular state-of-the-art speech intelligibility estimator, on the other hand, relies on linear correlation of speech temporal envelopes. This raises the question if a DNN training criterion based on envelope linear correlation&nbsp;ELC can lead to improved speech intelligibility performance of DNN-based speech enhancement algorithms compared to algorithms based on the STSA–MSE criterion. In this paper, we derive that, under certain general conditions, the STSA–MSE and ELC criteria are practically equivalent, and we provide empirical data to support our theoretical results. Furthermore, our experimental findings suggest that the standard STSA minimum-MSE estimator is near optimal, if the objective is to enhance noisy speech in a manner, which is optimal with respect to the STOI speech intelligibility estimator.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {283–295},
numpages = {13}
}

@article{10.1109/TASLP.2018.2879855,
author = {Imoto, Keisuke and Ono, Nobutaka},
title = {Acoustic Topic Model for Scene Analysis With Intermittently Missing Observations},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2879855},
doi = {10.1109/TASLP.2018.2879855},
abstract = {We propose a sophisticated method of acoustic scene analysis with intermittently missing observations, which analyzes acoustic scenes and restores missing observations simultaneously on the basis of the temporal correlation between acoustic words. One effective strategy for analyzing acoustic scenes is to characterize them as a combination of acoustic words. An acoustic topic model ATM is one of the techniques, which models the process generating multiple acoustic words. Here, an acoustic word corresponds to a sound category, while it has a homogenous time duration and is defined time frame by time frame. In the ATM, it is assumed that all acoustic words are observed, and therefore, it cannot be applied if any acoustic observations are missing. However, acoustic observations may sometimes be missing because of poor recording conditions, transmission loss, or privacy reasons. In the proposed method, focusing on the fact that acoustic words are temporally correlated, we consider the transition of acoustic words in two ways: First, by modeling the temporal transition of acoustic words directly using a Markov process and finally, by modeling the temporal transition of hidden states that generate acoustic words using a hidden Markov model. We then incorporate each transition model in a process generating acoustic words based on the ATM. The proposed method allows us to analyze acoustic scenes from acoustic words by restoring missing acoustic words. In our experiments, the proposed method exhibited a classification accuracy of acoustic scenes close to that for the case of no missing observations even when 50% of the observations were missing. Moreover, the model considering the hidden-state transition can classify acoustic scenes more accurately than the model considering the acoustic word transition directly.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {367–382},
numpages = {16}
}

@article{10.1109/TASLP.2018.2868428,
author = {Zhang, Yichi and Pardo, Bryan and Duan, Zhiyao},
title = {Siamese Style Convolutional Neural Networks for Sound Search by Vocal Imitation},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2868428},
doi = {10.1109/TASLP.2018.2868428},
abstract = {Conventional methods for finding audio in databases typically search text labels, rather than the audio itself. This can be problematic as labels may be missing, irrelevant to the audio content, or not known by users. Query by vocal imitation lets users query using vocal imitations instead. To do so, appropriate audio feature representations and effective similarity measures of imitations and original sounds must be developed. In this paper, we build upon our preliminary work to propose Siamese style convolutional neural networks to learn feature representations and similarity measures in a unified end-to-end training framework. Our Siamese architecture uses two convolutional neural networks to extract features, one from vocal imitations and the other from original sounds. The encoded features are then concatenated and fed into a fully connected network to estimate their similarity. We propose two versions of the system: IMINET is symmetric where the two encoders have an identical structure and are trained from scratch, while TL-IMINET is asymmetric and adopts the transfer learning idea by pretraining the two encoders from other relevant tasks: spoken language recognition for the imitation encoder and environmental sound classification for the original sound encoder. Experimental results show that both versions of the proposed system outperform a state-of-the-art system for sound search by vocal imitation, and the performance can be further improved when they are fused with the state of the art system. Results also show that transfer learning significantly improves the retrieval performance. This paper also provides insights to the proposed networks by visualizing and sonifying input patterns that maximize the activation of certain neurons in different layers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {429–441},
numpages = {13}
}

@article{10.1109/TASLP.2018.2878618,
author = {Koutrouvelis, Andreas I. and Hendriks, Richard C. and Heusdens, Richard and Jensen, Jesper},
title = {A Convex Approximation of the Relaxed Binaural Beamforming Optimization Problem},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2878618},
doi = {10.1109/TASLP.2018.2878618},
abstract = {The recently proposed relaxed binaural beamforming RBB optimization problem provides a flexible tradeoff between noise suppression and binaural-cue preservation of the sound sources in the acoustic scene. It minimizes the output noise power, under the constraints, which guarantee that the target remains unchanged after processing and the binaural-cue distortions of the acoustic sources will be less than a user-defined threshold. However, the RBB problem is a computationally demanding non convex optimization problem. The only existing suboptimal method which approximately solves the RBB is a successive convex optimization SCO method which, typically, requires to solve multiple convex optimization problems per frequency bin, in order to converge. Convergence is achieved when all constraints of the RBB optimization problem are satisfied. In this paper, we propose a semidefinite convex relaxation SDCR of the RBB optimization problem. The proposed suboptimal SDCR method solves a single convex optimization problem per frequency bin, resulting in a much lower computational complexity than the SCO method. Unlike the SCO method, the SDCR method does not guarantee user-controlled upper-bounded binaural-cue distortions. To tackle this problem, we also propose a suboptimal hybrid method that combines the SDCR and SCO methods. Instrumental measures combined with a listening test show that the SDCR and hybrid methods achieve significantly lower computational complexity than the SCO method, and in most cases better tradeoff between predicted intelligibility and binaural-cue preservation than the SCO method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {321–331},
numpages = {11}
}

@article{10.1109/TASLP.2018.2880607,
author = {Xiao, Ke and Wang, Supin and Wan, Mingxi and Wu, Liang},
title = {Reconstruction of Mandarin Electrolaryngeal Fricatives With Hybrid Noise Source},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2880607},
doi = {10.1109/TASLP.2018.2880607},
abstract = {The Mandarin electrolaryngeal EL speech is suffering from severe fricative confusion due to improper EL source in EL speech production and abnormal physiological structure of vocal tract in the laryngectomized condition. To reduce the fricative confusions, this paper proposes a hybrid noise source by combining the typical natural fricative sources and compensation sources that consider the acoustic defects in the frequency domain caused by the truncated vocal tract and abnormal source location in EL speech production. All parameters of the model are fricative-specific and the parameters of the compensation sources are determined by analyzing the vocal tract transfer functions before and after the laryngectomy. All five Mandarin fricatives are produced by laryngectomized subjects with an experimental EL system loading the hybrid noise source and the wideband noise source. The acoustic and perceptual features of these reconstructed EL fricatives are analyzed and evaluated by comparing with the conventional EL fricatives and normal fricatives. The results indicate that the hybrid noise source successfully improves the acoustic properties of the EL fricatives by forming better spectral shapes, raising the frequencies of average energy concentration, and producing better spectral skewness and kurtosis. Finally, due to these improvements of acoustic properties, the hybrid noise sources achieve much larger intelligibility for EL fricatives than the wideband noise source and the conventional EL source. Thus, the hybrid noise source is an effective, feasible, and promising method of reducing the severe fricative confusions and improving the intelligibility of EL speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {383–391},
numpages = {9}
}

@article{10.1109/TASLP.2018.2881925,
author = {Feng, Fangchen and Kowalski, Matthieu},
title = {Underdetermined Reverberant Blind Source Separation: Sparse Approaches for Multiplicative and Convolutive Narrowband Approximation},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2881925},
doi = {10.1109/TASLP.2018.2881925},
abstract = {We consider the problem of blind source separation for underdetermined convolutive mixtures. Based on the multiplicative narrowband approximation in the time-frequency domain with the help of the short-time-Fourier-transform STFT and the sparse representation of the source signals, we formulate the separation problem in an optimization framework. This framework is then generalized based on the recently investigated convolutive narrowband approximation and the statistics of the room impulse response. Algorithms with convergence proof are then employed to solve the proposed optimization problems. The evaluation of the proposed frameworks and algorithms for synthesized and live recorded mixtures are illustrated. The proposed approaches are also tested for mixtures with input noise. Numerical evaluations show the advantages of the proposed methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {442–456},
numpages = {15}
}

@article{10.1109/TASLP.2018.2878616,
author = {Pishdadian, Fatemeh and Pardo, Bryan},
title = {Multi-Resolution Common Fate Transform},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2878616},
doi = {10.1109/TASLP.2018.2878616},
abstract = {The multi-resolution common fate transform MCFT is an audio signal representation useful for representing mixtures of multiple audio signals that overlap in both time and frequency. The MCFT combines the invertibility of a state-of-the-art representation, the common fate transform CFT, and the multi-resolution property of the cortical stage output of an auditory model. Since the MCFT is computed based on a fully invertible complex time–frequency representation, separation of audio sources with high time–frequency overlap may be performed directly in the MCFT domain, where there is less overlap between sources than in the time–frequency domain. The MCFT circumvents the resolution issue of the CFT by using a multi-resolution two-dimensional 2D filter bank instead of fixed-size 2D windows. This enables higher quality separation without the need to hand-tune the window size to the specific case. In this work, we describe the MCFT, discuss the properties of the MCFT with the aid of illustrative examples, and provide definitions and objective measures for two desirable representation properties: separability of source signals and clusterability of components of each signal. The utility of the MCFT for source separation is illustrated by performing ideal masking on a comprehensive dataset of audio mixtures of musical tones played in unison, including audio samples from a wide pitch range and a variety of instruments/playing techniques. Results show that the ideal masks made in the MCFT domain yield better separability than those made in commonly used time–frequency signal representations as well as the CFT. The use of the MCFT also results in more reliable clusterability than the CFT in most cases.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {342–354},
numpages = {13}
}

@article{10.1109/TASLP.2018.2878384,
author = {Hansen, Martin Weiss and Jensen, Jesper Rindom and Christensen, Mads Graesboll},
title = {Estimation of Fundamental Frequencies in Stereophonic Music Mixtures},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2878384},
doi = {10.1109/TASLP.2018.2878384},
abstract = {In this paper, a method for multi-pitch estimation of stereophonic mixtures of harmonic signals, e.g., instrument recordings, is presented. The proposed method is based on a signal model that includes the panning parameters of the sources in a stereophonic mixture, such as those applied artificially in a recording studio. If the sources in a mixture have different panning parameters, this diversity can be used to simplify the pitch estimation problem. The mixing parameters of the sources might be shared, resulting in a multi-pitch estimation problem, which is solved using an approach based on an expectation–maximization algorithm for Gaussian sources, where the fundamental frequencies and model orders are estimated jointly. The fundamental frequencies may be related, resulting in overlapping harmonics, complicating the estimation of the parameters. A codebook of harmonic amplitude vectors is trained on recordings of instruments playing single notes, and used when estimating the amplitudes of the mixture components. The proposed method is evaluated using stereophonic mixtures of instrument recordings and is compared to state-of-the-art transcription and multi-pitch estimation methods. Experiments show an increase in performance when knowledge about the panning parameters is taken into account. The proposed method provides a full parameterization of the components of the observed signal. Possible applications include instrument tuning, audio editing tools, modification of harmonic mixture components, and audio effects.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {296–310},
numpages = {15}
}

@article{10.1109/TASLP.2018.2878381,
author = {Bao, Junwei and Tang, Duyu and Duan, Nan and Yan, Zhao and Zhou, Ming and Zhao, Tiejun},
title = {Text Generation From Tables},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2878381},
doi = {10.1109/TASLP.2018.2878381},
abstract = {This paper proposes a neural generative model, namely Table2Seq, to generate a natural language sentence based on a table. Specifically, the model maps a table to continuous vectors and then generates a natural language sentence by leveraging the semantics of a table. Since rare words, e.g., entities and values, usually appear in a table, we develop a flexible copying mechanism that selectively replicates contents from the table to the output sequence. We conduct extensive experiments to demonstrate the effectiveness of our Table2Seq model and the utility of the designed copying mechanism. On the WIKIBIO and SIMPLEQUESTIONS datasets, the Table2Seq model improves the state-of-the-art results from 34.70 to 40.26 and from 33.32 to 39.12 in terms of BLEU-4 scores, respectively. Moreover, we construct an open-domain dataset WIKITABLETEXT that includes 13&nbsp;318 descriptive sentences for 4962 tables. Our Table2Seq model achieves a BLEU-4 score of 38.23 on WIKITABLETEXT outperforming template-based and language model based approaches. Furthermore, through experiments on 1&nbsp;M table-query pairs from a search engine, our Table2Seq model considering the structured part of a table, i.e., table attributes and table cells, as additional information outperforms a sequence-to-sequence model considering only the sequential part of a table, i.e., table caption.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {311–320},
numpages = {10}
}

@article{10.1109/TASLP.2018.2880336,
author = {Zakeri, Vahid and Hodgson, Antony J.},
title = {Automatic Identification of Hard and Soft Bone Tissues by Analyzing Drilling Sounds},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2880336},
doi = {10.1109/TASLP.2018.2880336},
abstract = {Our purpose is to assess if bone drilling sounds could be used to automatically distinguish between hard- and soft-bone tissues. Such a capability could be of value in a variety of surgical procedures involving drilling through bone. We acquired sound signals from six bovine tibial bones that were being drilled with a surgical drill. We investigated various classifiers including logistic regression, support vector machine SVM, random forest RF, and hidden Markov model. We explored different time and frequency features, and considered two training/testing scenarios: leave-one bone-out LOBO and bone-specific BSP. Moreover, we conducted a survey of practicing surgeons to provide a baseline measure for assessing whether our proposed algorithm could improve the ability of surgeons to identify hard- and soft-bone tissues based on drilling sounds. The average accuracies for LOBO and BSP were 79.1% and 84.3%, respectively. In both scenarios, the feature that resulted in the highest accuracy of classification was wavelet packet transform coefficients. Moreover, RF and SVM produced the highest accuracies for LOBO and BSP, respectively. In the survey, the surgeons were unsure on 29.2% of questions, and had an average accuracy of 51.4% on their judged answers. In conclusion, the drilling in hard and soft bones could be automatically identified with good accuracy based on the drilling sounds. Several of the algorithms tested were generalizable across specimens and their accuracy significantly exceeded surgeons’ performance. The significance is that these algorithms can potentially be used to reduce challenges associated with bone-drilling procedures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {404–414},
numpages = {11}
}

@article{10.1109/TASLP.2018.2877894,
author = {Xiong, Feifei and Goetze, Stefan and Kollmeier, Birger and Meyer, Bernd T.},
title = {Joint Estimation of Reverberation Time and Early-To-Late Reverberation Ratio From Single-Channel Speech Signals},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2877894},
doi = {10.1109/TASLP.2018.2877894},
abstract = {The reverberation time&nbsp;RT and the early-to-late reverberation ratio&nbsp;ELR are two key parameters commonly used to characterize acoustic room environments. In contrast to conventional blind estimation methods that process the two parameters separately, we propose a model for joint estimation to predict the RT and the ELR simultaneously from single-channel speech signals from either full-band or sub-band frequency data, which is referred to as joint room parameter estimator&nbsp;jROPE. An artificial neural network is employed to learn the mapping from acoustic observations to the RT and the ELR classes. Auditory-inspired acoustic features obtained by temporal modulation filtering of the speech time-frequency representations are used as input for the neural network. Based on an in-depth analysis of the dependency between the RT and the ELR, a two-dimensional RT,&nbsp;ELR distribution with constrained boundaries is derived, which is then exploited to evaluate four different configurations for jROPE. Experimental results show that—in comparison to the single-task ROPE system which individually estimates the RT or the ELR—jROPE provides improved results for both tasks in various reverberant and diffuse noisy environments. Among the four proposed joint types, the one incorporating multi-task learning with shared input and hidden layers yields the best estimation accuracies on average. When encountering extreme reverberant conditions with RTs and ELRs lying beyond the derived RT,&nbsp;ELR distribution, the type considering RT and ELR as a joint parameter performs robustly, in particular. From state-of-the-art algorithms that were tested in the acoustic characterization of environments challenge, jROPE achieves comparable results among the best for all individual tasks RT and ELR estimation from full-band and sub-band signals.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {255–267},
numpages = {13}
}

@article{10.1109/TASLP.2018.2880317,
author = {Krishnan, Lakshmi and Betlehem, Terence and Teal, Paul D.},
title = {Fast Algorithms for Acoustic Impulse Response Shaping},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2880317},
doi = {10.1109/TASLP.2018.2880317},
abstract = {Impulse response shaping IRS is a prefiltering technique for modifying the characteristics of a linear channel to achieve desirable characteristics. Acoustic impulse response shaping is used to apply partial equalization to reduce the effects of reverberation on audio signals propagating inside a room and is thus used for listening room compensation. To be capable of responding rapidly to time varying room acoustics, a shaping algorithm must be computationally fast; this is difficult to achieve since the filters to be designed may have thousands of coefficients. This paper derives variants of the Dual Augmented Lagrangian Method, presents their application to the design of the shaping filters, and validates their efficacy through experimental studies. In addition, the formulations have been modified to design shaping filters that are robust to change in microphone positions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {392–403},
numpages = {12}
}

@article{10.1109/TASLP.2018.2878949,
author = {Hashimoto, Tetsuya and Saito, Daisuke and Minematsu, Nobuaki},
title = {Many-to-Many and Completely Parallel-Data-Free Voice Conversion Based on Eigenspace DNN},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2878949},
doi = {10.1109/TASLP.2018.2878949},
abstract = {Media conversion of image, text, speech, etc., generally requires a large amount of parallel data for training a conversion model. Recently, methods for training the model using no or a small amount of parallel data draw researchers’ attention. In many-to-many voice conversion, since it is often hard to collect parallel data from every pair of speakers, the conversion models requiring no parallel data are desired. Conventional many-to-many voice conversion models required a large amount of prestored parallel data to acquire prior knowledge of the entire speaker space. Then, a specific model from an arbitrary speaker to another can be realized by adapting a few model parameters. Although these conversion models certainly do not use parallel data in an adaptation step, they still use parallel data for prior training. In this study, we aim at realizing completely parallel-data-free and many-to-many voice conversion. The proposed method uses both Eigenvoice Gaussian mixture models EVGMM and Deep neural network DNN. EVGMM is a many-to-many conversion model that constructs the entire speaker space called eigenspace by analyzing mean vectors of Gaussian mixture models and it is used in our method to decompose training speakers’ features into their eigenspace components. By using the speaker features and the obtained components as pseudo parallel data, multiple DNNs are trained to realize conversion between them. With these DNNs, features of any target speaker can be represented by a weighted sum of the components. It should be noted that all the processes of our proposal do not require any parallel data. A key technique is to estimate covariance terms of EVGMM with no parallel data. Experiments indicate that individuality scores of the proposed method using no parallel data are comparable enough to those of a baseline system trained with parallel data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {332–341},
numpages = {10}
}

@article{10.1109/TASLP.2018.2881336,
author = {Bilbao, Stefan and Hamilton, Brian},
title = {Directional Sources in Wave-Based Acoustic Simulation},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2881336},
doi = {10.1109/TASLP.2018.2881336},
abstract = {Volumetric wave-based acoustic simulation relies on the complete solution to the three-dimensional wave equation over a spatial grid. Detailed modeling of sources, however, requires interpolation over the grid, which is complicated by the directional character of the source itself. In this paper, a new model of point sources of arbitrary directivity and location with respect to an underlying grid is presented. The model is framed in the spatio-temporal domain directly through the differentiation of Dirac distributions, leading to a spatial Fourier-based approximation strategy. Various approximants are presented, of both separable and nonseparable type, which allow for optimization over a specified wavenumber range. Such approximants are then employed in a finite difference time domain setting, yielding numerical results for sources of various types, which are then compared against exact solutions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {415–428},
numpages = {14}
}

@article{10.1109/TASLP.2018.2877465,
author = {Nakashika, Toru and Takaki, Shinji and Yamagishi, Junichi},
title = {Complex-Valued Restricted Boltzmann Machine for Speaker-Dependent Speech Parameterization From Complex Spectra},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2877465},
doi = {10.1109/TASLP.2018.2877465},
abstract = {This paper describes a novel energy-based probabilistic distribution that represents complex-valued data and explains how to apply it to direct feature extraction from complex-valued spectra. The proposed model, the complex-valued restricted Boltzmann machine CRBM, is designed to deal with complex-valued visible units as an extension of the well-known restricted Boltzmann machine RBM. Like the RBM, the CRBM learns the relationships between visible and hidden units without having connections between units in the same layer, which dramatically improves training efficiency by using Gibbs sampling or contrastive divergence. Another important characteristic is that the CRBM also has connections between real and imaginary parts of each of the complex-valued visible units that help represent the data distribution in the complex domain. In speech signal processing, classification and generation features are often based on amplitude spectra e.g., MFCC, cepstra, and mel-cepstra even if they are calculated from complex spectra, and they ignore phase information. In contrast, the proposed feature extractor using the CRBM directly encodes the complex spectra or another complex-valued representation of the complex spectra into binary-valued latent features hidden units. Since the visible-hidden connections are undirected, we can also recover decode the complex spectra from the latent features directly. Our speech representation experiments demonstrated that the CRBM outperformed other speech representation methods, such as methods using a conventional RBM, a mel-log spectrum approximate decoder, etc.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {244–254},
numpages = {11}
}

@article{10.1109/TASLP.2018.2877892,
author = {Stoter, Fabian-Robert and Chakrabarty, Soumitro and Edler, Bernd and Habets, Emanuel A. P.},
title = {CountNet: Estimating the Number of Concurrent Speakers Using Supervised Learning},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2877892},
doi = {10.1109/TASLP.2018.2877892},
abstract = {Estimating the maximum number of concurrent speakers from single-channel mixtures is a challenging problem and an essential first step to address various audio-based tasks such as blind source separation, speaker diarization, and audio surveillance. We propose a unifying probabilistic paradigm, where deep neural network architectures are used to infer output posterior distributions. These probabilities are in turn processed to yield discrete point estimates. Designing such architectures often involves two important and complementary aspects that we investigate and discuss. First, we study how recent advances in deep architectures may be exploited for the task of speaker count estimation. In particular, we show that convolutional recurrent neural networks outperform recurrent networks used in a previous study when adequate input features are used. Even for short segments of speech mixtures, we can estimate up to five speakers, with a significantly lower error than other methods. Second, through comprehensive evaluation, we compare the best-performing method to several baselines, as well as the influence of gain variations, different data sets, and reverberation. The output of our proposed method is compared to human performance. Finally, we give insights into the strategy used by our proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {268–282},
numpages = {15}
}

@article{10.1109/TASLP.2018.2884853,
author = {Hakkani-Tur, Dilek},
title = {Inaugural Editorial Innovations in an Era of Ubiquitous Audio, Speech, and Language Processing},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2884853},
doi = {10.1109/TASLP.2018.2884853},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {5–6},
numpages = {2}
}

@article{10.1109/TASLP.2018.2875325,
title = {Erratum for “Nonlinear Audio Systems Identification Through Audio Input Gaussianization”},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2875325},
doi = {10.1109/TASLP.2018.2875325},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {240},
numpages = {1}
}

@article{10.1109/TASLP.2018.2873897,
author = {MV, Achuth Rao and Ghosh, Prasanta Kumar},
title = {Glottal Inverse Filtering Using Probabilistic Weighted Linear Prediction},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2873897},
doi = {10.1109/TASLP.2018.2873897},
abstract = {Glottal inverse filtering is a noninvasive method for getting the glottal flow estimate from the speech. In this paper, we propose a method for glottal inverse filtering based on probabilistic weighted linear prediction PWLP in which the speech is assumed to be the output of an all-pole filter with glottal flow as an excitation. First, we introduce a probabilistic interpretation of the WLP, and we propose a probabilistic temporal weighting as convolution of a binary vector and a fixed window. We construct the posterior distribution based on the PWLP likelihood and a Gaussian prior on the filter coefficients. The parameters are estimated using the Gibbs sampling. The experiments are performed using the Lijencrants–Fant LF model based synthetic data, a physical model based synthetic data of different vowels and real speech data. Results demonstrate that the proposed method outperforms the best of the existing state-of-the-art methods in terms of the normalized amplitude quotient by 0.035 and 0.12 for the LF model and physical model based synthetic data, respectively. The results based on real speech data show that the glottal flow estimated by the proposed method in the closed phase is flatter and has less formant ripple compared to existing state-of-the-art methods. We also show two key features of the proposed method: first, the proposed method does not need prior detection of glottal closure or opening instants. The temporal weights are learnt in a data-driven manner, which is often found to be high near the closed phase of the glottal cycle, second, the Gaussian prior helps in estimating the filter coefficients when the closed phase duration is small.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {114–124},
numpages = {11}
}

@article{10.1109/TASLP.2018.2869692,
author = {Duong, Thanh Thi Hien and Duong, Ngoc Q. K. and Nguyen, Phuong Cong and Nguyen, Cuong Quoc and Nguyen, Phuong Cong and Duong, Ngoc Q. K. and Nguyen, Cuong Quoc and Duong, Thanh Thi Hien},
title = {Gaussian Modeling-Based Multichannel Audio Source Separation Exploiting Generic Source Spectral Model},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2869692},
doi = {10.1109/TASLP.2018.2869692},
abstract = {As blind audio source separation has remained very challenging in real-world scenarios, some existing works, including ours, have investigated the use of a weakly informed approach where generic source spectral models GSSM can be learned a priori based on nonnegative matrix factorization NMF. Such approach was derived for single-channel audio mixtures and shown to be efficient in different settings. This paper proposes a multichannel source separation approach where the GSSM is combined with the source spatial covariance model within a unified Gaussian modeling framework. We present the generalized expectation-minimization EM algorithm for the para-meter estimation. Especially, for guiding the estimation of the intermediate source variances in each EM iteration, we investigate the use of two criteria: First, the estimated variances of each source are constrained by NMF, and finally, the total variances of all sources are constrained by NMF altogether. While the former can be seen as a source variance denoising step, the latter is viewed as an additional separation step applied to the source variance. We demonstrate the speech separation performance, together with its convergence and stability with respect to parameter setting, of the proposed approach using a benchmark dataset provided within the 2016 Signal Separation Evaluation Campaign.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {32–43},
numpages = {12}
}

@article{10.1109/TASLP.2018.2869686,
author = {Zhang, Guoqiang and Tao, Jiancheng and Qiu, Xiaojun and Burnett, Ian and Burnett, Ian and Qiu, Xiaojun and Zhang, Guoqiang and Tao, Jiancheng},
title = {Decentralized Two-Channel Active Noise Control for Single Frequency by Shaping Matrix Eigenvalues},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2869686},
doi = {10.1109/TASLP.2018.2869686},
abstract = {In an active noise control ANC system, computational complexity is one major concern when designing practical control algorithms. For an ANC system with multiple secondary sources and error microphones, one approach to reducing computational complexity is to apply a decentralized control scheme rather than centralized approaches. A decentralized scheme attempts to control a number of small-size ANC subsystems independently. In this paper, we consider the decentralized control of a two-channel ANC system tackling a noise disturbance in the frequency domain, where each channel consists of one secondary source and one error microphone. We propose a decentralized control method that is able to achieve the same noise reduction performance as the centralized controller with guaranteed convergence. The key step in designing the control method is to properly shape the eigenvalues of a matrix that models the two-channel secondary paths for each frequency index.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {44–52},
numpages = {9}
}

@article{10.1109/TASLP.2018.2875170,
author = {Yu, Jianfei and Jiang, Jing and Xia, Rui},
title = {Global Inference for Aspect and Opinion Terms Co-Extraction Based on Multi-Task Neural Networks},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2875170},
doi = {10.1109/TASLP.2018.2875170},
abstract = {Extracting aspect terms and opinion terms are two fundamental tasks in opinion mining. The recent success of deep learning has inspired various neural network architectures, which have been shown to achieve highly competitive performance in these two tasks. However, most existing methods fail to explicitly consider the syntactic relations among aspect terms and opinion terms, which may lead to the inconsistencies between the model predictions and the syntactic constraints. To this end, we first apply a multi-task learning framework to implicitly capture the relations between the two tasks, and then propose a global inference method by explicitly modelling several syntactic constraints among aspect term extraction and opinion term extraction to uncover their intra-task and inter-task relationship, which seeks an optimal solution over the neural predictions for both tasks. Extensive evaluations on three benchmark datasets demonstrate that our global inference approach is able to bring consistent improvements over several base models in different scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {168–177},
numpages = {10}
}

@article{10.1109/TASLP.2018.2871755,
author = {Moriya, Takafumi and Tanaka, Tomohiro and Shinozaki, Takahiro and Watanabe, Shinji and Duh, Kevin},
title = {Evolution-Strategy-Based Automation of System Development for High-Performance Speech Recognition},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2871755},
doi = {10.1109/TASLP.2018.2871755},
abstract = {The state-of-the-art large vocabulary speech recognition systems consist of several components including hidden Markov model and deep neural network. To realize the highest recognition performance, numerous meta-parameters specifying the designs and training setups of these components must be optimized. A prominent obstacle in system development is the laborious effort required by human experts in tuning these meta-parameters. To automate the process, we propose to tune the meta-parameters of a whole large vocabulary speech recognition system using the evolution strategy with a multi-objective Pareto optimization. As the result of the evolution, the system is optimized for both low word error rate and compact model size. Since the approach requires repeated training and evaluation of the recognition systems that require large computation, we make use of parallel computation on cloud computers. Experimental results show the effectiveness of the proposed approach by discovering appropriate configuration for large vocabulary speech recognition systems automatically.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {77–88},
numpages = {12}
}

@article{10.1109/TASLP.2018.2876177,
author = {Laufer, Yaron and Gannot, Sharon},
title = {A Bayesian Hierarchical Model for Speech Enhancement With Time-Varying Audio Channel},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2876177},
doi = {10.1109/TASLP.2018.2876177},
abstract = {We present a fully Bayesian hierarchical approach for multichannel speech enhancement with time-varying audio channel. Our probabilistic approach relies on a Gaussian prior for the speech signal and a Gamma hyperprior for the speech precision, combined with a multichannel linear-Gaussian state-space model for the acoustic channel. Furthermore, we assume a Wishart prior for the noise precision matrix. We derive a variational expectation-maximization VEM algorithm that uses a variant of a multichannel Wiener filter MCWF to infer the sound source and a Kalman smoother to infer the acoustic channel. It is further shown that the VEM speech estimator can be recasted as a multichannel minimum variance distortionless response MVDR beamformer followed by a single-channel variational postfilter. The proposed algorithm was evaluated using both simulated and real room environments with several noise types and reverberation levels. Both static and dynamic scenarios are considered. In terms of speech quality, it is shown that a significant improvement is obtained with respect to the noisy signal, and that the proposed method outperforms a baseline algorithm. In terms of channel alignment and tracking ability, a superior channel estimate is demonstrated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {225–239},
numpages = {15}
}

@article{10.1109/TASLP.2018.2876172,
author = {Amini, Jamal and Hendriks, Richard C. and Heusdens, Richard and Guo, Meng and Jensen, Jesper},
title = {Asymmetric Coding for Rate-Constrained Noise Reduction in Binaural Hearing Aids},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2876172},
doi = {10.1109/TASLP.2018.2876172},
abstract = {Binaural hearing aids HAs can potentially perform advanced noise reduction algorithms, leading to an improvement over monaural/bilateral HAs. Due to the limited transmission capacities between the HAs and given knowledge of the complete joint noisy signal statistics, the optimal rate-constrained beamforming strategy is known from the literature. However, as these joint statistics are unknown in practice, sub-optimal strategies have been presented. In this paper, we present a unified framework to study the performance of these existing optimal and sub-optimal rate-constrained beamforming methods for binaural HAs. Moreover, we propose to use an asymmetric sequential coding scheme to estimate the joint statistics between the microphones in the two HAs. We show that under certain assumptions, this leads to sub-optimal performance in one HA but allows to obtain the truly optimal performance in the second HA. Based on the mean square error distortion measure, we evaluate the performance improvement between monaural beamforming no communication and the proposed scheme, as well as the optimal and the existing sub-optimal strategies in terms of the information bit-rate. The results show that the proposed method outperforms existing practical approaches in most scenarios, especially at middle rates and high rates, without having the prior knowledge of the joint statistics.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {154–167},
numpages = {14}
}

@article{10.1109/TASLP.2018.2874708,
author = {Sun, Yang and Wang, Wenwu and Chambers, Jonathon and Naqvi, Syed Mohsen},
title = {Two-Stage Monaural Source Separation in Reverberant Room Environments Using Deep Neural Networks},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2874708},
doi = {10.1109/TASLP.2018.2874708},
abstract = {Deep neural networks DNNs have been used for dereverberation and separation in the monaural source separation problem. However, the performance of current state-of-the-art methods is limited, particularly when applied in highly reverberant room environments. In this paper, we propose a two-stage approach with two DNN-based methods to address this problem. In the first stage, the dereverberation of the speech mixture is achieved with the proposed dereverberation mask DM. In the second stage, the dereverberant speech mixture is separated with the ideal ratio mask IRM. To realize this two-stage approach, in the first DNN-based method, the DM is integrated with the IRM to generate the enhanced time-frequency T-F mask, namely the ideal enhanced mask IEM, as the training target for the single DNN. In the second DNN-based method, the DM and the IRM are predicted with two individual DNNs. The IEEE and the TIMIT corpora with real room impulse responses and noise from the NOISEX dataset are used to generate speech mixtures for evaluations. The proposed methods outperform the state-of-the-art specifically in highly reverberant room environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {125–139},
numpages = {15}
}

@article{10.1109/TASLP.2018.2875269,
author = {Ngo, Gia H. and Nguyen, Minh and Chen, Nancy F.},
title = {Phonology-Augmented Statistical Framework for Machine Transliteration Using Limited Linguistic Resources},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2875269},
doi = {10.1109/TASLP.2018.2875269},
abstract = {Transliteration converts words in a source language e.g., English into words in a target language e.g., Vietnamese. This conversion considers the phonological structure of the target language, as the transliterated output needs to be pronounceable in the target language. For example, a word in Vietnamese that begins with a consonant cluster is phonologically invalid and thus would be an incorrect output of a transliteration system. Most statistical transliteration approaches, albeit being widely adopted, do not explicitly model the target language's phonology, which often results in invalid outputs. The problem is compounded by the limited linguistic resources available when converting foreign words to transliterated words in the target language. In this paper, we present a phonology-augmented statistical framework suitable for transliteration, especially when only limited linguistic resources are available. We propose the concept of pseudo-syllables as structures representing how segments of a foreign word are organized according to the syllables of the target language's phonology. We performed transliteration experiments on Vietnamese and Cantonese. We show that the proposed framework outperforms the statistical baseline by up to 44.68% relative, when there are limited training examples 587 entries.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {199–211},
numpages = {13}
}

@article{10.1109/TASLP.2018.2877258,
author = {Koizumi, Yuma and Saito, Shoichiro and Uematsu, Hisashi and Kawachi, Yuta and Harada, Noboru},
title = {Unsupervised Detection of Anomalous Sound Based on Deep Learning and the Neyman–Pearson Lemma},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2877258},
doi = {10.1109/TASLP.2018.2877258},
abstract = {This paper proposes a novel optimization principle and its implementation for unsupervised anomaly detection in sound ADS using an autoencoder AE. The goal of the unsupervised-ADS is to detect unknown anomalous sounds without training data of anomalous sounds. The use of an AE as a normal model is a state-of-the-art technique for the unsupervised-ADS. To decrease the false positive rate FPR, the AE is trained to minimize the reconstruction error of normal sounds, and the anomaly score is calculated as the reconstruction error of the observed sound. Unfortunately, since this training procedure does not take into account the anomaly score for anomalous sounds, the true positive rate TPR does not necessarily increase. In this study, we define an objective function based on the Neyman–Pearson lemma by considering the ADS as a statistical hypothesis test. The proposed objective function trains the AE to maximize the TPR under an arbitrary low FPR condition. To calculate the TPR in the objective function, we consider that the set of anomalous sounds is the complementary set of normal sounds and simulate anomalous sounds by using a rejection sampling algorithm. Through experiments using synthetic data, we found that the proposed method improved the performance measures of the ADS under low FPR conditions. In addition, we confirmed that the proposed method could detect anomalous sounds in real environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {212–224},
numpages = {13}
}

@article{10.1109/TASLP.2018.2876169,
author = {Wang, Zhong-Qiu and Zhang, Xueliang and Wang, DeLiang},
title = {Robust Speaker Localization Guided by Deep Learning-Based Time-Frequency Masking},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2876169},
doi = {10.1109/TASLP.2018.2876169},
abstract = {Deep learning-based time-frequency T-F masking has dramatically advanced monaural single-channel speech separation and enhancement. This study investigates its potential for direction of arrival DOA estimation in noisy and reverberant environments. We explore ways of combining T-F masking and conventional localization algorithms, such as generalized cross correlation with phase transform, as well as newly proposed algorithms based on steered-response SNR and steering vectors. The key idea is to utilize deep neural networks DNNs to identify speech dominant T-F units containing relatively clean phase for DOA estimation. Our DNN is trained using only monaural spectral information, and this makes the trained model directly applicable to arrays with various numbers of microphones arranged in diverse geometries. Although only monaural information is used for training, experimental results show strong robustness of the proposed approach in new environments with intense noise and room reverberation, outperforming traditional DOA estimation methods by large margins. Our study also suggests that the ideal ratio mask and its variants remain effective training targets for robust speaker localization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {178–188},
numpages = {11}
}

@article{10.1109/TASLP.2018.2869684,
author = {Magron, Paul and Virtanen, Tuomas and Magron, Paul and Virtanen, Tuomas},
title = {Complex ISNMF: A Phase-Aware Model for Monaural Audio Source Separation},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2869684},
doi = {10.1109/TASLP.2018.2869684},
abstract = {This paper introduces a phase-aware probabilistic model for audio source separation. Classical source models in the short-time Fourier transform domain use circularly-symmetric Gaussian or Poisson random variables. This is equivalent to assuming that the phase of each source is uniformly distributed, which is not suitable for exploiting the underlying structure of the phase. Drawing on preliminary works, we introduce here a Bayesian anisotropic Gaussian source model in which the phase is no longer uniform. Such a model permits us to favor a phase value that originates from a signal model through a Markov chain prior structure. The variance of the latent variables are structured with nonnegative matrix factorization NMF. The resulting model is called complex Itakura–Saito NMF ISNMF since it generalizes the ISNMF model to the case of nonisotropic variables. It combines the advantages of ISNMF, which uses a distortion measure adapted to audio and yields a set of estimates which preserve the overall energy of the mixture, and of complex NMF, which enables one to account for some phase constraints. We derive a generalized expectation-maximization algorithm to estimate the model parameters. Experiments conducted on a musical source separation task in a semiinformed setting show that the proposed approach outperforms state-of-the-art phase-aware separation techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {20–31},
numpages = {12}
}

@article{10.1109/TASLP.2018.2870742,
author = {Zheng, Naijun and Zhang, Xiao-Lei},
title = {Phase-Aware Speech Enhancement Based on Deep Neural Networks},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2870742},
doi = {10.1109/TASLP.2018.2870742},
abstract = {Short-time frequency transform STFT is fundamental in speech processing. Because of the difficulty of processing highly unstructured STFT phase, most speech-processing algorithms only operate with STFT magnitude, leaving the STFT phase far from explored. However, with the recent development of deep neural network DNN based speech processing, e.g., speech enhancement and recognition, phase processing is becoming more important than ever before as a new growing point of DNN-based methods. In this paper, we propose a phase-aware speech enhancement algorithm based on DNN. Specifically, in the training stage, when incorporating phase as a target, our core idea is to transform an unstructured phase spectrogram to its derivative along the time axis, i.e., instantaneous frequency deviation IFD, which has a similar structure with its corresponding magnitude spectrogram. We further propose to optimize both IFD and magnitude jointly in a multiobjective learning framework. In the test stage, we propose a postprocessing method to recover the phase spectrogram from the estimated IFD. Experimental results demonstrate the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {63–76},
numpages = {14}
}

@article{10.1109/TASLP.2018.2875794,
author = {Ferrer, Luciana and Nandwana, Mahesh Kumar and McLaren, Mitchell and Castan, Diego and Lawson, Aaron},
title = {Toward Fail-Safe Speaker Recognition: Trial-Based Calibration With a Reject Option},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2875794},
doi = {10.1109/TASLP.2018.2875794},
abstract = {The output scores of most of the speaker recognition systems are not directly interpretable as stand-alone values. For this reason, a calibration step is usually performed on the scores to convert them into proper likelihood ratios, which have a clear probabilistic interpretation. The standard calibration approach transforms the system scores using a linear function trained using data selected to closely match the evaluation conditions. This selection, though, is not feasible when the evaluation conditions are unknown. In previous work, we proposed a calibration approach for this scenario called trial-based calibration TBC. TBC trains a separate calibration model for each test trial using data that is dynamically selected from a candidate training set to match the conditions of the trial. In this work, we extend the TBC method, proposing: 1 a new similarity metric for selecting training data that result in significant gains over the one proposed in the original work; 2 a new option that enables the system to reject a trial when not enough matched data are available for training the calibration model; and 3 the use of regularization to improve the robustness of the calibration models trained for each trial. We test the proposed algorithms on a development set composed of several conditions and on the Federal Bureau of Investigation multi-condition speaker recognition dataset, and we demonstrate that the proposed approach reduces calibration loss to values close to 0 for most of the conditions when matched calibration data are available for selection, and that it can reject most of the trials for which relevant calibration data are unavailable.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {140–153},
numpages = {14}
}

@article{10.1109/TASLP.2018.2868407,
author = {Bao, Feng and Abdulla, Waleed H. and Bao, Feng and Abdulla, Waleed H.},
title = {A New Ratio Mask Representation for CASA-Based Speech Enhancement},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2868407},
doi = {10.1109/TASLP.2018.2868407},
abstract = {In the computational auditory scene analysis method, the ideal ratio mask or alternatively the ideal binary mask is the key point to reconstruct the enhanced signal. The ratio mask in its Wiener filtering or its square root form is currently considered. However, this kind of ratio mask overlooked one important issue. It does not exploit the inter-channel correlation ICC in the noisy speech, noise, and clean speech spectra. Thus, in this paper, we first propose a novel ratio mask representation by utilizing the ICC. In this way, we adaptively reallocate the power ratio of the speech and noise during the construction of ratio mask, thus, more speech and noise components are retained and masked at the same time, respectively. Second, the channel-weight contour based on the equal loudness hearing attribute is adopted to revise this new ratio mask in each Gammatone filterbank channel. Finally, the revised ratio mask is effectively used to train a five-layer structured deep neural network. Experiments show that the proposed ratio mask performs better than the conventional ratio mask representation and other series of enhancement algorithms in terms of speech quality, intelligibility, and spectral distortion under different signal to noise ratio conditions using six types of noises.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {7–19},
numpages = {13}
}

@article{10.1109/TASLP.2018.2876171,
author = {Tan, Ke and Chen, Jitong and Wang, DeLiang},
title = {Gated Residual Networks With Dilated Convolutions for Monaural Speech Enhancement},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2876171},
doi = {10.1109/TASLP.2018.2876171},
abstract = {For supervised speech enhancement, contextual information is important for accurate mask estimation or spectral mapping. However, commonly used deep neural networks DNNs are limited in capturing temporal contexts. To leverage long-term contexts for tracking a target speaker, we treat speech enhancement as a sequence-to-sequence mapping, and present a novel convolutional neural network CNN architecture for monaural speech enhancement. The key idea is to systematically aggregate contexts through dilated convolutions, which significantly expand receptive fields. The CNN model additionally incorporates gating mechanisms and residual learning. Our experimental results suggest that the proposed model generalizes well to untrained noises and untrained speakers. It consistently outperforms a DNN, a unidirectional long short-term memory LSTM model, and a bidirectional LSTM model in terms of objective speech intelligibility and quality metrics. Moreover, the proposed model has far fewer parameters than DNN and LSTM models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {189–198},
numpages = {10}
}

@article{10.1109/TASLP.2018.2870725,
author = {Zhao, Yan and Wang, Zhong-Qiu and Wang, DeLiang},
title = {Two-Stage Deep Learning for Noisy-Reverberant Speech Enhancement},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2870725},
doi = {10.1109/TASLP.2018.2870725},
abstract = {In real-world situations, speech reaching our ears is commonly corrupted by both room reverberation and background noise. These distortions are detrimental to speech intelligibility and quality, and also pose a serious problem to many speech-related applications, including automatic speech and speaker recognition. In order to deal with the combined effects of noise and reverberation, we propose a two-stage strategy to enhance corrupted speech, where denoising and dereverberation are conducted sequentially using deep neural networks. In addition, we design a new objective function that incorporates clean phase during model training to better estimate spectral magnitudes, which would in turn yield better phase estimates when combined with iterative phase reconstruction. The two-stage model is then jointly trained to optimize the proposed objective function. Systematic evaluations and comparisons show that the proposed algorithm improves objective metrics of speech intelligibility and quality substantially, and significantly outperforms previous one-stage enhancement systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {53–62},
numpages = {10}
}

@article{10.1109/TASLP.2018.2872128,
author = {Kavalekalam, Mathew Shaji and Nielsen, Jesper Kjar and Boldt, Jesper Bunsow and Christensen, Mads Grasboll},
title = {Model-Based Speech Enhancement for Intelligibility Improvement in Binaural Hearing Aids},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2872128},
doi = {10.1109/TASLP.2018.2872128},
abstract = {Speech intelligibility is often severely degraded among hearing impaired individuals in situations such as the cocktail party scenario. The performance of the current hearing aid technology has been observed to be limited in these scenarios. In this paper, we propose a binaural speech enhancement framework that takes into consideration the speech production model. The enhancement framework proposed here is based on the Kalman filter that allows us to take the speech production dynamics into account during the enhancement process. The usage of a Kalman filter requires the estimation of clean speech and noise short term predictor STP parameters, and the clean speech pitch parameters. In this work, a binaural codebook-based method is proposed for estimating the STP parameters, and a directional pitch estimator based on the harmonic model and maximum likelihood principle is used to estimate the pitch parameters. The proposed method for estimating the STP and pitch parameters jointly uses the information from left and right ears, leading to a more robust estimation of the filter parameters. Objective measures such as PESQ and STOI have been used to evaluate the enhancement framework in different acoustic scenarios representative of the cocktail party scenario. We have also conducted subjective listening tests on a set of nine normal hearing subjects, to evaluate the performance in terms of intelligibility and quality improvement. The listening tests show that the proposed algorithm, even with access to only a single channel noisy observation, significantly improves the overall speech quality, and the speech intelligibility by up to $text{15}{%}$.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {99–113},
numpages = {15}
}

@article{10.1109/TASLP.2018.2872106,
author = {Kamper, Herman and Shakhnarovich, Gregory and Livescu, Karen},
title = {Semantic Speech Retrieval With a Visually Grounded Model of Untranscribed Speech},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2872106},
doi = {10.1109/TASLP.2018.2872106},
abstract = {There is a growing interest in models that can learn from unlabelled speech paired with visual context. This setting is relevant for low-resource speech processing, robotics, and human language acquisition research. Here, we study how a visually grounded speech model, trained on images of scenes paired with spoken captions, captures aspects of semantics. We use an external image tagger to generate soft text labels from images, which serve as targets for a neural model that maps untranscribed speech to semantic keyword labels. We introduce a newly collected data set of human semantic relevance judgements and an associated task, semantic speech retrieval, where the goal is to search for spoken utterances that are semantically relevant to a given text query. Without seeing any text, the model trained on parallel speech and images achieves a precision of almost 60% on its top ten semantic retrievals. Compared to a supervised model trained on transcriptions, our model matches human judgements better by some measures, especially in retrieving non-verbatim semantic matches. We perform an extensive analysis of the model and its resulting representations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {89–98},
numpages = {10}
}

@article{10.1109/TASLP.2018.2879265,
author = {Li, H.},
title = {Farewell Editorial},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2879265},
doi = {10.1109/TASLP.2018.2879265},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2489},
numpages = {1}
}

@article{10.1109/TASLP.2018.2866707,
author = {Lin, Wei-wei and Mak, Man-Wai and Chien, Jen-Tzung},
title = {Multisource I-Vectors Domain Adaptation Using Maximum Mean Discrepancy Based Autoencoders},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2866707},
doi = {10.1109/TASLP.2018.2866707},
abstract = {Like many machine learning tasks, the performance of speaker verification SV systems degrades when training and test data come from very different distributions. What's more, both training and test data themselves could be composed of heterogeneous subsets. These multisource mismatches are detrimental to SV performance. This paper proposes incorporating maximum mean discrepancy MMD into the loss function of autoencoders to reduce these mismatches. MMD is a nonparametric method for measuring the distance between two probability distributions. With a properly chosen kernel, MMD can match up to infinite moments of data distributions. We generalize MMD to measure the discrepancies of multiple distributions. We call the generalized MMD domainwise MMD. Using domainwise MMD as an objective function, we propose two autoencoders, namely nuisance-attribute autoencoder NAE and domain-invariant autoencoder DAE, for multisource i-vector adaptation. NAE encodes the features that cause most of the multisource mismatch measured by domainwise MMD. DAE directly encodes the features that minimize the multisource mismatch. Using these MMD-based autoencoders as a preprocessing step for PLDA training, we achieve a relative improvement of 19.2% EER on the NIST 2016 SRE compared to PLDA without adaptation. We also found that MMD-based autoencoders are more robust to unseen domains. In the domain robustness experiments, MMD-based autoencoders show 6.8% and 5.2% improvements over IDVC on female and male Cantonese speakers, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2412–2422},
numpages = {11}
}

@article{10.1109/TASLP.2018.2867081,
author = {El Badawy, Dalia and Dokmanic, Ivan},
title = {Direction of Arrival With One Microphone, a Few LEGOs, and Non-Negative Matrix Factorization},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2867081},
doi = {10.1109/TASLP.2018.2867081},
abstract = {Conventional approaches to sound source localization require at least two microphones. It is known, however, that people with unilateral hearing loss can also localize sounds. Monaural localization is possible thanks to the scattering by the head, though it hinges on learning the spectra of the various sources. We take inspiration from this human ability to propose algorithms for accurate sound source localization using a single microphone embedded in an arbitrary scattering structure. The structure modifies the frequency response of the microphone in a direction-dependent way giving each direction a signature. While knowing those signatures is sufficient to localize sources of white noise, localizing speech is much more challenging: it is an ill-posed inverse problem, which we regularize by prior knowledge in the form of learned non-negative dictionaries. We demonstrate a monaural speech localization algorithm based on non-negative matrix factorization that does not depend on sophisticated, designed scatterers. In fact, we show experimental results with ad hoc scatterers made of LEGO bricks. Even with these rudimentary structures we can accurately localize arbitrary speakers; that is, we do not need to learn the dictionary for the particular speaker to be localized. Finally, we discuss multi-source localization and the related limitations of our approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2436–2446},
numpages = {11}
}

@article{10.1109/TASLP.2018.2865615,
author = {Martin-Morato, Irene and Cobos, Maximo and Ferri, Francesc J.},
title = {Adaptive Mid-Term Representations for Robust Audio Event Classification},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2865615},
doi = {10.1109/TASLP.2018.2865615},
abstract = {Low-level audio features are commonly used in many audio analysis tasks, such as audio scene classification or acoustic event detection. Due to the variable length of audio signals, it is a common approach to create fixed-length feature vectors consisting of a set of statistics that summarize the temporal variability of such short-term features. To avoid the loss of temporal information, the audio event can be divided into a set of mid-term segments or texture windows. However, such an approach requires to estimate accurately the onset and offset times of the audio events in order to obtain a robust mid-term statistical description of their temporal evolution. This paper proposes the use of an alternative event representation based on nonlinear time normalization prior to the extraction of mid-term statistics. The short-term features are transformed into a new fixed-length representation that considers uniform distance subsampling over a defined feature space in contrast to the classical short-term temporal framing. The results show that the use of distance-based texture windows provides an improved statistical description of the event robust to errors in the event segmentation stage under noisy conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2381–2392},
numpages = {12}
}

@article{10.1109/TASLP.2018.2852739,
author = {Lee, Hung-Yi and Chung, Pei-Hung and Wu, Yen-Chen and Lin, Tzu-Hsiang and Wen, Tsung-Hsien},
title = {Interactive Spoken Content Retrieval by Deep Reinforcement Learning},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2852739},
doi = {10.1109/TASLP.2018.2852739},
abstract = {For text content retrieval, the user can easily scan through and select from a list of retrieved items. This is impossible for spoken content retrieval, because the retrieved items are not easily displayed on-screen. In addition, due to the high degree of uncertainty for speech recognition, retrieval results can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval results before showing them to the user. For example, the machine can request extra information from the user, return a list of topics for the user to select from, and so on. In this paper, we propose using deep-Q-network DQN to determine the machine actions for interactive spoken content retrieval. DQN bypasses the need to estimate hand-crafted states, and directly determines the best action based on the present retrieval results even without any human knowledge. It is shown to achieve significantly better performance as compared with the previous hand-crafted states. We further find that double DQN and dueling DQN improve the naive version.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2447–2459},
numpages = {13}
}

@article{10.1109/TASLP.2018.2862641,
author = {Enzner, Gerald and Thune, Philipp},
title = {Bayesian MMSE Filtering of Noisy Speech by SNR Marginalization With Global PSD Priors},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2862641},
doi = {10.1109/TASLP.2018.2862641},
abstract = {MMSE filtering of speech with additive noise and latent speech power-spectral density PSD is addressed. This problem is strong in single-channel speech enhancement and restricts the utility of stationary Wiener filters or other statistical estimators based on PSDs. The issue typically manifests itself in residual noise after filtering, despite the availability of the noise PSD. Our paper therefore incorporates the latent speech PSD state via marginalization into the MMSE estimation framework of complex speech spectral amplitudes. The hence involved joint posterior distribution of the complex speech amplitude and speech PSD, conditioned on just the noisy observations, is then resolved in the Bayesian sense into a speech and a speech-PSD posterior. The latter is expressed via the local data likelihood and a hyper-prior of the local speech PSD or a-priori SNR—i.e., a global distribution across the entire speech signal. Marginalization, in this way, turns into expectation over a latent Wiener filter, such that explicit estimation of local a-priori SNR is eliminated. The local input data in the form of the a-posteriori SNR and the global SNR value as a descriptor of the overall speech-in-noise condition turns out sufficient to control our resulting MMSE spectral gain function, and, potentially, can be provided much easier than the latent and time-varying a-priori SNR. An improved balance of residual noise and speech quality in the enhancement of noisy speech is demonstrated by objective experimental evaluation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2289–2304},
numpages = {16}
}

@article{10.1109/TASLP.2018.2864535,
author = {Surendran, Sudeep and Kumar, T. Kishore},
title = {Oblique Projection and Cepstral Subtraction in Signal Subspace Speech Enhancement for Colored Noise Reduction},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2864535},
doi = {10.1109/TASLP.2018.2864535},
abstract = {In this paper, a subspace speech enhancement method handling the case of colored noise using oblique projection in the cepstral domain is proposed. Perceptual features and variance normalization are used to reduce the residual noise and improve the intelligibility of the output speech. Initially, the additive noise present in the noisy speech is removed by removing the orthogonal noise subspace from the noisy speech subspace to obtain the speech subspace. Then, the oblique projection of the noise subspace on the speech subspace along the additive noise subspace is used to determine the colored noise that remains. Colored noise removal is performed by power spectral subtraction in the cepstral domain. The spectral domain constrained estimator that incorporates the combined masking property of the human auditory system is employed to estimate the clean speech signal using the variance of the colored noise. To avoid the occurrence of any abrupt spikes in the output, variance normalization is performed by adaptively changing the control parameter of the estimator's gain matrix. The spectrograms, the objective measures and the subjective intelligibility test show the superior performance of the proposed method over the other existing speech enhancement methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2328–2340},
numpages = {13}
}

@article{10.1109/TASLP.2018.2868416,
author = {Bao, Yu and Chen, Huawei},
title = {A Chance-Constrained Programming Approach to the Design of Robust Broadband Beamformers With Microphone Mismatches},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2868416},
doi = {10.1109/TASLP.2018.2868416},
abstract = {This paper addresses the design of robust broadband beamformers by taking into account the prior knowledge of uncertain means and variances of microphone mismatches. A chance-constrained programming CCP based design approach is proposed, which is a generic extension of the existing worst case mean performance optimization WMPO based counterpart by using a chance-constrained stopband level constraint instead of a hard one. The original CCP-based design problem, however, is NP-hard to solve. To deal with the problem, a relaxed convex optimization formulation is developed by exploiting the distribution property of array response distortion due to microphone mismatches. The relation between the proposed CCP- and WMPO-based design approaches is analyzed theoretically. It shows that there always exists a chance constraint probability under which the CCP-based design is comparable to the WMPO-based one, which provides an insight into the overconservatism problem in the WMPO-based design from the perspective of the CCP-based framework. Moreover, a criterion on the setting of the chance constraint probability is also derived for the proposed design approach in order to remedy the overconservatism problem. Simulation and real experimental results are presented to show the superior performance of the proposed design approach compared with its state-of-the-art counterparts.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2475–2488},
numpages = {14}
}

@article{10.1109/TASLP.2018.2865091,
author = {Firtha, Gergely and Fiala, Peter and Schultz, Frank and Spors, Sascha and Fiala, Peter and Schultz, Frank and Spors, Sascha and Firtha, Gergely},
title = {On the General Relation of Wave Field Synthesis and Spectral Division Method for Linear Arrays},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2865091},
doi = {10.1109/TASLP.2018.2865091},
abstract = {Sound field synthesis aims at the reproduction of an arbitrary target sound field over an extended listening area applying a densely spaced loudspeaker ensemble. Two basic analytic methodologies—the explicit and the implicit—exist in order to derive the required loudspeaker driving functions. The explicit solution aims at the direct solution of the involved integral equation describing the general sound field synthesis problem, resulting in driving functions in the form of a spectral integral. The implicit solution extracts the driving function from an appropriate boundary integral representation of the target sound field. So far the relationship between two approaches was investigated for target field specific synthesis scenarios. For linear arrays this paper introduces a high-frequency approximation for the explicit solution resulting in a novel, purely spatial domain formulation of the direct approach. The presented driving functions allow the synthesis of an arbitrary virtual sound field, optimizing the reproduction on an arbitrary reference line. It is furthermore shown that for an arbitrary virtual sound field, the implicit solution constitutes a high-frequency approximation of the explicit method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2393–2403},
numpages = {11}
}

@article{10.1109/TASLP.2018.2864577,
author = {Zhang, Wen and Hofmann, Christian and Buerger, Michael and Abhayapala, Thushara Dheemantha and Kellermann, Walter},
title = {Spatial Noise-Field Control With Online Secondary Path Modeling: A Wave-Domain Approach},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2864577},
doi = {10.1109/TASLP.2018.2864577},
abstract = {Due to strong interchannel interference in multichannel active noise control ANC, there are fundamental problems associated with the filter adaptation and online secondary path modeling remains a major challenge. This paper proposes a wave-domain adaptation algorithm for multichannel ANC with online secondary path modelling to cancel tonal noise over an extended region of two-dimensional plane in a reverberant room. The design is based on exploiting the diagonal-dominance property of the secondary path in the wave domain. The proposed wave-domain secondary path model is applicable to both concentric and nonconcentric circular loudspeakers and microphone array placement, and is also robust against array positioning errors. Normalized least mean squares-type algorithms are adopted for adaptive feedback control. Computational complexity is analyzed and compared with the conventional time-domain and frequency-domain multichannel ANCs. Through simulation-based verification in comparison with existing methods, the proposed algorithm demonstrates more efficient adaptation with low-level auxiliary noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2355–2370},
numpages = {16}
}

@article{10.1109/TASLP.2018.2865609,
author = {Birkholz, Peter and Stone, Simon and Wolf, Klaus and Plettemeier, Dirk and Wolf, Klaus and Plettemeier, Dirk and Stone, Simon and Birkholz, Peter},
title = {Non-Invasive Silent Phoneme Recognition Using Microwave Signals},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2865609},
doi = {10.1109/TASLP.2018.2865609},
abstract = {Besides the recognition of audible speech, there is currently an increasing interest in the recognition of silent speech, which has a range of novel applications. A major obstacle for a wide spread of silent-speech technology is the lack of measurement methods for speech movements that are convenient, non-invasive, portable, and robust at the same time. Therefore, as an alternative to established methods, we examined to what extent different phonemes can be discriminated from the electromagnetic transmission and reflection properties of the vocal tract. To this end, we attached two Vivaldi antennas on the cheek and below the chin of two subjects. While the subjects produced 25 phonemes in multiple phonetic contexts each, we measured the electromagnetic transmission spectra from one antenna to the other, and the reflection spectra for each antenna radar, in a frequency band from 2–12&nbsp;GHz. Two classification methods k-nearest neighbors and linear discriminant analysis were trained to predict the phoneme identity from the spectral data. With linear discriminant analysis, cross-validated phoneme recognition rates of 93% and 85% were achieved for the two subjects. Although these results are speaker- and session-dependent, they suggest that electromagnetic transmission and reflection measurements of the vocal tract have great potential for future silent-speech interfaces.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2404–2411},
numpages = {8}
}

@article{10.1109/TASLP.2018.2860287,
author = {Wang, Xing and Tu, Zhaopeng and Zhang, Min},
title = {Incorporating Statistical Machine Translation Word Knowledge Into Neural Machine Translation},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2860287},
doi = {10.1109/TASLP.2018.2860287},
abstract = {Neural machine translation NMT has gained more and more attention in recent years, mainly due to its simplicity yet state-of-the-art performance. However, previous research has shown that NMT suffers from several limitations: source coverage guidance, translation of rare words, and the limited vocabulary, while statistical machine translation SMT has complementary properties that correspond well to these limitations. It is straightforward to improve the translation performance by combining the advantages of two kinds of models. This paper proposes a general framework for incorporating the SMT word knowledge into NMT to alleviate above word-level limitations. In our framework, the NMT decoder makes more accurate word prediction by referring to the SMT word recommendations in both training and testing phases. Specifically, the SMT model offers informative word recommendations based on the NMT decoding information. Then, we use the SMT word predictions as prior knowledge to adjust the NMT word generation probability, which unitizes a neural network based classifier to digest the discrete word knowledge. In this paper, we use two model variants to implement the framework, one with a gating mechanism and the other with a direct competition mechanism. Experimental results on Chinese-to-English and English-to-German translation tasks show that the proposed framework can take advantage of the SMT word knowledge and consistently achieve significant improvements over NMT and SMT baseline systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2255–2266},
numpages = {12}
}

@article{10.1109/TASLP.2018.2862826,
author = {Huang, Gongping and Chen, Jingdong and Benesty, Jacob},
title = {Insights Into Frequency-Invariant Beamforming With Concentric Circular Microphone Arrays},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2862826},
doi = {10.1109/TASLP.2018.2862826},
abstract = {This paper studies the problem of frequency-invariant beamforming with concentric circular microphone arrays CCMAs and presents an approach to the design of frequency-invariant and symmetric beampatterns. We first apply the Jacobi-Anger expansion to each ring of the CCMA to approximate the beampattern. The beamformer is then designed by using all the expansions from different rings. In comparison with the existing work in the literature where a Jacobi-Anger expansion of the same order is applied to different rings, here in this contribution the order of the Jacobi-Anger expansion at a ring is related to its number of sensors and, as a result, the expansion order at different rings may be different. The developed approach is rather general. It is not only able to mitigate the deep nulls problem in the directivity factor and the white noise gain, that is common to circular microphone arrays CMAs, and improve the steering flexibility, but is also flexible to use in practice where a smaller ring can have less microphones than a larger one. We discuss the conditions for the design of $N$th-order symmetric beampatterns and examples of frequency-invariant beampatterns with commonly used array geometries such as CMAs, CMAs with a sensor at the center, and CCMAs. We show the advantage of adding one microphone at the center of either a CMA or a CCMA, i.e., circumventing the deep nulls problem caused by the 0th-order Bessel function.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2305–2318},
numpages = {14}
}

@article{10.1109/TASLP.2018.2867099,
author = {Abdelwahab, Mohammed and Busso, Carlos},
title = {Domain Adversarial for Acoustic Emotion Recognition},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2867099},
doi = {10.1109/TASLP.2018.2867099},
abstract = {The performance of speech emotion recognition is affected by the differences in data distributions between train source domain and test target domain sets used to build and evaluate the models. This is a common problem, as multiple studies have shown that the performance of emotional classifiers drops when they are exposed to data that do not match the distribution used to build the emotion classifiers. The difference in data distributions becomes very clear when the training and testing data come from different domains, causing a large performance gap between development and testing performance. Due to the high cost of annotating new data and the abundance of unlabeled data, it is crucial to extract as much useful information as possible from the available unlabeled data. This study looks into the use of adversarial multitask training to extract a common representation between train and test domains. The primary task is to predict emotional-attribute-based descriptors for arousal, valence, or dominance. The secondary task is to learn a common representation, where the train and test domains cannot be distinguished. By using a gradient reversal layer, the gradients coming from the domain classifier are used to bring the source and target domain representations closer. We show that exploiting unlabeled data consistently leads to better emotion recognition performance across all emotional dimensions. We visualize the effect of adversarial training on the feature representation across the proposed deep learning architecture. The analysis shows that the data representations for the train and test domains converge as the data are passed to deeper layers of the network. We also evaluate the difference in performance when we use a shallow neural network versus a deep neural network and the effect of the number of shared layers used by the task and domain classifiers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2423–2435},
numpages = {13}
}

@article{10.1109/TASLP.2018.2862353,
author = {Meynard, Adrien and Torresani, Bruno},
title = {Spectral Analysis for Nonstationary Audio},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2862353},
doi = {10.1109/TASLP.2018.2862353},
abstract = {A new approach for the analysis of nonstationary signals is proposed, with a focus on audio applications. Following earlier contributions, nonstationarity is modeled via stationarity-breaking operators acting on Gaussian stationary random signals. The focus is on time warping and amplitude modulation, and an approximate maximum-likelihood approach based on suitable approximations in the wavelet transform domain is developed. This paper provides theoretical analysis of the approximations, and introduces joint estimation of frequency, amplitude, and spectrum, a corresponding estimation algorithm. The latter is tested and validated on synthetic as well as real audio signal.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2371–2380},
numpages = {10}
}

@article{10.1109/TASLP.2018.2860682,
author = {Zhao, Yunxin and Kuruvilla-Dugdale, Mili and Song, Minguang},
title = {Structured Sparse Spectral Transforms and Structural Measures for Voice Conversion},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2860682},
doi = {10.1109/TASLP.2018.2860682},
abstract = {We investigate a structured sparse spectral transform method for voice conversion VC to perform frequency warping and spectral shaping simultaneously on high-dimensional D STRAIGHT spectra. Learning a large transform matrix for high-D data often results in an overfit matrix with low sparsity, which leads to muffled speech in VC. We address this problem by using the frequency-warping characteristic of a source–target speaker pair to define a region of support ROS in a transform matrix, and further optimize it by nonnegative matrix factorization NMF to obtain structured sparse transform. We also investigate structural measures of spectral and temporal covariance and variance at different scales for assessing VC speech quality. Our experiments on ARCTIC dataset of 12 speaker pairs show that embedding the ROS in spectral transforms offers flexibility in tradeoffs between spectral distortion and structure preservation, and the structural measures provide quantitatively reasonable results on converted speech. Our subjective listening tests show that the proposed VC method achieves a mean opinion score of “very good” relative to natural speech, and in comparison with three other VC methods, it is the most preferred one in naturalness and in voice similarity to target speakers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2267–2276},
numpages = {10}
}

@article{10.1109/TASLP.2018.2864648,
author = {Li, Qiang and Wong, Derek F. and Chao, Lidia S. and Zhu, Muhua and Xiao, Tong and Zhu, Jingbo and Zhang, Min},
title = {Linguistic Knowledge-Aware Neural Machine Translation},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2864648},
doi = {10.1109/TASLP.2018.2864648},
abstract = {Recently, researchers have shown an increasing interest in incorporating linguistic knowledge into neural machine translation NMT. To this end, previous works choose either to alter the architecture of NMT encoder to incorporate syntactic information into the translation model, or to generalize the embedding layer of the encoder to encode additional linguistic features. The former approach mainly focuses on injecting the syntactic structure of the source sentence into the encoding process, leading to a complicated model that lacks the flexibility to incorporate other types of knowledge. The latter extends word embeddings by considering additional linguistic knowledge as features to enrich the word representation. It thus does not explicitly balance the contribution from word embeddings and the contribution from additional linguistic knowledge. To address these limitations, this paper proposes a knowledge-aware NMT approach that models additional linguistic features in parallel to the word feature. The core idea is that we propose modeling a series of linguistic features at the word level knowledge block using a recurrent neural network RNN. And in sentence level, those word-corresponding feature blocks are further encoded using a RNN encoder. In decoding, we propose a knowledge gate and an attention gate to dynamically control the proportions of information contributing to the generation of target words from different sources. Extensive experiments show that our approach is capable of better accounting for importance of additional linguistic, and we observe significant improvements from 1.0 to 2.3 BLEU points on Chinese$leftrightarrow$ English and English$rightarrow$German translation tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2341–2354},
numpages = {14}
}

@article{10.1109/TASLP.2018.2842432,
author = {Shen, Shi-qi and Chen, Yun and Yang, Cheng and Liu, Zhi-yuan and Sun, Mao-song},
title = {Zero-Shot Cross-Lingual Neural Headline Generation},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2842432},
doi = {10.1109/TASLP.2018.2842432},
abstract = {Neural headline generation NHG has been proven to be effective in generating a fully abstractive headline recently. Existing NHG systems are only capable of producing headline of the same language as the original document. Cross lingual headline generation is an important task since it provides an efficient way to understand the key point of a document in a different language. Due to the lack of those parallel corpora of direct source language articles and target language headlines, we propose to deal with the cross-lingual neural headline generation CNHG under the zero-shot scenario. A trivial solution is to translate and summarize the source document in a pipeline way. However, a pipeline solution will lead to error propagation in the translation and summarization phases. This challenge motivates us to build a direct source-to-target CNHG model based on existing parallel corpora of translation and monolingual headline generation. Specifically, we let a parameterized CNHG model student model mimic the output of a pretrained translation or headline generation model teacher model. To the best of our knowledge, this is the first effort to address CNHG problem. Besides, we construct English–Chinese headline generation evaluation datasets by manual translation. Experimental results on English-to-Chinese cross-lingual headline generation demonstrate that our proposed method significantly outperforms the baseline models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2319–2327},
numpages = {9}
}

@article{10.1109/TASLP.2018.2860786,
author = {Salehi, Haniyeh and Suelzle, David and Folkeard, Paula and Parsa, Vijay},
title = {Learning-Based Reference-Free Speech Quality Measures for Hearing Aid Applications},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2860786},
doi = {10.1109/TASLP.2018.2860786},
abstract = {Objective measures of speech quality are highly desirable in benchmarking and monitoring the performance of hearing aids HAs. Existing HA speech quality indices such as the hearing aid speech quality index HASQI are intrusive in that they require a properly time-aligned and frequency-shaped reference signal to predict the quality of HA output. Two new reference-free HA speech quality indices are proposed in this paper, based on a model that amalgamates perceptual linear prediction PLP, hearing loss HL modeling, and machine learning concepts. For the first index, HL-modified PLP coefficients and their statistics were used as the feature set, which was subsequently mapped to the predicted quality scores using support vector regression SVR. For the second index, HL-impacted gammatone auditory filterbank energies and their second-order statistics constituted the feature set, which was again mapped using SVR. Two databases involving HA recordings were collected and utilized for the evaluation of the robustness and generalizability of the two indices. Experimental results showed that the index based on the gammatone filterbank energies not only correlated well with HA quality ratings by hearing impaired listeners, but also exhibited robust performance across different test conditions and was comparable to the full-reference HASQI performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2277–2288},
numpages = {12}
}

@article{10.1109/TASLP.2018.2867947,
author = {Elshamy, Samy and Madhu, Nilesh and Tirry, Wouter and Fingscheidt, Tim},
title = {DNN-Supported Speech Enhancement With Cepstral Estimation of Both Excitation and Envelope},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2867947},
doi = {10.1109/TASLP.2018.2867947},
abstract = {In this paper, we propose and compare various techniques for the estimation of clean spectral envelopes in noisy conditions. The source-filter model of human speech production is employed in combination with a hidden Markov model and/or a deep neural network approach to estimate clean envelope-representing coefficients in the cepstral domain. The cepstral estimators for speech spectral envelope-based noise reduction are both evaluated alone and also in combination with the recently introduced cepstral excitation manipulation CEM technique for a priori SNR estimation in a noise reduction framework. Relative to the classical MMSE short time spectral amplitude estimator, we obtain more than 2&nbsp;dB higher noise attenuation, and relative to our recent CEM technique still 0.5&nbsp;dB more, in both cases maintaining the quality of the speech component and obtaining considerable SNR improvement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2460–2474},
numpages = {15}
}

@article{10.1109/TASLP.2018.2851157,
author = {Zhang, Jie and Heusdens, Richard and Hendriks, Richard Christian},
title = {Rate-Distributed Spatial Filtering Based Noise Reduction in Wireless Acoustic Sensor Networks},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851157},
doi = {10.1109/TASLP.2018.2851157},
abstract = {In wireless acoustic sensor networks WASNs, sensors typically have a limited energy budget as they are often battery-driven. Energy efficiency is, therefore, essential for the design of algorithms in WASNs. One way to reduce energy costs is to select only the sensors that are most informative, a problem known as sensor selection . In this way, only sensors that significantly contribute to the task at hand will be involved. In this paper, we consider a more general approach, which is based on rate-distributed spatial filtering. Depending on the distance over which a transmission takes place, the bit rate directly influences the energy consumption. We try to minimize the battery usage due to transmission, while constraining the noise reduction performance. This results in an efficient rate allocation strategy, which depends on the underlying signal statistics, as well as the distance from sensors to a fusion center FC. Through the utilization of a linearly constrained minimum variance beamformer, the problem is derived as a semidefinite program. Furthermore, we show that rate allocation is more general than sensor selection, and sensor selection can be seen as a special case of the presented rate-allocation solution, e.g., the best microphone subset can be determined by thresholding the rates. Finally, numerical simulations for estimating several target sources in a WASN demonstrate that the proposed method outperforms the sensor-selection-based approaches in terms of energy usage, and we find that the sensors close to the FC and point sources are allocated with higher rates.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2015–2026},
numpages = {12}
}

@article{10.1109/TASLP.2018.2852500,
author = {Heck, Michael and Sakti, Sakriani and Nakamura, Satoshi},
title = {Dirichlet Process Mixture of Mixtures Model for Unsupervised Subword Modeling},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2852500},
doi = {10.1109/TASLP.2018.2852500},
abstract = {We develop a parallelizable Markov chain Monte Carlo sampler for a Dirichlet process mixture of mixtures model. Our sampler jointly infers a codebook and clusters. The codebook is a global collection of components. Clusters are mixtures, defined over the codebook. We combine a nonergodic Gibbs sampler with two layers of split and merge samplers on codebook and mixture level to form a valid ergodic chain. We design an additional switch sampler for components that supports convergence in our experimental results. In the use case of unsupervised subword modeling, we show that our method infers complex classes from real speech feature vectors that consistently show higher quality on several evaluation metrics. At the same time, we infer fewer classes that represent subword units more consistently and show longer durations, compared to a standard Dirichlet process mixture model sampler.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2027–2042},
numpages = {16}
}

@article{10.1109/TASLP.2018.2856625,
author = {Xu, Jingjing and He, Hangfeng and Sun, Xu and Ren, Xuancheng and Li, Sujian},
title = {Cross-Domain and Semisupervised Named Entity Recognition in Chinese Social Media: A Unified Model},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2856625},
doi = {10.1109/TASLP.2018.2856625},
abstract = {Named entity recognition NER in Chinese social media is an important, but challenging task because Chinese social media language is informal and noisy. Most previous methods on NER focus on in-domain supervised learning, which is limited by scarce annotated data in social media. In this paper, we present that sufficient corpora in formal domains and massive unannotated text can be combined to improve the NER performance in social media. We propose a unified model which can learn from out-of-domain corpora and in-domain unannotated text. The unified model is composed of two parts. One is for cross-domain learning and the other is for semisupervised learning. Cross-domain learning can learn out-of-domain information based on domain similarity. Semisupervised learning can learn in-domain unannotated information by self-training. Experimental results show that our unified model yields a 9.57% improvement over strong baselines and achieves the state-of-the-art performance.1},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2142–2152},
numpages = {11}
}

@article{10.1109/TASLP.2018.2851664,
author = {Weisz, Gellert and Budzianowski, Pawel and Su, Pei-Hao and Gasic, Milica},
title = {Sample Efficient Deep Reinforcement Learning for Dialogue Systems With Large Action Spaces},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851664},
doi = {10.1109/TASLP.2018.2851664},
abstract = {In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. A part of this effort is the policy optimization task, which attempts to find a policy describing how to respond to humans, in the form of a function taking the current state of the dialogue and returning the response of the system. In this paper, we investigate deep reinforcement learning approaches to solve this problem. Particular attention is given to actor-critic methods, off-policy reinforcement learning with experience replay, and various methods aimed at reducing the bias and variance of estimators. When combined, these methods result in the previously proposed ACER algorithm that gave competitive results in gaming environments. These environments, however, are fully observable and have a relatively small action set so, in this paper, we examine the application of ACER to dialogue policy optimization. We show that this method beats the current state of the art in deep learning approaches for spoken dialogue systems. This not only leads to a more sample efficient algorithm that can train faster, but also allows us to apply the algorithm in more difficult environments than before. We thus experiment with learning in a very large action space, which has two orders of magnitude more actions than previously considered. We find that ACER trains significantly faster than the current state of the art.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2083–2097},
numpages = {15}
}

@article{10.1109/TASLP.2018.2855968,
author = {Wu, Shuangzhi and Zhang, Dongdong and Zhang, Zhirui and Yang, Nan and Li, Mu and Zhou, Ming},
title = {Dependency-to-Dependency Neural Machine Translation},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2855968},
doi = {10.1109/TASLP.2018.2855968},
abstract = {Recent research has proven that syntactic knowledge is effective to improve the performance of neural machine translation NMT. Most previous work focuses on leveraging either source or target syntax in the recurrent neural network RNN based encoder–decoder model. In this paper, we simultaneously use both source and target dependency tree to improve the NMT model. First, we propose a simple but effective syntax-aware encoder to incorporate source dependency tree into NMT. The new encoder enriches each source state with dependence relations from the tree. Then, we propose a novel sequence-to-dependence framework. In this framework, the target translation and its corresponding dependence tree are jointly constructed and modeled. During decoding, the tree structure is used as context to facilitate word generations. Finally, we extend the sequence-to-dependence framework with the syntax-aware encoder to build a dependence-NMT model and apply the dependence-based framework to the Transformer. Experimental results on several translation tasks show that both source and target dependence structures can improve the translation quality and their effects can be accumulated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2132–2141},
numpages = {10}
}

@article{10.1109/TASLP.2018.2854871,
author = {Lin, Shoufeng},
title = {Reverberation-Robust Localization of Speakers Using Distinct Speech Onsets and Multichannel Cross Correlations},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2854871},
doi = {10.1109/TASLP.2018.2854871},
abstract = {Many speaker localization methods can be found in the literature. However, speaker localization under strong reverberation still remains a major challenge in the real-world applications. This paper proposes two algorithms for localizing speakers using microphone array recordings of reverberated sounds. To separate concurrent speakers, the first algorithm decomposes microphone signals spectrotemporally into subbands via an auditory filterbank. To suppress reverberation, we propose a novel speech onset detection approach derived from the speech signal and impulse response models, and further propose to formulate the multichannel cross-correlation coefficient of encoded speech onsets in each subband. The subband results are combined to estimate the directions-of-arrival of speakers. The second algorithm extends the generalized cross-correlation phase transform method by using redundant information of multiple microphones to address the reverberation problem. The proposed methods have been evaluated under adverse conditions using not only simulated signals reverberation time $T_{60}$ of up to $1$s but also recordings in a real reverberant room $T_{60} approx 0.65$s. Comparing with some state-of-the-art localization methods, experimental results confirm that the proposed methods can reliably locate static and moving speakers, in presence of reverberation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2098–2111},
numpages = {14}
}

@article{10.1109/TASLP.2018.2848701,
author = {Hadian, Hossein and Sameti, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
title = {Flat-Start Single-Stage Discriminatively Trained HMM-Based Models for ASR},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2848701},
doi = {10.1109/TASLP.2018.2848701},
abstract = {In recent years, end-to-end approaches to automatic speech recognition have received considerable attention as they are much faster in terms of preparing resources. However, conventional multistage approaches, which rely on a pipeline of training hidden Markov models HMM-GMM models and tree-building steps still give the state-of-the-art results on most databases. In this study, we investigate flat-start one-stage training of neural networks using lattice-free maximum mutual information LF-MMI objective function with HMM for large vocabulary continuous speech recognition. We thoroughly look into different issues that arise in such a setup and propose a standalone system, which achieves word error rates WER comparable with that of the state-of-the-art multi-stage systems while being much faster to prepare. We propose to use full biphones to enable flat-start context-dependent CD modeling and show through experiments that our CD modeling approach can be almost as effective as regular tree-based CD modeling. We show that our flat-start LF-MMI setup together with this tree-free CD modeling technique achieves 10 to 25 % relative WER reduction compared to other end-to-end methods on well-known databases. The improvements are larger for smaller databases.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1949–1961},
numpages = {13}
}

@article{10.1109/TASLP.2018.2851151,
author = {Nie, Shuai and Liang, Shan and Liu, Wenju and Zhang, Xueliang and Tao, Jianhua},
title = {Deep Learning Based Speech Separation via NMF-Style Reconstructions},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851151},
doi = {10.1109/TASLP.2018.2851151},
abstract = {Deep learning based speech separation usually uses a supervised algorithm to learn a mapping function from noisy features to separation targets. These separation targets, either ideal masks or magnitude spectrograms, have prominent spectro-temporal structures. Nonnegative matrix factorization NMF is a well-known representation learning technique that is capable of capturing the basic spectral structures. Therefore, the combination of deep learning and NMF as an organic whole is a smart strategy. However, previous methods typically use deep neural networks DNN and NMF for speech separation in a separate manner. In this paper, we propose a jointly combinatorial scheme to concentrate the strengths of both DNN and NMF for speech separation. NMF is used to learn the basis spectra that then are integrated into a DNN to directly reconstruct the magnitude spectrograms of speech and noise. Instead of predicting activation coefficients inferred by NMF, which is used as an intermediate target by the previous methods, DNN directly optimizes an actual separation objective in our system, so that the accumulated errors could be alleviated. Moreover, we explore a discriminative training objective with sparsity constraints to suppress noise and preserve more speech components further. Systematic experiments show that the proposed models are competitive with the previous methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2043–2055},
numpages = {13}
}

@article{10.1109/TASLP.2018.2858559,
author = {McFee, Brian and Salamon, Justin and Bello, Juan Pablo},
title = {Adaptive Pooling Operators for Weakly Labeled Sound Event Detection},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2858559},
doi = {10.1109/TASLP.2018.2858559},
abstract = {Sound event detection SED methods are tasked with labeling segments of audio recordings by the presence of active sound sources. SED is typically posed as a supervised machine learning problem, requiring strong annotations for the presence or absence of each sound source at every time instant within the recording. However, strong annotations of this type are both labor- and cost-intensive for human annotators to produce, which limits the practical scalability of SED methods. In this paper, we treat SED as a multiple instance learning MIL problem, where training labels are static over a short excerpt, indicating the presence or absence of sound sources but not their temporal locality. The models, however, must still produce temporally dynamic predictions, which must be aggregated pooled when comparing against static labels during training. To facilitate this aggregation, we develop a family of adaptive pooling operators—referred to as autopool—which smoothly interpolate between common pooling operators, such as min-, max-, or average-pooling, and automatically adapt to the characteristics of the sound sources in question. We evaluate the proposed pooling operators on three datasets, and demonstrate that in each case, the proposed methods outperform nonadaptive pooling operators for static prediction, and nearly match the performance of models trained with strong, dynamic annotations. The proposed method is evaluated in conjunction with convolutional neural networks, but can be readily applied to any differentiable model for time-series label prediction. While this paper focuses on SED applications, the proposed methods are general, and could be applied widely to MIL problems in any domain.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2180–2193},
numpages = {14}
}

@article{10.1109/TASLP.2018.2851144,
author = {Katzberg, Fabrice and Mazur, Radoslaw and Maass, Marco and Koch, Philipp and Mertins, Alfred},
title = {A Compressed Sensing Framework for Dynamic Sound-Field Measurements},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851144},
doi = {10.1109/TASLP.2018.2851144},
abstract = {The conventional sampling of sound fields by use of stationary microphones is impractical for large bandwidths. Satisfying the Nyquist–Shannon sampling theorem in three-dimensional space requires a huge number of sampling positions. Dynamic sound-field measurements with moving microphones together with a compressed-sensing recovery allow for weakening the spatial sampling problem. For bandlimited signals, the dynamic samples taken along the microphone trajectory may be related to the room impulse responses on a virtual grid in space via spatial interpolation. The tracking of the microphone positions and the knowledge of the excitation sequence allow for setting up a linear system of equations that can be solved for the room impulse responses on the modeled virtual grid. Nevertheless, there is still the necessity for recovering a huge number of sound-field variables, in order to ensure aliasing-free reconstruction. Thus, for practical applications, random or suboptimally chosen trajectories may be expected to lead to underdetermined sampling problems for a given volume of interest. In this paper, we present a compressed sensing framework that enables us to uniquely solve the dynamic sampling problem despite having underdetermined variables. The spatio-temporal sampling problem is integrated into compressed sensing models that allow for stable and robust sub-Nyquist sampling given incoherent measurements. For a modeled equidistant grid and sparse Fourier representations, the influence of the microphone trajectories on the compressed sensing problem is investigated and a simple expression is derived for evaluating trajectories with regard to compressed-sensing based recovery.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1962–1975},
numpages = {14}
}

@article{10.1109/TASLP.2018.2851147,
author = {Sundar, Harshavardhan and Sreenivas, Thippur V. and Seelamantula, Chandra Sekhar},
title = {TDOA-Based Multiple Acoustic Source Localization Without Association Ambiguity},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851147},
doi = {10.1109/TASLP.2018.2851147},
abstract = {Multiple source localization using time-differences of arrival TDOAs is challenging because of the ambiguity involved in associating the TDOAs computed across microphone pairs to the sources. We show that the association ambiguity of the TDOAs can be effectively resolved using the concept of an inverse delay interval region IDIR, which we introduce in this paper. By examining the association between a spatial domain and the TDOAs, we define IDIR as an interhyperboloidal spatial region corresponding to an interval of delays for a given pair of microphones. The proposed scheme for localizing multiple sources involves two stages. In the first stage, the given enclosure is partitioned into nonoverlapping elemental regions and the ones that contain a source are detected using a measure based on the generalized cross-correlation with phase transform and the IDIRs. In the second stage, the sources are finely localized within each of the detected elemental regions by identifying the IDIRs containing a single source and a novel region-constrained localization approach. We evaluate the performance of the proposed approach on real recordings from the AV16.3 corpus and in a simulated reverberation setting with a reverberation time RT60 of up to 500&nbsp;ms, and show that the DOA estimation error with two active speakers is within 2° and the spatial localization error is less than 30&nbsp;cm for each speaker.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1976–1990},
numpages = {15}
}

@article{10.1109/TASLP.2018.2855960,
author = {Ma, Ning and Gonzalez, Jose A. and Brown, Guy J.},
title = {Robust Binaural Localization of a Target Sound Source by Combining Spectral Source Models and Deep Neural Networks},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2855960},
doi = {10.1109/TASLP.2018.2855960},
abstract = {Despite there being a clear evidence for top–down e.g., attentional effects in biological spatial hearing, relatively few machine hearing systems exploit the top–down model-based knowledge in sound localization. This paper addresses this issue by proposing a novel framework for the binaural sound localization that combines the model-based information about the spectral characteristics of sound sources and deep neural networks DNNs. A target source model and a background source model are first estimated during a training phase using spectral features extracted from sound signals in isolation. When the identity of the background source is not available, a universal background model can be used. During testing, the source models are used jointly to explain the mixed observations and improve the localization process by selectively weighting source azimuth posteriors output by a DNN-based localization system. To address the possible mismatch between the training and testing, a model adaptation process is further employed the on-the-fly during testing, which adapts the background model parameters directly from the noisy observations in an iterative manner. The proposed system, therefore, combines the model-based and data-driven information flow within a single computational framework. The evaluation task involved localization of a target speech source in the presence of an interfering source and room reverberation. Our experiments show that by exploiting the model-based information in this way, the sound localization performance can be improved substantially under various noisy and reverberant conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2122–2131},
numpages = {10}
}

@article{10.1109/TASLP.2018.2856374,
author = {Van Kuyk, Steven and Kleijn, W. Bastiaan and Hendriks, Richard Christian},
title = {An Evaluation of Intrusive Instrumental Intelligibility Metrics},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2856374},
doi = {10.1109/TASLP.2018.2856374},
abstract = {Instrumental intelligibility metrics are commonly used as an alternative to listening tests. This paper evaluates 12 monaural intrusive intelligibility metrics: SII, HEGP, CSII, HASPI, NCM, QSTI, STOI, ESTOI, MIKNN, SIMI, SIIB, and $text{sEPSM}^text{corr}$. In addition, this paper investigates the ability of intelligibility metrics to generalize to new types of distortions and analyzes why the top performing metrics have high performance. The intelligibility data were obtained from 11 listening tests described in the literature. The stimuli included Dutch, Danish, and English speech that was distorted by additive noise, reverberation, competing talkers, preprocessing enhancement, and postprocessing enhancement. SIIB and HASPI had the highest performance achieving a correlation with listening test scores on average of $bf rho =0.92$ and $bf rho =0.89$, respectively. The high performance of SIIB may, in part, be the result of SIIBs developers having access to all the intelligibility data considered in the evaluation. The results show that intelligibility metrics tend to perform poorly on datasets that were not used during their development. By modifying the original implementations of SIIB and STOI, the advantage of reducing statistical dependencies between input features is demonstrated. Additionally, this paper presents a new version of SIIB called $text{SIIB}^text{Gauss}$, which has similar performance to SIIB and HASPI, but takes less time to compute by two orders of magnitude.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2153–2166},
numpages = {14}
}

@article{10.1109/TASLP.2018.2848698,
author = {Dubey, Harishchandra and Sangwan, Abhijeet and Hansen, John H. L.},
title = {Leveraging Frequency-Dependent Kernel and DIP-Based Clustering for Robust Speech Activity Detection in Naturalistic Audio Streams},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2848698},
doi = {10.1109/TASLP.2018.2848698},
abstract = {Speech activity detection SAD is front-end in most speech systems, e.g., speaker verification, speech recognition&nbsp;etc. Supervised SAD typically leverages machine learning models trained on annotated data. For applications like zero-resource speech processing and NIST-OpenSAT-2017 public safety communications task, it might not be feasible to collect SAD annotations. SAD is challenging for naturalistic audio streams containing multiple noise-sources simultaneously. We propose a novel frequency-dependent kernel FDK based SAD features. FDK provides enhanced spectral decomposition from which several statistical descriptors are derived. FDK statistical descriptors are combined by principal component analysis into one-dimensional FDK-SAD features. We further proposed two decision backends: First, variable model-size Gaussian mixture model VMGMM; and second, Hartigan dip-based robust feature clustering. While VMGMM is a model-based approach, the DipSAD is nonparametric. We used both backends for comparative evaluations in two phases: first, standalone SAD performance; and second, the effect of SAD on text-dependent speaker verification using RedDots data. The NIST-OpenSAD-2015 and NIST-OpenSAT-2017 corpora are used for standalone SAD evaluations. We establish two Center for Robust Speech Systems CRSS corpora namely CRSS-PLTL-II and CRSS long-duration naturalistic noise corpus. The CRSS corpora facilitate standalone SAD evaluations on naturalistic audio streams. We performed comparative studies of the proposed approaches with multiple baselines including SohnSAD, rSAD, semisupervised Gaussian mixture model, and Gammatone spectrogram features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2056–2071},
numpages = {16}
}

@article{10.1109/TASLP.2018.2852502,
author = {Ouyang, Xi and Gu, Kang and Zhou, Pan},
title = {Spatial Pyramid Pooling Mechanism in 3D Convolutional Network for Sentence-Level Classification},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2852502},
doi = {10.1109/TASLP.2018.2852502},
abstract = {In this paper, we investigate the usage of the convolutional neural network CNN to propose a novel end-to-end language processing structure to model textual data for this task. In particular, we propose a 3D CNN structure for the task, which is featured by spatial pyramid pooling SPP. To our knowledge, it is the first time that 3D convolution and SPP structure are applied together in language processing issues. Compared with methods of 2D CNNs, the proposed method can effectively and efficiently capture the complicated internal relations in sentences. Furthermore, in previous work, the issue of sentence length variety is usually addressed by padding zero to make all sentences vectors to a fixed length, which causes too much redundant and useless noise. Inspired by the SPP structure for object detection in image processing, this issue can be well handled with the SPP, which divides the sentences into several length sections for respective pooling processing. Experiments are conducted for the task of sentence classification as well as relation classification. Experiments on Stanford Treebank, TREC, subj, and Yelp datasets demonstrate that our proposed method can outperform other state-of-the-art models, with respect to classification accuracy. Auxiliary attempts to leverage our method to SemEval-2010 Task 8 dataset further substantiate the model's capability of extracting features efficiently.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2167–2179},
numpages = {13}
}

@article{10.1109/TASLP.2018.2851155,
author = {Dinkel, Heinrich and Qian, Yanmin and Yu, Kai},
title = {Investigating Raw Wave Deep Neural Networks for End-to-End Speaker Spoofing Detection},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851155},
doi = {10.1109/TASLP.2018.2851155},
abstract = {Recent advances in automatic speaker verification ASV lead to an increased interest in securing these systems for real-world applications. Malicious spoofing attempts against ASV systems can lead to serious security breaches. A spoofing attack within the context of ASV is a condition in which a potentially harmful person successfully masks as another, to the ASV system already known person by falsifying or manipulating data. While most previous work focuses on enhanced, spoof-aware features, end-to-end models can be a potential alternative. In this paper, we investigate the training of a raw wave front-ends for deep convolutional, long short-term memory LSTM and vanilla neural networks, which are analyzed for their suitability toward spoofing detection, regarding the influence of frame size, number of output neurons, and sequence length. A joint convolutional LSTM neural network CLDNN is proposed, which outperforms previous attempts on the BTAS2016 dataset 0.82% $rightarrow$ 0.19% HTER, placing itself as the current state-of-the-art model for the dataset. We show that end-to-end approaches are appropriate for the important replay detection task and show that the proposed model is capable of distinguishing device-invariant spoofing attempts. Regarding the ASVspoof2015 dataset, the end-to-end solution achieves an equal error rate EER of 0.00% for the S1-S9 conditions. We show that the end-to-end approach based on a raw waveform input can outperform common cepstral features, without the use of context-dependent frame extensions. In addition, a cross-database domain mismatch scenario is also evaluated, which shows that the proposed CLDNN model trained on the BTAS2016 dataset achieves an EER of 25.7% on the ASVspoof2015 dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2002–2014},
numpages = {13}
}

@article{10.1109/TASLP.2018.2858932,
author = {Coteli, Mert Burkay and Olgun, Orhun and Hacihabiboglu, Huseyin},
title = {Multiple Sound Source Localization With Steered Response Power Density and Hierarchical Grid Refinement},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2858932},
doi = {10.1109/TASLP.2018.2858932},
abstract = {Estimation of the direction-of-arrival DOA of sound sources is an important step in sound field analysis. Rigid spherical microphone arrays allow the calculation of a compact spherical harmonic representation of the sound field. The standard method for analyzing sound fields recorded using such arrays is steered response power SRP maps wherein the source DOA can be estimated as the steering direction that maximizes the output power of a maximally directive beam. This approach is computationally costly since it requires steering the beam in all possible directions. This paper presents an extension to SRP called steered response power density SRPD and an associated, signal-adaptive search method called hierarchical grid refinement for reducing the number of steering directions needed for DOA estimation. The proposed method can localize near-coherent as well as incoherent sources while jointly providing the number of prominent sources in the scene. It is shown to be robust to reverberation and additive white noise. An evaluation of the proposed method using simulations and real recordings under highly reverberant conditions as well as a comparison with the state-of-the-art methods are presented.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2215–2229},
numpages = {15}
}

@article{10.1109/TASLP.2018.2851145,
author = {Sahraeian, Reza and Van Compernolle, Dirk},
title = {Cross-Entropy Training of DNN Ensemble Acoustic Models for Low-Resource ASR},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851145},
doi = {10.1109/TASLP.2018.2851145},
abstract = {Deep neural networks DNNs have shown a great promise in exploiting out-of-language data, particularly for under-resourced languages. The common trend is to merge data from various source languages to train a multilingual DNN and then reuse the hidden layers as language-independent feature extractors for a low-resource target language. While there is a consensus that using as much data from various languages results in a better and more general multilingual DNN, employing only source languages similar to the target language has proven effective. In this study, we propose a novel framework for multilingual DNN training, which employs all the available training data and exploits complementary information from individual source languages at the same time. Toward this goal, we borrow the idea of an ensemble with one generalist and many specialists. The generalist is derived from a multilingual DNN acoustic model trained on all available multilingual data; the specialists are the DNNs derived from the source languages individually. Then, the constituents in the ensemble are combined using weighted averaging schemes, where the combination weights are trained to minimize the cross-entropy objective function. In this framework, we seek for complementary information among the constituents while it is possible to get at least the performance equal to the baseline. Moreover, unlike previous well-known system combination schemes, only one model is required during decoding. We successfully examined two combination methodologies and demonstrated their usefulness in different scenarios using the multilingual GlobalPhone dataset. It is observed that, specifically, speech recognition systems developed in low-resource settings profit from the proposed strategy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1991–2001},
numpages = {11}
}

@article{10.1109/TASLP.2018.2854861,
author = {Abidin, Shamsiah and Togneri, Roberto and Sohel, Ferdous},
title = {Spectrotemporal Analysis Using Local Binary Pattern Variants for Acoustic Scene Classification},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2854861},
doi = {10.1109/TASLP.2018.2854861},
abstract = {In this paper, we present an approach for acoustic scene classification, which aggregates spectral and temporal features. We do this by proposing the first use of the variable-Q transform VQT to generate the time–frequency representation for acoustic scene classification. The VQT provides finer control over the resolution compared to the constant-Q transform CQT or short time fourier transform and can be tuned to better capture acoustic scene information. We then adopt a variant of the local binary pattern LBP, the adjacent evaluation completed LBP AECLBP, which is better suited to extracting features from acoustic time–frequency images. Our results yield a 5.2% improvement on the DCASE 2016 dataset compared to the application of standard CQT with LBP. Fusing our proposed AECLBP with HOG features, we achieve a classification accuracy of 85.5%, which outperforms one of the top performing systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2112–2121},
numpages = {10}
}

@article{10.1109/TASLP.2018.2858538,
author = {Barbancho, Isabel and Tzanetakis, George and Barbancho, Ana M. and Tardon, Lorenzo J.},
title = {Discrimination Between Ascending/Descending Pitch Arpeggios},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2858538},
doi = {10.1109/TASLP.2018.2858538},
abstract = {Automatic music transcription can be defined as the analysis of the acoustic signal to extract a symbolic representation of music. Existing transcription systems typically consider just the notes played at a given moment; however, other aspects such as expressiveness and playing technique can also be considered. This work is focused on how chords are played. Specifically, we consider a special type of chords, those played in arpeggio style, or simply arpeggios, in which the notes are played fast, sequentially from the lowest to the highest pitched note or vice versa and with a large overlap of the notes’ sound. The main goal of this paper is to determine the pitch direction in which the arpeggiated chord was played. Two different classification methods are considered: a Fisher linear discriminant and an SVM linear classification scheme. Different features are presented for this task: one is based on the Mel-frequency cepstral coefficients MFCCs and two others, specifically designed for this task, rely on different analyses of the spectrogram. Evaluations have been done with a wide number of musical instruments. The results show that the pitch direction can be reliably detected using the proposed methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2194–2203},
numpages = {10}
}

@article{10.1109/TASLP.2018.2858923,
author = {Kim, Younggwan and Kim, Myungjong and Goo, Jahyun and Kim, Hoirin},
title = {Learning Self-Informed Feature Contribution for Deep Learning-Based Acoustic Modeling},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2858923},
doi = {10.1109/TASLP.2018.2858923},
abstract = {In this paper, we introduce a new feature engineering approach for deep learning-based acoustic modeling, which utilizes input feature contributions. For this purpose, we propose an auxiliary deep neural network DNN called a feature contribution network FCN whose output layer is composed of sigmoid-based contribution gates. In our framework, the FCN tries to learn element-level discriminative contributions of input features and an acoustic model network AMN is trained by gated features generated by element-wise multiplication between contribution gate outputs and input features. In addition, we also propose a regularization method for the FCN, which helps the FCN to activate the minimum number of the gates. The proposed methods were evaluated on the TED-LIUM release 1 corpus. We applied the proposed methods to DNN- and long short-term memory-based AMNs. Experimental results results showed that AMNs with the FCNs consistently improved recognition performance compared with AMN-only frameworks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2204–2214},
numpages = {11}
}

@article{10.1109/TASLP.2018.2852479,
author = {Bu, Bing and Bao, Chang-chun and Jia, Mao-shen},
title = {Design of a Planar First-Order Loudspeaker Array for Global Active Noise Control},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2852479},
doi = {10.1109/TASLP.2018.2852479},
abstract = {This paper proposes a method to design a planar first-order loudspeaker array structure for global active noise control. Compared with the traditional spherical loudspeaker array, the planar array provides a practical design with flexible source locations. The planar array is capable of achieving global noise control, provided that the loudspeakers have general variable first-order responses in elevation. On $boldsymbol {x{-}y}$ plane, we use spherical harmonics to analyze the required first-order loudspeakers consisting of monopole and tangential dipole components. By exploiting the properties of the associated Legendre functions and its derivative, we can divide the primary soundfield into even harmonics controlled by the monopole component, and odd harmonics controlled by the dipole component. Through the appropriate choice of radii of circles, we avoid the ill-conditioning problem of matrix inversion and derive a robust solution for loudspeaker weights to suppress the primary noise field. Besides, we use the closely-located monopole pairs, instead of the ideal general first-order loudspeakers, to design an alternative planar array for practical implementation. As an illustration, we use several simulation examples to validate the performance of the two proposed planar loudspeaker arrays.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2240–2250},
numpages = {11}
}

@article{10.1109/TASLP.2018.2852492,
author = {Jang, Youngsoo and Ham, Jiyeon and Lee, Byung-Jun and Kim, Kee-Eung},
title = {Cross-Language Neural Dialog State Tracker for Large Ontologies Using Hierarchical Attention},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2852492},
doi = {10.1109/TASLP.2018.2852492},
abstract = {Dialog state tracking, which refers to identifying the user intent from utterances, is one of the most important tasks in dialog management. In this paper, we present our dialog state tracker developed for the fifth dialog state tracking challenge, which focused on cross-language adaptation using a very scarce machine-translated training data when compared to the size of the ontology. Our dialog state tracker is based on the bi-directional long short-term memory network with a hierarchical attention mechanism in order to spot important words in user utterances. The user intent is predicted by finding the closest keyword in the ontology to the attention-weighted word vector. With the suggested methodology, our tracker can overcome various difficulties due to the scarce training data that existing machine learning-based trackers had, such as predicting user intents they have not seen before. We show that our tracker outperforms other trackers submitted to the challenge with respect to most of the performance measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2072–2082},
numpages = {11}
}

@article{10.1109/TASLP.2018.2859777,
author = {Bao, Junwei and Gong, Yeyun and Duan, Nan and Zhou, Ming and Zhao, Tiejun},
title = {Question Generation With Doubly Adversarial Nets},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2859777},
doi = {10.1109/TASLP.2018.2859777},
abstract = {We study the problem of question generation on a specific domain, where there are no labeled data. To address this problem, we propose a novel neural question generation approach called DoubAN, or doubly adversarial nets, which fully utilizes labeled data from other domains source domains and unlabeled data from the target domain. Learning a DoubAN involves two adversarial procedures between a question generator and two adversaries. One adversary is a domain-classification discriminator DC-Dis, which is designed to help the generator learn domain-general representations of the input text. The other is a question-answering discriminator QA-Dis, which provides more training data with estimated reward scores for generated text-question pairs. We conduct experiments on the SQuAD dataset as target-domain unlabeled data and the NewsQA dataset as source-domain labeled data. Experiment results show that our DoubAN achieves better results than baselines. Compared to model variants, which adopt only DC-Dis or QA-Dis, we find that the DC-Dis and QA-Dis indirectly interact with each other and jointly improve the quality of generated questions on the target domain. Moreover, extensive analysis and discussion prove the reasonableness and effectiveness of our proposed approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2230–2239},
numpages = {10}
}

@article{10.1109/TASLP.2018.2842159,
author = {Wang, DeLiang and Chen, Jitong},
title = {Supervised Speech Separation Based on Deep Learning: An Overview},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2842159},
doi = {10.1109/TASLP.2018.2842159},
abstract = {Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This paper provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then, we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement speech-nonspeech separation, speaker separation multitalker separation, and speech dereverberation, as well as multimicrophone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1702–1726},
numpages = {25}
}

@article{10.1109/TASLP.2018.2843537,
author = {Xiong, Feifei and Goetze, Stefan and Kollmeier, Birger and Meyer, Bernd T.},
title = {Exploring Auditory-Inspired Acoustic Features for Room Acoustic Parameter Estimation From Monaural Speech},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2843537},
doi = {10.1109/TASLP.2018.2843537},
abstract = {Room acoustic parameters that characterize acoustic environments can help to improve signal enhancement algorithms such as for dereverberation, or automatic speech recognition by adapting models to the current parameter set. The reverberation time&nbsp;RT and the early-to-late reverberation ratio&nbsp;ELR are two key parameters. In this paper, we propose a blind ROom Parameter Estimator&nbsp;ROPE based on an artificial neural network that learns the mapping to discrete ranges of the RT and the ELR from single-microphone speech signals. Auditory-inspired acoustic features are used as neural network input, which are generated by a temporal modulation filter bank applied to the speech time-frequency representation. ROPE performance is analyzed in various reverberant environments in both clean and noisy conditions for both fullband and subband RT and ELR estimations. The importance of specific temporal modulation frequencies is analyzed by evaluating the contribution of individual filters to the ROPE performance. Experimental results show that ROPE is robust against different variations caused by room impulse responses measured versus&nbsp;simulated, mismatched noise levels, and speech variability reflected through different corpora. Compared to state-of-the-art algorithms that were tested in the acoustic characterisation of environments ACE challenge, the ROPE model is the only one that is among the best for all individual tasks RT and ELR estimation from fullband and subband signals. Improved fullband estimations are even obtained by ROPE when integrating speech-related frequency subbands. Furthermore, the model requires the least computational resources with a real time factor that is at least two times faster than competing algorithms. Results are achieved with an average observation window of 3&nbsp;s, which is important for real-time applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1809–1820},
numpages = {12}
}

@article{10.1109/TASLP.2018.2842156,
author = {Koizumi, Yuma and Niwa, Kenta and Hioka, Yusuke and Kobayashi, Kazunori and Haneda, Yoichi},
title = {DNN-Based Source Enhancement to Increase Objective Sound Quality Assessment Score},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2842156},
doi = {10.1109/TASLP.2018.2842156},
abstract = {We propose a training method for deep neural network DNN based source enhancement to increase objective sound quality assessment OSQA scores such as the perceptual evaluation of speech quality. In many conventional studies, DNNs have been used as a mapping function to estimate time–frequency masks and trained to minimize an analytically tractable objective function such as the mean squared error MSE. Since OSQA scores have been used widely for sound-quality evaluation, constructing DNNs to increase OSQA scores would be better than using the minimum MSE to create high-quality output signals. However, since most OSQA scores are not analytically tractable, i.e., they are black boxes, the gradient of the objective function cannot be calculated by simply applying backpropagation. To calculate the gradient of the OSQA-based objective function, we formulated a DNN optimization scheme on the basis of black-box optimization, which is used for training a computer that plays a game. For a black-box-optimization scheme, we adopt the policy gradient method for calculating the gradient on the basis of a sampling algorithm. To simulate output signals using the sampling algorithm, DNNs are used to estimate the probability density function of the output signals that maximize OSQA scores. The OSQA scores are calculated from the simulated output signals, and the DNNs are trained to increase the probability of generating the simulated output signals that achieve high OSQA scores. Through several experiments, we found that OSQA scores significantly increased by applying the proposed method, even though the MSE was not minimized.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1780–1792},
numpages = {13}
}

@article{10.1109/TASLP.2018.2842435,
author = {Winter, Fiete and Wierstorf, Hagen and Hold, Christoph and Kruger, Frank and Raake, Alexander and Spors, Sascha},
title = {Colouration in Local Wave Field Synthesis},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2842435},
doi = {10.1109/TASLP.2018.2842435},
abstract = {Sound field synthesis techniques including wave field synthesis and near-field-compensated higher order ambisonics aim at a physically accurate reproduction of a desired sound field inside an extended listening area. This area is surrounded by loudspeakers individually driven by their respective driving signals. The latter have to be chosen such that the superposition of all emitted sound fields coincides with the desired one. Due to practical limitations, artefacts impair the synthesis accuracy resulting in a perceivable change in timbre. Recently, two approaches to so-called local wave field synthesis were published that enhance the reproduction accuracy in a limited region while allowing stronger artefacts outside. This paper reports on two listening experiments comparing conventional techniques for sound field synthesis with the mentioned approaches. Furthermore, the influence of different parametrizations for local wave field synthesis is investigated. The results show that the enhanced reproduction accuracy in local wave field synthesis leads to a reduction of perceived colouration, if a suitable parametrization is chosen.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1913–1924},
numpages = {12}
}

@article{10.1109/TASLP.2018.2845111,
author = {Huang, Jizhou and Sun, Yaming and Zhang, Wei and Wang, Haifeng and Liu, Ting},
title = {Entity Highlight Generation as Statistical and Neural Machine Translation},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2845111},
doi = {10.1109/TASLP.2018.2845111},
abstract = {Entity highlight refers to a short, concise, and characteristic description for an entity, which can be applied to various applications. In this article, we study the problem of automatically generating entity highlights from the descriptive sentences of entities. Specifically, we develop two computational approaches, one is inspired by the statistical machine translation SMT and another is a sequence-to-sequence learning Seq2Seq approach, which has been successfully applied in neural machine translation and neural summarization. In the Seq2Seq approach, we use attention mechanism, copy mechanism, and coverage mechanism. To generate entity-specific highlights, we also incorporate entity name into the Seq2Seq model to guide the decoding process. We automatically collect large-scale instances as training data without any manual annotation, and ask annotators to create a test set. We compare with several strong baseline methods, and evaluate the approaches with both automatic evaluation and manual evaluation. Experimental results show that the entity enhanced Seq2Seq model with attention, copy, and coverage mechanisms significantly outperforms all other approaches in terms of multiple evaluation metrics.1},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1860–1872},
numpages = {13}
}

@article{10.1109/TASLP.2018.2835719,
author = {Khan, Faheem Ullah and Milner, Ben P. and Le Cornu, Thomas},
title = {Using Visual Speech Information in Masking Methods for Audio Speaker Separation},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2835719},
doi = {10.1109/TASLP.2018.2835719},
abstract = {This paper examines whether visual speech information can be effective within audio-masking-based speaker separation to improve the quality and intelligibility of the target speech. Two visual-only methods of generating an audio mask for speaker separation are first developed. These use a deep neural network to map the visual speech features to an audio feature space from which both visually derived binary masks and visually derived ratio masks are estimated, before application to the speech mixture. Second, an audio ratio masking method forms a baseline approach for speaker separation which is extended to exploit visual speech information to form audio-visual ratio masks. Speech quality and intelligibility tests are carried out on the visual-only, audio-only, and audio-visual masking methods of speaker separation at mixing levels from $-$ 10 to +10 dB. These reveal substantial improvements in the target speech when applying the visual-only and audio-only masks, but with highest performance occurring when combining audio and visual information to create the audio-visual masks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1742–1754},
numpages = {13}
}

@article{10.1109/TASLP.2018.2846402,
author = {Do, Quoc Truong and Sakti, Sakriani and Nakamura, Satoshi},
title = {Sequence-to-Sequence Models for Emphasis Speech Translation},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2846402},
doi = {10.1109/TASLP.2018.2846402},
abstract = {Speech-to-speech translation S2ST systems are capable of breaking language barriers in cross-lingual communication by translating speech across languages. Recent studies have introduced many improvements that allow existing S2ST systems to handle not only linguistic meaning but also paralinguistic information such as emphasis by proposing additional emphasis estimation and translation components. However, the approach used for emphasis translation is not optimal for sequence translation tasks and fails to easily handle the long-term dependencies of words and emphasis levels. It also requires the quantization of emphasis levels and treats them as discrete labels instead of continuous values. Moreover, the whole translation pipeline is fairly complex and slow because all components are trained separately without joint optimization. In this paper, we make two contributions: 1 we propose an approach that can handle continuous emphasis levels based on sequence-to-sequence models, and 2 we combine machine and emphasis translation into a single model, which greatly simplifies the translation pipeline and make it easier to perform joint optimization. Our results on an emphasis translation task indicate that our translation models outperform previous models by a large margin in both objective and subjective tests. Experiments on a joint translation model also show that our models can perform joint translation of words and emphasis with one-word delays instead of full-sentence delays while preserving the translation performance of both tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1873–1883},
numpages = {11}
}

@article{10.1109/TASLP.2018.2837384,
author = {Senel, Lutfi Kerem and Utlu, Ihsan and Yucesoy, Veysel and Koc, Aykut and Cukur, Tolga},
title = {Semantic Structure and Interpretability of Word Embeddings},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2837384},
doi = {10.1109/TASLP.2018.2837384},
abstract = {Dense word embeddings, which encode meanings of words to low-dimensional vector spaces, have become very popular in natural language processing NLP research due to their state-of-the-art performances in many NLP tasks. Word embeddings are substantially successful in capturing semantic relations among words, so a meaningful semantic structure must be present in the respective vector spaces. However, in many cases, this semantic structure is broadly and heterogeneously distributed across the embedding dimensions making interpretation of dimensions a big challenge. In this study, we propose a statistical method to uncover the underlying latent semantic structure in the dense word embeddings. To perform our analysis, we introduce a new dataset SEMCAT that contains more than 6500 words semantically grouped under 110 categories. We further propose a method to quantify the interpretability of the word embeddings. The proposed method is a practical alternative to the classical word intrusion test that requires human intervention.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1769–1779},
numpages = {11}
}

@article{10.1109/TASLP.2018.2839362,
author = {Li, Xiaofei and Gannot, Sharon and Girin, Laurent and Horaud, Radu},
title = {Multichannel Identification and Nonnegative Equalization for Dereverberation and Noise Reduction Based on Convolutive Transfer Function},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2839362},
doi = {10.1109/TASLP.2018.2839362},
abstract = {This paper addresses the problems of blind multichannel identification and equalization for joint speech dereverberation and noise reduction. The time-domain cross-relation method is hardly applicable for blind room impulse response identification due to the near-common zeros of the long impulse responses. We extend the cross-relation method to the short-time Fourier transform STFT domain, in which the time-domain impulse response is approximately represented by the convolutive transfer function CTF with much less coefficients. For the oversampled STFT, CTFs suffer from the common zeros caused by the nonflat frequency response of the STFT window. To overcome this, we propose to identify CTFs using the STFT framework with oversampled signals and critically sampled CTFs, which is a good tradeoff between the frequency aliasing of the signals and the common zeros problem of CTFs. The identified complex-valued CTFs are not accurate enough for multichannel equalization due to the frequency aliasing of the CTFs. Hence, we only use the CTF magnitudes, which leads to a nonnegative multichannel equalization method based on a nonnegative convolution model between the STFT magnitude of the source signal and the CTF magnitude. Compared with the complex-valued convolution model, this nonnegative convolution model is shown to be more robust against the CTF perturbations. To recover the STFT magnitude of the source signal and to reduce the additive noise, the $ell _2$-norm fitting error between the STFT magnitude of the microphone signals and the nonnegative convolution is constrained to be less than a noise power related tolerance. Meanwhile, the $ell _1$ -norm of the STFT magnitude of the source signal is minimized to impose the sparsity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1755–1768},
numpages = {14}
}

@article{10.1109/TASLP.2018.2847442,
author = {Fontana, Federico and Bozzo, Enrico},
title = {Explicit Fixed-Point Computation of Nonlinear Delay-Free Loop Filter Networks},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2847442},
doi = {10.1109/TASLP.2018.2847442},
abstract = {An iterative method is proposed for the explicit computation of discrete-time nonlinear filter networks containing delay-free loops. The method relies on a fixed-point search of the signal values at every temporal step. The formal as well as numerical properties of fixed-point solvers delimit its applicability: On the one hand, the method allows for a reliable prediction of the frequency rates where the simulation is stable, while, on the other hand, its straightforward applicability is counterbalanced by low speed of convergence. Especially in presence of specific nonlinear characteristics, the use of a fixed-point search is limited if the real-time constraint holds. For this reason, the method becomes useful especially during the digital model prototyping stage, as exemplified while revisiting a previous discrete-time realization of the voltage-controlled filter aboard the EMS VCS3 analog synthesizer. Further tests conducted on a digital ring modulator model support the above considerations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1884–1896},
numpages = {13}
}

@article{10.1109/TASLP.2018.2837223,
author = {Wang, Rui and Utiyama, Masao and Finch, Andrew and Liu, Lemao and Chen, Kehai and Sumita, Eiichiro},
title = {Sentence Selection and Weighting for Neural Machine Translation Domain Adaptation},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2837223},
doi = {10.1109/TASLP.2018.2837223},
abstract = {Neural machine translation NMT has been prominent in many machine translation tasks. However, in some domain-specific tasks, only the corpora from similar domains can improve translation performance. If out-of-domain corpora are directly added into the in-domain corpus, the translation performance may even degrade. Therefore, domain adaptation techniques are essential to solve the NMT domain problem. Most existing methods for domain adaptation are designed for the conventional phrase-based machine translation. For NMT domain adaptation, there have been only a few studies on topics such as fine tuning, domain tags, and domain features. In this paper, we have four goals for sentence level NMT domain adaptation. First, the NMT's internal sentence embedding is exploited and the sentence embedding similarity is used to select out-of-domain sentences that are close to the in-domain corpus. Second, we propose three sentence weighting methods, i.e., sentence weighting, domain weighting, and batch weighting, to balance the data distribution during NMT training. Third, in addition, we propose dynamic training methods to adjust the sentence selection and weighting during NMT training. Fourth, to solve the multidomain problem in a real-world NMT scenario where the domain distributions of training and testing data often mismatch, we proposed a multidomain sentence weighting method to balance the domain distributions of training data and match the domain distributions of training and testing data. The proposed methods are evaluated in international workshop on spoken language translation IWSLT English-to-French/German tasks and a multidomain English-to-French task. Empirical results show that the sentence selection and weighting methods can significantly improve the NMT performance, outperforming the existing baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1727–1741},
numpages = {15}
}

@article{10.1109/TASLP.2018.2845121,
author = {Wu, Kai and Reju, Vaninirappuputhenpurayil Gopalan and Khong, Andy W. H.},
title = {Multisource DOA Estimation in a Reverberant Environment Using a Single Acoustic Vector Sensor},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2845121},
doi = {10.1109/TASLP.2018.2845121},
abstract = {We address the problem of direction-of-arrival DOA estimation for multiple speech sources in an enclosed environment using a single acoustic vector sensor. The challenges in such scenario include reverberation and overlapping of the source signals. In this work, we exploit low-reverberant-single-source LRSS points in the time–frequency TF domain, where a particular source is dominant with high signal-to-reverberation ratio. Unlike conventional algorithms having limitation that such potential points need to be detected at “TF-zone” level, the proposed algorithm performs LRSS detection at “TF-point” level. Therefore, for the proposed algorithm, the potential LRSS points need not be neighbors of each other within a TF zone to be detected, resulting an increased number of detected LRSS points. The detected LRSS points are further screened by an outlier removal step such that only reliable LRSS points will be used for DOA estimation. Simulations and experiments were conducted to demonstrate the effectiveness of the proposed algorithm in multisource reverberant environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1848–1859},
numpages = {12}
}

@article{10.1109/TASLP.2018.2845665,
author = {Xue, Wei and Moore, Alastair. H. and Brookes, Mike and Naylor, Patrick A.},
title = {Modulation-Domain Multichannel Kalman Filtering for Speech Enhancement},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2845665},
doi = {10.1109/TASLP.2018.2845665},
abstract = {Compared with single-channel speech enhancement methods, multichannel methods can utilize spatial information to design optimal filters. Although some filters adaptively consider second-order signal statistics, the temporal evolution of the speech spectrum is usually neglected. By using linear prediction LP to model the inter-frame temporal evolution of speech, single-channel Kalman filtering KF based methods have been developed for speech enhancement. In this paper, we derive a multichannel KF MKF that jointly uses both interchannel spatial correlation and interframe temporal correlation for speech enhancement. We perform LP in the modulation domain, and by incorporating the spatial information, derive an optimal MKF gain in the short-time Fourier transform domain. We show that the proposed MKF reduces to the conventional multichannel Wiener filter if the LP information is discarded. Furthermore, we show that, under an appropriate assumption, the MKF is equivalent to a concatenation of the minimum variance distortion response beamformer and a single-channel modulation-domain KF and therefore present an alternative implementation of the MKF. Experiments conducted on a public head-related impulse response database demonstrate the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1833–1847},
numpages = {15}
}

@article{10.1109/TASLP.2018.2842146,
author = {Paleologu, Constantin and Benesty, Jacob and Ciochina, Silviu},
title = {Linear System Identification Based on a Kronecker Product Decomposition},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2842146},
doi = {10.1109/TASLP.2018.2842146},
abstract = {Linear system identification is a key problem in many important applications, among which echo cancelation is a very challenging one. Due to the long length impulse responses i.e., echo paths to be identified, there is always room and needs to improve the performance of the echo cancelers, especially in terms of complexity, convergence rate, robustness, and accuracy. In this paper, we propose a new way to address the system identification problem from the echo cancelation perspective, by exploiting an optimal approximation of the impulse response based on the nearest Kronecker product decomposition. Also, we make a first step toward this direction, by developing an iterative Wiener filter based on this approach. As compared to the conventional Wiener filter, the proposed solution is much more attractive since its gain is twofold. First, the matrices to be inverted or, preferably, linear systems to be solved are smaller as compared to the conventional approach. Second, as a consequence, the iterative Wiener filter leads to a good estimate of the impulse response, even when a small amount of data is available for the estimation of the statistics. Simulation results support the theoretical findings and indicate the good results of the proposed approach, for the identification of different network and acoustic impulse responses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1793–1808},
numpages = {16}
}

@article{10.1109/TASLP.2018.2844025,
author = {Le Lan, Gael and Charlet, Delphine and Larcher, Anthony and Meignier, Sylvain},
title = {An Adaptive Method for Cross-Recording Speaker Diarization},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2844025},
doi = {10.1109/TASLP.2018.2844025},
abstract = {Nowadays, state-of-the-art speaker diarization systems heavily rely on between-recording variability compensation methods to accurately process large collections of recordings. Variability estimation is performed on consequent training datasets, which must be labeled by speaker. One major problem of such systems is the acoustic mismatch between training and target data that degrades performances. Most of the collections contain lots of speakers speaking in various acoustic conditions. In this paper, we investigate how unlabeled speakers can help improve between-recording variability estimation, to overcome the mismatch issue. We propose a scalable unsupervised adaptation framework for two types of variability compensation. The proposed framework consists in adapting a state-of-the-art diarization and linking system, trained on out-of-domain data, using the data of the collection itself. Experiments in mismatch condition are run on two French Television shows, while the initial training dataset is composed of Radio recordings. Results indicate that the proposed adaptation framework reduces the cross-recording DER of 13% in average for variable collection sizes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1821–1832},
numpages = {12}
}

@article{10.1109/TASLP.2018.2847459,
author = {Andersen, Asger Heidemann and de Haan, Jan Mark and Tan, Zheng-Hua and Jensen, Jesper},
title = {Nonintrusive Speech Intelligibility Prediction Using Convolutional Neural Networks},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2847459},
doi = {10.1109/TASLP.2018.2847459},
abstract = {Speech Intelligibility Prediction SIP algorithms are becoming popular tools within the development and operation of speech processing devices and algorithms. However, many&nbsp;SIP algorithms require knowledge of the underlying clean speech;&nbsp;a signal that is often not available in real-world applications. This has led to increased interest in&nbsp;nonintrusive&nbsp;SIP algorithms, which do not require clean speech to make predictions. In this paper, we investigate the use of&nbsp;Convolutional Neural Networks CNNs for nonintrusive&nbsp;SIP. To do so, we utilize a&nbsp;CNN architecture that shows similarities to existing&nbsp;SIP algorithms, in terms of computational structure, and which allows for easy and meaningful visualization and interpretation of trained weights. We evaluate this architecture using a large dataset obtained by combining datasets from the literature. The proposed method shows high prediction performance when compared with four existing intrusive and nonintrusive&nbsp;SIP algorithms. This demonstrates the potential of deep learning for speech intelligibility prediction.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1925–1939},
numpages = {15}
}

@article{10.1109/TASLP.2018.2839355,
author = {Widmark, Simon},
title = {Causal IIR Audio Precompensator Filters Subject to Quadratic Constraints},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2839355},
doi = {10.1109/TASLP.2018.2839355},
abstract = {Infinite impulse response IIR Wiener precompensator design, with constraints on causality, is here also extended to incorporate general quadratic constraints. A method for finding a linear quadratic optimal, causal discrete-time multiple-input multiple-output filter subject to a set of user defined constraints is proposed and analyzed. A method for designing causal filters subject to constraints on the power gains in a large number of small frequency intervals is also proposed. The resulting set of methods provide constrained stable IIR filters with optimal parameterization. Compared to finite impulse response Wiener filtering, the computational complexity is much lower; and compared to noncausal frequency domain designs, we gain control of the time-domain properties of the compensated system. The design methods are applied to a room compensation audio problem subject to filter power gain constraints and are compared to a corresponding noncausal per-frequency method. The results are presented with audio filtering and sound field control as main motivating applications but the methods extend to other areas of linear feedforward controller design and Wiener filtering.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1897–1912},
numpages = {16}
}

@article{10.1109/TASLP.2018.2827300,
author = {Menzies, Dylan and Fazi, Filippo Maria},
title = {A Complex Panning Method for Near-Field Imaging},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2827300},
doi = {10.1109/TASLP.2018.2827300},
abstract = {Conventional amplitude panning can be used to produce images of distant objects. An extended panning method is presented that can also produce image cues for the near-field region, by the control of Inter-aural Level Difference cues in the low frequency range below ∼1000 Hz. The approach has grown from an adaptive panning method that corrects for the dependence of image direction on head orientation. Stereo panning functions are derived from a general formulation. A single first order filter is required for each image. The method is tested by simulating a range of configurations using measured Head Related Transfer Functions, and also with listening tests. The results confirm the ability of the method to control near-field cues, while also compensating image direction for head rotation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1539–1548},
numpages = {10}
}

@article{10.1109/TASLP.2018.2828321,
author = {Evers, Christine and Naylor, Patrick A.},
title = {Acoustic SLAM},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2828321},
doi = {10.1109/TASLP.2018.2828321},
abstract = {An algorithm is presented that enables devices equipped with microphones, such as robots, to move within their environment in order to explore, adapt to, and interact with sound sources of interest. Acoustic scene mapping creates a three-dimensional 3D representation of the positional information of sound sources across time and space. In practice, positional source information is only provided by Direction-of-Arrival DoA estimates of the source directions; the source-sensor range is typically difficult to obtain. DoA estimates are also adversely affected by reverberation, noise, and interference, leading to errors in source location estimation and consequent false DoA estimates. Moreover, many acoustic sources, such as human talkers, are not continuously active, such that periods of inactivity lead to missing DoA estimates. Withal, the DoA estimates are specified relative to the observer's sensor location and orientation. Accurate positional information about the observer therefore is crucial. This paper proposes Acoustic Simultaneous Localization and Mapping aSLAM, which uses acoustic signals to simultaneously map the 3D positions of multiple sound sources while passively localizing the observer within the scene map. The performance of aSLAM is analyzed and evaluated using a series of realistic simulations. Results are presented to show the impact of the observer motion and sound source localization accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1484–1498},
numpages = {15}
}

@article{10.1109/TASLP.2018.2834729,
author = {Xiao, Ke and Wang, Supin and Wan, Mingxi and Wu, Liang},
title = {Radiated Noise Suppression for Electrolarynx Speech Based on Multiband Time-Domain Amplitude Modulation},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2834729},
doi = {10.1109/TASLP.2018.2834729},
abstract = {Radiated noise severely degrades the electrolarynx EL speech. It cannot be thoroughly suppressed by conventional frequency-domain enhancement methods. In this paper, a new method, called multiband time-domain amplitude modulation MTAM, is proposed to reduce the radiated noise of EL speech. In the proposed method, the speech components changing slowly that represent the radiated noise are removed by directly modulating the time-domain amplitudes in multiple frequency bands. The EL speech enhanced by the proposed MTAM and the conventional frequency-domain enhancement methods spectral subtraction and Wiener filtering are evaluated on both acoustic and perceptual characteristics. The acoustic analysis reveals that the MTAM not only can reduce the radiated noise more thoroughly but can also easily control the residual noise intensity by adjusting a modulation parameter $lambda $. Moreover, the MTAM can avoid causing new artificial noise that cannot be avoided by the conventional frequency-domain enhancement methods. The perceptual analysis indicates that the MTAM also have better performance on increasing the acceptability and the consonant intelligibility of EL speech than spectral subtraction and Wiener filtering. These findings validate that the MTAM indeed works well in suppressing the radiated noise of EL speech and avoiding the artificial noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1585–1593},
numpages = {9}
}

@article{10.5555/3232296.3232304,
author = {Misra, Abhinav and Hansen, John H. L.},
title = {Maximum-Likelihood Linear Transformation for Unsupervised Domain Adaptation in Speaker Verification},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
abstract = {Recent advances in front-end factor analysis through development of i-Vectors have led to significant gains in speaker recognition technology. However, the problem of mismatch between the domains of system development and evaluation data remains a challenging one. This domain mismatch occurs primarily because of the variability in the sources of development and evaluation data. In this study, we propose a novel method of unsupervised probabilistic feature transformation UPFT to reduce this domain mismatch by transforming an out-of-domain development data toward in-domain development data. We formulate the alignment of two different domains as a probability density estimation problem. We first train a Gaussian mixture model GMM using the out-of-domain i-Vectors. Next, we employ an expectation–maximization EM algorithm to fit the means of the GMM to the in-domain i-Vectors by maximizing the overall likelihood. At the optimum, the two domains become closer to each other in the i-Vector space. While reaching the optimum through multiple iterations of the EM, we reparameterize the centroid locations using the following set of transformation parameters: rotation, translation, and scaling. These transformation parameters, which are obtained during the optimization process, are later used to transform the out-of-domain i-Vectors toward in-domain i-Vectors. We observe that such a transformation leads to an improvement in performance of the out-of-domain speaker recognition system. Our proposed method has an added advantage of being completely unsupervised, and thus does not rely on any tuning parameters. We conduct experiments on both 2013 domain adaptation challenge corpus as well as National Institute of Standards and Technology Speaker Recognition Evaluation SRE—2016 corpus. On both corpora, we obtain significant improvements using the proposed UPFT solution. Specifically for the SRE-2016 corpus, using a cosine distance scoring based system, we are able to recover almost 90% of the performance gap between an in-domain and out-of-domain system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1549–1558},
numpages = {10}
}

@article{10.1109/TASLP.2018.2830117,
author = {Zhang, Meishan and Yu, Nan and Fu, Guohong},
title = {A Simple and Effective Neural Model for Joint Word Segmentation and POS Tagging},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2830117},
doi = {10.1109/TASLP.2018.2830117},
abstract = {Joint models have shown stronger capabilities for Chinese word segmentation and POS tagging, and have received great interests in the community of Chinese natural language processing. In this paper, we follow this line of work, presenting a simple yet effective sequence-to-sequence neural model for the joint task, based on a well-defined transition system, by using long short term memory neural network structures. We conduct experiments on five different datasets. The results demonstrate that our proposed model is highly competitive. By using well-trained character-level embeddings, the proposed neural joint model is able to obtain the best-reported performances in the literature.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1528–1538},
numpages = {11}
}

@article{10.1109/TASLP.2018.2830105,
author = {Carabias-Orti, Julio Jose and Nikunen, Joonas and Virtanen, Tuomas and Vera-Candeas, Pedro},
title = {Multichannel Blind Sound Source Separation Using Spatial Covariance Model With Level and Time Differences and Nonnegative Matrix Factorization},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2830105},
doi = {10.1109/TASLP.2018.2830105},
abstract = {This paper presents an algorithm for multichannel sound source separation using explicit modeling of level and time differences in source spatial covariance matrices SCM. We propose a novel SCM model in which the spatial properties are modeled by the weighted sum of direction of arrival DOA kernels. DOA kernels are obtained as the combination of phase and level difference covariance matrices representing both time and level differences between microphones for a grid of predefined source directions. The proposed SCM model is combined with the NMF model for the magnitude spectrograms. Opposite to other SCM models in the literature, in this work, source localization is implicitly defined in the model and estimated during the signal factorization. Therefore, no localization preprocessing is required. Parameters are estimated using complex-valued nonnegative matrix factorization with both Euclidean distance and Itakura–Saito divergence. Separation performance of the proposed system is evaluated using the two-channel SiSEC development dataset and four channels signals recorded in a regular room with moderate reverberation. Finally, a comparison to other state-of-the-art methods is performed, showing better achieved separation performance in terms of SIR and perceptual measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1512–1527},
numpages = {16}
}

@article{10.1109/TASLP.2018.2836143,
author = {Mahe, Gael and Jaidane, Meriem},
title = {Perceptually Controlled Reshaping of Sound Histograms},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2836143},
doi = {10.1109/TASLP.2018.2836143},
abstract = {Many audio processing algorithms have optimal performance for specific signal statistical distributions that may not be fulfilled for all signals. When the original signal is available, we propose to add an inaudible noise so that the distribution of the signal-plus-noise mixture is as close as possible to a given target distribution. The proposed generic algorithm independent from the application adds iteratively a low-power white noise to a flat-spectrum version of the signal, until the target distribution or the noise audibility is reached. The latter is assessed through a frequency masking model. Two implementations of this sound reshaping are described, according to the level of the targeted transformation and to the foreseen application: histogram global reshaping HGR to change the global shape of the histogram and histogram local reshaping HLR to locally “chisel” the histogram, but keeping the global shape unchanged. These two variants are illustrated by two applications, where the inaudibility of the noise generated by the algorithm is required: “sparsification” for source separation and low-pass filtering of the histogram for application of the quantization theorem, respectively. In both cases, the target histogram is reached or almost reached, and the transformation is inaudible. The experiments show that the source separation performs better with HGR and that the HLR allows a better application of the quantization theorem.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1671–1683},
numpages = {13}
}

@article{10.1109/TASLP.2018.2835720,
author = {Airaksinen, Manu and Juvela, Lauri and Bollepalli, Bajibabu and Yamagishi, Junichi and Alku, Paavo},
title = {A Comparison Between STRAIGHT, Glottal, and Sinusoidal Vocoding in Statistical Parametric Speech Synthesis},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2835720},
doi = {10.1109/TASLP.2018.2835720},
abstract = {A vocoder is used to express a speech waveform with a controllable parametric representation that can be converted back into a speech waveform. Vocoders representing their main categories mixed excitation, glottal, and sinusoidal vocoders were compared in this study with formal and crowd-sourced listening tests. The vocoder quality was measured within the context of analysis–synthesis as well as text-to-speech TTS synthesis in a modern statistical parametric speech synthesis framework. Furthermore, the TTS experiments were divided into synthesis with vocoder-specific features and synthesis with a shared envelope model, where the waveform generation method of the vocoders is mainly responsible for the quality differences. Finally, all of the tests included four distinct voices as a way to investigate the effect of different speakers on the synthesized speech quality. The obtained results suggest that the choice of the voice has a profound impact on the overall quality of the vocoder-generated speech, and the best vocoder for each voice can vary case by case. The single best-rated TTS system was obtained with the glottal vocoder GlottDNN using a male voice with low expressiveness. However, the results indicate that the sinusoidal vocoder PML pulse model in log-domain has the best overall performance across the performed tests. Finally, when controlling for the spectral models of the vocoders, the observed differences are similar to the baseline results. This indicates that the waveform generation method of a vocoder is essential for quality improvements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1658–1670},
numpages = {13}
}

@article{10.1109/TASLP.2018.2831456,
author = {Zhang, Chunlei and Koishida, Kazuhito and Hansen, John H. L.},
title = {Text-Independent Speaker Verification Based on Triplet Convolutional Neural Network Embeddings},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2831456},
doi = {10.1109/TASLP.2018.2831456},
abstract = {The effectiveness of introducing deep neural networks into conventional speaker recognition pipelines has been broadly shown to benefit system performance. A novel text-independent speaker verification SV framework based on the triplet loss and a very deep convolutional neural network architecture i.e., Inception-Resnet-v1 are investigated in this study, where a fixed-length speaker discriminative embedding is learned from sparse speech features and utilized as a feature representation for the SV tasks. A concise description of the neural network based speaker discriminative training with triplet loss is presented. An Euclidean distance similarity metric is applied in both network training and SV testing, which ensures the SV system to follow an end-to-end fashion. By replacing the final max/average pooling layer with a spatial pyramid pooling layer in the Inception-Resnet-v1 architecture, the fixed-length input constraint is relaxed and an obvious performance gain is achieved compared with the fixed-length input speaker embedding system. For datasets with more severe training/test condition mismatches, the probabilistic linear discriminant analysis PLDA back end is further introduced to replace the distance based scoring for the proposed speaker embedding system. Thus, we reconstruct the SV task with a neural network based front-end speaker embedding system and a PLDA that provides channel and noise variabilities compensation in the back end. Extensive experiments are conducted to provide useful hints that lead to a better testing performance. Comparison with the state-of-the-art SV frameworks on three public datasets i.e., a prompt speech corpus, a conversational speech Switchboard corpus, and NIST SRE10 10&nbsp;s–10&nbsp;s condition justifies the effectiveness of our proposed speaker embedding system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1633–1644},
numpages = {12}
}

@article{10.1109/TASLP.2018.2830113,
author = {Wu, Chih-Wei and Dittmar, Christian and Southall, Carl and Vogl, Richard and Widmer, Gerhard and Hockman, Jason and Muller, Meinard and Lerch, Alexander},
title = {A Review of Automatic Drum Transcription},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2830113},
doi = {10.1109/TASLP.2018.2830113},
abstract = {In Western popular music, drums and percussion are an important means to emphasize and shape the rhythm, often defining the musical style. If computers were able to analyze the drum part in recorded music, it would enable a variety of rhythm-related music processing tasks. Especially the detection and classification of drum sound events by computational methods is considered to be an important and challenging research problem in the broader field of music information retrieval. Over the last two decades, several authors have attempted to tackle this problem under the umbrella term automatic drum transcription ADT. This paper presents a comprehensive review of ADT research, including a thorough discussion of the task-specific challenges, categorization of existing techniques, and evaluation of several state-of-the-art systems. To provide more insights on the practice of ADT systems, we focus on two families of ADT techniques, namely methods based on non-negative matrix factorization and recurrent neural networks. We explain the methods’ technical details and drum-specific variations and evaluate these approaches on publicly available data sets with a consistent experimental setup. Finally, the open issues and underexplored areas in ADT research are identified and discussed, providing future directions in this field.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1457–1483},
numpages = {27}
}

@article{10.1109/TASLP.2018.2830116,
author = {Laroche, Clement and Kowalski, Matthieu and Papadopoulos, Helene and Richard, Gael},
title = {Hybrid Projective Nonnegative Matrix Factorization With Drum Dictionaries for Harmonic/Percussive Source Separation},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2830116},
doi = {10.1109/TASLP.2018.2830116},
abstract = {One of the most general models of music signals considers that such signals can be represented as a sum of two distinct components: a tonal part that is sparse in frequency and temporally stable and a transient or percussive part that is composed of short-term broadband sounds. In this paper, we propose a novel hybrid method built upon nonnegative matrix factorization NMF that decomposes the time frequency representation of an audio signal into such two components. The tonal part is estimated by a sparse and orthogonal nonnegative decomposition, and the transient part is estimated by a straightforward NMF decomposition constrained by a pre-learned dictionary of smooth spectra. The optimization problem at the heart of our method remains simple with very few hyperparameters and can be solved thanks to simple multiplicative update rules. The extensive benchmark on a large and varied music database against four state of the art harmonic/percussive source separation algorithms demonstrate the merit of the proposed approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1499–1511},
numpages = {13}
}

@article{10.5555/3232296.3232303,
author = {Wakabayashi, Yukoh and Fukumori, Takahiro and Nakayama, Masato and Nishiura, Takanobu and Yamashita, Yoichi},
title = {Single-Channel Speech Enhancement With Phase Reconstruction Based on Phase Distortion Averaging},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
abstract = {Speech enhancement has been widely investigated for several decades, but by modifying only the amplitude spectrum of a speech signal, ignoring the phase spectrum, which has been regarded as an unimportant feature. However, it was recently reported that the phase spectrum plays an important role in speech quality and intelligibility. In this paper, we propose a phase reconstruction method based on harmonic enhancement using the fundamental frequency and phase distortion feature. This feature is known to show fluctuations in the phase spectrum with respect to time and frequency. We estimate the speech phase spectrum by considering the relationship between harmonic phase spectra. Experimental evaluations indicate that the proposed phase reconstruction method improves speech quality in various noisy environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1559–1569},
numpages = {11}
}

@article{10.1109/TASLP.2018.2834733,
author = {MV, Achuth Rao and Ghosh, Prasanta Kumar},
title = {PSFM—A Probabilistic Source Filter Model for Noise Robust Glottal Closure Instant Detection},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2834733},
doi = {10.1109/TASLP.2018.2834733},
abstract = {Accurate estimation of glottal closure instant GCI enables several pitch synchronous speech analysis, such as prosody modifications, glottal inverse filtering, and study of pathological speech. We propose a probabilistic source-filter model PSFM for voiced speech, where the source is modeled using the Bernoulli Gaussian distribution, which models the GCI locations and the all-pole filter coefficients are modeled using Gaussian distribution. The probability of GCIs at each speech sample is estimated using the Gibbs sampling. We propose a cost to estimate the exact GCI locations using the N-best dynamic programming. A key feature of the proposed PSFM is that it allows us to include the second-order statistics of the noise for estimating the GCI locations, thereby resulting in a noise robust GCI detection technique, although it has high computational complexity. Evaluation on archivable priority list actual-word database APLAWD database shows the proposed algorithm performs at par with the state-of-the-art GCI detection method on clean speech. However, when evaluated in noisy conditions using five types of noises at six different signal-to-noise ratio SNR levels, we observe that the proposed method performs better than the best of the existing GCI detection scheme, particularly at low SNR condition indicating the noise robustness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1645–1657},
numpages = {13}
}

@article{10.5555/3232296.3232307,
author = {Fahim, Abdullah and Samarasinghe, Prasanga N. and Abhayapala, Thushara D.},
title = {PSD Estimation and Source Separation in a Noisy Reverberant Environment Using a Spherical Microphone Array},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
abstract = {In this paper, we propose an efficient technique for estimating individual power spectral density PSD components, i.e., PSD of each desired sound source as well as of noise and reverberation, in a multisource reverberant sound scene with coherent background noise. We formulate the problem in the spherical harmonics domain to take the advantage of the inherent orthogonality of the spherical harmonics basis functions and extract the PSD components from the cross-correlation between the different sound field modes. We also investigate an implementation issue that occurs at the nulls of the Bessel functions and offer an engineering solution. The performance evaluation takes place in a practical environment with a commercial microphone array in order to measure the robustness of the proposed algorithm against all the deviations incurred in practice. We also exhibit an application of the proposed PSD estimator through a source septation algorithm and compare the performance with a contemporary method in terms of different objective measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1594–1607},
numpages = {14}
}

@article{10.1109/TASLP.2018.2821903,
author = {Fu, Szu-Wei and Wang, Tao-Wei and Tsao, Yu and Lu, Xugang and Kawai, Hisashi},
title = {End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics Optimization by Fully Convolutional Neural Networks},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2821903},
doi = {10.1109/TASLP.2018.2821903},
abstract = {Speech enhancement model is used to map a noisy speech to a clean speech. In the training stage, an objective function is often adopted to optimize the model parameters. However, in the existing literature, there is an inconsistency between the model optimization criterion and the evaluation criterion for the enhanced speech. For example, in measuring speech intelligibility, most of the evaluation metric is based on a short-time objective intelligibility STOI measure, while the frame based mean square error MSE between estimated and clean speech is widely used in optimizing the model. Due to the inconsistency, there is no guarantee that the trained model can provide optimal performance in applications. In this study, we propose an end-to-end utterance-based speech enhancement framework using fully convolutional neural networks FCN to reduce the gap between the model optimization and the evaluation criterion. Because of the utterance-based optimization, temporal correlation information of long speech segments, or even at the entire utterance level, can be considered to directly optimize perception-based objective functions. As an example, we implemented the proposed FCN enhancement framework to optimize the STOI measure. Experimental results show that the STOI of a test speech processed by the proposed approach is better than conventional MSE-optimized speech due to the consistency between the training and the evaluation targets. Moreover, by integrating the STOI into model optimization, the intelligibility of human subjects and automatic speech recognition system on the enhanced speech is also substantially improved compared to those generated based on the minimum MSE criterion.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1570–1584},
numpages = {15}
}

@article{10.5555/3232296.3232314,
author = {Zhang, Weiwei and Chen, Zhe and Yin, Fuliang and Zhang, Qiaoling},
title = {Melody Extraction From Polyphonic Music Using Particle Filter and Dynamic Programming},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
abstract = {Melody extraction from polyphonic music is one important but challenging task in the music information retrieval community. In this paper, a new melody extraction method based on the particle filter and dynamic programming is proposed. The constant-Q transform is first introduced for multiresolution spectral analysis of polyphonic music. Then, the melody extraction is modeled in the Bayesian filtering framework, and the particle filter is used to get a rough melody contour. Specially, the pitch transition probability of adjacent frames is approximated according to the statistical analysis based on one publicly available dataset, and the likelihood of frame-wise pitches is defined by considering pitch salience, spectral smoothness, and timbre similarity. After that, the preliminary melodic contour obtained by particle filter is smoothed to achieve the frame-wise pitch range limitation. Finally, the dynamic programming is used to accurately track the final melodic contour. The proposed method requires no prior information, and is suitable for both instrumental and vocal melodies. The experimental results show that the performances of the proposed method is robust among four publicly available datasets comparing with the state-of-the-art methods, and it achieves the highest averaged raw pitch accuracy and raw chroma accuracy performances with lower octave errors.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1620–1632},
numpages = {13}
}

@article{10.1109/TASLP.2018.2836436,
author = {Huang, Qinghua and Zhang, Lin and Fang, Yong},
title = {Two-Step Spherical Harmonics ESPRIT-Type Algorithms and Performance Analysis},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2836436},
doi = {10.1109/TASLP.2018.2836436},
abstract = {Spherical arrays have been widely used in direction-of-arrival DOA estimation in recent years, and the high-resolution estimation of signal parameter via rotational invariance technique ESPRIT was developed in the spherical harmonics domain. However, the spherical harmonics ESPRIT SHESPRIT cannot estimate the DOA when the elevation approaches 90°. To solve this problem, we present a two-step SHESPRIT TS-SHESPRIT based on two new recurrence relations of complex spherical harmonics. Furthermore, we develop a real-valued two-step SHESPRIT RTS-SHESPRIT that exploits a unitary matrix to obtain a real-valued relation between the signal subspace and the steering matrix to further reduce the computational complexity. However, the number of sources that are estimated by RTS-SHESPRIT is limited. Therefore, we propose the semi-RTS-SHESPRIT method, which reduces the computational complexity associated with eigenvalue decomposition EVD and avoids the limitations of RTS-SHESPRIT. Relative to SHESPRIT and TS-SHESPRIT, RTS-SHESPRIT and semi-RTS-SHESPRIT reduce the computational burden by 75% during EVD. Furthermore, we derive the mean square errors MSEs of the above algorithms and significantly simplify the MSE expressions. Different expressions for the MSEs are due to different recurrence relations used by different SHESPRIT-type algorithms. All proposed two-step SHESPRIT-type algorithms have higher accuracy than traditional SHESPRIT. The simulation results demonstrate the satisfactory performance of our methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1684–1697},
numpages = {14}
}

@article{10.1109/TASLP.2018.2835729,
author = {He, Hongsen and Chen, Jingdong and Benesty, Jacob and Yang, Tao},
title = {Noise Robust Frequency-Domain Adaptive Blind Multichannel Identification With $\ell _p$-Norm Constraint},
year = {2018},
issue_date = {September 2018},
publisher = {IEEE Press},
volume = {26},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2835729},
doi = {10.1109/TASLP.2018.2835729},
abstract = {Blind multichannel identification is a challenging problem in many domains. The normalized multichannel frequency-domain least-mean-square NMCFLMS algorithm was developed to blindly identify a single-input multiple-output acoustic system, which can yield good performance in noise-free environments. However, the robustness of this algorithm to noise has been shown to be problematic. One way to improve the robustness is by applying a constraint on the spectral flatness of the channel impulse responses, which led to the development of the so-called robust normalized multichannel frequency-domain least-mean-square RNMCFLMS algorithm. This spectral flatness constraint, however, may not be always proper or reasonable in realistic acoustic environments. In this paper, we develop an $ell _p$-norm constraint based robust normalized multichannel frequency-domain least-mean-square $ell _p$-RNMCFLMS algorithm. The $ell _p$ -norm constraint is introduced into the NMCFLMS algorithm to control the effect of different $ell _p$-norm penalties on the adaptive filter for the impulse responses with different degrees of sparseness. Numerical and realistic experiments justify the effectiveness of the proposed $ell _p$-RNMCFLMS algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1608–1619},
numpages = {12}
}

@article{10.1109/TASLP.2018.2828980,
author = {Valentini-Botinhao, Cassia and Yamagishi, Junichi},
title = {Speech Enhancement of Noisy and Reverberant Speech for Text-to-Speech},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2828980},
doi = {10.1109/TASLP.2018.2828980},
abstract = {Text-to-speech voices created from noisy and reverberant recordings are of lower quality. A simple way to improve this is to increase the quality of the recordings prior to text-to-speech training with speech enhancement methods such as noise suppression and dereverberation. In this paper, we opted for this approach and to perform the enhancement, we used a recurrent neural network. The network is trained with parallel data of clean and lower quality recordings of speech. The lower quality data was artificially created by adding recordings of environmental noise to studio-quality recordings of speech and by convolving room impulse responses with these clean recordings. We trained separate networks with noise-only, reverberation-only, and both reverberation and additive noise data. The quality of voices trained with lower quality data that has been enhanced using these networks was significantly higher in all cases. For the noise-only case, the enhanced synthetic voice ranked as high as the voice trained with clean data. For the most realistic and challenging scenario, when both noise and reverberation were present, the improvements were more modest, but still significant.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1420–1433},
numpages = {14}
}

@article{10.1109/TASLP.2018.2829405,
author = {Koutrouvelis, Andreas I. and Sherson, Thomas W. and Heusdens, Richard and Hendriks, Richard C.},
title = {A Low-Cost Robust Distributed Linearly Constrained Beamformer for Wireless Acoustic Sensor Networks With Arbitrary Topology},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2829405},
doi = {10.1109/TASLP.2018.2829405},
abstract = {We propose a new robust distributed linearly constrained beamformer that utilizes a set of linear equality constraints to reduce the cross power spectral density matrix to a block-diagonal form. The proposed beamformer has a convenient objective function for use in arbitrary distributed network topologies while having identical performance to a centralized implementation. Moreover, the new optimization problem is robust to relative acoustic transfer function RATF estimation errors and to target activity detection TAD errors. Two variants of the proposed beamformer are presented and evaluated in the context of multimicrophone speech enhancement in a wireless acoustic sensor network, and are compared with other state-of-the-art distributed beamformers in terms of communication costs and robustness to RATF estimation errors and TAD errors.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1434–1448},
numpages = {15}
}

@article{10.1109/TASLP.2018.2819941,
author = {Yu, Kai and Zhao, Zijian and Wu, Xueyang and Lin, Hongtao and Liu, Xuan},
title = {Rich Short Text Conversation Using Semantic-Key-Controlled Sequence Generation},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2819941},
doi = {10.1109/TASLP.2018.2819941},
abstract = {With the recent advances of the sequence-to-sequence framework, generation approaches for the short text conversation STC become attractive. The traditional sequence-to-sequence approaches for the STC often suffer from poor diversity and general reply without substantiality. It is also hard to control the topic or semantics of the selected reply from multiple generated candidates. In this paper, a novel external-memory-driven sequence-to-sequence learning approach is proposed to address these problems. A tensor of the external memory is constructed to represent interpretable topics or semantics. During generation, a controllable memory trigger is extracted given the input sequence, and a reply is then generated using the memory trigger as well as the sequence-to-sequence model. Experiments show that the proposed approach can generate much richer diversity than the traditional sequence-to-sequence training with attention. Meanwhile, it achieves better quality score in human evaluation. It is also observed that by manually manipulating the memory trigger, it is possible to interpretably guide the topics or semantics of the reply.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1359–1368},
numpages = {10}
}

@article{10.1109/TASLP.2018.2825108,
author = {Lehner, Bernhard and Schluter, Jan and Widmer, Gerhard},
title = {Online, Loudness-Invariant Vocal Detection in Mixed Music Signals},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2825108},
doi = {10.1109/TASLP.2018.2825108},
abstract = {Singing voice detection, also referred to as vocal detection VD, aims at automatically identifying the regions in a music recording where at least one person sings. It is highly challenging due to the timbral and expressive richness of the human singing voice, as well as the practically endless variety of interfering instrumental accompaniment. Additionally, certain instruments have an inherent risk of being misclassified as vocals due to similarities of the sound production system. In this paper, we present a machine learning approach that is based on our previous work for VD, which is specifically designed to deal with those challenging conditions. The contribution of this paper is threefold: First, we present a new method for VD that passes a compact set of features to a long short-term memory recurrent neural network classifier that obtains state-of-the-art results. Second, we thoroughly evaluate the proposed method along with related approaches to really probe the weaknesses of the methods. In order to allow for such a thorough evaluation, we make a curated collection of datasets available to the research community. Finally, we focus on a specific problem that was not obvious and had not been discussed in the literature so far. The reason for this is precisely because limited evaluations had not revealed this as a problem: the lack of loudness invariance. We will discuss the implications of utilizing loudness-related features and show that our method successfully deals with this problem due to the specific set of features it uses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1369–1380},
numpages = {12}
}

@article{10.1109/TASLP.2018.2825432,
author = {Tan, Tian and Qian, Yanmin and Hu and Zhou, Ying and Ding, Wen and Yu, Kai},
title = {Adaptive Very Deep Convolutional Residual Network for Noise Robust Speech Recognition},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2825432},
doi = {10.1109/TASLP.2018.2825432},
abstract = {Although great progress has been made in automatic speech recognition, significant performance degradation still exists in noisy environments. Our previous work has demonstrated the superior noise robustness of very deep convolutional neural networks VDCNN. Based on our work on VDCNNs, this paper proposes a more advanced model referred to as the very deep convolutional residual network VDCRN. This new model incorporates batch normalization and residual learning, showing more robustness than previous VDCNNs.Then, to alleviate the mismatch between the training and testing conditions, model adaptation and adaptive training are developed and compared for the new VDCRN. This paper focuses on factor aware training FAT and cluster adaptive training CAT. For FAT, a unified framework is explored. For CAT, two schemes are first explored to construct the bases in the canonical model; furthermore, a factorized version of CAT is designed to address multiple nonspeech variabilities in one model. Finally, a complete multipass system is proposed to achieve the best system performance in the noisy scenarios. The proposed new approaches are evaluated on three different tasks: Aurora4 simulated data with additive noise and channel distortion, CHiME4 both simulated and real data with additive noise and reverberation, and the AMI meeting transcription task real data with significant reverberation.The evaluation not only includes different noisy conditions, but also covers both simulated and real noisy data. The experiments show that the new VDCRN is more robust, and the adaptation on this model can further significantly reduce the word error rate WER. The proposed best architecture obtains consistent and very large improvements on all tasks compared to the baseline VDCNN or long short-term memory. Particularly, on Aurora4 a new milestone 5.67% WER is achieved by only improving acoustic modeling.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1393–1405},
numpages = {13}
}

@article{10.1109/TASLP.2017.2788181,
author = {Yang, Liner and Zhang, Meishan and Liu, Yang and Sun, Maosong and Yu, Nan and Fu, Guohong},
title = {Joint POS Tagging and Dependence Parsing With Transition-Based Neural Networks},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2788181},
doi = {10.1109/TASLP.2017.2788181},
abstract = {While part-of-speech POS tagging and dependency parsing are observed to be closely related, existing work on joint modeling with manually crafted feature templates suffers from the feature sparsity and incompleteness problems. In this paper, we propose an approach to joint POS tagging and dependency parsing using transition-based neural networks. Three neural network based classifiers are designed to resolve shift/reduce, tagging, and labeling conflicts. Experiments show that our approach significantly outperforms previous methods for joint POS tagging and dependency parsing across a variety of natural languages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1352–1358},
numpages = {7}
}

@article{10.1109/TASLP.2017.2738443,
author = {Wang, Chien-Yao and Wang, Jia-Ching and Santoso, Andri and Chiang, Chin-Chin and Wu, Chung-Hsien},
title = {Sound Event Recognition Using Auditory-Receptive-Field Binary Pattern and Hierarchical-Diving Deep Belief Network},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2738443},
doi = {10.1109/TASLP.2017.2738443},
abstract = {Automatic sound event recognition SER has recently attracted renewed interest. Although practical SER system has many useful applications in everyday life, SER is challenging owing to the variations among sounds and noises in the real-world environment. This paper presents a novel feature extraction and classification method to solve the problem of SER. An audio–visual descriptor, called the auditory-receptive-field binary pattern, is designed based on the spectrogram image feature, the cepstral features, and the human auditory receptive field model. The extracted features are then fed into a classifier to perform event classification. The proposed classifier, called the hierarchical-diving deep belief network, is a deep neural network system that hierarchically learns the discriminative characteristics from physical feature representation to the abstract concept. The performance of our proposed system was verified using several experiments under various conditions. Using the RWCP dataset, the proposed system achieved a recognition rate of 99.27% for real-world sound data in 105 categories. Under noisy conditions, the developed system is very robust, with which it achieved 95.06% recognition rate with 0 dB signal-to-noise ratio. Using the TUT sound event dataset, the proposed system achieves error rates of 0.81 and 0.73 in sound event detection in home and residential area scenes. The experimental results reveal that the proposed system outperformed the other systems in this field.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1336–1351},
numpages = {16}
}

@article{10.1109/TASLP.2018.2825601,
author = {Stone, Simon and Marxen, Michael and Birkholz, Peter},
title = {Construction and Evaluation of a Parametric One-Dimensional Vocal Tract Model},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2825601},
doi = {10.1109/TASLP.2018.2825601},
abstract = {Articulatory speech synthesis based on aero-acoustic simulations of the vocal tract is computationally expensive and, therefore, requires simple yet precise models. Modeling the one-dimensional vocal tract area function directly instead of a higher dimensional vocal tract model is an efficient way to minimize the computational overhead of the simulations. In this paper, we propose a new parametric vocal tract model that is controlled by six points and capable of modeling a large variety of vocal tract shapes. We geometrically and perceptually evaluated the model on a set of 22 reference area functions corresponding to German vowels and consonants. The model was able to geometrically approximate the reference area functions with a minimum root-mean-square error of 0.302&nbsp;cm$^2$, a maximum error of 1.142&nbsp;cm$^2$, and a median error of 0.891&nbsp;cm$^2$. After optimizations, a perceptual evaluation of the synthesis using our model in combination with a state-of-the-art aero-acoustic simulation achieved a vowel recognition rate of 90.7% and a consonant recognition rate of 73.2%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1381–1392},
numpages = {12}
}

@article{10.1109/TASLP.2018.2825440,
author = {Rafii, Zafar and Liutkus, Antoine and Stoter, Fabian-Robert and Mimilakis, Stylianos Ioannis and FitzGerald, Derry and Pardo, Bryan},
title = {An Overview of Lead and Accompaniment Separation in Music},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2825440},
doi = {10.1109/TASLP.2018.2825440},
abstract = {Popular music is often composed of an accompaniment and a lead component, the latter typically consisting of vocals. Filtering such mixtures to extract one or both components has many applications, such as automatic karaoke and remixing. This particular case of source separation yields very specific challenges and opportunities, including the particular complexity of musical structures, but also relevant prior knowledge coming from acoustics, musicology or sound engineering. Due to both its importance in applications and its challenging difficulty, lead and accompaniment separation has been a popular topic in signal processing for decades. In this article, we provide a comprehensive review of this research topic, organizing the different approaches according to whether they are model-based or data-centered. For model-based methods, we organize them according to whether they concentrate on the lead signal, the accompaniment, or both. For data-centered approaches, we discuss the particular difficulty of obtaining data for learning lead separation systems, and then review recent approaches, notably those based on deep learning. Finally, we discuss the delicate problem of evaluating the quality of music separation through adequate metrics and present the results of the largest evaluation, to-date, of lead and accompaniment separation systems. In conjunction with the above, a comprehensive list of references is provided, along with relevant pointers to available implementations and repositories.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1307–1335},
numpages = {29}
}

@article{10.1109/TASLP.2018.2828650,
author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
title = {Autoregressive Neural F0 Model for Statistical Parametric Speech Synthesis},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2828650},
doi = {10.1109/TASLP.2018.2828650},
abstract = {Recurrent neural networks RNNs have been successfully used as fundamental frequency F0 models for text-to-speech synthesis. However, this paper showed that a normal RNN may not take into account the statistical dependency of the F0 data across frames and consequently only generate noisy F0 contours when F0 values are sampled from the model. A better model may take into account the causal dependency of the current F0 datum on the previous frames’ F0 data. One such model is the shallow autoregressive AR recurrent mixture density network SAR that we recently proposed. However, as this study showed, an SAR is equivalent to the combination of trainable linear filters and a conventional RNN. It is still weak for F0 modeling. To better model the temporal dependency in F0 contours, we propose a deep AR model DAR. On the basis of an RNN, this DAR propagates the previous frame's F0 value through the RNN, which allows nonlinear AR dependency to be achieved. We also propose F0 quantization and data dropout strategies for the DAR. Experiments on a Japanese corpus demonstrated that this DAR can generate appropriate F0 contours by using the random-sampling-based generation method, which is impossible for the baseline RNN and SAR. When a conventional mean-based generation method was used in the proposed DAR and other experimental models, the DAR generated accurate and less oversmoothed F0 contours and achieved a better mean-opinion-score in a subjective evaluation test.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1406–1419},
numpages = {14}
}

@article{10.1109/TASLP.2018.2821899,
author = {Santos, Joao Felipe and Falk, Tiago H.},
title = {Speech Dereverberation With Context-Aware Recurrent Neural Networks},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2821899},
doi = {10.1109/TASLP.2018.2821899},
abstract = {In this paper, we propose a model to perform speech dereverberation by estimating its spectral magnitude from the reverberant counterpart. Our models are capable of extracting features that take into account both short- and long-term dependencies in the signal through a convolutional encoder which extracts features from a short, bounded context of frames and a recurrent neural network for extracting long-term information. Our model outperforms a recently proposed model that uses different context information depending on the reverberation time, without requiring any sort of additional input, yielding improvements of up to 0.4 on perceptual evaluation of speech quality, 0.3 on short-time objective intelligibility, and 1.0 on perceptual objective listening quality assessment relative to reverberant speech. We also show our model is able to generalize to real room impulse responses even when only trained with simulated room impulse responses, different speakers, and high reverberation times. Finally, listening tests show the proposed method outperforming benchmark models in reduction of perceived reverberation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1232–1242},
numpages = {11}
}

@article{10.1109/TASLP.2018.2825110,
author = {Farmani, Mojtaba and Pedersen, Michael Syskind and Tan, Zheng-Hua and Jensen, Jesper},
title = {Bias-Compensated Informed Sound Source Localization Using Relative Transfer Functions},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2825110},
doi = {10.1109/TASLP.2018.2825110},
abstract = {In this paper, we consider the problem of estimating the target sound direction of arrival DoA for a hearing aid HA system, which can connect to a wireless microphone worn by the talker of interest. The wireless microphone “informs” the HA system about the noise-free target speech. To estimate the DoA, we consider a maximum-likelihood approach, and we assume that a database of DoA-dependent relative transfer functions RTFs has been measured in advance and is available. The proposed DoA estimator is able to take the available noise-free target speech, ambient noise characteristics, and the shadowing effect of the user's head on the received signals into account, and it supports both monaural and binaural microphone array configurations. Moreover, we analytically analyze the bias in the proposed estimator and introduce a modified estimator, which has been compensated for the bias. We demonstrate that the proposed method has lower computational complexity and better performance than recent RTF-based estimators. Furthermore, to decrease the number of parameters required to be wirelessly exchanged between the HAs in binaural configurations, we propose an information fusion strategy, which avoids transmitting microphone signals between the HAs. An important benefit of the proposed IF strategy is that the number of parameters to be exchanged between the HAs is independent of the number of HA microphones. Finally, we investigate the performance of variants of the proposed estimator extensively in different noisy and reverberant situations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1271–1285},
numpages = {15}
}

@article{10.1109/TASLP.2018.2819900,
author = {Del-Agua, Miguel Angel and Gimenez, Adria and Sanchis, Albert and Civera, Jorge and Juan, Alfons},
title = {Speaker-Adapted Confidence Measures for ASR Using Deep Bidirectional Recurrent Neural Networks},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2819900},
doi = {10.1109/TASLP.2018.2819900},
abstract = {In the last years, deep bidirectional recurrent neural networks DBRNN and DBRNN with long short-term memory cells DBLSTM have outperformed the most accurate classifiers for confidence estimation in automatic speech recognition. At the same time, we have recently shown that speaker adaptation of confidence measures using DBLSTM yields significant improvements over non-adapted confidence measures. In accordance with these two recent contributions to the state of the art in confidence estimation, this paper presents a comprehensive study of speaker-adapted confidence measures using DBRNN and DBLSTM models. First, we present new empirical evidences of the superiority of recurrent neural networks RNN-based confidence classifiers evaluated over a large speech corpus consisting of the English LibriSpeech and the Spanish poliMedia tasks. Second, we show new results on speaker-adapted confidence measures considering a multitask framework in which RNN-based confidence classifiers trained with LibriSpeech are adapted to speakers of the TED-LIUM corpus. These experiments confirm that speaker-adapted confidence measures outperform their non-adapted counterparts. Last, we describe an unsupervised adaptation method of the acoustic DBLSTM model based on confidence measures that results in better automatic speech recognition performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1194–1202},
numpages = {9}
}

@article{10.1109/TASLP.2018.2823081,
author = {Marquardt, Daniel and Doclo, Simon},
title = {Interaural Coherence Preservation for Binaural Noise Reduction Using Partial Noise Estimation and Spectral Postfiltering},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2823081},
doi = {10.1109/TASLP.2018.2823081},
abstract = {The objective of binaural speech enhancement algorithms is to reduce the undesired noise component, while preserving the desired speech source and the binaural cues of all sound sources. For the scenario of a single desired speech source in a diffuse noise field, an extension of the binaural multichannel Wiener filter MWF, namely the MWF-IC, has been recently proposed, which aims to preserve the interaural coherence IC of the noise component. However, due to the large complexity of the MWF-IC, in this paper we propose several alternative algorithms at a lower computational complexity. First, we consider a quasi-distortionless version of the MWF-IC, denoted as minimum-variance-distortionless response MVDR-IC. Second, we propose to preserve the IC of the noise component using the binaural MWF with partial noise estimation MWF-N and the binaural MVDR beamformer with partial noise estimation MVDR-N, for which closed-form expressions exist. In addition, we show that for the MVDR-N a closed-form expression can be derived for the tradeoff parameter yielding a desired magnitude squared coherence MSC for the output noise component. Since contrary to the MWF-IC and the MWF-N the MVDR-IC and the MVDR-N do not take into account the spectro-temporal properties of the speech and the noise components, we propose to apply a spectral postfilter to the filter outputs, improving the noise reduction performance. The performance of all algorithms is compared in several diffuse noise scenarios. The simulation results show that both the MVDR-IC and the MVDR-N are able to preserve the MSC of the noise component, while generally the MVDR-IC shows a slightly better noise reduction performance at a larger complexity. Further, simulation results show that applying a spectral postfilter leads to a very similar performance for all considered algorithms in terms of noise reduction and speech distortion.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1257–1270},
numpages = {14}
}

@article{10.1109/TASLP.2018.2815268,
author = {Tao, Fei and Busso, Carlos},
title = {Gating Neural Network for Large Vocabulary Audiovisual Speech Recognition},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2815268},
doi = {10.1109/TASLP.2018.2815268},
abstract = {Audio-based automatic speech recognition A-ASR systems are affected by noisy conditions in real-world applications. Adding visual cues to the ASR system is an appealing alternative to improve the robustness of the system, replicating the audiovisual perception process used during human interactions. A common problem observed when using audiovisual automatic speech recognition AV-ASR is the drop in performance when speech is clean. In this case, visual features may not provide complementary information, introducing variability that negatively affects the performance of the system. The experimental evaluation in this study clearly demonstrates this problem when we train an audiovisual state-of-the-art hybrid system with a deep neural network DNN and hidden Markov models HMMs. This study proposes a framework that addresses this problem, improving, or at least, maintaining the performance when visual features are used. The proposed approach is a deep learning solution with a gating layer that diminishes the effect of noisy or uninformative visual features, keeping only useful information. The framework is implemented with a subset of the audiovisual CRSS-4ENGLISH-14 corpus which consists of 61 h of speech from 105 subjects simultaneously collected with multiple cameras and microphones. The proposed framework is compared with conventional HMMs with observation models implemented with either a Gaussian mixture model or DNNs. We also compare the system with a multi-stream HMM system. The experimental evaluation indicates that the proposed framework outperforms alternative methods under all configurations, showing the robustness of the gating-based framework for AV-ASR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1286–1298},
numpages = {13}
}

@article{10.1109/TASLP.2018.2818408,
author = {Yoshimura, Takenori and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
title = {Mel-Cepstrum-Based Quantization Noise Shaping Applied to Neural-Network-Based Speech Waveform Synthesis},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2818408},
doi = {10.1109/TASLP.2018.2818408},
abstract = {This paper presents a mel-cepstrum-based quantization noise shaping method for improving the quality of synthetic speech generated by neural-network-based speech waveform synthesis systems. Since mel-cepstral coefficients closely match the characteristics of human auditory perception, the proposed method effectively masks the white noise introduced by the quantization typically used in neural-network-based speech waveform synthesis systems. The paper also describes a computationally efficient implementation of the proposed method using the structure of the mel-log spectrum approximation filter. Experiments using the WaveNet generative model, which is a state-of-the-art model for neural-network-based speech waveform synthesis, showed that speech quality is significantly improved by the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1173–1180},
numpages = {8}
}

@article{10.1109/TASLP.2018.2817798,
author = {Wang, Qing and Du, Jun and Dai, Li-Rong and Lee, Chin-Hui},
title = {A Multiobjective Learning and Ensembling Approach to High-Performance Speech Enhancement With Compact Neural Network Architectures},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2817798},
doi = {10.1109/TASLP.2018.2817798},
abstract = {In this study, we propose a novel deep neural network DNN architecture for speech enhancement SE via a multiobjective learning and ensembling MOLE framework to achieve a compact and lowlatency design, while maintaining good performance in quality evaluations. MOLE follows the boosting concept when combining weak models into a strong classifier and consists of two compact DNNs. The first, called the multiobjective learning DNN MOL-DNN, takes multiple features, such as log-power spectra LPS, mel-frequency cepstral coefficients MFCCs and Gammatone frequency cepstral coefficients GFCCs to predict a multiobjective set that includes clean speech feature, dynamic noise feature, and ideal ratio mask IRM. The second, called the multiobjective ensembling DNN MOE-DNN, takes the learned features from MOL-DNN as inputs and separately predicts clean LPS and IRM, clean MFCC and IRM, and clean GFCC and IRM using three sets of weak regression functions. Finally, a postprocessing operation can be applied to the estimated clean features by leveraging the multiple targets learned from both the MOL-DNN and the MOE-DNN. On speech corrupted by 15 noise types not seen in model training the SE results show that the MOLE approach, which features a small model size and low run-time latency, can achieve consistent improvements over both DNN- and long short-term memory LSTM-based techniques in terms of all the objective metrics evaluated in this study for all three cases the input contexts contain 1-frame, 4-frame and 7-frame instances. The 1-frame MOLE-based SE system outperforms the DNN-based SE system with a 7-frame input expansion at a 3-frame delay and also achieves better performance than the LSTM-based SE system with 4-frame, no delay expansion by including only 3 previous frames, and with 170 times less processing latency.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1181–1193},
numpages = {13}
}

@article{10.1109/TASLP.2018.2819819,
author = {Stankovic, Ljubisa and Brajovic, Milos},
title = {Analysis of the Reconstruction of Sparse Signals in the DCT Domain Applied to Audio Signals},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2819819},
doi = {10.1109/TASLP.2018.2819819},
abstract = {Sparse signals can be reconstructed from a reduced set of signal samples using compressive sensing CS methods. The discrete cosine transform DCT can provide highly concentrated representations of audio signals. This property implies the DCT as a good sparsity domain for the audio signals. In this paper, the DCT is studied within the context of sparse audio signal processing using the CS theory and methods. The DCT coefficients of a sparse signal, calculated with a reduced set of available samples, can be modeled as random variables. It has been shown that the statistical properties of these variables are closely related to the unique reconstruction conditions. The main result of this paper is in an exact formula for the mean-square reconstruction error in the case of approximately sparse and nonsparse noisy signals reconstructed under the sparsity assumption. Based on the presented analysis, a simple and computationally efficient reconstruction algorithm is proposed. The presented theoretical concepts and the efficiency of the reconstruction algorithm are verified numerically, including examples with synthetic and recorded audio signals with unavailable or corrupted samples. Random disturbances and disturbances simulating clicks or inpainting in audio signals are considered. Statistical verification is done on a dataset with experimental signals. Results are compared with some classical and recent methods used in similar signal and disturbance scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1216–1231},
numpages = {16}
}

@article{10.1109/TASLP.2018.2821846,
author = {Geronazzo, Michele and Spagnol, Simone and Avanzini, Federico},
title = {Do We Need Individual Head-Related Transfer Functions for Vertical Localization? The Case Study of a Spectral Notch Distance Metric},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2821846},
doi = {10.1109/TASLP.2018.2821846},
abstract = {This paper deals with the issue of individualizing the head-related transfer function HRTF rendering process for auditory elevation perception. Is it possible to find a nonindividual, personalized HRTF set that allows a listener to have an equally accurate localization performance than with his/her individual HRTFs? We propose a psychoacoustically motivated, anthropometry based mismatch function between HRTF pairs that exploits the close relation between the listener's pinna geometry and localization cues. This is evaluated using an auditory model that computes a mapping between HRTF spectra and perceived spatial locations. Results on a large number of subjects in the center for image processing and integrated computing CIPIC and acoustics research institute ARI HRTF databases suggest that there exists a nonindividual HRTF set, which allows a listener to have an equally accurate vertical localization than with individual HRTFs. Furthermore, we find the optimal parameterization of the proposed mismatch function, i.e., the one that best reflects the information given by the auditory model. Our findings show that the selection procedure yields statistically significant improvements with respect to dummy-head HRTFs or random HRTF selection, with potentially high impact from an applicative point of view.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1243–1256},
numpages = {14}
}

@article{10.1109/TASLP.2018.2820429,
author = {Proenca, Jorge and Lopes, Carla and Tjalve, Michael and Stolcke, Andreas and Candeias, Sara and Perdigao, Fernando},
title = {Mispronunciation Detection in Children's Reading of Sentences},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Press},
volume = {26},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2820429},
doi = {10.1109/TASLP.2018.2820429},
abstract = {This paper proposes an approach to automatically parse children's reading of sentences by detecting word pronunciations and extra content, and to classify words as correctly or incorrectly pronounced. This approach can be directly helpful for automatic assessment of reading level or for automatic reading tutors, where a correct reading must be identified. We propose a first segmentation stage to locate candidate word pronunciations based on allowing repetitions and false starts of a word's syllables. A decoding grammar based solely on syllables allows silence to appear during a word pronunciation. At a second stage, word candidates are classified as mispronounced or not. The feature that best classifies mispronunciations is found to be the log-likelihood ratio between a free phone loop and a word spotting model in the very close vicinity of the candidate segmentation. Additional features are combined in multifeature models to further improve classification, including: normalizations of the log-likelihood ratio, derivations from phone likelihoods, and Levenshtein distances between the correct pronunciation and recognized phonemes through two phoneme recognition approaches. Results show that most extra events were detected close to 2% word error rate achieved and that using automatic segmentation for mispronunciation classification approaches the performance of manual segmentation. Although the log-likelihood ratio from a spotting approach is already a good metric to classify word pronunciations, the combination of additional features provides a relative reduction of the miss rate of 18% from 34.03% to 27.79% using manual segmentation and from 35.58% to 29.35% using automatic segmentation, at constant 5% false alarm rate.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1203–1215},
numpages = {13}
}

@article{10.1109/TASLP.2018.2804172,
author = {Braun, Sebastian and Kuklasinski, Adam and Schwartz, Ofer and Thiergart, Oliver and Habets, Emanuel A. P. and Gannot, Sharon and Doclo, Simon and Jensen, Jesper},
title = {Evaluation and Comparison of Late Reverberation Power Spectral Density Estimators},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2804172},
doi = {10.1109/TASLP.2018.2804172},
abstract = {Reduction of late reverberation can be achieved using spatio-spectral filters, such as the multichannel Wiener filter. To compute this filter, an estimate of the late reverberation power spectral density PSD is required. In recent years, a multitude of late reverberation PSD estimators have been proposed. In this paper, these estimators are categorized into several classes, their relations and differences are discussed, and a comprehensive experimental comparison is provided. To compare their performance, simulations in controlled as well as practical scenarios are conducted. It is shown that a common weakness of spatial coherence-based estimators is their performance in high direct-to-diffuse ratio conditions. To mitigate this problem, a correction method is proposed and evaluated. It is shown that the proposed correction method can decrease the speech distortion without significantly affecting the reverberation reduction.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1052–1067},
numpages = {16}
}

@article{10.1109/TASLP.2018.2798804,
author = {Donley, Jacob and Ritz, Christian and Kleijn, W. Bastiaan},
title = {Multizone Soundfield Reproduction With Privacy- and Quality-Based Speech Masking Filters},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2798804},
doi = {10.1109/TASLP.2018.2798804},
abstract = {Reproducing zones of personal sound is a challenging signal processing problem that has garnered considerable research interest in recent years. We introduce in this work an extended method to multizone soundfield reproduction that overcomes issues with speech privacy and quality. Measures of speech intelligibility contrast SIC and speech quality are used as cost functions in an optimization of speech privacy and quality. Novel spatial and temporal frequency domain speech masker filter designs are proposed to accompany the optimization process. Spatial masking filters are designed using multizone soundfield algorithms that are dependent on the target speech multizone reproduction. Combinations of estimates of acoustic contrast and long term average speech spectra are proposed to provide equal masking influence on speech privacy and quality. Spatial aliasing specific to multizone soundfield reproduction geometry is further considered in analytically derived low-pass filters. Simulated and real-world experiments are conducted to verify the performance of the proposed method using semi-circular and linear loudspeaker arrays. Simulated implementations of the proposed method show that significant SIC and speech quality is achievable between zones. A range of perceptual evaluation of speech quality mean opinion scores that indicate good quality are obtained while at the same time providing confidential privacy as indicated by SIC. The simulations also show that the method is robust to variations in the speech, virtual source location, array geometry, and number of loudspeakers. Real-world experiments confirm the practicality of the proposed methods by showing that good quality and confidential privacy are achievable.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1037–1051},
numpages = {15}
}

@article{10.1109/TASLP.2018.2795746,
author = {Kameoka, Hirokazu and Higuchi, Takuya and Tanaka, Mikihiro and Li},
title = {Nonnegative Matrix Factorization With Basis Clustering Using Cepstral Distance Regularization},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2795746},
doi = {10.1109/TASLP.2018.2795746},
abstract = {One successful approach for audio source separation involves applying nonnegative matrix factorization NMF to a magnitude spectrogram regarded as a nonnegative matrix. This can be interpreted as approximating the observed spectra at each time frame as the linear sum of the basis spectra scaled by time-varying amplitudes. This paper deals with the problem of the unsupervised instrument-wise source separation of polyphonic signals based on an extension of the NMF approach. We focus on the fact that each piece of music is typically played on a handful of musical instruments, which allows us to assume that the spectra of the underlying audio events in a polyphonic signal can be grouped into a reasonably small number of clusters in the mel-frequency cepstral coefficient MFCC domain. Based on this assumption, we propose formulating factorization of a magnitude spectrogram and clustering of the basis spectra in the MFCC domain as a joint optimization problem and derive a novel optimization algorithm based on the majorization–minimization principle. Experimental results revealed that our method was superior to a two-stage algorithm that consists of performing factorization followed by clustering the basis spectra, thus showing the advantage of the joint optimization approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1025–1036},
numpages = {12}
}

@article{10.1109/TASLP.2018.2816241,
author = {Krawczyk-Becker, Martin and Gerkmann, Timo},
title = {On Speech Enhancement Under PSD Uncertainty},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2816241},
doi = {10.1109/TASLP.2018.2816241},
abstract = {Many well-known and frequently employed Bayesian clean speech estimators have been derived under the assumption that the true power spectral densities PSDs of speech and noise are exactly known. In practice, however, only power spectral density PSD estimates are available. Simply neglecting PSD estimation errors and handling the estimates as true values leads to speech estimation errors causing musical noise and undesired suppression of speech. In this paper, the uncertainty of the available speech PSD estimates is addressed. The main contributions are the following. First, we summarize and examine ways to model and incorporate the uncertainty of PSD estimates for a more robust speech enhancement performance. Second, a novel nonlinear clean speech estimator is derived that takes into account prior knowledge about the absolute value of typical speech PSDs. Third, we show that the derived statistical framework provides uncertainty-aware counterparts to a number of well-known conventional clean speech estimators such as the Wiener filter and Ephraim and Malah's amplitude estimators. Fourth, we show how modern PSD estimators can be incorporated into the theoretical framework and propose to employ frequency dependent priors. Finally, the effects and benefits of considering the uncertainty of speech PSD estimates are analyzed, discussed, and evaluated via instrumental measures and a listening experiment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1140–1149},
numpages = {10}
}

@article{10.1109/TASLP.2018.2811247,
author = {Braun, Sebastian and Habets, Emanuel A. P.},
title = {Linear Prediction-Based Online Dereverberation and Noise Reduction Using Alternating Kalman Filters},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2811247},
doi = {10.1109/TASLP.2018.2811247},
abstract = {Multichannel linear prediction-based dereverberation in the short-time Fourier transform STFT domain has been shown to be highly effective. Using this framework, the desired dereverberated multichannel signal is obtained by filtering the noise-free reverberant signals using the estimated multichannel autoregressive MAR coefficients. To use such methods in the presence of noise, especially in the case of online processing, remains a challenging problem. Existing sequential enhancement structures, which first remove the noise and then estimate the MAR coefficients, suffer from a causality problem as both the optimal noise reduction and dereverberation stages depend on the current output of each other. To address this problem, an algorithm that consists of two alternating Kalman filters to estimate the noise-free reverberant signals and the MAR coefficients is proposed. The causality of the estimation procedure is important when dealing with time-variant acoustic scenarios, where the MAR coefficients are time-varying. The proposed method is evaluated using simulated and measured acoustic impulse responses and is compared to a method based on the same signal model. In addition, a method to control the reverberation reduction and noise reduction independently is derived.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1115–1125},
numpages = {11}
}

@article{10.1109/TASLP.2018.2811540,
author = {Magron, Paul and Badeau, Roland and David, Bertrand},
title = {Model-Based STFT Phase Recovery for Audio Source Separation},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2811540},
doi = {10.1109/TASLP.2018.2811540},
abstract = {For audio source separation applications, it is common to estimate the magnitude of the short-time Fourier transform STFT of each source. In order to further synthesize time-domain signals, it is necessary to recover the phase of the corresponding complex-valued STFT. Most authors in this field choose a Wiener-like filtering approach, which boils down to use the phase of the original mixture. In this paper, a different standpoint is adopted. Many music events are partially composed of slowly varying sinusoids and the STFT phase increment over time of those frequency components takes a specific form. This allows phase recovery by an unwrapping technique once a short-term frequency estimate has been obtained. Herein, a novel iterative source separation procedure is proposed that builds upon these results. It consists in minimizing the mixing error by means of the auxiliary function method. This procedure is initialized by exploiting the unwrapping technique in order to generate estimates that benefit from a temporal continuity property. Experiments conducted on realistic music pieces show that, given accurate magnitude estimates, this procedure outperforms the state-of-the-art consistent Wiener filter.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1091–1101},
numpages = {11}
}

@article{10.1109/TASLP.2018.2811184,
author = {Kodrasi, Ina and Doclo, Simon},
title = {Analysis of Eigenvalue Decomposition-Based Late Reverberation Power Spectral Density Estimation},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2811184},
doi = {10.1109/TASLP.2018.2811184},
abstract = {Many speech dereverberation techniques require an estimate of the late reverberation power spectral density&nbsp;PSD. State-of-the-art multichannel methods for estimating the late reverberation PSD typically rely on first, an estimate of the relative transfer functions&nbsp;RTFs of the target signal; second, a model for the spatial coherence matrix of the late reverberation; and finally, an estimate of the reverberant speech or reverberant and noisy speech PSD matrix. The RTFs, the spatial coherence matrix, and the speech PSD matrix are all prone to modeling and estimation errors in practice, with the RTFs being particularly difficult to estimate accurately, especially in highly reverberant and noisy scenarios. Recently, we proposed an eigenvalue decomposition&nbsp;EVD-based late reverberation PSD estimator, which does not require an estimate of the RTFs. In this paper, this EVD-based PSD estimator is further analyzed and its estimation accuracy and computational complexity are analytically compared to a state-of-the-art maximum likelihood ML based PSD estimator. It is shown that for perfect knowledge of the RTFs, spatial coherence matrix, and reverberant speech PSD matrix, the ML-based and the EVD-based PSD estimates are both equal to the true late reverberation PSD. In addition, it is shown that for erroneous RTFs but perfect knowledge of the spatial coherence matrix and reverberant speech PSD matrix, the ML-based PSD estimate is larger than or equal to the true late reverberation PSD, whereas the EVD-based PSD estimate is obviously still equal to the true late reverberation PSD. Finally, it is shown that when modeling and estimation errors occur in all quantities, the ML-based PSD estimate is larger than or equal to the EVD-based PSD estimate. Simulation results for several realistic acoustic scenarios demonstrate the advantages of using the EVD-based PSD estimator in a multichannel Wiener filter, yielding a significantly better performance than the ML-based PSD estimator.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1102–1114},
numpages = {13}
}

@article{10.1109/TASLP.2018.2815780,
author = {Ram, Dhananjay and Asaei, Afsaneh and Bourlard, Herve},
title = {Sparse Subspace Modeling for Query by Example Spoken Term Detection},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2815780},
doi = {10.1109/TASLP.2018.2815780},
abstract = {This paper focuses on the problem of query by example spoken term detection QbE-STD in zero-resource scenario. Current state-of-the-art approaches to tackle this problem rely on dynamic programming based template matching techniques using phone posterior features extracted at the output of a deep neural network. Previously, it has been shown that the space of phone posteriors is highly structured, as a union of low-dimensional subspaces. To exploit the temporal and sparse structure of the speech data, we investigate here three different QbE-STD systems based on sparse model recovery. More specifically, we use query examples to model the query subspace using dictionary for sparse coding. Reconstruction errors calculated using sparse representation of feature vectors are then used to characterize the underlying subspaces. The first approach uses these reconstruction errors in a dynamic programming framework to detect the spoken query, resulting in a much faster search compared to standard template matching. The other two methods aim at merging template matching and sparsity-based approaches to further improve the performance. The first one proposes to regularize the template matching local distances using sparse reconstruction errors. The second approach aims at using the sparse reconstruction errors to rescore improve the template matching likelihood. Experiments on two different databases AMI and MediaEval show that the proposed hybrid systems perform better than a highly competitive QbE-STD baseline system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1126–1139},
numpages = {14}
}

@article{10.1109/TASLP.2018.2813011,
author = {Leglaive, Simon and Badeau, Roland and Richard, Gael},
title = {Student's t Source and Mixing Models for Multichannel Audio Source Separation},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2813011},
doi = {10.1109/TASLP.2018.2813011},
abstract = {This paper presents a Bayesian framework for under-determined audio source separation in multichannel reverberant mixtures. We model the source signals as Student's t latent random variables in a time-frequency domain. The specific structure of musical signals in this domain is exploited by means of a nonnegative matrix factorization model. Conversely, we design the mixing model in the time domain. In addition to leading to an exact representation of the convolutive mixing process, this approach allows us to develop simple probabilistic priors for the mixing filters. Indeed, as those filters correspond to room responses they exhibit a simple characteristic structure in the time domain that can be used to guide their estimation. We also rely on the Student's t distribution for modeling the impulse response of the mixing filters. From this model, we develop a variational inference algorithm in order to perform source separation. The experimental evaluation demonstrates the potential of this approach for separating multichannel reverberant mixtures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1150–1164},
numpages = {15}
}

@article{10.1109/TASLP.2018.2806745,
author = {Benaroya, Elie Laurent and Obin, Nicolas and Liuni, Marco and Roebel, Axel and Raumel, Wilson and Argentieri, Sylvain},
title = {Binaural Localization of Multiple Sound Sources by Non-Negative Tensor Factorization},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2806745},
doi = {10.1109/TASLP.2018.2806745},
abstract = {This paper presents non-negative factorization of audio signals for the binaural localization of multiple sound sources within realistic and unknown sound environments. Non-negative tensor factorization NTF provides a sparse representation of multichannel audio signals in time, frequency, and space that can be exploited in computational audio scene analysis and robot audition for the separation and localization of sound sources. In the proposed formulation, each sound source is represented by means of spectral dictionaries, temporal activation, and its distribution within each channel here, left and right ears. This distribution, being dependent on the frequency, can be interpreted as an explicit estimation of the Head-Related Transfer Function HRTF of a binaural head which can then be converted into the estimated sound source position. Moreover, the semisupervised formulation of the non-negative factorization allows us to integrate prior knowledge about some sound sources of interest whose dictionaries can be learned in advance, whereas the remaining sources are considered as background sound, which remains unknown and is estimated on the fly. The proposed NTF-based sound source localization is applied here to binaural sound source localization of multiple speakers within realistic sound environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1068–1078},
numpages = {11}
}

@article{10.1109/TASLP.2018.2809864,
author = {Perraudin, Nathanael and Holighaus, Nicki and Majdak, Piotr and Balazs, Peter},
title = {Inpainting of Long Audio Segments With Similarity Graphs},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2809864},
doi = {10.1109/TASLP.2018.2809864},
abstract = {We present a novel method for the compensation of long duration data loss in audio signals, in particular music. The concealment of such signal defects is based on a graph that encodes signal structure in terms of time-persistent spectral similarity. A suitable candidate segment for the substitution of the lost content is proposed by an intuitive optimization scheme and smoothly inserted into the gap, i.e., the lost or distorted signal region. Extensive listening tests show that the proposed algorithm provides highly promising results when applied to a variety of real-world music signals.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1079–1090},
numpages = {12}
}

@article{10.1109/TASLP.2018.2798811,
author = {Ling, Zhen-Hua and Ai, Yang and Gu, Yu and Dai, Li-Rong},
title = {Waveform Modeling and Generation Using Hierarchical Recurrent Neural Networks for Speech Bandwidth Extension},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2798811},
doi = {10.1109/TASLP.2018.2798811},
abstract = {This paper presents a waveform modeling and generation method using hierarchical recurrent neural networks HRNN for speech bandwidth extension BWE. Different from conventional BWE methods that predict spectral parameters for reconstructing wideband speech waveforms, this BWE method models and predicts waveform samples directly without using vocoders. Inspired by SampleRNN, which is an unconditional neural audio generator, the HRNN model represents the distribution of each wideband or high-frequency waveform sample conditioned on the input narrowband waveform samples using a neural network composed of long short-term memory LSTM layers and feed-forward layers. The LSTM layers form a hierarchical structure and each layer operates at a specific temporal resolution to efficiently capture long-span dependencies between temporal sequences. Furthermore, additional conditions, such as the bottleneck features derived from narrowband speech using a deep neural network based state classifier, are employed as auxiliary input to further improve the quality of generated wideband speech. The experimental results of comparing several waveform modeling methods show that the HRNN-based method can achieve better speech quality and run-time efficiency than the dilated convolutional neural network based method and the plain sample-level recurrent neural network based method. Our proposed method also outperforms the conventional vocoder-based BWE method using LSTM-RNNs in terms of the subjective quality of the reconstructed wideband speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {883–894},
numpages = {12}
}

@article{10.1109/TASLP.2018.2800290,
author = {Chang, Jiho and Marschall, Marton},
title = {Periphony-Lattice Mixed-Order Ambisonic Scheme for Spherical Microphone Arrays},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2800290},
doi = {10.1109/TASLP.2018.2800290},
abstract = {Most methods for sound field reconstruction and spherical beamforming with spherical microphone arrays are mathematically based on the spherical harmonics expansion. In many cases, this expansion is truncated at a certain order as in higher order ambisonics HOA. This truncation leads to performance that is independent of the incident direction of the sound waves. On the other hand, mixed-order ambisonic MOA schemes that select an appropriate subset of spherical harmonics can improve the performance for horizontal directions at the expense of other directions. This paper proposes an MOA scheme called Periphony-Lattice to improve sound field reconstruction performance for horizontally incident sound waves. The proposed scheme is compared with the previously introduced MOA and HOA schemes in terms of theoretical truncation error and performance in sound field reconstruction and spherical beamforming. Computer simulations and measurements are conducted with a spherical array of 52 microphones with a nonuniform layout. The results show that the proposed MOA scheme has better performance in sound field reconstruction and spherical beamforming for horizontal sound waves than the other schemes for a given number of microphones. This scheme can be applied to other spherical array layouts if the number of microphones is greater than that of the required spherical harmonics coefficients, and may improve the horizontal performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {924–936},
numpages = {13}
}

@article{10.1109/TASLP.2018.2800283,
author = {Zheng, Chengshi and Deleforge, Antoine and Li, Xiaodong and Kellermann, Walter},
title = {Statistical Analysis of the Multichannel Wiener Filter Using a Bivariate Normal Distribution for Sample Covariance Matrices},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2800283},
doi = {10.1109/TASLP.2018.2800283},
abstract = {This paper studies the statistical performance of the multichannel Wiener filter MWF when the weights are computed using estimates of the sample covariance matrices of the noisy and the noise signals. It is well known that the optimal weights of the minimum variance distortionless response beamformer are only determined by the noisy sample covariance matrix or the noise sample covariance matrix, while those of the MWF are determined by both of them. Therefore, the difficulty increases dramatically in statistically analyzing the MWF when compared to analyzing the MVDR, where the main reason is that expressing the general joint probability density function p.d.f. of the two sample covariance matrices presented a Hitherto unsolved problem, to the best of our knowledge. For a deeper insight into the statistical performance of the MWF, this paper first introduces a bivariate normal distribution to approximately model the joint p.d.f. of the noisy and the noise sample covariance matrices. Each sample covariance matrix is approximately modeled by a random scalar multiplied by its true covariance matrix. This approximation is designed to preserve both the bias and the mean squared error of the matrix with respect to a natural distance on covariance matrices. The correlation of the bivariate normal distribution, referred to as the sample covariance matrices intrinsic correlation coefficient, captures all second-order dependencies of the noisy and the noise sample covariance matrices. By using the proposed bivariate normal distribution, the performance of the MWF can be predicted from the derived analytical expressions and many interesting results are revealed. As an example, the theoretical analysis demonstrates that the MWF performance may degrade in terms of noise reduction and signal-to-noise-ratio improvement when using more sensors in some noise scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {951–966},
numpages = {16}
}

@article{10.1109/TASLP.2018.2806305,
author = {Cumani, Sandro and Laface, Pietro},
title = {Scoring Heterogeneous Speaker Vectors Using Nonlinear Transformations and Tied PLDA Models},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2806305},
doi = {10.1109/TASLP.2018.2806305},
abstract = {Most current state-of-the-art text-independent speaker recognition systems are based on i-vectors, and on probabilistic linear discriminant analysis PLDA. PLDA assumes that the i-vectors of a trial are homogeneous, i.e., that they have been extracted by the same system. In other words, the enrollment and test i-vectors belong to the same class. However, it is sometimes important to score trials including “heterogeneous” i-vectors, for instance, enrollment i-vectors extracted by an old system, and test i-vectors extracted by a newer, more accurate, system. In this paper, we introduce a PLDA model that is able to score heterogeneous i-vectors independent of their extraction approach, dimensions, and any other characteristics that make a set of i-vectors of the same speaker belong to different classes. The new model, which will be referred to as nonlinear tied-PLDA NL-Tied-PLDA, is obtained by a generalization of our recently proposed nonlinear PLDA approach, which jointly estimates the PLDA parameters and the parameters of a nonlinear transformation of the i-vectors. The generalization consists of estimating a class-dependent nonlinear transformation of the i-vectors, with the constraint that the transformed i-vectors of the same speaker share the same speaker factor. The resulting model is flexible and accurate, as assessed by the results of a set of experiments performed on the extended core NIST SRE 2012 evaluation. In particular, NL-Tied-PLDA provides better results on heterogeneous trials with respect to the corresponding homogeneous trials scored by the old system, and, in some configurations, it also reaches the accuracy of the new system. Similar results were obtained on the female-extended core NIST SRE 2010 telephone condition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {995–1009},
numpages = {15}
}

@article{10.1109/TASLP.2018.2798821,
author = {Delcroix, Marc and Kinoshita, Keisuke and Ogawa, Atsunori and Huemmer, Christian and Nakatani, Tomohiro},
title = {Context Adaptive Neural Network Based Acoustic Models for Rapid Adaptation},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2798821},
doi = {10.1109/TASLP.2018.2798821},
abstract = {The adaptation of automatic speech recognition systems to a speaker or an environment is important if we are to achieve high speech recognition performance ubiquitously. Recently, deep neural network DNN based acoustic models have been made adaptive to speakers or environments by the addition of an auxiliary feature representing the acoustic context information such as speaker or noise characteristics to the network input. The addition of such auxiliary features to the input realizes only the adaptation of the bias term of the input layer. In this paper, we introduce “context adaptive neural networks,” which are an alternative approach for exploiting auxiliary features that can achieve adaptation of all the parameters of a layer including the linear transformation matrices and the bias terms. A context adaptive neural network is a neural network with one of its layers factorized into sublayers, each associated with an acoustic context class representing a class of speakers or noise conditions. The output of the factorized layer is obtained as a weighted sum of the contributions of all of the sublayers. The weighting coefficients, or context class weights, are derived from the auxiliary features, by transforming them through an auxiliary network. The auxiliary network and the main network can be trained jointly, which enables the context classes that optimize the training criterion to be learned automatically. We perform experiments on three tasks, i.e., two speaker adaptation experiments using DNN models with medium-sized Wall Street Journal and large Continuous Spontaneous Japanese training datasets, and one environmental adaptation of a convolutional neural network based acoustic model with CHiME3 data. These experiments confirm the potential of the proposed approach in various settings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {895–908},
numpages = {14}
}

@article{10.1109/TASLP.2018.2798822,
author = {Tran, Linh Thi Thuc and Nordholm, Sven Erik and Schepker, Henning and Dam, Hai Huyen and Doclo, Simon},
title = {Two-Microphone Hearing Aids Using Prediction Error Method for Adaptive Feedback Control},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2798822},
doi = {10.1109/TASLP.2018.2798822},
abstract = {A challenge in hearing aids is adaptive feedback control which often uses an adaptive filter to estimate the feedback path. This estimate of the feedback path usually results in a bias due to the correlation between the loudspeaker signal and the incoming signal. The prediction error method PEM is a popular method for reducing this bias for adaptive feedback control AFC in hearing aids, providing a significant performance improvement compared to conventional adaptive feedback control techniques. However, the PEM-based AFC PEM-AFC applications are still limited to single-microphone single-loudspeaker SMSL systems. This paper investigates the application of the PEM-AFC to a two-microphone single-loudspeaker hearing aid with detailed theoretical analysis as well as practical experiments. In the proposed method, PEM-AFC2, we use the two-microphone adaptive feedback control AFC2 method with two microphones and one loudspeaker. The incoming signals at the two microphones are related by a relative transfer function RTF which is used to predict the incoming signal at the main microphone. In addition, a prefilter is employed to prewhiten the loudspeaker and the microphone signals before the adaptive filter estimates. As a result, the proposed method obtains a lower bias and a faster tracking rate compared to the PEM-AFC and the AFC2 method, while still maintaining a good quality of the incoming signal. A new derivation for optimal filters in the AFC2 method will also be provided. The performance of the proposed method is evaluated for speech shaped noise as incoming signal and with undermodeling the RTF as well as with perfect modeling the RTF. Moreover, different types of incoming signals and a sudden change of feedback paths are also considered. The experimental results show that the proposed approach yields a significant performance improvement compared to existing state-of-the-art AFC methods such as the PEM-AFC and the AFC2.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {909–923},
numpages = {15}
}

@article{10.1109/TASLP.2018.2803263,
author = {Wang, Lin and Cavallaro, Andrea},
title = {Pseudo-Determined Blind Source Separation for Ad-Hoc Microphone Networks},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2803263},
doi = {10.1109/TASLP.2018.2803263},
abstract = {We propose a pseudo-determined blind source separation framework that exploits the information from a large number of microphones in an ad-hoc network to extract and enhance sound sources in a reverberant scenario. After compensating for the time offsets and sampling rate mismatch between asynchronous signals, we interpret as a determined $Mtimes M$ mixture the over-determined $Mtimes N$ mixture, where $M&gt;N$ is the number of microphones and $N$ is the number of sources. Next, we propose a pseudodetermined mixture model that can apply an $Mtimes M$ independent component analysis ICA directly to the $M$ -channel recordings. Moreover, we propose a reference-based permutation alignment scheme that aligns the permutation of the ICA outputs and classifies them into target channels, which contain the $N$ sources, and nontarget channels, which contain reverberation residuals. Finally, using the signals from nontarget channels, we estimate in each target channel the power spectral density of the noise component that we suppress with a spectral postfilter. Interestingly, we also obtain late-reverberation suppression as by-product. Experiments show that each processing block improves incrementally source separation and that the performance of the proposed pseudodetermined separation improves as the number of microphones increases.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {981–994},
numpages = {14}
}

@article{10.1109/TASLP.2018.2800280,
author = {Vaz, Colin and Ramanarayanan, Vikram and Narayanan, Shrikanth},
title = {Acoustic Denoising Using Dictionary Learning With Spectral and Temporal Regularization},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2800280},
doi = {10.1109/TASLP.2018.2800280},
abstract = {We present a method for speech enhancement of data collected in extremely noisy environments, such as those obtained during magnetic resonance imaging scans. We propose an algorithm based on dictionary learning to perform this enhancement. We use complex nonnegative matrix factorization with intrasource additivity CMF-WISA to learn dictionaries of the noise and speech+noise portions of the data and use these to factor the noisy spectrum into estimated speech and noise components. We augment the CMF-WISA cost function with spectral and temporal regularization terms to improve the noise modeling. Based on both objective and subjective assessments, we find that our algorithm significantly outperforms traditional techniques such as least mean squares filtering, while not requiring prior knowledge or specific assumptions such as periodicity of the noise waveforms that current state-of-the-art algorithms require.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {967–980},
numpages = {14}
}

@article{10.1109/TASLP.2017.2784298,
author = {Baba, Youssef El and Walther, Andreas and Habets, Emanuel A. P.},
title = {3D Room Geometry Inference Based on Room Impulse Response Stacks},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2784298},
doi = {10.1109/TASLP.2017.2784298},
abstract = {Room geometry inference is concerned with the localization of reflective boundaries in an enclosed space. This paper outlines a method for inferring room geometry based on the positions of loudspeakers and real or image microphones, which are computed using sets of times of arrival TOAs obtained from room impulse responses RIRs. These RIRs describe the acoustic propagation between the loudspeakers in an array and a single microphone. First, peaks corresponding to TOAs in these RIRs are detected and labeled using an automated method. Second, the labeled TOA sets are used to estimate the real and image microphone positions, with knowledge of the loudspeaker array geometry. Third, using all these positions, the positions of reflection points on the available reflectors in the room are determined. The reflection points determine the reflectors’ locations and orientations. This approach is largely automated and usable in real-world scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {857–872},
numpages = {16}
}

@article{10.1109/TASLP.2018.2808042,
author = {Bernardi, Giuliano and van Waterschoot, Toon and Wouters, Jan and Moonen, Marc},
title = {Subjective and Objective Sound-Quality Evaluation of Adaptive Feedback Cancellation Algorithms},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2808042},
doi = {10.1109/TASLP.2018.2808042},
abstract = {Objective measures are widely used for the perceptual sound-quality evaluation of audio signal processing algorithms. Nevertheless, the use of subjective-evaluation measures remains relevant, in particular when application-specific objective measures are lacking. In this paper, we present a perceptual sound-quality evaluation of different algorithms for adaptive feedback cancellation AFC, with both speech and music signals. Three algorithms are compared: the block normalized least mean squares algorithm, the prediction-error method PEM based frequency-domain adaptive filter, and the PEM-based frequency-domain Kalman filter PEM-FDKF. The subjective evaluation results for the tested algorithms suggest that there is a large difference in statistical significance, and a corresponding large effect size, between the PEM-FDKF and the other algorithms, when using speech signals. A smaller statistical significance, and a lower effect size, is reported when using music signals. The subjective evaluation results are then compared with the results obtained with several objective measures. The correlation between subjective and objective scores shows that objective measures can be effectively used to predict the sound-quality degradation caused by acoustic feedback and AFC artifacts.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1010–1024},
numpages = {15}
}

@article{10.1109/TASLP.2018.2797420,
author = {Zhang, Qian and Hansen, John H. L.},
title = {Language/Dialect Recognition Based on Unsupervised Deep Learning},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2797420},
doi = {10.1109/TASLP.2018.2797420},
abstract = {Over the past decade, bottleneck features within an i-Vector framework have been used for state-of-the-art language/dialect identification LID/DID. However, traditional bottleneck feature extraction requires additional transcribed speech information. Alternatively, two types of unsupervised deep learning methods are introduced in this study. To address this limitation, an unsupervised bottleneck feature extraction approach is proposed, which is derived from the traditional bottleneck structure but trained with estimated phonetic labels. In addition, based on a generative modeling autoencoder, two types of latent variable learning algorithms are introduced for speech feature processing, which have been previous considered for image processing/reconstruction. Specifically, a variational autoencoder and adversarial autoencoder are utilized on alternative phase of speech processing. To demonstrate the effectiveness of the proposed methods, three corpora are evaluated: 1 a four Chinese dialect dataset, 2 a five Arabic dialect corpus, and 3 multigenre broadcast challenge corpus MGB-3 for arabic DID. The proposed features are shown to outperform traditional acoustic feature MFCCs consistently across three corpora. Taken collectively, the proposed features achieve up to a relative +58% improvement in $C_{text{avg}}$ for LID/DID without the need of any secondary speech corpora.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {873–882},
numpages = {10}
}

@article{10.1109/TASLP.2018.2800525,
author = {Dionelis, Nikolaos and Brookes, Mike},
title = {Phase-Aware Single-Channel Speech Enhancement With Modulation-Domain Kalman Filtering},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Press},
volume = {26},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2800525},
doi = {10.1109/TASLP.2018.2800525},
abstract = {We present a speech enhancement algorithm that performs modulation-domain Kalman filtering to track the speech phase using circular statistics, along with the spectral log-amplitudes of speech and noise. In the proposed algorithm, the speech phase posterior is used to create an enhanced speech phase spectrum for the signal reconstruction of speech. The Kalman filter prediction step separately models the temporal inter-frame correlation of the speech and noise spectral log-amplitudes and of the speech phase, while the Kalman filter update step models their nonlinear relations under the assumption that speech and noise add in the complex short-time Fourier transform domain. The phase-sensitive enhancement algorithm is evaluated with speech quality and intelligibility metrics, using a variety of noise types over a range of SNRs. Instrumental measures predict that tracking the speech log-spectrum and phase with modulation-domain Kalman filtering leads to consistent improvements in speech quality, over both conventional enhancement algorithms and other algorithms that perform modulation-domain Kalman filtering.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {937–950},
numpages = {14}
}

@article{10.1109/TASLP.2017.2789320,
author = {Itakura, Kousuke and Bando, Yoshiaki and Nakamura, Eita and Itoyama, Katsutoshi and Yoshii, Kazuyoshi and Kawahara, Tatsuya},
title = {Bayesian Multichannel Audio Source Separation Based on Integrated Source and Spatial Models},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2789320},
doi = {10.1109/TASLP.2017.2789320},
abstract = {This paper presents new statistical methods of multichannel audio source separation based on unified source and spatial models that, respectively, represent the generative process of latent source spectrograms and that of observed mixture spectrograms. One possibility of the source model is a factor model based on nonnegative matrix factorization that represents each time-frequency TF bin as the weighted sum of basis spectra. Another possibility is a mixture model inspired by latent Dirichlet allocation that exclusively classifies each TF bin into one of basis spectra. Similarly, the spatial model can either be a factor model that represents each TF bin as the weighted sum of source spectra or a mixture model that classifies each bin into one of those spectra. To unify these models in a principled manner and incorporate prior knowledge of a microphone array, we propose hierarchical Bayesian models of all the source–spatial combinations factor–factor, mixture–factor, factor–mixture, and mixture–mixture models and derive efficient Gibbs sampling algorithms for posterior inference. Experimental results showed that the proposed unified models outperformed the state-of-the-art method using only the spatial mixture model. Among the four unified models, the spatial factor model tended to work better than the spatial mixture model in exchange for larger computational cost, and the choice of source models had a little impact on the performance and computational cost.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {831–846},
numpages = {16}
}

@article{10.1109/TASLP.2018.2795749,
author = {Luo, Yi and Chen, Zhuo and Mesgarani, Nima},
title = {Speaker-Independent Speech Separation With Deep Attractor Network},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2795749},
doi = {10.1109/TASLP.2018.2795749},
abstract = {Despite the recent success of deep learning for many speech processing tasks, single-microphone, speaker-independent speech separation remains challenging for two main reasons. The first reason is the arbitrary order of the target and masker speakers in the mixture permutation problem, and the second is the unknown number of speakers in the mixture output dimension problem. We propose a novel deep learning framework for speech separation that addresses both of these issues. We use a neural network to project the time-frequency representation of the mixture signal into a high-dimensional embedding space. A reference point attractor is created in the embedding space to represent each speaker which is defined as the centroid of the speaker in the embedding space. The time-frequency embeddings of each speaker are then forced to cluster around the corresponding attractor point which is used to determine the time-frequency assignment of the speaker. We propose three methods for finding the attractors for each source in the embedding space and compare their advantages and limitations. The objective function for the network is standard signal reconstruction error which enables end-to-end operation during both training and test phases. We evaluated our system using the Wall Street Journal dataset WSJ0 on two and three speaker mixtures and report comparable or better performance than other state-of-the-art deep learning methods for speech separation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {787–796},
numpages = {10}
}

@article{10.1109/TASLP.2018.2795756,
author = {Zhang, Jihui and Abhayapala, Thushara D. and Zhang, Wen and Samarasinghe, Prasanga N. and Jiang, Shouda},
title = {Active Noise Control Over Space: A Wave Domain Approach},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2795756},
doi = {10.1109/TASLP.2018.2795756},
abstract = {Noise control and cancellation over a spatial region is a fundamental problem in acoustic signal processing. In this paper, we utilize wave-domain adaptive algorithms to iteratively calculate the secondary source driving signals and to cancel the primary noise field over the control region. We propose wave-domain active noise control algorithms based on two minimization problems: first, minimizing the wave-domain residual signal coefficients, and second, minimizing the acoustic potential energy over the region, and derive the update equations with respect to two variables, the loudspeaker weights and wave-domain secondary source coefficients. Simulation results demonstrate the effectiveness of the proposed algorithms, more specifically the convergence speed and the noise cancellation performance in terms of the noise reduction level and acoustic potential energy reduction level over the entire spatial region.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {774–786},
numpages = {13}
}

@article{10.1109/TASLP.2018.2793670,
author = {Xu, Longting and Lee, Kong Aik and Li, Haizhou and Yang, Zhen},
title = {Generalizing I-Vector Estimation for Rapid Speaker Recognition},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2793670},
doi = {10.1109/TASLP.2018.2793670},
abstract = {An i-vector is a compact representation that captures both the speaker and session variabilities rendered in a spoken utterance. Over the past years, it has prevailed over other techniques and is now the de facto representation for text-independent speaker recognition. Standard i-vector extraction requires intense computation at run-time. Reducing the computation will allow effective use of i-vector in more applications. Such intense computation arises from the posterior covariance matrix, when estimating the i-vector. There have been studies on how to simplify the computation of posterior covariance matrix with modest success. In this paper, we propose a novel approach to i-vector extraction without the need to evaluate the full posterior covariance thereby speeding up the run-time extraction process. This is achieved by generalizing the i-vector estimation in two ways. First, we introduce the use of occupancy reweighting in conjunction with whitening over the Baum–Welch statistics as part of the preprocessing step. Second, we introduce the so-called subspace-orthogonalizing prior SOP to replace the standard Gaussian prior in i-vector formulation. Experiments conducted on the extended-core task of NIST SRE’10 show that the proposed rapid SOP approach achieves considerable speed-up over the standard i-vector with comparable equal error rates.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {749–759},
numpages = {11}
}

@article{10.1109/TASLP.2018.2791804,
author = {Hu, Ya-Jun and Ling, Zhen-Hua},
title = {Extracting Spectral Features Using Deep Autoencoders With Binary Distributed Hidden Units for Statistical Parametric Speech Synthesis},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2791804},
doi = {10.1109/TASLP.2018.2791804},
abstract = {This paper presents a spectral feature extraction method using deep autoencoders DAEs with binary distributed hidden units BDAE for statistical parametric speech synthesis SPSS. Conventional DAEs are trained to minimize the error of reconstructing raw features. In this paper, we investigate another important property of DAEs that may influence their performances as feature extractors for regression tasks, i.e., the degree of binarization of hidden units. Our analysis shows that making the hidden units of DAEs to be binary may help alleviate the over-smoothing effect caused by acoustic modeling and parameter generation, which are one of the main deficiencies of current SPSS systems. This paper further proposes an effective BDAE training method by adding noise to the input of hidden units during model training and applying DBN-based pretraining strategies. Our experiments adopt feedforward deep neural networks as acoustic models for SPSS and compare the performances of different spectral feature extractors. Experimental results show that when extracting low-dimensional spectral features by BDAEs, the predicted spectral features can reconstruct spectral envelopes closer to natural samples than using conventional DAEs. Subjective evaluations on the synthetic voices of a Chinese speaker and an English speaker demonstrate that BDAEs achieve better naturalness of synthetic speech than conventional mel-cepstra and other neural network based feature extractors, such as DAEs and DBNs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {713–724},
numpages = {12}
}

@article{10.1109/TASLP.2018.2791105,
author = {Tan, Zhili and Mak, Man-Wai and Mak, Brian Kan-Wing},
title = {DNN-Based Score Calibration With Multitask Learning for Noise Robust Speaker Verification},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2791105},
doi = {10.1109/TASLP.2018.2791105},
abstract = {This paper proposes and investigates several deep neural network DNN based score compensation, transformation, and calibration algorithms for enhancing the noise robustness of i-vector speaker verification systems. Unlike conventional calibration methods where the required score shift is a linear function of SNR or log-duration, the DNN approach learns the complex relationship between the score shifts and the combination of i-vector pairs and uncalibrated scores. Furthermore, with the flexibility of DNNs, it is possible to explicitly train a DNN to recover the clean scores without having to estimate the score shifts. To alleviate the overfitting problem, multitask learning is applied to incorporate auxiliary information such as SNRs and speaker ID of training utterances into the DNN. Experiments on NIST 2012 SRE show that score calibration derived from multitask DNNs can improve the performance of the conventional score-shift approch significantly, especially under noisy conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {700–712},
numpages = {13}
}

@article{10.1109/TASLP.2018.2791806,
author = {Cumani, Sandro and Laface, Pietro},
title = {Speaker Recognition Using E–Vectors},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2791806},
doi = {10.1109/TASLP.2018.2791806},
abstract = {Systems based on i–vectors represent the current state–of–the–art in text-independent speaker recognition. Unlike joint factor analysis JFA, which models both speaker and intersession subspaces separately, in the i–vector approach all the important variability is modeled in a single low-dimensional subspace. This paper is based on the observation that JFA estimates a more informative speaker subspace than the “total variability” i–vector subspace, because the latter is obtained by considering each training segment as belonging to a different speaker. We propose a speaker modeling approach that extracts a compact representation of a speech segment, similar to the speaker factors of JFA and to i–vectors, referred to as “e–vector.” Estimating the e–vector subspace follows a procedure similar to i–vector training, but produces a more accurate speaker subspace, as confirmed by the results of a set of tests performed on the NIST 2012 and 2010 Speaker Recognition Evaluations. Simply replacing the i–vectors with e–vectors we get approximately 10% average improvement of the C $_{text{primary}}$ cost function, using different systems and classifiers. It is worth noting that these performance gains come without any additional memory or computational costs with respect to the standard i–vector systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {736–748},
numpages = {13}
}

@article{10.1109/TASLP.2018.2790707,
author = {Laufer-Goldshtein, Bracha and Talmon, Ronen and Gannot, Sharon},
title = {A Hybrid Approach for Speaker Tracking Based on TDOA and Data-Driven Models},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2790707},
doi = {10.1109/TASLP.2018.2790707},
abstract = {The problem of speaker tracking in noisy and reverberant enclosures is addressed in this paper. We present a hybrid algorithm, combining traditional tracking schemes with a new learning-based approach. A state-space representation, consisting of a propagation and observation models, is learned from signals measured by several distributed microphone pairs. The proposed representation is based on two data modalities corresponding to high-dimensional acoustic features representing the full reverberant acoustic channels as well as low-dimensional time difference of arrival TDOA estimates. The state-space representation is accompanied by a statistical model based on a Gaussian process used to relate the variations of the acoustic channels to the physical variations of the associated source positions, thereby forming a data-driven propagation model for the source movement. In the observation model, the source positions are nonlinearly mapped to the associated TDOA readings. The obtained propagation and observation models establish the basis for employing an extended Kalman filter. The simulation results demonstrate the robustness of the proposed method in noisy and reverberant conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {725–735},
numpages = {11}
}

@article{10.1109/TASLP.2018.2796844,
author = {Buchris, Yaakov and Cohen, Israel and Benesty, Jacob},
title = {Frequency-Domain Design of Asymmetric Circular Differential Microphone Arrays},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2796844},
doi = {10.1109/TASLP.2018.2796844},
abstract = {Circular differential microphone arrays CDMAs facilitate compact superdirective beamformers whose beampatterns are nearly frequency invariant. In contrast to linear differential microphone arrays where the optimal steering direction is at the endfire, CDMAs provide perfect steering for all azimuthal directions. Herein, we extend the traditional symmetric model of DMAs and establish an analytical asymmetric model for $N$th-order CDMAs. This model exploits the circular geometry to eliminate the inherent limitation of symmetric beampatterns associated with a linear geometry and allows also asymmetric beampatterns. This new model is then used to develop asymmetric versions of two optimal commonly used beampatterns namely the hypercardioid and the supercardioid. Experimental results demonstrate the advantages of the asymmetric model compared to the traditional symmetric one, when additional directional constraints are imposed. The proposed model yields superior performance in terms of white noise gain, directivity factor, and front-to-back ratio, as well as more flexible design of nulls for the interfering signals.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {760–773},
numpages = {14}
}

@article{10.1109/TASLP.2018.2795754,
author = {Joy, Neethu Mariam and Kothinti, Sandeep Reddy and Umesh, Srinivasan},
title = {FMLLR Speaker Normalization With I-Vector: In Pseudo-FMLLR and Distillation Framework},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2795754},
doi = {10.1109/TASLP.2018.2795754},
abstract = {When an automatic speech recognition ASR system is deployed for real-world applications, it often receives only one utterance at a time for decoding. This single utterance could be of short duration depending on the ASR task. In these cases, robust estimation of speaker normalizing methods like feature-space maximum likelihood linear regression FMLLR and i-vectors may not be feasible. In this paper, we propose two unsupervised speaker normalization techniques—one at feature level and other at model level of acoustic modeling—to overcome the drawbacks of FMLLR and i-vectors in real-time scenarios. At feature level, we propose the use of deep neural networks DNN to generate pseudo-FMLLR features from time-synchronous pair of filterbank and FMLLR features. These pseudo-FMLLR features can then be used for DNN acoustic model training and decoding. At model level, we propose a generalized distillation framework, where a teacher DNN trained on FMLLR features guides the training and optimization of a student DNN trained on filterbank features. In both the proposed methods, the ambiguity in choosing the speaker-specific FMLLR transform can be reduced by augmenting i-vectors to the input filterbank features. Experiments conducted on 33-h and 110-h subsets of Switchboard corpus show that the proposed methods provide significant gains over DNNs trained on FMLLR, i-vector appended FMLLR, filterbank and i -vector appended filterbank features, in real-time scenario.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {797–805},
numpages = {9}
}

@article{10.1109/TASLP.2018.2796843,
author = {Tan, Zhili and Mak, Man-Wai and Mak, Brian Kan-Wing and Zhu, Yingke},
title = {Denoised Senone I-Vectors for Robust Speaker Verification},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2796843},
doi = {10.1109/TASLP.2018.2796843},
abstract = {Recently, it has been shown that senone i-vectors, whose posteriors are produced by senone deep neural networks DNNs, outperform the conventional Gaussian mixture model GMM i-vectors in both speaker and language recognition tasks. The success of senone i-vectors relies on the capability of the DNN to incorporate phonetic information into the i-vector extraction process. In this paper, we argue that to apply senone i-vectors in noisy environments, it is important to robustify the phonetically discriminative acoustic features and senone posteriors estimated by the DNN. To this end, we propose a deep architecture formed by stacking a deep belief network on top of a denoising autoencoder DAE. After backpropagation fine-tuning, the network, referred to as denoising autoencoder–deep neural network DAE–DNN, facilitates the extraction of robust phonetically-discriminitive bottleneck BN features and senone posteriors for i-vector extraction. We refer to the resulting i-vectors as denoised BN-based senone i-vectors. Results on NIST 2012 SRE show that senone i-vectors outperform the conventional GMM i-vectors. More interestingly, the BN features are not only phonetically discriminative, results suggest that they also contain sufficient speaker information to produce BN-based senone i-vectors that outperform the conventional senone i-vectors. This work also shows that DAE training is more beneficial to BN feature extraction than senone posterior estimation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {820–830},
numpages = {11}
}

@article{10.1109/TASLP.2018.2797425,
author = {Chandna, Swati and Wang, Wenwu},
title = {Bootstrap Averaging for Model-Based Source Separation in Reverberant Conditions},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2797425},
doi = {10.1109/TASLP.2018.2797425},
abstract = {Recently proposed model-based methods use time-frequency T-F masking for source separation, where the T-F masks are derived from various cues described by a frequency domain Gaussian mixture model GMM. These methods work well for separating mixtures recorded in low-to-medium level of reverberation, however, their performance degrades as the level of reverberation is increased. We note that the relatively poor performance of these methods under reverberant conditions can be attributed to the high variance of the frequency-dependent GMM parameter estimates. To address this limitation, a novel bootstrap-based approach is proposed to improve the accuracy of expectation maximization estimates of a frequency-dependent GMM based on an a priori chosen initialization scheme. It is shown how the proposed technique allows us to construct time-frequency masks which lead to improved model-based source separation for reverberant speech mixtures. Experiments and analysis are performed on speech mixtures formed using real room-recorded impulse responses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {806–819},
numpages = {14}
}

@article{10.1109/TASLP.2017.2779787,
author = {Wang, Syu-Siang and Lin, Payton and Tsao, Yu and Hung, Jeih-Weih and Su, Borching},
title = {Suppression by Selecting Wavelets for Feature Compression in Distributed Speech Recognition},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2779787},
doi = {10.1109/TASLP.2017.2779787},
abstract = {Distributed speech recognition DSR splits the processing of data between a mobile device and a network server. In the front-end, features are extracted and compressed to transmit over a wireless channel to a back-end server, where the incoming stream is received and reconstructed for recognition tasks. In this paper, we propose a feature compression algorithm termed suppression by selecting wavelets SSW to achieve the two main goals of DSR: Minimizing memory and device requirements while also maintaining or even improving the recognition performance. The SSW approach first applies the discrete wavelet transform DWT to filter the incoming speech feature sequence into two temporal subsequences at the client terminal. Feature compression is achieved by keeping the low modulation frequency subsequence while discarding the high frequency counterpart. The low-frequency subsequence is then transmitted across the remote network for specific feature statistics normalization. Wavelets are favorable for resolving the temporal properties of the feature sequence, and the down-sampling process in DWT achieves data compression by reducing the amount of data at the terminal prior to transmission across the network. Once the compressed features have arrived at the server, the feature sequence can be enhanced by statistics normalization, reconstructed with inverse DWT, and compensated with a simple post filter to alleviate any over-smoothing effects from the compression stage. Results on a standard robustness task Aurora-4 and on a Mandarin Chinese news corpus showed SSW outperforms conventional noise-robustness techniques while also providing nearly a 50% compression rate during the transmission stage of DSR systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {564–579},
numpages = {16}
}

@article{10.1109/TASLP.2017.2778562,
author = {Salvador, Cesar D. and Sakamoto, Shuichi and Trevino, Jorge and Suzuki, Yoiti},
title = {Boundary Matching Filters for Spherical Microphone and Loudspeaker Arrays},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2778562},
doi = {10.1109/TASLP.2017.2778562},
abstract = {Conversion of microphone array signals into loudspeaker array signals is an essential process in high-definition spatial audio. This paper presents the theory of boundary matching filters BMFs for spherical array signal conversion. BMFs adapt the physical boundary conditions used during recording to the ones required for reproduction by relying on a theoretical framework provided by the Kirchhoff–Helmholtz integral equation KHIE. Computationally, array signal conversion is performed in a transform domain where sound fields are represented in terms of spherical harmonic functions. Related research on transform-domain signal conversion filters is interpreted in the context of the KHIE. The case of a rigid recording boundary and an open reproduction boundary is addressed. The proposed rigid-to-open BMFs provide a suitable basis for designing gain-limited filters to deal with the problem of excessive gains at certain frequency bands, observed when using high-resolution arrays. Spatial, spectral, and temporal effects in sound field reconstruction when finite numbers of transducers are used in anechoic conditions are investigated analytically and exemplified numerically. Results show that the proposed gain-limited rigid-to-open BMFs outperform the existing gain-limited filters based on Tikhonov regularization because they reduce the spatial discretization effects and yield impulse responses that are more localized around their main peaks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {461–474},
numpages = {14}
}

@article{10.1109/TASLP.2017.2782360,
author = {Do, Van Hai and Chen, Nancy F. and Lim, Boon Pang and Hasegawa-Johnson, Mark A.},
title = {Multitask Learning for Phone Recognition of Underresourced Languages Using Mismatched Transcription},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2782360},
doi = {10.1109/TASLP.2017.2782360},
abstract = {It is challenging to obtain large amounts of native matched labels for speech audio in underresourced languages. This challenge is often due to a lack of literate speakers of the language, or in extreme cases, a lack of universally acknowledged orthography as well. One solution is to increase the amount of labeled data by using mismatched transcription, which employs transcribers who do not speak the underresourced language of interest called the target language in place of native speakers, to transcribe what they hear as nonsense speech in their own annotation language $ne$ target language. Previous uses of mismatched transcription converted it to a probabilistic transcription PT, but PT is limited by the errors of nonnative perception. This paper proposes, instead, a multitask learning framework in which one deep neural network DNN is trained to optimize two separate tasks: acoustic modeling of a small number of matched transcription with matched target-language graphemes; and acoustic modeling of a large number of mismatched transcription with mismatched annotation-language graphemes. We find that: first, the multitask learning framework gives significant improvement over monolingual, semisupervised learning, multilingual DNN training, and transfer learning baselines; second, a Gaussian Mixture Model-Hidden-Markov Model GMM-HMM model adapted using PT improves alignments, thereby improving training; and third, bottleneck features trained on the mismatched transcriptions lead to even better alignments, resulting in further performance gains of the multitask DNN. Our experiments are conducted on the IARPA Georgian and Vietnamese BABEL corpora as well as on our newly collected speech corpus of Singapore Hokkien, an underresourced language with no standard written form.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {501–514},
numpages = {14}
}

@article{10.1109/TASLP.2017.2782004,
author = {Emura, Satoru},
title = {Residual Echo Reduction for Multichannel Acoustic Echo Cancelers With a Complex-Valued Residual Echo Estimate},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2782004},
doi = {10.1109/TASLP.2017.2782004},
abstract = {This paper proposes a new method to reduce residual echo for multichannel acoustic echo cancelers AECs. This method estimates complex-valued residual echo and corrects its overestimated amplitude on the basis of a bias model of coherence estimation. Unlike conventional residual echo reduction methods for single-channel AECs, this method takes into account not only the magnitudes but also the phases of far-end signals and frequency responses. The inclusion of the phases gives the correct model of residual echo for multichannel AECs. The correction of the overestimated amplitude of residual echo decreases the distortion of near-end speech during double talk. The proposed method reduces the residual echo in the AEC output by 5–10&nbsp;dB immediately after environmental changes such as a far-end talker change and an echo-path change. The proposed method provides less distorted near-end speech than the square-root Wiener filter during double talk.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {485–500},
numpages = {16}
}

@article{10.1109/TASLP.2017.2786863,
author = {Wang, Yu and Brookes, Mike},
title = {Model-Based Speech Enhancement in the Modulation Domain},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2786863},
doi = {10.1109/TASLP.2017.2786863},
abstract = {This paper presents an algorithm for modulation-domain speech enhancement using a Kalman filter. The proposed estimator jointly models the estimated dynamics of the spectral amplitudes of speech and noise to obtain an MMSE estimation of the speech amplitude spectrum with the assumption that the speech and noise are additive in the complex domain. In order to include the dynamics of noise amplitudes with those of speech amplitudes, we propose a statistical “Gaussring” model that comprises a mixture of Gaussians whose centers lie in a circle on the complex plane. The performance of the proposed algorithm is evaluated using the perceptual evaluation of speech quality measure, segmental SNR measure, and short-time objective intelligibility measure. For speech quality measures, the proposed algorithm is shown to give a consistent improvement over a wide range of SNRs when compared to competitive algorithms. Speech recognition experiments also show that the Gaussring-model-based algorithm performs well for two types of noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {580–594},
numpages = {15}
}

@article{10.1109/TASLP.2017.2788183,
author = {Huemmer, Christian and Hofmann, Christian and Maas, Roland and Kellermann, Walter},
title = {Estimating Parameters of Nonlinear Systems Using the Elitist Particle Filter Based on Evolutionary Strategies},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2788183},
doi = {10.1109/TASLP.2017.2788183},
abstract = {In this paper, we present the elitist particle filter based on evolutionary strategies EPFES as an efficient approach to estimate the statistics of a latent state vector capturing the relevant information of a nonlinear system. Similar to classical particle filtering, the EPFES consists of a set of particles and respective weights which represent different realizations of the latent state vector and their likelihood of being the solution of the optimization problem. As main innovation, the EPFES includes an evolutionary elitist-particle selection scheme which combines long-term information with instantaneous sampling from an approximated continuous posterior distribution. In this paper, we propose two advancements of the previously published elitist-particle selection process. Further, the EPFES is shown to be a generalization of the widely-used Gaussian particle filter and thus evaluated with respect to the latter: First, we consider the univariate nonstationary growth model with time-variant latent state variable to evaluate the tracking capabilities of the EPFES for instantaneously calculated particle weights. This is followed by addressing the problem of single-channel nonlinear acoustic echo cancellation as a challenging benchmark task for identifying an unknown system of large search space:&nbsp;the nonlinear acoustic echo path is modeled by a cascade of a parameterized preprocessor to model the loudspeaker signal distortions and a linear FIR filter to model the sound wave propagation and the microphone. By using long-term information, we highlight the efficacy of the well-generalizing EPFES in estimating the preprocessor parameters for a simulated scenario&nbsp;and a real smartphone recording. Finally, we illustrate similarities between the EPFES and evolutionary algorithms to outline future improvements by fusing the achievements of both fields of research.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {595–608},
numpages = {14}
}

@article{10.1109/TASLP.2017.2783545,
author = {Abdelaziz, Ahmed Hussen},
title = {Comparing Fusion Models for DNN-Based Audiovisual Continuous Speech Recognition},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2783545},
doi = {10.1109/TASLP.2017.2783545},
abstract = {Audiovisual fusion is one of the most challenging tasks that continues to attract substantial research interest in the field of audiovisual automatic speech recognition AV-ASR. In the last few decades, many approaches for integrating the audio and video modalities were proposed to enhance the performance of automatic speech recognition in both clean and noisy conditions. However, very few studies can be found in the literature that compare different fusion models for AV-ASR. Even less research work compares audiovisual fusion models for large vocabulary continuous speech recognition LVCSR models using deep neural networks DNNs. This paper reviews and compares the performance of five audiovisual fusion models: the feature fusion model, the decision fusion model, the multistream hidden Markov model HMM, the coupled HMM, and the turbo decoders. A complete evaluation of these fusion models is conducted using a standard speaker-independent DNN-based LVCSR Kaldi recipe in three experimental setups: a clean-train-clean-test, a clean-train-noisy-test, and a matched-training setup. All experiments have been applied to the recently released NTCD-TIMIT audiovisual corpus. The task of NTCD-TIMIT is phone recognition in continuous speech. Using NTCD-TIMIT with its freely available visual features and 37 clean and noisy acoustic signals allows for this study to be a common benchmark, to which novel LVCSR AV-ASR models and approaches can be compared.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {475–484},
numpages = {10}
}

@article{10.1109/TASLP.2017.2789321,
author = {Salvati, Daniele and Drioli, Carlo and Foresti, Gian Luca},
title = {A Low-Complexity Robust Beamforming Using Diagonal Unloading for Acoustic Source Localization},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2789321},
doi = {10.1109/TASLP.2017.2789321},
abstract = {In acoustic array processing, beamforming is a class of algorithms commonly used to estimate the position of a radiating sound source. This paper presents a diagonal unloading DU transformation method for the conventional response power beamforming to achieve robust localization with low computational complexity. The transformation is obtained by subtracting an opportune diagonal matrix from the covariance matrix of the array output vector. Specifically, the DU beamformer aims at subtracting the signal subspace from the noisy signal space. It is, hence, a data-dependent covariance matrix conditioning method. We show how to calculate precisely the unloading parameters, and we present a comparison of the proposed DU beamforming, the robust minimum variance distortionless response MVDR filter, and the multiple signal classification MUSIC method, in terms of their respective eigenanalyses. Theoretical analysis and experiments conducted on both simulated and real acoustic data demonstrate that the DU beamformer localization performance is comparable to that of robust MVDR and MUSIC. Since its computational cost is equivalent to that of a conventional beamformer, the proposed DU beamformer method can, thus, be very attractive due to its effectiveness and computational efficiency.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {609–622},
numpages = {14}
}

@article{10.1109/TASLP.2017.2786544,
author = {Zhang, Jie and Chepuri, Sundeep Prabhakar and Hendriks, Richard Christian and Heusdens, Richard},
title = {Microphone Subset Selection for MVDR Beamformer Based Noise Reduction},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2786544},
doi = {10.1109/TASLP.2017.2786544},
abstract = {In large-scale wireless acoustic sensor networks WASNs, many of the sensors will only have a marginal contribution to a certain estimation task. Involving all sensors increases the energy budget unnecessarily and decreases the lifetime of the WASN. Using microphone subset selection, also termed as sensor selection, the most informative sensors can be chosen from a set of candidate sensors to achieve a prescribed inference performance. In this paper, we consider microphone subset selection for minimum variance distortionless response MVDR beamformer based noise reduction. The best subset of sensors is determined by minimizing the transmission cost while constraining the output noise power or signal-to-noise ratio. Assuming the statistical information on correlation matrices of the sensor measurements is available, the sensor selection problem for this model-driven scheme is first solved by utilizing convex optimization techniques. In addition, to avoid estimating the statistics related to all the candidate sensors beforehand, we also propose a data-driven approach to select the best subset using a greedy strategy. The performance of the greedy algorithm converges to that of the model-driven method, while it displays advantages in dynamic scenarios as well as on computational complexity. Compared to a sparse MVDR or radius-based beamformer, experiments show that the proposed methods can guarantee the desired performance with significantly less transmission costs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {550–563},
numpages = {14}
}

@article{10.1109/TASLP.2018.2789399,
author = {Kheder, Waad Ben and Matrouf, Driss and Ajili, Moez and Bonastre, Jean-Francois},
title = {A Unified Joint Model to Deal With Nuisance Variabilities in the I-Vector Space},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2789399},
doi = {10.1109/TASLP.2018.2789399},
abstract = {The past decade has witnessed a significant improvement in speaker recognition SR technology in terms of performance with the introduction of the i-vectors framework. Despite these advances, the performance of SR systems considerably suffers in the presence of acoustic nuisances and variabilities. In this paper, we develop a data-driven nuisance compensation technique in the i-vector space without referring to the effects of the targeted nuisances in the temporal domain. This approach is nonparametric as it does not suppose a specific relationship between a “good” version of an i-vector and its corrupted version. Instead, our algorithm models directly the joint distribution of both representations the good i-vector and its corrupted version and takes advantage of the reproducibility of acoustic corruptions to generate the corrupted i-vectors. We then build an MMSE estimator that computes an improved version of a corrupted test i-vector, given this joint distribution. Experiments are carried out on NIST SRE 2010 and speakers in the wild databases where the proposed algorithm is used to deal with additive noise and short utterances. Our technique is shown to be efficient, improving the baseline system performance in terms of equal-error rate by up to 70% when used on known test noises and up to 65% in the context of unseen noises using a generic model. It was also proven efficient in the context of duration mismatch reaching up to 40% of relative improvement when used on short utterances using multiple models corresponding to different durations and up to 36% when used on arbitrary duration test segments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {633–645},
numpages = {13}
}

@article{10.1109/TASLP.2017.2782487,
author = {Xiang, Yong and Natgunanathan, Iynkaran and Peng, Dezhong and Hua, Guang and Liu, Bo},
title = {Spread Spectrum Audio Watermarking Using Multiple Orthogonal PN Sequences and Variable Embedding Strengths and Polarities},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2782487},
doi = {10.1109/TASLP.2017.2782487},
abstract = {Copyright protection of audio data is a serious problem and spread spectrum SS based audio watermarking is a promising technology to tackle this problem. Although a number of SS-based audio watermarking methods have been reported in the literature, they cannot achieve high robustness and embedding capacity at the same time. In this paper, we propose a novel SS-based audio watermarking method that can embed a large number of watermark bits into an audio signal without compromising the robustness against common attacks. Compared with the existing audio watermarking methods, the proposed one is especially robust against severe noise addition and compression attacks, while achieving high embedding capacity. Moreover, the new audio watermarking method is computationally efficient. The validity of the proposed SS-based audio watermarking method is demonstrated by simulation results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {529–539},
numpages = {11}
}

@article{10.1109/TASLP.2017.2780993,
author = {Taseska, Maja and Habets, Emanuel A. P.},
title = {Blind Source Separation of Moving Sources Using Sparsity-Based Source Detection and Tracking},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2780993},
doi = {10.1109/TASLP.2017.2780993},
abstract = {Sparsity-based blind source separation BSS algorithms in the short time–frequency TF domain have received a lot of attention due to their versatility and noise reduction capabilities. In most of these algorithms, the estimation of the BSS filters relies on the accurate association of each time–frequency bin to the dominant source at that bin. The TF bin associations are then used to estimate the statistics of the source signals, and BSS is achieved by optimal spatial filters computed using the estimated statistics. The main objective of this paper is to apply such a framework to scenarios with an unknown number of moving sources. While state-of-the-art approaches employ online clustering algorithms to solve the problem for moving sources, we propose an approximate Bayesian tracker and perform the association of each TF bin to the dominant source using the tracker's measurement-to-source association probabilities. Therefore, the choice of the underlying narrowband models and measurements for the tracker as well as the resulting tracking algorithm constitute the main contributions of this paper. The TF bin associations obtained from the tracker are then used to estimate the statistics of the source signals. The performance of the resulting BSS filters is compared to the performance of state-of-the-art sparsity-based and independent vector analysis-based BSS algorithms. Our proposed approach targets scenarios with at least two spatially separated microphone arrays, with known microphone positions and relative orientations. The framework also allows for efficient management of a time-varying number of sources.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {657–670},
numpages = {14}
}

@article{10.1109/TASLP.2017.2769220,
author = {Gelly, Gregory and Gauvain, Jean-Luc},
title = {Optimization of RNN-Based Speech Activity Detection},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2769220},
doi = {10.1109/TASLP.2017.2769220},
abstract = {Speech activity detection SAD is an essential component of automatic speech recognition systems impacting the overall system performance. This paper investigates an optimization process for recurrent neural network RNN based SAD. This process optimizes all system parameters including those used for feature extraction, the NN weights, and the back-end parameters. Three cost functions are considered for SAD optimization: the frame error rate, the NIST detection cost function, and the word error rate of a downstream speech recognizer. Different types of RNN models and optimization methods are investigated. Three types of RNNs are compared: a basic RNN, long short-term memory LSTM network with peepholes, and a coordinated-gate LSTM CG-LSTM network introduced by Gelly and Gauvain. Well suited for nondifferentiable optimization problems, quantum-behaved particle swarm optimization is used to optimize feature extraction and posterior smoothing, as well as for the initial training of the neural networks. Experimental SAD results are reported on the NIST 2015 SAD evaluation data as well as REPERE and AMI meeting corpora. Speech recognition results are reported on the OpenKWS’13 test data. For all tasks and conditions, the proposed optimization method significantly improves the SAD performance and among all the tested SAD methods the CG-LSTM model gives the best results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {646–656},
numpages = {11}
}

@article{10.1109/TASLP.2017.2788182,
author = {Yu, Liang-Chih and Wang, Jin and Lai, K. Robert and Zhang, Xuejie},
title = {Refining Word Embeddings Using Intensity Scores for Sentiment Analysis},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2788182},
doi = {10.1109/TASLP.2017.2788182},
abstract = {Word embeddings that provide continuous low-dimensional vector representations of words have been extensively used for various natural language processing tasks. However, existing context-based word embeddings such as Word2vec and GloVe typically fail to capture sufficient sentiment information, which may result in words with similar vector representations having an opposite sentiment polarity e.g., good and bad, thus degrading sentiment analysis performance. To tackle this problem, recent studies have suggested learning sentiment embeddings to incorporate the sentiment polarity positive and negative information from labeled corpora. This study adopts another strategy to learn sentiment embeddings. Instead of creating a new word embedding from labeled corpora, we propose a word vector refinement model to refine existing pretrained word vectors using real-valued sentiment intensity scores provided by sentiment lexicons. The idea of the refinement model is to improve each word vector such that it can be closer in the lexicon to both semantically and sentimentally similar words i.e., those with similar intensity scores and further away from sentimentally dissimilar words i.e., those with dissimilar intensity scores. An obvious advantage of the proposed method is that it can be applied to any pretrained word embeddings. In addition, the intensity scores can provide more fine-grained real-valued sentiment information than binary polarity labels to guide the refinement process. Experimental results show that the proposed refinement model can improve both conventional word embeddings and previously proposed sentiment embeddings for binary, ternary, and fine-grained sentiment classification on the SemEval and Stanford Sentiment Treebank datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {671–681},
numpages = {11}
}

@article{10.1109/TASLP.2018.2789721,
author = {Su, Jinsong and Zeng, Jiali and Xiong, Deyi and Liu, Yang and Wang, Mingxuan and Xie, Jun},
title = {A Hierarchy-to-Sequence Attentional Neural Machine Translation Model},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2789721},
doi = {10.1109/TASLP.2018.2789721},
abstract = {Although sequence-to-sequence attentional neural machine translation NMT has achieved great progress recently, it is confronted with two challenges: learning optimal model parameters for long parallel sentences and well exploiting different scopes of contexts. In this paper, partially inspired by the idea of segmenting a long sentence into short clauses, each of which can be easily translated by NMT, we propose a hierarchy-to-sequence attentional NMT model to handle these two challenges. Our encoder takes the segmented clause sequence as input and explores a hierarchical neural network structure to model words, clauses, and sentences at different levels, particularly with two layers of recurrent neural networks modeling semantic compositionality at the word and clause level. Correspondingly, the decoder sequentially translates segmented clauses and simultaneously applies two types of attention models to capture contexts of interclause and intraclause for translation prediction. In this way, we can not only improve parameter learning, but also well explore different scopes of contexts for translation. Experimental results on Chinese–English and English–German translation demonstrate the superiorities of the proposed model over the conventional NMT model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {623–632},
numpages = {10}
}

@article{10.1109/TASLP.2017.2782491,
author = {Zohourian, Mehdi and Enzner, Gerald and Martin, Rainer},
title = {Binaural Speaker Localization Integrated Into an Adaptive Beamformer for Hearing Aids},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2782491},
doi = {10.1109/TASLP.2017.2782491},
abstract = {In this paper, we present and compare novel algorithms to localize simultaneous speakers using four microphones distributed on a pair of binaural hearing aids. The framework consists of two groups of localization algorithms, namely, beamforming-based and statistical model based localization algorithms. We first generalize our previously proposed methods based on beamforming techniques to the binaural configuration with 2 $times$ 2 microphones. Next, we contribute two statistical model based methods for binaural localization using the maximum likelihood approach that also takes head-related transfer functions and unknown noise conditions into account. The methods enable the localization of multiple source positions for all azimuth angles and do not require prior training of binaural cues. The proposed localization algorithms are integrated into a generalized side-lobe canceller GSC to extract the desired speaker in the presence of competing speakers and background noise and when the head of the listener turns. The GSC components are adapted with the frequency-wise target presence probability and the frame-wise broadband direction-of-arrival DOA estimates that track the turns of the listener's head. We evaluate the performance of the localization algorithms individually and also in the context of the adaptive binaural beamformer in various noisy and reverberant conditions. Finally, we introduce a new adaptive beamformer, which combines the GSC with multichannel speech presence probability estimation and achieves superior source separation performance in noisy environment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {515–528},
numpages = {14}
}

@article{10.1109/TASLP.2017.2788198,
author = {Dorfan, Yuval and Plinge, Axel and Hazan, Gershon and Gannot, Sharon},
title = {Distributed Expectation-Maximization Algorithm for Speaker Localization in Reverberant Environments},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2788198},
doi = {10.1109/TASLP.2017.2788198},
abstract = {Localization of acoustic sources has attracted a considerable amount of research attention in recent years. A major obstacle to achieving high localization accuracy is the presence of reverberation, the influence of which obviously increases with the number of active speakers in the room. Human hearing is capable of localizing acoustic sources even in extreme conditions. In this study, we propose to combine a method based on human hearing mechanisms and a modified incremental distributed expectation-maximization IDEM algorithm. Rather than using phase difference measurements that are modeled by a mixture of complex-valued Gaussians, as proposed in the original IDEM framework, we propose to use time difference of arrival measurements in multiple subbands and model them by a mixture of real-valued truncated Gaussians. Moreover, we propose to first filter the measurements in order to reduce the effect of the multipath conditions. The proposed method is evaluated using both simulated data and real-life recordings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {682–695},
numpages = {14}
}

@article{10.1109/TASLP.2017.2785283,
author = {Tan, Chuanqi and Wei, Furu and Zhou, Qingyu and Yang, Nan and Du, Bowen and Lv, Weifeng and Zhou, Ming},
title = {Context-Aware Answer Sentence Selection With Hierarchical Gated Recurrent Neural Networks},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2785283},
doi = {10.1109/TASLP.2017.2785283},
abstract = {In this paper, we study the task of reading comprehension style answer sentence selection that aims to select the best sentence from a given passage to answer a question. Unlike most previous works that match the question and each candidate sentence separately, we observe that the context information among sentences in the same passage plays a vital role in this task. We propose modeling context information with hierarchical gated recurrent neural networks. Specifically, we first apply a word level recurrent neural network to model the context independent matching between the question and each candidate sentence. We then employ a sentence level recurrent neural network to incorporate the context information among all candidate sentences. Moreover, we introduce the gate mechanism to select matching information before feeding into recurrent neural networks at both word and sentence level. Experiments on the WikiQA and SQuAD datasets show that our model outperforms state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {540–549},
numpages = {10}
}

@article{10.1109/TASLP.2017.2774925,
author = {Nikunen, Joonas and Diment, Aleksandr and Virtanen, Tuomas},
title = {Separation of Moving Sound Sources Using Multichannel NMF and Acoustic Tracking},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2774925},
doi = {10.1109/TASLP.2017.2774925},
abstract = {In this paper, we propose a method for separation of moving sound sources. The method is based on first tracking the sources and then estimation of source spectrograms using multichannel nonnegative matrix factorization NMF and extracting the sources from the mixture by single-channel Wiener filtering. We propose a novel multichannel NMF model with time-varying mixing of the sources denoted by spatial covariance matrices SCM and provide update equations for optimizing model parameters minimizing squared Frobenius norm. The SCMs of the model are obtained based on estimated directions of arrival of tracked sources at each time frame. The evaluation is based on established objective separation criteria and using real recordings of two and three simultaneous moving sound sources. The compared methods include conventional beamforming and ideal ratio mask separation. The proposed method is shown to exceed the separation quality of other evaluated blind approaches according to all measured quantities. Additionally, we evaluate the method's susceptibility toward tracking errors by comparing the separation quality achieved using annotated ground truth source trajectories.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {281–295},
numpages = {15}
}

@article{10.1109/TASLP.2017.2774919,
author = {Wu, Chunyang and Gales, Mark J. F. and Ragni, Anton and Karanasou, Penny and Sim, Khe Chai},
title = {Improving Interpretability and Regularization in Deep Learning},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2774919},
doi = {10.1109/TASLP.2017.2774919},
abstract = {Deep learning approaches yield state-of-the-art performance in a range of tasks, including automatic speech recognition. However, the highly distributed representation in a deep neural network DNN or other network variations is difficult to analyze, making further parameter interpretation and regularization challenging. This paper presents a regularization scheme acting on the activation function output to improve the network interpretability and regularization. The proposed approach, referred to as activation regularization, encourages activation function outputs to satisfy a target pattern. By defining appropriate target patterns, different learning concepts can be imposed on the network. This method can aid network interpretability and also has the potential to reduce overfitting. The scheme is evaluated on several continuous speech recognition tasks: the Wall Street Journal continuous speech recognition task, eight conversational telephone speech tasks from the IARPA Babel program and a U.S. English broadcast news task. On all the tasks, the activation regularization achieved consistent performance gains over the standard DNN baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {256–265},
numpages = {10}
}

@article{10.1109/TASLP.2017.2775800,
author = {Sward, Johan and Li, Hongbin and Jakobsson, Andreas},
title = {Off-Grid Fundamental Frequency Estimation},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2775800},
doi = {10.1109/TASLP.2017.2775800},
abstract = {In this paper, we propose a gridless method for estimating an unknown number of fundamental frequencies. Starting with a conventional dictionary matrix, containing sets of candidate fundamental frequencies and their corresponding harmonics, a nonconvex log-sum cost function is formed such that it imposes the harmonic structure and treats every fundamental frequency in the dictionary as a parameter. The cost function is iteratively decreased by minimizing a surrogate function, and, in each iteration, the fundamental frequencies are refined, whereas redundant parameters are omitted from the dictionary. The proposed method is tested on both real and simulated data, showing its preferred performance as compared to other state-of-the-art multipitch estimators.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {296–303},
numpages = {8}
}

@article{10.1109/TASLP.2017.2779862,
author = {Chien, Jen-Tzung},
title = {Bayesian Nonparametric Learning for Hierarchical and Sparse Topics},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2779862},
doi = {10.1109/TASLP.2017.2779862},
abstract = {This paper presents the Bayesian nonparametric BNP learning for hierarchical and sparse topics from natural language. Traditionally, the Indian buffet process provides the BNP prior on a binary matrix for an infinite latent feature model consisting of a flat layer of topics. The nested model paves an avenue to construct a tree model instead of a flat-layer model. This paper presents the nested Indian buffet process nIBP to achieve the sparsity and flexibility in topic model where the model complexity and topic hierarchy are learned from the groups of words. The mixed membership modeling is conducted by representing a document using the tree nodes or dishes that a document or a customer chooses according to the nIBP scenario. A tree stick-breaking process is implemented to select topic weights from a subtree for flexible topic modeling. Such an nIBP relaxes the constraint of adopting a single tree path in the nested Chinese restaurant process nCRP and, therefore, improves the variety of topic representation for heterogeneous documents. A Gibbs sampling procedure is developed to infer the nIBP topic model. Compared to the nested hierarchical Dirichlet process nhDP, the compactness of the estimated topics in a tree using nIBP is improved. Experimental results show that the proposed nIBP reduces the error rate of nCRP and nhDP by 18% and 8% on Reuters task for document classification, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {422–435},
numpages = {14}
}

@article{10.1109/TASLP.2017.2765819,
author = {May, Tobias},
title = {Robust Speech Dereverberation With a Neural Network-Based Post-Filter That Exploits Multi-Conditional Training of Binaural Cues},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2765819},
doi = {10.1109/TASLP.2017.2765819},
abstract = {This study presents an algorithm for binaural speech dereverberation based on the supervised learning of short-term binaural cues. The proposed system combined a delay-and-sum beamformer with a neural network-based post-filter that attenuated reverberant components in individual time-frequency units. A multi-conditional training procedure was used to simulate the uncertainties of short-term binaural cues in response to room reverberation by mixing the direct part of head related impulse responses HRIRs with diffuse noise. Despite being trained with only anechoic HRIRs, the proposed dereverberation algorithm was tested in a variety of reverberant environments and achieved considerable improvements relative to a coherence-based approach in terms of three objective metrics reflecting speech quality and speech intelligibility. Moreover, a systematic evaluation showed that the proposed system generalized very well to a wide range of acoustic conditions, including various measured binaural room impulse responses reflecting different reverberation times, azimuth positions spanning the entire frontal hemifield, various source-receiver distances as well as different artificial heads.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {406–414},
numpages = {9}
}

@article{10.1109/TASLP.2017.2778151,
author = {Rehr, Robert and Gerkmann, Timo},
title = {On the Importance of Super-Gaussian Speech Priors for Machine-Learning Based Speech Enhancement},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2778151},
doi = {10.1109/TASLP.2017.2778151},
abstract = {For enhancing noisy signals, machine-learning based single-channel speech enhancement schemes exploit prior knowledge about typical speech spectral structures. To ensure a good generalization and to meet requirements in terms of computational complexity and memory consumption, certain methods restrict themselves to learning speech spectral envelopes. We refer to these approaches as machine-learning spectral envelope MLSE-based approaches. In this paper, we show by means of theoretical and experimental analyses that for MLSE-based approaches, super-Gaussian priors allow for a reduction of noise between speech spectral harmonics which is not achievable using Gaussian estimators such as the Wiener filter. For the evaluation, we use a deep neural network based phoneme classifier and a low-rank nonnegative matrix factorization framework as examples of MLSE-based approaches. A listening experiment and instrumental measures confirm that while super-Gaussian priors yield only moderate improvements for classic enhancement schemes, for MLSE-based approaches super-Gaussian priors clearly make an important difference and significantly outperform Gaussian priors.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {357–366},
numpages = {10}
}

@article{10.1109/TASLP.2017.2778150,
author = {Djaziri-Larbi, Sonia and Mahe, Gael and Mezghani, Imen and Turki, Monia and Jaidane, Meriem},
title = {Watermark-Driven Acoustic Echo Cancellation},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2778150},
doi = {10.1109/TASLP.2017.2778150},
abstract = {The performance of adaptive acoustic echo cancelers AEC is sensitive to the nonstationarity and correlation of speech signals. In this paper, we explore a new approach based on an adaptive AEC driven by data hidden in speech, to enhance the AEC robustness. We propose a two-stage AEC, where the first stage is a classical NLMS-based AEC driven by the far-end speech. In the signal, we embed—in an extended conception of data hiding—an imperceptible white and stationary signal, i.e., a watermark. The goal of the second stage AEC is to identify the misalignment of the first stage. It is driven by the watermark solely and takes advantage of its appropriate properties stationary and white to improve the robustness of the two-stage AEC to the nonstationarity and correlation of speech, and thus reduce the overall system misadjustment. We test two kinds of implementations: in the first implementation, referred to as adaptive watermark driven AEC A-WdAEC, the watermark is a white stationary Gaussian noise. Driven by this signal, the second stage converges faster than the classical AEC and provides better performance in steady state. In the second implementation, referred to as maximum length sequences WdAEC MLS-WdAEC, the watermark is built from MLS. Thus, the second stage performs a block identification of the first stage misalignment, given by the circular correlation watermark/preprocessed version of the first stage residual echo. The advantage of this implementation lies in its robustness against noise and undermodeling. Simulation results show the relevance of the “WdAEC” approach, compared to the classical “error-driven AEC.”},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {367–378},
numpages = {12}
}

@article{10.1109/TASLP.2017.2779405,
author = {Stahl, Johannes and Mowlaee, Pejman},
title = {A Pitch-Synchronous Simultaneous Detection-Estimation Framework for Speech Enhancement},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2779405},
doi = {10.1109/TASLP.2017.2779405},
abstract = {Speech enhancement methods formulated in the short-time Fourier transform STFT domain vary in the statistical assumptions made on the STFT coefficients, in the optimization criteria applied or in the models of the signal components. Recently, approaches relying on a stochastic-deterministic speech model have been proposed. The deterministic part of the signal corresponds to harmonically related sinusoids, often used to represent voiced speech. The stochastic part models signal components that are not captured by the deterministic components. In this paper, we consider this scenario under a new perspective yielding three main contributions. First, a pitch-synchronous signal representation is considered and shown to be advantageous for the estimation of the harmonic model parameters. Second, we model the harmonic amplitudes in voiced speech as random variables with frequency bin dependent Gamma distributions. Finally, distinct estimators for the different models of voiced speech, unvoiced speech, and speech absence are derived. To select from the arising estimates, we take into account the mutual impact of detection and estimation by proposing a binary decision framework that is derived from a Bayesian risk function. The resulting pitch-synchronous stochastic-deterministic estimator outperforms several benchmark methods in terms of speech intelligibility and perceived quality predicted by instrumental measures for various noise types and different signal-to-noise ratios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {436–450},
numpages = {15}
}

@article{10.1109/TASLP.2017.2772846,
author = {Chen, Kehai and Zhao, Tiejun and Yang, Muyun and Liu, Lemao and Tamura, Akihiro and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro},
title = {A Neural Approach to Source Dependence Based Context Model for Statistical Machine Translation},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2772846},
doi = {10.1109/TASLP.2017.2772846},
abstract = {In statistical machine translation, translation prediction considers not only the aligned source word itself but also its source contextual information. Learning context representation is a promising method for improving translation results, particularly through neural networks. Most of the existing methods process context words sequentially and neglect source long-distance dependencies. In this paper, we propose a novel neural approach to source dependence-based context representation for translation prediction. The proposed model is capable of not only encoding source long-distance dependencies but also capturing functional similarities to better predict translations i.e., word form translations and ambiguous word translations. To verify our method, the proposed mode is incorporated into phrase-based and hierarchical phrase-based translation models, respectively. Experiments on large-scale Chinese-to-English and English-to-German translation tasks show that the proposed approach achieves significant improvement over the baseline systems and outperforms several existing context-enhanced methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {266–280},
numpages = {15}
}

@article{10.1109/TASLP.2017.2775742,
author = {Dropuljic, Branimir and Mijic, Igor and Petrinovic, Davor and Jovanovic, Tanja and Cosic, Kresimir and Dropuljic, Branimir and Mijic, Igor and Petrinovic, Davor and Jovanovic, Tanja and Cosic, Kresimir},
title = {Vocal Analysis of Acoustic Startle Responses},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2775742},
doi = {10.1109/TASLP.2017.2775742},
abstract = {This paper presents an extensive statistical analysis of the acoustic startle response of two vocal parameters: fundamental frequency F0 and root-mean-square energy E, as well as of the orbicularis oculi eyeblink surface electromyography sEMG. An experiment was conducted in which fourteen participants were exposed to acoustic startle stimuli of varying parameters, i.e., intensity level, duration, rise time, and spectral type, during periods of sustained phonation. Voice recordings of the phonations were taken alongside several physiological signals, of which only the sEMG was analyzed in this paper. Response features peak value, peak time, latency, rise time, fall time, and duration were extracted on F0, E and sEMG data, and statistical analysis was conducted using linear mixed effects models to show the response behavior with the varying stimuli. The results for vocal F0 and E data were congruent with sEMG data and earlier work in the field. The results demonstrated that vocal analysis can be used as a feasible alternative to the sEMG eyeblink analysis of acoustic startle responses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {318–329},
numpages = {12}
}

@article{10.1109/TASLP.2017.2778423,
author = {Mesaros, Annamaria and Heittola, Toni and Benetos, Emmanouil and Foster, Peter and Lagrange, Mathieu and Virtanen, Tuomas and Plumbley, Mark D.},
title = {Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2778423},
doi = {10.1109/TASLP.2017.2778423},
abstract = {Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events DCASE 2016 has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {379–393},
numpages = {15}
}

@article{10.1109/TASLP.2017.2772340,
author = {Bando, Yoshiaki and Itoyama, Katsutoshi and Konyo, Masashi and Tadokoro, Satoshi and Nakadai, Kazuhiro and Yoshii, Kazuyoshi and Kawahara, Tatsuya and Okuno, Hiroshi G.},
title = {Speech Enhancement Based on Bayesian Low-Rank and Sparse Decomposition of Multichannel Magnitude Spectrograms},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2772340},
doi = {10.1109/TASLP.2017.2772340},
abstract = {This paper presents a blind multichannel speech enhancement method that can deal with the time-varying layout of microphones and sound sources. Since nonnegative tensor factorization NTF separates a multichannel magnitude or power spectrogram into source spectrograms without phase information, it is robust against the time-varying mixing system. This method, however, requires prior information such as the spectral bases templates of each source spectrogram in advance. To solve this problem, we develop a Bayesian model called robust NTF Bayesian RNTF that decomposes a multichannel magnitude spectrogram into target speech and noise spectrograms based on their sparseness and low rankness. Bayesian RNTF is applied to the challenging task of speech enhancement for a microphone array distributed on a hose-shaped rescue robot. When the robot searches for victims under collapsed buildings, the layout of the microphones changes over time and some of them often fail to capture target speech. Our method robustly works under such situations, thanks to its characteristic of time-varying mixing system. Experiments using a 3-m hose-shaped rescue robot with eight microphones show that the proposed method outperforms conventional blind methods in enhancement performance by the signal-to-noise ratio of 1.03&nbsp;dB.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {215–230},
numpages = {16}
}

@article{10.1109/TASLP.2017.2773198,
author = {Ruan, Yu-Ping and Chen, Qian and Ling, Zhen-Hua},
title = {A Sequential Neural Encoder With Latent Structured Description for Modeling Sentences},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2773198},
doi = {10.1109/TASLP.2017.2773198},
abstract = {In this paper, we propose a sequential neural encoder with latent structured description SNELSD for modeling sentences. This model introduces latent chunk-level representations into conventional sequential neural encoders, i.e., recurrent neural networks with long short-term memory LSTM units, to consider the compositionality of languages in semantic modeling. An SNELSD model has a hierarchical structure that includes a detection layer and a description layer. The detection layer predicts the boundaries of latent word chunks in an input sentence and derives a chunk-level vector for each word. The description layer utilizes modified LSTM units to process these chunk-level vectors in a recurrent manner and produces sequential encoding outputs. These output vectors are further concatenated with word vectors or the outputs of a chain LSTM encoder to obtain the final sentence representation. All the model parameters are learned in an end-to-end manner without a dependency on additional text chunking or syntax parsing. A natural language inference task and a sentiment analysis task are adopted to evaluate the performance of our proposed model. The experimental results demonstrate the effectiveness of the proposed SNELSD model on exploring task-dependent chunking patterns during the semantic modeling of sentences. Furthermore, the proposed method achieves better performance than conventional chain LSTMs and tree-structured LSTMs on both tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {231–242},
numpages = {12}
}

@article{10.1109/TASLP.2017.2778948,
author = {Chung, Cheng-Tao and Lee, Lin-Shan},
title = {Unsupervised Discovery of Structured Acoustic Tokens With Applications to Spoken Term Detection},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2778948},
doi = {10.1109/TASLP.2017.2778948},
abstract = {In this paper, we compare two paradigms for unsupervised discovery of structured acoustic tokens directly from speech corpora without any human annotation. The multigranular paradigm seeks to capture all available information in the corpora with multiple sets of tokens for different model granularities. The hierarchical paradigm attempts to jointly learn several levels of signal representations in a hierarchical structure. The two paradigms are unified within a theoretical framework in this paper. Query-by-example spoken term detection QbE-STD experiments on the query by example search on speech task dataset of MediaEval 2015 verifies the competitiveness of the acoustic tokens. The enhanced relevance score proposed in this work improves both paradigms for the task of QbE-STD. We also list results on the ABX evaluation task of the Zero Resource Challenge 2015 for comparison of the paradigms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {394–405},
numpages = {12}
}

@article{10.1109/TASLP.2017.2772831,
author = {Alexandridis, Anastasios and Mouchtaris, Athanasios},
title = {Multiple Sound Source Location Estimation in Wireless Acoustic Sensor Networks Using DOA Estimates: The Data-Association Problem},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2772831},
doi = {10.1109/TASLP.2017.2772831},
abstract = {In this paper, we consider the data-association problem for the localization of multiple sound sources in a wireless acoustic sensor network, where each node is a microphone array, using direction of arrival DOA estimates. The data-association problem arises because the central node that receives the multiple DOA estimates from the nodes cannot know to which source they belong. Hence, the DOAs from the different nodes that correspond to the same source must be found in order to perform accurate localization. We present a method to identify the correct association of DOAs to the sources and thus accurately estimate their locations. Our method results in high association and localization accuracy in realistic scenarios with missed detections, reverberation, noise, and moving sources and outperforms other recently proposed methods. It also incorporates a bitrate reduction scheme in order to keep the amount of information that needs to be transmitted in the network at low levels without affecting performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {342–356},
numpages = {15}
}

@article{10.1109/TASLP.2017.2774921,
author = {Gully, Amelia J. and Daffern, Helena and Murphy, Damian T. and Gully, Amelia J. and Daffern, Helena and Murphy, Damian T.},
title = {Diphthong Synthesis Using the Dynamic 3D Digital Waveguide Mesh},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2774921},
doi = {10.1109/TASLP.2017.2774921},
abstract = {Articulatory speech synthesis has the potential to offer more natural sounding synthetic speech than established concatenative or parametric synthesis methods. Time-domain acoustic models are particularly suited to the dynamic nature of the speech signal, and recent work has demonstrated the potential of dynamic vocal tract models that accurately reproduce the vocal tract geometry. This paper presents a dynamic 3D digital waveguide mesh DWM vocal tract model, capable of movement to produce diphthongs. The technique is compared to existing dynamic 2D and static 3D DWM models, for both monophthongs and diphthongs. The results indicate that the proposed model provides improved formant accuracy over existing DWM vocal tract models. Furthermore, the computational requirements of the proposed method are significantly lower than those of comparable dynamic simulation techniques. This paper represents another step toward a fully functional articulatory vocal tract model which will lead to more natural speech synthesis systems for use across society.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {243–255},
numpages = {13}
}

@article{10.1109/TASLP.2017.2761233,
author = {Aichinger, Philipp and Hagmuller, Martin and Schneider-Stickler, Berit and Schoentgen, Jean and Pernkopf, Franz},
title = {Tracking of Multiple Fundamental Frequencies in Diplophonic Voices},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2761233},
doi = {10.1109/TASLP.2017.2761233},
abstract = {Diplophonia is a type of pathological voice in which two fundamental frequencies $f_o$ are present simultaneously. Specialized audio analyzers that can handle up to two $f_o$s in diplophonic voices are in their infancy. We propose the tracking of up to two $f_o$ s in diplophonic voices by audio waveform modeling AWM, which involves obtaining candidates by repetitive execution of the Viterbi algorithm, followed by waveform Fourier synthesis, and heuristic candidate selection with majority voting. Our approach is evaluated with reference $f_o$-tracks obtained from laryngeal high-speed videos of 29 sustained phonations and compared to state-of-the-art tracking algorithms for multiple $f_o$s. An accurate and a fast variant of our algorithm are tested. The median error rate of the accurate variant is 6.52%, whereas the most accurate benchmark achieves 11.11%. The fast variant is more than twice as fast as the fastest relevant benchmark, and the median error rate is 9.52%. Furthermore, illustrative results of connected speech analysis are reported. Our approach may help to improve detection and analysis of diplophonia in clinical research and practice, as well as to advance synthesis of disordered voices.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {330–341},
numpages = {12}
}

@article{10.1109/TASLP.2017.2775860,
author = {Menzies, Dylan and Galvez, Marcos F. Simon and Fazi, Filippo Maria},
title = {A Low-Frequency Panning Method With Compensation for Head Rotation},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2775860},
doi = {10.1109/TASLP.2017.2775860},
abstract = {Amplitude panning produces interaural time difference ITD cues that help localize images in directions between loudspeakers. However, if the panning gains are static, then the ITD cues produced in this way vary inconsistently as the listener's head rotates, compared with a real source, and so the dynamic ITD cues are inaccurate. This effect destabilizes the perception of the image and overall scene, and is worse for loudspeakers that are more widely spaced relative to the listener. Based on a simple head model that is accurate in the low-frequency ITD regime, the ITD is calculated for a general field, including those produced by panning. A simple formula is derived relating head orientation, image direction, and a field description vector. Panning functions are then found that compensate for head orientation and are valid for any image direction. For the special case when the listener is facing the image, the functions are equivalent to vector base amplitude panning. The performance is first assessed objectively using measured binaural responses, rather than the simple head model. Subjective comparison is then made with pre-existing listening tests and new listening tests in which the listener's head is tracked to control the panning gains in real-time. These show that images can be stabilized as predicted, and, furthermore, that with the same panning functions, images can be produced in all directions using two loudspeakers placed in front.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {304–317},
numpages = {14}
}

@article{10.1109/TASLP.2017.2780989,
author = {Mirbagheri, Majid and Atlas, Les and Lee, Adrian K. C.},
title = {Regression Factor Analysis With an Application to Continuous HRIR Measurement},
year = {2018},
issue_date = {February 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2780989},
doi = {10.1109/TASLP.2017.2780989},
abstract = {Linear time-varying LTV regression models play a central role in input–output analysis of many real-world dynamical systems. Most existing models for such systems consider either a switching dynamics with a discrete latent variable driving the system or a linear dynamics directly applied to the regression coefficients. These models usually fall short of capturing structural regularities or the total variability in the dynamics of the system. Addressing these issues in this paper, we propose a method to parametrize joint variations of regression coefficients in LTV systems with continuous latent variables based on factor analysis technique, i.e., regression factor analysis . By using a linear Gaussian dynamical model for the time evolution of factor weights, our model constrains the dynamics of the process. Our inference scheme takes advantage of the expectation–maximization algorithm to estimate the model parameters. We show how our proposed algorithm can be utilized in a key application for fast continuous measurement of personalized head-related impulse responses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {415–421},
numpages = {7}
}

@article{10.1109/TASLP.2017.2759338,
author = {Deng, Jun and Xu, Xinzhou and Zhang, Zixing and Fruhholz, Sascha and Schuller, Bjorn and Jun Deng and Xinzhou Xu and Zixing Zhang and Fruhholz, Sascha and Schuller, Bjorn},
title = {Semisupervised Autoencoders for Speech Emotion Recognition},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2759338},
doi = {10.1109/TASLP.2017.2759338},
abstract = {Despite the widespread use of supervised learning methods for speech emotion recognition, they are severely restricted due to the lack of sufficient amount of labelled speech data for the training. Considering the wide availability of unlabelled speech data, therefore, this paper proposes semisupervised autoencoders to improve speech emotion recognition. The aim is to reap the benefit from the combination of the labelled data and unlabelled data. The proposed model extends a popular unsupervised autoencoder by carefully adjoining a supervised learning objective. We extensively evaluate the proposed model on the INTERSPEECH 2009 Emotion Challenge database and other four public databases in different scenarios. Experimental results demonstrate that the proposed model achieves state-of-the-art performance with a very small number of labelled data on the challenge task and other tasks, and significantly outperforms other alternative methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {31–43},
numpages = {13}
}

@article{10.1109/TASLP.2017.2761236,
author = {Abel, Johannes and Fingscheidt, Tim and Abel, Johannes and Fingscheidt, Tim},
title = {Artificial Speech Bandwidth Extension Using Deep Neural Networks for Wideband Spectral Envelope Estimation},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2761236},
doi = {10.1109/TASLP.2017.2761236},
abstract = {Estimating a wideband spectral envelope having only narrowband speech at hand is a challenging task. In this paper, we explore ways to do so in the context of an artificial speech bandwidth extension ABE framework. Starting from a typical hidden Markov model HMM/Gaussian mixture model baseline scheme, we investigate two types of features, topologies, and regularization approaches of deep neural networks DNNs to obtain estimates of wideband spectral envelopes with smallest cepstral distance to the original ones. In order to draw realistic conclusions, we employ a database for test, which is acoustically different to the training and validation speech material. Interestingly, it turns out that a DNN regression approach outperforms all other investigated methods, although the HMM has been dropped. Cepstral distance was reduced by 1.18 dB, wideband PESQ was improved by 0.23 MOS points, and a subjective comparison category rating listening test showed a significant preference of the best DNN ABE approach versus narrowband speech of 1.37 CMOS points.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {71–83},
numpages = {13}
}

@article{10.1109/TASLP.2017.2752364,
author = {Chakrabarty, Soumitro and Habets, Emanuel A. P. and Chakrabarty, Soumitro and Habets, Emanuel A. P.},
title = {A Bayesian Approach to Informed Spatial Filtering With Robustness Against DOA Estimation Errors},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2752364},
doi = {10.1109/TASLP.2017.2752364},
abstract = {A Bayesian approach to spatial filtering is presented, which is robust to uncertain or erroneous direction-of-arrival DOA information. The proposed framework aims to capture multiple sound sources at each time-frequency instant with an arbitrary direction-dependent gain, while attenuating diffuse sound and noise. For robustness, the DOA corresponding to each sound source is assumed to be a discrete random variable with a prior defined on a discrete set of candidate DOAs over the whole DOA space. With this assumption, the desired spatial filter is given as a weighted sum of spatial filters corresponding to a specific combination of probable DOA values, where the weights are given by the joint posterior probabilities of the combination of DOA values. Assuming the whole DOA space as the support for each random variable results in redundant computations and contributes to a high computational cost. To alleviate this problem, a narrowband DOA estimate-based posterior probability approximation method is proposed, which isolates regions in the DOA space with high probability of containing the actual source DOAs to compute time-adaptive supports for each random variable. Through experimental analysis, we demonstrate the robustness of the proposed framework against DOA estimation errors. Experimental evaluation with simulated and measured room impulse responses, in terms of objective performance measures, demonstrates the effectiveness of the framework to perform spatial filtering in noisy and reverberant acoustic environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {145–160},
numpages = {16}
}

@article{10.1109/TASLP.2017.2764545,
author = {Chen, Kuan-Yu and Liu, Shih-Hung and Chen, Berlin and Wang, Hsin-Min and Kuan-Yu Chen and Shih-Hung Liu and Berlin Chen and Hsin-Min Wang},
title = {An Information Distillation Framework for Extractive Summarization},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2764545},
doi = {10.1109/TASLP.2017.2764545},
abstract = {In the context of natural language processing, representation learning has emerged as a newly active research subject because of its excellent performance in many applications. Learning representations of words is a pioneering study in this school of research. However, paragraph or sentence and document embedding learning is more suitable/reasonable for some realistic tasks such as document summarization. Nevertheless, classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently may mislead the embedding learning process to produce a misty paragraph representation. Motivated by these observations, our major contributions in this paper are threefold. First, we propose a novel unsupervised paragraph embedding method, named the essence vector EV model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative low-dimensional vector representation for the paragraph of interest. Second, in view of the increasing importance of spoken content processing, an extension of the EV model, named the denoising essence vector D-EV model, is proposed. The D-EV model not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph against imperfect speech recognition. Third, a new summarization framework, which can take both relevance and redundancy information into account simultaneously, is also introduced. We evaluate the proposed embedding methods i.e., EV and D-EV and the summarization framework on two benchmark summarization corpora. The experimental results demonstrate the effectiveness and applicability of the proposed framework in relation to several well-practiced and state-of-the-art summarization methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {161–170},
numpages = {10}
}

@article{10.1109/TASLP.2017.2727684,
author = {Yee, Dianna and Kamkar-Parsi, Homayoun and Martin, Rainer and Puder, Henning and Yee, Dianna and Kamkar-Parsi, Homayoun and Martin, Rainer and Puder, Henning},
title = {A Noise Reduction Postfilter for Binaurally Linked Single-Microphone Hearing Aids Utilizing a Nearby External Microphone},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2727684},
doi = {10.1109/TASLP.2017.2727684},
abstract = {The use of a nearby external microphone for addressing front-back ambiguity in single-microphone hearing aid devices is investigated. Strategic placement of the external microphone is able to provide benefits from the body shielding back-directional noise and therefore information for discriminating between the frontal and back hemispheres. The scattering effects of the body are first analyzed to yield a placement strategy of the external microphone for maximizing the shielding effect of the body against back-directional noise sources while also optimizing for speech intelligibility. Assuming optimal placement, a frontal target source presence probability FTSPP estimator is derived. Using the FTSPP estimator, a more comprehensive noise estimator is proposed, which considers both stationary and nonstationary interferers. The performance of the proposed noise estimator is evaluated in its application for postfiltering the output of a binaural beamformer. The effect of postfiltering using the proposed noise estimator is to provide further reduction of directional noise from the lateral and back direction while preserving the frontal target signal. The resulting enhancement with the proposed noise estimator provides a better signal-to-noise ratio and improved objective speech quality and intelligibility of a frontal target speaker compared to the state of the art.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {5–18},
numpages = {14}
}

@article{10.1109/TASLP.2017.2761699,
author = {Andersen, Kristian Timm and Moonen, Marc and Andersen, Kristian Timm and Moonen, Marc},
title = {Robust Speech-Distortion Weighted Interframe Wiener Filters for Single-Channel Noise Reduction},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2761699},
doi = {10.1109/TASLP.2017.2761699},
abstract = {In this paper, speech-distortion weighted SDW interframe Wiener filters IFWFs are investigated for single-channel noise reduction in a filter bank structure. The filters utilize a parameter μ that explicitly sets a tradeoff between noise reduction and speech distortion and have traditionally been used in multichannel applications under the term SDW multichannel Wiener filter. The application of these SDW-IFWFs relies on the estimation of interframe correlation IFC coefficients, and it is shown that the IFC coefficients can be more robustly estimated using a secondary higher resolution filter bank HRFB. It is then shown how real-valued scalar gains, which are optimal in the primary filter bank, can be applied directly in the HRFB instead of the interframe filtering in the primary filter bank, which leads to a more robust noise reduction performance for any value of μ. Computing these gains is also cheaper since matrix inversions are avoided and the primary filter bank is not needed in the actual implementation. Experimental results are given that support the claims, where the proposed methods are compared to relevant reference methods using measures such as the segmental SNR and the objective PESQ.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {97–107},
numpages = {11}
}

@article{10.1109/TASLP.2017.2760243,
author = {Sahidullah, Md. and Thomsen, Dennis Alexander Lehmann and Hautamaki, Rosa Gonzalez and Kinnunen, Tomi and Tan, Zheng-Hua and Parts, Robert and Pitkanen, Martti and Sahidullah, Md and Thomsen, Dennis Alexander Lehmann and Gonzalez Hautamaki, Rosa and Kinnunen, Tomi and Zheng-Hua Tan and Parts, Robert and Pitkanen, Martti},
title = {Robust Voice Liveness Detection and Speaker Verification Using Throat Microphones},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2760243},
doi = {10.1109/TASLP.2017.2760243},
abstract = {While having a wide range of applications, automatic speaker verification ASV systems are vulnerable to spoofing attacks, in particular, replay attacks that are effective and easy to implement. Most prior work on detecting replay attacks uses audio from a single acoustic microphone only, leading to difficulties in detecting high-end replay attacks close to indistinguishable from live human speech. In this paper, we study the use of a special body-conducted sensor, throat microphone TM, for combined voice liveness detection VLD and ASV in order to improve both robustness and security of ASV against replay attacks. We first investigate the possibility and methods of attacking a TM-based ASV system, followed by a pilot data collection. Second, we study the use of spectral features for VLD using both single-channel and dual-channel ASV systems. We carry out speaker verification experiments using Gaussian mixture model with universal background model GMM-UBM and i-vector based systems on a dataset of 38 speakers collected by us. We have achieved considerable improvement in recognition accuracy, with the use of dual-microphone setup. In experiments with noisy test speech, the false acceptance rate FAR of the dual-microphone GMM-UBM based system for recorded speech reduces from 69.69% to 18.75%. The FAR of replay condition further drops to 0% when this dual-channel ASV system is integrated with the new dual-channel voice liveness detector.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {44–56},
numpages = {13}
}

@article{10.1109/TASLP.2017.2761547,
author = {Saito, Yuki and Takamichi, Shinnosuke and Saruwatari, Hiroshi and Saito, Yuki and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
title = {Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2761547},
doi = {10.1109/TASLP.2017.2761547},
abstract = {A method for statistical parametric speech synthesis incorporating generative adversarial networks GANs is proposed. Although powerful deep neural networks techniques can be applied to artificially synthesize speech waveform, the synthetic speech quality is low compared with that of natural speech. One of the issues causing the quality degradation is an oversmoothing effect often observed in the generated speech parameters. A GAN introduced in this paper consists of two neural networks: a discriminator to distinguish natural and generated samples, and a generator to deceive the discriminator. In the proposed framework incorporating the GANs, the discriminator is trained to distinguish natural and generated speech parameters, while the acoustic models are trained to minimize the weighted sum of the conventional minimum generation loss and an adversarial loss for deceiving the discriminator. Since the objective of the GANs is to minimize the divergence i.e., distribution difference between the natural and generated speech parameters, the proposed method effectively alleviates the oversmoothing effect on the generated speech parameters. We evaluated the effectiveness for text-to-speech and voice conversion, and found that the proposed method can generate more natural spectral parameters and F0 than conventional minimum generation error training algorithm regardless of its hyperparameter settings. Furthermore, we investigated the effect of the divergence of various GANs, and found that a Wasserstein GAN minimizing the Earth-Mover's distance works the best in terms of improving the synthetic speech quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {84–96},
numpages = {13}
}

@article{10.1109/TASLP.2017.2765834,
author = {Chen, Zhehuai and Droppo, Jasha and Li, Jinyu and Xiong, Wayne and Zhehuai Chen and Droppo, Jasha and Jinyu Li and Xiong, Wayne},
title = {Progressive Joint Modeling in Unsupervised Single-Channel Overlapped Speech Recognition},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2765834},
doi = {10.1109/TASLP.2017.2765834},
abstract = {Unsupervised single-channel overlapped speech recognition is one of the hardest problems in automatic speech recognition ASR. Permutation invariant training PIT is a state of the art model-based approach, which applies a single neural network to solve this single-input, multiple-output modeling problem. We propose to advance the current state of the art by imposing a modular structure on the neural network, applying a progressive pretraining regimen, and improving the objective function with transfer learning and a discriminative training criterion. The modular structure splits the problem into three subtasks: frame-wise interpreting, utterance-level speaker tracing, and speech recognition. The pretraining regimen uses these modules to solve progressively harder tasks. Transfer learning leverages parallel clean speech to improve the training targets for the network. Our discriminative training formulation is a modification of standard formulations that also penalizes competing outputs of the system. Experiments are conducted on the artificial overlapped switchboard and hub5e-swb dataset. The proposed framework achieves over 30% relative improvement of word error rate over both a strong jointly trained system, PIT for ASR, and a separately optimized system, PIT for speech separation with clean speech ASR model. The improvement comes from better model generalization, training efficiency, and the sequence level linguistic knowledge integration.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {184–196},
numpages = {13}
}

@article{10.1109/TASLP.2017.2761546,
author = {Degottex, Gilles and Lanchantin, Pierre and Gales, Mark and Degottex, Gilles and Lanchantin, Pierre and Gales, Mark},
title = {A Log Domain Pulse Model for Parametric Speech Synthesis},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2761546},
doi = {10.1109/TASLP.2017.2761546},
abstract = {Most of the degradation in current Statistical Parametric Speech Synthesis SPSS results from the form of the vocoder. One of the main causes of degradation is the reconstruction of the noise. In this article, a new signal model is proposed that leads to a simple synthesizer, without the need for ad-hoc tuning of model parameters. The model is not based on the traditional additive linear source-filter model, it adopts a combination of speech components that are additive in the log domain. Also, the same representation for voiced and unvoiced segments is used, rather than relying on binary voicing decisions. This avoids voicing error discontinuities that can occur in many current vocoders. A simple binary mask is used to denote the presence of noise in the time-frequency domain, which is less sensitive to classification errors. Four experiments have been carried out to evaluate this new model. The first experiment examines the noise reconstruction issue. Three listening tests have also been carried out that demonstrate the advantages of this model: comparison with the STRAIGHT vocoder; the direct prediction of the binary noise mask by using a mixed output configuration; and partial improvements of creakiness using a mask correction mechanism.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {57–70},
numpages = {14}
}

@article{10.1109/TASLP.2017.2764271,
author = {Tang, Zhiyuan and Wang, Dong and Chen, Yixiang and Li, Lantian and Abel, Andrew and Zhiyuan Tang and Dong Wang and Yixiang Chen and Lantian Li and Abel, Andrew},
title = {Phonetic Temporal Neural Model for Language Identification},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2764271},
doi = {10.1109/TASLP.2017.2764271},
abstract = {Deep neural models, particularly the long short-term memory recurrent neural network LSTM-RNN model, have shown great potential for language identification LID. However, the use of phonetic information has been largely overlooked by most existing neural LID methods, although this information has been used very successfully in conventional phonetic LID systems. We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system that accepts phonetic features produced by a phone-discriminative DNN as the input, rather than raw acoustic features. This new model is similar to traditional phonetic LID methods, but the phonetic knowledge here is much richer: It is at the frame level and involves compacted information of all phones. Our experiments conducted on the Babel database and the AP16-OLR database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic neural models. It also outperforms the conventional i-vector approach on short utterances and in noisy conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {134–144},
numpages = {11}
}

@article{10.1109/TASLP.2017.2766023,
author = {Jin, Ma and Song, Yan and McLoughlin, Ian and Dai, Li-Rong and Ma Jin and Yan Song and McLoughlin, Ian and Li-Rong Dai},
title = {LID-Senones and Their Statistics for Language Identification},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2766023},
doi = {10.1109/TASLP.2017.2766023},
abstract = {Recent research on end-to-end training structures for language identification has raised the possibility that intermediate language-sensitive feature units exist which are analogous to phonetically sensitive senones in automatic speech recognition systems. Termed language identification LID-senones, the statistics derived from these feature units have been shown to be beneficial in discriminating between languages, particularly for short utterances. This paper examines the evidence for the existence of LID-senones before designing and evaluating LID systems based on low- and high-level statistics of LID-senones with both generative and discriminative models. For the standard NIST LRE 2009 task on 23 languages, LID-senone-based systems are shown to outperform state-of-the-art deep neural network/i-vector methods both when LID-senones are used directly for classification and when LID-senone statistics are used for i-vector formation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {171–183},
numpages = {13}
}

@article{10.1109/TASLP.2017.2757601,
author = {Backstrom, Tom and Fischer, Johannes and Backstrom, Tom and Fischer, Johannes},
title = {Fast Randomization for Distributed Low-Bitrate Coding of Speech and Audio},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2757601},
doi = {10.1109/TASLP.2017.2757601},
abstract = {Efficient coding of speech and audio in a distributed system requires that quantization errors across nodes are uncorrelated. Yet, with conventional methods at low bitrates, quantization levels become increasingly sparse, which does not correspond to the distribution of the input signal and, importantly, also reduces coding efficiency in a distributed system. We have recently proposed a distributed speech and audio codec design, which applies quantization in a randomized domain such that quantization errors are randomly rotated in the output domain. Similar to dithering, this ensures that quantization errors across nodes are uncorrelated and coding efficiency is retained. In this paper, we improve this approach by proposing faster randomization methods, with a computational complexity of ON log N. The presented experiments demonstrate that the proposed randomizations yield uncorrelated signals, that perceptual quality is competitive, and that the complexity of the proposed methods is feasible for practical applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {19–30},
numpages = {12}
}

@article{10.1109/TASLP.2017.2765832,
author = {Ranjan, Shivesh and Hansen, John H. L. and Ranjan, Shivesh and Hansen, John H. L.},
title = {Curriculum Learning Based Approaches for Noise Robust Speaker Recognition},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2765832},
doi = {10.1109/TASLP.2017.2765832},
abstract = {Performance of speaker identification SID systems is known to degrade rapidly in the presence of mismatch such as noise and channel degradations. This study introduces a novel class of curriculum learning CL based algorithms for noise robust speaker recognition. We introduce CL-based approaches at two stages within a state-of-the-art speaker verification system: at the i-Vector extractor estimation and at the probabilistic linear discriminant PLDA back-end. Our proposed CL-based approaches operate by categorizing the available training data into progressively more challenging subsets using a suitable difficulty criterion. Next, the corresponding training algorithms are initialized with a subset that is closest to a clean noise-free set, and progressively moving to subsets that are more challenging for training as the algorithms progress. We evaluate the performance of our proposed approaches on the noisy and severely degraded data from the DARPA RATS SID task, and show consistent and significant improvement across multiple test sets over a baseline SID framework with a standard i-Vector extractor and multisession PLDA-based back-end. We also construct a very challenging evaluation set by adding noise to the NIST SRE 2010 C5 extended condition trials, where our proposed CL-based PLDA is shown to offer significant improvements over a traditional PLDA based back-end.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {197–210},
numpages = {14}
}

@article{10.1109/TASLP.2017.2763243,
author = {Liu, Bingquan and Xu, Zhen and Sun, Chengjie and Wang, Baoxun and Wang, Xiaolong and Wong, Derek F. and Zhang, Min and Bingquan Liu and Zhen Xu and Chengjie Sun and Baoxun Wang and Xiaolong Wang and Wong, Derek F. and Min Zhang},
title = {Content-Oriented User Modeling for Personalized Response Ranking in Chatbots},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2763243},
doi = {10.1109/TASLP.2017.2763243},
abstract = {Automatic chatbots also known as chat-agents have attracted much attention from both researching and industrial fields. Generally, the semantic relevance between users' queries and the corresponding responses is considered as the essential element for conversation modeling in both generation and ranking based chat systems. By contrast, it is a nontrivial task to adopt the users' information, such as preference, social role, etc., into conversational models reasonably, while users' profiles play a significant role in the procedure of conversations by providing the implicit contexts. This paper aims to address the personalized response ranking task by incorporating user profiles into the conversation model. In our approach, users' personalized representations are latently learned from the contents posted by them via a two-branch neural network. After that, a deep neural network architecture is further presented to learn the fusion representation of posts, responses, and personal information. In this way, the proposed model could understand conversations from the users' perspective; hence, the more appropriate responses are selected for a specified person. The experimental results on two datasets from social network services demonstrate that our approach is hopeful to represent users' personal information implicitly based on user generated contents, and it is promising to perform as an important component in chatbots to select the personalized responses for each user.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {122–133},
numpages = {12}
}

@article{10.1109/TASLP.2017.2762432,
author = {Chiang, Chen-Yu and Chen-Yu Chiang},
title = {Cross-Dialect Adaptation Framework for Constructing Prosodic Models for Chinese Dialect Text-to-Speech Systems},
year = {2018},
issue_date = {January 2018},
publisher = {IEEE Press},
volume = {26},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2762432},
doi = {10.1109/TASLP.2017.2762432},
abstract = {This paper presents an efficient cross-dialect adaptation framework for constructing prosodic models for Chinese dialect text-to-speech systems. In this framework, dialect prosodic models are adapted from an existing Mandarin speaking rate-dependent hierarchical prosodic model. The rationale of the framework is based on the cross-dialectal similarities between Mandarin and other Chinese dialects in terms of syntactic and prosodic structures. Two main problems are addressed in this study: One problem pertains to the use of cross-dialectal similarities in the design and adaptation of the dialect speaking rate-dependent hierarchical prosodic model. The other problem pertains to the data sparseness caused by the insufficiency of an adaptation corpus covering essential linguistic contexts and prosodic events as well as a wide speaking rate range. This problem is solved by employing the structural maximum a posteriori method that hierarchically organizes the dialect speaking rate-dependent hierarchical prosodic model parameters into decision trees to facilitate parameter estimations. The effectiveness of the proposed approach was evaluated by experiments on two Chinese dialects: Min and Hakka. Objective and subjective evaluations demonstrated that the prosodic features generated by the dialect speaking rate-dependent hierarchical prosodic models were quite natural in various speaking rates ranging from 3.3 to 6.7 syllables per second. These results confirm that the proposed cross-dialect adaptation framework is effective and promising.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {108–121},
numpages = {14}
}

@article{10.1109/TASLP.2017.2768838,
author = {Schultz, T. and Hueber, T. and Krusienski, D. J. and Brumberg, J. S.},
title = {Introduction to the Special Issue on Biosignal-Based Spoken Communication},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2768838},
doi = {10.1109/TASLP.2017.2768838},
abstract = {The papers in this special section focus on biosignal-based spoken communication. Speech production is a complex process resulting from human activities initiated in the brain, eventually leading to muscle activities that produce respiratory, laryngeal, and articulatory gestures which finally create acoustic signals. Traditional speech processing systems capture and interpret the acoustic signal of speech. However, speech is not only limited to acoustics - speech-related activities can be measured at each level of speech processing, including the central and peripheral nervous systems, muscular action potentials, and speech kinematics. Their measurement, obtained through recordings from variety of sensor technologies, results in speech-related "biosignals" that have been studied for decades to better understand the underlying mechanisms of human speech processing. However, there is more: speech-related biosignals have the potential to overcome limitations of traditional acoustic-based systems for spoken communication. Biosignals can be captured before the airborne acoustic signal and are thus less prone to environmental noise. Also, they do not rely on the production of audible speech - both features open up newtracks for "Biosignalbased Spoken Communication". Examples of these tracks include Brain-Computer Interfaces allowing for communication by directly decoding cortical brain activity into speech representations, and Silent-Speech Interfaces, which offer a way to communicate privately without disturbing bystanders and to restore spoken communication for people who lost their voice due to severe speech impairments. Furthermore, biosignals could provide valuable articulatory biofeedback to speakers about their own voice production for increasing articulatory awareness in speech therapy or language learning.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2254–2256},
numpages = {3}
}

@article{10.1109/TASLP.2017.2758999,
author = {Kim, Myungjong and Cao, Beiming and Mau, Ted and Wang, Jun},
title = {Speaker-Independent Silent Speech Recognition From Flesh-Point Articulatory Movements Using an LSTM Neural Network},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2758999},
doi = {10.1109/TASLP.2017.2758999},
abstract = {Silent speech recognition SSR converts nonaudio information such as articulatory movements into text. SSR has the potential to enable persons with laryngectomy to communicate through natural spoken expression. Current SSR systems have largely relied on speaker-dependent recognition models. The high degree of variability in articulatory patterns across different speakers has been a barrier for developing effective speaker-independent SSR approaches. Speaker-independent SSR approaches, however, are critical for reducing the amount of training data required from each speaker. In this paper, we investigate speaker-independent SSR from the movements of flesh points on tongue and lips with articulatory normalization methods that reduce the interspeaker variation. To minimize the across-speaker physiological differences of the articulators, we propose Procrustes matching-based articulatory normalization by removing locational, rotational, and scaling differences. To further normalize the articulatory data, we apply feature-space maximum likelihood linear regression and i-vector. In this paper, we adopt a bidirectional long short-term memory recurrent neural network BLSTM as an articulatory model to effectively model the articulatory movements with long-range articulatory history. A silent speech dataset with flesh-point articulatory movements was collected using an electromagnetic articulograph from 12 healthy and two laryngectomized English speakers. Experimental results showed the effectiveness of our speaker-independent SSR approaches on healthy as well as laryngectomy speakers. In addition, BLSTM outperformed the standard deep neural network. The best performance was obtained by the BLSTM with all the three normalization approaches combined.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2323–2336},
numpages = {14}
}

@article{10.1109/TASLP.2017.2753583,
author = {Tobing, Patrick Lumban and Kobayashi, Kazuhiro and Toda, Tomoki},
title = {Articulatory Controllable Speech Modification Based on Statistical Inversion and Production Mappings},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2753583},
doi = {10.1109/TASLP.2017.2753583},
abstract = {In this paper, we present an innovative way of utilizing the natural relationship between speech sounds and articulatory movements by developing an articulatory controllable speech modification system. Specifically, we employ statistical acoustic-to-articulatory inversion mapping and articulatory-to-acoustic production mapping based on a Gaussian mixture model, allowing flexible modification of the model parameters and the independence of the text input features. Modification of an input speech signal through manipulation of the unobserved articulatory movements is achievable through a sequence of inversion and production mappings. To ensure the naturalness of articulatory movement trajectories, we introduce a method for manipulating articulatory parameters by considering their intercorrelation. Moreover, to generate high-quality modified speech sounds, we avoid the use of vocoder-based excitation generation by presenting several implementations of direct waveform modification capable of directly filtering an input speech signal using the differences in spectral parameters. The experimental results demonstrate that: 1 higher accuracy in the estimation of spectral parameters is achieved by using sequential inversion and production mappings than for conventional production mapping using measured articulatory parameters, 2 the method for manipulating articulatory parameters by considering their intercorrelation makes it possible to generate more natural trajectories of modified articulatory movements; 3 the implementations of the direct waveform modification method significantly improve the quality of modified speech sounds, even under varying speaking conditions; and 4 the controllability of the system is ensured by its capability of producing modified vowel sounds through the manipulation of appropriate articulatory configurations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2337–2350},
numpages = {14}
}

@article{10.1109/TASLP.2017.2756440,
author = {Xiong, Wayne and Droppo, Jasha and Huang, Xuedong and Seide, Frank and Seltzer, Michael L. and Stolcke, Andreas and Yu, Dong and Zweig, Geoffrey},
title = {Toward Human Parity in Conversational Speech Recognition},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2756440},
doi = {10.1109/TASLP.2017.2756440},
abstract = {Conversational speech recognition has served as a flagship speech recognition task since the release of the Switchboard corpus in the 1990s. In this paper, we measure a human error rate on the widely used NIST 2000 test set for commercial bulk transcription. The error rate of professional transcribers is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion, where friends and family members have open-ended conversations. In both cases, our automated system edges past the human benchmark, achieving error rates of 5.8% and 11.0%, respectively. The key to our system's performance is the use of various convolutional and long-short-term memory acoustic model architectures, combined with a novel spatial smoothing method and lattice-free discriminative acoustic training, multiple recurrent neural network language modeling approaches, and a systematic use of system combination. Comparing frequent errors in our human and machine transcripts, we find them to be remarkably similar, and highly correlated as a function of the speaker. Human subjects find it very difficult to tell which errorful transcriptions come from humans. Overall, this suggests that, given sufficient matched training data, conversational speech transcription engines are approximating human parity in both quantitative and qualitative terms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2410–2423},
numpages = {14}
}

@article{10.1109/TASLP.2017.2738445,
author = {Asaei, Afsaneh and Cernak, Milos and Bourlard, Herve},
title = {Perceptual Information Loss Due to Impaired Speech Production},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2738445},
doi = {10.1109/TASLP.2017.2738445},
abstract = {Phonological classes define articulatory-free and articulatory-bound phone attributes. Deep neural network is used to estimate the probability of phonological classes from the speech signal. In theory, a unique combination of phone attributes form a phoneme identity. Probabilistic inference of phonological classes thus enables estimation of their compositional phoneme probabilities. A novel information theoretic framework is devised to quantify the information conveyed by each phone attribute, and assess the speech production quality for perception of phonemes. As a use case, we hypothesize that disruption in speech production leads to information loss in phone attributes, and thus confusion in phoneme identification. We quantify the amount of information loss due to dysarthric articulation recorded in the TORGO database. A novel information measure is formulated to evaluate the deviation from an ideal phone attribute production leading us to distinguish healthy production from pathological speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2433–2443},
numpages = {11}
}

@article{10.1109/TASLP.2017.2738568,
author = {Janke, Matthias and Diener, Lorenz},
title = {EMG-to-Speech: Direct Generation of Speech From Facial Electromyographic Signals},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2738568},
doi = {10.1109/TASLP.2017.2738568},
abstract = {Silent speech interfaces are systems that enable speech communication even when an acoustic signal is unavailable. Over the last years, public interest in such interfaces has intensified. They provide solutions for some of the challenges faced by today's speech-driven technologies, such as robustness to noise and usability for people with speech impediments. In this paper, we provide an overview over our silent speech interface. It is based on facial surface electromyography EMG, which we use to record the electrical signals that control muscle contraction during speech production. These signals are then converted directly to an audible speech waveform, retaining important paralinguistic speech cues for information such as speaker identity and mood. This paper gives an overview over our state-of-the-art direct EMG-to-speech transformation system. This paper describes the characteristics of the speech EMG signal, introduces techniques for extracting relevant features, presents different EMG-to-speech mapping methods, and finally, presents an evaluation of the different methods for real-time capability and conversion quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2375–2385},
numpages = {11}
}

@article{10.1109/TASLP.2017.2759002,
author = {Borsky, Michal and Mehta, Daryush D. and Van Stan, Jarrad H. and Gudnason, Jon},
title = {Modal and Nonmodal Voice Quality Classification Using Acoustic and Electroglottographic Features},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2759002},
doi = {10.1109/TASLP.2017.2759002},
abstract = {The goal of this study was to investigate the performance of different feature types for voice quality classification using multiple classifiers. The study compared the COVAREP feature set; which included glottal source features, frequency warped cepstrum, and harmonic model features; against the mel-frequency cepstral coefficients MFCCs computed from the acoustic voice signal, acoustic-based glottal inverse filtered GIF waveform, and electroglottographic EGG waveform. Our hypothesis was that MFCCs can capture the perceived voice quality from either of these three voice signals. Experiments were carried out on recordings from 28 participants with normal vocal status who were prompted to sustain vowels with modal and nonmodal voice qualities. Recordings were rated by an expert listener using the Consensus Auditory-Perceptual Evaluation of Voice CAPE-V, and the ratings were transformed into a dichotomous label presence or absence for the prompted voice qualities of modal voice, breathiness, strain, and roughness. The classification was done using support vector machines, random forests, deep neural networks, and Gaussian mixture model classifiers, which were built as speaker independent using a leave-one-speaker-out strategy. The best classification accuracy of 79.97% was achieved for the full COVAREP set. The harmonic model features were the best performing subset, with 78.47% accuracy, and the static+dynamic MFCCs scored at 74.52%. A closer analysis showed that MFCC and dynamic MFCC features were able to classify modal, breathy, and strained voice quality dimensions from the acoustic and GIF waveforms. Reduced classification performance was exhibited by the EGG waveform.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2281–2291},
numpages = {11}
}

@article{10.1109/TASLP.2017.2755400,
author = {Chen, Fei and Wang, Lan and Chen, Hui and Peng, Gang},
title = {Investigations on Mandarin Aspiratory Animations Using an Airflow Model},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2755400},
doi = {10.1109/TASLP.2017.2755400},
abstract = {Various three-dimensional 3-D talking heads have been developed lately for language learning, with both external and internal articulatory movements being visualized to guide learning. Mandarin pronunciation animation is challenging due to its confusable stops and affricates with similar places of articulation. Until now, less attention has been paid to the biosignal information of aspiratory airflow, which is essential in distinguishing Mandarin consonants. This study fills a research gap by presenting the quantitative analyses of airflow, and then designing an airflow model for a 3-D pronunciation system. The airflow information was collected by Phonatory Aerodynamic System, so that confusable consonants in Mandarin could be discerned by mean airflow rate, peak airflow rate, airflow duration, and peak time. Based on the airflow parameters, an airflow model using the physical equation of fluid flow was proposed and solved, which was then combined and synchronized with the existing 3-D articulatory model. Therefore, the new multimodal system was implemented to synchronously exhibit the airflow motions and articulatory movements of uttering Mandarin syllables. Both an audio-visual perception test and a pronunciation training study were conducted to assess the effectiveness of our system. Perceptual results indicated that identification accuracy was improved for both native and nonnative groups with the help of airflow motions, while native perceivers exhibited higher accuracy due to long-term language experience. Moreover, our system helped Japanese learners of Mandarin enhance their production skills of Mandarin aspirated consonants, reflected by higher gain values of voice onset time after training.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2399–2409},
numpages = {11}
}

@article{10.1109/TASLP.2017.2752365,
author = {Schultz, Tanja and Wand, Michael and Hueber, Thomas and Krusienski, Dean J. and Herff, Christian and Brumberg, Jonathan S.},
title = {Biosignal-Based Spoken Communication: A Survey},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2752365},
doi = {10.1109/TASLP.2017.2752365},
abstract = {Speech is a complex process involving a wide range of biosignals, including but not limited to acoustics. These biosignals—stemming from the articulators, the articulator muscle activities, the neural pathways, and the brain itself—can be used to circumvent limitations of conventional speech processing in particular, and to gain insights into the process of speech production in general. Research on biosignal-based speech processing is a wide and very active field at the intersection of various disciplines, ranging from engineering, computer science, electronics and machine learning to medicine, neuroscience, physiology, and psychology. Consequently, a variety of methods and approaches have been used to investigate the common goal of creating biosignal-based speech processing devices for communication applications in everyday situations and for speech rehabilitation, as well as gaining a deeper understanding of spoken communication. This paper gives an overview of the various modalities, research approaches, and objectives for biosignal-based spoken communication.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2257–2271},
numpages = {15}
}

@article{10.1109/TASLP.2017.2757263,
author = {Gonzalez, Jose A. and Cheah, Lam A. and Gomez, Angel M. and Green, Phil D. and Gilbert, James M. and Ell, Stephen R. and Moore, Roger K. and Holdsworth, Ed},
title = {Direct Speech Reconstruction From Articulatory Sensor Data by Machine Learning},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2757263},
doi = {10.1109/TASLP.2017.2757263},
abstract = {This paper describes a technique that generates speech acoustics from articulator movements. Our motivation is to help people who can no longer speak following laryngectomy, a procedure that is carried out tens of thousands of times per year in the Western world. Our method for sensing articulator movement, permanent magnetic articulography, relies on small, unobtrusive magnets attached to the lips and tongue. Changes in magnetic field caused by magnet movements are sensed and form the input to a process that is trained to estimate speech acoustics. In the experiments reported here this “Direct Synthesis” technique is developed for normal speakers, with glued-on magnets, allowing us to train with parallel sensor and acoustic data. We describe three machine learning techniques for this task, based on Gaussian mixture models, deep neural networks, and recurrent neural networks RNNs. We evaluate our techniques with objective acoustic distortion measures and subjective listening tests over spoken sentences read from novels the CMU Arctic corpus. Our results show that the best performing technique is a bidirectional RNN BiRNN, which employs both past and future contexts to predict the acoustics from the sensor data. BiRNNs are not suitable for synthesis in real time but fixed-lag RNNs give similar results and, because they only look a little way into the future, overcome this problem. Listening tests show that the speech produced by this method has a natural quality that preserves the identity of the speaker. Furthermore, we obtain up to 92% intelligibility on the challenging CMU Arctic material. To our knowledge, these are the best results obtained for a silent-speech system without a restricted vocabulary and with an unobtrusive device that delivers audio in close to real time. This work promises to lead to a technology that truly will give people whose larynx has been removed their voices back.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2362–2374},
numpages = {13}
}

@article{10.1109/TASLP.2017.2738564,
author = {Dromey, Christopher and Black, Katherine M.},
title = {Effects of Laryngeal Activity on Articulation},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2738564},
doi = {10.1109/TASLP.2017.2738564},
abstract = {This study examined the effects of three speech conditions voiced, whispered, and mouthed on global average measures of articulatory movement during sentence production. Participants were 20 adults ten males and ten females with no history of speech, language, or hearing disorders. They produced six target utterances in the three speaking conditions. Movements of the following articulators were recorded with an electromagnetic articulograph: Mid tongue, front of tongue, jaw, lower lip, and upper lip. The kinematic metrics were averages derived from movement strokes defined by minima in the speed record of each articulator. These measures revealed a number of significant changes between the voiced and mouthed conditions, with relatively few differences between voiced and whispered speech. Significant increases in sentence duration, articulatory stroke count, and stroke duration as well as significant decreases in peak stroke speed, stroke distance, and hull areas were found in the mouthed condition. These findings suggest that both laryngeal activation and auditory feedback play an important role in the production of normally articulated vocal tract movements, and that the absence of these may account for the significant changes in articulation between the voiced and mouthed conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2272–2280},
numpages = {9}
}

@article{10.1109/TASLP.2017.2758164,
author = {Rezazadeh Sereshkeh, Alborz and Trott, Robert and Bricout, Aurelien and Chau, Tom},
title = {EEG Classification of Covert Speech Using Regularized Neural Networks},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2758164},
doi = {10.1109/TASLP.2017.2758164},
abstract = {Communication using brain–computer interfaces BCIs can be non-intuitive, often requiring the performance of a conversation-irrelevant task such as hand motor imagery. In this paper, the reliability of electroencephalography EEG signals in discriminating between different covert speech tasks is investigated. Twelve participants, across two sessions each, were asked to perform multiple iterations of three differing mental tasks for 10&nbsp;s each: unconstrained rest or the mental repetition of the words “yes” or “no.” A multilayer perceptron MLP artificial neural network ANN was used to classify all three pairwise combinations of “yes,” “no,” and rest trials and also for ternary classification. An average accuracy of 75.7%&nbsp;±&nbsp;9.6 was reached in the classification of covert speech trials versus rest, with all participants exceeding chance level 57.8%. The classification of “yes” versus “no” yielded an average accuracy of 63.2&nbsp;±&nbsp;6.4 with ten participants surpassing chance level 57.8%. Finally, the ternary classification yielded an average accuracy of 54.1%&nbsp;±&nbsp;9.7 with all participants exceeding chance level 39.1%. The proposed MLP network provided significantly higher accuracies compared to some of the most common classification techniques in BCI. To our knowledge, this is the first report of using ANN for the classification of EEG covert speech across multiple sessions. Our findings support further study of covert speech as a BCI activation task, potentially leading to the development of more intuitive BCIs for communication.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2292–2300},
numpages = {9}
}

@article{10.1109/TASLP.2017.2738559,
author = {Grozdic, Dorde T. and Jovicic, Slobodan T.},
title = {Whispered Speech Recognition Using Deep Denoising Autoencoder and Inverse Filtering},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2738559},
doi = {10.1109/TASLP.2017.2738559},
abstract = {Due to the profound differences between acoustic characteristics of neutral and whispered speech, the performance of traditional automatic speech recognition ASR systems trained on neutral speech degrades significantly when whisper is applied. In order to deeply analyze this mismatched train/test situation and to develop an efficient way for whisper recognition, this study first analyzes acoustic characteristics of whispered speech, addresses the problems of whispered speech recognition in mismatched conditions, and then proposes a new robust cepstral features and preprocessing approach based on deep denoising autoencoder DDAE that enhance whisper recognition. The experimental results confirm that Teager-energy-based cepstral features, especially TECCs, are more robust and better whisper descriptors than traditional Mel-frequency cepstral coefficients MFCC. Further detailed analysis of cepstral distances, distributions of cepstral coefficients, confusion matrices, and experiments with inverse filtering, prove that voicing in speech stimuli is the main cause of word misclassification in mismatched train/test scenarios. The new framework based on DDAE and TECC feature, significantly improves whisper recognition accuracy and outperforms traditional MFCC and GMM-HMM Gaussian mixture density—Hidden Markov model baseline, resulting in an absolute 31% improvement of whisper recognition accuracy. The achieved word recognition rate in neutral/whisper scenario is 92.81%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2313–2322},
numpages = {10}
}

@article{10.1109/TASLP.2017.2751420,
author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Duan, Hong},
title = {A Context-Aware Recurrent Encoder for Neural Machine Translation},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2751420},
doi = {10.1109/TASLP.2017.2751420},
abstract = {Neural machine translation NMT heavily relies on its encoder to capture the underlying meaning of a source sentence so as to generate a faithful translation. However, most NMT encoders are built upon either unidirectional or bidirectional recurrent neural networks, which either do not deal with future context or simply concatenate the history and future context to form context-dependent word representations, implicitly assuming the independence of the two types of contextual information. In this paper, we propose a novel context-aware recurrent encoder CAEncoder, as an alternative to the widely-used bidirectional encoder, such that the future and history contexts can be fully incorporated into the learned source representations. Our CAEncoder involves a two-level hierarchy: The bottom level summarizes the history information, whereas the upper level assembles the summarized history and future context into source representations. Additionally, CAEncoder is as efficient as the bidirectional RNN encoder in terms of both training and decoding. Experiments on both Chinese–English and English–German translation tasks show that CAEncoder achieves significant improvements over the bidirectional RNN encoder on a widely-used NMT system. 1},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2424–2432},
numpages = {9}
}

@article{10.1109/TASLP.2017.2740000,
author = {Meltzner, Geoffrey S. and Heaton, James T. and Deng, Yunbin and De Luca, Gianluca and Roy, Serge H. and Kline, Joshua C.},
title = {Silent Speech Recognition as an Alternative Communication Device for Persons With Laryngectomy},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2740000},
doi = {10.1109/TASLP.2017.2740000},
abstract = {Each year thousands of individuals require surgical removal of the larynx voice box due to trauma or disease, and thereby require an alternative voice source or assistive device to verbally communicate. Although natural voice is lost after laryngectomy, most muscles controlling speech articulation remain intact. Surface electromyographic sEMG activity of speech musculature can be recorded from the neck and face, and used for automatic speech recognition to provide speech-to-text or synthesized speech as an alternative means of communication. This is true even when speech is mouthed or spoken in a silent subvocal manner, making it an appropriate communication platform after laryngectomy. In this study, eight individuals at least 6 months after total laryngectomy were recorded using eight sEMG sensors on their face 4 and neck 4 while reading phrases constructed from a 2500-word vocabulary. A unique set of phrases were used for training phoneme-based recognition models for each of the 39 commonly used phonemes in English, and the remaining phrases were used for testing word recognition of the models based on phoneme identification from running speech. Word error rates were on average 10.3% for the full eight-sensor set averaging 9.5% for the top four participants, and 13.6% when reducing the sensor set to four locations per individual n = 7. This study provides a compelling proof-of-concept for sEMG-based alaryngeal speech recognition, with the strong potential to further improve recognition performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2386–2398},
numpages = {13}
}

@article{10.1109/TASLP.2017.2750760,
author = {Ma, Ning and May, Tobias and Brown, Guy J.},
title = {Exploiting Deep Neural Networks and Head Movements for Robust Binaural Localization of Multiple Sources in Reverberant Environments},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2750760},
doi = {10.1109/TASLP.2017.2750760},
abstract = {This paper presents a novel machine-hearing system that exploits deep neural networks DNNs and head movements for robust binaural localization of multiple sources in reverberant environments. DNNs are used to learn the relationship between the source azimuth and binaural cues, consisting of the complete cross-correlation function CCF and interaural level differences ILDs. In contrast to many previous binaural hearing systems, the proposed approach is not restricted to localization of sound sources in the frontal hemifield. Due to the similarity of binaural cues in the frontal and rear hemifields, front–back confusions often occur. To address this, a head movement strategy is incorporated in the localization model to help reduce the front–back errors. The proposed DNN system is compared to a Gaussian-mixture-model-based system that employs interaural time differences ITDs and ILDs as localization features. Our experiments show that the DNN is able to exploit information in the CCF that is not available in the ITD cue, which together with head movements substantially improves localization accuracies under challenging acoustic scenarios, in which multiple talkers and room reverberation are present.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2444–2453},
numpages = {10}
}

@article{10.1109/TASLP.2017.2751747,
author = {Sahraeian, Reza and Van Compernolle, Dirk},
title = {Crosslingual and Multilingual Speech Recognition Based on the Speech Manifold},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2751747},
doi = {10.1109/TASLP.2017.2751747},
abstract = {Speech signals are produced by the smooth and continuous movements of the human articulators. An articulatory representation of speech is considered to be a more compact, more universal, and language-independent speech feature space and can, therefore, improve crosslingual and multilingual speech recognition systems, especially when porting components from one language to another in low-resource scenarios. However, learning the acoustic-to-articulatory conversion has proven to be a very challenging task. In this paper, we utilize a manifold learning technique to derive a nonlinear feature transformation from the conventional filterbank feature space to an articulatory-like feature space. The coordinates in the resultant representation of which some have demonstrable phonological meaning are shown to be highly portable across languages. We propose a proper framework in terms of data selection and graph construction to train coordinates from multilingual data, which allows for training the coordinate space when we have abundant out-of-language data. Deep neural network DNN bottleneck features are demonstrated to exhibit a greater degree of language independence when using this representation than in the case of filterbank features as inputs. The usability of this representation is further demonstrated in a number of speech recognition experiments using DNNs in a variety of crosslingual and multilingual scenarios using the multilingual GlobalPhone dataset. Especially, speech recognition systems developed in low-resource settings profit from the improved portability across languages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2301–2312},
numpages = {12}
}

@article{10.1109/TASLP.2017.2756818,
author = {Steiner, Ingmar and Le Maguer, Sebastien and Hewer, Alexander},
title = {Synthesis of Tongue Motion and Acoustics From Text Using a Multimodal Articulatory Database},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2756818},
doi = {10.1109/TASLP.2017.2756818},
abstract = {We present an end-to-end text-to-speech TTS synthesis system that generates audio and synchronized tongue motion directly from text. This is achieved by adapting a three-dimensional model of the tongue surface to an articulatory dataset and training a statistical parametric speech synthesis system directly on the tongue model parameters. We evaluate the model at every step by comparing the spatial coordinates of predicted articulatory movements against the reference data. The results indicate a global mean Euclidean distance of less than 2.8 mm, and our approach can be adapted to add an articulatory modality to conventional TTS applications without the need for extra data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2351–2361},
numpages = {11}
}

@article{10.1109/TASLP.2017.2763038,
author = {Stefanakis, N. and Pavlidi, D. and Mouchtaris, A.},
title = {Corrections to "Perpendicular Cross-Spectra Fusion for Sound Source Localization With a Planar Microphone Array" [Sep 17 1821-1835]},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2763038},
doi = {10.1109/TASLP.2017.2763038},
abstract = {Presents corrections to the paper, "Perpendicular cross-spectra fusion for sound source localization with a planar microphone array," Stefanakis, N., et al, IEEE/ACMTrans. Audio, Speech, Lang. Process., vol. 25, no. 9, pp. 1821-1835, Sep. 2017.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2251},
numpages = {1}
}

@article{10.1109/TASLP.2017.2743620,
author = {Pal, Monisankha and Saha, Goutam and Pal, Monisankha and Saha, Goutam},
title = {Spectral Mapping Using Prior Re-Estimation of i-Vectors and System Fusion for Voice Conversion},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2743620},
doi = {10.1109/TASLP.2017.2743620},
abstract = {In this paper, we propose a new voice conversion VC method using i-vectors which consider low-dimensional representation of speech utterances. An attempt is made to restrict the i-vector variability in the intermediate computation of total variability T matrix by using a novel approach that uses modified-prior distribution of the intermediate i-vectors. This T-modification improves the speaker individuality conversion. For further improvement of conversion score and to keep a better balance between similarity and quality, band-wise spectrogram fusion between conventional joint density Gaussian mixture model JDGMM and i-vector based converted spectrograms is employed. The fused spectrogram retains more spectral details and leverages the complementary merits of each subsystem. Experiments in terms of objective and subjective evaluation are conducted extensively on CMU ARCTIC database. The results show that the proposed technique can produce a better trade-off between similarity and quality score than other state-of-the-art baseline VC methods. Furthermore, it works better than JDGMM in limited VC training data. The proposed VC performs moderately better both objective and subjective than mixture of factor analyzer based baseline VC. In addition, the proposed VC provides better quality converted speech as compared to maximum likelihood-GMM VC with dynamic feature constraint.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2071–2084},
numpages = {14}
}

@article{10.1109/TASLP.2017.2740002,
author = {Hayashi, Tomoki and Watanabe, Shinji and Toda, Tomoki and Hori, Takaaki and Le Roux, Jonathan and Takeda, Kazuya and Hayashi, Tomoki and Watanabe, Shinji and Toda, Tomoki and Hori, Takaaki and Le Roux, Jonathan and Takeda, Kazuya},
title = {Duration-Controlled LSTM for Polyphonic Sound Event Detection},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2740002},
doi = {10.1109/TASLP.2017.2740002},
abstract = {This paper presents a new hybrid approach called duration-controlled long short-term memory LSTM for polyphonic sound event detection SED. It builds upon a state-of-the-art SED method that performs frame-by-frame detection using a bidirectional LSTM recurrent neural network BLSTM, and incorporates a duration-controlled modeling technique based on a hidden semi-Markov model. The proposed approach makes it possible to model the duration of each sound event precisely and to perform sequence-by-sequence detection without having to resort to thresholding, as in conventional frame-by-frame methods. Furthermore, to effectively reduce sound event insertion errors, which often occur under noisy conditions, we also introduce a binary-mask-based postprocessing that relies on a sound activity detection network to identify segments with any sound event activity, an approach inspired by the well-known benefits of voice activity detection in speech recognition systems. We conduct an experiment using the DCASE2016 task 2 dataset to compare our proposed method with typical conventional methods, such as nonnegative matrix factorization and standard BLSTM. Our proposed method outperforms the conventional methods both in an event-based evaluation, achieving a 75.3% F1 score and a 44.2% error rate, and in a segment-based evaluation, achieving an 81.1% F1 score, and a 32.9% error rate, outperforming the best results reported in the DCASE2016 task 2 Challenge.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2059–2070},
numpages = {12}
}

@article{10.1109/TASLP.2017.2738438,
author = {Schwartz, Boaz and Gannot, Sharon and Habets, Emanuel A. P. and Schwartz, Boaz and Gannot, Sharon and Habets, Emanuel A. P.},
title = {Two Model-Based EM Algorithms for Blind Source Separation in Noisy Environments},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2738438},
doi = {10.1109/TASLP.2017.2738438},
abstract = {The problem of blind separation of speech signals in the presence of noise using multiple microphones is addressed. Blind estimation of the acoustic parameters and the individual source signals is carried out by applying the expectation-maximization EM algorithm. Two models for the speech signals are used, namely an unknown deterministic signal model and a complex-Gaussian signal model. For the two alternatives, we define a statistical model and develop EM-based algorithms to jointly estimate the acoustic parameters and the speech signals. The resulting algorithms are then compared from both theoretical and performance perspectives. In both cases, the latent data differently defined for each alternative are estimated in the E-step, where in the M-step, the two algorithms estimate the acoustic transfer functions of each source and the noise covariance matrix. The algorithms differ in the way the clean speech signals are used in the EM scheme. When the clean signal is assumed deterministic unknown, only the a posteriori probabilities of the presence of each source are estimated in the E-step, whereas their time-frequency coefficients are the parameters that are estimated in the M-step using the minimum variance distortionless response beamformer. If the clean speech signals are modeled as complex Gaussian signals, their power spectral densities are estimated in the E-step using the multichannel Wiener filter output. The proposed algorithms were tested using reverberant noisy mixtures of two speech sources in different reverberation and noise conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2209–2222},
numpages = {14}
}

@article{10.1109/TASLP.2017.2750767,
author = {Ottosen, Emil Solsbaek and Dorfler, Monika and Ottosen, Emil Solsbaek and Dorfler, Monika},
title = {A Phase Vocoder Based on Nonstationary Gabor Frames},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2750767},
doi = {10.1109/TASLP.2017.2750767},
abstract = {We propose a new algorithm for time stretching music signals based on the theory of nonstationary Gabor frames NSGFs. The algorithm extends the techniques of the classical phase vocoder PV by incorporating adaptive time-frequency TF representations and adaptive phase locking. The adaptive TF representations imply good time resolution for the onsets of attack transients and good frequency resolution for the sinusoidal components. We estimate the phase values only at peak channels and the remaining phases are then locked to the values of the peaks in an adaptive manner. During attack transients we keep the stretch factor equal to one and we propose a new strategy for determining which channels are relevant for reinitializing the corresponding phase values. In contrast to previously published algorithms we use a non-uniform NSGF to obtain a low redundancy of the corresponding TF representation. We show that with just three times as many TF coefficients as signal samples, artifacts such as phasiness and transient smearing can be greatly reduced compared to the classical PV. The proposed algorithm is tested on both synthetic and real-world signals and compared with state-of-the-art algorithms in a reproducible manner.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2199–2208},
numpages = {10}
}

@article{10.1109/TASLP.2017.2746264,
author = {Song, Eunwoo and Soong, Frank K. and Kang, Hong-Goo and Eunwoo Song and Soong, Frank K. and Hong-Goo Kang},
title = {Effective Spectral and Excitation Modeling Techniques for LSTM-RNN-Based Speech Synthesis Systems},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2746264},
doi = {10.1109/TASLP.2017.2746264},
abstract = {In this paper, we report research results on modeling the parameters of an improved time-frequency trajectory excitation ITFTE and spectral envelopes of an LPC vocoder with a long short-term memory LSTM-based recurrent neural network RNN for high-quality text-to-speech TTS systems. The ITFTE vocoder has been shown to significantly improve the perceptual quality of statistical parameter-based TTS systems in our prior works. However, a simple feed-forward deep neural network DNN with a finite window length is inadequate to capture the time evolution of the ITFTE parameters. We propose to use the LSTM to exploit the time-varying nature of both trajectories of the excitation and filter parameters, where the LSTM is implemented to use the linguistic text input and to predict both ITFTE and LPC parameters holistically. In the case of LPC parameters, we further enhance the generated spectrum by applying LP bandwidth expansion and line spectral frequency-sharpening filters. These filters are not only beneficial for reducing unstable synthesis filter conditions but also advantageous toward minimizing the muffling problem in the generated spectrum. Experimental results have shown that the proposed LSTM-RNN system with the ITFTE vocoder significantly outperforms both similarly configured band aperiodicity-based systems and our best prior DNN-trainecounterpart, both objectively and subjectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2152–2161},
numpages = {10}
}

@article{10.1109/TASLP.2017.2748240,
author = {Sharma, Pulkit and Abrol, Vinayak and Sao, Anil Kumar and Sharma, Pulkit and Abrol, Vinayak and Sao, Anil Kumar},
title = {Deep-Sparse-Representation-Based Features for Speech Recognition},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2748240},
doi = {10.1109/TASLP.2017.2748240},
abstract = {Features derived using sparse representation SR-based approaches have been shown to yield promising results for speech recognition tasks. In most of the approaches, the SR corresponding to speech signal is estimated using a dictionary, which could be either exemplar based or learned. However, a single-level decomposition may not be suitable for the speech signal, as it contains complex hierarchical information about various hidden attributes. In this paper, we propose to use a multilevel decomposition having multiple layers, also known as the deep sparse representation DSR, to derive a feature representation for speech recognition. Instead of having a series of sparse layers, the proposed framework employs a dense layer between two sparse layers, which helps in efficient implementation. Our studies reveal that the representations obtained at different sparse layers of the proposed DSR model have complimentary information. Thus, the final feature representation is derived after concatenating the representations obtained at the sparse layers. This results in a more discriminative representation, and improves the speech recognition performance. Since the concatenation results in a high-dimensional feature, principal component analysis is used to reduce the dimension of the obtained feature. Experimental studies demonstrate that the proposed feature outperforms existing features for various speech recognition tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2162–2175},
numpages = {14}
}

@article{10.1109/TASLP.2017.2744264,
author = {Menzies, Dylan and Fazi, Filippo Maria and Menzies, Dylan and Fazi, Filippo Maria},
title = {Decoding and Compression of Channel and Scene Objects for Spatial Audio},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2744264},
doi = {10.1109/TASLP.2017.2744264},
abstract = {Sound fields can be encoded with a fixed number of signals, using microphones or panning functions. The sound field may later be reproduced approximately by decoding the signals to a loudspeaker array. The Stereo and Ambisonic systems provide examples. A framework is presented for addressing general questions about such encodings. The first problem considered is the conversion between encodings. The solution is applied to the decoding of scene encodings to a loudspeaker array. This is generalized to the decoding of subscenes where the resolution is focused in an angular window. Within an object-based audio framework such subscenes are useful for representing complex objects without using all the channels required for a full scene. The second problem considered is the compression of a scene encoding to a smaller encoding, from which the original can be reconstructed. The spatial distribution of compression error can be controlled.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2138–2151},
numpages = {14}
}

@article{10.1109/TASLP.2017.2743340,
author = {Muckenhirn, Hannah and Korshunov, Pavel and Magimai-Doss, Mathew and Marcel, Sebastien and Muckenhirn, Hannah and Korshunov, Pavel and Magimai-Doss, Mathew and Marcel, Sebastien},
title = {Long-Term Spectral Statistics for Voice Presentation Attack Detection},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2743340},
doi = {10.1109/TASLP.2017.2743340},
abstract = {Automatic speaker verification systems can be spoofed through recorded, synthetic, or voice converted speech of target speakers. To make these systems practically viable, the detection of such attacks, referred to as presentation attacks, is of paramount interest. In that direction, this paper investigates two aspects: 1 a novel approach to detect presentation attacks where, unlike conventional approaches, no speech signal modeling related assumptions are made, rather the attacks are detected by computing first-order and second-order spectral statistics and feeding them to a classifier, and 2 generalization of the presentation attack detection systems across databases. Our investigations on ASVspoof 2015 challenge database and AVspoof database show that, when compared to the approaches based on conventional short-term spectral features, the proposed approach with a linear discriminative classifier yields a better system, irrespective of whether the spoofed signal is replayed to the microphone or is directly injected into the system software process. Cross-database investigations show that neither the short-term spectral processing-based approaches nor the proposed approach yield systems which are able to generalize across databases or methods of attack. Thus, revealing the difficulty of the problem and the need for further resources and research.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2098–2111},
numpages = {14}
}

@article{10.1109/TASLP.2017.2744799,
author = {Hamilton, Brian and Bilbao, Stefan and Hamilton, Brian and Bilbao, Stefan},
title = {FDTD Methods for 3-D Room Acoustics Simulation With High-Order Accuracy in Space and Time},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2744799},
doi = {10.1109/TASLP.2017.2744799},
abstract = {Time-domain finite difference FDTD methods are popular tools for three-dimensional 3-D room acoustics modeling, but numerical dispersion is an inherent problem that can place limitations on the usable bandwidth of a given scheme. Compact explicit 27-point schemes and “large-star” schemes with high-order spatial differences offer improvements to the simplest scheme, but are ultimately limited by their second-order accuracy in time. In this paper, we use modified equation methods to derive FDTD schemes with high orders of accuracy in both space and time, resulting in significant improvements in numerical dispersion as compared to the aforementioned schemes. In comparison to such schemes, the high-order accurate schemes presented in this paper use significantly less memory and fewer operations when low error tolerances in numerical phase velocities are critical, leading to higher usable bandwidths for auralization purposes. Simulation results are also presented, demonstrating improved approximations to modal frequencies of a shoe-box room and free-space propagation of a bandlimited pulse.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2112–2124},
numpages = {13}
}

@article{10.1109/TASLP.2017.2756443,
author = {Giorgi, Bruno Di and Dixon, Simon and Zanoni, Massimiliano and Sarti, Augusto and Di Giorgi, Bruno and Dixon, Simon and Zanoni, Massimiliano and Sarti, Augusto},
title = {A Data-Driven Model of Tonal Chord Sequence Complexity},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2756443},
doi = {10.1109/TASLP.2017.2756443},
abstract = {We present a compound language model of tonal chord sequences, and evaluate its capability to estimate perceived harmonic complexity. In order to build the compound model, we trained three different models: prediction by partial matching, a hidden Markov model and a deep recurrent neural network on a novel large dataset containing half a million annotated chord sequences. We describe the training process and propose an interpretation of the harmonic patterns that are learned by the hidden states of these models. We use the compound model to generate new chord sequences and estimate their probability, which we then relate to perceived harmonic complexity. In order to collect subjective ratings of complexity, we devised a listening test comprising two different experiments. In the first, subjects choose the more complex chord sequence between two. In the second, subjects rate with a continuous scale the complexity of a single chord sequence. The results of both experiments show a strong relation between negative log probability, given by our language model, and the perceived complexity ratings. The relation is stronger for subjects with high musical sophistication index, acquired through the GoldMSI standard questionnaire. The analysis of the results also includes the preference ratings that have been collected along with the complexity ratings; a weak negative correlation emerged between preference and log probability.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2237–2250},
numpages = {14}
}

@article{10.1109/TASLP.2017.2747097,
author = {Yu, Chengzhu and Hansen, John H. L. and Chengzhu Yu and Hansen, John H. L.},
title = {Active Learning Based Constrained Clustering For Speaker Diarization},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2747097},
doi = {10.1109/TASLP.2017.2747097},
abstract = {Most speaker diarization research has focused on unsupervised scenarios, where no human supervision is available. However, in many real-world applications, a certain amount of human input could be expected, especially when minimal human supervision brings significant performance improvement. In this study, we propose an active learning based bottom-up speaker clustering algorithm to effectively improve speaker diarization performance with limited human input. Specifically, the proposed active learning based speaker clustering has two different stages: explore and constrained clustering. The explore stage is to quickly discover at least one sample for each speaker for boosting speaker clustering process with reliable initial speaker clusters. After discovering all, or a majority, of the involved speakers during explore stage, the constrained clustering is performed. Constrained clustering is similar to traditional bottom-up clustering process with an important difference that the clusters created during explore stage are restricted from merging with each other. Constrained clustering continues until only the clusters generated from the explore stage are left. Since the objective of active learning based speaker clustering algorithm is to provide good initial speaker models, performance saturates as soon as sufficient examples are ensured for each cluster. To further improve diarization performance with increasing human input, we propose a second method which actively select speech segments that account for the largest expected speaker error from existing cluster assignments for human evaluation and reassignment. The algorithms are evaluated on our recently created Apollo Mission Control Center dataset as well as augmented multiparty interaction meeting corpus. The results indicate that the proposed active learning algorithms are able to reduce diarization error rate significantly with a relatively small amount of human supervision.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2188–2198},
numpages = {11}
}

@article{10.1109/TASLP.2017.2747082,
author = {Mowlaee, Pejman and Blass, Martin and Kleijn, W. Bastiaan and Mowlaee, Pejman and Blass, Martin and Kleijn, W. Bastiaan},
title = {New Results in Modulation-Domain Single-Channel Speech Enhancement},
year = {2017},
issue_date = {November 2017},
publisher = {IEEE Press},
volume = {25},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2747082},
doi = {10.1109/TASLP.2017.2747082},
abstract = {We investigate single-channel speech enhancement using the double spectrum DS consisting of pitch-synchronous and modulation transforms. We first explore the fundamentals of the proposed DS domain and its advantageous properties for pitch estimation and speech presence probability estimation. We then propose speech enhancement methods based on adaptive weighting and Wiener filtering in the DS domain. We demonstrate the effectiveness of the proposed DS-based methods compared to the conventional benchmarks in the modulation or short-time Fourier transform domains. Our results show a good tradeoff between improved perceived quality and slight degradation in speech intelligibility is achieved by the proposed method across different signal-to-noise ratios and noise types.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2125–2137},
numpages = {13}
}

@article{10.1109/TASLP.2017.2738698,
author = {Jo, Byeongho and Choi, Jung-Woo and Byeongho Jo and Jung-Woo Choi},
title = {Spherical Harmonic Smoothing for Localizing Coherent Sound Sources},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2738698},
doi = {10.1109/TASLP.2017.2738698},
abstract = {Correlated or coherent sources can cause the localization performance of subspace-based beamformers to deteriorate. To solve this problem, various smoothing techniques have been proposed for the localization of multiple coherent sound sources. A common principle of smoothing techniques is to increase the rank of a covariance matrix by constructing multiple subarrays in the space, time, or frequency domain. The construction of such subarrays, however, requires the satisfaction of strong assumptions regarding the microphone positions or the temporal/spectral structures of the signals. In this paper, we propose a spherical harmonic smoothing technique that can perform smoothing in terms of spherical harmonic coefficients only. Unlike other smoothing techniques, the proposed technique constructs subarrays of spherical harmonic coefficients and uses them to increase the number of linearly independent observations in a signal subspace. Subarray construction in the spherical harmonics domain enables the accurate localization of multiple coherent sources even at a single frequency. The proposed technique can be applied to an arbitrary microphone array as long as the spherical harmonic coefficients can be measured up to a finite order.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1969–1984},
numpages = {16}
}

@article{10.1109/TASLP.2017.2740004,
author = {Jokinen, Emma and Remes, Ulpu and Alku, Paavo and Jokinen, Emma and Remes, Ulpu and Alku, Paavo},
title = {Intelligibility Enhancement of Telephone Speech Using Gaussian Process Regression for Normal-to-Lombard Spectral Tilt Conversion},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2740004},
doi = {10.1109/TASLP.2017.2740004},
abstract = {Noise in the environment can decrease the quality and intelligibility of a telephone conversation. This study focuses on the intelligibility enhancement of narrowband telephone speech in a near-end noise scenario using a postprocessing method based on normal-to-Lombard spectral tilt conversion. The proposed technique uses nonparallel, conversational normal, and Lombard speech together with Gaussian process regression in order to mimic the flattening of the spectral tilt that occurs in the production of natural speech in noisy conditions. The performance of the proposed method was evaluated in comparison to two reference methods, a fixed high-pass filter, and a baseline spectral tilt conversion, as well as in comparison to unprocessed speech in terms of intelligibility and listening preference in noisy conditions and in terms of pressedness in silent conditions. The results indicate that while the proposed technique provides a similar benefit in terms of intelligibility as fixed high-pass filtering, it is also able to produce a notable increase in pressedness. This suggests that the developed processing of the spectral tilt can compete with fixed high-pass filtering in intelligibility enhancement, but it is also able to convert speech to become perceptually closer to natural Lombard speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1985–1996},
numpages = {12}
}

@article{10.1109/TASLP.2017.2740001,
author = {Li, Xiaofei and Girin, Laurent and Horaud, Radu and Gannot, Sharon and Xiaofei Li and Girin, Laurent and Horaud, Radu and Gannot, Sharon},
title = {Multiple-Speaker Localization Based on Direct-Path Features and Likelihood Maximization With Spatial Sparsity Regularization},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2740001},
doi = {10.1109/TASLP.2017.2740001},
abstract = {This paper addresses the problem of multiple-speaker localization in noisy and reverberant environments, using binaural recordings of an acoustic scene. A complex-valued Gaussian mixture model CGMM is adopted, whose components correspond to all the possible candidate source locations defined on a grid. After optimizing the CGMM-based objective function, given an observed set of complex-valued binaural features, both the number of sources and their locations are estimated by selecting the CGMM components with the largest weights. An entropy-based penalty term is added to the likelihood to impose sparsity over the set of CGMM component weights. This favors a small number of detected speakers with respect to the large number of initial candidate source locations. In addition, the direct-path relative transfer function DP-RTF is used to build robust binaural features. The DP-RTF, recently proposed for single-source localization, encodes interchannel information corresponding to the direct path of sound propagation and is thus robust to reverberations. In this paper, we extend the DP-RTF estimation to the case of multiple sources. In the short-time Fourier transform domain, a consistency test is proposed to check whether a set of consecutive frames is associated with the same source or not. Reliable DP-RTF features are selected from the frames that pass the consistency test to be used for source localization. Experiments carried out using both simulation data and real data recorded with a robotic head confirm the efficiency of the proposed multisource localization method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1997–2012},
numpages = {16}
}

@article{10.1109/TASLP.2017.2736067,
author = {Hafezi, Sina and Moore, Alastair H. and Naylor, Patrick A. and Hafezi, Sina and Moore, Alastair H. and Naylor, Patrick A.},
title = {Augmented Intensity Vectors for Direction of Arrival Estimation in the Spherical Harmonic Domain},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2736067},
doi = {10.1109/TASLP.2017.2736067},
abstract = {Pseudointensity vectors PIVs provide a means of direction of arrival DOA estimation for spherical microphone arrays using only the zeroth and the first-order spherical harmonics. An augmented intensity vector AIV is proposed which improves the accuracy of PIVs by exploiting higher order spherical harmonics. We compared DOA estimation using our proposed AIVs against PIVs, steered response power SRP and subspace methods where the number of sources, their angular separation, the reverberation time of the room and the sensor noise level are varied. The results show that the proposed approach outperforms the baseline methods and performs at least as accurately as the state-of-theart method with strong robustness to reverberation, sensor noise, and number of sources. In the single and multiple source scenarios tested, which include realistic levels of reverberation and noise, the proposed method had average error of 1.5° and 2°, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1956–1968},
numpages = {13}
}

@article{10.1109/TASLP.2017.2724203,
author = {Wang, Siying and Ewert, Sebastian and Dixon, Simon and Siying Wang and Ewert, Sebastian and Dixon, Simon},
title = {Identifying Missing and Extra Notes in Piano Recordings Using Score-Informed Dictionary Learning},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2724203},
doi = {10.1109/TASLP.2017.2724203},
abstract = {The goal of automatic music transcription AMT is to obtain a high-level symbolic representation of the notes played in a given audio recording. Despite being researched for several decades, current methods are still inadequate for many applications. To boost the accuracy in a music tutoring scenario, we exploit that the score to be played is specified and we only need to detect the differences to the actual performance. In contrast to previous work that uses score information for postprocessing, we employ the score to construct a transcription method that is tailored to the given audio recording. By adapting a score-informed dictionary learning technique as used for source separation, we learn for each score pitch a spectral pattern describing the energy distribution of associated notes in the recording. In this paper, we identify several systematic weaknesses in our previous approach and introduce three extensions to improve its performance. First, we extend our dictionary of spectral templates to a dictionary of variable-length spectrotemporal patterns. Second, we integrate the score information using soft rather than hard constraints, to better take into account that differences from the score indeed occur. Third, we introduce new regularizers to guide the learning process. Our experiments show that these extensions particularly improve the accuracy for identifying extra notes, while the accuracy for correct and missing notes remains at a similar level. The influence of each extension is demonstrated with further experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1877–1889},
numpages = {13}
}

@article{10.1109/TASLP.2017.2726762,
author = {Kolbaek, Morten and Yu, Dong and Tan, Zheng-Hua and Jensen, Jesper and Kolbaek, Morten and Dong Yu and Zheng-Hua Tan and Jensen, Jesper},
title = {Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2726762},
doi = {10.1109/TASLP.2017.2726762},
abstract = {In this paper, we propose the utterance-level permutation invariant training uPIT technique. uPIT is a practically applicable, end-to-end, deep-learning-based solution for speaker independent multitalker speech separation. Specifically, uPIT extends the recently proposed permutation invariant training PIT technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using recurrent neural networks RNNs that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multitalker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity, or gender. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on nonnegative matrix factorization and computational auditory scene analysis, and compares favorably with deep clustering, and the deep attractor network. Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1901–1913},
numpages = {13}
}

@article{10.1109/TASLP.2017.2729024,
author = {Chung, Cheng-Tao and Tsai, Cheng-Yu and Liu, Chia-Hsiang and Lee, Lin-Shan and Cheng-Tao Chung and Cheng-Yu Tsai and Chia-Hsiang Liu and Lin-Shan Lee},
title = {Unsupervised Iterative Deep Learning of Speech Features and Acoustic Tokens with Applications to Spoken Term Detection},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2729024},
doi = {10.1109/TASLP.2017.2729024},
abstract = {In this paper, we aim to automatically discover high-quality frame-level speech features and acoustic tokens directly from unlabeled speech data. A multigranular acoustic tokenizer MAT was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters describing the model configuration. These different sets of acoustic tokens carry different characteristics for the given corpus and the language behind and, thus, can be mutually reinforced. The multiple sets of token labels are then used as the targets of a multitarget deep neural network MDNN trained on frame-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. The multigranular acoustic token sets and the frame-level speech features can be iteratively optimized in the iterative deep learning framework. We call this framework the MAT deep neural network. The results were evaluated using the metrics and corpora defined in the Zero Resource Speech Challenge organized at Interspeech 2015, and improved performance was obtained with a set of experiments of query-by-example spoken term detection on the same corpora. Visualization for the discovered tokens against the English phonemes was also shown.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1914–1928},
numpages = {15}
}

@article{10.1109/TASLP.2017.2723721,
author = {Tian, Xiaohai and Lee, Siu Wa and Wu, Zhizheng and Chng, Eng Siong and Li, Haizhou and Xiaohai Tian and Siu Wa Lee and Zhizheng Wu and Eng Siong Chng and Haizhou Li},
title = {An Exemplar-Based Approach to Frequency Warping for Voice Conversion},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2723721},
doi = {10.1109/TASLP.2017.2723721},
abstract = {The voice conversion's task is to modify a source speaker's voice to sound like that of a target speaker. A conversion method is considered successful when the produced speech sounds natural and similar to the target speaker. This paper presents a new voice conversion framework in which we combine frequency warping and exemplar-based method for voice conversion. Our method maintains high-resolution details during conversion by directly applying frequency warping on the high-resolution spectrum to represent the target. The warping function is generated by a sparse interpolation from a dictionary of exemplar warping functions. As the generated warping function is dependent only on a very small set of exemplars, we do away with the statistical averaging effects inherited from Gaussian mixture models. To compensate for the conversion error, we also apply residual exemplars into the conversion process. Both objective and subjective evaluations on the VOICES database validated the effectiveness of the proposed voice conversion framework. We observed a significant improvement in speech quality over the state-of-the-art parametric methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1863–1876},
numpages = {14}
}

@article{10.1109/TASLP.2017.2724198,
author = {Cumani, Sandro and Laface, Pietro and Cumani, Sandro and Laface, Pietro},
title = {Joint Estimation of PLDA and Nonlinear Transformations of Speaker Vectors},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2724198},
doi = {10.1109/TASLP.2017.2724198},
abstract = {The Gaussian probabilistic linear discriminant anal-ysis PLDA model assumes Gaussian distributed priors for the latent variables that represent the speaker and channel factors. Assuming that each training i-vector belongs to a different speaker, as is usually done in i-vector extraction, i-vectors generated by a PLDA model can be considered independent and identically distributed with Gaussian distribution. Thus, we have recently proposed to transform the development i-vectors so that their distribution becomes more Gaussian-like. This is obtained by means of a sequence of affine and nonlinear transformations whose parameters are trained by maximum likelihood ML estimation on the development set. The evaluation i-vectors are then subject to the same transformation. Although the i-vector “gaussianization” has shown to be effective, since the i-vectors extracted from segments of the same speaker are not independent, the original assumption is not satisfactory. In this work, we show that the model can be improved by properly exploiting the information about the speaker labels, which was ignored in the previous model. In particular, a more effective PLDA model can be obtained by jointly estimating the PLDA parameters and the parameters of the nonlinear transformation of the i-vectors. In other words, while the goal of the previous approach was to “gaussianize” the training i-vectors distribution, the objective of this work is to embed the estimation of the nonlinear i-vector transformation in the PLDA model estimation. We will thus refer to this model as the nonlinear PLDA model. We show that this new approach provides significant gain with respect to PLDA, and a small, yet consistent, improvement with respect to our former i-vector “gaussianization” approach, without further additional costs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1890–1900},
numpages = {11}
}

@article{10.1109/TASLP.2017.2744261,
author = {Baby, Deepak and Van hamme, Hugo and Baby, Deepak and Van hamme, Hugo},
title = {Joint Denoising and Dereverberation Using Exemplar-Based Sparse Representations and Decaying Norm Constraint},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2744261},
doi = {10.1109/TASLP.2017.2744261},
abstract = {Exemplar-based nonnegative models, where the noisy speech is decomposed as a sparse nonnegative linear combination of the speech and noise exemplars stored in a dictionary, have been successfully used for speech denoising. This paper extends this technique for the single-channel speech enhancement in noisy reverberant environments using a novel approximation of the noisy reverberant speech in the frequency domain and nonnegative matrix deconvolution. In the proposed model, the room impulse response RIR in the magnitude short-time Fourier transform domain is defined such that its decaying structure can also be estimated from the test data itself, whereas the existing models used a suboptimal binwise clamping procedure to impose such a decaying structure that does not hold in a typical RIR. This paper presents multiplicative updates for estimating the RIR, its decay, and the underlying anechoic speech and noise. The proposed model is evaluated on a synthetically created dataset created by convolving TIMIT recordings with RIRs measured from different rooms and varying speaker-and-microphone locations, and adding background noises taken from the CHiME corpus. Simulation results show that the proposed model results in a better RIR estimate over the existing model and improves various instrumental speech quality measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {2024–2035},
numpages = {12}
}

@article{10.1109/TASLP.2017.2735179,
author = {Arnela, Marc and Guasch, Oriol and Arnela, Marc and Guasch, Oriol},
title = {Finite Element Synthesis of Diphthongs Using Tuned Two-Dimensional Vocal Tracts},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2735179},
doi = {10.1109/TASLP.2017.2735179},
abstract = {Three-dimensional 3-D vocal tract acoustic modeling has the potential to generate high quality and natural voice sounds, but at the price of a large computational cost. Alternatively, 2-D models based on tuned vocal tracts have shown to provide similar results to the 3-D ones but with less computational demands. However, they are currently limited to the synthesis of static vowel sounds. In this paper, the tuned 2-D approach is extended by considering moving vocal tracts to generate dynamic vowel sounds, like diphthongs. Four tuning steps are followed to build a dynamic 2-D vocal tract model that can recover, to a large extent, the formant locations, bandwidths, and energies of a 3-D vocal tract with circular cross section, set in a spherical baffle representing the human head. Acoustic waves propagating through the time evolving vocal tract and radiating to free-field are simulated using the finite element method in the time-domain. As examples, the diphthongs [Ai] and [Au] have been generated using the tuning approach and compared, by means of objective and subjective evaluations, to those resulting from 3-D and conventional 2-D simulations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {2013–2023},
numpages = {11}
}

@article{10.1109/TASLP.2017.2730284,
author = {Antonello, Niccolo and De Sena, Enzo and Moonen, Marc and Naylor, Patrick A. and van Waterschoot, Toon and Antonello, Niccolo and De Sena, Enzo and Moonen, Marc and Naylor, Patrick A. and van Waterschoot, Toon},
title = {Room Impulse Response Interpolation Using a Sparse Spatio-Temporal Representation of the Sound Field},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2730284},
doi = {10.1109/TASLP.2017.2730284},
abstract = {Room Impulse Responses RIRs are typically measured using a set of microphones and a loudspeaker. When RIRs spanning a large volume are needed, many microphone measurements must be used to spatially sample the sound field. In order to reduce the number of microphone measurements, RIRs can be spatially interpolated. In the present study, RIR interpolation is formulated as an inverse problem. This inverse problem relies on a particular acoustic model capable of representing the measurements. Two different acoustic models are compared: the plane wave decomposition model and a novel time-domain model, which consists of a collection of equivalent sources creating spherical waves. These acoustic models can both approximate any reverberant sound field created by a far-field sound source. In order to produce an accurate RIR interpolation, sparsity regularization is employed when solving the inverse problem. In particular, by combining different acoustic models with different sparsity promoting regularizations, spatial sparsity, spatio-spectral sparsity, and spatio-temporal sparsity are compared. The inverse problem is solved using a matrix-free large-scale optimization algorithm. Simulations show that the best RIR interpolation is obtained when combining the novel time-domain acoustic model with the spatio-temporal sparsity regularization, outperforming the results of the plane wave decomposition model even when far fewer microphone measurements are available.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1929–1941},
numpages = {13}
}

@article{10.1109/TASLP.2017.2732162,
author = {Qian, Yanmin and Chen, Nanxin and Dinkel, Heinrich and Wu, Zhizheng and Yanmin Qian and Nanxin Chen and Dinkel, Heinrich and Zhizheng Wu},
title = {Deep Feature Engineering for Noise Robust Spoofing Detection},
year = {2017},
issue_date = {October 2017},
publisher = {IEEE Press},
volume = {25},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2732162},
doi = {10.1109/TASLP.2017.2732162},
abstract = {Spoofing detection for automatic speaker verification ASV aims to discriminate between genuine and spoofed speech. This topic has received increased attentions recently due to safety concerns with deploying an ASV system. While the performance of spoofing detection has improved significantly in clean condition in recent studies, the performance degrades dramatically in noisy conditions. To address this issue, in this paper, we propose to extract robust and discriminative deep features by using deep learning techniques for spoofing detection. In particular, we employ deep feedforward, recurrent, and convolutional neural networks to extract discriminative features. We also introduce multicondition training, noise-aware training, and annealed dropout training to make neural networks more robust against noise and to avoid overfitting to specific spoofing attacks and noise types. The proposed neural networks and training techniques are combined into a single framework for spoofing detection. Experimental evaluation is carried out on a noisy version of the standard ASVspoof 2015 corpus, including both additive noisy and reverberant scenarios. Experimental results confirm that the proposed system dramatically decreases averaged equal error rates from 19.1% and 22.6% to 3.2% and 5.1% for seen and unseen noisy conditions, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1942–1955},
numpages = {14}
}

@article{10.1109/TASLP.2017.2717499,
author = {Patel, Vinal and Cheer, Jordan and George, Nithin V. and Patel, Vinal and Cheer, Jordan and George, Nithin V.},
title = {Modified Phase-Scheduled-Command FxLMS Algorithm for Active Sound Profiling},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2717499},
doi = {10.1109/TASLP.2017.2717499},
abstract = {Active sound profiling, or active noise equalization strategies have been proposed to achieve spectral shaping of a primary disturbance signal. The control algorithms proposed to achieve such spectral shaping have either suffered from poor robustness to plant modeling uncertainties or required high levels of control effort. To improve the robustness of active sound profiling to uncertainties in the plant model, while avoiding increased control effort, a modified phase-scheduled-command filtered-x least-mean-square algorithm is proposed in this paper. The new algorithm provides improved stability, while requiring the minimum control effort. This improvement is achieved by replacing the plant model with an intelligent adaptive-hysteresis switching mechanism to allow the necessary estimation of the disturbance signal phase. The improved performance and robustness of the proposed algorithm is demonstrated through a series of simulations using measured acoustic responses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1799–1808},
numpages = {10}
}

@article{10.1109/TASLP.2017.2722103,
author = {Nakamura, Eita and Yoshii, Kazuyoshi and Dixon, Simon and Nakamura, Eita and Yoshii, Kazuyoshi and Dixon, Simon},
title = {Note Value Recognition for Piano Transcription Using Markov Random Fields},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2722103},
doi = {10.1109/TASLP.2017.2722103},
abstract = {This paper presents a statistical method for use in music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times or note values and, thus, could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results show that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1846–1858},
numpages = {13}
}

@article{10.1109/TASLP.2017.2718843,
author = {Janod, Killian and Morchid, Mohamed and Dufour, Richard and Linares, Georges and De Mori, Renato and Janod, Killian and Morchid, Mohamed and Dufour, Richard and Linares, Georges and De Mori, Renato},
title = {Denoised Bottleneck Features From Deep Autoencoders for Telephone Conversation Analysis},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2718843},
doi = {10.1109/TASLP.2017.2718843},
abstract = {Automatic transcription of spoken documents is affected by automatic transcription errors that are especially frequent when speech is acquired in severe noisy conditions. Automatic speech recognition errors induce errors in the linguistic features used for a variety of natural language processing tasks. Recently, denoisng autoencoders DAE and stacked autoencoders SAE have been proposed with interesting results for acoustic feature denoising tasks. This paper deals with the recovery of corrupted linguistic features in spoken documents. Solutions based on DAEs and SAEs are considered and evaluated in a spoken conversation analysis task. In order to improve conversation theme classification accuracy, the possibility of combining abstractions obtained from manual and automatic transcription features is considered. As a result, two original representations of highly imperfect spoken documents are introduced. They are based on bottleneck features of a supervised autoencoder that takes advantage of both noisy and clean transcriptions to improve the robustness of error prone representations. Experimental results on a spoken conversation theme identification task show substantial accuracy improvements obtained with the proposed recovery of corrupted features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1809–1820},
numpages = {12}
}

@article{10.1109/TASLP.2017.2702384,
author = {Abeser, Jakob and Schuller, Gerald and Abeser, Jakob and Schuller, Gerald},
title = {Instrument-Centered Music Transcription of Solo Bass Guitar Recordings},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2702384},
doi = {10.1109/TASLP.2017.2702384},
abstract = {This paper deals with the automatic transcription of solo bass guitar recordings with an additional estimation of playing techniques and fretboard positions used by the musician. Our goal is to first develop a system for a robust estimation of the note parameters pitch, onset, and duration score-level parameters. As a second step, we aim to automatically detect the applied plucking and expression style as well as the fret and string positions for each note instrument-level parameters. Our approach is to first apply a note onset detection followed by a tracking of the fundamental frequency contours based on a reassigned magnitude spectrogram. Then, we model the spectral envelope of each note and derive various timbre-related audio features. Using a support vector machine classifier, we automatically classify the instrument-level parameters for each detected note event. Our results show that the proposed system achieves accuracy values above 0.88 for the estimation of the plucking style, expression style, and string number for isolated note samples. As an additional contribution, we analyze the influence of the note duration characteristics in the classification performance. In a score-level evaluation on a novel public dataset of solo bass guitar tracks, our method outperforms three existing transcription algorithms for bass transcription in polyphonic music as well as a melody transcription algorithm for monophonic music.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1741–1750},
numpages = {10}
}

@article{10.1109/TASLP.2017.2716195,
author = {Liu, Lemao and Fujita, Atsushi and Utiyama, Masao and Finch, Andrew and Sumita, Eiichiro and Lemao Liu and Fujita, Atsushi and Utiyama, Masao and Finch, Andrew and Sumita, Eiichiro},
title = {Translation Quality Estimation Using Only Bilingual Corpora},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2716195},
doi = {10.1109/TASLP.2017.2716195},
abstract = {In computer-aided translation scenarios, quality estimation of machine translation hypotheses plays a critical role. Existing methods for word-level translation quality estimation TQE rely on the availability of manually annotated TQE training data obtained via direct annotation or postediting. However, due to the cost of human labor, such data are either limited in size or is only available for few tasks in practice. To avoid the reliance on such annotated TQE data, this paper proposes an approach to train word-level TQE models using bilingual corpora, which are typically used in machine translation training and is relatively easier to access. We formalize the training of our proposed method under the framework of maximum marginal likelihood estimation. To avoid degenerated solutions, we propose a novel regularized training objective whose optimization is achieved by an efficient approximation. Extensive experiments on both written and spoken language datasets empirically show that our approach yields comparable performance to the standard training on annotated data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1762–1772},
numpages = {11}
}

@article{10.1109/TASLP.2017.2716188,
author = {Bernardi, Giuliano and van Waterschoot, Toon and Wouters, Jan and Moonen, Marc and Bernardi, Giuliano and van Waterschoot, Toon and Wouters, Jan and Moonen, Marc},
title = {Adaptive Feedback Cancellation Using a Partitioned-Block Frequency-Domain Kalman Filter Approach With PEM-Based Signal Prewhitening},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2716188},
doi = {10.1109/TASLP.2017.2716188},
abstract = {Adaptive filtering based feedback cancellation is a widespread approach to acoustic feedback control. However, traditional adaptive filtering algorithms have to be modified in order to work satisfactorily in a closed-loop scenario. In particular, the undesired signal correlation between the loudspeaker signal and the source signal in a closed-loop scenario is one of the major problems to address when using adaptive filters for feedback cancellation. Slow convergence speed and limited tracking capabilities are other important limitations to be considered. Additionally, computationally expensive algorithms as well as long delays should be avoided, for instance, in hearing aid applications, because of power constraints, important to extend battery life, and real-time implementations requirements, respectively. We present an algorithm combining good decorrelation properties, by means of the prediction-error method based signal prewhitening, fast convergence, good tracking behavior, and low computational complexity by means of the frequency-domain Kalman filter, and low delay by means of a partitioned-block implementation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1784–1798},
numpages = {15}
}

@article{10.1109/TASLP.2017.2718733,
author = {Stefanakis, Nikolaos and Pavlidi, Despoina and Mouchtaris, Athanasios and Stefanakis, Nikolaos and Pavlidi, Despoina and Mouchtaris, Athanasios},
title = {Perpendicular Cross-Spectra Fusion for Sound Source Localization With a Planar Microphone Array},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2718733},
doi = {10.1109/TASLP.2017.2718733},
abstract = {Multiple sound source localization in reverberant environments stands as one of the most difficult challenges for many applications related to microphone array signal processing. In this paper, we describe perpendicular cross-spectra fusion PCSF, a new direction-of-arrival DOA estimation algorithm, which utilizes an analytic formula for direction estimation in the time-frequency TF domain. Inherent to this technique is the presence of multiple direction estimation subsystems which operate in parallel, producing a multiplicity of candidate DOAs at each TF point. We define a metric of coherence, based on the property of divergence of the different DOA estimators, for assessing the reliability of different signal portions, so that only TF bins with a high quality of directional information are exploited for local DOA estimation. The resulting collection of local DOAs is provided as input to a recently proposed histogram processing approach, which is based on matching pursuit. Results based on simulation and real recordings illustrate the advantages of PCSF compared to other DOA estimation techniques subject to the same histogram-based processing, in the context of real-time multiple source localization and counting; improved performance in reverberant conditions and high tolerance to diffuse and common mode noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1821–1835},
numpages = {15}
}

@article{10.1109/TASLP.2017.2721219,
author = {Yoshimura, Takenori and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi and Yoshimura, Takenori and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
title = {Simultaneous Optimization of Multiple Tree-Based Factor Analyzed HMM for Speech Synthesis},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2721219},
doi = {10.1109/TASLP.2017.2721219},
abstract = {This paper proposes a novel method to build multiple decision trees as a structure of factor analyzed hidden Markov model for speech synthesis. In the proposed method, the multiple decision trees grow simultaneously rather than sequentially to take into account the relationship between the trees. However, the simultaneous growing is computationally infeasible due to an exponential increase in the number of tree structures to be evaluated. To solve the problem, we further propose two computational complexity reduction algorithms that achieve a significant reduction in the computational time. Experimental results show that the proposed method outperforms the conventional one based on a single decision tree.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1836–1845},
numpages = {10}
}

@article{10.1109/TASLP.2017.2716178,
author = {Le Cornu, Thomas and Milner, Ben and Le Cornu, Thomas and Milner, Ben},
title = {Generating Intelligible Audio Speech From Visual Speech},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2716178},
doi = {10.1109/TASLP.2017.2716178},
abstract = {This paper is concerned with generating intelligible audio speech from a video of a person talking. Regression and classification methods are proposed first to estimate static spectral envelope features from active appearance model visual features. Two further methods are then developed to incorporate temporal information into the prediction: A feature-level method using multiple frames and a model-level method based on recurrent neural networks. Speech excitation information is not available from the visual signal, so methods to artificially generate aperiodicity and fundamental frequency are developed. These are combined within the STRAIGHT vocoder to produce a speech signal. The various systems are optimized through objective tests before applying subjective intelligibility tests that determine a word accuracy of 85% from a set of human listeners on the GRID audio-visual speech database. This compares favorably with a previous regression-based system that serves as a baseline, which achieved a word accuracy of 33%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1751–1761},
numpages = {11}
}

@article{10.1109/TASLP.2017.2716443,
author = {Grais, Emad M. and Roma, Gerard and Simpson, Andrew J. R. and Plumbley, Mark D. and Grais, Emad M. and Roma, Gerard and Simpson, Andrew J. R. and Plumbley, Mark D.},
title = {Two-Stage Single-Channel Audio Source Separation Using Deep Neural Networks},
year = {2017},
issue_date = {September 2017},
publisher = {IEEE Press},
volume = {25},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2716443},
doi = {10.1109/TASLP.2017.2716443},
abstract = {Most single channel audio source separation approaches produce separated sources accompanied by interference from other sources and other distortions. To tackle this problem, we propose to separate the sources in two stages. In the first stage, the sources are separated from the mixed signal. In the second stage, the interference between the separated sources and the distortions are reduced using deep neural networks DNNs. We propose two methods that use DNNs to improve the quality of the separated sources in the second stage. In the first method, each separated source is improved individually using its own trained DNN, while in the second method all the separated sources are improved together using a single DNN. To further improve the quality of the separated sources, the DNNs in the second stage are trained discriminatively to further decrease the interference and the distortions of the separated sources. Our experimental results show that using two stages of separation improves the quality of the separated signals by decreasing the interference between the separated sources and distortions compared to separating the sources using a single stage of separation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1773–1783},
numpages = {11}
}

@article{10.1109/TASLP.2017.2711059,
author = {Saki, Fatemeh and Kehtarnavaz, Nasser and Saki, Fatemeh and Kehtarnavaz, Nasser},
title = {Real-Time Unsupervised Classification of Environmental Noise Signals},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2711059},
doi = {10.1109/TASLP.2017.2711059},
abstract = {This paper presents a real-time unsupervised classification of environmental noise signals without knowing the number of noise classes or clusters. A previously developed online frame-based clustering algorithm is modified by adding feature extraction, a smoothing step and a fading step. The developed unsupervised classification or clustering is examined in terms of purity of clusters and normalized mutual information. The results obtained for actual noise signals exhibit the effectiveness of the introduced unsupervised classification in terms of both classification outcome and computational efficiency.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1657–1667},
numpages = {11}
}

@article{10.1109/TASLP.2017.2705346,
author = {Pradhan, Somanath and Patel, Vinal and Somani, Dipen and George, Nithin V. and Pradhan, Somanath and Patel, Vinal and Somani, Dipen and George, Nithin V.},
title = {An Improved Proportionate Delayless Multiband-Structured Subband Adaptive Feedback Canceller for Digital Hearing Aids},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2705346},
doi = {10.1109/TASLP.2017.2705346},
abstract = {Acoustic feedback cancellation is one of the challenging tasks in the design of a behind the ear digital hearing aid. This feedback cancellation is usually achieved by using an adaptive filter. The finite correlation between the desired microphone input signal and the input signal to the loudspeaker results in a biased estimation of the adaptive filter, which may produce disturbances in the hearing aid. Prediction error method PEM has been used in literature to reduce the bias effects. The convergence of a PEM-based feedback canceller can be improved by implementing the adaptive filter in the subband domain. However, a direct subband implementation results in aliasing issues, band-edge problems, and introduces a delay due to analysis and synthesis filters. In order to reduce the aliasing and delay issues, a delayless subband implementation of a PEM-based feedback canceller is designed in this paper. A delayless multiband-structured subband implementation of the feedback canceller is also attempted to further reduce the aliasing and band-edge effects. This implementation aims at having all the subbands collectively updating the fullband adaptive filter, without the need for a subband to fullband weight conversion and offers improved feedback cancellation at reduced computational load in comparison with a delayless subband implementation of a PEM-based feedback canceller. In addition, an attempt has been made to further improve the convergence behavior by using an improved proportionate learning scheme. The improved convergence offered by the proposed scheme is evident from the simulation study. The improvement has been further quantified using a perceptual evaluation of speech quality and the proposed approach has been shown to provide enhanced speech quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1633–1643},
numpages = {11}
}

@article{10.1109/TASLP.2017.2699424,
author = {Stevens, Francis and Murphy, Damian T and Savioja, Lauri and Valimaki, Vesa and Stevens, Francis and Murphy, Damian T. and Savioja, Lauri and Valimaki, Vesa},
title = {Modeling Sparsely Reflecting Outdoor Acoustic Scenes Using the Waveguide Web},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2699424},
doi = {10.1109/TASLP.2017.2699424},
abstract = {Computer games and virtual reality require digital reverberation algorithms, which can simulate a broad range of acoustic spaces, including locations in the open air. Additionally, the detailed simulation of environmental sound is an area of significant interest due to the propagation of noise pollution over distances and its related impact on well-being, particularly in urban spaces. This paper introduces the waveguide web digital reverberator design for modeling the acoustics of sparsely reflecting outdoor environments; a design that is, in part, an extension of the scattering delay network reverberator. The design of the algorithm is based on a set of digital waveguides connected by scattering junctions at nodes that represent the reflection points of the environment under study. The structure of the proposed reverberator allows for accurate reproduction of reflections between discrete reflection points. Approximation errors are caused when the assumption of point-like nodes does not hold true. Three example cases are presented comparing waveguide web simulated impulse responses for a traditional shoebox room, a forest scenario, and an urban courtyard, with impulse responses created using other simulation methods or from real-world measurements. The waveguide web algorithm can better enable the acoustic simulation of outdoor spaces and so contribute toward sound design for virtual reality applications, gaming, and auralization, with a particular focus on acoustic design for the urban environment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1566–1578},
numpages = {13}
}

@article{10.1109/TASLP.2017.2702385,
author = {Elshamy, Samy and Madhu, Nilesh and Tirry, Wouter and Fingscheidt, Tim and Elshamy, Samy and Madhu, Nilesh and Tirry, Wouter and Fingscheidt, Tim},
title = {Instantaneous A Priori SNR Estimation by Cepstral Excitation Manipulation},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2702385},
doi = {10.1109/TASLP.2017.2702385},
abstract = {As the a priori signal-to-noise ratio SNR contains crucial information about a signal's mixture of speech and noise, its estimation is subject to steady research. In this paper, we introduce a novel a priori SNR estimator based on synthesizing an idealized excitation signal in the cepstral domain. Our approach utilizes a source-filter decomposition in combination with a cepstral excitation manipulation in order to recreate an idealized excitation, which is subsequently shaped by an immanent envelope. In contrast to the well-known decision-directed approach by Ephraim and Malah, an instantaneous estimate is obtained, which is less prone to sudden acoustic environmental changes and musical noise. Additionally, the proposed estimator is able to preserve weak harmonic structures resulting in a spectrum that is more full-bodied. We present both a speaker-independent and a speaker-dependent variant of the new a priori SNR estimator, both showing more than 2 dB ΔSNR improvement versus state of the art, without any significant increase in speech distortion.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1592–1605},
numpages = {14}
}

@article{10.1109/TASLP.2017.2696308,
author = {Schwartz, Ofer and Gannot, Sharon and Habets, Emanuel A. P. and Schwartz, Ofer and Gannot, Sharon and Habets, Emanuel A. P.},
title = {Cram\'{e}r-Rao Bound Analysis of Reverberation Level Estimators for Dereverberation and Noise Reduction},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2696308},
doi = {10.1109/TASLP.2017.2696308},
abstract = {The reverberation power spectral density PSD is often required for dereverberation and noise reduction algorithms. In this work, we compare two maximum likelihood ML estimators of the reverberation PSD in a noisy environment. In the first estimator, the direct path is first blocked. Then, the ML criterion for estimating the reverberation PSD is stated according to the probability density function of the blocking matrix BM outputs. In the second estimator, the speech component is not blocked. Instead, the ML criterion for estimating the speech and reverberation PSD is stated according to the probability density function of the microphone signals. To compare the expected mean square error MSE between the two ML estimators of the reverberation PSD, the Cram\'{e}r-Rao Bounds CRBs for the two ML estimators are derived. We show that the CRB for the joint reverberation and speech PSD estimator is lower than the CRB for estimating the reverberation PSD from the BM outputs. Experimental results show that the MSE of the two estimators indeed obeys the CRB curves. Experimental results of multimicrophone dereverberation and noise reduction algorithm show the benefits of using the ML estimators in comparison with another baseline estimators.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1680–1693},
numpages = {14}
}

@article{10.1109/TASLP.2017.2678164,
author = {Kaushik, Lakshmish and Sangwan, Abhijeet and Hansen, John H. L. and Kaushik, Lakshmish and Sangwan, Abhijeet and Hansen, John H. L.},
title = {Automatic Sentiment Detection in Naturalistic Audio},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2678164},
doi = {10.1109/TASLP.2017.2678164},
abstract = {Audio sentiment analysis using automatic speech recognition is an emerging research area where opinion or sentiment exhibited by a speaker is detected from natural audio. It is relatively underexplored when compared to text based sentiment detection. Extracting speaker sentiment from natural audio sources is a challenging problem. Generic methods for sentiment extraction generally use transcripts from a speech recognition system, and process the transcript using text-based sentiment classifiers. In this study, we show that this baseline system is suboptimal for audio sentiment extraction. Alternatively, new architecture using keyword spotting KWS is proposed for sentiment detection. In the new architecture, a text-based sentiment classifier is utilized to automatically determine the most useful and discriminative sentiment-bearing keyword terms, which are then used as a term list for KWS. In order to obtain a compact yet discriminative sentiment term list, iterative feature optimization for maximum entropy sentiment model is proposed to reduce model complexity while maintaining effective classification accuracy. A new hybrid ME-KWS joint scoring methodology is developed to model both text and audio based parameters in a single integrated formulation. For evaluation, two new databases are developed for audio based sentiment detection, namely, YouTube sentiment database and another newly developed corpus called UT-Opinion Opinion audio archive. These databases contain naturalistic opinionated audio collected in real-world conditions. The proposed solution is evaluated on audio obtained from videos in youtube.com and UT-Opinion corpus. Our experimental results show that the proposed KWS based system significantly outperforms the traditional ASR architecture in detecting sentiment for challenging practical tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1668–1679},
numpages = {12}
}

@article{10.1109/TASLP.2017.2714839,
author = {Chien, Yu-Ren and Mehta, Daryush D. and Guenason, Jon and Zanartu, Matias and Quatieri, Thomas F. and Yu-Ren Chien and Mehta, Daryush D. and Guenason, Jon and Zanartu, Matias and Quatieri, Thomas F.},
title = {Evaluation of Glottal Inverse Filtering Algorithms Using a Physiologically Based Articulatory Speech Synthesizer},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2714839},
doi = {10.1109/TASLP.2017.2714839},
abstract = {Glottal inverse filtering aims to estimate the glottal airflow signal from a speech signal for applications such as speaker recognition and clinical voice assessment. Nonetheless, evaluation of inverse filtering algorithms has been challenging due to the practical difficulties of directly measuring glottal airflow. Apart from this, it is acknowledged that the performance of many methods degrade in voice conditions that are of great interest, such as breathiness, high pitch, soft voice, and running speech. This paper presents a comprehensive, objective, and comparative evaluation of state-of-the-art inverse filtering algorithms that takes advantage of speech and glottal airflow signals generated by a physiological speech synthesizer. The synthesizer provides a physics-based simulation of the voice production process and thus an adequate test bed for revealing the temporal and spectral performance characteristics of each algorithm. Included in the synthetic data are continuous speech utterances and sustained vowels, which are produced with multiple voice qualities pressed, slightly pressed, modal, slightly breathy, and breathy, fundamental frequencies, and subglottal pressures to simulate the natural variations in real speech. In evaluating the accuracy of a glottal flow estimate, multiple error measures are used, including an error in the estimated signal that measures overall waveform deviation, as well as an error in each of several clinically relevant features extracted from the glottal flow estimate. Waveform errors calculated from glottal flow estimation experiments exhibited mean values around 30% for sustained vowels, and around 40% for continuous speech, of the amplitude of true glottal flow derivative. Closed-phase approaches showed remarkable stability across different voice qualities and subglottal pressures. The algorithms of choice, as suggested by significance tests, are closed-phase covariance analysis for the analysis of sustained vowels, and sparse linear prediction for the analysis of continuous speech. Results of data subset analysis suggest that analysis of close rounded vowels is an additional challenge in glottal flow estimation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1718–1730},
numpages = {13}
}

@article{10.1109/TASLP.2017.2714424,
author = {Khademi, Seyran and Hendriks, Richard C. and Kleijn, W. Bastiaan and Khademi, Seyran and Hendriks, Richard C. and Kleijn, W. Bastiaan},
title = {Intelligibility Enhancement Based on Mutual Information},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2714424},
doi = {10.1109/TASLP.2017.2714424},
abstract = {Speech intelligibility enhancement is considered for multiple-microphone acquisition and single loudspeaker rendering. This is based on the mutual information measured between the message spoken at far-end environment and the message perceived by a listener at near-end. We prove that the joint optimal processing can be decomposed into far-end and near-end processing. The former is a minimum variance distortionless response beamformer that reduces the noise in the talker environment and the latter is a post-filter that redistributes the power over the frequency bands. Disjoint processing is optimal provided that the post-filtering operation is aware of the residual noise from the beamforming operation. Our results show that both processing steps are necessary for the effective conveyance of a message and, importantly, that the second step must be aware of the remaining noise from the beamforming operation in the first step. In addition, we study the use of the mutual information applied on the perceptually more relevant powers per critical band.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1694–1708},
numpages = {15}
}

@article{10.1109/TASLP.2017.2703650,
author = {Pang, Cheng and Liu, Hong and Zhang, Jie and Li, Xiaofei and Cheng Pang and Hong Liu and Jie Zhang and Xiaofei Li},
title = {Binaural Sound Localization Based on Reverberation Weighting and Generalized Parametric Mapping},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2703650},
doi = {10.1109/TASLP.2017.2703650},
abstract = {Binaural sound source localization is an important technique for speech enhancement, video conferencing, and human-robot interaction, etc. However, in realistic scenarios, the reverberation and environmental noise would degrade the precision of sound direction estimation. Therefore, reliable sound localization is essential to practical applications. To deal with these disturbances, this paper presents a novel binaural sound source localization approach based on reverberation weighting and generalized parametric mapping. First, the reverberation weighting as a preprocessing stage, is used to separately suppress the early and late reverberation, while preserving interaural cues. Then, two binaural cues, i.e., interaural time and intensity differences, are extracted from the frequency-domain representations of dereverberated binaural signals for the online localization. Their corresponding templates are established using the training data. Furthermore, the generalized parametric mapping is proposed to build a generalized parametric model for describing relationships between azimuth and binaural cues analytically. Finally, a two-step sound localization process is introduced to refine azimuth estimation based on the generalized parametric model and template matching. Experiments in both simulated and real scenarios validate that the proposed method can achieve better localization performance compared to state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1618–1632},
numpages = {15}
}

@article{10.1109/TASLP.2017.2700945,
author = {Olivieri, Ferdinando and Fazi, Filippo Maria and Fontana, Simone and Menzies, Dylan and Nelson, Philip Arthur and Olivieri, Ferdinando and Fazi, Filippo Maria and Fontana, Simone and Menzies, Dylan and Nelson, Philip Arthur},
title = {Generation of Private Sound With a Circular Loudspeaker Array and the Weighted Pressure Matching Method},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2700945},
doi = {10.1109/TASLP.2017.2700945},
abstract = {In this paper, we propose a system for private sound which is based on the weighted pressure matching method WPMM. The aim is to design the input signals to an array of loudspeakers that allow for the synthesis of a target field defined with large amplitude variations between the so-called dark points and the listener's position. The system enables listeners to control the trade-off between directivity performance and the accuracy of reproduction of the target field at the listening position when the input energy to the array is limited. This is achieved by calculating the WPMM weight in the dark zone based on a constraint on the characteristics of the sound field in the listening zone. The system is validated for a number of predefined use-case scenarios. The results of the experiments in an anechoic environment with a circular array prototype show that listeners can control the performance trade-off in a wide frequency range. In the second part of the paper, algorithms are presented for the fast update of the input signals when the user selects a new value of the performance constraint.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1579–1591},
numpages = {13}
}

@article{10.1109/TASLP.2017.2709909,
author = {Drgas, Szymon and Virtanen, Tuomas and Lucke, Jorg and Hurmalainen, Antti and Drgas, Szymon and Virtanen, Tuomas and Lucke, Jorg and Hurmalainen, Antti},
title = {Binary Non-Negative Matrix Deconvolution for Audio Dictionary Learning},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2709909},
doi = {10.1109/TASLP.2017.2709909},
abstract = {In this study, we propose an unsupervised method for dictionary learning in audio signals. The new method, called binary nonnegative matrix deconvolution BNMD, is developed and used to discover patterns from magnitude-scale spectrograms. The BNMD models an audio spectrogram as a sum of delayed patterns having binary gains activations. Only small subsets of patterns can be active for a given spectrogram excerpt. The proposed method was applied to speaker identification and separation tasks. The experimental results show that dictionaries obtained by the BNMD bring much higher speaker identification accuracies averaged over a range of SNRs from -6 dB to 9 dB 91.3% than the NMD-based dictionaries 37.8-75.4%. The BNMD also gives a benefit over dictionaries obtained using vector quantization 87.8%. For bigger dictionaries the difference between the BNMD and the vector quantization VQ is getting smaller. For the speech separation task the BNMD dictionary gave a slight improvement over the VQ.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1644–1656},
numpages = {13}
}

@article{10.1109/TASLP.2017.2705280,
author = {Hatano, Yuta and Shi, Chuang and Kajikawa, Yoshinobu and Hatano, Yuta and Chuang Shi and Kajikawa, Yoshinobu},
title = {Compensation for Nonlinear Distortion of the Frequency Modulation-Based Parametric Array Loudspeaker},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2705280},
doi = {10.1109/TASLP.2017.2705280},
abstract = {The parametric array loudspeaker PAL modulates the audio signal on an ultrasonic carrier. When the modulated signal is transmitted in air, an audio beam is created based on the nonlinear acoustic principle. Each modulation method has advantages and disadvantages. The frequency modulation FM is favorable for its low cost and high volume, but the tradeoff is its complicated nonlinear distortion, which is difficult to be reduced. In this paper, the Volterra filter is adopted to model the nonlinearity of the FM-based PAL. A novel complex inverse system is devised to effectively reduce the nonlinear distortion. Three practical aspects are addressed. First, the computational complexity of the Volterra filter is reduced by the parallel cascade structure with almost no compromise to the model accuracy. Second, Volterra filters are identified at discrete input levels to treat the nonlinearity that keeps changing with the time-varying audio input. Third, when the input level is high, a separation approach is proposed to refine the identified Volterra filters, which eventually improves the performance of the proposed inverse system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1709–1717},
numpages = {9}
}

@article{10.1109/TASLP.2017.2703165,
author = {Alku, Paavo and Saeidi, Rahim and Alku, Paavo and Saeidi, Rahim},
title = {The Linear Predictive Modeling of Speech From Higher-Lag Autocorrelation Coefficients Applied to Noise-Robust Speaker Recognition},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2703165},
doi = {10.1109/TASLP.2017.2703165},
abstract = {A linear predictive spectral estimation method based on higher-lag autocorrelation coefficients is proposed for the noise-robust feature extraction from speech. The method, called higher-lag linear prediction, is derived from a signal prediction model that is optimized in the mean square sense using a cost function that has two prediction error terms, the first of which is similar to that of conventional linear prediction and the second of which is a delayed version introducing an integer delay of M samples. This basic form is developed further into the combined higher-lag linear prediction CHLLP model by simultaneously taking advantage of the zero-lag and higher-lag predictions. The CHLLP model was used in the computation of mel-frequency cepstral coefficients and compared with several reference feature extraction methods in speaker recognition. The experiments were conducted by using a modern i-vector-based system. Noise-corruption was done using both additive car, babble, and factory noise in different signal-to-noise ratio conditions as well as speech recordings from real noisy conditions. The results indicate that CHLLP outperformed the reference feature extraction methods in almost all the comparisons in the noise-corrupted conditions and the performance of CHLLP was only slightly inferior to the nonparametric FFT-based spectral modeling in the clean condition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1606–1617},
numpages = {12}
}

@article{10.1109/TASLP.2017.2698723,
author = {Lu, Liang and Renals, Steve and Liang Lu and Renals, Steve},
title = {Small-Footprint Highway Deep Neural Networks for Speech Recognition},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2698723},
doi = {10.1109/TASLP.2017.2698723},
abstract = {State-of-the-art speech recognition systems typically employ neural network acoustic models. However, compared to Gaussian mixture models, deep neural network DNN based acoustic models often have many more model parameters, making it challenging for them to be deployed on resource-constrained platforms, such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network HDNN for training small-footprint acoustic models. HDNNs are a depth-gated feedforward neural network, which include two types of gate functions to facilitate the information flow through different layers. Our study demonstrates that HDNNs are more compact than regular DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with many fewer model parameters. Furthermore, HDNNs are more controllable than DNNs: The gate functions of an HDNN can control the behavior of the whole network using a very small number of model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For example, simply updating the gate functions using adaptation data can result in considerable gains in accuracy. We demonstrate these aspects by experiments using the publicly available AMI corpus, which has around 80 h of training data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1502–1511},
numpages = {10}
}

@article{10.1109/TASLP.2017.2695718,
author = {Fakhry, Mahmoud and Svaizer, Piergiorgio and Omologo, Maurizio and Fakhry, Mahmoud and Svaizer, Piergiorgio and Omologo, Maurizio},
title = {Audio Source Separation in Reverberant Environments Using $\beta$-Divergence-Based Nonnegative Factorization},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2695718},
doi = {10.1109/TASLP.2017.2695718},
abstract = {In Gaussian model-based multichannel audio source separation, the likelihood of observed mixtures of source signals is parametrized by source spectral variances and by associated spatial covariance matrices. These parameters are estimated by maximizing the likelihood through an expectation-maximization algorithm and used to separate the signals by means of multichannel Wiener filtering. We propose to estimate these parameters by applying nonnegative factorization based on prior information on source variances. In the nonnegative factorization, spectral basis matrices can be defined as the prior information. The matrices can be either extracted or indirectly made available through a redundant library that is trained in advance. In a separate step, applying nonnegative tensor factorization, two algorithms are proposed in order to either extract or detect the basis matrices that best represent the power spectra of the source signals in the observed mixtures. The factorization is achieved by minimizing the β-divergence through multiplicative update rules. The sparsity of factorization can be controlled by tuning the value of β. Experiments show that sparsity, rather than the value assigned to β in the training, is crucial in order to increase the separation performance. The proposed method was evaluated in several mixing conditions. It provides better separation quality with respect to other comparable algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1462–1476},
numpages = {15}
}

@article{10.1109/TASLP.2017.2694708,
author = {Zeinali, Hossein and Sameti, Hossein and Burget, Lukas and Zeinali, Hossein and Sameti, Hossein and Burget, Lukas},
title = {HMM-Based Phrase-Independent i-Vector Extractor for Text-Dependent Speaker Verification},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2694708},
doi = {10.1109/TASLP.2017.2694708},
abstract = {The low-dimensional i-vector representation of speech segments is used in the state-of-the-art text-independent speaker verification systems. However, i-vectors were deemed unsuitable for the text-dependent task, where simpler and older speaker recognition approaches were found more effective. In this work, we propose a straightforward hidden Markov model HMM based extension of the i-vector approach, which allows i-vectors to be successfully applied to text-dependent speaker verification. In our approach, the Universal Background Model UBM for training phrase-independent i-vector extractor is based on a set of monophone HMMs instead of the standard Gaussian Mixture Model GMM. To compensate for the channel variability, we propose to precondition i-vectors using a regularized variant of within-class covariance normalization, which can be robustly estimated in a phrase-dependent fashion on the small datasets available for the text-dependent task. The verification scores are cosine similarities between the i-vectors normalized using phrase-dependent s-norm. The experimental results on RSR2015 and RedDots databases confirm the effectiveness of the proposed approach, especially in rejecting test utterances with a wrong phrase. A simple MFCC based i-vector/HMM system performs competitively when compared to very computationally expensive DNN-based approaches or the conventional relevance MAP GMM-UBM, which does not allow for compact speaker representations. To our knowledge, this paper presents the best published results obtained with a single system on both RSR2015 and RedDots dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1421–1435},
numpages = {15}
}

@article{10.1109/TASLP.2017.2700540,
author = {Wang, Yannan and Du, Jun and Dai, Li-Rong and Lee, Chin-Hui and Yannan Wang and Jun Du and Li-Rong Dai and Chin-Hui Lee},
title = {A Gender Mixture Detection Approach to Unsupervised Single-Channel Speech Separation Based on Deep Neural Networks},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2700540},
doi = {10.1109/TASLP.2017.2700540},
abstract = {We propose an unsupervised speech separation framework for mixtures of two unseen speakers in a single-channel setting based on deep neural networks DNNs. We rely on a key assumption that two speakers could be well segregated if they are not too similar to each other. A dissimilarity measure between two speakers is first proposed to characterize the separation ability between competing speakers. We then show that speakers with the same or different genders can often be separated if two speaker clusters, with large enough distances between them, for each gender group could be established, resulting in four speaker clusters. Next, a DNN-based gender mixture detection algorithm is proposed to determine whether the two speakers in the mixture are females, males, or from different genders. This detector is based on a newly proposed DNN architecture with four outputs, two of them representing the female speaker clusters and the other two characterizing the male groups. Finally, we propose to construct three independent speech separation DNN systems, one for each of the female-female, male-male, and female-male mixture situations. Each DNN gives dual outputs, one representing the target speaker group and the other characterizing the interfering speaker cluster. Trained and tested on the speech separation challenge corpus, our experimental results indicate that the proposed DNN-based approach achieves large performance gains over the state-of-the-art unsupervised techniques without using any specific knowledge about the mixed target and interfering speakers being segregated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1535–1546},
numpages = {12}
}

@article{10.1109/TASLP.2017.2693565,
author = {Chen, Yu-An and Wang, Ju-Chiang and Yang, Yi-Hsuan and Chen, Homer H. and Yu-An Chen and Ju-Chiang Wang and Yi-Hsuan Yang and Chen, Homer H.},
title = {Component Tying for Mixture Model Adaptation in Personalization of Music Emotion Recognition},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2693565},
doi = {10.1109/TASLP.2017.2693565},
abstract = {Personalizing a music emotion recognition model is needed because the perception of music emotion is highly subjective, but it is a time-consuming process. In this paper, we consider how to expedite the personalization process that begins with a general model trained offline using a general user base and progressively adapts the model to a music listener using the emotion annotations of the listener. Specifically, we focus on reducing the number of user annotations needed for the personalization. We investigate and evaluate four component tying methods: single group tying, quadrantwise tying, hierarchical tying, and random tying. These methods aim to exploit the available annotations by identifying related model parameters on-the-fly and updating them jointly. In the evaluation, we use the AMG1608 dataset, which contains the clip-level valence-arousal emotion ratings of 1608 30-s music clips annotated by 665 listeners. Also, we use the acoustic emotion Gaussians model as the general model that uses a mixture of Gaussian components to learn the mapping between the acoustic feature space and the emotion space. The results show that the model adaptation with component tying requires only 10-20 personal annotations to obtain the same level of prediction accuracy as the baseline model adaptation method that uses 50 personal annotations without component tying.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1409–1420},
numpages = {12}
}

@article{10.1109/TASLP.2017.2694699,
author = {Korpusik, Mandy and Glass, James and Korpusik, Mandy and Glass, James},
title = {Spoken Language Understanding for a Nutrition Dialogue System},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2694699},
doi = {10.1109/TASLP.2017.2694699},
abstract = {Food logging is recommended by dieticians for prevention and treatment of obesity, but currently available mobile applications for diet tracking are often too difficult and time-consuming for patients to use regularly. For this reason, we propose a novel approach to food journaling that uses speech and language understanding technology in order to enable efficient self-assessment of energy and nutrient consumption. This paper presents ongoing language understanding experiments conducted as part of a larger effort to create a nutrition dialogue system that automatically extracts food concepts from a user's spoken meal description. We first summarize the data collection and annotation of food descriptions performed via Amazon Mechanical Turk AMT, for both a written corpus and spoken data from an in-domain speech recognizer. We show that the addition of word vector features improves conditional random field CRF performance for semantic tagging of food concepts, achieving an average F1 test score of 92.4 on written data; we also demonstrate that a convolutional neural network CNN with no hand-crafted features outperforms the best CRF on spoken data, achieving an F1 test score of 91.3. We illustrate two methods for associating foods with properties: segmenting meal descriptions with a CRF, and a complementary method that directly predicts associations with a feed-forward neural network. Finally, we conduct an end-to-end system evaluation through an AMT user study with worker ratings of 83% semantic tagging accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1450–1461},
numpages = {12}
}

@article{10.1109/TASLP.2017.2699326,
author = {Kodrasi, Ina and Doclo, Simon and Kodrasi, Ina and Doclo, Simon},
title = {Signal-Dependent Penalty Functions for Robust Acoustic Multi-Channel Equalization},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2699326},
doi = {10.1109/TASLP.2017.2699326},
abstract = {Acoustic multi-channel equalization techniques, which aim to achieve dereverberation by reshaping the room impulse responses RIRs between the source and the microphone array, are known to be highly sensitive to RIR perturbations. In order to increase the robustness against RIR perturbations, several signal-independent methods have been proposed, which only rely on the available perturbed RIRs and do not incorporate any knowledge about the output signal. This paper presents a novel signal-dependent method to increase the robustness of equalization techniques by enforcing the output signal to exhibit spectrotemporal characteristics of a clean speech signal. Motivated by the sparse nature of clean speech, we propose to extend the cost function of state-of-the-art least squares equalization techniques, i.e., the multiple-input/output inverse theorem MINT, relaxed multi-channel least squares RMCLS, and partial multi-channel equalization based on MINT PMINT, with a signal-dependent penalty function promoting sparsity of the output signal in the short-time Fourier transform domain. Three conventionally used sparsity-promoting penalty functions are investigated, i.e., the l0-norm, the l1-norm, and the weighted l1-norm, and the sparsitypromoting reshaping filters are iteratively computed using the alternating direction method of multipliers. Simulation results for several acoustic systems and RIR perturbations demonstrate that incorporating sparsity-promoting penalty functions significantly increases the robustness of MINT, RMCLS, and PMINT, with the weighted l1-norm typically outperforming the l0-norm and the l1-norm. Furthermore, it is shown that the weighted l1-norm sparsity-promoting PMINT technique outperforms the other sparsity-promoting techniques in terms of perceptual speech quality. Finally, it is shown that the signal-dependent weighted l1-norm sparsity-promoting PMINT technique yields a similar or better dereverberation performance than the signal-independent regularized PMINT technique, confirming the advantage of using signal-dependent penalty functions for robust dereverberation filter design.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1512–1525},
numpages = {14}
}

@article{10.1109/TASLP.2017.2696310,
author = {Laufer-Goldshtein, Bracha and Talmon, Ronen and Gannot, Sharon and Laufer-Goldshtein, Bracha and Talmon, Ronen and Gannot, Sharon},
title = {Semi-Supervised Source Localization on Multiple Manifolds With Distributed Microphones},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2696310},
doi = {10.1109/TASLP.2017.2696310},
abstract = {The problem of single-source localization with ad hoc microphone networks in noisy and reverberant enclosures is addressed in this paper. A training set is formed by prerecorded measurements collected in advance and consists of a limited number of labelled measurements, attached with corresponding positions, and a larger number of unlabelled measurements from unknown locations. Further information about the enclosure characteristics or the microphone positions is not required. We propose a Bayesian inference approach for estimating a function that maps measurement-based features to the corresponding positions. The signals measured by the microphones represent different viewpoints, which are combined in a unified statistical framework. For this purpose, the mapping function is modelled by a Gaussian process with a covariance function that encapsulates both the connections between pairs of microphones and the relations among the samples in the training set. The parameters of the process are estimated by optimizing a maximum likelihood criterion. In addition, a recursive adaptation mechanism is derived, where the new streaming measurements are used to update the model. Performance is demonstrated for both simulated data and real-life recordings in a variety of reverberation and noise levels.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1477–1491},
numpages = {15}
}

@article{10.1109/TASLP.2017.2696307,
author = {Williamson, Donald S. and Wang, DeLiang and Williamson, Donald S. and DeLiang Wang},
title = {Time-Frequency Masking in the Complex Domain for Speech Dereverberation and Denoising},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2696307},
doi = {10.1109/TASLP.2017.2696307},
abstract = {In real-world situations, speech is masked by both background noise and reverberation, which negatively affect perceptual quality and intelligibility. In this paper, we address monaural speech separation in reverberant and noisy environments. We perform dereverberation and denoising using supervised learning with a deep neural network. Specifically, we enhance the magnitude and phase by performing separation with an estimate of the complex ideal ratio mask. We define the complex ideal ratio mask so that direct speech results after the mask is applied to reverberant and noisy speech. Our approach is evaluated using simulated and real room impulse responses, and with background noises. The proposed approach improves objective speech quality and intelligibility significantly. Evaluations and comparisons show that it outperforms related methods in many reverberant and noisy environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1492–1501},
numpages = {10}
}

@article{10.1109/TASLP.2017.2700940,
author = {Vairetti, Giacomo and De Sena, Enzo and Catrysse, Michael and Jensen, Soren Holdt and Moonen, Marc and van Waterschoot, Toon and Vairetti, Giacomo and De Sena, Enzo and Catrysse, Michael and Jensen, Soren Holdt and Moonen, Marc and van Waterschoot, Toon},
title = {A Scalable Algorithm for Physically Motivated and Sparse Approximation of Room Impulse Responses With Orthonormal Basis Functions},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2700940},
doi = {10.1109/TASLP.2017.2700940},
abstract = {Parametric modeling of room acoustics aims at representing room transfer functions by means of digital filters and finds application in many acoustic signal enhancement algorithms. In previous work by other authors, the use of orthonormal basis functions OBFs for modeling room acoustics has been proposed. Some advantages of OBF models over all-zero and pole-zero models have been illustrated, mainly focusing on the fact that OBF models typically require less model parameters to provide the same model accuracy. In this paper, it is shown that the orthogonality of the OBF model brings several additional advantages, which can be exploited if a suitable algorithm for identifying the OBF model parameters is applied. Specifically, the orthogonality of OBF models does not only lead to improved model efficiency as pointed out in previous work, but also leads to improved model scalability and model stability. Its appealing scalability property derives from a previously unexplored interpretation of the OBF model as an approximation to a solution of the inhomogeneous acoustic wave equation. Following this interpretation, a novel identification algorithm is proposed that takes advantage of the OBF model orthogonality to deliver efficient, scalable, and stable OBF model estimates, which is not necessarily the case for nonlinear estimation techniques that are normally applied.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1547–1561},
numpages = {15}
}

@article{10.1109/TASLP.2017.2699325,
author = {Kim, Jung-Hee and Kim, Jin and Jeon, Jae Hyeon and Nam, Sang Won and Jung-Hee Kim and Jin Kim and Jae Hyeon Jeon and Sang Won Nam},
title = {Delayless Individual-Weighting-Factors Sign Subband Adaptive Filter With Band-Dependent Variable Step-Sizes},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2699325},
doi = {10.1109/TASLP.2017.2699325},
abstract = {Sign subband adaptive filter algorithms with individual weighting factors IWF-SSAF were recently proposed to achieve improved convergence performance in impulsive noise environments by fully utilizing the decorrelating property of subband adaptive filters. However, such sign subband adaptive filter SSAF algorithms may have an inherent signal path delay problem for real-time applications, and thus, can be restrictively applied. In this paper, a delayless IWF-SSAF with band-dependent variable step-sizes BD-VSS, robust in impulsive noise environments, is proposed for real-time applications, whereby two delayless filter structures developed for the ℓ2 -norm-based SAF are employed along with the IWF-SSAF. In particular, a BD-VSS algorithm is also introduced for better convergence by applying the recent ℓ1-norm minimization technique to respective subband. Finally, the performance of the proposed delayless IWF-SSAF with BD-VSS is verified in various impulsive interference environments such as system identification with impulsive noise, acoustic echo cancellation in a double-talk scenario, and active impulsive noise control.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1526–1534},
numpages = {9}
}

@article{10.1109/TASLP.2017.2694704,
author = {Xu, Xinzhou and Deng, Jun and Cummins, Nicholas and Zhang, Zixing and Wu, Chen and Zhao, Li and Schuller, Bjorn and Xinzhou Xu and Jun Deng and Cummins, Nicholas and Zixing Zhang and Chen Wu and Li Zhao and Schuller, Bjorn},
title = {A Two-Dimensional Framework of Multiple Kernel Subspace Learning for Recognizing Emotion in Speech},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Press},
volume = {25},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2694704},
doi = {10.1109/TASLP.2017.2694704},
abstract = {As a highly active topic in computational paralinguistics, speech emotion recognition SER aims to explore ideal representations for emotional factors in speech. In order to improve the performance of SER, multiple kernel learning MKL dimensionality reduction has been utilized to obtain effective information for recognizing emotions. However, the solution of MKL usually provides only one nonnegative mapping direction for multiple kernels; this may lead to loss of valuable information. To address this issue, we propose a two-dimensional framework for multiple kernel subspace learning. This framework provides more linear combinations on the basis of MKL without nonnegative constraints, which preserves more information in the learning procedures. It also leverages both of MKL and two-dimensional subspace learning, combining them into a unified structure. To apply the framework to SER, we also propose an algorithm, namely generalised multiple kernel discriminant analysis GMKDA, by employing discriminant embedding graphs in this framework. GMKDA takes advantage of the additional mapping directions for multiple kernels in the proposed framework. In order to evaluate the performance of the proposed algorithm a wide range of experiments is carried out on several key emotional corpora. These experimental results demonstrate that the proposed methods can achieve better performance compared with some conventional and subspace learning methods in dealing with SER.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1436–1449},
numpages = {14}
}

@article{10.1109/TASLP.2017.2699334,
author = {Richard, G. and Virtanen, T. and Bello, J. P. and Ono, N. and Glotin, H.},
title = {Introduction to the Special Section on Sound Scene and Event Analysis},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2699334},
doi = {10.1109/TASLP.2017.2699334},
abstract = {The papers in this special section are devoted to the growing field of acoustic scene classification and acoustic event recognition. Machine listening systems still have difficulties to reach the ability of human listeners in the analysis of realistic acoustic scenes. If sustained research efforts have been made for decades in speech recognition, speaker identification and to a lesser extent in music information retrieval, the analysis of other types of sounds, such as environmental sounds, is the subject of growing interest from the community and is targeting an ever increasing set of audio categories. This problem appears to be particularly challenging due to the large variety of potential sound sources in the scene, which may in addition have highly different acoustic characteristics, especially in bioacoustics. Furthermore, in realistic environments, multiple sources are often present simultaneously, and in reverberant conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1169–1171},
numpages = {3}
}

@article{10.1109/TASLP.2017.2690565,
author = {Stowell, Dan and Benetos, Emmanouil and Gill, Lisa F. and Stowell, Dan and Benetos, Emmanouil and Gill, Lisa F.},
title = {On-Bird Sound Recordings: Automatic Acoustic Recognition of Activities and Contexts},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690565},
doi = {10.1109/TASLP.2017.2690565},
abstract = {We introduce a novel approach to studying animal behavior and the context in which it occurs, through the use of microphone backpacks carried on the backs of individual free-flying birds. These sensors are increasingly used by animal behavior researchers to study individual vocalizations of freely behaving animals, even in the field. However, such devices may record more than an animal's vocal behavior, and have the potential to be used for investigating specific activities movement and context background within which vocalizations occur. To facilitate this approach, we investigate the automatic annotation of such recordings through two different sound scene analysis paradigms: A scene-classification method using feature learning, and an event-detection method using probabilistic latent component analysis. We analyze recordings made with Eurasian jackdaws Corvus monedula in both captive and field settings. Results are comparable with the state of the art in sound scene analysis; we find that the current recognition quality level enables scalable automatic annotation of audio logger data, given partial annotation, but also find that individual differences between animals and/or their backpacks limit the generalization from one individual to another. we consider the interrelation of “scenes” and “events” in this particular task, and issues of temporal resolution.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1193–1206},
numpages = {14}
}

@article{10.1109/TASLP.2016.2625423,
author = {Jameel, Abu Shafin Mohammad Mahdee and Fattah, Shaikh Anowarul and Goswami, Rajib and Zhu, Wei-Ping and Ahmad, M. Omair and Jameel, Abu Shafin Mohammad Mahdee and Fattah, Shaikh Anowarul and Goswami, Rajib and Wei-Ping Zhu and Ahmad, M. Omair},
title = {Noise Robust Formant Frequency Estimation Method Based on Spectral Model of Repeated Autocorrelation of Speech},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2625423},
doi = {10.1109/TASLP.2016.2625423},
abstract = {In this paper, a noise robust formant frequency estimation scheme is developed based on a spectral model matching algorithm. Considering the vocal tract as an autoregressive system, a spectral model of repeated autocorrelation function RACF of band-limited speech signal is proposed. It is shown that because of the repeated autocorrelation operation on band-limited signal, the proposed model can exhibit prominent formant characteristics. First from given noisy speech observations, an adaptive band selection criterion is developed. Next, on each resulting band-limited noisy speech signal, a repeated autocorrelation operation is carried out, which not only reduces the effect of noise but also strengthens the dominant poles corresponding to the formant frequencies. Finally, spectrum of the RACF is computed and instead of direct spectral peak picking, a model fitting scheme is introduced to find out model parameters which lead to formant estimation. The proposed algorithm has been tested on natural vowels as well as some naturally spoken sentences in the presence of different environmental noises. It is found that the proposed scheme provides better formant estimation accuracy in comparison to some of the existing methods at low levels of signal-to-noise ratio.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1357–1370},
numpages = {14}
}

@article{10.1109/TASLP.2017.2690561,
author = {Rakotomamonjy, Alain and Rakotomamonjy, Alain},
title = {Supervised Representation Learning for Audio Scene Classification},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690561},
doi = {10.1109/TASLP.2017.2690561},
abstract = {This paper investigates the use of supervised feature learning approaches for extracting relevant and discriminative features from acoustic scene recordings. Owing to the recent release of open datasets for acoustic scene classification problems, representation learning techniques can now be envisioned for solving the problem of feature extraction. This paper makes a step toward this goal by first introducing a supervised nonnegative matrix factorization SNMF. Our goal through this SNMF is to induce the matrix decomposition to carry out discriminative information in addition to the usual generative ones. We achieve this objective by augmenting the nonnegative matrix factorization optimization problem with a novel loss function related to class labels of each column of the matrix to decompose. While the scale of the datasets available is still small compared to those available in computer vision, we have studied models based on convolutional neural networks. We have analyzed the performances of these models on the DCASE-16 dataset and a corrected version of the LITIS Rouen one. Our experiments show that despite the small-scale setting, supervised feature learning is favorably competitive compared to the current state-of-the-art features. We also point out that for smaller scale dataset, SNMF is indeed slightly less prone to overfitting than convolutional neural networks. While the performances of these learned features are interesting per se, a deeper analysis of their behavior in the acoustic scene problem context raises open and difficult questions that we believe, need to be addressed for further performance breakthroughs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1253–1265},
numpages = {13}
}

@article{10.1109/TASLP.2017.2690575,
author = {Cakir, Emre and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas and Cakir, Emre and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
title = {Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690575},
doi = {10.1109/TASLP.2017.2690575},
abstract = {Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks CNNs are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks RNNs are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a convolutional recurrent neural network CRNN and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1291–1303},
numpages = {13}
}

@article{10.1109/TASLP.2017.2690573,
author = {Trowitzsch, Ivo and Mohr, Johannes and Kashef, Youssef and Obermayer, Klaus and Trowitzsch, Ivo and Mohr, Johannes and Kashef, Youssef and Obermayer, Klaus},
title = {Robust Detection of Environmental Sounds in Binaural Auditory Scenes},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690573},
doi = {10.1109/TASLP.2017.2690573},
abstract = {In realistic acoustic scenes, the detection of particular types of environmental sounds is often impeded by the simultaneous presence of multiple sound sources. In this work, we use simulations to systematically investigate the impact of superimposed distractor sources on sound type classification in a binaural robotic system and suggest techniques for increasing the robustness under such conditions. First, we demonstrate that by superimposing target sounds with strongly varying general environmental sounds during training, sound type classifiers are less affected by the presence of a distractor source. Moreover, we show that generalization performance of such models depends on how similar the angular source configuration and the signal-to-noise ratio are to the conditions under which the models were trained. Based on these results, we demonstrate how robust models can be obtained by including a variety of different conditions in the training data, a procedure called multi-conditional training. We evaluate this technique training both with ambient sources as well as with point sources with varying angular configurations, and show that this is an effective approach to produce models with close-to-optimal performance under a wide range of conditions. Moreover, we investigate the impact of head orientation and find that it has a significant influence on classification performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1344–1356},
numpages = {13}
}

@article{10.1109/TASLP.2017.2692304,
author = {Li, Na and Mak, Man-Wai and Chien, Jen-Tzung and Na Li and Man-Wai Mak and Jen-Tzung Chien},
title = {DNN-Driven Mixture of PLDA for Robust Speaker Verification},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2692304},
doi = {10.1109/TASLP.2017.2692304},
abstract = {The mismatch between enrollment and test utterances due to different types of variabilities is a great challenge in speaker verification. Based on the observation that the SNR-level variability or channel-type variability causes heterogeneous clusters in i-vector space, this paper proposes to apply supervised learning to drive or guide the learning of probabilistic linear discriminant analysis PLDA mixture models. Specifically, a deep neural network DNN is trained to produce the posterior probabilities of different SNR levels or channel types given i-vectors as input. These posteriors then replace the posterior probabilities of indicator variables in the mixture of PLDA. The discriminative training causes the mixture model to perform more reasonable soft divisions of the i-vector space as compared to the conventional mixture of PLDA. During verification, given a test i-vector and a target-speaker's i-vector, the marginal likelihood for the same-speaker hypothesis is obtained by summing the component likelihoods weighted by the component posteriors produced by the DNN, and likewise for the different-speaker hypothesis. Results based on NIST 2012 SRE demonstrate that the proposed scheme leads to better performance under more realistic situations where both training and test utterances cover a wide range of SNRs and different channel types. Unlike the previous SNR-dependent mixture of PLDA which only focuses on SNR mismatch, the proposed model is more general and is potentially applicable to addressing different types of variability in speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1371–1383},
numpages = {13}
}

@article{10.1109/TASLP.2017.2690579,
author = {Sanchez-Hevia, Hector A. and Ayllon, David and Gil-Pita, Roberto and Rosa-Zurera, Manuel and Sanchez-Hevia, Hector A. and Ayllon, David and Gil-Pita, Roberto and Rosa-Zurera, Manuel},
title = {Maximum Likelihood Decision Fusion for Weapon Classification in Wireless Acoustic Sensor Networks},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690579},
doi = {10.1109/TASLP.2017.2690579},
abstract = {Gunshot acoustic analysis is a field with many practical applications, but due to the multitude of factors involved in the generation of the acoustic signature of firearms, it is not a trivial task. The main problem arises with the strong spatial dependence shown by the recorded waveforms even when dealing with the same weapon. However, this can be lessen by using a spatially diverse receiver such as a wireless acoustic sensor network. In this work, we address multichannel acoustic weapon classification using spatial information and a novel decision fusion rule based on it. We propose a fusion rule based on maximum likelihood estimation that takes advantage of diverse classifier ensembles to improve upon classic decision fusion techniques. Classifier diversity comes from a spatial segmentation that is performed locally at each node. The same segmentation is also used to improve the accuracy of the local classification by means of a divide and conquer approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1172–1182},
numpages = {11}
}

@article{10.1109/TASLP.2017.2690574,
author = {Grzeszick, Rene and Plinge, Axel and Fink, Gernot A. and Grzeszick, Rene and Plinge, Axel and Fink, Gernot A.},
title = {Bag-of-Features Methods for Acoustic Event Detection and Classification},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690574},
doi = {10.1109/TASLP.2017.2690574},
abstract = {The detection and classification of acoustic events in various environments is an important task. Its applications range from multimedia analysis to surveillance of humans or even animal life. Several of these tasks require the capability of online processing. Besides many approaches that tackle the task of acoustic event detection, methods that are based on the well known bag-of-features principle also emerged into the field. Acoustic features are calculated for all frames in a given time window. Then, applying the bag-of-features concept, these features are quantized with respect to a learned codebook and a histogram representation is computed. Bag-of-features approaches are particularly interesting for online processing as they have a low computational cost. In this paper, the bag-of-features principle and various extensions are reviewed, including soft quantization, supervised codebook learning, and temporal modeling. Furthermore, Mel and Gammatone frequency cepstral coefficients that originate from psychoacoustic models are used as the underlying feature set for the bag-of-features. The possibility of fusing the results of multiple channels in order to improve the robustness is shown. Two databases are used for the experiments: The DCASE 2013 office live dataset and the ITC-IRST multichannel dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1242–1252},
numpages = {11}
}

@article{10.1109/TASLP.2017.2690559,
author = {Imoto, Keisuke and Ono, Nobutaka and Imoto, Keisuke and Ono, Nobutaka},
title = {Spatial Cepstrum as a Spatial Feature Using a Distributed Microphone Array for Acoustic Scene Analysis},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690559},
doi = {10.1109/TASLP.2017.2690559},
abstract = {In this paper, with the aim of using the spatial information obtained from a distributed microphone array employed for acoustic scene analysis, we propose a robust and efficient method, which is called the spatial cepstrum. In our approach, similarly to the cepstrum, which is widely used as a spectral feature, the logarithm of the amplitude in multichannel observation is converted to a feature vector by a linear orthogonal transformation. This linear orthogonal transformation is achieved by principal component analysis PCA in general. Moreover, we also show that for a circularly symmetric microphone arrangement with an isotropic sound field, PCA is identical to the inverse discrete Fourier transform and the spatial cepstrum exactly corresponds to the cepstrum. The proposed approach does not require the positions of the microphones and is robust against the synchronization mismatch of channels, thus ensuring its suitability for use with a distributed microphone array. Experimental results obtained using actual environmental sounds verify the validity of our approach even when a smaller feature dimension than the original one is used, which is achieved by dimensionality reduction through PCA. Additionally, experimental results also indicate that the robustness of the proposed method is satisfactory for observations that have the synchronization mismatch of channels.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1335–1343},
numpages = {9}
}

@article{10.1109/TASLP.2017.2690562,
author = {Koluguri, Nithin Rao and Meenakshi, G. Nisha and Ghosh, Prasanta Kumar and Koluguri, Nithin Rao and Meenakshi, G. Nisha and Ghosh, Prasanta Kumar},
title = {Spectrogram Enhancement Using Multiple Window Savitzky-Golay MWSG Filter for Robust Bird Sound Detection},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690562},
doi = {10.1109/TASLP.2017.2690562},
abstract = {Bird sound detection from real-field recordings is essential for identifying bird species in bioacoustic monitoring. Variations in the recording devices, environmental conditions, and the presence of vocalizations from other animals make the bird sound detection very challenging. In order to overcome these challenges, we propose an unsupervised algorithm comprising two main stages. In the first stage, a spectrogram enhancement technique is proposed using a multiple window Savitzky-Golay MWSG filter. We show that the spectrogram estimate using MWSG filter is unbiased and has lower variance compared with its single window counterpart. It is known that bird sounds are highly structured in the time-frequency T-F plane. We exploit these cues of prominence of T-F activity in specific directions from the enhanced spectrogram, in the second stage of the proposed method, for bird sound detection. In this regard, we use a set of four moving average filters that when applied to the enhanced spectrogram, yield directional spectrograms that capture the direction specific information. We propose a thresholding scheme on the time varying energy profile computed from each of these directional spectrograms to obtain frame-level binary decisions of bird sound activity. These individual decisions are then combined to obtain the final decision. Experiments are performed with three different datasets, with varying recording and noise conditions. Frame level F-score is used as the evaluation metric for bird sound detection. We find that the proposed method, on average, achieves higher F-score $10.24%$ relative compared to the best of the six baseline schemes considered in this work.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1183–1192},
numpages = {10}
}

@article{10.1109/TASLP.2017.2690558,
author = {Yang, Wenjun and Krishnan, Sridhar and Wenjun Yang and Krishnan, Sridhar},
title = {Combining Temporal Features by Local Binary Pattern for Acoustic Scene Classification},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690558},
doi = {10.1109/TASLP.2017.2690558},
abstract = {The popular frequency-domain features Mel-frequency cepstral coefficients MFCCs have been widely used for the task of acoustic scene classification ASC. The MFCC feature vector describes only the power spectral envelope of a single frame, but it seems like environmental audio signal would benefit from information in the temporal dynamics. However, the classic approach of integrating them would lose this important information. Here, we adopt local binary pattern LBP as a tool to characterize the latent information on the temporal dynamics. The frame-level MFCC features are viewed as a 2-D image, where we use LBP to encode the evolution process. Besides, some complementary spectral features such as spectral centroid SC, spectral bandwidth SBW is utilized to further improve the ASC performance. The proposed features are then fed into an ensemble classifier called D3C for recognizing environmental sounds. The results show that the proposed method was able to achieve a classification improvement of 8% compared to the baseline system. Our work presented a new method for combing the temporal features, demonstrating the significance of the temporal evolution features for characterizing the environmental sound.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1315–1321},
numpages = {7}
}

@article{10.1109/TASLP.2017.2693566,
author = {Wu, Kai and Reju, Vaninirappuputhenpurayil Gopalan and Khong, Andy W. H. and Goh, Shu Ting and Kai Wu and Reju, Vaninirappuputhenpurayil Gopalan and Khong, Andy W. H. and Shu Ting Goh},
title = {Swarm Intelligence Based Particle Filter for Alternating Talker Localization and Tracking Using Microphone Arrays},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2693566},
doi = {10.1109/TASLP.2017.2693566},
abstract = {We address the problem of localizing and tracking alternating moving or stationary talkers using microphone arrays in a room environment. One of the main challenges is the frequent and possibly abrupt change of talker positions, which requires the algorithm to capture the active talker rapidly. In addition, the presence of interference, background noise, and room reverberation degrades the tracking performance. We propose a new algorithm that jointly exploits the advantages of the particle filter PF and particle swarm intelligence. The PF is used as a general tracking framework, which incorporates a proposed alternating source-dynamic model for recursive estimation of talker position. Unlike the conventional PF, where particles operate independently in the particle sampling stage, the use of swarm intelligence allows particles to interact with each other, thereby improving convergence toward the active talker location. In addition, the memory mechanism in swarm intelligence allows particles to remain at their previous best-fit state estimate when signals are corrupted by interference, noise, and/or reverberation. Simulations and experiments were conducted to demonstrate the effectiveness of the proposed algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1384–1397},
numpages = {14}
}

@article{10.1109/TASLP.2017.2690567,
author = {Carroll, Brandon T. and Whitaker, Bradley M. and Dayley, Wayne and Anderson, David V. and Carroll, Brandon T. and Whitaker, Bradley M. and Dayley, Wayne and Anderson, David V.},
title = {Outlier Learning via Augmented Frozen Dictionaries},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690567},
doi = {10.1109/TASLP.2017.2690567},
abstract = {A frozen dictionary learning method is proposed to automatically separate the aspects of environmental audio that are different between normal data and anomalous data. The approach involves learning a dictionary-based sparse representation of the normal data, then freezing this portion of the dictionary while an added portion of the dictionary is learned on the anomalous data. Two dictionary learning algorithms are modified to allow training some elements while holding others constant. Both algorithms demonstrate the ability to separate the anomalies for two sets of test data. One set is chicken recordings with human crowd noise anomalies artificially added at -6 dB. The other set consists of recordings of chickens that are healthy and sick. Both dictionary methods are able to identify data anomalies with high accuracy and precision.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1207–1215},
numpages = {9}
}

@article{10.1109/TASLP.2017.2690568,
author = {Dov, David and Talmon, Ronen and Cohen, Israel and Dov, David and Talmon, Ronen and Cohen, Israel},
title = {Multimodal Kernel Method for Activity Detection of Sound Sources},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690568},
doi = {10.1109/TASLP.2017.2690568},
abstract = {We consider the problem of acoustic scene analysis of multiple sound sources. In our setting, the sound sources are measured by a single microphone, and a particular source of interest is also captured by a video camera during a short time interval. The goal in this paper is to detect the activity of the source of interest even when the video data are missing, while ignoring the other sound sources. To address this problem, we propose a kernel-based algorithm that incorporates the audio-visual data by a combination of affinity kernels, constructed separately from the audio and the video data. We introduce a distance measure between data points that is associated with the source of interest, while reducing the effect of the other interfering sources. Using this distance, we devise a measure for the presence of the source of interest, which is naturally extended to time intervals, in which only the audio signal is available. Experimental results demonstrate the improved performance of the proposed algorithm compared to competing approaches implying the significance of the video signal in the analysis of complex acoustic scenes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1322–1334},
numpages = {13}
}

@article{10.1109/TASLP.2017.2690564,
author = {Phan, Huy and Hertel, Lars and Maass, Marco and Koch, Philipp and Mazur, Radoslaw and Mertins, Alfred and Huy Phan and Hertel, Lars and Maass, Marco and Koch, Philipp and Mazur, Radoslaw and Mertins, Alfred},
title = {Improved Audio Scene Classification Based on Label-Tree Embeddings and Convolutional Neural Networks},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690564},
doi = {10.1109/TASLP.2017.2690564},
abstract = {In this paper, we present an efficient approach for audio scene classification. We aim at learning representations for scene examples by exploring the structure of their class labels. A category taxonomy is automatically learned by collectively optimizing a tree-structured clustering of the given labels into multiple metaclasses. A scene recording is then transformed into a label-tree embedding image. Elements of the image represent the likelihoods that the scene instance belongs to the metaclasses. We investigate classification with label-tree embedding features learned from different low-level features as well as their fusion. We show that the combination of multiple features is essential to obtain good performance. While averaging label-tree embedding images over time yields good performance, we argue that average pooling possesses an intrinsic shortcoming. We alternatively propose an improved classification scheme to bypass this limitation. We aim at automatically learning common templates that are useful for the classification task from these images using simple but tailored convolutional neural networks. The trained networks are then employed as a feature extractor that matches the learned templates across a label-tree embedding image and produce the maximum matching scores as features for classification. Since audio scenes exhibit rich content, template learning and matching on low-level features would be inefficient. With label-tree embedding features, we have quantized and reduced the low-level features into the likelihoods of the metaclasses, on which the template learning and matching are efficient. We study both training convolutional neural networks on stacked label-tree embedding images and multistream networks. Experimental results on the DCASE2016 and LITIS Rouen datasets demonstrate the efficiency of the proposed methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1278–1290},
numpages = {13}
}

@article{10.1109/TASLP.2017.2690569,
author = {Schroder, Jens and Moritz, Niko and Anemuller, Jorn and Goetze, Stefan and Kollmeier, Birger and Schr\"{o}der, Jens and Moritz, Niko and Anem\"{u}ller, J\"{o}rn and Goetze, Stefan and Kollmeier, Birger},
title = {Classifier Architectures for Acoustic Scenes and Events: Implications for DNNs, TDNNs, and Perceptual Features from DCASE 2016},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690569},
doi = {10.1109/TASLP.2017.2690569},
abstract = {This paper evaluates neural network NN based systems and compares them to Gaussian mixture model GMM and hidden Markov model HMM approaches for acoustic scene classification SC and polyphonic acoustic event detection AED that are applied to data of the “Detection and Classification of Acoustic Scenes and Events 2016” DCASE'16 challenge, task 1 and task 3, respectively. For both tasks, the use of deep neural networks DNNs and features based on an amplitude modulation filterbank and a Gabor filterbank GFB are evaluated and compared to standard approaches. For SC, additionally a time-delay NN approach is proposed that enables analysis of long contextual information similar to recurrent NNs but with training efforts comparable to conventional DNNs. The SC system proposed for task 1 of the DCASE'16 challenge attains a recognition accuracy of 77.5%, which is 5.6% higher compared to the DCASE'16 baseline system. For the AED task, DNNs are adopted in tandem and hybrid approaches, i.e., as part of HMM-based systems. These systems are evaluated for the polyphonic data of task 3 from the DCASE'16 challenge. Several strategies to address the issue of polyphony are considered. It is shown that DNN-based systems perform less accurate than the traditional systems for this task. Best results are achieved using GFB features in combination with a multiclass GMM-HMM back end.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1304–1314},
numpages = {11}
}

@article{10.1109/TASLP.2017.2690570,
author = {Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gael and Bisot, Victor and Serizel, Romain and Essid, Slim and Richard, Gael},
title = {Feature Learning With Matrix Factorization Applied to Acoustic Scene Classification},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690570},
doi = {10.1109/TASLP.2017.2690570},
abstract = {In this paper, we study the usefulness of various matrix factorization methods for learning features to be used for the specific acoustic scene classification ASC problem. A common way of addressing ASC has been to engineer features capable of capturing the specificities of acoustic environments. Instead, we show that better representations of the scenes can be automatically learned from time-frequency representations using matrix factorization techniques. We mainly focus on extensions including sparse, kernel-based, convolutive and a novel supervised dictionary learning variant of principal component analysis and nonnegative matrix factorization. An experimental evaluation is performed on two of the largest ASC datasets available in order to compare and discuss the usefulness of these methods for the task. We show that the unsupervised learning methods provide better representations of acoustic scenes than the best conventional hand-crafted features on both datasets. Furthermore, the introduction of a novel nonnegative supervised matrix factorization model and deep neural networks trained on spectrograms, allow us to reach further improvements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1216–1229},
numpages = {14}
}

@article{10.1109/TASLP.2017.2690576,
author = {Benetos, Emmanouil and Lafay, Gregoire and Lagrange, Mathieu and Plumbley, Mark D. and Benetos, Emmanouil and Lafay, Gregoire and Lagrange, Mathieu and Plumbley, Mark D.},
title = {Polyphonic Sound Event Tracking Using Linear Dynamical Systems},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690576},
doi = {10.1109/TASLP.2017.2690576},
abstract = {In this paper, a system for polyphonic sound event detection and tracking is proposed, based on spectrogram factorization techniques and state space models. The system extends probabilistic latent component analysis PLCA and is modeled around a four-dimensional spectral template dictionary of frequency, sound event class, exemplar index, and sound state. In order to jointly track multiple overlapping sound events over time, the integration of linear dynamical systems LDS within the PLCA inference is proposed. The system assumes that the PLCA sound event activation is the noisy observation in an LDS, with the latent states corresponding to the true event activations. LDS training is achieved using fully observed data, making use of ground truth-informed event activations produced by the PLCA-based model. Several LDS variants are evaluated, using polyphonic datasets of office sounds generated from an acoustic scene simulator, as well as real and synthesized monophonic datasets for comparative purposes. Results show that the integration of LDS tracking within PLCA leads to an improvement of +8.5-10.5% in terms of frame-based F-measure as compared to the use of the PLCA model alone. In addition, the proposed system outperforms several state-of-the-art methods for the task of polyphonic sound event detection.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1266–1277},
numpages = {12}
}

@article{10.1109/TASLP.2017.2690563,
author = {Xu, Yong and Huang, Qiang and Wang, Wenwu and Foster, Peter and Sigtia, Siddharth and Jackson, Philip J. B. and Plumbley, Mark D. and Yong Xu and Qiang Huang and Wenwu Wang and Foster, Peter and Sigtia, Siddharth and Jackson, Philip J. B. and Plumbley, Mark D.},
title = {Unsupervised Feature Learning Based on Deep Models for Environmental Audio Tagging},
year = {2017},
issue_date = {June 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2690563},
doi = {10.1109/TASLP.2017.2690563},
abstract = {Environmental audio tagging aims to predict only the presence or absence of certain acoustic events in the interested acoustic scene. In this paper, we make contributions to audio tagging in two parts, respectively, acoustic modeling and feature learning. We propose to use a shrinking deep neural network DNN framework incorporating unsupervised feature learning to handle the multilabel classification task. For the acoustic modeling, a large set of contextual frames of the chunk are fed into the DNN to perform a multilabel classification for the expected tags, considering that only chunk or utterance level rather than frame-level labels are available. Dropout and background noise aware training are also adopted to improve the generalization capability of the DNNs. For the unsupervised feature learning, we propose to use a symmetric or asymmetric deep denoising auto-encoder syDAE or asyDAE to generate new data-driven features from the logarithmic Mel-filter banks features. The new features, which are smoothed against background noise and more compact with contextual information, can further improve the performance of the DNN baseline. Compared with the standard Gaussian mixture model baseline of the DCASE 2016 audio tagging challenge, our proposed method obtains a significant equal error rate EER reduction from 0.21 to 0.13 on the development set. The proposed asyDAE system can get a relative 6.7% EER reduction compared with the strong DNN baseline on the development set. Finally, the results also show that our approach obtains the state-of-the-art performance with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while EER of the first prize of this challenge is 0.17.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1230–1241},
numpages = {12}
}

@article{10.1109/TASLP.2017.2678166,
author = {Prusa, Zdenek and Balazs, Peter and Sondergaard, Peter Lempel and Prusa, Zdenek and Balazs, Peter and Sondergaard, Peter Lempel},
title = {A Noniterative Method for Reconstruction of Phase From STFT Magnitude},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2678166},
doi = {10.1109/TASLP.2017.2678166},
abstract = {A noniterative method for the reconstruction of the short-time fourier transform STFT phase from the magnitude is presented. The method is based on the direct relationship between the partial derivatives of the phase and the logarithm of the magnitude of the un-sampled STFT with respect to the Gaussian window. Although the theory holds in the continuous setting only, the experiments show that the algorithm performs well even in the discretized setting discrete Gabor transform with low redundancy using the sampled Gaussian window, the truncated Gaussian window and even other compactly supported windows such as the Hann window. Due to the noniterative nature, the algorithm is very fast and it is suitable for long audio signals. Moreover, solutions of iterative phase reconstruction algorithms can be improved considerably by initializing them with the phase estimate provided by the present algorithm. We present an extensive comparison with the state-of-the-art algorithms in a reproducible manner.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1154–1164},
numpages = {11}
}

@article{10.1109/TASLP.2017.2678162,
author = {Kanda, Naoyuki and Lu, Xugang and Kawai, Hisashi and Kanda, Naoyuki and Xugang Lu and Kawai, Hisashi},
title = {Maximum-a-Posteriori-Based Decoding for End-to-End Acoustic Models},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2678162},
doi = {10.1109/TASLP.2017.2678162},
abstract = {This paper presents a novel decoding framework for acoustic models AMs based on end-to-end neural networks e.g., connectionist temporal classification. The end-to-end training of AMs has recently demonstrated high accuracy and efficiency in automatic speech recognition ASR. When using the trained AM in decoding, although a language model LM is implicitly involved in such an end-to-end AM, it is still essential to integrate an external LM trained with a large text corpus to achieve the best results. While there is no theoretical justification, most of the studies suggest using a naive interpolation of the end-to-end AM score and the external LM score, empirically. In this paper, we propose a more theoretically sound decoding framework derived from a maximization of the posterior probability of a word sequence given an observation. As a consequence of the theory, the subword LM is newly introduced to seamlessly integrate the external LM score with the end-to-end AM score. Our proposed method can be achieved by a small modification of the conventional weighted finite-state transducer-based implementation, without having to heavily increase the graph size. We tested the proposed decoding framework on ASR experiments with the Corpus of the Wall Street Journal and the Corpus of Spontaneous Japanese. The results showed that the proposed framework achieved significant and consistent improvements over the conventional interpolation-based decoding framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1023–1034},
numpages = {12}
}

@article{10.1109/TASLP.2017.2676356,
author = {Li, Songbin and Jia, Yizhen and Kuo, C. -C. Jay and Songbin Li and Yizhen Jia and Kuo, C.-C Jay},
title = {Steganalysis of QIM Steganography in Low-Bit-Rate Speech Signals},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2676356},
doi = {10.1109/TASLP.2017.2676356},
abstract = {Steganalysis of the quantization index modulation QIM steganography in a low-bit-rate encoded speech stream is conducted in this research. According to the speech generation theory and the phoneme distribution properties in language, we first point out that the correlation characteristics of split vector quantization VQ codewords of linear predictive coding filter coefficients are changed after the QIM steganography. Based on this observation, we construct a model called the Quantization codeword correlation network QCCN based on split VQ codeword from adjacent speech frames. Furthermore, the QCCN model is pruned to yield a stronger correlation network. After quantifying the correlation characteristics of vertices in the pruned correlation network, we obtain feature vectors that are sensitive to steganalysis. Finally, we build a high-performance detector using the support vector machine SVM classifier. It is shown by experimental results that the proposed QCCN steganalysis method can effectively detect the QIM steganography in encoded speech stream when it is applied to low-bit-rate speech codec such as G.723.1 and G.729.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1011–1022},
numpages = {12}
}

@article{10.1109/TASLP.2017.2671422,
author = {Yang, Feiran and Enzner, Gerald and Yang, Jun and Yang, Feiran and Enzner, Gerald and Jun Yang},
title = {Statistical Convergence Analysis for Optimal Control of DFT-Domain Adaptive Echo Canceler},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2671422},
doi = {10.1109/TASLP.2017.2671422},
abstract = {The frequency-domain adaptive filter FDAF is widely used in echo cancellation systems due to its low complexity and fast convergence rate. However, the FDAF algorithm with a fixed step size exhibits a tradeoff among the convergence rate, steady-state misalignment, tracking ability, and robustness to near-end speech interferences. Several variable step-size FDAF algorithms were presented to address this problem. However, the state-of-the-art variable step-size FDAF algorithms did not handle this problem comprehensively. This paper presents a new robust variable step-size control approach to the FDAF algorithm. Based on a statistical analysis of the FDAF algorithm, an optimal step size for each frequency bin is derived by minimizing the mean-square deviation MSD between the true weight vector and estimated weight vector at each frame. Calculation of the step size requires the system distance and the observation noise power spectral density PSD. The system distance is estimated using the deterministic recursive equations of MSD and the noise PSD is computed using the magnitude squared coherence function between the far-end signal and error signal. Moreover, a close link between the proposed FDAF and the frequency-domain Kalman filter is revealed. Specifically, the work presented here can be understood as a means to adaptively monitor and control the underlying acoustic state space of the Kalman filter, including means for fast readaptation of the adaptive filter after abrupt echo path changes. Simulation results demonstrate that the proposed algorithm can achieve fast convergence and low steady-state misalignment. Furthermore, the algorithm is robust to the double-talk interferences, but it does not require an explicit double-talk detector.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1095–1106},
numpages = {12}
}

@article{10.1109/TASLP.2017.2655258,
author = {Schwartz, Ofer and Gannot, Sharon and Habets, Emanuel A. P. and Schwartz, Ofer and Gannot, Sharon and Habets, Emanuel A. P.},
title = {Multispeaker LCMV Beamformer and Postfilter for Source Separation and Noise Reduction},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2655258},
doi = {10.1109/TASLP.2017.2655258},
abstract = {The problem of source separation and noise reduction using multiple microphones is addressed. The minimum mean square error MMSE estimator for the multispeaker case is derived and a novel decomposition of this estimator is presented. The MMSE estimator is decomposed into two stages: first, a multispeaker linearly constrained minimum variance LCMV beamformer BF; and second, a subsequent multispeaker Wiener postfilter. The first stage separates and enhances the signals of the individual speakers by utilizing the spatial characteristics of the speakers [as manifested by the respective acoustic transfer functions ATFs] and the noise power spectral density PSD matrix, while the second stage exploits the speakers' PSD matrix to reduce the residual noise at the output of the first stage. The output vector of the multispeaker LCMV BF is proven to be the sufficient statistic for estimating the marginal speech signals in both the classic sense and the Bayesian sense. The log spectral amplitude estimator for the multispeaker case is also derived given the multispeaker LCMV BF outputs. The performance evaluation was conducted using measured ATFs and directional noise with various signal-to-noise ratio levels. It is empirically verified that the multispeaker postfilters are beneficial in terms of signal-to-interference plus noise ratio improvement when compared with the single-speaker postfilter.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {940–951},
numpages = {12}
}

@article{10.1109/TASLP.2016.2620718,
author = {Airaksinen, Manu and Backstrom, Tom and Alku, Paavo and Airaksinen, Manu and Backstrom, Tom and Alku, Paavo},
title = {Quadratic Programming Approach to Glottal Inverse Filtering by Joint Norm-1 and Norm-2 Optimization},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2620718},
doi = {10.1109/TASLP.2016.2620718},
abstract = {This study proposes an approach for glottal inverse filtering of acoustic speech signals using quadratic programming QPR. The method aims to jointly model the effect of vocal tract and lip radiation with a single filter whose coefficients are optimized using QPR. This optimization is based on the principles of closed phase analysis, where the contribution of the glottal source is attenuated in optimizing the inverse model of the vocal tract. By expressing the optimization problem in terms of the output of a filter, we can apply physically motivated optimization such as flatness of the closed phase. The proposed method was objectively evaluated using a synthetic Liljencrants-Fant model based test set of sustained vowels, as well as a real speech test set where the glottal flow estimates' closed phases were compared in terms of their flatness. The results based on synthetic speech indicate that the proposed method is robust to changes in f0, and state-of-the-art quality results were obtained for high-pitched voices, when f0 is in the range 330-450 Hz. The results based on real speech indicate that the proposed method produces glottal flow estimates that have flatter closed phases with less formant ripple in comparison to estimates computed with known reference methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {929–939},
numpages = {11}
}

@article{10.1109/TASLP.2017.2689681,
author = {Huang, Gongping and Benesty, Jacob and Chen, Jingdong and Gongping Huang and Benesty, Jacob and Jingdong Chen},
title = {On the Design of Frequency-Invariant Beampatterns With Uniform Circular Microphone Arrays},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2689681},
doi = {10.1109/TASLP.2017.2689681},
abstract = {This paper deals with two critical issues about uniform circular arrays UCAs: frequency-invariant response and steering flexibility. It focuses on some optimal design of frequency-invariant beampatterns in any desired direction along the sensor plane. The major contributions are as follows. 1 We explain how to include the steering information in the desired directivity pattern. 2 We show that the optimal approximation of the beamformer's beampattern with a UCA from a least-squares error perspective is the Jacobi-Anger expansion. 3 We develop an approach to the design of any desired symmetric directivity pattern, where the deduced beampattern is almost frequency invariant and its main beam can be pointed to any wanted direction in the sensor plane. 4 With the proposed approach, we derive an explicit form of the white noise gain WNG and the directivity factor DF, and explain clearly the white noise amplification problem at low frequencies and the DF degradation at high frequencies. The analysis also indicates that increasing the number of microphones can always improve the WNG. We show that the proposed method is a generalization of circular differential microphone arrays. The relationship between the proposed method and the so-called circular harmonics beamformers is also discussed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1140–1153},
numpages = {14}
}

@article{10.1109/TASLP.2017.2678684,
author = {Shokouhi, Navid and Hansen, John H. L. and Shokouhi, Navid and Hansen, John H. L.},
title = {Teager–Kaiser Energy Operators for Overlapped Speech Detection},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2678684},
doi = {10.1109/TASLP.2017.2678684},
abstract = {Overlapped speech is referred to a monophonic audio signal in which at least two speakers are present at the same time. In this study, the focus is on distinguishing overlapped from single-speaker speech, i.e., overlapped speech detection. We develop an overlap detection algorithm using an enhanced time-frequency representation, called Pyknogram, estimated directly from the input audio signal. Pyknograms use the Teager-Kaiser energy operator to detect resonant time-frequency units and thereby suppress nonharmonic structures. We show how the resulting Pyknograms provide high separability in terms of detecting the presence of interfering speech. Our proposed unsupervised Pyknogram-based detection results in over 30% relative improvement in overlap detection error rates across different signal-to-interference ratios SIR compared to baseline systems. In addition, a case study is presented where we evaluate speaker verification performance under different overlap conditions using the GRID database and observe that speaker verification equal error rates EER vary from 2% to 30%, depending on the average SIR values introduced to train and test sets. In order to estimate the reliability of speaker verification scores across different trials, overlap detection results are interpreted as low-level information and stacked alongside verification outputs. The resulting high-dimensional space is passed through a support vector machine classifier to find the separating hyperplane between target and imposter scores. Combining overlap detection scores with speaker verification on average yields 20% relative decrease in EER. We also provide an upper bound for this approach using existing overlap labels, which yields 23% relative improvement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1035–1047},
numpages = {13}
}

@article{10.1109/TASLP.2017.2681742,
author = {Park, Jeongsoo and Shin, Jaeyoung and Lee, Kyogu and Park, Jeongsoo and Jaeyoung Shin and Kyogu Lee},
title = {Exploiting Continuity/Discontinuity of Basis Vectors in Spectrogram Decomposition for Harmonic-Percussive Sound Separation},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2681742},
doi = {10.1109/TASLP.2017.2681742},
abstract = {In this paper, we present a novel method for harmonic-percussive sound separation HPSS by exploiting continuity/discontinuity properties in a matrix decomposition framework. It is widely accepted in the HPSS research that the harmonic and percussive components have anisotropic characteristics: The spectrum of the harmonic components and the time activation of the percussive components are sparse, whereas the spectrum of the percussive components and the time activation of the harmonic components are smooth. However, conventional methods fail to fully utilize the characteristics leading to suboptimal performance. Based on the observations that not the degree of sparseness but the degree of fluctuation is an accurate measure for distinguishing the harmonic and percussive components, we propose a novel HPSS algorithm by incorporating the continuity control in the iterative update formula of the matrix decomposition algorithm. In doing so, we first utilize probabilistic latent component analysis with Dirichlet prior, and later reformulate the algorithm in the nonnegative matrix factorization framework to reduce the computational cost. The comparative evaluation results show that the proposed method outperforms conventional methods in terms of both objective and subjective evaluation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1061–1074},
numpages = {14}
}

@article{10.1109/TASLP.2017.2672401,
author = {Sainath, Tara N. and Weiss, Ron J. and Wilson, Kevin W. and Li, Bo and Narayanan, Arun and Variani, Ehsan and Bacchiani, Michiel and Shafran, Izhak and Senior, Andrew and Chin, Kean and Misra, Ananya and Kim, Chanwoo and Sainath, Tara N. and Weiss, Ron J. and Wilson, Kevin W. and Bo Li and Narayanan, Arun and Variani, Ehsan and Bacchiani, Michiel and Shafran, Izhak and Senior, Andrew and Chin, Kean and Misra, Ananya and Chanwoo Kim},
title = {Multichannel Signal Processing With Deep Neural Networks for Automatic Speech Recognition},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2672401},
doi = {10.1109/TASLP.2017.2672401},
abstract = {Multichannel automatic speech recognition ASR systems commonly separate speech enhancement, including localization, beamforming, and postfiltering, from acoustic modeling. In this paper, we perform multichannel enhancement jointly with acoustic modeling in a deep neural network framework. Inspired by beamforming, which leverages differences in the fine time structure of the signal at different microphones to filter energy arriving from different directions, we explore modeling the raw time-domain waveform directly. We introduce a neural network architecture, which performs multichannel filtering in the first layer of the network, and show that this network learns to be robust to varying target speaker direction of arrival, performing as well as a model that is given oracle knowledge of the true target speaker direction. Next, we show how performance can be improved by factoring the first layer to separate the multichannel spatial filtering operation from a single channel filterbank which computes a frequency decomposition. We also introduce an adaptive variant, which updates the spatial filter coefficients at each time frame based on the previous inputs. Finally, we demonstrate that these approaches can be implemented more efficiently in the frequency domain. Overall, we find that such multichannel neural networks give a relative word error rate improvement of more than 5% compared to a traditional beamforming-based multichannel ASR system and more than 10% compared to a single channel waveform model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {965–979},
numpages = {15}
}

@article{10.1109/TASLP.2017.2689245,
author = {Firtha, Gergely and Fiala, Peter and Schultz, Frank and Spors, Sascha and Firtha, Gergely and Fiala, Peter and Schultz, Frank and Spors, Sascha},
title = {Improved Referencing Schemes for 2.5D Wave Field Synthesis Driving Functions},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2689245},
doi = {10.1109/TASLP.2017.2689245},
abstract = {Wave Field Synthesis allows the reconstruction of an arbitrary target sound field within a listening area by using a secondary source contour of spherical monopoles. While phase correct synthesis is ensured over the whole listening area, amplitude deviations are present besides a predefined reference curve. So far, the existence and potential shapes of this reference curve was not extensively discussed in the Wave Field Synthesis literature. This paper introduces improved driving functions for 2.5D Wave Field Synthesis. The novel driving functions allow for the control of the locations of amplitude correct synthesis for arbitrarily shaped-possibly curved-secondary source distributions. This is achieved by deriving an expressive physical interpretation of the stationary phase approximation leading to the presented unified Wave Field Synthesis framework. The improved solutions are better suited for practical applications. Additionally, a consistent classification of existing implicit and explicit 2.5D sound field synthesis solutions as special cases of the unified framework is given.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1117–1127},
numpages = {11}
}

@article{10.1109/TASLP.2017.2687104,
author = {Zhang, Xueliang and Wang, DeLiang and Xueliang Zhang and DeLiang Wang},
title = {Deep Learning Based Binaural Speech Separation in Reverberant Environments},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2687104},
doi = {10.1109/TASLP.2017.2687104},
abstract = {Speech signal is usually degraded by room reverberation and additive noises in real environments. This paper focuses on separating target speech signal in reverberant conditions from binaural inputs. Binaural separation is formulated as a supervised learning problem, and we employ deep learning to map from both spatial and spectral features to a training target. With binaural inputs, we first apply a fixed beamformer and then extract several spectral features. A new spatial feature is proposed and extracted to complement the spectral features. The training target is the recently suggested ideal ratio mask. Systematic evaluations and comparisons show that the proposed system achieves very good separation performance and substantially outperforms related algorithms under challenging multisource and reverberant environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1075–1084},
numpages = {10}
}

@article{10.1109/TASLP.2017.2689241,
author = {Maestre, Esteban and Scavone, Gary P. and Smith, Julius O. and Maestre, Esteban and Scavone, Gary P. and Smith, Julius O.},
title = {Joint Modeling of Bridge Admittance and Body Radiativity for Efficient Synthesis of String Instrument Sound by Digital Waveguides},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2689241},
doi = {10.1109/TASLP.2017.2689241},
abstract = {In the context of efficient sound synthesis by digital waveguides, we present a novel methodology for joint modeling of string instrument body radiativity and driving-point bridge admittance functions, as obtained from experimental data. From our modeling framework, aimed at simulation of guitar and bowed string sound, here we focus on the body of the instrument and leave aside the strings. First, a modal decomposition of the measured bridge admittance is obtained by means of a novel frequency-domain algorithm for optimization of recursive digital filters in parallel form. Then, from extracted modal parameters, the radiativity, and admittance functions are modeled by projecting measurements over a common modal basis, enforcing passivity of the two-dimensional admittance model by means of semidefinite programming. We propose a formulation that enables the joint realization of bridge reflectance and sound radiativity as a lumped delay line termination in which a single bank of resonant filters is shared among all string reflection and body radiation outputs. Our approach provides efficient means to model two-dimensional 2-D bridge reflectance, 2-D string-string coupling, sound radiation with an arbitrary number of outputs, and implicitly vibrational energy loss from the bridge transmittance to nonradiating modes and dissipation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1128–1139},
numpages = {12}
}

@article{10.1109/TASLP.2017.2687829,
author = {Delfarah, Masood and Wang, DeLiang and Delfarah, Masood and DeLiang Wang},
title = {Features for Masking-Based Monaural Speech Separation in Reverberant Conditions},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2687829},
doi = {10.1109/TASLP.2017.2687829},
abstract = {Monaural speech separation is a fundamental problem in speech and signal processing. This problem can be approached from a supervised learning perspective by predicting an ideal time-frequency mask from features of noisy speech. In reverberant conditions at low signal-to-noise ratios SNRs, accurate mask prediction is challenging and can benefit from effective features. In this paper, we investigate an extensive set of acoustic-phonetic features extracted in adverse conditions. Deep neural networks are used as the learning machine, and separation performance is evaluated using standard objective speech intelligibility metrics. Separation performance is systematically evaluated in both nonspeech and speech interference, in a variety of SNRs, reverberation times, and direct-to-reverberant energy ratios. Considerable performance improvement is observed by using contextual information, likely due to temporal effects of room reverberation. In addition, we construct feature combination sets using a sequential floating forward selection algorithm, and combined features outperform individual ones. We also find that optimal feature sets in anechoic conditions are different from those in reverberant conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1085–1094},
numpages = {10}
}

@article{10.1109/TASLP.2017.2679603,
author = {Huang, Yi-Chin and Wu, Chung-Hsien and Chen, Yan-You and Shie, Ming-Ge and Wang, Jhing-Fa and Yi-Chin Huang and Chung-Hsien Wu and Yan-You Chen and Ming-Ge Shie and Jhing-Fa Wang},
title = {Personalized Spontaneous Speech Synthesis Using a Small-Sized Unsegmented Semispontaneous Speech},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2679603},
doi = {10.1109/TASLP.2017.2679603},
abstract = {A systematic approach is proposed to synthesizing personalized spontaneous speech using a small-sized unsegmented speech corpus of the target speaker. First, an automatic segmentation algorithm is employed to segment and label a collected semispontaneous speech corpus of the target speaker. Then, a pretrained average voice model is adapted to the voice model of the target speaker by using the segmented data. A postfilter based on modulation spectrum is adopted to further improve the speaker similarity of the synthesized speech as well as alleviate the over-smoothing problem of the synthesized speech. For generating spontaneous speech, a smoothing method applied at the prosodic word level is proposed to improve speech fluency. For objective evaluation on spontaneous speech segmentation, the segmentation accuracy of the proposed method is superior to that of Viterbi-based forced alignment. The results of subjective listening test also show that the proposed method can improve the spontaneity and speaker similarity of the synthesized speech compared to the maximum likelihood linear regression based speaker adaptation method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1048–1060},
numpages = {13}
}

@article{10.1109/TASLP.2017.2667879,
author = {Wang, Dongmei and Yu, Chengzhu and Hansen, John H. L. and Dongmei Wang and Chengzhu Yu and Hansen, John H. L.},
title = {Robust Harmonic Features for Classification-Based Pitch Estimation},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2667879},
doi = {10.1109/TASLP.2017.2667879},
abstract = {Pitch estimation in diverse naturalistic audio streams remains a challenge for speech processing and spoken language technology. In this study, we investigate the use of robust harmonic features for classification-based pitch estimation. The proposed pitch estimation algorithm is composed of two stages: pitch candidate generation and target pitch selection. Based on energy intensity and spectral envelope shape, five types of robust harmonic features are proposed to reflect pitch associated harmonic structure. A neural network is adopted for modeling the relationship between input harmonic features and output pitch salience for each specific pitch candidate. In the test stage, each pitch candidate is assessed with an output salience that indicates the potential as a true pitch value, based on its input feature vector processed through the neural network. Finally, according to the temporal continuity of pitch values, pitch contour tracking is performed using a hidden Markov model HMM, and the Viterbi algorithm is used for HMM decoding. Experimental results show that the proposed algorithm outperforms several state-of-the-art pitch estimation methods in terms of accuracy in both high and low levels of additive noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {952–964},
numpages = {13}
}

@article{10.1109/TASLP.2017.2674975,
author = {Franck, Andreas and Wang, Wenwu and Fazi, Filippo Maria and Franck, Andreas and Wenwu Wang and Fazi, Filippo Maria},
title = {Sparse $\ell _{1}$-Optimal Multiloudspeaker Panning and Its Relation to Vector Base Amplitude Panning},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2674975},
doi = {10.1109/TASLP.2017.2674975},
abstract = {Panning techniques, such as vector base amplitude panning VBAP, are a widely used practical approach for spatial sound reproduction using multiple loudspeakers. Although limited to a relatively small listening area, they are very efficient and offer good localization accuracy, timbral quality, as well as a graceful degradation of quality outside the sweet spot. The aim of this paper is to investigate optimal sound reproduction techniques that adopt some of the advantageous properties of VBAP, such as the sparsity and the locality of the active loudspeakers for the reproduction of a single audio object. To this end, we state the task of multiloudspeaker panning as an ℓ1 optimization problem. We demonstrate and prove that the resulting solutions are exactly sparse. Moreover, we show the effect of adding a nonnegativity constraint on the loudspeaker gains in order to preserve the locality of the panning solution. Adding this constraint, ℓ1-optimal panning can be formulated as a linear program. Using this representation, we prove that unique ℓ1-optimal panning solutions incorporating a nonnegativity constraint are identical to VBAP using a Delaunay triangulation for the loudspeaker setup. Using results from linear programming and duality theory, we describe properties and special cases, such as solution ambiguity, of the VBAP solution.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {996–1010},
numpages = {15}
}

@article{10.1109/TASLP.2017.2688585,
author = {Nose, Takashi and Arao, Yusuke and Kobayashi, Takao and Sugiura, Komei and Shiga, Yoshinori and Nose, Takashi and Arao, Yusuke and Kobayashi, Takao and Sugiura, Komei and Shiga, Yoshinori},
title = {Sentence Selection Based on Extended Entropy Using Phonetic and Prosodic Contexts for Statistical Parametric Speech Synthesis},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2688585},
doi = {10.1109/TASLP.2017.2688585},
abstract = {This paper proposes a sentence selection technique for constructing phonetically and prosodically balanced compact recording scripts for speech synthesis. In the conventional corpus design of speech synthesis, a greedy algorithm that maximizes phonetic coverage is often used. However, for statistical parametric speech synthesis, balances of multiple phonetic and prosodic contextual factors are important as well as the coverage. To take account of both of the phonetic and prosodic contextual balances in sentence selection, we introduce an extended entropy of phonetic and prosodic contexts, such as biphone/triphone, accent/stress/tone, and sentence length. For detailed investigation, conventional and proposed techniques are evaluated using Japanese, English, and Chinese corpora. The objective experimental results show that the proposed technique achieves better coverage and balance of contexts. In addition, speech synthesis experiments based on hidden Markov models reveal that the generated speech parameters become closer to those of the natural speech compared with other conventional sentence selection techniques. Subjective evaluations show that the proposed sentence selection based on the extended entropy improves the naturalness of the synthetic speech while maintaining the similarity to the original sample.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1107–1116},
numpages = {10}
}

@article{10.1109/TASLP.2017.2674971,
author = {Khalilian, Hanieh and Bajic, Ivan V. and Vaughan, Rodney G. and Khalilian, Hanieh and Bajic, Ivan V. and Vaughan, Rodney G.},
title = {A Simulation Study of a Three-Dimensional Sound Field Reproduction System for Immersive Communication},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Press},
volume = {25},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2674971},
doi = {10.1109/TASLP.2017.2674971},
abstract = {Immersive communication systems promise a greatly improved user experience through the use of advanced technologies tailored to various human senses, such as sight, hearing, and touch. This paper focuses on a simulation study of three-dimensional 3-D sound field reproduction SFR for immersive communication. At the transmitting end, the incident sound field is captured via a microphone array, active talkers are detected, and a clean version of the signals corresponding to active talkers are found by existing methods. The captured information is transmitted to the receiving end, where a 3-D sound field from virtual sources corresponding to active talkers is synthesized around listeners' heads by the proposed SFR method. In our system, the radiation patterns of the higher order directive loudspeakers are optimized by the constrained matching pursuit algorithm. Implementation of the directive loudspeaker patterns is not addressed here, and their deployment is assumed. Simulation results quantify the benefit of higher order loudspeakers for speech sound field synthesis in reverberant rooms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {980–995},
numpages = {16}
}

@article{10.1109/TASLP.2017.2674968,
author = {Bao, Yu and Chen, Huawei and Huawei Chen},
title = {Design of Robust Broadband Beamformers Using Worst-Case Performance Optimization: A Semidefinite Programming Approach},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2674968},
doi = {10.1109/TASLP.2017.2674968},
abstract = {Broadband beamformers are known sensitive to microphone mismatches, especially for small-sized arrays. To address the problem, the worst-case performance optimization WCPO criterion has been used to design robust broadband beamformers. Recently, a circular-model-based design using the WCPO criterion was proposed, which has been shown superior to the existing second-order cone programming SOCP based design also using the WCPO criterion. In this paper, however, we find that the circular-model-based design is sensitive to stopband level constraint. If the constraint is chosen tightly, it may become less robust and degrades even worse than the SOCP-based design. To achieve a better tradeoff between robustness and beamforming performance, we propose a semidefinite programming based approach for robust broadband beamformer design using the WCPO criterion. The relations of the proposed design to its existing counterparts are also theoretically analyzed. In particular, we show that the proposed design is less conservative than the SOCP-based design, and is also less sensitive to the setting of stopband level constraint and more robust when compared to the circular model-based design. Another contribution of this paper is that the relation between the SOCP- and the circular-model-based designs is theoretically revealed. Simulation results and real experimental results are presented to show the superior of the proposed design over its existing counterparts.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {895–907},
numpages = {13}
}

@article{10.1109/TASLP.2017.2658019,
author = {Yang, Liner and Chen, Xinxiong and Liu, Zhiyuan and Sun, Maosong and Liner Yang and Xinxiong Chen and Zhiyuan Liu and Maosong Sun},
title = {Improving Word Representations with Document Labels},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2658019},
doi = {10.1109/TASLP.2017.2658019},
abstract = {Word representations, aiming to build vectors for each word, have been successfully used in a variety of applications. Most word representations are learned from large amounts of documents ignoring other information. This is rather suboptimal because the side information of the documents, such as document labels, is not used in learning word representations. In this paper, we focus on how to exploit these side information to improve word representations. We propose to incorporate document labels into the learning process of word representations in two frameworks: neural network and matrix factorization. The experimental results on word analogy and word similarity task show that our models can better capture the semantic and syntactic information than the original models. Our models also improve the performance of word representations on text classification task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {863–870},
numpages = {8}
}

@article{10.1109/TASLP.2016.2625458,
author = {Ying, Dongwen and Zhou, Ruohua and Li, Junfeng and Yan, Yonghong and Dongwen Ying and Ruohua Zhou and Junfeng Li and Yonghong Yan},
title = {Window-Dominant Signal Subspace Methods for Multiple Short-Term Speech Source Localization},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2625458},
doi = {10.1109/TASLP.2016.2625458},
abstract = {Signal subspace has been widely exploited to localize multiple speech sources. However, most signal subspace methods cannot count the number of sources, and do not make use of speech sparsity in the frequency domain. This paper presents a grid search window-dominant signal subspace GS-WDSS method and a closed-form WDSS CF-WDSS method to localize short-term speech sources. Such methods are based upon the generalized sparsity assumption that each window containing some time-adjacent bins is dominated by one source, as opposed to the conventional assumption that each individual bin is dominated by one source. The generalized assumption enables the principal eigenvector of the spatial correlation matrix on each window to span the signal subspace of the window-dominant source. The direction-of-arrival DOA of the dominant source is estimated from the principal eigenvector. The DOAs and the number of sources are eventually summarized from the DOA histogram of all dominant sources. The conventional assumption is a special case of the generalized assumption. By using the generalized assumption, the performance in estimating DOAs of the window-dominant sources is significantly improved at the cost of acceptable masking effect. The superiority of the proposed methods is verified by simulated and real experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {731–744},
numpages = {14}
}

@article{10.1109/TASLP.2017.2667880,
author = {Sarfjoo, Seyyed Saeed and Demiroglu, Cenk and King, Simon and Sarfjoo, Seyyed Saeed and Demiroglu, Cenk and King, Simon},
title = {Using Eigenvoices and Nearest-Neighbors in HMM-Based Cross-Lingual Speaker Adaptation With Limited Data},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2667880},
doi = {10.1109/TASLP.2017.2667880},
abstract = {Cross-lingual speaker adaptation for speech synthesis has many applications, such as use in speech-to-speech translation systems. Here, we focus on cross-lingual adaptation for statistical speech synthesis systems using limited adaptation data. To that end, we propose two eigenvoice adaptation approaches exploiting a bilingual Turkish-English speech database that we collected. In one approach, eigenvoice weights extracted using Turkish adaptation data and Turkish voice models are transformed into the eigenvoice weights for the English voice models using linear regression. Weighting the samples depending on the distance of reference speakers to target speakers during linear regression was found to improve the performance. Moreover, importance weighting the elements of the eigenvectors during regression further improved the performance. The second approach proposed here is speaker-specific state-mapping, which performed significantly better than the baseline state-mapping algorithm both in objective and subjective tests. Performance of the proposed state mapping algorithm was further improved when it was used with the intralingual eigenvoice approach instead of the linear-regression based algorithms used in the baseline system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {839–851},
numpages = {13}
}

@article{10.1109/TASLP.2017.2670141,
author = {Karanasou, Penny and Wu, Chunyang and Gales, Mark and Woodland, Philip C. and Karanasou, Penny and Chunyang Wu and Gales, Mark and Woodland, Philip C.},
title = {I-Vectors and Structured Neural Networks for Rapid Adaptation of Acoustic Models},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2670141},
doi = {10.1109/TASLP.2017.2670141},
abstract = {A lot of interest has been risen in the last years on the adaptation of deep neural network DNN acoustic models, as the latter become the state-of-art in automatic speech recognition. This work focuses on approaches that allow for rapid and robust adaptation of such models. First, i-vectors are added to the DNN input as speaker-informed features. An informative prior is introduced to i-vector estimation to improve the robustness to limited adaptation data. I-vectors are then combined with a structured adaptive DNN, the multibasis adaptive neural network MBANN, and the complementarity of these adaptation techniques is investigated. Moreover, i-vectors are used to predict the MBANN transforms, avoiding the initial decoding pass and alignment. These approaches are evaluated on a U.S. English Broadcast News BN transcription task with two distinct sets of test data. The first, from the BN task and BN-style Youtube videos, yields test data acoustically matched to the training data, while the second set is from acoustically mismatched Youtube videos of diverse context. The performance gains from these schemes are found to be sensitive to the level of mismatch between training and test sets. The MBANN system combined with i-vector input achieves best performance for BN test sets. The i-vector-based predictive MBANN scheme is proven to be more robust to acoustically mismatched conditions and outperforms the other adaptation schemes in such scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {818–828},
numpages = {11}
}

@article{10.1109/TASLP.2016.2647702,
author = {Gannot, Sharon and Vincent, Emmanuel and Markovich-Golan, Shmulik and Ozerov, Alexey and Gannot, Sharon and Vincent, Emmanuel and Markovich-Golan, Shmulik and Ozerov, Alexey},
title = {A Consolidated Perspective on Multimicrophone Speech Enhancement and Source Separation},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2647702},
doi = {10.1109/TASLP.2016.2647702},
abstract = {Speech enhancement and separation are core problems in audio signal processing, with commercial applications in devices as diverse as mobile phones, conference call systems, hands-free systems, or hearing aids. In addition, they are crucial preprocessing steps for noise-robust automatic speech and speaker recognition. Many devices now have two to eight microphones. The enhancement and separation capabilities offered by these multichannel interfaces are usually greater than those of single-channel interfaces. Research in speech enhancement and separation has followed two convergent paths, starting with microphone array processing and blind source separation, respectively. These communities are now strongly interrelated and routinely borrow ideas from each other. Yet, a comprehensive overview of the common foundations and the differences between these approaches is lacking at present. In this paper, we propose to fill this gap by analyzing a large number of established and recent techniques according to four transverse axes: 1 the acoustic impulse response model, 2 the spatial filter design criterion, 3 the parameter estimation algorithm, and 4 optional postfiltering. We conclude this overview paper by providing a list of software and data resources and by discussing perspectives and future trends in the field.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {692–730},
numpages = {39}
}

@article{10.1109/TASLP.2017.2661705,
author = {Ghahabi, Omid and Hernando, Javier and Ghahabi, Omid and Hernando, Javier},
title = {Deep Learning Backend for Single and Multisession I-Vector Speaker Recognition},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2661705},
doi = {10.1109/TASLP.2017.2661705},
abstract = {The lack of labeled background data makes a big performance gap between cosine and Probabilistic Linear Discriminant Analysis PLDA scoring baseline techniques for i-vectors in speaker recognition. Although there are some unsupervised clustering techniques to estimate the labels, they cannot accurately predict the true labels and they also assume that there are several samples from the same speaker in the background data that could not be true in reality. In this paper, the authors make use of Deep Learning DL to fill this performance gap given unlabeled background data. To this goal, the authors have proposed an impostor selection algorithm and a universal model adaptation process in a hybrid system based on deep belief networks and deep neural networks to discriminatively model each target speaker. In order to have more insight into the behavior of DL techniques in both single- and multisession speaker enrollment tasks, some experiments have been carried out in this paper in both scenarios. Experiments on National Institute of Standards and Technology 2014 i-vector challenge show that 46% of this performance gap, in terms of minimum of the decision cost function, is filled by the proposed DL-based system. Furthermore, the score combination of the proposed DL-based system and PLDA with estimated labels covers 79% of this gap.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {807–817},
numpages = {11}
}

@article{10.1109/TASLP.2016.2572259,
author = {Chen, Yung-Yue and Zhang, Jia-Hao and Yung-Yue Chen and Jia-Hao Zhang},
title = {Background Noise Reduction Design for Dual Microphone Cellular Phones: Robust Approach},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2572259},
doi = {10.1109/TASLP.2016.2572259},
abstract = {Cellular phones are used almost everywhere for the communication purpose around the world. However, until now, in noisy environments, the speech communication quality of cellular phones is still awful because the noisy environments where people truly talk are quite complicated and there exists no suitable background noise reduction solution for dealing with this problem effectively. Based on these reasons, this research successfully proposes a novel noise reduction methodology to solve the background noise corrupted problem of cellular phones via a systematic design that comprises a dual microphone array hardware and a novel algorithm combined with a background noise whitening process, the system modeling method, and the robust estimation concept software. After practical experiments, a fact that this proposed method has the immunity to the really noisy environment where cellular phones are used can be found. This background noise reduction design possesses the following impressive advantages: an average of a 20 dB enhanced noise reduction performance the averaged value of state-of-the-art methods are about 10-15 dB, and an average similarity of 93% of pure speeches can be obtained for the purified speeches by applying this proposed method. Besides, this proposed method can be easily implemented in any of the one shelf or newly developed cellular phones with a really low cost.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {852–862},
numpages = {11}
}

@article{10.1109/TASLP.2017.2666425,
author = {Aneeja, G. and Yegnanarayana, B. and Aneeja, G. and Yegnanarayana, B.},
title = {Extraction of Fundamental Frequency From Degraded Speech Using Temporal Envelopes at High SNR Frequencies},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2666425},
doi = {10.1109/TASLP.2017.2666425},
abstract = {In this paper we propose a method for extracting the fundamental frequency fo from degraded speech signals using single frequency filtering SFF approach. The SFF of frequency-shifted speech signal gives high signal-to-noise ratio SNR segments at some frequencies and hence the SFF approach can be exploited for fo extraction using autocorrelation function of those segments. Since the fo is computed from the envelope of a single frequency component of the signal, the vocal tract resonances do not affect the fo extraction. The use of the high SNR frequency component in a given segment helps in overcoming the effects of degradations in the speech signal, without explicitly estimating the characteristics of noise. The proposed method of fo extraction is shown to give better performance for several types of real and simulated degradations, in comparison with some of the methods reported recently in the literature.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {829–838},
numpages = {10}
}

@article{10.1109/TASLP.2016.2644863,
author = {Yang, Xuefeng and Mao, Kezhi and Xuefeng Yang and Kezhi Mao},
title = {Task Independent Fine Tuning for Word Embeddings},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2644863},
doi = {10.1109/TASLP.2016.2644863},
abstract = {Representation learning of words, also known as word embedding technique, is based on the distributional hypothesis that words with similar semantic meanings have similar context. The selection of context window naturally has an influence on word vectors learned. However, it is found that the word vectors are often very sensitive to the defined context window, and unfortunately there is no unified optimal context window for all words. One impact of this issues is that, under a predefined context window, the semantic meanings of some words may not be well represented by the learned vectors. To alleviate the problem and improve word embeddings, we propose a task-independent fine-tuning framework in this paper. The main idea of the task-independent fine tuning is to integrate multiple word embeddings and lexical semantic resources to fine tune a target word embedding. The effectiveness of the proposed framework is tested by tasks of semantic similarity prediction, analogical reasoning, and sentence completion. Experiments results on six word embeddings and eight datasets show that the proposed fine-tuning framework could significantly improve word embeddings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {885–894},
numpages = {10}
}

@article{10.1109/TASLP.2017.2662232,
author = {Koizumi, Yuma and Niwa, Kenta and Hioka, Yusuke and Kobayashi, Kazunori and Ohmuro, Hitoshi and Koizumi, Yuma and Niwa, Kenta and Hioka, Yusuke and Kobayashi, Kazunori and Ohmuro, Hitoshi},
title = {Informative Acoustic Feature Selection to Maximize Mutual Information for Collecting Target Sources},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2662232},
doi = {10.1109/TASLP.2017.2662232},
abstract = {An informative acoustic-feature-selection method for collecting target sources in noisy environments is proposed. Wiener filtering is a powerful framework for sound-source enhancement. For Wiener-filter estimation, statistical-mapping functions, such as deep neural network based or Gaussian mixture model based mappings, have been used. In this framework, it is essential to find informative acoustic features that provide effective cues for Wiener-filter estimation. In this study, we measured the informativeness of acoustic features using mutual information between acoustic features and supervised Wiener-filter parameters, e.g., prior signal-to-noise ratios, and developed a method for automatically selecting informative acoustic features from a large number of feature candidates. To automatically select optimum features, we derived a differentiable objective function in proportion to mutual information based on the kernel method. Since the higher order correlations between acoustic features and Wiener-filter parameters are calculated using the kernel method, the statistical dependence of these variables is accurately calculated; thus, only meaningful acoustic features are selected. Through several experiments conducted on a mock sports field, we confirmed that the signal-to-distortion ratio score improved when various types of target sources were surrounded by loud cheering noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {768–779},
numpages = {12}
}

@article{10.1109/TASLP.2017.2661712,
author = {Spille, Constantin and Kollmeier, Birger and Meyer, Bernd T. and Spille, Constantin and Kollmeier, Birger and Meyer, Bernd T.},
title = {Combining Binaural and Cortical Features for Robust Speech Recognition},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2661712},
doi = {10.1109/TASLP.2017.2661712},
abstract = {The segregation of concurrent speakers and other sound sources is an important ability of the human auditory system, but is missing in most current systems for automatic speech recognition ASR, resulting in a large gap between human and machine performance. This study combines processing related to peripheral and cortical stages of the auditory pathway: A physiologically motivated binaural model estimates the positions of moving speakers to enhance the desired speech signal. Second, signals are converted to spectro-temporal Gabor features that resemble cortical speech representations and which have been shown to improve ASR in noisy conditions. Spectro-temporal Gabor features improve recognition results in all acoustic conditions under consideration compared with Mel-frequency cepstral coefficients. Binaural processing results in lower word error rates WERs in acoustic scenes with a concurrent speaker, whereas monaural processing should be preferred in the presence of a stationary masking noise. In-depth analysis of binaural processing identifies crucial processing steps such as localization of sound sources and estimation of the beamformer's noise coherence matrix, and shows how much each processing step affects the recognition performance in acoustic conditions with different complexity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {756–767},
numpages = {12}
}

@article{10.1109/TASLP.2017.2674966,
author = {Cumani, Sandro and Laface, Pietro and Cumani, Sandro and Laface, Pietro},
title = {Nonlinear I-Vector Transformations for PLDA-Based Speaker Recognition},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2674966},
doi = {10.1109/TASLP.2017.2674966},
abstract = {This paper proposes to estimate parametric nonlinear transformations of i-vectors for speaker recognition systems based on probabilistic linear discriminant analysis PLDA classification. The Gaussian PLDA model assumes that the i-vectors are distributed according to the standard normal distribution. However, it has been shown that the i-vectors are better modeled, for example, by Heavy-Tailed distributions, and that significant improvement of the classification performance can be obtained by whitening and length normalizing the i-vectors. In this paper, we propose to transform the i-vectors so that their distribution becomes more suitable to discriminate speakers using the PLDA model. This is performed by means of a sequence of affine and nonlinear transformations whose parameters are obtained by maximum likelihood estimation on the development set. Another contribution of this paper is the reduction of the mismatch between the development and evaluation i-vector length distributions by means of a scaling factor tuned for the estimated i-vector distribution, rather than by means of a blind length normalization. Relative improvement between 7% and 14% of the detection cost function was obtained with the proposed technique on the NIST SRE-2010 and SRE-2012 evaluation datasets, using both the traditional GMM/UBM and the hybrid DNN/GMM-based systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {908–919},
numpages = {12}
}

@article{10.1109/TASLP.2017.2656805,
author = {Wood, Sean U. N. and Rouat, Jean and Dupont, Stephane and Pironkov, Gueorgui and Wood, Sean U. N. and Rouat, Jean and Dupont, Stephane and Pironkov, Gueorgui},
title = {Blind Speech Separation and Enhancement With GCC-NMF},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2656805},
doi = {10.1109/TASLP.2017.2656805},
abstract = {We present a blind source separation algorithm named GCC-NMF that combines unsupervised dictionary learning via non-negative matrix factorization NMF with spatial localization via the generalized cross correlation GCC method. Dictionary learning is performed on the mixture signal, with separation subsequently achieved by grouping dictionary atoms, at each point in time, according to their spatial origins. The resulting source separation algorithm is simple yet flexible, requiring no prior knowledge or information. Separation quality is evaluated for three tasks using stereo recordings from the publicly available SiSEC signal separation evaluation campaign: 3 and 4 concurrent speakers in reverberant environments, speech mixed with real-world background noise, and noisy recordings of a moving speaker. Performance is quantified using perceptually motivated and SNR-based measures with the PEASS and BSS Eval toolkits, respectively. We evaluate the effects of model parameters on separation quality, and compare our approach with other unsupervised and semi-supervised speech separation and enhancement approaches. We show that GCC-NMF is a flexible source separation algorithm, outperforming task-specific approaches in each of the three settings, including both blind as well as several informed approaches that require prior knowledge or information.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {745–755},
numpages = {11}
}

@article{10.1109/TASLP.2017.2665341,
author = {Higuchi, Takuya and Ito, Nobutaka and Araki, Shoko and Yoshioka, Takuya and Delcroix, Marc and Nakatani, Tomohiro and Higuchi, Takuya and Ito, Nobutaka and Araki, Shoko and Yoshioka, Takuya and Delcroix, Marc and Nakatani, Tomohiro},
title = {Online MVDR Beamformer Based on Complex Gaussian Mixture Model With Spatial Prior for Noise Robust ASR},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2665341},
doi = {10.1109/TASLP.2017.2665341},
abstract = {This paper considers acoustic beamforming for noise robust automatic speech recognition. A beamformer attenuates background noise by enhancing sound components coming from a direction specified by a steering vector. Hence, accurate steering vector estimation is paramount for successful noise reduction. Recently, time-frequency masking has been proposed to estimate the steering vectors that are used for a beamformer. In particular, we have developed a new form of this approach, which uses a speech spectral model based on a complex Gaussian mixture model CGMM to estimate the time-frequency masks needed for steering vector estimation, and extended the CGMM-based beamformer to an online speech enhancement scenario. Our previous experiments showed that the proposed CGMM-based approach outperforms a recently proposed mask estimator based on a Watson mixture model and the baseline speech enhancement system of the CHiME-3 challenge. This paper provides additional experimental results for our online processing, which achieves performance comparable to that of batch processing with a suitable block-batch size. This online version reduces the CHiME-3 word error rate WER on the evaluation set from 8.37% to 8.06%. Moreover, in this paper, we introduce a probabilistic prior distribution for a spatial correlation matrix a CGMM parameter, which enables more stable steering vector estimation in the presence of interfering speakers. In practice, the performance of the proposed online beamformer degrades with observations that contain only noise or/and interference because of the failure of the CGMM parameter estimation. The introduced spatial prior enables the target speaker's parameter to avoid overfitting to noise or/and interference. Experimental results show that the spatial prior reduces the WER from 38.4% to 29.2% in a conversation recognition task compared with the CGMM-based approach without the prior, and outperforms a conventional online speech enhancement approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {780–793},
numpages = {14}
}

@article{10.1109/TASLP.2017.2662479,
author = {Nakamura, Eita and Yoshii, Kazuyoshi and Sagayama, Shigeki and Nakamura, Eita and Yoshii, Kazuyoshi and Sagayama, Shigeki},
title = {Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2662479},
doi = {10.1109/TASLP.2017.2662479},
abstract = {In a recent conference paper, we have reported a rhythm transcription method based on a merged-output hidden Markov model HMM that explicitly describes the multiple-voice structure of polyphonic music. This model solves a major problem of conventional methods that could not properly describe the nature of multiple voices as in polyrhythmic scores or in the phenomenon of loose synchrony between voices. In this paper, we present a complete description of the proposed model and develop an inference technique, which is valid for any merged-output HMMs, for which output probabilities depend on past events. We also examine the influence of the architecture and parameters of the method in terms of accuracies of rhythm transcription and voice separation and perform comparative evaluations with six other algorithms. Using MIDI recordings of classical piano pieces, we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non-polyrhythmic performances. This reveals the state-of-the-art methods of rhythm transcription for the first time in the literature. Publicly available source codes are also provided for future comparisons.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {794–806},
numpages = {13}
}

@article{10.1109/TASLP.2017.2672398,
author = {Zhang, Shiliang and Liu, Cong and Jiang, Hui and Wei, Si and Dai, Lirong and Hu, Yu and Shiliang Zhang and Cong Liu and Hui Jiang and Si Wei and Lirong Dai and Yu Hu},
title = {Nonrecurrent Neural Structure for Long-Term Dependence},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2672398},
doi = {10.1109/TASLP.2017.2672398},
abstract = {In this paper, we propose a novel neural network structure, namely feedforward sequential memory networks FSMN, to model long-term dependence in time series without using recurrent feedback. The proposed FSMN is a standard fully connected feedforward neural network equipped with some learnable memory blocks in its hidden layers. The memory blocks use a tapped-delay line structure to encode the long context information into a fixed-size representation as short-term memory mechanism which are somehow similar to the time-delay neural networks layers. We have evaluated the FSMNs in several standard benchmark tasks, including speech recognition and language modeling. Experimental results have shown that FSMNs outperform the conventional recurrent neural networks RNN while can be learned much more reliably and faster in modeling sequential signals like speech or language. Moreover, we also propose a compact feedforward sequential memory networks cFSMN by combining FSMN with low-rank matrix factorization and make a slight modification to the encoding method used in FSMNs in order to further simplify the network architecture. On the speech recognition Switchboard task, the proposed cFSMN structures can reduce the model size by 60% and speed up the learning by more than seven times while the model can still significantly outperform the popular bidirectional LSTMs for both frame-level cross-entropy criterion-based training and MMI-based sequence training.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {871–884},
numpages = {14}
}

@article{10.5555/3105672.3105691,
author = {Kuklasinski, Adam and Doclo, Simon and Holdt Jensen, Soren and Jensen, Jesper},
title = {Correction to "Maximum Likelihood PSD Estimation for Speech Enhancement in Reverberation and Noise" [Sep 16 1599-1612]},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
abstract = {Presents corrections to the paper, "A Novel Approach Based on Marine Radar Data Analysis for High-Resolution Bathymetry Map Generation."},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {687},
numpages = {1}
}

@article{10.1109/TASLP.2017.2653941,
author = {Omachi, Motoi and Ogawa, Tetsuji and Kobayashi, Tetsunori and Omachi, Motoi and Ogawa, Tetsuji and Kobayashi, Tetsunori},
title = {Associative Memory Model-Based Linear Filtering and Its Application to Tandem Connectionist Blind Source Separation},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2653941},
doi = {10.1109/TASLP.2017.2653941},
abstract = {We propose a blind source separation method that yields high-quality speech with low distortion. Time-frequency TF masking can effectively reduce interference, but it produces nonlinear distortion. By contrast, linear filtering using a separation matrix such as independent vector analysis IVA can avoid nonlinear distortion, but the separation performance is reduced under reverberant conditions. The tandem connectionist approach combines several separation methods and it has been used frequently to compensate for the disadvantages of these methods. In this study, we propose associative memory model AMM-based linear filtering and a tandem connectionist framework, which applies TF masking followed by linear filtering. By using AMM trained with speech spectra to optimize the separation matrix, the proposed linear filtering method considers the properties of speech that are not considered explicitly in IVA, such as the harmonic components of spectra. TF masking is applied in the proposed tandem connectionist framework to reduce unwanted components that hinder the optimization of the separation matrix, and it is approximated by using a linear separation matrix to reduce nonlinear distortion. The results obtained in simultaneous speech separation experiments demonstrate that although the proposed linear filtering method can increase the signal-to-distortion ratio SDR and signal-to-interference ratio SIR compared with IVA, the proposed tandem connectionist framework can obtain greater increases in SDR and SIR, and it reduces the phoneme error rate more than the proposed linear filtering method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {637–650},
numpages = {14}
}

@article{10.1109/TASLP.2016.2647713,
author = {Bahari, Mohamad Hasan and Bertrand, Alexander and Moonen, Marc and Bahari, Mohamad Hasan and Bertrand, Alexander and Moonen, Marc},
title = {Blind Sampling Rate Offset Estimation for Wireless Acoustic Sensor Networks Through Weighted Least-Squares Coherence Drift Estimation},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2647713},
doi = {10.1109/TASLP.2016.2647713},
abstract = {Microphone arrays allow to exploit the spatial coherence between simultaneously recorded microphone signals, e.g., to perform speech enhancement, i.e., to extract a speech signal and reduce background noise. However, in systems where the microphones are not sampled in a synchronous fashion, as it is often the case in wireless acoustic sensor networks, a sampling rate offset SRO exists between signals recorded in different nodes, which severely affects the speech enhancement performance. To avoid this performance reduction, the SRO should be estimated and compensated for. In this paper, we propose a new approach to blind SRO estimation for an asynchronous wireless acoustic sensor network, which exploits the phase drift of the coherence between the asynchronous microphones signals. We utilize the fact that the SRO causes a linearly increasing time delay between two signals and hence a linearly increasing phase-shift in the short-time Fourier transform domain. The increasing phase shift, observed as a phase drift of the coherence between the signals, is used in a weighted least-squares framework to estimate the SRO. This method is referred to as least-squares coherence drift LCD. Experimental results in different real-world recording and simulated scenarios show the effectiveness of LCD compared to different benchmark methods. The LCD is effective even for short signal segments. We finally demonstrate that the use of the LCD within a conventional compensation approach eliminates the performance loss due to SRO in a speech enhancement algorithm based on the multichannel Wiener filter.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {674–686},
numpages = {13}
}

@article{10.1109/TASLP.2016.2643280,
author = {Do, Quoc Truong and Toda, Tomoki and Neubig, Graham and Sakti, Sakriani and Nakamura, Satoshi and Quoc Truong Do and Toda, Tomoki and Neubig, Graham and Sakti, Sakriani and Nakamura, Satoshi},
title = {Preserving Word-Level Emphasis in Speech-to-Speech Translation},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2643280},
doi = {10.1109/TASLP.2016.2643280},
abstract = {Speech-to-speech translation S2ST is a technology that translates speech across languages, which can remove barriers in cross-lingual communication. In the conventional S2ST systems, the linguistic meaning of speech was translated, but paralinguistic information conveying other features of the speech such as emotion or emphasis were ignored. In this paper, we propose a method to translate paralinguistic information, specifically focusing on emphasis. The method consists of a series of components that can accurately translate emphasis using all acoustic features of speech. First, linear-regression hidden semi-Markov models LRHSMMs are used to estimate a real-numbered emphasis value for every word in an utterance, resulting in a sequence of values for the utterance. After that the emphasis translation module translates the estimated emphasis sequence into a target language emphasis sequence using a conditional random field model considering the features of emphasis levels, words, and part-of-speech tags. Finally, the speech synthesis module synthesizes emphasized speech with LR-HSMMs, taking into account the translated emphasis sequence and transcription. The results indicate that our translation model can translate emphasis information, correctly emphasizing words in the target language with 91.6% F-measure by objective evaluation. A listening test with human subjects further showed that they could identify the emphasized words with 87.8% F-measure, and that the naturalness of the audio was preserved.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {544–556},
numpages = {13}
}

@article{10.1109/TASLP.2016.2641901,
author = {Sharma, Bidisha and Prasanna, S. R. Mahadeva and Sharma, Bidisha and Mahadeva Prasanna, S. R.},
title = {Sonority Measurement Using System, Source, and Suprasegmental Information},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2641901},
doi = {10.1109/TASLP.2016.2641901},
abstract = {Sonorant sounds are characterized by regions with a prominent formant structure, high energy, and high degree of periodicity. In this work, the vocal-tract system, excitation source, and suprasegmental features derived from the speech signal are analyzed to measure the sonority information present in each of them. Vocal-tract system information is extracted from the Hilbert envelope of the numerator of the group-delay function. It is derived from a zero-time-windowed speech signal that provides a better resolution of the formants. A 5-D feature set is computed from the estimated formants to measure the prominence of the spectral peaks. A feature representing strength of excitation is derived from the Hilbert envelope of linear prediction residual, which represents the source information. Correlation of speech over ten consecutive pitch periods is used as the suprasegmental feature representing periodicity information. The combination of evidence from the three different aspects of speech provides a better discrimination among different sonorant classes, compared to the baseline mel frequency cepstral coefficient features. The usefulness of the proposed sonority feature is demonstrated in the tasks of phoneme recognition and sonorant classification.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {505–518},
numpages = {14}
}

@article{10.1109/TASLP.2016.2637280,
author = {Wang, Zhongqing and Lee, Sophia Yat Mei and Li, Shoushan and Zhou, Guodong and Zhongqing Wang and Mei Lee, Sophia Yat and Shoushan Li and Guodong Zhou},
title = {Emotion Analysis in Code-Switching Text With Joint Factor Graph Model},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2637280},
doi = {10.1109/TASLP.2016.2637280},
abstract = {Previous research on emotions analysis has placed much emphasis in monolingual instead of bilingual text. However, emotions on social media platforms are often found in bilingual or code-switching posts. Different from monolingual text, emotions in code-switching text can be expressed in both monolingual and bilingual forms. Moreover, more than one emotion can be expressed within a single post; yet they tend to be related in some ways which offers some implications. It is thus necessary to consider the correlation between different emotions. In this paper, a joint factor graph model is proposed to address this issue. In particular, attribute functions of the factor graph model are utilized to learn both monolingual and bilingual information from each post, factor functions are used to explore the relationship among different emotions, and a belief propagation algorithm is employed to learn and predict the model. Empirical studies demonstrate the importance of emotion analysis in code-switching text and the effectiveness of our proposed joint learning model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {469–480},
numpages = {12}
}

@article{10.1109/TASLP.2017.2651373,
author = {Farmani, Mojtaba and Pedersen, Michael Syskind and Tan, Zheng-Hua and Jensen, Jesper and Farmani, Mojtaba and Pedersen, Michael Syskind and Zheng-Hua Tan and Jensen, Jesper},
title = {Informed Sound Source Localization Using Relative Transfer Functions for Hearing Aid Applications},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2651373},
doi = {10.1109/TASLP.2017.2651373},
abstract = {Recent hearing aid systems HASs can connect to a wireless microphone worn by the talker of interest. This feature gives the HASs access to a noise-free version of the target signal. In this paper, we address the problem of estimating the target sound direction of arrival DoA for a binaural HAS given access to the noise-free content of the target signal. To estimate the DoA, we present a maximum-likelihood framework which takes the shadowing effect of the user's head on the received signals into account by modeling the relative transfer functions RTFs between the HAS's microphones. We propose three different RTF models which have different degrees of accuracy and individualization. Furthermore, we show that the proposed DoA estimators can be formulated in terms of inverse discrete Fourier transforms to evaluate the likelihood function computationally efficiently. We extensively assess the performance of the proposed DoA estimators for various DoAs, signal to noise ratios, and in different noisy and reverberant situations. The results show that the proposed estimators improve the performance markedly over other recently proposed “informed” DoA estimator.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {611–623},
numpages = {13}
}

@article{10.1109/TASLP.2016.2636445,
author = {He, Qi and Bao, Feng and Bao, Changchun and Qi He and Feng Bao and Changchun Bao},
title = {Multiplicative Update of Auto-Regressive Gains for Codebook-Based Speech Enhancement},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2636445},
doi = {10.1109/TASLP.2016.2636445},
abstract = {In this paper, we present a novel method for estimating short-term linear predictive parameters of speech and noise in the codebook-driven Wiener filtering speech enhancement method. We only use pretrained spectral shape codebook of speech to model the a priori information about linear predictive coefficients of speech, and the spectral shape of noise is estimated online directly instead of using noise codebook to solve the problem of noise classification. Differing from the existing codebook-driven methods that the linear predictive gains of speech and noise are estimated by maximum-likelihood method, in the proposed method we exploit a multiplicative update rule to estimate the linear predictive gains more accurately. The estimated gains can help to reserve more speech components in the enhanced speech. Meanwhile, the Bayesian parameter-estimator without the noise codebook is also developed. Moreover, we develop an improved codebook-driven Wiener filter combined with the speech-presence probability, so that the proposed method achieves the goal of removing the residual noise between the harmonics of noisy speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {457–468},
numpages = {12}
}

@article{10.1109/TASLP.2016.2641904,
author = {Doire, Clement Samuel Joseph and Brookes, Mike and Naylor, Patrick A. and Hicks, Christopher M. and Betts, Dave and Dmour, Mohammad A. and Jensen, Soren Holdt and Doire, Clement Samuel Joseph and Brookes, Mike and Naylor, Patrick A. and Hicks, Christopher M. and Betts, Dave and Dmour, Mohammad A. and Jensen, Soren Holdt},
title = {Single-Channel Online Enhancement of Speech Corrupted by Reverberation and Noise},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2641904},
doi = {10.1109/TASLP.2016.2641904},
abstract = {This paper proposes an online single-channel speech enhancement method designed to improve the quality of speech degraded by reverberation and noise. Based on an autoregressive model for the reverberation power and on a hidden Markov model for clean speech production, a Bayesian filtering formulation of the problem is derived and online joint estimation of the acoustic parameters and mean speech, reverberation, and noise powers is obtained in mel-frequency bands. From these estimates, a real-valued spectral gain is derived and spectral enhancement is applied in the short-time Fourier transform STFT domain. The method yields state-of-the-art performance and greatly reduces the effects of reverberation and noise while improving speech quality and preserving speech intelligibility in challenging acoustic environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {572–587},
numpages = {16}
}

@article{10.1109/TASLP.2016.2635445,
author = {Lee, Hung-Yi and Tseng, Bo-Hsiang and Wen, Tsung-Hsien and Tsao, Yu and Hung-Yi Lee and Bo-Hsiang Tseng and Tsung-Hsien Wen and Yu Tsao},
title = {Personalizing Recurrent-Neural-Network-Based Language Model by Social Network},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2635445},
doi = {10.1109/TASLP.2016.2635445},
abstract = {With the popularity of mobile devices, personalized speech recognizers have become more attainable and are highly attractive. Since each mobile device is used primarily by a single user, it is possible to have a personalized recognizer that well matches the characteristics of the individual user. Although acoustic model personalization has been investigated for decades, much less work has been reported on personalizing language models, presumably because of the difficulties in collecting sufficient personalized corpora. In this paper, we propose a general framework for personalizing recurrent-neural-network-based language models RNNLMs using data collected from social networks, including the posts of many individual users and friend relationships among the users. Two major directions for this are model-based and feature-based RNNLM personalization. In model-based RNNLM personalization, the RNNLM parameters are fine-tuned to an individual user's wording patterns by incorporating social texts posted by the target user and his or her friends. For the feature-based approach, the RNNLM model parameters are fixed across users, but the RNNLM input features are instead augmented with personalized information. Both approaches not only drastically reduce the model perplexity, but also moderately reduce word error rates in n-best rescoring tests.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {519–530},
numpages = {12}
}

@article{10.1109/TASLP.2016.2639323,
author = {Tang, Zhiyuan and Li, Lantian and Wang, Dong and Vipperla, Ravichander and Zhiyuan Tang and Lantian Li and Dong Wang and Vipperla, Ravichander},
title = {Collaborative Joint Training With Multitask Recurrent Model for Speech and Speaker Recognition},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2639323},
doi = {10.1109/TASLP.2016.2639323},
abstract = {Automatic speech and speaker recognition are traditionally treated as two independent tasks and are studied separately. The human brain in contrast deciphers the linguistic content, and the speaker traits from the speech in a collaborative manner. This key observation motivates the work presented in this paper. A collaborative joint training approach based on multitask recurrent neural network models is proposed, where the output of one task is backpropagated to the other tasks. This is a general framework for learning collaborative tasks and fits well with the goal of joint learning of automatic speech and speaker recognition. Through a comprehensive study, it is shown that the multitask recurrent neural net models deliver improved performance on both automatic speech and speaker recognition tasks as compared to single-task systems. The strength of such multitask collaborative learning is analyzed, and the impact of various training configurations is investigated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {493–504},
numpages = {12}
}

@article{10.1109/TASLP.2017.2651406,
author = {Ming, Ji and Crookes, Danny and Ji Ming and Crookes, Danny},
title = {Speech Enhancement Based on Full-Sentence Correlation and Clean Speech Recognition},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2651406},
doi = {10.1109/TASLP.2017.2651406},
abstract = {Conventional speech enhancement methods, based on frame, multiframe, or segment estimation, require knowledge about the noise. This paper presents a new method that aims to reduce or effectively remove this requirement. It is shown that by using the zero-mean normalized correlation coefficient ZNCC as the comparison measure, and by extending the effective length of speech segment matching to sentence-long speech utterances, it is possible to obtain an accurate speech estimate from noise without requiring specific knowledge about the noise. The new method, thus, could be used to deal with unpredictable noise or noise without proper training data. This paper is focused on realizing and evaluating this potential. We propose a novel realization that integrates full-sentence speech correlation with clean speech recognition, formulated as a constrained maximization problem, to overcome the data sparsity problem. Then we propose an efficient implementation algorithm to solve this constrained maximization problem to produce speech sentence estimates. For evaluation, we build the new system on one training dataset and test it on two different test datasets across two databases, for a range of different noises including highly nonstationary ones. It is shown that the new approach, without any estimation of the noise, is able to significantly outperform conventional methods that use optimized noise tracking, in terms of various objective measures including automatic speech recognition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {531–543},
numpages = {13}
}

@article{10.1109/TASLP.2017.2651398,
author = {Girin, Laurent and Hueber, Thomas and Alameda-Pineda, Xavier and Girin, Laurent and Hueber, Thomas and Alameda-Pineda, Xavier},
title = {Extending the Cascaded Gaussian Mixture Regression Framework for Cross-Speaker Acoustic-Articulatory Mapping},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2651398},
doi = {10.1109/TASLP.2017.2651398},
abstract = {This paper addresses the adaptation of an acoustic-articulatory inversion model of a reference speaker to the voice of another source speaker, using a limited amount of audio-only data. In this study, the articulatory-acoustic relationship of the reference speaker is modeled by a Gaussian mixture model and inference of articulatory data from acoustic data is made by the associated Gaussian mixture regression GMR. To address speaker adaptation, we previously proposed a general framework called Cascaded-GMR C-GMR which decomposes the adaptation process into two consecutive steps: spectral conversion between source and reference speaker and acoustic-articulatory inversion of converted spectral trajectories. In particular, we proposed the integrated C-GMR technique IC-GMR in which both steps are tied together in the same probabilistic model. In this paper, we extend the C-GMR framework with another model called Joint-GMR J-GMR. Contrary to the IC-GMR, this model aims at exploiting all potential acoustic-articulatory relationships, including those between the source speaker's acoustics and the reference speaker's articulation. We present the full derivation of the exact expectation-maximization EM training algorithm for the J-GMR. It exploits the missing data methodology of machine learning to deal with limited adaptation data. We provide an extensive evaluation of the J-GMR on both synthetic acoustic-articulatory data and on the multispeaker MOCHA EMA database. We compare the J-GMR performance to other models of the C-GMR framework, notably the IC-GMR, and discuss their respective merits.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {662–673},
numpages = {12}
}

@article{10.1109/TASLP.2017.2651361,
author = {Sheikh, Imran and Fohr, Dominique and Illina, Irina and Linares, Georges and Sheikh, Imran and Fohr, Dominique and Illina, Irina and Linares, Georges},
title = {Modelling Semantic Context of OOV Words in Large Vocabulary Continuous Speech Recognition},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2651361},
doi = {10.1109/TASLP.2017.2651361},
abstract = {The diachronic nature of broadcast news data leads to the problem of out-of-vocabulary OOV words in large vocabulary continuous speech recognition LVCSR systems. Analysis of OOV words reveals that a majority of them are proper names PNs. However, PNs are important for automatic indexing of audio-video content and for obtaining reliable automatic transcriptions. In this paper, we focus on the problem of OOV PNs in diachronic audio documents. To enable the recovery of the PNs missed by the LVCSR system, relevant OOV PNs are retrieved by exploiting the semantic context of the LVCSR transcriptions. For retrieval of OOV PNs, we explore topic and semantic context derived from latent Dirichlet allocation LDA topic models, continuous word vector representations and the neural bag-of-words NBOW model which is capable of learning task specific word and context representations. We propose a neural bag-of-weighted words NBOW2 model which learns to assign higher weights to words that are important for retrieval of an OOV PN. With experiments on French broadcast news videos, we show that the NBOW and NBOW2 models outperform the methods based on raw embeddings from LDA and Skip-gram models. Combining the NBOW and NBOW2 models gives a faster convergence during training. Second pass speech recognition experiments, in which the LVCSR vocabulary and language model are updated with the retrieved OOV PNs, demonstrate the effectiveness of the proposed context models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {598–610},
numpages = {13}
}

@article{10.1109/TASLP.2017.2651391,
author = {Vikram, C. M. and Prasanna, S. R. Mahadeva and Vikram, C. M. and Mahadeva Prasanna, S. R.},
title = {Epoch Extraction From Telephone Quality Speech Using Single Pole Filter},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2651391},
doi = {10.1109/TASLP.2017.2651391},
abstract = {Epoch extraction from speech involves the suppression of vocal tract resonances, either by linear prediction based inverse filtering or filtering at very low frequency. Degradations due to channel effect and significant attenuation of low frequency components &lt;;300 Hz create challenges for the epoch extraction from telephone quality speech. An epoch extraction method is proposed that considers the vertical striations present in the time-frequency representation of voiced speech as the representative candidates for the epochs. Time-frequency representation with better localized vertical striations is estimated using single pole filter based filter bank. The time marginal of time-frequency representation is computed to locate the epochs. The proposed algorithm is evaluated on the database of five speakers, which provide simultaneous speech and electroglottographic recordings. Telephone quality speech is simulated using G.191 software tools. The identification rate of the state-of-the-art methods degrades substantially for the telephone quality speech whereas that of the proposed method remains the same, comparable to that of clean speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {624–636},
numpages = {13}
}

@article{10.1109/TASLP.2017.2651377,
author = {Sizov, Aleksandr and Lee, Kong Aik and Kinnunen, Tomi and Sizov, Aleksandr and Kong Aik Lee and Kinnunen, Tomi},
title = {Direct Optimization of the Detection Cost for I-Vector-Based Spoken Language Recognition},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2651377},
doi = {10.1109/TASLP.2017.2651377},
abstract = {We explore a method to boost discriminative capabilities of probabilistic linear discriminant analysis PLDA model without losing its generative advantages. We show a sequential projection and training steps leading to a classifier that operates in the original i-vector space but is discriminatively trained in a low-dimensional PLDA latent subspace. We use extended Baum-Welch technique to optimize the model with respect to two objective functions for discriminative training. One of them is the well-known maximum mutual information objective, while the other one is a new objective that we propose to approximate the language detection cost. We evaluate the performance on NIST language recognition evaluation LRE 2015 and our development dataset comprised of the utterances from previous LREs. We improve the detection cost by 10% and 6% relative compared to our fine-tuned generative and discriminative baselines, and by 10% over the best of our previously reported results. The proposed approximation method of the cost function and PLDA subspace training are applicable for a broad range of tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {588–597},
numpages = {10}
}

@article{10.1109/TASLP.2016.2639322,
author = {Bellur, Ashwin and Elhilali, Mounya and Bellur, Ashwin and Elhilali, Mounya},
title = {Feedback-Driven Sensory Mapping Adaptation for Robust Speech Activity Detection},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2639322},
doi = {10.1109/TASLP.2016.2639322},
abstract = {Parsing natural acoustic scenes using computational methodologies poses many challenges. Given the rich and complex nature of the acoustic environment, data mismatch between train and test conditions is a major hurdle in data-driven audio processing systems. In contrast, the brain exhibits a remarkable ability at segmenting acoustic scenes with relative ease. When tackling challenging listening conditions that are often faced in everyday life, the biological system relies on a number of principles that allow it to effortlessly parse its rich soundscape. In the current study, we leverage a key principle employed by the auditory system: its ability to adapt the neural representation of its sensory input in a high-dimensional space. We propose a framework that mimics this process in a computational model for robust speech activity detection. The system employs a 2-D Gabor filter bank whose parameters are returned offline to improve the separability between the feature representation of speech and nonspeech sounds. This retuning process, driven by feedback from statistical models of speech and nonspeech classes, attempts to minimize the misclassification risk of mismatched data, with respect to the original statistical models. We hypothesize that this risk minimization procedure results in an emphasis of unique speech and nonspeech modulations in the high-dimensional space. We show that such an adapted system is indeed robust to other novel conditions, with a marked reduction in equal error rates for a variety of databases with additive and convolutive noise distortions. We discuss the lessons learned from biology with regard to adapting to an ever-changing acoustic environment and the impact on building truly intelligent audio processing systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {481–492},
numpages = {12}
}

@article{10.1109/TASLP.2017.2655259,
author = {Cherkassky, Dani and Gannot, Sharon and Cherkassky, Dani and Gannot, Sharon},
title = {Blind Synchronization in Wireless Acoustic Sensor Networks},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2655259},
doi = {10.1109/TASLP.2017.2655259},
abstract = {The challenge of blindly resynchronizing the data acquisition processes in a wireless acoustic sensor network WASN is addressed in this paper. The sampling rate offset SRO is precisely modeled as a time scaling. The applicability of a wideband correlation processor for estimating the SRO, even in a reverberant and multiple source environment, is presented. An explicit expression for the ambiguity function, which in our case involves time scaling of the received signals, is derived by applying truncated band-limited interpolation. We then propose the recursive band-limited interpolation RBI algorithm for recursive SRO estimation. A complete resynchronization scheme utilizing the RBI algorithm, in parallel with the SRO compensation module, is presented. The resulting resynchronization method operates in the time domain in a sequential manner and is, thus, capable of tracking a potentially time-varying SRO. We compared the performance of the proposed RBI algorithm to other available methods in a simulation study. The importance of resynchronization in a beamforming application is demonstrated by both a simulation study and experiments with a real WASN. Finally, we present an experimental study evaluating the expected SRO level between typical data acquisition devices.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {651–661},
numpages = {11}
}

@article{10.1109/TASLP.2016.2644262,
author = {Li, Zhenghua and Chao, Jiayuan and Zhang, Min and Chen, Wenliang and Zhang, Meishan and Fu, Guohong and Zhenghua Li and Jiayuan Chao and Min Zhang and Wenliang Chen and Meishan Zhang and Guohong Fu},
title = {Coupled POS Tagging on Heterogeneous Annotations},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {25},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2644262},
doi = {10.1109/TASLP.2016.2644262},
abstract = {The limited scale and genre coverage of labeled data greatly hinders the effectiveness of supervised models, especially when analyzing spoken languages, such as texts transcribed from speech and informal text including tweets and product comments in Internet. In order to effectively utilize multiple labeled datasets with heterogeneous annotations for the same task, this paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously, using Chinese part-of-speech POS tagging as our case study. The key idea is to bundle two sets of POS tags together e.g., “[NN, n]n, and build a conditional random field CRF based tagging model in the enlarged space of bundled tags with the help of ambiguous labeling. To train our model on two nonoverlapping datasets that each has only one-side tags, we transform a one-side tag into a set of bundled tags by concatenating the tag with every possible tag at the missing side according to a predefined context-free tag-to-tag mapping function, thus producing ambiguous labeling as weak supervision. We design and investigate four different context-free tag-to-tag mapping functions, and find out that the coupled model achieves its best performance when each one-side tag is mapped to all tags at the other side namely complete mapping, indicating that the model can effectively learn the loose mapping between the two heterogeneous annotations, without the need of manually designed mapping rules. Moreover, we propose a context-aware online pruning strategy that can more accurately capture mapping relationships between annotations based on contextual evidences and thus effectively solve the severe inefficiency problem with our coupled model under complete mapping, making it comparable with the baseline CRF model. Experiments on benchmark datasets show that our coupled model significantly outperforms the state-of-the-art baselines on both one-side POS tagging and annotation conversion tasks. The codes and newly annotated data are released for research usage.1},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {557–571},
numpages = {15}
}

@article{10.1109/TASLP.2016.2633799,
author = {Ma, Yaping and Xiao, Yegui and Yaping Ma and Yegui Xiao},
title = {A New Strategy for Online Secondary-Path Modeling of Narrowband Active Noise Control},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2633799},
doi = {10.1109/TASLP.2016.2633799},
abstract = {Online secondary-path modeling SPM based on the use of random auxiliary noise is a very promising and attractive technique for implementing active noise control ANC. In this paper, a new strategy is proposed for the online SPM of narrowband ANC NANC. A bandpass filter bank consisting of IIR notch filters is applied to the residual noise to separate the uncancelled sinusoidal noise from other broadband noises due to the injected auxiliary noise and an additive noise. The extracted sinusoidal noise is used simultaneously to scale the auxiliary noise, to clean the desired signal for the SPM subsystem, and to update the controller. The benefits of this strategy are twofold. It can effectively reduce the contribution of injected auxiliary noise to the residual noise power. Meanwhile, it can also considerably raise the convergence rate of overall system as well as the accuracy of online SPM. Preliminary analysis is also provided to support the proposed strategy. Extensive simulations with synthetic/real secondary paths and noises are performed to demonstrate the superiority of the proposed SPM strategy over the conventional techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {420–434},
numpages = {15}
}

@article{10.1109/TASLP.2016.2633811,
author = {Samarasinghe, Prasanga N. and Abhayapala, Thushara D. and Chen, Hanchi and Samarasinghe, Prasanga N. and Abhayapala, Thushara D. and Hanchi Chen},
title = {Estimating the Direct-to-Reverberant Energy Ratio Using a Spherical Harmonics-Based Spatial Correlation Model},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2633811},
doi = {10.1109/TASLP.2016.2633811},
abstract = {The direct-to-reverberant ratio DRR, which describes the energy ratio between the direct and reverberant component of a soundfield, is an important parameter in many audio applications. In this paper, we present a multichannel algorithm, which utilizes the blind recordings of a spherical microphone array to estimate the DRR of interest. The algorithm is developed based on a spatial correlation model formulated in the spherical harmonics domain. This model expresses the cross correlation matrix of the recorded soundfield coefficients in terms of two spatial correlation matrices, one for direct sound and the other for reverberation. While the direct path arrives from the source, the reverberant path is considered to be a nondiffuse soundfield with varying directional gains. The direct and reverberant sound energies are estimated from the aforementioned spatial correlation model, which then leads to the DRR estimation. The practical feasibility of the proposed algorithm was evaluated using the speech corpus of the acoustic characterization of environments challenge. The experimental results revealed that the proposed method was able to effectively estimate the DRR of a large collection of reverberant speech recordings including various environmental noise types, room types and speakers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {310–319},
numpages = {10}
}

@article{10.1109/TASLP.2016.2637298,
author = {Cheer, Jordan and Daley, Stephen and Cheer, Jordan and Daley, Stephen},
title = {An Investigation of Delayless Subband Adaptive Filtering for Multi-Input Multi-Output Active Noise Control Applications},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2637298},
doi = {10.1109/TASLP.2016.2637298},
abstract = {The broadband control of noise and vibration using multi-input, multi-output MIMO active control systems has a potentially wide variety of applications. However, the performance of MIMO systems is often limited in practice by high computational demand and slow convergence speeds. In the somewhat simpler context of single-input, single-output broadband control, these problems have been overcome through a variety of methods including subband adaptive filtering. This paper presents an extension of the subband adaptive filtering technique to the MIMO active control problem and presents a comprehensive study of both the computational requirements and control performance. The implementation of the MIMO filtered-x LMS algorithm using subband adaptive filtering is described and the details of two specific implementations are presented. The computational demands of the two MIMO subband active control algorithms are then compared to that of the standard full-band algorithm. This comparison shows that as the number of subbands employed in the subband algorithms is increased, the computational demand is significantly reduced compared to the full-band implementation provided that a restructured analysis filter-bank is employed. An analysis of the convergence of the MIMO subband adaptive algorithm is then presented and this demonstrates that although the convergence of the control filter coefficients is dependent on the eigenvalue spread of the subband Hessian matrix, which reduces as the number of subbands is increased, the convergence of the cost function is limited for large numbers of subbands due to the simultaneous increase in the weight stacking distortion. The performance of the two MIMO subband algorithms and the standard full-band algorithm has then been assessed through a series of time-domain simulations of a practical active control system and it has been shown that the subband algorithms are able to achieve a significant increase in the convergence speed compared to the full-band implementation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {359–373},
numpages = {15}
}

@article{10.1109/TASLP.2016.2632528,
author = {Badawy, Dalia El and Duong, Ngoc Q. K. and Ozerov, Alexey and El Badawy, Dalia and Duong, Ngoc Q. K. and Ozerov, Alexey},
title = {On-the-Fly Audio Source Separation—A Novel User-Friendly Framework},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2632528},
doi = {10.1109/TASLP.2016.2632528},
abstract = {This paper addresses the challenging problem of single-channel audio source separation. We introduce a novel user-guided framework where source models that govern the separation process are learned on-the-fly from audio examples retrieved online. The user only provides the search keywords that describe the sources in the mixture. In this framework, the generic spectral characteristics of each source are modeled by a universal sound class model learned from the retrieved examples via nonnegative matrix factorization. We propose several group sparsity-inducing constraints in order to efficiently exploit a relevant subset of the universal model adapted to the mixture to be separated. We then derive the corresponding multiplicative update rules for parameter estimation. Separation results obtained from automated and user tests on mixtures containing various types of sounds confirm the effectiveness of the proposed framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {261–272},
numpages = {12}
}

@article{10.1109/TASLP.2016.2634123,
author = {Granell, Emilio and Martinez-Hinarejos, Carlos-D. and Granell, Emilio and Martinez-Hinarejos, Carlos-D},
title = {Multimodal Crowdsourcing for Transcribing Handwritten Documents},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2634123},
doi = {10.1109/TASLP.2016.2634123},
abstract = {Transcription of handwritten documents is an important research topic for multiple applications, such as document classification or information extraction. In the case of historical documents, their transcription allows to preserve cultural heritage because of the amount of historical data contained in those documents. The transcription process can employ state-of-the-art handwritten text recognition systems in order to obtain an initial transcription. This transcription is usually not good enough for the quality standards, but that may speed up the final transcription of the expert. In this framework, the use of collaborative transcription applications crowdsourcing has risen in the recent years, but these platforms are mainly limited by the use of non-mobile devices. Thus, the recruiting initiatives get reduced to a smaller set of potential volunteers. In this paper, an alternative that allows the use of mobile devices is presented. The proposal consists of using speech dictation of handwritten text lines. Then, by using multimodal combination of speech and handwritten text images, a draft transcription can be obtained, presenting more quality than that obtained by only using handwritten text recognition. The speech dictation platform is implemented as a mobile device application, which allows for a wider range of population for recruiting volunteers. A real acquisition on the contents of a Spanish historical handwritten book was obtained with the platform. This data was used to perform experiments on the behaviour of the proposed framework. Some experiments were performed to study how to optimise the collaborators effort in terms of number of collaborations, including how many lines and which lines should be selected for the speech dictation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {409–419},
numpages = {11}
}

@article{10.1109/TASLP.2016.2633803,
author = {Rehr, Robert and Gerkmann, Timo and Rehr, Robert and Gerkmann, Timo},
title = {An Analysis of Adaptive Recursive Smoothing with Applications to Noise PSD Estimation},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2633803},
doi = {10.1109/TASLP.2016.2633803},
abstract = {First-order recursive smoothing filters using a fixed smoothing constant are in general unbiased estimators of the mean of a random process. Due to their efficiency in terms of memory consumption and computational complexity, they are of high practical relevance and are also often used to track the first-order moment of nonstationary random processes. However, in single-channel speech-enhancement applications, e.g., for the estimation of the noise power spectral density, an adaptively changing smoothing factor is often employed. Here, the adaptivity is used to avoid speech leakage by raising the smoothing factor when speech is likely to be present. In this paper, we investigate the properties of adaptive first-order recursive smoothing factors applied to noise power spectral density estimators. We show that in contrast to a smoothing with fixed smoothing factors, adaptive smoothing is in general biased. We propose different methods to quantify and to compensate for the bias. We demonstrate that the proposed correction methods reduce the estimation error and increases the perceptual evaluation of speech quality scores in a speech enhancement framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {397–408},
numpages = {12}
}

@article{10.1109/TASLP.2016.2630305,
author = {Bell, Peter and Swietojanski, Pawel and Renals, Steve and Bell, Peter and Swietojanski, Pawel and Renals, Steve},
title = {Multitask Learning of Context-Dependent Targets in Deep Neural Network Acoustic Models},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2630305},
doi = {10.1109/TASLP.2016.2630305},
abstract = {This paper investigates the use of multitask learning to improve context-dependent deep neural network DNN acoustic models. The use of hybrid DNN systems with clustered triphone targets is now standard in automatic speech recognition. However, we suggest that using a single set of DNN targets in this manner may not be the most effective choice, since the targets are the result of a somewhat arbitrary clustering process that may not be optimal for discrimination. We propose to remedy this problem through the addition of secondary tasks predicting alternative content-dependent or context-independent targets. We present a comprehensive set of experiments on a lecture recognition task showing that DNNs trained through multitask learning in this manner give consistently improved performance compared to standard hybrid DNNs. The technique is evaluated across a range of data and output sizes. Improvements are seen when training uses the cross entropy criterion and also when sequence training is applied.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {238–247},
numpages = {10}
}

@article{10.1109/TASLP.2016.2632521,
author = {Zhao, Rui and Mao, Kezhi and Rui Zhao and Kezhi Mao},
title = {Topic-Aware Deep Compositional Models for Sentence Classification},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2632521},
doi = {10.1109/TASLP.2016.2632521},
abstract = {In recent years, deep compositional models have emerged as a popular technique for representation learning of sentence in computational linguistic and natural language processing. These models normally train various forms of neural networks on top of pretrained word embeddings using a task-specific corpus. However, most of these works neglect the multisense nature of words in the pretrained word embeddings. In this paper we introduce topic models to enrich the word embeddings for multisenses of words. The integration of the topic model with various semantic compositional processes leads to topic-aware convolutional neural network and topic-aware long short term memory networks. Different from previous multisense word embeddings models that assign multiple independent and sense-specific embeddings to each word, our proposed models are lightweight and have flexible frameworks that regard word sense as the composition of two parts: a general sense derived from a large corpus and a topic-specific sense derived from a task-specific corpus. In addition, our proposed models focus on semantic composition instead of word understanding. With the help of topic models, we can integrate the topic-specific sense at word-level before the composition and sentence-level after the composition. Comprehensive experiments on five public sentence classification datasets are conducted and the results show that our proposed topic-aware deep compositional models produce competitive or better performance than other text representation learning methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {248–260},
numpages = {13}
}

@article{10.1109/TASLP.2016.2633802,
author = {Remaggi, Luca and Jackson, Philip J. B. and Coleman, Philip and Wang, Wenwu and Remaggi, Luca and Jackson, Philip J. B. and Coleman, Philip and Wenwu Wang},
title = {Acoustic Reflector Localization: Novel Image Source Reversion and Direct Localization Methods},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2633802},
doi = {10.1109/TASLP.2016.2633802},
abstract = {Acoustic reflector localization is an important issue in audio signal processing, with direct applications in spatial audio, scene reconstruction, and source separation. Several methods have recently been proposed to estimate the 3-D positions of acoustic reflectors given room impulse responses RIRs. In this paper, we categorize these methods as “image-source reversion,” which localizes the image source before finding the reflector position, and “direct localization,” which localizes the reflector without intermediate steps. We present five new contributions. First, an onset detector, called the clustered dynamic programing projected phase-slope algorithm, is proposed to automatically extract the time of arrival for early reflections within the RIRs of a compact microphone array. Second, we propose an image-source reversion method that uses the RIRs from a single loudspeaker. It is constructed by combining an image source locator the image source direction and range ISDAR algorithm, and a reflector locator using the loudspeaker-image bisection LIB algorithm. Third, two variants of it, exploiting multiple loudspeakers, are proposed. Fourth, we present a direct localization method, the ellipsoid tangent sample consensus ETSAC, exploiting ellipsoid properties to localize the reflector. Finally, systematic experiments on simulated and measured RIRs are presented, comparing the proposed methods with the state-of-the-art. ETSAC generates errors lower than the alternative methods compared through our datasets. Nevertheless, the ISDAR-LIB combination performs well and has a run time 200 times faster than ETSAC.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {296–309},
numpages = {14}
}

@article{10.1109/TASLP.2016.2635029,
author = {Ahmed, Shakeel and Akhtar, Muhammad Tahir and Ahmed, Shakeel and Akhtar, Muhammad Tahir},
title = {Gain Scheduling of Auxiliary Noise and Variable Step-Size for Online Acoustic Feedback Cancellation in Narrow-Band Active Noise Control Systems},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2635029},
doi = {10.1109/TASLP.2016.2635029},
abstract = {In this paper, we investigate active noise control systems, which use auxiliary white Gaussian noise for online feedback path modeling and neutralization. Such systems comprise three adaptive filters: 1 active noise control filter, 2 linear prediction filter, and 3 feedback path modeling and neutralization filter. The injection of white Gaussian noise with large/small variance improves/degrades the convergence of feedback path modeling and neutralization filter but on the other hand degrades/improves the noise-reduction performance of active noise control systems. The focus of this paper is to have an adequate compromise between the convergence of feedback path modeling and neutralization filter and the noise-reduction-performance of active noise control systems. In this paper: 1 A tuning free gain scheduling of white Gaussian noise is used to improve the noise-reduction performance of active noise control systems, and 2 a variable step-size is used to compensate for the decrease in the convergence of feedback path modeling and neutralization filter due to gain scheduling. The proposed idea is explained for a single channel as well as for multichannel active noise control systems. Computer simulations have been carried out to demonstrate the performance of the proposed method for a single channel as well as for multichannel active noise control systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {333–343},
numpages = {11}
}

@article{10.1109/TASLP.2016.2635027,
author = {Schlecht, Sebastian J. and Habets, Emanuel A. P. and Schlecht, Sebastian J. and Habets, Emanuel A. P.},
title = {Feedback Delay Networks: Echo Density and Mixing Time},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2635027},
doi = {10.1109/TASLP.2016.2635027},
abstract = {Feedback delay networks FDNs are frequently used to generate artificial reverberation. This paper discusses the temporal features of impulse responses produced by FDNs, i.e., the number of echoes per time unit and its evolution over time. This so-called echo density is related to known measures of mixing time and their psychoacoustic correlates such as auditive perception of the room size. It is shown that the echo density of FDNs follows a polynomial function, whereby the polynomial coefficients can be derived from the lengths of the delays for which an explicit method is given. The mixing time of impulse responses can be predicted from the echo density, and conversely, a desired mixing time can be achieved by a derived mean delay length. A Monte Carlo simulation confirms the accuracy of the derived relation of mixing time and delay lengths.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {374–383},
numpages = {10}
}

@article{10.1109/TASLP.2016.2631338,
author = {Belloch, Jose A. and Gonzalez, Alberto and Quintana-Orti, Enrique S. and Ferrer, Miguel and Valimaki, Vesa and Belloch, Jose A. and Gonzalez, Alberto and Quintana-Orti, Enrique S. and Ferrer, Miguel and Valimaki, Vesa},
title = {GPU-Based Dynamic Wave Field Synthesis Using Fractional Delay Filters and Room Compensation},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2631338},
doi = {10.1109/TASLP.2016.2631338},
abstract = {Wave field synthesis WFS is a multichannel audio reproduction method, of a considerable computational cost that renders an accurate spatial sound field using a large number of loudspeakers to emulate virtual sound sources. The moving of sound source locations can be improved by using fractional delay filters, and room reflections can be compensated by using an inverse filter bank that corrects the room effects at selected points within the listening area. However, both the fractional delay filters and the room compensation filters further increase the computational requirements of the WFS system. This paper analyzes the performance of a WFS system composed of 96 loudspeakers which integrates both strategies. In order to deal with the large computational complexity, we explore the use of a graphics processing unit GPU as a massive signal co-processor to increase the capabilities of the WFS system. The performance of the method as well as the benefits of the GPU acceleration are demonstrated by considering different sizes of room compensation filters and fractional delay filters of order 9. The results show that a 96-speaker WFS system that is efficiently implemented on a state-of-art GPU can synthesize the movements of 94 sound sources in real time and, at the same time, can manage 9216 room compensation filters having more than 4000 coefficients each.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {435–447},
numpages = {13}
}

@article{10.1109/TASLP.2016.2635022,
author = {Abel, Johannes and Kaniewska, Magdalena and Guillaume, Cyril and Tirry, Wouter and Fingscheidt, Tim and Abel, Johannes and Kaniewska, Magdalena and Guillaume, Cyril and Tirry, Wouter and Fingscheidt, Tim},
title = {An Instrumental Quality Measure for Artificially Bandwidth-Extended Speech Signals},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2635022},
doi = {10.1109/TASLP.2016.2635022},
abstract = {Various studies have shown that the instrumental measures wideband PESQ and POLQA are not reliably predicting speech quality for artificial speech bandwidth extension ABE test conditions, as this has never been their scope. Based on data from a coordinated subjective listening test with 12 ABE variants developed by 6 different institutions, conducted in 4 languages, we propose in this work a novel instrumental quality measure that is specifically suited for narrowband-to-wideband ABE test conditions. In particular, our contributions are fourfold: First, we propose quality indicators particularly being able to detect ABE-related distortions. Second, we investigate the combination of perceptually and nonperceptually motivated distortion-related statistics. Third, we propose a support-vector-machine-based high-performance MOS predictor for ABE speech quality assessment, finally, we present the training process based on the subjective listening test data. A k-fold cross-validation test on 1 disjoint languages, 2 disjoint speakers, and 3 disjoint ABE solutions proves the superiority of our proposed measure in the ITU-T-recommended categories accuracy, consistency, and linearity compared to both, wideband PESQ and POLQA.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {384–396},
numpages = {13}
}

@article{10.1109/TASLP.2016.2634118,
author = {Elvander, Filip and Sward, Johan and Jakobsson, Andreas and Elvander, Filip and Sward, Johan and Jakobsson, Andreas},
title = {Online Estimation of Multiple Harmonic Signals},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2634118},
doi = {10.1109/TASLP.2016.2634118},
abstract = {In this paper, we propose a time-recursive multipitch estimation algorithm using a sparse reconstruction framework, assuming that only a few pitches from a large set of candidates are active at each time instant. The proposed algorithm does not require any training data, and instead utilizes a sparse recursive least-squares formulation augmented by an adaptive penalty term specifically designed to enforce a pitch structure on the solution. The amplitudes of the active pitches are also recursively updated, allowing for a smooth and more accurate representation. When evaluated on a set of ten music pieces, the proposed method is shown to outperform other general purpose multipitch estimators in either accuracy or computational speed, although not being able to yield performance as good as the state-of-the art methods, which are being optimally tuned and specifically trained on the present instruments. However, the method is able to outperform such a technique when used without optimal tuning, or when applied to instruments not included in the training data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {273–284},
numpages = {12}
}

@article{10.1109/TASLP.2016.2633806,
author = {Markovich-Golan, Shmulik and Gannot, Sharon and Kellermann, Walter and Markovich-Golan, Shmulik and Gannot, Sharon and Kellermann, Walter},
title = {Combined LCMV-TRINICON Beamforming for Separating Multiple Speech Sources in Noisy and Reverberant Environments},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2633806},
doi = {10.1109/TASLP.2016.2633806},
abstract = {The problem of source separation using an array of microphones in reverberant and noisy conditions is addressed. We consider applying the well-known linearly constrained minimum variance LCMV beamformer BF for extracting individual speakers. Constraints are defined using relative transfer functions RTFs for the sources, which are ratios of acoustic transfer functions ATFs between any microphone and a reference microphone. The latter are usually estimated by methods that rely on single-talk time segments where only a single source is active and on reliable knowledge of the source activity. Two novel algorithms for estimation of RTFs using the “Triple N” ICA for convolutive mixtures TRINICON framework are proposed, not resorting to the usually unavailable source activity pattern. The first algorithm estimates the RTFs of the sources by applying multiple two-channel geometrically constrained GC TRINICON units, where approximate direction of arrival information for the sources is utilized for ensuring convergence to the desired solution. The GC-TRINICON is applied to all microphone pairs using a common reference microphone. In the second algorithm, we propose to estimate RTFs iteratively using GC-TRINICON, where instead of using a fixed reference microphone as before, we suggest to use the output signals of LCMV-BFs from the previous iteration as spatially processed references with improved signal-to-interference-and-noise ratio. For both algorithms, a simple detection of noise-only time segments is required for estimating the covariance matrix of noise and interference. We conduct an experimental study in which the performance of the proposed methods is confirmed and compared to corresponding supervised methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {320–332},
numpages = {13}
}

@article{10.1109/TASLP.2016.2601222,
author = {Chen, Hanchi and Abhayapala, Thushara Dheemantha and Samarasinghe, Prasanga N. and Zhang, Wen and Hanchi Chen and Abhayapala, Thushara Dheemantha and Samarasinghe, Prasanga N. and Wen Zhang},
title = {Direct-to-Reverberant Energy Ratio Estimation Using a First-Order Microphone},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2601222},
doi = {10.1109/TASLP.2016.2601222},
abstract = {The direct-to-reverberant ratio DRR is an important characterization of a reverberant environment. This paper presents a novel blind DRR estimation method based on the coherence function between the sound pressure and particle velocity at a point. First, a general expression of coherence function and DRR is derived in the spherical harmonic domain, without imposing assumptions on the reverberation. In this paper, DRR is expressed in terms of the coherence function as well as two parameters that are related to statistical characteristics of the reverberant environment. Then, a method to estimate the values of these two parameters using a microphone system capable of capturing first-order spherical harmonics is proposed, under three assumptions which are more realistic than the diffuse field model. Furthermore, a theoretical analysis on the use of plane wave model for direct path signal and its effect on DRR estimation is presented, and a rule of thumb is provided for determining whether the point source model should be used for the direct path signal. Finally, the ACE challenge dataset is used to validate the proposed DRR estimation method. The results show that the average full band estimation error is within 2 dB, with no clear trend of bias.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {226–237},
numpages = {12}
}

@article{10.1109/TASLP.2016.2635031,
author = {Sargent, Gabriel and Bimbot, Frederic and Vincent, Emmanuel and Sargent, Gabriel and Bimbot, Frederic and Vincent, Emmanuel},
title = {Estimating the Structural Segmentation of Popular Music Pieces Under Regularity Constraints},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2635031},
doi = {10.1109/TASLP.2016.2635031},
abstract = {Music structure estimation has recently emerged as a central topic within the field of music information retrieval. Indeed, as music is a highly structured information stream, knowledge of how a music piece is organized represents a key challenge to enhance the management and exploitation of large music collections. This paper focuses on the benefits that can be expected from a regularity constraint on the structural segmentation of popular music pieces. Specifically, here, we study how a constraint that favors structural segments of comparable size provides a better conditioning of the boundary estimation process. First, we propose a formulation of the structural segmentation task as an optimization process, which separates the contribution from the audio features and the one from the constraint. We illustrate how the corresponding cost function can be minimized using a Viterbi algorithm. We present briefly its implementation and results in three systems designed for and submitted to the MIREX 2010, 2011, and 2012 evaluation campaigns. Then, we explore the benefits of the regularity constraint as an efficient mean for combining the outputs of a selection of systems presented at MIREX between 2010 and 2015, yielding a level of performance competitive to that of the state-of-the-art on the “MIREX10” dataset 100 J-Pop songs from the RWC database.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {344–358},
numpages = {15}
}

@article{10.1109/TASLP.2016.2633812,
author = {Renkens, Vincent and Van hamme, Hugo and Renkens, Vincent and Van hamme, Hugo},
title = {Weakly Supervised Learning of Hidden Markov Models for Spoken Language Acquisition},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2633812},
doi = {10.1109/TASLP.2016.2633812},
abstract = {In this paper, a spoken command and control interface that acquires spoken language through demonstrations from the user is discussed. The user can train the system by uttering a command and subsequently demonstrating the required action through an alternative interface. From the demonstration, a bag of semantic concepts representation that represents which semantic concepts are present in the demonstration is extracted. In the previous work, we have proposed a method for learning words for these concepts by linking the bag of semantic concepts representation to a bag of features representation of the acoustics. In this method, the order in which the words occur is lost. However, in many cases, the order in which the words occur is important to be able to determine the correct action. In this paper, the vocabulary acquisition based on nonnegative matrix factorization is jointly trained with a hidden Markov model HMM, making it possible to use the bag of concepts representation as a weak supervision for HMM learning. This model can better utilize the timing information to improve the results and the order in which the words occur is retained making it possible to learn vocabulary and grammar. The proposed system is tested on several command and control tasks and it is shown that for unimpaired speech the resulting system outperforms the system solely based on vocabulary acquisition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {285–295},
numpages = {11}
}

@article{10.1109/TASLP.2016.2623559,
author = {Wu, Bo and Li, Kehuang and Yang, Minglei and Lee, Chin-Hui and Bo Wu and Kehuang Li and Minglei Yang and Chin-Hui Lee and Wu, Bo and Yang, Minglei and Lee, Chin-Hui and Li, Kehuang},
title = {A Reverberation-Time-Aware Approach to Speech Dereverberation Based on Deep Neural Networks},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2623559},
doi = {10.1109/TASLP.2016.2623559},
abstract = {A reverberation-time-aware deep-neural-network DNN-based speech dereverberation framework is proposed to handle a wide range of reverberation times. There are three key steps in designing a robust system. First, in contrast to sigmoid activation and min-max normalization in state-of-the-art algorithms, a linear activation function at the output layer and global mean-variance normalization of target features are adopted to learn the complicated nonlinear mapping function from reverberant to anechoic speech and to improve the restoration of the low-frequency and intermediate-frequency contents. Next, two key design parameters, namely, frame shift size in speech framing and acoustic context window size at the DNN input, are investigated to show that RT60-dependent parameters are needed in the DNN training stage in order to optimize the system performance in diverse reverberant environments. Finally, the reverberation time is estimated to select the proper frame shift and context window sizes for feature extraction before feeding the log-power spectrum features to the trained DNNs for speech dereverberation. Our experimental results indicate that the proposed framework outperforms the conventional DNNs without taking the reverberation time into account, while achieving a performance only slightly worse than the oracle cases with known reverberation times even for extremely weak and severe reverberant conditions. It also generalizes well to unseen room sizes, loudspeaker and microphone positions, and recorded room impulse responses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {102–111},
numpages = {10}
}

@article{10.1109/TASLP.2016.2623563,
author = {Xing, Hua and Hansen, John H. L. and Hua Xing and Hansen, John H. L. and Xing, Hua and Hansen, John H. L.},
title = {Single Sideband Frequency Offset Estimation and Correction for Quality Enhancement and Speaker Recognition},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2623563},
doi = {10.1109/TASLP.2016.2623563},
abstract = {Communication system mismatch represents a major influence in the losses of both speech quality and speaker recognition system performance. Although microphone and handset differences have been considered for speaker recognition e.g., NIST SRE, nonlinear communication system differences, such as modulation/demodulation Mod/DeMod carrier mismatch, have yet to be explored. While such mismatch was common in traditional analog communications, today, with the diversity and blending of communication technologies, it is reconsidered as a major distortion. This paper is focused on estimating and correcting the frequency-shift distortion resulting from Mod/DeMod carrier frequency mismatch in high-frequency single sideband HF-SSB speech. To overcome the drawbacks of existing solutions, a two-step algorithm is proposed to improve estimation performance. In the first step, the offset of speech is scaled to a small frequency interval, which eliminates or reduces the nonuniqueness issue due to the periodicity within the spectrum; the second step performs fine tuning within the estimated predetermined uniqueness interval UI. For the first time, a statistical framework is developed for UI detection, where an innovative acoustic feature is proposed to represent alternative frequency shifts. Additionally, in the estimation process, statistical techniques such as GMM-SVM, i-Vector, and deep neural networks are applied in the first step to improve the estimation accuracy. An evaluation using DARPA RATS HF-SSB data shows that the proposed algorithm achieves a significant improvement in the estimation performance up to +35.6% improvement in accuracy, speech quality measurement up to +27.3% relative improvement in the PESQ score, and speaker verification up to +59.9% relative improvement in equal error rate.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {124–136},
numpages = {13}
}

@article{10.1109/TASLP.2016.2620600,
author = {Al-Tmeme, Ahmed and Woo, Wai Lok and Dlay, Satnam Singh and Gao, Bin and Al-Tmeme, Ahmed and Wai Lok Woo and Dlay, Satnam Singh and Bin Gao and Woo, Wai Lok and Dlay, Satnam Singh and Al-Tmeme, Ahmed and Gao, Bin},
title = {Underdetermined Convolutive Source Separation Using GEM-MU With Variational Approximated Optimum Model Order NMF2D},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2620600},
doi = {10.1109/TASLP.2016.2620600},
abstract = {An unsupervised machine learning algorithm based on nonnegative matrix factor Two-dimensional deconvolution NMF2D with approximated optimum model order is proposed. The proposed algorithm adapted under the hybrid framework that combines the generalized EM algorithm with multiplicative update. As the number of parameters in the NMF2D grows exponentially the number of frequency basis increases linearly, the issues of model-order fitness, initialization, and parameters estimation become ever more critical. This paper proposes a variational Bayesian method to optimize the number of components in the NMF2D by using the Gamma-Exponential process as the observation-latent model. In addition, it is shown that the proposed Gamma-Exponential process can be used to initialize the NMF2D parameters. Finally, the paper investigates the issue and advantages of using different window length. Experimental results for the synthetic convolutive mixtures and live recordings verify the competence of the proposed algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {35–49},
numpages = {15}
}

@article{10.1109/TASLP.2016.2626965,
author = {Chen, Hongjie and Xie, Lei and Leung, Cheung-Chi and Lu, Xiaoming and Ma, Bin and Li, Haizhou and Hongjie Chen and Lei Xie and Cheung-Chi Leung and Xiaoming Lu and Bin Ma and Haizhou Li and Leung, Cheung-Chi and Li, Haizhou and Lu, Xiaoming and Ma, Bin and Chen, Hongjie and Xie, Lei},
title = {Modeling Latent Topics and Temporal Distance for Story Segmentation of Broadcast News},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2626965},
doi = {10.1109/TASLP.2016.2626965},
abstract = {This paper studies a strategy to model latent topics and temporal distance of text blocks for story segmentation, that we call graph regularization in topic modeling or GRTM. We propose two novel approaches that consider both temporal distance and lexical similarity of text blocks, collectively referred to as data proximity, in learning latent topic representation, where a graph regularizer is involved to derive the latent topic representation while preserving data proximity. In the first approach, we extend the idea of Laplacian probabilistic latent semantic analysis LapPLSA by introducing a distance penalty function in the affinity matrix of a graph for latent topic estimation. The estimated latent topic distributions are used to replace the traditional term-frequency vectors as the data representation of the text blocks and to measure the cohesive strength between them. In the second approach, we perform Laplacian eigenmaps, which makes use of the graph regularizer for dimensionality reduction, on latent topic distributions estimated by conventional topic modeling. We conduct the experiments on the automatic speech recognition transcripts of the TDT2 English broadcast news corpus. The experiments show the proposed strategy outperforms the conventional techniques. LapPLSA performs the best with the highest F1-measure of 0.816. The effects of the penalty constant in the distance penalty function, the number of latent topics, and the size of training data on the segmentation performances are also studied.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {112–123},
numpages = {12}
}

@article{10.1109/TASLP.2016.2621669,
author = {Huang, Zhen and Siniscalchi, Sabato Marco and Lee, Chin-Hui and Zhen Huang and Siniscalchi, Sabato Marco and Chin-Hui Lee and Lee, Chin-Hui and Huang, Zhen and Siniscalchi, Sabato Marco},
title = {Bayesian Unsupervised Batch and Online Speaker Adaptation of Activation Function Parameters in Deep Models for Automatic Speech Recognition},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2621669},
doi = {10.1109/TASLP.2016.2621669},
abstract = {We present a Bayesian framework to obtain maximum a posteriori MAP estimation of a small set of hidden activation function parameters in context-dependent-deep neural network-hidden markov model CD-DNN-HMM-based automatic speech recognition ASR systems. When applied to speaker adaptation, we aim at transfer learning from a well-trained deep model for a “general” usage to a “personalized” model geared toward a particular talker by using a collection of speaker-specific data. To make the framework applicable to practical situations, we perform adaptation in an unsupervised manner assuming that the transcriptions of the adaptation utterances are not readily available to the ASR system. We conduct a series of comprehensive batch adaptation experiments on the Switchboard ASR task and show that the proposed approach is effective even with CD-DNN-HMM built with discriminative sequential training. Indeed, MAP speaker adaptation reduces the word error rate WER to 20.1% from an initial 21.9% on the full NIST 2000 Hub5 benchmark test set. Moreover, MAP speaker adaptation compares favorably with other techniques evaluated on the same speech tasks. We also demonstrate its complementarity to other approaches by applying MAP adaptation to CD-DNN-HMM trained with speaker adaptive features generated through constrained maximum likelihood linear regression and further reduces the WER to 18.6%. Leveraging upon the intrinsic recursive nature in Bayesian adaptation and mitigating possible system constraints on batch learning, we also proposed an incremental approach to unsupervised online speaker adaptation by simultaneously updating the hyperparameters of the approximate posterior densities and the DNN parameters sequentially. The advantage of such a sequential learning algorithm over a batch method is not necessarily in the final performance, but in computational efficiency and reduced storage needs, without having to wait for all the data to be processed. So far, the experimental results are promising.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {64–75},
numpages = {12}
}

@article{10.1109/TASLP.2016.2632307,
author = {Han, Yoonchang and Kim, Jaehun and Lee, Kyogu and Yoonchang Han and Jaehun Kim and Kyogu Lee},
title = {Deep Convolutional Neural Networks for Predominant Instrument Recognition in Polyphonic Music},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2632307},
doi = {10.1109/TASLP.2016.2632307},
abstract = {Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the class-wise average followed by normalization, and the other perform temporally local class-wise max-pooling on the output probability prior to averaging and normalization steps to minimize the effect of averaging process suppresses the activation of sporadically appearing instruments. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Our analysis on the instrument-wise performance found that the onset type is a critical factor for recall and precision of each instrument. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.619 for micro and 0.513 for macro, respectively, achieving 23.1% and 18.8% in performance improvement compared with the state-of-the-art algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {208–221},
numpages = {14}
}

@article{10.1109/TASLP.2016.2628641,
author = {Kolbk, Morten and Tan, Zheng-Hua and Jensen, Jesper and Kolbk, Morten and Zheng-Hua Tan and Jensen, Jesper},
title = {Speech Intelligibility Potential of General and Specialized Deep Neural Network Based Speech Enhancement Systems},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2628641},
doi = {10.1109/TASLP.2016.2628641},
abstract = {In this paper, we study aspects of single microphone speech enhancement SE based on deep neural networks DNNs. Specifically, we explore the generalizability capabilities of state-of-the-art DNN-based SE systems with respect to the background noise type, the gender of the target speaker, and the signal-to-noise ratio SNR. Furthermore, we investigate how specialized DNN-based SE systems, which have been trained to be either noise type specific, speaker specific or SNR specific, perform relative to DNN based SE systems that have been trained to be noise type general, speaker general, and SNR general. Finally, we compare how a DNN-based SE system trained to be noise type general, speaker general, and SNR general performs relative to a state-of-the-art short-time spectral amplitude minimum mean square error STSA-MMSE based SE algorithm. We show that DNN-based SE systems, when trained specifically to handle certain speakers, noise types and SNRs, are capable of achieving large improvements in estimated speech quality SQ and speech intelligibility SI, when tested in matched conditions. Furthermore, we show that improvements in estimated SQ and SI can be achieved by a DNN-based SE system when exposed to unseen speakers, genders and noise types, given a large number of speakers and noise types have been used in the training of the system. In addition, we show that a DNN-based SE system that has been trained using a large number of speakers and a wide range of noise types outperforms a state-of-the-art STSA-MMSE based SE method, when tested using a range of unseen speakers and noise types. Finally, a listening test using several DNN-based SE systems tested in unseen speaker conditions show that these systems can improve SI for some SNR and noise type configurations but degrade SI for others.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {153–167},
numpages = {15}
}

@article{10.1109/TASLP.2016.2623565,
author = {Durand, Simon and Bello, Juan Pablo and David, Bertrand and Richard, Gael and Durand, Simon and Bello, Juan Pablo and David, Bertrand and Richard, Gael and David, Bertrand and Richard, Gael and Durand, Simon and Bello, Juan Pablo},
title = {Robust Downbeat Tracking Using an Ensemble of Convolutional Networks},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2623565},
doi = {10.1109/TASLP.2016.2623565},
abstract = {In this paper, we present a novel state-of-the-art system for automatic downbeat tracking from music signals. The audio signal is first segmented in frames which are synchronized at the tatum level of the music. We then extract different kind of features based on harmony, melody, rhythm, and bass content to feed convolutional neural networks that are adapted to take advantage of the characteristics of each feature. This ensemble of neural networks is combined to obtain one downbeat likelihood per tatum. The downbeat sequence is finally decoded with a flexible and efficient temporal model which takes advantage of the assumed metrical continuity of a song. We then perform an evaluation of our system on a large base of nine datasets, compare its performance to four other published algorithms and obtain a significant increase of 16.8% points compared to the second-best system, for altogether a moderate cost in test and training. The influence of each step of the method is studied to show its strengths and shortcomings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {76–89},
numpages = {14}
}

@article{10.1109/TASLP.2016.2621659,
author = {Hasegawa-Johnson, Mark A. and Jyothi, Preethi and McCloy, Daniel and Mirbagheri, Majid and Liberto, Giovanni M. di and Das, Amit and Ekin, Bradley and Liu, Chunxi and Manohar, Vimal and Tang, Hao and Lalor, Edmund C. and Chen, Nancy F. and Hager, Paul and Kekona, Tyler and Sloan, Rose and Lee, Adrian K. C. and Hasegawa-Johnson, Mark A. and Jyothi, Preethi and McCloy, Daniel and Mirbagheri, Majid and Di Liberto, Giovanni M. and Das, Amit and Ekin, Bradley and Chunxi Liu and Manohar, Vimal and Hao Tang and Lalor, Edmund C. and Chen, Nancy F. and Hager, Paul and Kekona, Tyler and Sloan, Rose and Lee, Adrian K. C. and Chen, Nancy F. and Manohar, Vimal and McCloy, Daniel and Liu, Chunxi and Hager, Paul and Lee, Adrian K. C. and Mirbagheri, Majid and Sloan, Rose and Kekona, Tyler and Lalor, Edmund C. and Tang, Hao and Ekin, Bradley and Liberto, Giovanni M. di and Das, Amit and Hasegawa-Johnson, Mark A. and Jyothi, Preethi},
title = {ASR for Under-Resourced Languages From Probabilistic Transcription},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2621659},
doi = {10.1109/TASLP.2016.2621659},
abstract = {In many under-resourced languages it is possible to find text, and it is possible to find speech, but transcribed speech suitable for training automatic speech recognition ASR is unavailable. In the absence of native transcripts, this paper proposes the use of a probabilistic transcript: A probability mass function over possible phonetic transcripts of the waveform. Three sources of probabilistic transcripts are demonstrated. First, self-training is a well-established semisupervised learning technique, in which a cross-lingual ASR first labels unlabeled speech, and is then adapted using the same labels. Second, mismatched crowdsourcing is a recent technique in which nonspeakers of the language are asked to write what they hear, and their nonsense transcripts are decoded using noisy channel models of second-language speech perception. Third, EEG distribution coding is a new technique in which nonspeakers of the language listen to it, and their electrocortical response signals are interpreted to indicate probabilities. ASR was trained in four languages without native transcripts. Adaptation using mismatched crowdsourcing significantly outperformed self-training, and both significantly outperformed a cross-lingual baseline. Both EEG distribution coding and text-derived phone language models were shown to improve the quality of probabilistic transcripts derived from mismatched crowdsourcing.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {50–63},
numpages = {14}
}

@article{10.1109/TASLP.2016.2625459,
author = {Chen, Zhehuai and Zhuang, Yimeng and Qian, Yanmin and Yu, Kai and Zhehuai Chen and Yimeng Zhuang and Yanmin Qian and Kai Yu and Yu, Kai and Zhuang, Yimeng and Chen, Zhehuai and Qian, Yanmin},
title = {Phone Synchronous Speech Recognition With CTC Lattices},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2625459},
doi = {10.1109/TASLP.2016.2625459},
abstract = {Connectionist temporal classification CTC has recently shown improved performance and efficiency in automatic speech recognition. One popular decoding implementation is to use a CTC model to predict the phone posteriors at each frame and then perform Viterbi beam search on a modified WFST network. This is still within the traditional frame synchronous decoding framework. In this paper, the peaky posterior property of CTC is carefully investigated and it is found that ignoring blank frames will not introduce additional search errors. Based on this phenomenon, a novel phone synchronous decoding framework is proposed by removing tremendous search redundancy due to blank frames, which results in significant search speed up. The framework naturally leads to an extremely compact phone-level acoustic space representation: CTC lattice. With CTC lattice, efficient and effective modular speech recognition approaches, second pass rescoring for large vocabulary continuous speech recognition LVCSR, and phone-based keyword spotting KWS, are also proposed in this paper. Experiments showed that phone synchronous decoding can achieve 3-4 times search speed up without performance degradation compared to frame synchronous decoding. Modular LVCSR with CTC lattice can achieve further WER improvement. KWS with CTC lattice not only achieved significant equal error rate improvement, but also greatly reduced the KWS model size and increased the search speed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {90–101},
numpages = {12}
}

@article{10.1109/TASLP.2016.2614351,
author = {Papadopoulos, Helene and Tzanetakis, George and Papadopoulos, Helene and Tzanetakis, George and Tzanetakis, George and Papadopoulos, Helene},
title = {Models for Music Analysis From a Markov Logic Networks Perspective},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2614351},
doi = {10.1109/TASLP.2016.2614351},
abstract = {Analyzing and formalizing the intricate mechanisms of music is a very challenging goal for Artificial Intelligence. Dealing with real audio recordings requires the ability to handle both uncertainty and complex relational structure at multiple levels of representation. Until now, these two aspects have been generally treated separately, probability being the standard way to represent uncertainty in knowledge, while logical representation being the standard way to represent knowledge and complex relational information. Several approaches attempting a unification of logic and probability have recently been proposed. In particular, Markov logic networks MLNs, which combine first-order logic and probabilistic graphical models, have attracted increasing attention in recent years in many domains. This paper introduces MLNs as a highly flexible and expressive formalism for the analysis of music that encompasses most of the commonly used probabilistic and logic-based models. We first review and discuss existing approaches for music analysis. We then introduce MLNs in the context of music signal processing by providing a deep understanding of how they specifically relate to traditional models, specifically hidden Markov models and conditional random fields. We then present a detailed application of MLNs for tonal harmony music analysis that illustrates the potential of this framework for music processing.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {19–34},
numpages = {16}
}

@article{10.1109/TASLP.2016.2627186,
author = {Abeber, Jakob and Frieler, Klaus and Cano, Estefania and Pfleiderer, Martin and Zaddach, Wolf-Georg and Abesser, Jakob and Frieler, Klaus and Cano, Estefan\'{\i}a and Pfleiderer, Martin and Zaddach, Wolf-Georg},
title = {Score-Informed Analysis of Tuning, Intonation, Pitch Modulation, and Dynamics in Jazz Solos},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2627186},
doi = {10.1109/TASLP.2016.2627186},
abstract = {Both the collection and analysis of large music repertoires constitute major challenges within musicological disciplines such as jazz research. Automatic methods of music analysis based on audio signal processing have the potential to assist researchers and to accelerate the transcription and analysis of music recordings significantly. In this paper, we propose a framework for analyzing improvised monophonic solos in multi-instrumental jazz recordings with special focus on reed and brass instruments. The analysis algorithms rely on prior score-information, which is taken from high quality manual solo transcriptions. Following an initial solo and accompaniment source separation, we propose algorithms for tone-wise extraction of fundamental frequency and intensity contours. Based on this fine-grained representation of recorded jazz solos, we perform several exploratory experiments motivated by questions relating to jazz research in order to analyze the use of expressive stylistic devices such as intonation, pitch modulation, and dynamics in jazz solos. The results show that a score-informed audio analysis of jazz recordings can provide valuable insights into the individual stylistic characteristics of jazz musicians.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {168–177},
numpages = {10}
}

@article{10.1109/TASLP.2016.2621675,
author = {Li, Kun and Qian, Xiaojun and Meng, Helen and Kun Li and Xiaojun Qian and Meng, Helen},
title = {Mispronunciation Detection and Diagnosis in L2 English Speech Using Multidistribution Deep Neural Networks},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2621675},
doi = {10.1109/TASLP.2016.2621675},
abstract = {This paper investigates the use of multidistribution deep neural networks DNNs for mispronunciation detection and diagnosis MDD, to circumvent the difficulties encountered in an existing approach based on extended recognition networks ERNs. The ERNs leverage existing automatic speech recognition technology by constraining the search space via including the likely phonetic error patterns of the target words in addition to the canonical transcriptions. MDDs are achieved by comparing the recognized transcriptions with the canonical ones. Although this approach performs reasonably well, it has the following issues: 1 Learning the error patterns of the target words to generate the ERNs remains a challenging task. Phones or phone errors missing from the ERNs cannot be recognized even if we have well-trained acoustic models; and 2 acoustic models and phonological rules are trained independently, and hence, contextual information is lost. To address these issues, we propose an acoustic-graphemic-phonemic model AGPM using a multidistribution DNN, whose input features include acoustic features, as well as corresponding graphemes and canonical transcriptions encoded as binary vectors. The AGPM can implicitly model both grapheme-to-likely-pronunciation and phoneme-to-likely-pronunciation conversions, which are integrated into acoustic modeling. With the AGPM, we develop a unified MDD framework, which works much like free-phone recognition. Experiments show that our method achieves a phone error rate PER of 11.1%. The false rejection rate FRR, false acceptance rate FAR, and diagnostic error rate DER for MDD are 4.6%, 30.5%, and 13.5%, respectively. It outperforms the ERN approach using DNNs as acoustic models, whose PER, FRR, FAR, and DER are 16.8%, 11.0%, 43.6%, and 32.3%, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {193–207},
numpages = {15}
}

@article{10.1109/TASLP.2016.2613280,
author = {Moore, Alastair H. and Evers, Christine and Naylor, Patrick A. and Moore, Alastair H. and Evers, Christine and Naylor, Patrick A.},
title = {Direction of Arrival Estimation in the Spherical Harmonic Domain Using Subspace Pseudointensity Vectors},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2613280},
doi = {10.1109/TASLP.2016.2613280},
abstract = {Direction of arrival DOA estimation is a fundamental problem in acoustic signal processing. It is used in a diverse range of applications, including spatial filtering, speech dereverberation, source separation and diarization. Intensity vector-based DOA estimation is attractive, especially for spherical sensor arrays, because it is computationally efficient. Two such methods are presented that operate on a spherical harmonic decomposition of a sound field observed using a spherical microphone array. The first uses pseudointensity vectors PIVs and works well in acoustic environments where only one sound source is active at any time. The second uses subspace pseudointensity vectors SSPIVs and is targeted at environments where multiple simultaneous soures and significant levels of reverberation make the problem more challenging. Analytical models are used to quantify the effects of an interfering source, diffuse noise, and sensor noise on PIVs and SSPIVs. The accuracy of DOA estimation using PIVs and SSPIVs is compared against the state of the art in simulations including realistic reverberation and noise for single and multiple, stationary and moving sources. Finally, robust performance of the proposed methods is demonstrated by using speech recordings in a real acoustic environment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {178–192},
numpages = {15}
}

@article{10.1109/TASLP.2016.2628642,
author = {Koutrouvelis, Andreas I. and Hendriks, Richard Christian and Heusdens, Richard and Jensen, Jesper and Koutrouvelis, Andreas I. and Hendriks, Richard Christian and Heusdens, Richard and Jensen, Jesper},
title = {Relaxed Binaural LCMV Beamforming},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2628642},
doi = {10.1109/TASLP.2016.2628642},
abstract = {In this paper, we propose a new binaural beamforming technique, which can be seen as a relaxation of the linearly constrained minimum variance LCMV framework. The proposed method can achieve simultaneous noise reduction and exact binaural cue preservation of the target source, similar to the binaural minimum variance distortionless response BMVDR method. However, unlike BMVDR, the proposed method is also able to preserve the binaural cues of multiple interferers to a certain predefined accuracy. Specifically, it is able to control the trade-off between noise reduction and binaural cue preservation of the interferers by using a separate trade-off parameter per-interferer. Moreover, we provide a robust way of selecting these trade-off parameters in such a way that the preservation accuracy for the binaural cues of the interferers is always better than the corresponding ones of the BMVDR. The relaxation of the constraints in the proposed method achieves approximate binaural cue preservation of more interferers than other previously presented LCMV-based binaural beamforming methods that use strict equality constraints.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {137–152},
numpages = {16}
}

@article{10.1109/TASLP.2016.2614725,
author = {Wu, Jin Chu and Martin, Alvin F. and Greenberg, Craig S. and Kacker, Raghu N. and Jin Chu Wu and Martin, Alvin F. and Greenberg, Craig S. and Kacker, Raghu N. and Greenberg, Craig S. and Wu, Jin Chu and Martin, Alvin F. and Kacker, Raghu N.},
title = {The Impact of Data Dependence on Speaker Recognition Evaluation},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2614725},
doi = {10.1109/TASLP.2016.2614725},
abstract = {The data dependence due to multiple use of the same subjects has impact on the standard error SE of the detection cost function DCF in speaker recognition evaluation. The DCF is defined as a weighted sum of the probabilities of type I and type II errors at a given threshold. A two-layer data structure is constructed: Target scores are grouped into target sets based on the dependence, and likewise for non-target scores. On account of the needed equal probabilities for scores being selected when resampling, target sets must contain the same number of target scores, and so must non-target sets. In addition to the bootstrap method with i.i.d. assumption, the nonparametric two-sample one-layer and two-layer bootstrap methods are carried out based on whether the resampling takes place only on sets, or subsequently on scores within the sets. Due to the stochastic nature of the bootstrap, the distributions of the SEs of the DCF estimated using the three different bootstrap methods are created and compared. After performing hypothesis testing, it is found that data dependence increases not only the SE but also the variation of the SE, and the two-layer bootstrap is more conservative than the one-layer bootstrap. The rationale regarding the different impacts of the three bootstrap methods on the estimated SEs is investigated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {5–18},
numpages = {14}
}

@article{10.1109/TASLP.2016.2618007,
author = {Chazan, Shlomo E. and Goldberger, Jacob and Gannot, Sharon and Chazan, Shlomo E. and Goldberger, Jacob and Gannot, Sharon},
title = {A Hybrid Approach for Speech Enhancement Using MoG Model and Neural Network Phoneme Classifier},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2618007},
doi = {10.1109/TASLP.2016.2618007},
abstract = {In this paper, we present a single-microphone speech enhancement algorithm. A hybrid approach is proposed merging the generative mixture of Gaussians MoG model and the discriminative deep neural network DNN. The proposed algorithm is executed in two phases, the training phase, which does not recur, and the test phase. First, the noise-free speech log-power spectral density is modeled as an MoG, representing the phoneme-based diversity in the speech signal. A DNN is then trained with phoneme labeled database of clean speech signals for phoneme classification with mel-frequency cepstral coefficients as the input features. In the test phase, a noisy utterance of an untrained speech is processed. Given the phoneme classification results of the noisy speech utterance, a speech presence probability SPP is obtained using both the generative and discriminative models. SPP-controlled attenuation is then applied to the noisy speech while simultaneously, the noise estimate is updated. The discriminative DNN maintains the continuity of the speech and the generative phoneme-based MoG preserves the speech spectral structure. Extensive experimental study using real speech and noise signals is provided. We also compare the proposed algorithm with alternative speech enhancement algorithms. We show that we obtain a significant improvement over previous methods in terms of speech quality measures. Finally, we analyze the contribution of all components of the proposed algorithm indicating their combined importance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2516–2530},
numpages = {15}
}

@article{10.1109/TASLP.2016.2615239,
author = {Moritz, Niko and Kollmeier, Birger and Anemuller, Jorn and Moritz, Niko and Kollmeier, Birger and Anemuller, Jorn},
title = {Integration of Optimized Modulation Filter Sets Into Deep Neural Networks for Automatic Speech Recognition},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2615239},
doi = {10.1109/TASLP.2016.2615239},
abstract = {Inspired by physiological studies on the human auditory system and by results from psychoacoustics, an amplitude modulation filter bank AMFB has been developed and successfully applied to feature extraction for automatic speech recognition ASR in earlier work. Here, we address the question as to which amplitude modulation AM frequency decomposition leads to optimal ASR performance by proposing a parameterized functional relationship between modulation center frequency and modulation bandwidth. Word error rates WERs of ASR experiments with 1551 different AMFBs are systematically evaluated and compared, resulting in the identification of a comparatively narrow range of optimal modulation frequency to modulation bandwidth characteristics. To integrate modulation processing with deep neural network DNN acoustic modeling, we propose merging of modulation filter coefficients with DNN weights prior to a final training step and an improved mean-variance normalization scheme for AMFBs. These modifications are shown to result in further reduction of WERs and are indicative of the proposed system's improved generalization ability, when compared across corpora of 100-960 h of data with mismatched training and test conditions. Analysis of DNN-learned temporal AM filtering properties is carried out and implications for the relevance of different modulation regions as well as the relation to psychoacoustic findings are discussed. ASR experiments with the proposed system demonstrate a high degree of robustness against extrinsic acoustic distortions, resulting in, e.g., an average WER of 9.79% on the Aurora-4 task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2439–2452},
numpages = {14}
}

@article{10.1109/TASLP.2016.2602549,
author = {Krawczyk-Becker, Martin and Gerkmann, Timo and Krawczyk-Becker, Martin and Gerkmann, Timo},
title = {On MMSE-Based Estimation of Amplitude and Complex Speech Spectral Coefficients Under Phase-Uncertainty},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2602549},
doi = {10.1109/TASLP.2016.2602549},
abstract = {Among the most commonly used single-channel approaches for the enhancement of noise corrupted speech are Bayesian estimators of clean speech coefficients in the short-time Fourier transform domain. However, the vast majority of these approaches effectively only modifies the spectral amplitude and does not consider any information about the clean speech spectral phase. More recently, clean speech estimators that can utilize prior phase information have been proposed and shown to lead to improvements over the traditional, phase-blind approaches. In this work, we revisit phase-aware estimators of clean speech amplitudes and complex coefficients. To complete the existing set of estimators, we first derive a novel amplitude estimator given uncertain prior phase information. Second, we derive a closed-form solution for complex coefficients when the prior phase information is completely uncertain or not available. We put the novel estimators into the context of existing estimators and discuss their advantages and disadvantages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2251–2262},
numpages = {12}
}

@article{10.1109/TASLP.2016.2604212,
author = {Liu, Jinxin and Chen, Xuefeng and Jinxin Liu and Xuefeng Chen},
title = {Adaptive Compensation of Misequalization in Narrowband Active Noise Equalizer Systems},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2604212},
doi = {10.1109/TASLP.2016.2604212},
abstract = {With the development of active noise control, some residual noise reshaping other than cancelling demands have arisen in fields like active sound quality control and psychoacoustics. Active noise equalizer ANE is one of the most popular methods of residual noise reshaping. However, the problem of misequalization, i.e., the sensitivity to secondary path modeling errors, restricts its applications. In this paper, an enhanced ANE EANE algorithm is proposed to compensate for the misequalization problem. Firstly, the misequalization phenomenon is analytically investigated. Next, EANE, which can adaptively tune the equalization parameters is proposed according to the analytical results. Multiple-harmonic EANE is also studied. Finally, numerical simulations are carried out, in which the results show that EANE can perform a more accurate residual amplitude control at the reference frequencies than traditional ANE system when there are secondary path modeling errors.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2390–2399},
numpages = {10}
}

@article{10.1109/TASLP.2016.2602546,
author = {Barker, Tom and Virtanen, Tuomas and Barker, Tom and Virtanen, Tuomas},
title = {Blind Separation of Audio Mixtures Through Nonnegative Tensor Factorization of Modulation Spectrograms},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2602546},
doi = {10.1109/TASLP.2016.2602546},
abstract = {This paper presents an algorithm for unsupervised single-channel source separation of audio mixtures. The approach specifically addresses the challenging case of separation where no training data are available. By representing mixtures in the modulation spectrogram MS domain, we exploit underlying similarities in patterns present across frequency. A three-dimensional tensor factorization is able to take advantage of these redundant patterns, and is used to separate a mixture into an approximated sum of components by minimizing a divergence cost. Furthermore, we show that the basic tensor factorization can be extended with convolution in time being used to improve separation results and provide update rules to learn components in such a manner. Following factorization, sources are reconstructed in the audio domain from estimated components using a novel approach based on reconstruction masks that are learned using MS activations, and then applied to a mixture spectrogram. We demonstrate that the proposed method produces superior separation performance to a spectrally based nonnegative matrix factorization approach, in terms of source-to-distortion ratio. We also compare separation with the perceptually motivated interference-related perceptual score metric and identify cases with higher performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2377–2389},
numpages = {13}
}

@article{10.1109/TASLP.2016.2607343,
author = {Villalba, Jesus and Miguel, Antonio and Ortega, Alfonso and Lleida, Eduardo and Villalba, Jesus and Miguel, Antonio and Ortega, Alfonso and Lleida, Eduardo},
title = {Bayesian Networks to Model the Variability of Speaker Verification Scores in Adverse Environments},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2607343},
doi = {10.1109/TASLP.2016.2607343},
abstract = {State-of-the-art speaker recognition technology attains great performance in controlled conditions. However, when the speech segments suffer distortions like noise or reverberation performance can severely deteriorate, this fact motivated us to investigate how score distributions diverge from the ideal ones in degraded conditions. We propose a Bayesian network model that assumes that two scores exist: one observed and another one hidden. The observed score or noisy score is the one given by the speaker verification system. Meanwhile, the hidden score or clean score is the ideal score that we would obtain in a trial with high-quality speech. A set of quality measures helps to relate both scores. We applied this network to two tasks. The first one consists in rejecting unreliable trials, i.e., trials that we cannot assure whether they are target or nontarget. We prove that this method outperforms previous approaches, based on another type of Bayesian networks. The second task is to compute an improved likelihood ratio, dependent on the quality measures. This ratio improved calibration in noisy conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2327–2340},
numpages = {14}
}

@article{10.1109/TASLP.2016.2602884,
author = {Qian, Yanmin and Bi, Mengxiao and Tan, Tian and Yu, Kai and Yanmin Qian and Mengxiao Bi and Tian Tan and Kai Yu},
title = {Very Deep Convolutional Neural Networks for Noise Robust Speech Recognition},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2602884},
doi = {10.1109/TASLP.2016.2602884},
abstract = {Although great progress has been made in automatic speech recognition, significant performance degradation still exists in noisy environments. Recently, very deep convolutional neural networks CNNs have been successfully applied to computer vision and speech recognition tasks. Based on our previous work on very deep CNNs, in this paper this architecture is further developed to improve recognition accuracy for noise robust speech recognition. In the proposed very deep CNN architecture, we study the best configuration for the sizes of filters, pooling, and input feature maps: the sizes of filters and poolings are reduced and dimensions of input features are extended to allow for adding more convolutional layers. Then the appropriate pooling, padding, and input feature map selection strategies are investigated and applied to the very deep CNN to make it more robust for speech recognition. In addition, an in-depth analysis of the architecture reveals key characteristics, such as compact model scale, fast convergence speed, and noise robustness. The proposed new model is evaluated on two tasks: Aurora4 task with multiple additive noise types and channel mismatch, and the AMI meeting transcription task with significant reverberation. Experiments on both tasks show that the proposed very deep CNNs can significantly reduce word error rate WER for noise robust speech recognition. The best architecture obtains a 10.0% relative reduction over the traditional CNN on AMI, competitive with the long short-term memory recurrent neural networks LSTM-RNN acoustic model. On Aurora4, even without feature enhancement, model adaptation, and sequence training, it achieves a WER of 8.81%, a 17.0% relative improvement over the LSTM-RNN. To our knowledge, this is the best published result on Aurora4.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2263–2276},
numpages = {14}
}

@article{10.1109/TASLP.2016.2615238,
author = {Tervo, Sakari and Tervo, Sakari},
title = {Single Snapshot Detection and Estimation of Reflections From Room Impulse Responses in the Spherical Harmonic Domain},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2615238},
doi = {10.1109/TASLP.2016.2615238},
abstract = {We study the detection and estimation of the parameters related to the deterministic model of the reflections of room impulse responses measured with a spherical microphone array. More specifically, we investigate the problem of detecting and estimating several reflections of a single snapshot of data in the spherical harmonic SH domain with four detection and four estimation methods, presented previously in the array processing research. Three of the estimation methods are based on Bayesian Maximum a Posteriori MAP estimation, and they employ a prior normal distribution on the signals. The estimation methods are compared with the deterministic maximum likelihood DML method [Tervo &amp; Politis 2015]. In the detection task, two information criteria-based methods, minimum description length MDL and Akaike information criteria AIC, a normalized likelihood NL based method, and a Bayesian detection scheme BDS are explored. The experiments study the performance of the methods with simulated and real data experiments. The simulation results show that the MAP estimation methods have a lower root mean squared error RMSE than DML in the reflection signal amplitude estimation, but all three have similar performance in the direction of arrival DOA and noise variance estimation, although in general RMSE of DOA estimation of MAP methods is slightly lower that that of DML. In addition, in the detection task, the BDS and AIC are the most robust against additive noise, and NL and AIC have the best detection rate when more than two reflections are simulated. The results of the real data experiments show that all the estimation methods have similar performance for DOA and noise variance estimation, while the Bayesian MAP methods have a clearly lower RMSE for reflection signal amplitude estimation than DML. In total, NL has the highest detection rate in the real data experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2466–2480},
numpages = {15}
}

@article{10.1109/TASLP.2016.2615242,
author = {Markovic, Dejan and Antonacci, Fabio and Bianchi, Lucio and Tubaro, Stefano and Sarti, Augusto and Markovic, Dejan and Antonacci, Fabio and Bianchi, Lucio and Tubaro, Stefano and Sarti, Augusto},
title = {Extraction of Acoustic Sources Through the Processing of Sound Field Maps in the Ray Space},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2615242},
doi = {10.1109/TASLP.2016.2615242},
abstract = {Our goal is to develop a model-based approach to acoustic source extraction from microphone array data, which is suitable for both near-field and far-field sources. A signal representation based on plane-wave PW decomposition is suitable for acoustic sources in the far field as the resulting spectrum turns out to be impulsive. When the source approaches the array, however, the curvature of the wavefront causes the spectrum of the PW components to depart from impulsive behavior, thus making source extraction harder to attain. In this paper, we adopt a sound field representation based on the local estimation of the plenacoustic function along the array line. This approach consists of dividing the array into subarrays, and applying the PW analysis on individual subarrays. This has the immediate result of extending the range of validity of the far-field hypothesis, as a source that enters the near-field range of the extended array is still in the far-field range of the subarrays. PW analysis on subarrays allows us to construct the so-called sound field map in a domain of acoustic visibility called ray space. The extraction of the desired source is accomplished through spatial filtering of the sound field map. The design of the spatial filter relies on a linear minimum mean square error criterion defined on the sound field map. The effectiveness of the proposed methodology is proven through an extensive simulation campaign as well as real experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2481–2494},
numpages = {14}
}

@article{10.1109/TASLP.2016.2614140,
author = {Leglaive, Simon and Badeau, Roland and Richard, Gael and Leglaive, Simon and Badeau, Roland and Richard, Gael},
title = {Multichannel Audio Source Separation With Probabilistic Reverberation Priors},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2614140},
doi = {10.1109/TASLP.2016.2614140},
abstract = {Incorporating prior knowledge about the sources and/or the mixture is a way to improve under-determined audio source separation performance. A great number of informed source separation techniques concentrate on taking priors on the sources into account, but fewer works have focused on constraining the mixing model. In this paper, we address the problem of underdetermined multichannel audio source separation in reverberant conditions. We target a semi-informed scenario where some room parameters are known. Two probabilistic priors on the frequency response of the mixing filters are proposed. Early reverberation is characterized by an autoregressive model while according to statistical room acoustics results, late reverberation is represented by an autoregressive moving average model. Both reverberation models are defined in the frequency domain. They aim to transcribe the temporal characteristics of the mixing filters into frequency-domain correlations. Our approach leads to a maximum a posteriori estimation of the mixing filters which is achieved thanks to the expectation-maximization algorithm. We experimentally show the superiority of this approach compared with a maximum likelihood estimation of the mixing filters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2453–2465},
numpages = {13}
}

@article{10.1109/TASLP.2016.2607339,
author = {Movassagh, Mahmood and Kabal, Peter and Movassagh, Mahmood and Kabal, Peter},
title = {Scalable Audio Coding Using Trellis-Based Optimized Joint Entropy Coding and Quantization},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2607339},
doi = {10.1109/TASLP.2016.2607339},
abstract = {There is a considerable performance gap between the current scalable audio coding schemes and a nonscalable coder operating at the same bitrate. This suboptimality results from the independent coding of the layers in these systems. One of the aspects that plays a role in this suboptimality is the entropy coding. In practical audio coding systems including MPEG advanced audio coding AAC, the transform domain coefficients are quantized using an entropy-constrained quantizer. In MPEG-4 scalable AAC S-AAC, the quantization and coding are performed separately at each layer. In case of Huffman coding, the redundancy introduced by the entropy coding at each layer is larger at lower quantization resolutions. Also, the redundancy for the overall coder becomes larger as the number of layers increases. In fact, there is a tradeoff between the overall redundancy and the fine-grain scalability in which the bitrate per layer is smaller and more layers are required. In this paper, a fine-grain scalable coder for audio signals is proposed where the entropy coding of a quantizer is made scalable via joint design of entropy coding and quantization. By constructing a Huffman-like coding tree where the internal nodes can be mapped to the reconstruction points, the tree can be pruned at any internal node to control the rate-distortion RD performance of the encoder in a fine-grain manner. A set of metrics and a trellis-based approach is proposed to create a coding tree so that an appropriate path is generated on the RD plane. The results show the proposed method outperforms the scalable audio coding performed based on reconstruction error quantization as used in practical systems, e.g., in S-AAC.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2288–2300},
numpages = {13}
}

@article{10.1109/TASLP.2016.2556280,
author = {Zhang, Sheng and Zhang, Jiashu and Han, Hongyu and Sheng Zhang and Jiashu Zhang and Hongyu Han},
title = {Robust Variable Step-Size Decorrelation Normalized Least-Mean-Square Algorithm and Its Application to Acoustic Echo Cancellation},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2556280},
doi = {10.1109/TASLP.2016.2556280},
abstract = {In this paper, we present a robust variable step-size decorrelation normalized least-mean-square RVSSDNLMS algorithm. A new constrained minimization problem is developed by minimizing the l2 norm of the a decorrelated posteriori error signal with a constraint on the filter coefficients in the l2 norm sense. Solving this minimization problem gives birth to the efficient RVSSDNLMS algorithm. The convergence performance and computational complexity of RVSSDNLMS algorithm are analyzed. Finally, simulations show that the proposed RVSSDNLMS considerably outperforms the normalized least-mean-square NLMS, robust variable step-size NLMS, and pseudoaffine projection algorithms in terms of convergence rate and steady-state error in Gaussian noise and impulsive noise environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2368–2376},
numpages = {9}
}

@article{10.1109/TASLP.2016.2603006,
author = {Wu, Yi-Chan and Chen, Homer H. and Yi-Chan Wu and Chen, Homer H.},
title = {Generation of Affective Accompaniment in Accordance With Emotion Flow},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2603006},
doi = {10.1109/TASLP.2016.2603006},
abstract = {The emotion expressed by a music piece varies as the music unrolls in time. To create such dynamic expression, we develop an algorithm that automatically generates the accompaniment for a melody according to the emotion flow specified by a user. The emotion flow is given in the form of arousal and valence curves, each as a function of time. The affective accompaniment is composed of chord progression and accompaniment pattern. The chord progression, which controls the valence of the composed music, is generated by dynamic programming using the input melody and valence data as constraints. A mathematical model is developed to describe the temporal relationship between valence and chord progression. The accompaniment pattern, which controls the arousal of the composed music, is determined according to the quantized arousal values. The performance of the system is evaluated by subjective tests. The cross-correlation coefficient between the input arousal valence and the perceived arousal valence of the composed music is 0.85 0.52. It rises to 0.92 for arousal and 0.88 for valence if only musician subjects in the test are considered. Overall, the proposed system is capable of generating subjectively appropriate accompaniments conforming to the user specification.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2277–2287},
numpages = {11}
}

@article{10.1109/TASLP.2016.2616543,
author = {Titze, Ingo R. and Palaparthi, Anil and Titze, Ingo R. and Palaparthi, Anil},
title = {Sensitivity of Source–Filter Interaction to Specific Vocal Tract Shapes},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2616543},
doi = {10.1109/TASLP.2016.2616543},
abstract = {A systematic variation of length and cross-sectional area of specific segments of the vocal tract trachea to lips was conducted computationally to quantify the effects of source-filter interaction. A one-dimensional Navier-Stokes transmission line solution was used to compute peak glottal airflow, maximum flow declination rate, and formant ripple on glottal flow for Level 1 aero-acoustic interactions. For Level 2 tissue movement interaction, peak glottal area, phonation threshold pressure, and deviation in fo were quantified. Results show that the ventricle, the false-fold glottis, the conus elasticus entry, and the laryngeal vestibule are the regions to which acoustic variables are most sensitive. Generally, any narrow section of the vocal tract increases the degree of interaction, both in terms of its length and its cross-sectional area. The closer the narrow section is to the vocal folds, the greater the effect.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2507–2515},
numpages = {9}
}

@article{10.1109/TASLP.2016.2615240,
author = {Papadopoulos, Pavlos and Tsiartas, Andreas and Narayanan, Shrikanth and Papadopoulos, Pavlos and Tsiartas, Andreas and Narayanan, Shrikanth},
title = {Long-Term SNR Estimation of Speech Signals in Known and Unknown Channel Conditions},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2615240},
doi = {10.1109/TASLP.2016.2615240},
abstract = {Many speech processing algorithms and applications rely on the explicit knowledge of signal-to-noise ratio SNR in their design and implementation. Estimating the SNR of a signal can enhance the performance of such technologies. We propose a novel method for estimating the long-term SNR of speech signals based on features, from which we can approximately detect regions of speech presence in a noisy signal. By measuring the energy in these regions, we create sets of energy ratios, from which we train regression models for different types of noise. If the type of noise that corrupts a signal is known, we use the corresponding regression model to estimate the SNR. When the noise is unknown, we use a deep neural network to find the “closest” regression model to estimate the SNR. Evaluations were done based on the TIMIT speech corpus, using noises from the NOISEX-92 noise database. Furthermore, we performed cross-corpora experiments by training on TIMIT and NOISEX-92 and testing on the Wall Street Journal speech corpus and DEMAND noise database. Our results show that our system provides accurate SNR estimations across different noise types, corpora, and that it outperforms other SNR estimation methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2495–2506},
numpages = {12}
}

@article{10.1109/TASLP.2016.2611938,
author = {Li, Bochen and Duan, Zhiyao and Bochen Li and Zhiyao Duan},
title = {An Approach to Score Following for Piano Performances With the Sustained Effect},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2611938},
doi = {10.1109/TASLP.2016.2611938},
abstract = {One challenge in score following for piano music is the sustained effect, i.e., the waveform of a note lasts longer than what is notated in the score. This can be caused by expressive performing styles such as the legato articulation and the usage of the sustain and the sostenuto pedals and can also be caused by the reverberation in the recording environment. This effect creates nonnotated overlappings between sustained notes and latter notes in the audio. It decreases the audio-score alignment accuracy and robustness of score following systems and makes them be prone to delay errors, i.e., aligning audio to a score position that is earlier than the correct position. In this paper, we propose to modify the feature representation of the audio to attenuate the sustained effect. We show that this idea can be applied to both the chromagram and the spectral-peak representations, which are commonly used in score following systems. Experiments on the MAPS dataset show that the proposed method significantly improves the alignment accuracy and robustness of score following systems for piano performances, in both anechoic and highly reverberant environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2425–2438},
numpages = {14}
}

@article{10.1109/TASLP.2016.2608948,
author = {Norholm, Sidsel Marie and Jensen, Jesper Rindom and Christensen, Mads Graesboll and Norholm, Sidsel Marie and Jensen, Jesper Rindom and Christensen, Mads Graesboll},
title = {Instantaneous Fundamental Frequency Estimation With Optimal Segmentation for Nonstationary Voiced Speech},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2608948},
doi = {10.1109/TASLP.2016.2608948},
abstract = {In speech processing, the speech is often considered stationary within segments of 20-30 ms even though it is well known not to be true. In this paper, we take the nonstationarity of voiced speech into account by using a linear chirp model to describe the speech signal. We propose a maximum likelihood estimator of the fundamental frequency and chirp rate of this model, and show that it reaches the Cramer-Rao lower bound. Since the speech varies over time, a fixed segment length is not optimal, and we propose making a segmentation of the signal based on the maximum a posteriori criterion. Using this segmentation method, the segments are on average longer for the chirp model compared to the traditional harmonic model. For the signal under test, the average segment length is 24.4 and 17.1 ms for the chirp model and traditional harmonic model, respectively. This suggests a better fit of the chirp model than the harmonic model to the speech signal. The methods are based on an assumption of white Gaussian noise, and, therefore, two prewhitening filters are also proposed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2354–2367},
numpages = {14}
}

@article{10.1109/TASLP.2016.2618003,
author = {Huang, Gongping and Benesty, Jacob and Chen, Jingdong and Gongping Huang and Benesty, Jacob and Jingdong Chen},
title = {Superdirective Beamforming Based on the Krylov Matrix},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2618003},
doi = {10.1109/TASLP.2016.2618003},
abstract = {Superdirective beamforming has attracted a significant amount of research interest in speech and audio applications, since it can maximize the directivity factor DF given an array geometry and, therefore, is efficient in dealing with signal acquisition in diffuse-like noise environments. However, this beamformer is very sensitive to sensor self-noise and mismatch among sensors, which considerably restricts its use in practical systems. This paper develops an approach to superdirective beamforming based on the Krylov matrix. We show that the columns of a proposed Krylov matrix, which span a chosen dimension of the whole space, are interesting beamformers; consequently, all different linear combinations of those columns lead to beamformers that have good properties. In particular, we develop the Krylov maximum white noise gain and Krylov maximum DF beamformers, which are obtained by maximizing the WNG and the DF, respectively. By properly choosing the dimension of the Krylov subspace, the developed beamformers that can make a compromise between reasonable values of the DF and white noise amplification. We also extend the basic idea to the design of the Krylov maximum front-to-back ratio, parametric superdirective, and parametric supercardioid beamformers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2531–2543},
numpages = {13}
}

@article{10.1109/TASLP.2016.2602542,
author = {Kelly, Finnian and Hansen, John H. L. and Kelly, Finnian and Hansen, John H. L.},
title = {Score-Aging Calibration for Speaker Verification},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2602542},
doi = {10.1109/TASLP.2016.2602542},
abstract = {The gradual changes that occur in the human voice due to aging create challenges for speaker verification. This study presents an approach to calibrating the output scores of a speaker verification system using the time interval between comparison samples as additional information. Several functions are proposed for the incorporation of this time information, which is viewed as aging information, in a conventional linear score calibration transformation. Experiments are presented on data with shortterm aging intervals ranging between 2 months and 3 years, and long-term aging intervals of up to 30 years. The aging calibration proposal is shown to offset the decreased discrimination and calibration performance for both shortand long-term intervals, and to extrapolate well to unseen aging intervals. Relative reductions in Cuℓℓr cost of log-likelihood ratio of 1-4% and 10-43% are obtained at shortand long-term intervals, respectively. Assuming that a system has knowledge of the time interval between samples under comparison, this approach represents a straightforward means of compensating for the detrimental impact of aging on speaker verification performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2414–2424},
numpages = {11}
}

@article{10.1109/TASLP.2016.2601146,
author = {Samarakoon, Lahiru and Sim, Khe Chai and Samarakoon, Lahiru and Khe Chai Sim},
title = {Factorized Hidden Layer Adaptation for Deep Neural Network Based Acoustic Modeling},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2601146},
doi = {10.1109/TASLP.2016.2601146},
abstract = {In this paper, we propose the factorized hidden layer FHL approach to adapt the deep neural network DNN acoustic models for automatic speech recognition ASR. FHL aims at modeling speaker dependent SD hidden layers by representing an SD affine transformation as a linear combination of bases. The combination weights are low-dimensional speaker parameters that can be initialized using speaker representations like i-vectors and then reliably refined in an unsupervised adaptation fashion. Therefore, our method provides an efficient way to perform both adaptive training and test-time adaptation. Experimental results have shown that the FHL adaptation improves the ASR performance significantly, compared to the standard DNN models, as well as other state-of-the-art DNN adaptation approaches, such as training with the speaker-normalized CMLLR features, speaker-aware training using i-vector and learning hidden unit contributions LHUC. For Aurora 4, FHL achieves 3.8% and 2.3% absolute improvements over the standard DNNs trained on the LDA + STC and CMLLR features, respectively. It also achieves 1.7% absolute performance improvement over a system that combines the i-vector adaptive training with LHUC adaptation. For the AMI dataset, FHL achieved 1.4% and 1.9% absolute improvements over the sequence-trained CMLLR baseline systems, for the IHM and SDM tasks, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2241–2250},
numpages = {10}
}

@article{10.1109/TASLP.2016.2603599,
author = {Ogawa, Atsunori and Hori, Takaaki and Nakamura, Atsushi and Ogawa, Atsunori and Hori, Takaaki and Nakamura, Atsushi},
title = {Estimating Speech Recognition Accuracy Based on Error Type Classification},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2603599},
doi = {10.1109/TASLP.2016.2603599},
abstract = {Methods for estimating the speech recognition accuracy without using manually transcribed references are beneficial to the research and development of automatic speech recognition technology. This paper proposes recognition accuracy estimation methods based on error type classification ETC. ETC is an extension of confidence estimation. In ETC, each word in the recognition results recognized word sequences for the target speech data is probabilistically classified into three categories: the correct recognition C, substitution error S, and insertion error I. Deletion errors D that can occur at interword positions in the recognition results are also probabilistically detected. By summing these CSID probabilities individually, the numbers of CSIDs and, as a result, the two standard recognition accuracy measures, i.e., the percent correct and word accuracy WAcc, for the speech data can be estimated without using the reference transcriptions. Two recognition accuracy estimation methods based on ETC are proposed. In the first easy-to-use method, ETC is performed by converting the recognition results represented as word confusion networks into word alignment networks WANs. In the second and more accurate method, the WAN-based ETC results are refined with conditional random fields CRFs using various types of additional features extracted for each of the recognized words. Experiments using English and Japanese lecture speech corpora show that the recognition accuracy can be accurately estimated with the CRF-based method. The correlation coefficient and root mean square error between the lecture-level true WAccs calculated using the reference transcriptions and those estimated with the CRF-based method are 0.97 and lower than 2%, respectively. A series of additional experiments and analyses are also conducted to better understand the effectiveness of the CRF-based method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2400–2413},
numpages = {14}
}

@article{10.1109/TASLP.2016.2604566,
author = {Cernak, Milos and Lazaridis, Alexandros and Asaei, Afsaneh and Garner, Philip N. and Cernak, Milos and Lazaridis, Alexandros and Asaei, Afsaneh and Garner, Philip N.},
title = {Composition of Deep and Spiking Neural Networks for Very Low Bit Rate Speech Coding},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2604566},
doi = {10.1109/TASLP.2016.2604566},
abstract = {Most current very low bit rate VLBR speech coding systems use hidden Markov model HMM based speech recognition and synthesis techniques. This allows transmission of information such as phonemes segment by segment; this decreases the bit rate. However, an encoder based on a phoneme speech recognition may create bursts of segmental errors; these would be further propagated to any suprasegmental such as syllable information coding. Together with the errors of voicing detection in pitch parametrization, HMM-based speech coding leads to speech discontinuities and unnatural speech sound artifacts. In this paper, we propose a novel VLBR speech coding framework based on neural networks NNs for end-to-end speech analysis and synthesis without HMMs. The speech coding framework relies on a phonological subphonetic representation of speech. It is designed as a composition of deep and spiking NNs: a bank of phonological analyzers at the transmitter, and a phonological synthesizer at the receiver. These are both realized as deep NNs, along with a spiking NN as an incremental and robust encoder of syllable boundaries for coding of continuous fundamental frequency F0. A combination of phonological features defines much more sound patterns than phonetic features defined by HMM-based speech coders; this finer analysis/synthesis code contributes to smoother encoded speech. Listeners significantly prefer the NN-based approach due to fewer discontinuities and speech artifacts of the encoded speech. A single forward pass is required during the speech encoding and decoding. The proposed VLBR speech coding operates at a bit rate of approximately 360 bits/s.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2301–2312},
numpages = {12}
}

@article{10.1109/TASLP.2016.2598305,
author = {Cogliati, Andrea and Duan, Zhiyao and Wohlberg, Brendt and Cogliati, Andrea and Zhiyao Duan and Wohlberg, Brendt},
title = {Context-Dependent Piano Music Transcription With Convolutional Sparse Coding},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598305},
doi = {10.1109/TASLP.2016.2598305},
abstract = {This paper presents a novel approach to automatic transcription of piano music in a context-dependent setting. This approach employs convolutional sparse coding to approximate the music waveform as the summation of piano note waveforms dictionary elements convolved with their temporal activations onset transcription. The piano note waveforms are pre-recorded for the specific piano to be transcribed in the specific environment. During transcription, the note waveforms are fixed and their temporal activations are estimated and post-processed to obtain the pitch and onset transcription. This approach works in the time domain, models temporal evolution of piano notes, and estimates pitches and onsets simultaneously in the same framework. Experiments show that it significantly outperforms a state-of-the-art music transcription method trained in the same context-dependent setting, in both transcription accuracy and time precision, in various scenarios including synthetic, anechoic, noisy, and reverberant environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2218–2230},
numpages = {13}
}

@article{10.1109/TASLP.2016.2566919,
author = {Dov, David and Talmon, Ronen and Cohen, Israel and Dov, David and Talmon, Ronen and Cohen, Israel},
title = {Kernel Method for Voice Activity Detection in the Presence of Transients},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2566919},
doi = {10.1109/TASLP.2016.2566919},
abstract = {Voice activity detection in the presence of transient interferences is a challenging problem since transients are often detected incorrectly as speech by existing detectors. In this paper, we deviate from traditional approaches and take a geometric standpoint, in which the key element in obtaining an accurate voice activity detection is finding a metric that appropriately distinguishes between speech and transients. For example, speech and transients may often appear similar through the Euclidean distance when represented, e.g., by the Mel-frequency cepstral coefficients, thereby resulting in incorrect speech detection. To address this challenge, we propose to use a metric based on the statistics of the signal in short temporal windows and justify its use by modeling speech and transients by their latent generating variables. These latent variables may be related to physical constraints controlling the generation of the signal, and, as such, they accurately represent the content of the signal - speech or transient. We show that the Euclidean distance between the latent variables is approximated by the proposed metric. Then, by incorporating this metric into a kernel-based manifold learning method, we devise a measure of voice activity and show it leads to improved detection scores compared with competing detectors.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2313–2326},
numpages = {14}
}

@article{10.1109/TASLP.2016.2607341,
author = {Sailor, Hardik B. and Patil, Hemant A. and Sailor, Hardik B. and Patil, Hemant A.},
title = {Novel Unsupervised Auditory Filterbank Learning Using Convolutional RBM for Speech Recognition},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2607341},
doi = {10.1109/TASLP.2016.2607341},
abstract = {To learn auditory filterbanks, recently, we have proposed an unsupervised learning model based on convolutional restricted Boltzmann machine RBM with rectified linear units. In this paper, theory, training algorithm of our proposed model, and detailed analysis of learned filterbank are being presented. Learning of the model with different databases shows that the model is able to learn cochlear-like impulse responses that are localized in frequency-domain. An auditory-like scale obtained from filterbanks learned from clean and noisy datasets resembles the Mel scale, which is known to mimic perceptually relevant aspect of speech. We have experimented with both cepstral denoted as ConvRBM-CC as well as filterbank features denoted as ConvRBM-BANK. On large vocabulary continuous speech recognition task, we achieved relative improvement of 7.21-17.8% in word error rate WER compared to Mel frequency cepstral coefficient MFCC features and 1.35-6.82% compared to Mel filterbank FBANK features. On AURORA 4 multicondition training database, the relative improvement in WER by 4.8-13.65% was achieved using a Hybrid Deep Neural Network-Hidden Markov Model DNN-HMM system with ConvRBM-CC features. Using ConvRBM-BANK features, we achieve absolute reduction of 1.25-3.85% in WER on AURORA 4 test sets compared to FBANK features. A context-dependent DNN-HMM system further improves performance with a relative improvement of 3.6-4.6% on an average for bigram 5k and tri-gram 5k language models. Hence, our proposed learned filterbank performs better than traditional MFCC and Mel-filterbank features for both clean and multicondition automatic speech recognition ASR tasks. A system combination of ConvRBM-BANK and FBANK features further improve performance in all ASR tasks. Cross-domain experiments where subband filters trained on one database are used for the ASR task of another database show that model learns generalized representations of speech signals.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2341–2353},
numpages = {13}
}

@article{10.1109/TASLP.2016.2598308,
author = {Qian, Yanmin and Tan, Tian and Yu, Dong and Yanmin Qian and Tian Tan and Dong Yu},
title = {Neural Network Based Multi-Factor Aware Joint Training for Robust Speech Recognition},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598308},
doi = {10.1109/TASLP.2016.2598308},
abstract = {Although great progress has been made in automatic speech recognition ASR, significant performance degradation still exists in noisy environments. In this paper, a novel factor-aware training framework, named neural network-based multifactor aware joint training, is proposed to improve the recognition accuracy for noise robust speech recognition. This approach is a structured model which integrates several different functional modules into one computational deep model. We explore and extract speaker, phone, and environment factor representations using deep neural networks DNNs, which are integrated into the main ASR DNN to improve classification accuracy. In addition, the hidden activations in the main ASR DNN are used to improve factor extraction, which in turn helps the ASR DNN. All the model parameters, including those in the ASR DNN and factor extraction DNNs, are jointly optimized under the multitask learning framework. Unlike prior traditional techniques for the factor-aware training, our approach requires no explicit separate stages for factor extraction and adaptation. Moreover, the proposed neural network-based multifactor aware joint training can be easily combined with the conventional factor-aware training which uses the explicit factors, such as i-vector, noise energy, and T60 value to obtain additional improvement. The proposed method is evaluated on two main noise robust tasks: the AMI single distant microphone task in which reverberation is the main concern, and the Aurora4 task in which multiple noise types exist. Experiments on both tasks show that the proposed model can significantly reduce word error rate WER. The best configuration achieved more than 15% relative reduction in WER over the baselines on these two tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2231–2240},
numpages = {10}
}

@article{10.1109/TASLP.2016.2598306,
author = {Wang, Jia-Ching and Lee, Yuan-Shan and Lin, Chang-Hong and Wang, Shu-Fan and Shih, Chih-Hao and Wu, Chung-Hsien and Jia-Ching Wang and Yuan-Shan Lee and Chang-Hong Lin and Shu-Fan Wang and Chih-Hao Shih and Chung-Hsien Wu},
title = {Compressive Sensing-Based Speech Enhancement},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598306},
doi = {10.1109/TASLP.2016.2598306},
abstract = {This study proposes a speech enhancement method based on compressive sensing. The main procedures involved in the proposed method are performed in the frequency domain. First, an overcomplete dictionary is constructed from the trained speech frames. The atoms of this redundant dictionary are spectrum vectors that are trained by the K-SVD algorithm to ensure the sparsity of the dictionary. For a noisy speech spectrum, formant detection and a quasi-SNR criterion are first utilized to determine whether a frequency bin in the spectrogram is reliable, and a corresponding mask is designed. The mask-extracted reliable components in a speech spectrum are regarded as partial observations and a measurement matrix is constructed. The problem can therefore be treated as a compressive sensing problem. The K atoms of a K-sparsity speech spectrum are found using an orthogonal matching pursuit algorithm. Because the K atoms form the speech signal subspace, the removal of the noise projected onto these K atoms is achieved by multiplying the noisy spectrum with the optimized gain that corresponds to each selected atom. The proposed method is experimentally compared with the baseline methods and demonstrates its superiority.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2122–2131},
numpages = {10}
}

@article{10.1109/TASLP.2016.2598323,
author = {Jao, Ping-Keng and Su, Li and Yang, Yi-Hsuan and Wohlberg, Brendt and Ping-Keng Jao and Li Su and Yi-Hsuan Yang and Wohlberg, Brendt},
title = {Monaural Music Source Separation Using Convolutional Sparse Coding},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598323},
doi = {10.1109/TASLP.2016.2598323},
abstract = {We present a comprehensive performance study of a new time-domain approach for estimating the components of an observed monaural audio mixture. Unlike existing time-frequency approaches that use the product of a set of spectral templates and their corresponding activation patterns to approximate the spectrogram of the mixture, the proposed approach uses the sum of a set of convolutions of estimated activations with prelearned dictionary filters to approximate the audio mixture directly in the time domain. The approximation problem can be solved by an efficient convolutional sparse coding algorithm. The effectiveness of this approach for source separation of musical audio has been demonstrated in our prior work, but under rather restricted and controlled conditions, requiring the musical score of the mixture being informed a priori and little mismatch between the dictionary filters and the source signals. In this paper, we report an evaluation that considers wider, and more practical, experimental settings. This includes the use of an audio-based multipitch estimation algorithm to replace the musical score, and an external dataset of audio single notes to construct the dictionary filters. Our result shows that the proposed approach remains effective with a larger dictionary, and compares favorably with the state-of-the-art nonnegative matrix factorization approach. However, in the absence of the score and in the case of a small dictionary, our approach may not be better.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2158–2170},
numpages = {13}
}

@article{10.1109/TASLP.2016.2593801,
author = {Ewert, Sebastian and Sandler, Mark and Ewert, Sebastian and Sandler, Mark},
title = {Piano Transcription in the Studio Using an Extensible Alternating Directions Framework},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2593801},
doi = {10.1109/TASLP.2016.2593801},
abstract = {Given a musical audio recording, the goal of automatic music transcription is to determine a score-like representation of the piece underlying the recording. Despite significant interest within the research community, several studies have reported on a “glass ceiling” effect, an apparent limit on the transcription accuracy that current methods seem incapable of overcoming. In this paper, we explore how much this effect can be mitigated by focusing on a specific instrument class and making use of additional information on the recording conditions available in studio or home recording scenarios. In particular, exploiting the availability of single note recordings for the instrument in use, we develop a novel signal model employing variable-length spectro-temporal patterns as its central building blocks-tailored for pitched percussive instruments such as the piano. Temporal dependencies between spectral templates are modeled, resembling characteristics of factorial scaled hidden Markov models FS-HMM and other methods combining nonnegative matrix factorization with Markov processes. In contrast to FS-HMMs, our parameter estimation is developed in a global, relaxed form within the extensible alternating direction method of multipliers framework, which enables the systematic combination of basic regularizers propagating sparsity and local stationarity in note activity with more complex regularizers imposing temporal semantics. The proposed method achieves an f -measure of 93-95% for note onsets on pieces recorded on a Yamaha Disklavier MAPS DB.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1983–1997},
numpages = {15}
}

@article{10.1109/TASLP.2016.2594282,
author = {Chien, Yu-Ren and Wang, Hsin-Min and Jeng, Shyh-Kang and Yu-Ren Chien and Hsin-Min Wang and Shyh-Kang Jeng},
title = {Alignment of Lyrics With Accompanied Singing Audio Based on Acoustic-Phonetic Vowel Likelihood Modeling},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2594282},
doi = {10.1109/TASLP.2016.2594282},
abstract = {This study addresses the task of aligning lyrics with accompanied singing recordings. With a vowel-only representation of lyric syllables, our approach evaluates likelihood scores of vowel types with glottal pulse shapes and formant frequencies extracted from a small set of singing examples. The proposed vowel likelihood model is used in conjunction with a prior model of frame-wise syllable sequence in determining an optimal evolution of syllabic position. In lyrics alignment experiments, we optimized numerical parameters on two independent development sets and then tested the optimized system on two other datasets. New objective performance measures are introduced in the evaluation to provide further insight into the quality of alignment. Use of glottal pulse shapes and formant frequencies is shown by a controlled experiment to account for a 0.07 difference in average normalized alignment error. Another controlled experiment demonstrates that, with a difference of 0.03, F0-invariant glottal pulse shape gives a lower average normalized alignment error than does F0-invariant spectrum envelope, the latter being assumed by MFCC-based timbre models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1998–2008},
numpages = {11}
}

@article{10.1109/TASLP.2016.2594287,
author = {Wang, Jin and Yu, Liang-Chih and Lai, K. Robert and Zhang, Xuejie and Jin Wang and Liang-Chih Yu and Lai, K. Robert and Xuejie Zhang},
title = {Community-Based Weighted Graph Model for Valence-Arousal Prediction of Affective Words},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2594287},
doi = {10.1109/TASLP.2016.2594287},
abstract = {Compared to the categorical approach that represents affective states as several discrete classes e.g., positive and negative, the dimensional approach represents affective states as continuous numerical values in multiple dimensions, such as the valence-arousal VA space, thus allowing for more fine-grained sentiment analysis. In building dimensional sentiment applications, affective lexicons with VA ratings are useful resources but are still very rare. Several semi-supervised methods such as the kernel method, linear regression, and the pagerank algorithm have been investigated to automatically determine the VA ratings of affective words from a set of semantically similar seed words. These methods suffer from two major limitations. First, they apply an equal weight to all seeds similar to an unseen word in predicting its VA ratings. Second, even similar seeds may have quite different ratings or an inverse polarity of valence/arousal to the unseen word, thus reducing prediction performance. To overcome these limitations, this study proposes a community-based weighted graph model that can select seeds which are both similar to and have similar ratings or the same polarity with each unseen word to form a community subgraph so that its VA ratings can be estimated from such high-quality seeds using a weighted propagation scheme. That is, seeds more similar to unseen words contribute more to the estimation process. Experimental results show that the proposed method yields better prediction performance for both English and Chinese datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1957–1968},
numpages = {12}
}

@article{10.1109/TASLP.2016.2594383,
author = {Xiao, Tong and Wong, Derek F. and Zhu, Jingbo and Tong Xiao and Wong, Derek F. and Jingbo Zhu},
title = {A Loss-Augmented Approach to Training Syntactic Machine Translation Systems},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2594383},
doi = {10.1109/TASLP.2016.2594383},
abstract = {Current syntactic machine translation MT systems implicitly use beam-width unlimited search in learning model parameters e.g., feature values for each translation rule. However, a limited beam-width has to be adopted in decoding new sentences, and the MT output is in general evaluated by various metrics, such as BLEU and TER. In this paper, we address: 1 the mismatch of adopted beam-widths between training and decoding; and 2 the mismatch of training criteria and MT evaluation metrics. Unlike previous work, we model the two problems in a single training paradigm simultaneously. We design a loss-augmented approach that explicitly considers the limited beam-width and evaluation metric in training, and present a simple but effective method to learn the model. By using beam search and BLEU-related losses, our approach improves a state-of-the-art syntactic MT system by +1.0 BLEU on Chinese-to-English and English-to-Chinese translation tasks. It even outperforms seven previous training approaches over 0.8 BLEU points. More interestingly, promising improvements are observed when our approach works with TER.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2069–2083},
numpages = {15}
}

@article{10.1109/TASLP.2016.2593803,
author = {Carini, Alberto and Cecchi, Stefania and Romoli, Laura and Carini, Alberto and Cecchi, Stefania and Romoli, Laura},
title = {Robust Room Impulse Response Measurement Using Perfect Sequences for Legendre Nonlinear Filters},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2593803},
doi = {10.1109/TASLP.2016.2593803},
abstract = {The paper proposes a novel approach for measuring the room impulse response that is robust toward the nonlinearities affecting the power amplifier or the loudspeaker. The approach is implemented by modeling the acoustic path as a Legendre nonlinear filter and by measuring the first-order kernel using perfect periodic sequences and the cross-correlation method. Perfect sequences for Legendre filters are periodic sequences that guarantee the orthogonality of the Legendre basis functions over a period. They ensure the robustness of the first kernel measurement toward nonlinear distortions. The paper also explains how perfect periodic sequences for Legendre filters that are suitable for room impulse response identification can be developed. Experimental results involving both simulated and real environments illustrate the effectiveness and the robustness of the proposed approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1969–1982},
numpages = {14}
}

@article{10.1109/TASLP.2016.2554283,
author = {Gkiokas, Aggelos and Katsouros, Vassilis and Carayannis, George and Gkiokas, Aggelos and Katsouros, Vassilis and Carayannis, George},
title = {Towards Multi-Purpose Spectral Rhythm Features: An Application to Dance Style, Meter and Tempo Estimation},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2554283},
doi = {10.1109/TASLP.2016.2554283},
abstract = {This paper addresses the extraction of multipurpose spectral rhythm features that simultaneously tackle a variety of rhythm analysis tasks, namely, dance style classification, meter estimation, and tempo estimation. The term spectral rhythm features emanates from the origin of the extracted features, which is the periodicity function PF, a spectral representation that encapsulates the salience of the rhythm frequencies. Two dimensionality reduction techniques applied on the PF to extract expressive and compact features are compared, namely, a linear transformation resulting from Principal Component Analysis and a nonlinear mapping derived from a Restricted Boltzmann Machine. Subsequently, the derived features were used as input to an SVM classifier for each task. Moreover, an additional method is proposed that reformulates the well-studied tempo estimation task as a combination of multiple binary classification sub-problems. Evaluation was performed on a large number of datasets demonstrating that the same set of features learned from the PF provide a robust rhythmic representation that achieved comparable results to the current state-of-the-art methods for the aforementioned tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1885–1896},
numpages = {12}
}

@article{10.1109/TASLP.2016.2593800,
author = {Liu, Yuzong and Kirchhoff, Katrin and Yuzong Liu and Kirchhoff, Katrin},
title = {Graph-Based Semisupervised Learning for Acoustic Modeling in Automatic Speech Recognition},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2593800},
doi = {10.1109/TASLP.2016.2593800},
abstract = {In this paper, we investigate how to apply graph-based semisupervised learning to acoustic modeling in speech recognition. Graph-based semisupervised learning is a widely used transductive semisupervised learning method in which labeled and unlabeled data are jointly represented as a weighted graph; the resulting graph structure is then used as a constraint during the classification of unlabeled data points. We investigate suitable graph-based learning algorithms for speech data and evaluate two different frameworks for integrating graph-based learning into state-of-the-art, deep neural network DDN-based speech recognition systems. The first framework utilizes graph-based learning in parallel with a DNN classifier within a lattice-rescoring framework, whereas the second framework relies on an embedding of graph neighborhood information into continuous space using an autoencoder. We demonstrate significant improvements in framelevel phonetic classification accuracy and consistent reductions in word error rate on large-vocabulary conversational speech recognition tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1946–1956},
numpages = {11}
}

@article{10.1109/TASLP.2016.2588727,
author = {Huang, Yi-Chin and Wu, Chung-Hsien and Weng, Si-Ting and Yi-Chin Huang and Chung-Hsien Wu and Si-Ting Weng},
title = {Improving Mandarin Prosody Generation Using Alternative Smoothing Techniques},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2588727},
doi = {10.1109/TASLP.2016.2588727},
abstract = {Prosody plays a vital role for conveying both communicative meanings and specific speaking styles in speech communication. In recent years, Hidden Markov Model HMM-based synthesis system HTS has been developed in triumph, which can synthesize stable and smooth speech. However, the prosody of the synthesized speech suffers from the over-smoothing problem. Thus, a better prosodic model is required to improve the natural variability of the synthesized speech. This study exploits a hybrid method to alleviate this problem by combining the statistical and the template-based unit selection methods. First, a two-level clustering approach is proposed to obtain representative prosodic patterns denoted by codewords of the hierarchical prosodic structure modeled by a modified Fujisaki model. The prosodic codewords are then used to represent the prosody of each sentence in the parallel corpus consisting of the real speech corpus and the synthesized counterpart obtained from the HTS. The synthesized speech utterance is then used as the query for retrieving the prosodic codewords of the utterances in the synthesized corpus. The retrieved synthesized prosodic codewords are mapped to the prosodic codewords of the real speech based on linear mapping rules obtained from the parallel corpus. The prosodic codeword language models for prosodic word and prosodic phrase are employed respectively to choose the optimal codeword sequence of the real speech. Finally, the most likely sequence of prosodic codewords can be obtained based on the NURBS-based continuity measure for synthesizing speech with natural prosody. The experimental results of subjective and objective tests demonstrate that the proposed prosodic model substantially improves naturalness of the intonation of the synthesized speech compared to that of the HMM-based method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1897–1907},
numpages = {11}
}

@article{10.1109/TASLP.2016.2590146,
author = {Zhang, Qiaoling and Chen, Zhe and Yin, Fuliang and Qiaoling Zhang and Zhe Chen and Fuliang Yin},
title = {Distributed Marginalized Auxiliary Particle Filter for Speaker Tracking in Distributed Microphone Networks},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2590146},
doi = {10.1109/TASLP.2016.2590146},
abstract = {In this paper, a distributed marginalized auxiliary particle filter DMAPF is proposed for speaker tracking in distributed microphone networks. After marginalizing the state-space model, the speaker's velocity and position are estimated using the distributed Kalman filter and the distributed auxiliary particle filter APF, respectively. To overcome the adverse effects of noise and reverberation, a time difference of arrival selection scheme is presented to construct the local observation vector, based on the generalized cross-correlation function of the microphone pair signals at each node. Next, the multiple-hypothesis model is used as the local likelihood function of the DMAPF. Finally, the DMAPF is employed to estimate the time-varying positions of a moving speaker. The proposed method combines the strengths of the marginalized particle filter, APF, and distributed estimation. It can track the speaker successfully in noisy and reverberant environments. Moreover, it requires only local communication among neighboring nodes, and is scalable for speaker tracking. Experimental results reveal the validity of the proposed speaker tracking method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1921–1934},
numpages = {14}
}

@article{10.1109/TASLP.2016.2588002,
author = {Andersen, Asger Heidemann and de Haan, Jan Mark and Tan, Zheng-Hua and Jensen, Jesper and Andersen, Asger Heidemann and de Haan, Jan Mark and Zheng-Hua Tan and Jensen, Jesper},
title = {Predicting the Intelligibility of Noisy and Nonlinearly Processed Binaural Speech},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2588002},
doi = {10.1109/TASLP.2016.2588002},
abstract = {Objective speech intelligibility measures are gaining popularity in the development of speech enhancement algorithms and speech processing devices such as hearing aids. Such devices may process the input signals nonlinearly and modify the binaural cues presented to the user. We propose a method for predicting the intelligibility of noisy and nonlinearly processed binaural speech. This prediction is based on the noisy and processed signal as well as a clean speech reference signal. The method is obtained by extending a modified version of the short-time objective intelligibility STOI measure with a modified equalization-cancellation EC stage. We evaluate the performance of the method by comparing the predictions with measured intelligibility from four listening experiments. These comparisons indicate that the proposed measure can provide accurate predictions of 1 the intelligibility of diotic speech with an accuracy similar to that of the original STOI measure, 2 speech reception thresholds SRTs in conditions with a frontal target speaker and a single interferer in the horizontal plane, 3 SRTs in conditions with a frontal target and a single interferer when ideal time frequency segregation ITFS is applied to the left and right ears separately, and 4 the advantage of two-microphone beamforming as applied in state-of-the-art hearing aids. A MATLAB implementation of the proposed measure is available online1.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1908–1920},
numpages = {13}
}

@article{10.1109/TASLP.2016.2593944,
author = {Parthasarathy, Srinivas and Cowie, Roddy and Busso, Carlos and Parthasarathy, Srinivas and Cowie, Roddy and Busso, Carlos},
title = {Using Agreement on Direction of Change to Build Rank-Based Emotion Classifiers},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2593944},
doi = {10.1109/TASLP.2016.2593944},
abstract = {Automatic emotion recognition in realistic domains is a challenging task given the subtle expressive behaviors that occur during human interactions. The challenges start with noisy emotional descriptors provided by multiple evaluators, which are characterized by low interevaluator agreement. Studies have suggested that evaluators are more consistent in detecting qualitative relations between episodes i.e., emotional contrasts, rather than absolute scores i.e., the actual emotion. Based on these observations, this study explores the use of relative labels to train machine learning algorithms that can rank expressive behaviors. Instead of deriving relative labels from expensive and time-consuming subjective evaluations, the labels are extracted from existing time-continuous evaluations over expressive attributes annotated with FEELTRACE. We rely on the qualitative agreement QA analysis to estimate relative labels which are used to train rank-based classifiers rankers. The experimental evaluation on the SEMAINE database demonstrates the benefits of the proposed approach. The ranking performance using the QA-based labels compare favorably against preference learning rankers trained with relative labels obtained by simply aggregating the absolute values of the emotional traces across evaluators, which is the common approach used by other studies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2108–2121},
numpages = {14}
}

@article{10.1109/TASLP.2016.2577879,
author = {Ikemiya, Yukara and Itoyama, Katsutoshi and Yoshii, Kazuyoshi and Ikemiya, Yukara and Itoyama, Katsutoshi and Yoshii, Kazuyoshi},
title = {Singing Voice Separation and Vocal F0 Estimation Based on Mutual Combination of Robust Principal Component Analysis and Subharmonic Summation},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2577879},
doi = {10.1109/TASLP.2016.2577879},
abstract = {This paper presents a new method of singing voice analysis that performs mutually-dependent singing voice separation and vocal fundamental frequency F0 estimation. Vocal F0 estimation is considered to become easier if singing voices can be separated from a music audio signal, and vocal F0 contours are useful for singing voice separation. This calls for an approach that improves the performance of each of these tasks by using the results of the other. The proposed method first performs robust principal component analysis RPCA for roughly extracting singing voices from a target music audio signal. The F0 contour of the main melody is then estimated from the separated singing voices by finding the optimal temporal path over an F0 saliency spectrogram. Finally, the singing voices are separated again more accurately by combining a conventional time-frequency mask given by RPCA with another mask that passes only the harmonic structures of the estimated F0s. Experimental results showed that the proposed method significantly improved the performances of both singing voice separation and vocal F0 estimation. The proposed method also outperformed all the other methods of singing voice separation submitted to an international music analysis competition called MIREX 2014.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2084–2095},
numpages = {12}
}

@article{10.1109/TASLP.2016.2598428,
author = {Le, Duc and Licata, Keli and Persad, Carol and Provost, Emily Mower and Le, Duc and Licata, Keli and Persad, Carol and Provost, Emily Mower},
title = {Automatic Assessment of Speech Intelligibility for Individuals With Aphasia},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598428},
doi = {10.1109/TASLP.2016.2598428},
abstract = {Traditional in-person therapy may be difficult to access for individuals with aphasia due to the shortage of speech-language pathologists and high treatment cost. Computerized exercises offer a promising low-cost and constantly accessible supplement to in-person therapy. Unfortunately, the lack of feedback for verbal expression in existing programs hinders the applicability and effectiveness of this form of treatment. A prerequisite for producing meaningful feedback is speech intelligibility assessment. In this work, we investigate the feasibility of an automated system to assess three aspects of aphasic speech intelligibility: clarity, fluidity, and prosody. We introduce our aphasic speech corpus, which contains speech-based interaction between individuals with aphasia and a tablet-based application designed for therapeutic purposes. We present our method for eliciting reliable ground-truth labels for speech intelligibility based on the perceptual judgment of nonexpert human evaluators. We describe and analyze our feature set engineered for capturing pronunciation, rhythm, and intonation. We investigate the classification performance of our system under two conditions, one using human-labeled transcripts to drive feature extraction, and another using transcripts generated automatically. We show that some aspects of aphasic speech intelligibility can be estimated at human-level performance. Our results demonstrate the potential for the computerized treatment of aphasia and lay the groundwork for bridging the gap between human and automatic intelligibility assessment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2187–2199},
numpages = {13}
}

@article{10.1109/TASLP.2016.2594255,
author = {Cui, Xiaodong and Goel, Vaibhava},
title = {Maximum Likelihood Nonlinear Transformations Based on Deep Neural Networks},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2594255},
doi = {10.1109/TASLP.2016.2594255},
abstract = {Feature transformations are commonly used in speech recognition to account for distribution mismatches between the source and target domains also referred to as covariate shift. Linear affine or piecewise linear transformations are typically considered. In this paper, we present deep neural network DNN based nonlinear feature transformations estimated under the maximum likelihood criterion. We use the hidden Markov model HMM to model speech feature sequences and features in each HMM state assume a Gaussian mixture model GMM distribution. The network is pre-trained close to a linear transformation followed by a fine-tuning using the gradient descent algorithm. Due to the nonlinearity, the gradients and the partition functions of GMM-HMM state distributions are evaluated using the Monte Carlo MC method based on importance sampling. In addition, a deep stacked architecture is proposed to hierarchically build a DNN as a series of sub-networks with each representing a nonlinear transformation itself, which can be learned using a block-wise learning strategy. Applications of the proposed nonlinear transformations in speaker/environment adaptation and acoustic modeling in large vocabulary continuous speech recognition tasks show its superior performance over the widely-used constrained maximum likelihood linear regression CMLLR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2023–2031},
numpages = {9}
}

@article{10.1109/TASLP.2016.2598307,
author = {Liao, I-Bin and Chiang, Chen-Yu and Wang, Yih-Ru and Chen, Sin-Horng and I-Bin Liao and Chen-Yu Chiang and Yih-Ru Wang and Sin-Horng Chen},
title = {Speaker Adaptation of SR-HPM for Speaking Rate-Controlled Mandarin TTS},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598307},
doi = {10.1109/TASLP.2016.2598307},
abstract = {In this paper, a structural maximum a posteriori SMAP speaker adaptation approach to adjusting the speaking rate SR-dependent hierarchical prosodic model SR-HPM of an existing SR-controlled Mandarin text-to-speech system to a new speaker's data for producing a new voice is discussed. Two main issues are addressed. One is the small SR coverage of the adaptation data and is solved by using the existing SR-HPM that was trained from a speech corpus of wide SR coverage as an informative prior. Another is the data sparseness problem resulting from the large number of parameters of the SR-HPM to be adjusted. It is solved by hierarchically organizing the SR-HPM parameters into decision trees so as to be efficiently adjusted by the SMAP method. The effectiveness of the proposed approach is evaluated on speech databases of five new speakers. Both objective and subjective evaluations show that the proposed method not only performs better than the maximum likelihood-based method in the observed SR range of the target speaker's data, but also is much better in the unseen SR ranges.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2046–2058},
numpages = {13}
}

@article{10.1109/TASLP.2016.2592698,
author = {Sigtia, Siddharth and Stark, Adam M. and Krstulovic, Sacha and Plumbley, Mark D. and Sigtia, Siddharth and Stark, Adam M. and Krstulovic, Sacha and Plumbley, Mark D.},
title = {Automatic Environmental Sound Recognition: Performance Versus Computational Cost},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2592698},
doi = {10.1109/TASLP.2016.2592698},
abstract = {In the context of the Internet of Things, sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition AESR algorithms are most often developed with limited consideration for computational cost, this paper seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance as a function of its computational cost. Results suggest that Deep Neural Networks yield the best ratio of sound classification accuracy across a range of computational costs, while Gaussian Mixture Models offer a reasonable accuracy at a consistently small cost, and Support Vector Machines stand between both in terms of compromise between accuracy and computational cost.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2096–2107},
numpages = {12}
}

@article{10.1109/TASLP.2016.2593263,
author = {Nakashika, Toru and Takiguchi, Tetsuya and Minami, Yasuhiro and Nakashika, Toru and Takiguchi, Tetsuya and Minami, Yasuhiro},
title = {Non-Parallel Training in Voice Conversion Using an Adaptive Restricted Boltzmann Machine},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2593263},
doi = {10.1109/TASLP.2016.2593263},
abstract = {In this paper, we present a voice conversion VC method that does not use any parallel data while training the model. VC is a technique where only speaker-specific information in source speech is converted while keeping the phonological information unchanged. Most of the existing VC methods rely on parallel data-pairs of speech data from the source and target speakers uttering the same sentences. However, the use of parallel data in training causes several problems: 1 the data used for the training are limited to the predefined sentences, 2 the trained model is only applied to the speaker pair used in the training, and 3 mismatches in alignment may occur. Although it is, thus, fairly preferable in VC not to use parallel data, a nonparallel approach is considered difficult to learn. In our approach, we achieve nonparallel training based on a speaker adaptation technique and capturing latent phonological information. This approach assumes that speech signals are produced from a restricted Boltzmann machine-based probabilistic model, where phonological information and speaker-related information are defined explicitly. Speaker-independent and speaker-dependent parameters are simultaneously trained under speaker adaptive training. In the conversion stage, a given speech signal is decomposed into phonological and speaker-related information, the speaker-related information is replaced with that of the desired speaker, and then voice-converted speech is obtained by mixing the two. Our experimental results showed that our approach outperformed another nonparallel approach, and produced results similar to those of the popular conventional Gaussian mixture models-based method that used parallel data in subjective and objective criteria.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2032–2045},
numpages = {14}
}

@article{10.1109/TASLP.2016.2598319,
author = {Li, Xiaofei and Girin, Laurent and Horaud, Radu and Gannot, Sharon and Xiaofei Li and Girin, Laurent and Horaud, Radu and Gannot, Sharon},
title = {Estimation of the Direct-Path Relative Transfer Function for Supervised Sound-Source Localization},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598319},
doi = {10.1109/TASLP.2016.2598319},
abstract = {This paper addresses the problem of sound-source localization of a single speech source in noisy and reverberant environments. For a given binaural microphone setup, the binaural response corresponding to the direct-path propagation of a single source is a function of the source direction. In practice, this response is contaminated by noise and reverberations. The direct-path relative transfer function DP-RTF is defined as the ratio between the direct-path acoustic transfer function of the two channels. We propose a method to estimate the DP-RTF from the noisy and reverberant microphone signals in the short-time Fourier transform STFT domain. First, the convolutive transfer function approximation is adopted to accurately represent the impulse response of the sensors in the STFT domain. Second, the DP-RTF is estimated by using the auto- and cross-power spectral densities at each frequency and over multiple frames. In the presence of stationary noise, an interframe spectral subtraction algorithm is proposed, which enables to achieve the estimation of noise-free auto- and cross-power spectral densities. Finally, the estimated DP-RTFs are concatenated across frequencies and used as a feature vector for the localization of speech source. Experiments with both simulated and real data show that the proposed localization method performs well, even under severe adverse acoustic conditions, and outperforms state-of-the-art localization methods under most of the acoustic conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2171–2186},
numpages = {16}
}

@article{10.1109/TASLP.2016.2585878,
author = {Jensen, Jesper and Taal, Cees H. and Jensen, Jesper and Taal, Cees H.},
title = {An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2585878},
doi = {10.1109/TASLP.2016.2585878},
abstract = {Intelligibility listening tests are necessary during development and evaluation of speech processing algorithms, despite the fact that they are expensive and time consuming. In this paper, we propose a monaural intelligibility prediction algorithm, which has the potential of replacing some of these listening tests. The proposed algorithm shows similarities to the short-time objective intelligibility STOI algorithm, but works for a larger range of input signals. In contrast to STOI, extended STOI ESTOI does not assume mutual independence between frequency bands. ESTOI also incorporates spectral correlation by comparing complete 400ms length spectrograms of the noisy/processed speech and the clean speech signals. As a consequence, ESTOI is also able to accurately predict the intelligibility of speech contaminated by temporally highly modulated noise sources in addition to noisy signals processed with time-frequency weighting. We show that ESTOI can be interpreted in terms of an orthogonal decomposition of short-time spectrograms into intelligibility subspaces, i.e., a ranking of spectrogram features according to their importance to intelligibility. A free MATLAB implementation of the algorithm is available for noncommercial use at http://kom.aau.dk/~jje/.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2009–2022},
numpages = {14}
}

@article{10.1109/TASLP.2016.2599275,
author = {van de Laar, Thijs and de Vries, Bert and van de Laar, Thijs and de Vries, Bert},
title = {A Probabilistic Modeling Approach to Hearing Loss Compensation},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2599275},
doi = {10.1109/TASLP.2016.2599275},
abstract = {Hearing Aid HA algorithms need to be tuned “fitted” to match the impairment of each specific patient. The lack of a fundamental HA fitting theory is a strong contributing factor to an unsatisfying sound experience for about 20% of HA patients. This paper proposes a probabilistic modeling approach to the design of HA algorithms. The proposed method relies on a generative probabilistic model for the hearing loss problem and provides for automated inference of the corresponding 1 signal processing algorithm, 2 the fitting solution as well as 3 a principled performance evaluation metric. All three tasks are realized as message passing algorithms in a factor graph representation of the generative model, which in principle allows for fast implementation on HA or mobile device hardware. The methods are theoretically worked out and simulated with a custom-built factor graph toolbox for a specific hearing loss model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2200–2213},
numpages = {14}
}

@article{10.1109/TASLP.2016.2590139,
author = {Ferras, Marc and Madikeri, Srikanth and Bourlard, Herve and Ferras, Marc and Madikeri, Srikanth and Bourlard, Herve},
title = {Speaker Diarization and Linking of Meeting Data},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2590139},
doi = {10.1109/TASLP.2016.2590139},
abstract = {Finding who spoke when in a collection of recordings, with speakers being uniquely identified across the database, is a challenging task. In this scenario, reasonable computing times and acoustic variation across recordings remain two major concerns to address in state-of-the-art speaker diarization systems. This paper extends prior work on diarizing large speech datasets using algorithms that scale well with increasing amounts of data while compensating for across-recording variability. We follow a two-stage approach performing speaker diarization and speaker linking, the former focusing on local within-recording speaker changes and the latter focusing on global speaker changes across the database. In this study, we explore how these two modules interact with each other, while proposing a diarization fusion approach that prevents diarization errors from propagating to the linking stage. We further explore the diarization fusion for speaker linking using different linking strategies and speaker modeling variants. Evaluation is performed on single distant microphone data from the augmented multiparty interaction corpus show the effectiveness of the fusion approach after speaker linking and intersession variability modeling via joint factor analysis.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1935–1945},
numpages = {11}
}

@article{10.1109/TASLP.2016.2598310,
author = {Ouchi, Hiroki and Duh, Kevin and Shindo, Hiroyuki and Matsumoto, Yuji and Ouchi, Hiroki and Duh, Kevin and Shindo, Hiroyuki and Matsumoto, Yuji},
title = {Transition-Based Dependency Parsing Exploiting Supertags},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598310},
doi = {10.1109/TASLP.2016.2598310},
abstract = {Lexical information, including surface word form and part-of-speech POS information, plays a crucial role when predicting ambiguous dependency relationships in dependency parsing. However, for resolving dependency ambiguities, surface word information may be too sparse, while POS information may be too coarse. Supertags, which are lexical templates that represent rich syntactic information, have been shown to provide effective features at an intermediate level on the coarse-to-fine scale. In this work, we present a supertag design framework that allows us to instantiate various supertag sets based on the dependency structures. Using this framework, we instantiate various supertag sets and utilize them as features in transition-based dependency parsing systems. Performing experiments on the Penn Treebank and Universal Dependencies data sets, we show that our supertags are effective for transition-based parsers in multilingual parsing as well as English parsing. The comparison of the results of the different supertag sets shows that it is crucial to incorporate the head directionality, head labels, and dependent possession information in supertags to improve the parser performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2059–2068},
numpages = {10}
}

@article{10.1109/TASLP.2016.2598318,
author = {Wang, Siying and Ewert, Sebastian and Dixon, Simon and Siying Wang and Ewert, Sebastian and Dixon, Simon},
title = {Robust and Efficient Joint Alignment of Multiple Musical Performances},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598318},
doi = {10.1109/TASLP.2016.2598318},
abstract = {The goal of music alignment is to map each temporal position in one version of a piece of music to the corresponding positions in other versions of the same piece. Despite considerable improvements in recent years, state-of-the-art methods still often fail to identify a correct alignment if versions differ substantially with respect to acoustic conditions or musical interpretation. To increase the robustness for these cases, we exploit in this work the availability of multiple versions of the piece to be aligned. By processing these jointly, we can supply the alignment process with additional examples of how a section might be interpreted or which acoustic conditions may arise. This way, we can use alignment information between two versions transitively to stabilize the alignment with a third version. Extending our previous work [1], we present two such joint alignment methods, progressive alignment and probabilistic profile, and discuss their fundamental differences and similarities on an algorithmic level. Our systematic experiments using 376 recordings of 9 pieces demonstrate that both methods can indeed improve the alignment accuracy and robustness over comparable pairwise methods. Further, we provide an in-depth analysis of the behavior of both joint alignment methods, studying the influence of parameters such as the number of performances available, comparing their computational costs, and investigating further strategies to increase both.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2132–2145},
numpages = {14}
}

@article{10.1109/TASLP.2016.2598304,
author = {Chen, Xie and Liu, Xunying and Wang, Yongqiang and Gales, Mark J. F. and Woodland, Philip C. and Xie Chen and Xunying Liu and Yongqiang Wang and Gales, Mark J. F. and Woodland, Philip C.},
title = {Efficient Training and Evaluation of Recurrent Neural Network Language Models for Automatic Speech Recognition},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Press},
volume = {24},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2598304},
doi = {10.1109/TASLP.2016.2598304},
abstract = {Recurrent neural network language models RNNLMs are becoming increasingly popular for a range of applications including automatic speech recognition. An important issue that limits their possible application areas is the computational cost incurred in training and evaluation. This paper describes a series of new efficiency improving approaches that allows RNNLMs to be more efficiently trained on graphics processing units GPUs and evaluated on CPUs. First, a modified RNNLM architecture with a nonclass-based, full output layer structure F-RNNLM is proposed. This modified architecture facilitates a novel spliced sentence bunch mode parallelization of F-RNNLM training using large quantities of data on a GPU. Second, two efficient RNNLM training criteria based on variance regularization and noise contrastive estimation are explored to specifically reduce the computation associated with the RNNLM output layer softmax normalisation term. Finally, a pipelined training algorithm utilizing multiple GPUs is also used to further improve the training speed. Initially, RNNLMs were trained on a moderate dataset with 20M words from a large vocabulary conversational telephone speech recognition task. The training time of RNNLM is reduced by up to a factor of 53 on a single GPU over the standard CPU-based RNNLM toolkit. A 56 times speed up in test time evaluation on a CPU was obtained over the baseline F-RNNLMs. Consistent improvements in both recognition accuracy and perplexity were also obtained over C-RNNLMs. Experiments on Google's one billion corpus also reveals that the training of RNNLM scales well.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2146–2157},
numpages = {12}
}

@article{10.1109/TASLP.2016.2584705,
author = {Kortlang, Steffen and Grimm, Giso and Hohmann, Volker and Kollmeier, Birger and Ewert, Stephan D. and Kortlang, Steffen and Grimm, Giso and Hohmann, Volker and Kollmeier, Birger and Ewert, Stephan D. and Grimm, Giso and Ewert, Stephan D. and Hohmann, Volker and Kollmeier, Birger and Kortlang, Steffen},
title = {Auditory Model-Based Dynamic Compression Controlled by Subband Instantaneous Frequency and Speech Presence Probability Estimates},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2584705},
doi = {10.1109/TASLP.2016.2584705},
abstract = {Sensorineural hearing loss typically results in elevated thresholds and steepened loudness growth significantly conditioned by a damage of outer hair cells OHC. In hearing aids, amplification and dynamic compression aim at widening the limited available dynamic range. However, speech perception particularly in complex acoustic scenes often remains difficult. Here, a physiologically motivated, fast acting, model-based dynamic compression algorithm MDC is introduced which aims at restoring the behaviorally estimated basilar membrane input-output BM I/O function in normal-hearing listeners. A system-specific gain prescription rule is suggested, based on the same model BM I/O function and a behavioral estimate of the individual OHC loss. Cochlear off-frequency component suppression is mimicked using an instantaneous frequency IF estimate. Increased loudness as a consequence of widened filters in the impaired system is considered in a further compensation stage. In an extended version, a subband estimate of the speech presence probability MDC+SPP additionally provides speech-selective amplification in stationary noise. Instrumental evaluation revealed that the IF control enhances the spectral contrast of vowels and benefits in quality predictions at higher signal-to-noise ratios SNRs were observed. Compared with a conventional multiband dynamic compressor, MDC achieved objective quality and intelligibility benefits for a competing talker at lower SNRs. MDC+SPP outperformed the conventional compressor in the quality predictions and reached comparable instrumental speech intelligibility as achieved with linear amplification. The proposed algorithm provides a first promising basis for auditory model-based compression with signal-type- and bandwidth-dependent gains.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1759–1772},
numpages = {14}
}

@article{10.1109/TASLP.2016.2585863,
author = {Ma, Xi and Wang, Dong and Tejedor, Javier and Xi Ma and Dong Wang and Tejedor, Javier and Tejedor, Javier and Ma, Xi and Wang, Dong},
title = {Similar Word Model for Unfrequent Word Enhancement in Speech Recognition},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2585863},
doi = {10.1109/TASLP.2016.2585863},
abstract = {The popular n-gram language model LM is weak for unfrequent words. Conventional approaches such as class-based LMs pre-define some sharing structures e.g., word classes to solve the problem. However, defining such structures requires prior knowledge, and the context sharing based on these structures is generally inaccurate. This paper presents a novel similar word model to enhance unfrequent words. In principle, we enrich the context of an unfrequent word by borrowing context information from some “similar words.” Compared to conventional class-based methods, this new approach offers a fine-grained context sharing by referring to words that best match the target word, and it is more flexible as no sharing structures need to be defined by hand. Experiments on a large-scale Chinese speech recognition task demonstrated that the similar word approach can improve performance on unfrequent words significantly, while keeping the performance on general tasks almost unchanged.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1819–1830},
numpages = {12}
}

@article{10.1109/TASLP.2016.2584700,
author = {Swietojanski, Pawel and Renals, Steve and Swietojanski, Pawel and Renals, Steve and Swietojanski, Pawel and Renals, Steve},
title = {Differentiable Pooling for Unsupervised Acoustic Model Adaptation},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2584700},
doi = {10.1109/TASLP.2016.2584700},
abstract = {We present a deep neural network DNN acoustic model that includes parametrised and differentiable pooling operators. Unsupervised acoustic model adaptation is cast as the problem of updating the decision boundaries implemented by each pooling operator. In particular, we experiment with two types of pooling parametrisations: learned Lp-norm pooling and weighted Gaussian pooling, in which the weights of both operators are treated as speaker-dependent. We perform investigations using three different large vocabulary speech recognition corpora: AMI meetings, TED talks, and Switchboard conversational telephone speech. We demonstrate that differentiable pooling operators provide a robust and relatively low-dimensional way to adapt acoustic models, with relative word error rates reductions ranging from 5-20% with respect to unadapted systems, which themselves are better than the baseline fully-connected DNN-based acoustic models. We also investigate how the proposed techniques work under various adaptation conditions including the quality of adaptation data and complementarity to other feature- and model-space adaptation methods, as well as providing an analysis of the characteristics of each of the proposed approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1773–1784},
numpages = {12}
}

@article{10.1109/TASLP.2016.2585879,
author = {Niwa, Kenta and Hioka, Yusuke and Kobayashi, Kazunori and Niwa, Kenta and Hioka, Yusuke and Kobayashi, Kazunori and Niwa, Kenta and Hioka, Yusuke and Kobayashi, Kazunori},
title = {Optimal Microphone Array Observation for Clear Recording of Distant Sound Sources},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2585879},
doi = {10.1109/TASLP.2016.2585879},
abstract = {We propose the principle for deriving an optimum design for a microphone array that uses mutual information to segregate distant sound sources. Many conventional studies on array signal processing have focused on methods for estimating sound sources from array observations. To record distant sound sources clearly, designing an optimum array structure to segregate a target from other noise is also necessary. In this study, we reveal that the optimum array observation was achieved by receiving signals that are physically decorrelated between microphones, which homogenizes the eigenvalues of the spatial correlation matrix. We theoretically explain this underlying principle using mutual information between sound sources and microphone observations whose relation to the existing minimum mean square error criterion for source separation is also discussed. The implementation of such a microphone array is possible by placing microphones in front of parabolic reflectors since the phase/amplitude around the focal point of the reflectors drastically varies with small perturbation of the microphone position. Crosscorrelation between observed signals can be reduced by optimally placing microphones. An array structure based on our proposed principle was tested by implementing minimum variance distortion-less response beamforming and postfiltering in the observations of a prototype microphone array. We experimentally confirmed that 1 the eigenvalues of the spatial correlation matrix were asymptotically homogenized and 2 the target source could be extracted clearly even when the sound sources were positioned 16.5 m from the array.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1785–1795},
numpages = {11}
}

@article{10.1109/TASLP.2016.2585864,
author = {Zorila, Tudor-Catalin and Stylianou, Yannis and Ishihara, Tatsuma and Akamine, Masami and Zorila, Tudor-Catalin and Stylianou, Yannis and Ishihara, Tatsuma and Akamine, Masami and Ishihara, Tatsuma and Stylianou, Yannis and Akamine, Masami and Zorila, Tudor-Catalin},
title = {Near and Far Field Speech-in-Noise Intelligibility Improvements Based on a Time–Frequency Energy Reallocation Approach},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2585864},
doi = {10.1109/TASLP.2016.2585864},
abstract = {An algorithm designed to enhance the intelligibility of speech signals before they are presented in noisy environments was evaluated. The processed and unprocessed speech had the same root-mean square level. Spectral energy was redistributed to increase the signal-to-noise ratio SNR in the mid- and high-frequency bands, while the softer segments of speech were increased in level by applying time-domain dynamic range compression. Noise level adaptation was introduced to increase the subjective quality of signals at high SNRs. Evaluations were conducted both in near field headphones and in far field outdoor conditions using listeners with normal hearing and two types of background. The results showed: a In the near field test, the proposed algorithm yielded significant intelligibility improvements relative to the unprocessed speech for both stationary and nonstationary backgrounds; b In the far field test, the proposed algorithm increased the intelligibility of unprocessed speech by a factor of seven at 200-m distance from the sound source; and c When the background level did not alter intelligibility, listeners preferred the quality of the speech processed by the noise-dependent version of the algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1808–1818},
numpages = {11}
}

@article{10.1109/TASLP.2016.2577502,
author = {Eaton, James and Gaubitch, Nikolay D. and Moore, Alastair H. and Naylor, Patrick A. and Eaton, James and Gaubitch, Nikolay D. and Moore, Alastair H. and Naylor, Patrick A. and Gaubitch, Nikolay D. and Eaton, James and Naylor, Patrick A. and Moore, Alastair H.},
title = {Estimation of Room Acoustic Parameters: The ACE Challenge},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2577502},
doi = {10.1109/TASLP.2016.2577502},
abstract = {Reverberation time T60 and Direct-to-reverberant ratio DRR are important parameters which together can characterize sound captured by microphones in nonanechoic rooms. These parameters are important in speech processing applications such as speech recognition and dereverberation. The values of T60 and DRR can be estimated directly from the acoustic impulse response AIR of the room. In practice, the AIR is not normally available, in which case these parameters must be estimated blindly from the observed speech in the microphone signal. The acoustic characterization of environments ACE challenge aimed to determine the state-of-the-art in blind acoustic parameter estimation and also to stimulate research in this area. A summary of the ACE challenge, and the corpus used in the challenge is presented together with an analysis of the results. Existing algorithms were submitted alongside novel contributions, the comparative results for which are presented in this paper. The challenge showed that T60 estimation is a mature field where analytical approaches dominate whilst DRR estimation is a less mature field where machine learning approaches are currently more successful.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1681–1693},
numpages = {13}
}

@article{10.1109/TASLP.2016.2580298,
author = {Nose, Takashi and Nose, Takashi and Nose, Takashi},
title = {Efficient Implementation of Global Variance Compensation for Parametric Speech Synthesis},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2580298},
doi = {10.1109/TASLP.2016.2580298},
abstract = {This paper proposes a simple and efficient technique for variance compensation to improve the perceptual quality of synthetic speech in parametric speech synthesis. First, we analyze the problem of spectral and F0 enhancement with global variance GV in HMM-based speech synthesis. In the conventional GV-based parameter generation, the enhancement is achieved by taking account of a GV probability density function with fixed GV model parameters for every output utterance through the speech parameter generation process. We find that the use of fixed GV parameters results in much smaller variations of GVs in synthesized utterances than those in natural speech. In addition, the computational cost is high because of iterative optimization. This paper examines these issues in terms of multiple objective measures such as variance characteristics, GV distortions, and GV correlations. We propose a simple and fast compensation method based on a global affine transformation that provides a GV distribution closer to that of natural speech and improves the correlation of GVs between natural and generated parameter sequences. The experimental results demonstrate that the proposed variance compensation methods outperform the conventional GV-based parameter generation in terms of objective and subjective speech similarity to natural speech while maintaining speech naturalness.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1694–1704},
numpages = {11}
}

@article{10.1109/TASLP.2016.2585859,
author = {Bokaei, Mohammad Hadi and Sameti, Hossein and Liu, Yang and Bokaei, Mohammad Hadi and Sameti, Hossein and Yang Liu and Bokaei, Mohammad Hadi and Sameti, Hossein and Liu, Yang},
title = {Summarizing Meeting Transcripts Based on Functional Segmentation},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2585859},
doi = {10.1109/TASLP.2016.2585859},
abstract = {In this paper, we aim to improve meeting summarization performance using discourse specific information. Since there are intrinsically different characteristics in utterances in different types of function segments, e.g., Monologue segments versus Discussion ones, we propose a new summarization framework where different summarizers are used for different segment types. For monologue segments, we adopt the integer linear programming-based summarization method; whereas for discussion segments, we use a graph-based method to incorporate speaker information. Performance of our proposed method is evaluated using the standard AMI meeting corpus. Results show a good improvement over previous state-of-the-art algorithms according to various evaluation metrics and different compress ratios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1831–1841},
numpages = {11}
}

@article{10.1109/TASLP.2016.2580944,
author = {Ghaffarzadegan, Shabnam and Boril, Hynek and Hansen, John H. L. and Ghaffarzadegan, Shabnam and Boril, Hynek and Hansen, John H. L. and Ghaffarzadegan, Shabnam and Hansen, John H. L. and Boril, Hynek},
title = {Generative Modeling of Pseudo-Whisper for Robust Whispered Speech Recognition},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2580944},
doi = {10.1109/TASLP.2016.2580944},
abstract = {Whisper is a common means of communication used to avoid disturbing individuals or to exchange private information. As a vocal style, whisper would be an ideal candidate for human-handheld/computer interactions in open-office or public area scenarios. Unfortunately, current speech technology is predominantly focused on modal neutral speech and completely breaks down when exposed to whisper. One of the major barriers for successful whisper recognition engines is the lack of available large transcribed whispered speech corpora. This study introduces two strategies that require only a small amount of untranscribed whisper samples to produce excessive amounts of whisper-like pseudo-whisper utterances from easily accessible modal speech recordings. Once generated, the pseudo-whisper samples are used to adapt modal acoustic models of a speech recognizer toward whisper. The first strategy is based on Vector Taylor Series VTS where a whisper “background” model is first trained to capture a rough estimate of global whisper characteristics from a small amount of actual whisper data. Next, that background model is utilized in the VTS to establish specific broad phone classes' unvoiced/voiced phones transformations from each input modal utterance to its pseudo-whispered version. The second strategy generates pseudo-whisper samples by means of denoising autoencoders DAE. Two generative models are investigated-one produces pseudo-whisper cepstral features on a frame-by-frame basis, while the second generates pseudo-whisper statistics for whole phone segments. It is shown that word error rates of a TIMIT-trained speech recognizer are considerably reduced for a whisper recognition task with a constrained lexicon after adapting the acoustic model toward the VTS or DAE pseudo-whisper samples, compared to model adaptation on an available small whisper set.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1705–1720},
numpages = {16}
}

@article{10.1109/TASLP.2016.2588340,
author = {Ji, An and Johnson, Michael T. and Berry, Jeffrey J. and An Ji and Johnson, Michael T. and Berry, Jeffrey J. and Ji, An and Johnson, Michael T. and Berry, Jeffrey J.},
title = {Parallel Reference Speaker Weighting for Kinematic-Independent Acoustic-to-Articulatory Inversion},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2588340},
doi = {10.1109/TASLP.2016.2588340},
abstract = {Acoustic-to-articulatory inversion, the estimation of articulatory kinematics from an acoustic waveform, is a challenging but important problem. Accurate estimation of articulatory movements has the potential for significant impact on our understanding of speech production, on our capacity to assess and treat pathologies in a clinical setting, and on speech technologies such as computer aided pronunciation assessment and audio-video synthesis. However, because of the complex and speaker-specific relationship between articulation and acoustics, existing approaches for inversion do not generalize well across speakers. As acquiring speaker-specific kinematic data for training is not feasible in many practical applications, this remains an important and open problem. This paper proposes a novel approach to acoustic-to-articulatory inversion, Parallel Reference Speaker Weighting PRSW, which requires no kinematic data for the target speaker and a small amount of acoustic adaptation data. PRSW hypothesizes that acoustic and kinematic similarities are correlated and uses speaker-adapted articulatory models derived from acoustically derived weights. The system was assessed using a 20-speaker data set of synchronous acoustic and Electromagnetic Articulography EMA kinematic data. Results demonstrate that by restricting the reference group to a subset consisting of speakers with strong individual speaker-dependent inversion performance, the PRSW method is able to attain kinematic-independent acoustic-to-articulatory inversion performance nearly matching that of the speaker-dependent model, with an average correlation of 0.62 versus 0.63. This indicates that given a sufficiently complete and appropriately selected reference speaker set for adaptation, it is possible to create effective articulatory models without kinematic training data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1865–1875},
numpages = {11}
}

@article{10.1109/TASLP.2016.2586608,
author = {Zhang, Jiajun and Zhou, Yu and Zong, Chengqing and Jiajun Zhang and Yu Zhou and Chengqing Zong and Zhang, Jiajun and Zong, Chengqing and Zhou, Yu},
title = {Abstractive Cross-Language Summarization via Translation Model Enhanced Predicate Argument Structure Fusing},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2586608},
doi = {10.1109/TASLP.2016.2586608},
abstract = {Cross-language multidocument summarization is the task to generate a summary in a target language e.g., Chinese from a collection of documents in a different source language e.g., English. Previous methods such as the extractive and compressive algorithms focus only on single sentence selection and compression, which cannot make full use of the similar sentences containing complementary information. Furthermore, the translation model knowledge is not fully explored in previous approaches. To address these two problems, we propose in this paper an abstractive cross-language summarization framework. First, the source language documents are translated into target language with a machine translation system. Then, the method constructs a pool of bilingual concepts and facts represented by the bilingual elements of the source-side predicate-argument structures PAS and their target-side counterparts. Finally, new summary sentences are produced by fusing bilingual PAS elements with the integer linear programming algorithm to maximize both of the salience and translation quality of the PAS elements. The experimental results on English-to-Chinese cross-language summarization demonstrate that our proposed method outperforms the state-of-the-art extractive systems in both automatic and manual evaluations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1842–1853},
numpages = {12}
}

@article{10.1109/TASLP.2016.2583794,
author = {Adiloglu, Kamil and Vincent, Emmanuel and Adiloglu, Kamil and Vincent, Emmanuel and Vincent, Emmanuel and Adiloglu, Kamil},
title = {Variational Bayesian Inference for Source Separation and Robust Feature Extraction},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2583794},
doi = {10.1109/TASLP.2016.2583794},
abstract = {We consider the task of separating and classifying individual sound sources mixed together. The main challenge is to achieve robust classification despite residual distortion of the separated source signals. A promising paradigm is to estimate the uncertainty about the separated source signals and to propagate it through the subsequent feature extraction and classification stages. We argue that variational Bayesian VB inference offers a mathematically rigorous way of deriving uncertainty estimators, which contrasts with state-of-the-art estimators based on heuristics or on maximum likelihood ML estimation. We propose a general VB source separation algorithm, which makes it possible to jointly exploit spatial and spectral models of the sources. This algorithm achieves 6% and 5% relative error reduction compared to ML uncertainty estimation on the CHiME noise-robust speaker identification and speech recognition benchmarks, respectively, and it opens the way for more complex VB approximations of uncertainty.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1746–1758},
numpages = {13}
}

@article{10.1109/TASLP.2016.2580943,
author = {Mirsamadi, Seyedmahdad and Hansen, John H. L. and Mirsamadi, Seyedmahdad and Hansen, John H. L. and Hansen, John H. L. and Mirsamadi, Seyedmahdad},
title = {A Generalized Nonnegative Tensor Factorization Approach for Distant Speech Recognition With Distributed Microphones},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2580943},
doi = {10.1109/TASLP.2016.2580943},
abstract = {Automatic speech recognition ASR using distant far-field microphones is a challenging task, in which room reverberation is one of the primary causes of performance degradation. This study proposes a multichannel spectral enhancement method for reverberation-robust ASR using distributed microphones. The proposed method uses the techniques of nonnegative tensor factorization in order to identify the clean speech component from a set of observed reverberant spectrograms from the different channels. The general family of alpha-beta divergences is used for the tensor decomposition task which provides increased flexibility for the algorithm and is shown to provide improvements in highly reverberant scenarios. Unlike many conventional array processing solutions, the proposed method does not require closely-spaced microphones and is independent of source and microphone locations. The algorithm can automatically adapt to unbalanced direct-to-reverberation ratios among different channels, which is useful in blind scenarios in which no information is available about source-to-microphone distances. For a medium vocabulary distant ASR task based on TIMIT utterances, and using clean-trained deep neural network acoustic models, absolute WER improvements of +17.2%, +20.7%, and +23.2% are achieved in single-channel, two-channel, and four-channel scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1721–1731},
numpages = {11}
}

@article{10.1109/TASLP.2016.2587218,
author = {Lafay, Gregoire and Lagrange, Mathieu and Rossignol, Mathias and Benetos, Emmanouil and Roebel, Axel and Lafay, Gregoire and Lagrange, Mathieu and Rossignol, Mathias and Benetos, Emmanouil and Roebel, Axel and Roebel, Axel and Lafay, Gregoire and Lagrange, Mathieu and Rossignol, Mathias and Benetos, Emmanouil},
title = {A Morphological Model for Simulating Acoustic Scenes and Its Application to Sound Event Detection},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2587218},
doi = {10.1109/TASLP.2016.2587218},
abstract = {This paper introduces a model for simulating environmental acoustic scenes that abstracts temporal structures from audio recordings. This model allows us to explicitly control key morphological aspects of the acoustic scene and to isolate their impact on the performance of the system under evaluation. Thus, more information can be gained on the behavior of an evaluated system, providing guidance for further improvements. To demonstrate its potential, this model is employed to evaluate the performance of nine state of the art sound event detection systems submitted to the IEEE DCASE 2013 Challenge. Results indicate that the proposed scheme is able to successfully build datasets useful for evaluating important aspects of the performance of sound event detection systems, such as their robustness to new recording conditions and to varying levels of background audio.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1854–1864},
numpages = {11}
}

@article{10.1109/TASLP.2016.2585862,
author = {Epain, Nicolas and Jin, Craig T. and Epain, Nicolas and Jin, Craig T. and Epain, Nicolas and Jin, Craig T.},
title = {Spherical Harmonic Signal Covariance and Sound Field Diffuseness},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2585862},
doi = {10.1109/TASLP.2016.2585862},
abstract = {Characterizing sound field diffuseness has many practical applications, from room acoustics analysis to speech enhancement and sound field reproduction. In this paper, we investigate how spherical microphone arrays SMAs can be used to characterize diffuseness. Due to their specific geometry, SMAs are particularly well suited for analyzing the spatial properties of sound fields. In particular, the signals recorded by an SMA can be analyzed in the spherical harmonic SH domain, which has special and desirable mathematical properties when it comes to analyzing diffuse sound fields. We present a new measure of diffuseness, the COMEDIE diffuseness estimate, which is based on the analysis of the SH signal covariance matrix. This algorithm is suited for the estimation of diffuseness arising either from the presence of multiple sources distributed around the SMA or from the presence of a diffuse noise background. As well, we introduce the concept of a diffuseness profile, which consists in measuring the diffuseness for several SH orders simultaneously. Experimental results indicate that diffuseness profiles better describe the properties of the sound field than a single diffuseness measurement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1796–1807},
numpages = {12}
}

@article{10.1109/TASLP.2016.2583065,
author = {Fuster, Laura and de Diego, Maria and Azpicueta-Ruiz, Luis A. and Ferrer, Miguel and Fuster, Laura and de Diego, Maria and Azpicueta-Ruiz, Luis A. and Ferrer, Miguel and de Diego, Maria and Fuster, Laura and Azpicueta-Ruiz, Luis A. and Ferrer, Miguel},
title = {Adaptive Filtered-x Algorithms for Room Equalization Based on Block-Based Combination Schemes},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2583065},
doi = {10.1109/TASLP.2016.2583065},
abstract = {Room equalization has become essential for sound reproduction systems to provide the listener with the desired acoustical sensation. Recently, adaptive filters have been proposed as an effective tool in the core of these systems. In this context, this paper introduces different novel schemes based on the combination of adaptive filters idea: a versatile and flexible approach that permits obtaining adaptive schemes combining the capabilities of several independent adaptive filters. In this way, we have investigated the advantages of a scheme called combination of block-based adaptive filters which allows a blockwise combination splitting the adaptive filters into nonoverlapping blocks. This idea was previously applied to the plant identification problem, but has to be properly modified to obtain a suitable behavior in the equalization application. Moreover, we propose a scheme with the aim of further improving the equalization performance using the a priori knowledge of the energy distribution of the optimal inverse filter, where the block filters are chosen to fit with the coefficients energy distribution. Furthermore, the biased block-based filter is also introduced as a particular case of the combination scheme, especially suited for low signal-to-noise ratios SNRs or sparse scenarios. Although the combined schemes can be employed with any kind of adaptive filter, we employ the filtered-x improved proportionate normalized least mean square algorithm as basis of the proposed algorithms, allowing to introduce a novel combination scheme based on partitioned block schemes where different blocks of the adaptive filter use different parameter settings. Several experiments are included to evaluate the proposed algorithms in terms of convergence speed and steady-state behavior for different degrees of sparseness and SNRs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1732–1745},
numpages = {14}
}

@article{10.1109/TASLP.2016.2573048,
author = {Wang, Lin and Reiss, Joshua D. and Cavallaro, Andrea},
title = {Over-Determined Source Separation and Localization Using Distributed Microphones},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2573048},
doi = {10.1109/TASLP.2016.2573048},
abstract = {We propose an overdetermined source separation and localization method for a set of M microphones distributed around an unknown number, N &lt; M, of sources. We reformulate the overdetermined acoustic mixing procedure with a new determined mixing model and apply a determined M \texttimes{} M independent component analysis (ICA) in each frequency bin directly. The reformulated ICA operates without knowing N and also leads to better separation in reverberant scenarios. To solve the challenging permutation ambiguity problem, we first employ a time activity-based clustering approach to cluster the separated frequency components into M channels. We then propose a remixing procedure to detect and merge channels from the same source. The detection is done by analyzing time and frequency activities, spectral likeliness, and spatial location. To estimate the spatial location, we propose a time--frequency masking-based steered response power algorithm. Simulated and real-data experiments in a very challenging reverberant scenario confirm the effectiveness of the proposed method in obtaining the number of sources, the separated signals, and the location and spatial likelihood of each source.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1569–1584},
numpages = {16},
keywords = {source localization, blind source separation, over-determined mixture, permutation alignment}
}

@article{10.5555/2992829.2992835,
author = {Pan, Chao and Chen, Jingdong and Benesty, Jacob},
title = {Reduced-Order Robust Superdirective Beamforming with Uniform Linear Microphone Arrays},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
abstract = {Sensor arrays for audio and speech signal acquisition are generally required to have frequency-invariant beampatterns to avoid adding spectral distortion to the broadband signals of interest. One way to obtain frequency-invariant beampatterns is via superdirective beamforming. However, traditional superdirective beamformers may cause significant white noise amplification (particularly at low frequencies), making them sensitive to uncorrelated white noise. To circumvent the problem of white noise amplification, a method was developed to find the superdirective beamforming filter with a constraint on the white noise gain (WNG), leading to the so-called WNG-constrained superdirective beamformer. But this method damages the frequency invariance of the beampattern. In this paper, we develop a flatness-constrained robust superdirective beamformer. We divide the overall beamformer into two subbeamformers, which are convolved together: one subbeamformer forms a lower order superdirective beampattern while the other attempts to improve the WNG. We show that this robust approach can improve the WNG while limiting the frequency dependency of the beampattern at the same time.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1544–1555},
numpages = {12},
keywords = {white noise gain, microphone arrays, robust beamforming, cardioid, directivity factor, beampattern design, superdirective beamforming}
}

@article{10.1109/TASLP.2016.2560523,
author = {Delikaris-Manias, Symeon and Vilkamo, Juha and Pulkki, Ville},
title = {Signal-Dependent Spatial Filtering Based on Weighted-Orthogonal Beamformers in the Spherical Harmonic Domain},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2560523},
doi = {10.1109/TASLP.2016.2560523},
abstract = {Spatial filtering with microphone arrays is a technique that can be utilized to obtain the signal of a target sound source from a specific direction. Typical approaches in the field of audio underperform in practical environments with multiple sound sources and diffuse sound. In this contribution, we propose a post-filtering technique to suppress the effect of interferers and diffuse sound. The proposed technique utilizes the cross-spectral estimates of the output of two beamformers to formulate a time-frequency soft masker. The beamformers' outputs are used only for parameter estimation and not for generating an audio signal. Two sets of beamformer weights, a constant and an adaptive, are applied to the microphone array signals for the parameter estimation. The weights of the constant beamformer are designed such that they provide a spatially narrow beam pattern that is time and frequency invariant, having a unity gain toward the direction of interest. The weights of the adaptive beamformer are formulated using linearly constrained optimization with the constraint of weighted orthogonality with respect to the constant beamformer weights, as well as the unity gain toward the look direction. The orthogonality constraint provides diffuse sound suppression while the unity gain distortionless response. The cross spectrum of these two beamformers provides the target energy at a given look direction for the post filter. The study focuses on compact microphone arrays with which the typical beamforming techniques feature a tradeoff between noise amplification and spatial selectivity, especially in the low frequency region. The proposed method is evaluated with instrumental measures and listening tests under different reverberation times, in dual and multitalker scenarios. The evaluation shows that the proposed method provides a better performance when compared with a previous state-of-the-art spatial filter based on cross-pattern coherence, a linearly constrained beamformer and a Wiener postfilter.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1507–1519},
numpages = {13},
keywords = {post filtering, beamforming, microphone array signal processing, spherical harmonics, cross spectrum}
}

@article{10.1109/TASLP.2016.2577501,
author = {Karimian-Azari, Sam and Jensen, Jesper Rindom and Christensen, Mads Gr\ae{}b\o{}ll},
title = {Computationally Efficient and Noise Robust DOA and Pitch Estimation},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2577501},
doi = {10.1109/TASLP.2016.2577501},
abstract = {Many natural signals, such as voiced speech and some musical instruments, are approximately periodic over short intervals. These signals are often described in mathematics by the sum of sinusoids (harmonics) with frequencies that are proportional to the fundamental frequency, or pitch. In sensor (microphone) array signal processing, the periodic signals are estimated from spatiotemporal samples regarding to the direction of arrival (DOA) of the signal of interest. In this paper, we consider the problem of pitch and DOA estimation of quasi-periodic audio signals. In reallife scenarios, recorded signals are often contaminated by different types of noise, which challenges the assumption of white Gaussian noise in most state-of-the-art methods. We establish filtering methods based on noise statistics to apply to nonparametric spectral and spatial parameter estimates of the harmonics. We design minimum variance solutions with distortionless constraints to estimate the pitch from the frequency estimates, and to estimate the DOA from multichannel phase estimates of the harmonics. Applying this filtering method as the sum of weighted frequency and DOA estimates of the harmonics, we also design a joint DOA and pitch estimator. In white Gaussian noise, we derive even more computationally efficient solutions which are designed using the narrowband power spectrum of the harmonics. Numerical results reveal the performance of the estimators in colored noise compared with the Cram\'{e}r--Rao lower bound. Experiments on real-life signals indicate the applicability of the methods in practical low local signal-to-noise ratios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1609–1621},
numpages = {13},
keywords = {harmonic signal model, analytical signal, minimum variance distortionless response (MVDR), pitch estimation, direction of arrival (DOA) estimation}
}

@article{10.1109/TASLP.2016.2570945,
author = {FitzGerald, Derry and Liutkus, Antoine and Badeau, Roland},
title = {Projection-Based Demixing of Spatial Audio},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2570945},
doi = {10.1109/TASLP.2016.2570945},
abstract = {We propose a method to unmix multichannel audio signals into their different constitutive spatial objects. To achieve this, we characterize an audio object through both a spatial and a spectro-temporal modeling. The particularity of the spatial model we pick is that it neither assumes an object has only one underlying source point, nor does it attempt to model the complex room acoustics. Instead, it focuses on a listener perspective, and takes each object as the superposition of many contributions with different incoming directions and interchannel delays. Our spectro-temporal probabilistic model is based on the recently proposed α-harmonisable processes, which are adequate for signals with large dynamics, such as audio. Then, the main originality of this paper is to provide a new way to estimate and exploit interchannel dependences of an object for the purpose of demixing. In the Gaussian α = 2 case, previous research focused on covariance structures. This approach is no longer valid for α &lt; 2 where covariances are not defined. Instead, we show how simple linear combinations of the mixture channels can be used to learn the model parameters, and the method we propose consists in pooling the estimates based on many projections to correctly account for the original multichannel audio. Intuitively, each such downmix of the mixture provides a new perspective where some objects are canceled or enhanced. Finally, we also explain how to recover the different spatial audio objects when all parameters have been computed. Performance of the method is illustrated on the separation of stereophonic music signals.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1556–1568},
numpages = {13},
keywords = {probabilistic models, source separation, musical source separation, nonnegative matrix factorization (NMF)}
}

@article{10.1109/TASLP.2016.2562505,
author = {Li, Sheng and Akita, Yuya and Kawahara, Tatsuya},
title = {Semi-Supervised Acoustic Model Training by Discriminative Data Selection from Multiple ASR Systems' Hypotheses},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2562505},
doi = {10.1109/TASLP.2016.2562505},
abstract = {While the performance of ASR systems depends on the size of the training data, it is very costly to prepare accurate and faithful transcripts. In this paper, we investigate a semisupervised training scheme, which takes the advantage of huge quantities of unlabeled video lecture archive, particularly for the deep neural network (DNN) acoustic model. In the proposed method, we obtain ASR hypotheses by complementary GMM- and DNN-based ASR systems. Then, a set of CRF-based classifiers is trained to select the correct hypotheses and verify the selected data. The proposed hypothesis combination shows higher quality compared with the conventional system combination method (ROVER). Moreover, compared with the conventional data selection based on confidence measure score, our method is demonstrated more effective for filtering usable data. Significant improvement in the ASR accuracy is achieved over the baseline system and in comparison with the models trained with the conventional system combination and data selection methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1520–1530},
numpages = {11},
keywords = {speech recognition, semisupervised training, acoustic model, lecture transcription}
}

@article{10.5555/2992829.2992841,
author = {Kitamura, Daichi and Ono, Nobutaka and Sawada, Hiroshi and Kameoka, Hirokazu and Saruwatari, Hiroshi},
title = {Determined Blind Source Separation Unifying Independent Vector Analysis and Nonnegative Matrix Factorization},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
abstract = {This paper addresses the determined blind source separation problem and proposes a new effective method unifying independent vector analysis (IVA) and nonnegative matrix factorization (NMF). IVA is a state-of-the-art technique that utilizes the statistical independence between sources in a mixture signal, and an efficient optimization scheme has been proposed for IVA. However, since the source model in IVA is based on a spherical multivariate distribution, IVA cannot utilize specific spectral structures such as the harmonic structures of pitched instrumental sounds. To solve this problem, we introduce NMF decomposition as the source model in IVA to capture the spectral structures. The formulation of the proposed method is derived from conventional multichannel NMF (MNMF), which reveals the relationship between MNMF and IVA. The proposed method can be optimized by the update rules of IVA and single-channel NMF. Experimental results show the efficacy of the proposed method compared with IVA and MNMF in terms of separation accuracy and convergence speed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1622–1637},
numpages = {16},
keywords = {determined, nonnegative matrix factorization, independent vector analysis, blind source separation}
}

@article{10.1109/TASLP.2016.2580302,
author = {Obin, Nicolas and Roebel, Axel},
title = {Similarity Search of Acted Voices for Automatic Voice Casting},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2580302},
doi = {10.1109/TASLP.2016.2580302},
abstract = {This paper presents a large-scale similarity search of professionally acted voices for computer-aided voice casting. The proposed voice casting system explores Gaussian mixture model-based acoustic models and multilabel recognition of perceived paralinguistic content (speaker states and speaker traits, e.g., age/gender, voice quality, emotion) for the voice casting of professionally acted voices. First, acoustic models (universal background model, super-vector, i-vector) are constructed to model the acoustic space of voices, from which the similarity between voices can be measured directly in the acoustic space. Second, multiple binary classification of speaker traits and states is added to the acoustic models in order to represent the vocal signature of a voice, which is then used to measure the similarity between voices in the paralinguistic space. Finally, a similarity search is processed in order to determine the set of target actors that are the most similar to the voice of a source actor. In a subjective experiment conducted in the real-context of cross-language voice casting, the multilabel scoring system significantly outperforms the acoustic scoring system. This constitutes a proof of concept for the role of perceived para-linguistic categories in the perception of voice similarity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1638–1647},
numpages = {10},
keywords = {speaker recognition, para-linguistics, multi-label classification, voice similarity, speaker traits and states, voice casting}
}

@article{10.1109/TASLP.2016.2573050,
author = {Liu, Yang and Li, Sujian and Wei, Furu and Ji, Heng},
title = {Relation Classification via Modeling Augmented Dependency Paths},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2573050},
doi = {10.1109/TASLP.2016.2573050},
abstract = {Previous research on relation classification has verified the effectiveness of using dependency shortest paths or dependency subtrees. How to efficiently unify these two kinds of dependency information in relation classification is still an open problem. In this paper, we propose a novel structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop the dependency-based neural networks (DepNN) model which combines the advantages of the recursive neural network (RNN) and the convolutional neural network (CNN). In DepNN, RNN is designed to model the dependency subtrees since it is good at capturing the hierarchical structures. Then, the semantic representation in subtrees is passed to the nodes on the shortest path and CNN is used to get the most important features on the ADP. Experiments on the SemEval-2010 dataset show that the ADP structure including both the shortest dependency path and the attached subtrees is helpful to classify the semantic relations between two entities and our proposed method can achieve the state-of-the-art performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1585–1594},
numpages = {10},
keywords = {relation classification, shortest dependency path, dependency subtree, recursive neural network, convolution neural network}
}

@article{10.5555/2992829.2992839,
author = {Kuklasi\'{n}ski, Adam and Doclo, Simon and Jensen, S\o{}ren Holdt and Jensen, Jesper},
title = {Maximum Likelihood PSD Estimation for Speech Enhancement in Reverberation and Noise},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
abstract = {In this contribution, we focus on the problem of power spectral density (PSD) estimation from multiple microphone signals in reverberant and noisy environments. The PSD estimation method proposed in this paper is based on the maximum likelihood (ML) methodology. In particular, we derive a novel ML PSD estimation scheme that is suitable for sound scenes which besides speech and reverberation consists of an additional noise component whose second-order statistics are known. The proposed algorithm is shown to outperform an existing similar algorithm in terms of PSD estimation accuracy. Moreover, it is shown numerically that the mean-squared estimation error achieved by the proposed method is near the limit set by the corresponding Cram\'{e}r--Rao lower bound. The speech dereverberation performance of a multichannel Wiener filter based on the proposed PSD estimators is measured using several instrumental measures and is shown to be higher than when the competing estimator is used. Moreover, we perform a speech intelligibility test where we demonstrate that both the proposed and the competing PSD estimators lead to similar intelligibility improvements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1595–1608},
numpages = {14},
keywords = {PSD estimation, Cram\'{e}r--Rao lower bound, reverberation, microphone array, maximum likelihood estimation}
}

@article{10.1109/TASLP.2016.2547743,
author = {Cavalieri, Daniel C. and Palazuelos-Cagigas, Sira E. and Bastos-Filho, Teodiano F. and Sarcinelli-Filho, M\'{a}rio},
title = {Combination of Language Models for Word Prediction: An Exponential Approach},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2547743},
doi = {10.1109/TASLP.2016.2547743},
abstract = {This paper proposes an exponential interpolation to merge a part-of-speech-based language model and a word-based n-gram language model to accomplish word prediction tasks. In order to find a set of mathematical equations to properly describe the language modeling, a model based on partial differential equations is proposed. With the appropriate initial conditions, it was found an interpolation model similar to the traditional maximum entropy language model. Improvements in keystroke saved and perplexity over the word-based n-gram language model and two other traditional interpolation models is obtained, considering three different languages. The proposed interpolation model also provides additional improvement in hit rate parameter.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1477–1490},
numpages = {14},
keywords = {word prediction (WP), natural language processing (NLP), combination of language models}
}

@article{10.1109/TASLP.2016.2567645,
author = {Dittmar, Christian and M\"{u}ller, Meinard},
title = {Reverse Engineering the Amen Break: Score-Informed Separation and Restoration Applied to Drum Recordings},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2567645},
doi = {10.1109/TASLP.2016.2567645},
abstract = {This work addresses the extraction of high-quality component signals from drum solo recordings (breakbeats) for music production and remixing purposes. Specifically, we employ audio source separation techniques to recover sound events from the drum sound mixture corresponding to the individual drum strokes. Our separation approach is based on an informed variant of non-negative matrix factor deconvolution (NMFD) that has been proposed and applied to drum transcription and separation in earlier works. In this paper, we systematically study the suitability of NMFD and the impact of audio- and score-based side information in the context of drum separation. In the case of imperfect decompositions, we observe different cross-talk artifacts appearing during the attack and the decay segment of the extracted drum sounds. Based on these findings, we propose and evaluate two extensions to the core technique. The first extension is based on applying a cascaded NMFD decomposition while retaining selected side information. The second extension is a time--frequency selective restoration approach using a dictionary of single note drum sounds. For all our experiments, we use a publicly available data set consisting of multitrack drum recordings and corresponding annotations that allows us to evaluate the source separation quality. Using this test set, we show that our proposed methods improve the quality of the component signals.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1531–1543},
numpages = {13},
keywords = {music decomposition, score-informed music processing, source separation, signal reconstruction}
}

@article{10.1109/TASLP.2016.2553457,
author = {Schwartz, Ofer and Gannot, Sharon and Habets, Emanu\"{e}l A. P.},
title = {An Expectation-Maximization Algorithm for Multimicrophone Speech Dereverberation and Noise Reduction with Coherence Matrix Estimation},
year = {2016},
issue_date = {September 2016},
publisher = {IEEE Press},
volume = {24},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2553457},
doi = {10.1109/TASLP.2016.2553457},
abstract = {In speech communication systems, the microphone signals are degraded by reverberation and ambient noise. The reverberant speech can be separated into two components, namely, an early speech component that consists of the direct path and some early reflections and a late reverberant component that consists of all late reflections. In this paper, a novel algorithm to simultaneously suppress early reflections, late reverberation, and ambient noise is presented. The expectation-maximization (EM) algorithm is used to estimate the signals and spatial parameters of the early speech component and the late reverberation components. As a result, a spatially filtered version of the early speech component is estimated in the E-step. The power spectral density (PSD) of the anechoic speech, the relative early transfer functions, and the PSD matrix of the late reverberation are estimated in the M-step of the EM algorithm. The algorithm is evaluated using real room impulse response recorded in our acoustic lab with a reverberation time set to 0.36 s and 0.61 s and several signal-to-noise ratio levels. It is shown that significant improvement is obtained and that the proposed algorithm outperforms baseline single-channel and multichannel dereverberation algorithms, as well as a state-of-the-art multichannel dereverberation algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1491–1506},
numpages = {16},
keywords = {dereverberation, noise reduction, expectation-maximization}
}

@article{10.5555/2992818.2992828,
author = {Zhang, Meng and Liu, Yang and Luan, Huanbo and Sun, Maosong},
title = {Listwise Ranking Functions for Statistical Machine Translation},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
abstract = {Decision rules play an important role in the tuning and decoding steps of statistical machine translation. The traditional decision rule selects the candidate with the greatest potential from a candidate space by examining each candidate individually. However, viewing each candidate as independent imposes a serious limitation on the translation task. We instead view the problem from a ranking perspective that naturally allows the consideration of an entire list of candidates as a whole through the adoption of a listwise ranking function. Our shift from a pointwise to a list-wise perspective proves to be a simple yet powerful extension to current modeling that allows arbitrary pairwise functions to be incorporated as features, whose weights can be estimated jointly with traditional ones. We further demonstrate that our formulation encompasses the minimum Bayes risk (MBR) approach, another decision rule that considers restricted listwise information, as a special case. Experiments show that our approach consistently outperforms the baseline and MBR methods across the considered test sets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1464–1472},
numpages = {9},
keywords = {discriminative reranking, statistical machine translation, listwise ranking function}
}

@article{10.1109/TASLP.2016.2558822,
author = {Du, Jun and Tu, Yanhui and Dai, Li-Rong and Lee, Chin-Hui},
title = {A Regression Approach to Single-Channel Speech Separation via High-Resolution Deep Neural Networks},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2558822},
doi = {10.1109/TASLP.2016.2558822},
abstract = {We propose a novel data-driven approach to single-channel speech separation based on deep neural networks (DNNs) to directly model the highly nonlinear relationship between speech features of a mixed signal containing a target speaker and other interfering speakers. We focus our discussion on a semisupervised mode to separate speech of the target speaker from an unknown interfering speaker, which is more flexible than the conventional supervised mode with known information of both the target and interfering speakers. Two key issues are investigated. First, we propose a DNN architecture with dual outputs of the features of both the target and interfering speakers, which is shown to achieve a better generalization capability than that with output features of only the target speaker. Second, we propose using a set of multiple DNNs, each intending to be signal-noise-dependent (SND), to cope with the difficulty that one single general DNN could not well accommodate all the speaker mixing variabilities at different signal-to-noise ratio (SNR) levels. Experimental results on the speech separation challenge (SSC) data demonstrate that our proposed framework achieves better separation results than other conventional approaches in a supervised or semisupervised mode. SND-DNNs could also yield significant performance improvements over a general DNN for speech separation in low SNR cases. Furthermore, for automatic speech recognition (ASR) following speech separation, this purely front-end processing with a single set of speaker-independent ASR acoustic models, achieves a relative word error rate (WER) reduction of 11.6% over a state-of-the-art separation and recognition system where a complicated joint back-end decoding framework with multiple sets of speaker-dependent ASR acoustic models needs to be implemented. When speaker-adaptive ASR acoustic models for the target speakers are adopted for the enhanced signals, another 12.1% WER reduction over our best speaker-independent ASR system is achieved.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1424–1437},
numpages = {14},
keywords = {deep neural network, speech separation, divide and conquer, dual outputs, robust speech recognition}
}

@article{10.1109/TASLP.2016.2560534,
author = {Swietojanski, Pawel and Li, Jinyu and Renals, Steve},
title = {Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2560534},
doi = {10.1109/TASLP.2016.2560534},
abstract = {This work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions (LHUC) ---a method that linearly re-combines hidden units in a speaker- or environment-dependent manner using small amounts of unsupervised adaptation data. We also extend LHUC to a speaker adaptive training (SAT) framework that leads to a more adaptable DNN acoustic model, working both in a speaker-dependent and a speaker-independent manner, without the requirements to maintain auxiliary speaker-dependent feature extractors or to introduce significant speaker-dependent changes to the DNN structure. Through a series of experiments on four different speech recognition benchmarks (TED talks, Switchboard, AMI meetings, and Aurora4) comprising 270 test speakers, we show that LHUC in both its test-only and SAT variants results in consistent word error rate reductions ranging from 5% to 23% relative depending on the task and the degree of mismatch between training and test data. In addition, we have investigated the effect of the amount of adaptation data per speaker, the quality of unsupervised adaptation targets, the complementarity to other adaptation techniques, one-shot adaptation, and an extension to adapting DNNs trained in a sequence discriminative manner.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1450–1463},
numpages = {14},
keywords = {factorisation, adaptation, learning hidden unit contributions (lHUC), deep neural networks (DNNs)}
}

@article{10.5555/2992818.2992819,
author = {Schepker, Henning and Doclo, Simon},
title = {Least-Squares Estimation of the Common Pole-Zero Filter of Acoustic Feedback Paths in Hearing Aids},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
abstract = {In adaptive feedback cancellation both the convergence speed and the computational complexity depend on the number of adaptive parameters used to model the acoustic feedback paths. To reduce the number of adaptive parameters, it has been proposed to model the acoustic feedback paths as the convolution of a time-invariant common pole-zero filter and time-varying all-zero filters, enabling to track fast changes. In this paper, a novel procedure to estimate the common pole-zero filter of acoustic feedback paths is presented. In contrast to previous approaches which minimize the so-called equation-error, we propose to approximate the desired output-error minimization by employing a weighted least-squares procedure motivated by the Steiglitz--McBride iteration. The estimation of the common pole-zero filter is formulated as a semidefinite programming problem, to which a constraint based on the Lyapunov theory is added in order to guarantee the stability of the estimated pole-zero filter. Experimental results using measured acoustic feedback paths from a two microphone behind-the-ear hearing aid show that the proposed optimization procedure using the Lyapunov constraint outperforms existing optimization procedures in terms of modelling accuracy and added stable gain.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1334–1347},
numpages = {14},
keywords = {Steiglitz--McBride, semidefinite programming, weighted equation-error, acoustic feedback cancellation, common part modeling, hearing aids, Lyapunov stability}
}

@article{10.1109/TASLP.2016.2556282,
author = {Pessentheiner, Hannes and Hagm\"{u}ller, Martin and Kubin, Gernot},
title = {Localization and Characterization of Multiple Harmonic Sources},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2556282},
doi = {10.1109/TASLP.2016.2556282},
abstract = {We introduce a new and intuitive algorithm to characterize and localize multiple harmonic sources intersecting in the spatial and frequency domains. It jointly estimates their fundamental frequencies, their respective amplitudes, and their directions of arrival based on an intelligent non-parametric signal representation. To obtain these parameters, we first apply variable-scale sampling on unbiased cross-correlation functions between pairs of microphone signals to generate a joint parameter space. Then, we employ a multidimensional maxima detector to represent the parameters in a sparse joint parameter space. In comparison to others, our algorithm solves the issue of pitch-period doubling when using cross-correlation functions, it estimates multiple harmonic sources with a signal power smaller than the signal power of the dominant harmonic source, and it associates the estimated parameters to their corresponding sources in a multidimensional sparse joint parameter space, which can be directly fed into a tracker. We tested our algorithm and three others on synthetic data and speech data recorded in a real reverberant environment and evaluated their performance by employing the joint recall measure, the root-mean-square error, and the cumulative distribution function of fundamental frequencies and directions of arrival. The evaluations show promising results: Our algorithm outperforms the others in terms of the joint recall measure, and it can achieve root-mean-square errors of 1 Hz or 1° and smaller, which facilitates, e.g., distant-speech enhancement or source separation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1348–1363},
numpages = {16},
keywords = {pitch-period doubling, data association, pitch estimation, joint estimation, direction of arrival (DOA), microphone array, fundamental frequency, sparse joint parameter space (SJPS)}
}

@article{10.1109/TASLP.2016.2555085,
author = {Laufer, Bracha and Talmon, Ronen and Gannot, Sharon},
title = {Semi-Supervised Sound Source Localization Based on Manifold Regularization},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2555085},
doi = {10.1109/TASLP.2016.2555085},
abstract = {Conventional speaker localization algorithms, based merely on the received microphone signals, are often sensitive to adverse conditions, such as: high reverberation or low signal-to-noise ratio (SNR). In some scenarios, e.g., in meeting rooms or cars, it can be assumed that the source position is confined to a predefined area, and the acoustic parameters of the environment are approximately fixed. Such scenarios give rise to the assumption that the acoustic samples from the region of interest have a distinct geometrical structure. In this paper, we show that the high-dimensional acoustic samples indeed lie on a low-dimensional manifold and can be embedded into a low-dimensional space. Motivated by this result, we propose a semi-supervised source localization algorithm based on two-microphone measurements, which recovers the inverse mapping between the acoustic samples and their corresponding locations. The idea is to use an optimization framework based on manifold regularization, that involves smoothness constraints of possible solutions with respect to the manifold. The proposed algorithm, termed manifold regularization for localization, is adapted while new unlabelled measurements (from unknown source locations) are accumulated during runtime. Experimental results show superior localization performance when compared with a recently presented algorithm based on a manifold learning approach and with the generalized cross-correlation algorithm as a baseline. The algorithm achieves 2° accuracy in typical noisy and reverberant environments (reverberation time between 200 and 800 ms and SNR between 5 and 20 dB).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1393–1407},
numpages = {15},
keywords = {relative transfer function (RTF), diffusion distance, reproducing kernel Hilbert space (RKHS), sound source localization, manifold regularization}
}

@article{10.1109/TASLP.2016.2556422,
author = {Yang, Cheng-Yen and Liu, Chih-Wei and Jou, Shyh-Jye},
title = {A Systematic ANSI S1.11 Filter Bank Specification Relaxation and Its Efficient Multirate Architecture for Hearing-Aid Systems},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2556422},
doi = {10.1109/TASLP.2016.2556422},
abstract = {Recently, emerging mobile computing requires the high integration of hearing aids into a single system-on-chip. Modern hearing aid systems include a frequency decomposer, noise reduction, feedback cancellation, auditory compensation, and intelligent adaptation. The majority of existing works concentrated on improving the performance and efficiency on a single-signal processing block or one-sided effect. These works lacked comprehensive discussions on system-wide aspects regarding the overall impacts. To design an optimal hearing aid system, frequency decomposers, or the filter banks that dominate in hearing aid systems, are the first priority. We propose a systematic relaxation of the ANSI S1.11 specification and its design procedure for filter banks. The proposed design procedure overcomes the drawbacks of previous works and changes the five performance indices of the filter bank: delay, complexity, sub-band rate reduction, ripples of synthesized output, and prescription matchingerrors. These performance indices help system or algorithm designers in selecting a beneficial system-relaxed filter bank to achieve optimal hearing aids. The proposed multirate filter bank using the resampling method provides an efficient, low complexity, and delay-constrained computing architecture. Finally, seven design cases are used to demonstrate the proposed method, and comprehensive discussions of the five performance indices are presented.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1380–1392},
numpages = {13},
keywords = {hearing aids, NAL-NL1, ANSI S1.11, quasi-ANSI, multirate filter bank, prescription matching}
}

@article{10.1109/TASLP.2016.2556860,
author = {Khalilian, Hanieh and Baji\'{c}, Ivan V. and Vaughan, Rodney G.},
title = {Comparison of Loudspeaker Placement Methods for Sound Field Reproduction},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2556860},
doi = {10.1109/TASLP.2016.2556860},
abstract = {This paper presents a comparison between several loudspeaker placement methods for sound field reproduction (SFR). The goal of these placement methods is to reduce the SFR error under a power constraint by selecting suitable locations for the loudspeakers. The first method is based on singular value decomposition of the acoustic transfer function (ATF) matrix. Depending on the configuration, an ideal ATF matrix is created and, then, approximated by selecting the appropriate locations for the loudspeakers. Another method is based on the constrained matching pursuit (CMP) algorithm, in which candidate locations of the loudspeakers are selected iteratively to minimize the approximation error of the desired sound field. The third method is based on sparsity-promoting sound field approximation using the least absolute shrinkage and selection operator. Loudspeaker placements obtained using these methods are compared against benchmark configuration of uniformly distributed loudspeakers. The comparison indicates that for constrained power, the CMP-based placement has the least reproduction error.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1364–1379},
numpages = {16},
keywords = {sound field reproduction (SFR), singular value decomposition (SVD), least absolute shrinkage and selection operator (Lasso), constrained matching pursuit (CMP)}
}

@article{10.5555/2992818.2992824,
author = {Kounades-Bastian, Dionyssos and Girin, Laurent and Alameda-Pineda, Xavier and Gannot, Sharon and Horaud, Radu},
title = {A Variational EM Algorithm for the Separation of Time-Varying Convolutive Audio Mixtures},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
abstract = {This paper addresses the problem of separating audio sources from time-varying convolutive mixtures. We propose a probabilistic framework based on the local complex-Gaussian model combined with non-negative matrix factorization. The time-varying mixing filters are modeled by a continuous temporal stochastic process. We present a variational expectation--maximization (VEM) algorithm that employs a Kalman smoother to estimate the time-varying mixing matrix, and that jointly estimate the source parameters. The sound sources are then separated by Wiener filters constructed with the estimators provided by the VEM algorithm. Extensive experiments on simulated data show that the proposed method outperforms a blockwise version of a state-of-the-art baseline method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1408–1423},
numpages = {16},
keywords = {time-varying mixing filters, Kalman smoother, moving sources, variational EM, audio source separation}
}

@article{10.1109/TASLP.2016.2558826,
author = {Liu, Xunying and Chen, Xie and Wang, Yongqiang and Gales, Mark J. F. and Woodland, Philip C.},
title = {Two Efficient Lattice Rescoring Methods Using Recurrent Neural Network Language Models},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2558826},
doi = {10.1109/TASLP.2016.2558826},
abstract = {An important part of the language modelling problem for automatic speech recognition (ASR) systems, and many other related applications, is to appropriately model long-distance context dependencies in natural languages. Hence, statistical language models (LMs) that can model longer span history contexts, for example, recurrent neural network language models (RNNLMs), have become increasingly popular for state-of-the-art ASR systems. As RNNLMs use a vector representation of complete history contexts, they are normally used to rescore N-best lists. Motivated by their intrinsic characteristics, two efficient lattice rescoring methods for RNNLMs are proposed in this paper. The first method uses an n-gram style clustering of history contexts. The second approach directly exploits the distance measure between recurrent hidden history vectors. Both methods produced 1-best performance comparable to a 10 k-best rescoring baseline RNNLM system on two large vocabulary conversational telephone speech recognition tasks for US English and Mandarin Chinese. Consistent lattice size compression and recognition performance improvements after confusion network (CN) decoding were also obtained over the prefix tree structured N-best rescoring approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1438–1449},
numpages = {12},
keywords = {recurrent neural network, lattice rescoring, speech recognition, language model}
}

@article{10.1109/TASLP.2016.2544661,
author = {Zhou, Guangyou and Xie, Zhiwen and He, Tingting and Zhao, Jun and Hu, Xiaohua Tony},
title = {Learning the Multilingual Translation Representations for Question Retrieval in Community Question Answering via Non-Negative Matrix Factorization},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2544661},
doi = {10.1109/TASLP.2016.2544661},
abstract = {Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question--answer pairs) in the absence of which they are troubled by noise issues. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via non-negative matrix factorization. Experiments conducted on real CQA data sets show that our proposed approach is promising.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1305–1314},
numpages = {10},
keywords = {community question answering, text mining, question retrieval, natural language processing, information retrieval}
}

@article{10.1109/TASLP.2016.2516439,
author = {Gao, Min and Lu, Jing and Qiu, Xiaojun},
title = {A Simplified Subband ANC Algorithm without Secondary Path Modeling},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2516439},
doi = {10.1109/TASLP.2016.2516439},
abstract = {Active noise control algorithms without secondary path modeling are very appealing because they can effectively overcome the problems caused by imperfect secondary path model. For broadband noise control, subband structures have been adopted in these kinds of algorithms, where the full-band adaptive filter coefficients are obtained from subband filter coefficients with weight transformation. However, the usually used two-direction search method suffers from slow convergence if the secondary path phase is close to± 90° To deal with this problem, a four-direction search method has been proposed in frequency domain, which is sometimes too complicated for practical applications. In this paper, a simplified algorithm is proposed to reduce the implementation complexity. In the proposed algorithm, the delay of the secondary path is estimated first, and then the estimated delay is used in the delay compensation to reduce the number of subbands. Two reference signals are generated in each subband and two update directions are used. Only one subband reference signal and one update direction are selected to approximate the phase response of the residual secondary path, and then the full-band adaptive controller coefficients are adjusted directly in time domain with them without the need of further weight transformation. Just like those existing subband algorithms, the simplified algorithm can obtain similar steady-state noise reduction performance as that of the FxLMS algorithm but without secondary path modeling. Simulations and experiments are carried out to demonstrate the feasibility of the proposed algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1164–1174},
numpages = {11},
keywords = {adaptive controller, simplified subband algorithm, active noise control (ANC), secondary path modeling}
}

@article{10.5555/2992803.2992814,
author = {Das, Rajib Lochan and Chakraborty, Mrityunjoy},
title = {Improving the Performance of the PNLMS Algorithm Using <i>l</i><sub>1</sub> Norm Regularization},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
abstract = {The proportionate normalized least mean square (PNLMS) algorithm and its variants are by far the most popular adaptive filters that are used to identify sparse systems. The convergence speed of the PNLMS algorithm, though very high initially, however, slows down at a later stage, even becoming worse than sparsity agnostic adaptive filters like the NLMS. In this paper, we address this problem by introducing a carefully constructed l1 norm (of the coefficients) penalty in the PNLMS cost function which favors sparsity. This results in certain zero attracting terms in the PNLMS weight update equation which help in the shrinkage of the coefficients, especially the inactive taps, thereby arresting the slowing down of convergence and also producing lesser steady state excess mean square error (EMSE). A rigorous convergence analysis of the proposed algorithm is presented that expresses the steady state mean square deviation of both the active and the inactive taps in terms of a zero attracting coefficient of the algorithm. The analysis reveals that further reduction of the EMSE is possible by deploying a variable step size (VSS) simultaneously with a variable zero attracting coefficient in the weight update process. Simulation results confirm superior performance of the proposed VSS zero attracting PNLMS algorithm over existing algorithms, especially in terms of having both higher convergence speed and lesser steady state EMSE simultaneously.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1280–1290},
numpages = {11},
keywords = {PNLMS algorithm, sparse systems, mean square deviation (msd), zero attractors, variable step size}
}

@article{10.1109/TASLP.2016.2522643,
author = {Aihara, Ryo and Takiguchi, Tetsuya and Ariki, Yasuo},
title = {Multiple Non-Negative Matrix Factorization for Many-to-Many Voice Conversion},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2522643},
doi = {10.1109/TASLP.2016.2522643},
abstract = {A novel voice conversion (VC) method for arbitrary speakers is proposed. Non-negative matrix factorization (NMF) has recently been applied to exemplar-based VC. It offers noise robustness and naturalness of the converted voice, compared with widely used Gaussian mixture model-based VC. However, because NMF-based VC requires parallel training data from source and target speakers, the voice of arbitrary speakers cannot be converted in this framework. In this study, we propose the multiple non-negative matrix factorization (Multi-NMF) to allow the implementation of many-to-many, exemplar-based VC. Our experimental results demonstrate that the conversion quality of the proposed method is close to that of conventional one-to-one VC, even though the proposed method requires neither the source speakers' spectra, nor the target speakers' spectra, to be included in the training set.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1175–1184},
numpages = {10},
keywords = {non-negative matrix factorization (nmf), voice conversion, many-to-many, speech synthesis, exemplar-based}
}

@article{10.1109/TASLP.2016.2551863,
author = {Degottex, Gilles and Ardaillon, Luc and Roebel, Axel},
title = {Multi-Frame Amplitude Envelope Estimation for Modification of Singing Voice},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2551863},
doi = {10.1109/TASLP.2016.2551863},
abstract = {Singingvoice synthesis benefits from very highquality estimation of the resonances and anti-resonances of the vocal tract filter (VTF), i.e., an amplitude spectral envelope. In the state of the art, a single frame of DFT transform is commonly used as a basis for building spectral envelopes. Even though multiple frame analysis (MFA) has already been suggested for envelope estimation, it is not yet used in concrete applications. Indeed, even though existing attempts have shown very interesting results, we will demonstrate that they are either over complicated or fail to satisfy the high accuracy that is necessary for singing voice. In order to allow future applications of MFA, this article aims to improve the theoretical understanding and advantages of MFA-based methods. The use of singing voice signals is very beneficial for studying MFA methods due to the fact that the VTF configuration can be relatively stable and, at the same time, the vibrato creates a regular variation that is easy to model. By simplifying and extending previous works, we also suggest and describe two MFA-based methods. To better understand the behaviors of the envelope estimates, we designed numerical measurements to assess single frame analysis and MFA methods using synthetic signals. With listening tests, we also designed two proofs of concept using pitch scaling and conversion of timbre. Both evaluations show clear and positive results for MFA-based methods, thus, encouraging this research direction for future applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1242–1254},
numpages = {13},
keywords = {voice synthesis, spectral envelope, singing voice, voice analysis and modeling, multi frame analysis}
}

@article{10.1109/TASLP.2016.2551865,
author = {Wu, Zhizheng and King, Simon},
title = {Improving Trajectory Modelling for DNN-Based Speech Synthesis by Using Stacked Bottleneck Features and Minimum Generation Error Training},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2551865},
doi = {10.1109/TASLP.2016.2551865},
abstract = {We propose two novel techniques---stacking bottleneck features and minimum generation error (MGE) training criterion---to improve the performance of deep neural network (DNN)-based speech synthesis. The techniques address the related issues of frame-by-frame independence and ignorance of the relationship between static and dynamic features, within current typical DNN-based synthesis frameworks. Stacking bottleneck features, which are an acoustically informed linguistic representation, provides an efficient way to include more detailed linguistic context at the input. The MGE training criterion minimises overall output trajectory error across an utterance, rather than minimising the error per frame independently, and thus takes into account the interaction between static and dynamic features. The two techniques can be easily combined to further improve performance. We present both objective and subjective results that demonstrate the effectiveness of the proposed techniques. The subjective results show that combining the two techniques leads to significantly more natural synthetic speech than from conventional DNN or long short-term memory recurrent neural network systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1255–1265},
numpages = {11},
keywords = {deep neural network, acoustic modelling, bottleneck, minimum generation error, speech synthesis}
}

@article{10.1109/TASLP.2016.2549699,
author = {Deepak, K. T. and Prasanna, S. R. Mahadeva},
title = {Foreground Speech Segmentation and Enhancement Using Glottal Closure Instants and Mel Cepstral Coefficients},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2549699},
doi = {10.1109/TASLP.2016.2549699},
abstract = {In this paper, the speech signal recorded from the desired speaker close to microphone in natural environment is regarded as foreground speech and rest of the interfering sources as background noise. The proposed paper exploits speech production features like glottal closure instants in time domain and vocal tract information in spectral domain to segment the desired speaker's speech and to further enhance it. The foreground speech is perceptually enhanced using the auditory perception feature in mel-frequency domain using mel-cepstral coefficients and its inversion using mel log spectrum approximation filter. The focus is on enhancing the production and perceptual features of foreground speech rather than relying on modeling the interfering sources. The speech data are collected in different natural environments from different speakers in order to evaluate the proposed method. The enhanced speech signals derived at three different stages of the proposed method are evaluated with state-of-the-art methods in terms of subjective and objective measures. The proposed method provides improved performance compared to the considered state-of-the-art methods. In terms of the proposed objective measure foreground to background Ratio, the enhancement approach presented in this paper gives an average improvement of 12 dB as opposed to existing spectral subtraction-based method which provides 3 dB. Moreover, subjective evaluation using 24 different subjects corroborates the objective test results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1204–1218},
numpages = {15},
keywords = {MCC, MLSA, speech enhancement, zero band filter, foreground segmentation, glottal closure instants}
}

@article{10.1109/TASLP.2016.2551864,
author = {Sahoo, Subhasmita and Routray, Aurobinda},
title = {A Novel Method of Glottal Inverse Filtering},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2551864},
doi = {10.1109/TASLP.2016.2551864},
abstract = {This paper presents a new technique for glottal inverse filtering using a distributed model of the vocal tract. A discrete state space model has been constructed for the speech production system by combining the concatenated tube model of the vocal tract and Liljencrants--Fant (LF) model of the glottal flow derivative waveform. An adaptive system identification technique, basedonextended Kalmanfiltering, has been used for estimation of the states and model parameters from continuous speech. The glottal signal, represented by the LF model, is piecewise differentiable in one glottal cycle. Hence, the hybrid system has been characterized by separate models during two different modes. Multiple model estimation has been performed by switching between the two models at the mode jumps. The open phase of the glottal cycle has been considered as Mode 1; whereas, the return phase and closed phase combined has been taken as Mode 2. The starting point of Mode 1, also known as glottal opening instant, was estimated by observing formant modulation, which remains negligible during closed phase, and starts to increase at the onset of opening. The starting point of Mode 2, also known as the glottal closing instant, was computed by peak-picking from linear prediction (LP) residual signal. The proposed method estimates the glottal waveform as well as changes in flow occurring at different sections of the vocal tract during speech production. This technique has been found to be accurate and robust to variations in pitch as compared to other LP-based methods in the literature. The method also estimates the air pressure distribution at different sections of the vocal tract.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1230–1241},
numpages = {12},
keywords = {LF model, extended Kalman filter, concatenated tube model, multiple model estimation, glottal inverse filtering}
}

@article{10.1109/TASLP.2016.2551041,
author = {Hajimolahoseini, Habib and Amirfattahi, Rassoul and Gazor, Saeed and Soltanian-Zadeh, Hamid},
title = {Robust Estimation and Tracking of Pitch Period Using an Efficient Bayesian Filter},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2551041},
doi = {10.1109/TASLP.2016.2551041},
abstract = {In this paper, we introduce an algorithm for estimating and tracking the pitch period of audio signals using Bayesian filters. For this purpose, we propose a general Bayesian model, which is robust to the nonstationary variations of the amplitude and frequency of the input signal. We also employ a state-space model, which uses the delayed versions of the input signal to model the periodicity of nonstationary audio signals. This simple model allows a significant reduction of the required number of particles for the estimation of the pitch period compared to the state-of-the-art particle filtering methods. Moreover, we propose to estimate the logarithm of the period instead of the period itself. We show that the resulting algorithm does not require prior knowledge about the initial state and is robust to the octave error phenomenon, which is a common problem in pitch period estimation methods. Most of the existing methods require that the processing window be longer than the largest existing period of the input signal. In contrast, the proposed method does not impose such a limit. Our method often results in a higher time-domain resolution with no perceptible compromise on the frequency-domain resolution, especially for high-pitched audio signals such as music. Simulation results reveal that the proposed algorithm outperforms the state-of-the-art pitch period detection algorithms at low signal to noise ratios assuming no prior knowledge about the initial conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1219–1229},
numpages = {11},
keywords = {audio signals, real-time estimation, Bayesian filtering, time-varying pitch period estimation, particle filtering, state-space model}
}

@article{10.5555/2992803.2992806,
author = {Chen, Kai and Huo, Qiang},
title = {Training Deep Bidirectional LSTM Acoustic Model for LVCSR by a Context-Sensitive-Chunk BPTT Approach},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
abstract = {This paper presents a study of using deep bidirectional long short-term memory (DBLSTM) recurrent neural network as acoustic model for DBLSTM-HMM based large vocabulary continuous speech recognition (LVCSR), where a context-sensitive-chunk (CSC) back-propagation through time (BPTT) approach is used to train DBLSTM by splitting each training sequence into chunks with appended contextual observations, and a CSC-based decoding method with possibly overlapped CSCs is used for recognition. Our approach makes mini-batch based training on GPU more efficient and reduces the latency of DBLSTM-based LVCSR from a whole utterance to a short chunk. Evaluations have been made on Switchboard-I benchmark task. In comparison with epoch-wise BPTT training, our method can achieve more than three times speedup on a single GPU card without degrading recognition accuracy. In comparison with a highly optimized DNN-HMM system trained by a frame-level cross entropy (CE) criterion, our CE-trained DBLSTM-HMM system achieves relative word error rate reductions of 9% and 5% on Eval2000 and RT03S testing sets, respectively. Furthermore, by running model averaging based parallel training of DBLSTM on a cluster of GPUs, CSC-BPTT incurs less accuracy degradation than epoch-wise BPTT while achieves a linear speedup.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1185–1193},
numpages = {9},
keywords = {BPTT, long short-term memory, model averaging, DNN, context sensitive chunk, DBLSTM, parallel training, LVCSR}
}

@article{10.1109/TASLP.2016.2553441,
author = {Jaureguiberry, Xabier and Vincent, Emmanuel and Richard, Ga\"{e}l},
title = {Fusion Methods for Speech Enhancement and Audio Source Separation},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2553441},
doi = {10.1109/TASLP.2016.2553441},
abstract = {A wide variety of audio source separation techniques exist and can already tackle many challenging industrial issues. However, in contrast with other application domains, fusion principles were rarely investigated in audio source separation despite their demonstrated potential in classification tasks. In this paper, we propose a general fusion framework which takes advantage of the diversity of existing separation techniques in order to improve separation quality. We obtain new source estimates by summing the individual estimates given by different separation techniques weighted by a set of fusion coefficients. We investigate three alternative fusion methods which are based on standard nonlinear optimization, Bayesian model averaging, or deep neural networks. Experiments conducted for both speech enhancement and singing voice extraction demonstrate that all the proposed methods outperform traditional model selection. The use of deep neural networks for the estimation of time-varying coefficients notably leads tolarge quality improvements, up to 3 dB in terms of signal-to-distortion ratio compared to model selection.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1266–1279},
numpages = {14},
keywords = {aggregation, variational Bayes, deep learning, audio source separation, non-negative matrix factorization (NMF), model averaging, ensemble, fusion, deep neural networks (DNNs), speech enhancement, singing voice extraction}
}

@article{10.1109/TASLP.2016.2545928,
author = {Kim, Chanwoo and Stern, Richard M.},
title = {Power-Normalized Cepstral Coefficients (PNCC) for Robust Speech Recognition},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2545928},
doi = {10.1109/TASLP.2016.2545928},
abstract = {This paper presents a new feature extraction algorithm called power normalized Cepstral coefficients (PNCC) that is motivated by auditory processing. Major new features of PNCC processing include the use of a power-law nonlinearity that replaces the traditional log nonlinearity used in MFCC coefficients, a noise-suppression algorithm based on asymmetric filtering that suppresses background excitation, and a module that accomplishes temporal masking. We also propose the use of medium-time power analysis in which environmental parameters are estimated over a longer duration than is commonly used for speech, as well as frequency smoothing. Experimental results demonstrate that PNCC processing provides substantial improvements in recognition accuracy compared to MFCC and PLP processing for speech in the presence of various types of additive noise and in reverberant environments, with only slightly greater computational cost than conventional MFCC processing, and without degrading the recognition accuracy that is observed while training and testing using clean speech. PNCC processing also provides better recognition accuracy in noisy environments than techniques such as vector Taylor series (VTS) and the ETSI advanced front end (AFE) while requiring much less computation. We describe an implementation of PNCC using "online processing" that does not require future knowledge of the input.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1315–1329},
numpages = {15},
keywords = {asymmetric filtering, medium-time power estimation, spectral weight smoothing, rate-level curve, on-line speech processing, feature extraction, temporal masking, physiological modeling, robust speech recognition, power function, modulation filtering}
}

@article{10.1109/TASLP.2016.2540815,
author = {Taseska, Maja and Habets, Emanu\"{e}l A. P.},
title = {Spotforming: Spatial Filtering with Distributed Arrays for Position-Selective Sound Acquisition},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2540815},
doi = {10.1109/TASLP.2016.2540815},
abstract = {Hands-free capture of speech often requires extraction of sources from a certain spot of interest (SOI), while reducing interferers and background noise. Although state-of-the-art spatial filters are fully data-dependent and computed using the power spectral density (PSD) matrices of the desired and the undesired signals, the existing solutions to extract sources from a SOI are only partially data-dependent. Estimating the time-varying PSD matrices from the data is a challenging problem, especially in dynamic and quickly time-varying acoustic scenes. Hence, the spot signal statistics are often pre-computed based on a near-field propagation model, resulting in suboptimal filters. In this work, we propose a fully data-dependent spatial filtering framework for extraction of speech signals that originate from a SOI. To achieve position-based spatial selectivity, distributed arrays are used, which offer larger spatial diversity compared to arrays of closely spaced microphones. The PSD matrices of the desired and the undesired signals are updated at each time-frequency bin by using a minimum Bayes risk detector that is based on a probabilistic model of narrowband position estimates. The proposed framework is applicable in challenging multitalk situations, without requiring any prior information, except the geometry, location, and orientation of the arrays.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1291–1304},
numpages = {14},
keywords = {PSD matrix estimation, signal detection., distributed arrays, spatial filtering, source extraction}
}

@article{10.1109/TASLP.2016.2546458,
author = {Stafylakis, Themos and Alam, Md. Jahangir and Kenny, Patrick},
title = {Text-Dependent Speaker Recognition with Random Digit Strings},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2546458},
doi = {10.1109/TASLP.2016.2546458},
abstract = {In this paper, we explore joint factor analysis (JFA) for text-dependent speaker recognition with random digit strings. The core of the proposed method is a JFA model by which we extract features. These features can either represent overall utterances or individual digits, and are fed into a trainable backend to estimate likelihood ratios. Within this framework, several extensions are proposed. First is a logistic regression method for combining log-likelihood ratios that correspond to individual mixture components. Second is the extraction of phonetically aware Baum--Welch statistics, by using forced alignment instead of the typical posterior probabilities that are derived by the universal background model. We also explore a digit-string-dependent way to apply score normalization that exhibits a notable improvement compared to the standard one. By fusing six JFA features, we attained 2.01% and 3.19% equal error rates on male and female, respectively, on the challenging RSR2015 (part III) dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1194–1203},
numpages = {10},
keywords = {text-dependent speaker recognition, joint factor analysis}
}

@article{10.1109/TASLP.2016.2537982,
author = {Chen, Yan-You and Wu, Chung-Hsien and Huang, Yi-Chin and Lin, Shih-Lun and Wang, Jhing-Fa},
title = {Candidate Expansion and Prosody Adjustment for Natural Speech Synthesis Using a Small Corpus},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2537982},
doi = {10.1109/TASLP.2016.2537982},
abstract = {This study proposes a hybrid approach to natural-sounding speech synthesis based on candidate expansion, unit selection, and prosody adjustment using a small corpus. The proposed method is more specific to tonal language, in particular Mandarin. In conventional speech synthesis studies, the quality of synthesized speech depends heavily on the size of the speech corpus. However, it is highly time-consuming and labor-intensive to prepare a large labeled corpus. In this work, candidate expansion is proposed to retrieve potential candidates that are unlikely to be retrieved using only linguistic features. The optimal unit sequence is then obtained from the expanded candidates by using the proposed unit selection mechanism at the phoneme and prosodic word levels. Finally, a prosodic word-level prosody adjustment is proposed to improve the continuity and smoothness of the prosody of the synthesized speech. To evaluate the proposed method, the Tsing-Hua corpus of speech synthesis was adopted. The results of an objective evaluation demonstrate the effectiveness of candidate expansion and the improvement of the continuity and smoothness of the prosody of the synthesized speech. The results of a subjective evaluation also show the proposed system could synthesize the speech with improved quality and naturalness, in particular for a small-sized or resource-limited corpus.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1052–1065},
numpages = {14},
keywords = {prosody adjustment, hybrid speech synthesis, articulatory features, candidate expansion}
}

@article{10.1109/TASLP.2016.2533865,
author = {Chen, Lijiang and Mao, Xia and Yan, Hong},
title = {Text-Independent Phoneme Segmentation Combining EGG and Speech Data},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2533865},
doi = {10.1109/TASLP.2016.2533865},
abstract = {A new approach for text-independent phoneme segmentation at sampling point level is proposed in this paper. The algorithm consists of two phases: First, the voiced sections in speech data are detected using the information of vocal folds vibration contained in electroglottograph (EGG). A Hilbert envelope feature is adopted to achieve sampling point level detection accuracy. Second, the voiced sections and other sections are treated separately. Each voiced section is divided into several candidate phonemes using the Viterbi algorithm. Then adjacent candidate phonemes are merged based on a Hotellings T-square test method. For other sections, the unvoiced consonants are detected from silence based on a singularity exponent feature. Comparison experiments show that the proposed method has better performance than the existing ones for a variety of tolerances, and is more robust to noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1029–1037},
numpages = {9},
keywords = {electroglottograph, singularity exponent, hotellings t-square test, viterbi algorithm, hilbert envelope}
}

@article{10.1109/TASLP.2016.2537202,
author = {Tavakoli, Vincent Mohammad and Jensen, Jesper Rindom and Christensen, Mads Gr\ae{}sb\o{}ll and Benesty, Jacob},
title = {A Framework for Speech Enhancement with Ad Hoc Microphone Arrays},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2537202},
doi = {10.1109/TASLP.2016.2537202},
abstract = {Speech enhancement is vital for improved listening practices. Ad hoc microphone arrays are promising assets for this purpose. Most well-established enhancement techniques with conventional arrays can be adapted into ad hoc scenarios. Despite recent efforts to introduce various ad hoc speech enhancement apparatus, a common framework for integration of conventional methods into this new scheme is still missing. This paper establishes such an abstraction based on inter and intra subarray speech coherencies. Along with measures for signal quality at the input of subarrays, a measure of coherency is proposed both for subarray selection in local enhancement approaches, and also for selecting a proper global reference when more than one subarray are used. Proposed methods within this framework are evaluated with regard to quantitative and qualitative measures, including array gains, the speech distortion ratio, the PESQ measure, and the STOI intelligibility measure. Major findings in this work are the observed changes in the superiority of different methods for certain conditions. When perceptual quality or intelligibility of the speech are the ultimate goals, there are turning points where the MVDR and the LCMV are superior to Wiener-based methods. Also, for certain scenarios, local approaches may be preferred to global ones.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1038–1051},
numpages = {14},
keywords = {pseudo-coherence vector, microphone array, speech enhancement, multichannel, ad hoc array, noise reduction}
}

@article{10.1109/TASLP.2016.2533859,
author = {Wang, Lin and Hon, Tsz-Kin and Reiss, Joshua D. and Cavallaro, Andrea},
title = {An Iterative Approach to Source Counting and Localization Using Two Distant Microphones},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2533859},
doi = {10.1109/TASLP.2016.2533859},
abstract = {We propose a time difference of arrival (TDOA) estimation framework based on time-frequency inter-channel phase difference (IPD) to count and localize multiple acoustic sources in a reverberant environment using two distant microphones. The time-frequency (T-F) processing enables exploitation of the non-stationarity and sparsity of audio signals, increasing robustness to multiple sources and ambient noise. For inter-channel phase difference estimation, we use a cost function, which is equivalent to the generalized cross correlation with phase transform (GCC) algorithm and which is robust to spatial aliasing caused by large inter-microphone distances. To estimate the number of sources, we further propose an iterative contribution removal (ICR) algorithm to count and locate the sources using the peaks of the GCC function. In each iteration, we first use IPD to calculate the GCC function, whose highest peak is detected as the location of a sound source; then we detect the T-F bins that are associated with this source and remove them from the IPD set. The proposed ICR algorithm successfully solves the GCC peak ambiguities between multiple sources and multiple reverberant paths.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1079–1093},
numpages = {15},
keywords = {ipd, microphone array, source counting, tdoa estimation, gcc-phat}
}

@article{10.1109/TASLP.2015.2511925,
author = {Celikyilmaz, Asli and Sarikaya, Ruhi and Jeong, Minwoo and Deoras, Anoop},
title = {An Empirical Investigation of Word Class-Based Features for Natural Language Understanding},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2511925},
doi = {10.1109/TASLP.2015.2511925},
abstract = {There are many studies that show using class-based features improves the performance of natural language processing (NLP) tasks such as syntactic part-of-speech tagging, dependency parsing, sentiment analysis, and slot filling in natural language understanding (NLU), but not much has been reported on the underlying reasons for the performance improvements. In this paper, we investigate the effects of the word class-based features for the exponential family of models specifically focusing on NLU tasks, and demonstrate that the performance improvements could be attributed to the regularization effect of the class-based features on the underlying model. Our hypothesis is based on empirical observation that shrinking the sum of parameter magnitudes in an exponential model tends to improve performance. We show on several semantic tagging tasks that there is a positive correlation between the model size reduction by the addition of the class-based features and the model performance on a held-out dataset. We also demonstrate that class-based features extracted from different data sources using alternate word clustering methods can individually contribute to the performance gain. Since the proposed features are generated in an unsupervised manner without significant computational overhead, the improvements in performance largely come for free and we show that such features provide gains for a wide range of tasks from semantic classification and slot tagging in NLU to named entity recognition (NER).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {994–1005},
numpages = {12},
keywords = {conditional random fields, regularization, exponential models, class-based features, shrinkage features, natural language understanding}
}

@article{10.1109/TASLP.2016.2541303,
author = {Ouali, Chahid and Dumouchel, Pierre and Gupta, Vishwa},
title = {Fast Audio Fingerprinting System Using GPU and a Clustering-Based Technique},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2541303},
doi = {10.1109/TASLP.2016.2541303},
abstract = {In this paper, we present our audio fingerprinting system that detects a transformed copy of an audio from a large collection of audios in a database. The audio fingerprints in this system encode the positions of salient regions of binary images derived from a spectrogram matrix. The similarity between two fingerprints is defined as the intersection of their elements (i.e. positions of the salient regions). The search algorithm labels each reference fingerprint in the database with the closest query frame and then counts the number of matching frames when the query is overlaid over the reference. The best match is based on this count. The salient regions fingerprints together with this nearest-neighbor search give excellent copy detection results. However, for a large database, this search is time consuming. To reduce the search time, we accelerate this similarity search by using a graphics processing unit (GPU). To speed this search even further, we use a two-step search based on a clustering technique and a lookup table that reduces the number of comparisons between the query and the reference fingerprints. We also explore the tradeoff between the speed of search and the copy detection performance. The resulting system achieves excellent results on TRECVID 2009 and 2010 datasets and outperforms several state-of-the-art audio copy detection systems in detection performance, localization accuracy and run time. For a fast detection scenario with detection speed comparable to the Ellis' Shazam-based system, our system achieved the same min NDCR as the NN-based system, and significantly better detection accuracy than Ellis' Shazam-based system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1106–1118},
numpages = {13},
keywords = {clustering, parallel processing, content-based copy detection, fast search, audio fingerprint, gpu}
}

@article{10.1109/TASLP.2016.2526782,
author = {Qian, Xiaojun and Meng, Helen and Soong, Frank},
title = {A Two-Pass Framework of Mispronunciation Detection and Diagnosis for Computer-Aided Pronunciation Training},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2526782},
doi = {10.1109/TASLP.2016.2526782},
abstract = {This paper presents a two-pass framework with discriminative acoustic modeling for mispronunciation detection and diagnoses (MD&amp;D). The first pass of mispronunciation detection does not require explicit phonetic error pattern modeling. The framework instantiates a set of antiphones and a filler model to augment the original phone model for each canonical phone. This guarantees full coverage of all possible error patterns while maximally exploiting the phonetic information derived from the text prompt. The antiphones can be used to detect substitutions. The filler model can detect insertions, and phone skips are allowed to detect deletions. As such, there is no prior assumption on the possible error patterns that can occur. The second pass of mispronunciation diagnosis expands the detected insertions and substitutions into phone networks, and another recognition pass attempts to reveal the phonetic identities of the detected mispronunciation errors. Discriminative training (DT) is applied respectively to the acoustic models of the mispronunciation detection pass and the mispronunciation diagnosis pass. DT effectively separates the acoustic models of the canonical phones and the antiphones. Overall, with DT in both passes of MD&amp;D, the error rate is reduced by 40.4% relative, compared with the maximum likelihood baseline. After DT, the error rates of the respective passes are also lower than those of a strong single-pass baseline with DT by 1.3% and 5.1% relative which are statistically significant.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1020–1028},
numpages = {9},
keywords = {discriminative training, mispronunciation detection and diagnosis, computer-aided pronunciation training}
}

@article{10.1109/TASLP.2016.2522646,
author = {Nguyen, Duc Hoang Ha and Xiao, Xiong and Chng, Eng Siong and Li, Haizhou},
title = {Feature Adaptation Using Linear Spectro-Temporal Transform for Robust Speech Recognition},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2522646},
doi = {10.1109/TASLP.2016.2522646},
abstract = {Spectral information represents short-term speech information within a frame of a few tens of milliseconds, while temporal information captures the evolution of speech statistics over consecutive frames. Motivated by the findings that human speech comprehension relies on the integrity of both the spectral content and temporal envelope of speech signal, we study a spectro-temporal transform framework that adapts run-time speech features to minimize the mismatch between run-time and training data, and its implementation that includes cross transform and cascaded transform. A Kullback-Leibler divergence based cost function is proposed to estimate the transform parameters. We conducted experiments on the REVERB Challenge 2014 task, where clean and multi-condition trained acoustic models are tested with real reverberant and noisy speech. We found that temporal information is important for reverberant speech recognition and the simultaneous use of spectral and temporal information for feature adaptation is effective. We also investigate the combination of the cross transform with fMLLR, the combination of batch, utterance and speaker mode adaptation, and multicondition adaptive training using proposed transforms. All experiments consistently report significant word error rate reductions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1006–1019},
numpages = {14},
keywords = {robust speech recognition, temporal filtering, feature adaptation, linear transform}
}

@article{10.1109/TASLP.2016.2541299,
author = {Raposo, Francisco and Ribeiro, Ricardo and de Matos, David Martins},
title = {Using Generic Summarization to Improve Music Information Retrieval Tasks},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2541299},
doi = {10.1109/TASLP.2016.2541299},
abstract = {In order to satisfy processing time constraints, many music information retrieval (MIR) tasks process only a segment of the whole music signal. This may lead to decreasing performance, as the most important information for the tasks may not be in the processed segments. We leverage generic summarization algorithms, previously applied to text and speech, to summarize items in music datasets. These algorithms build summaries (both concise and diverse), by selecting appropriate segments from the input signal, also making them good candidates to summarize music. We evaluate the summarization process on binary and multiclass music genre classification tasks, by comparing the accuracy when using summarized datasets against the accuracy when using human-oriented summaries, continuous segments (the traditional method used for addressing the previously mentioned time constraints), and full songs of the original dataset. We show that GRASSHOPPER, LexRank, LSA, MMR, and a Support Sets-based centrality model improve classification performance when compared to selected baselines. We also show that summarized datasets lead to a classification performance whose difference is not statistically significant from using full songs. Furthermore, we make an argument stating the advantages of sharing summarized datasets for future MIR research.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1119–1128},
numpages = {10},
keywords = {music classification, automatic music summarization, generic summarization algorithms}
}

@article{10.1109/TASLP.2016.2545920,
author = {Taghia, Jalal and Martin, Rainer},
title = {A Frequency-Domain Adaptive Line Enhancer with Step-Size Control Based on Mutual Information for Harmonic Noise Reduction},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2545920},
doi = {10.1109/TASLP.2016.2545920},
abstract = {We propose an adaptive line enhancer with a frequency-dependent step-size. The proposed frequency-domain adaptive line enhancer is used as a single-channel noise reduction system for removing harmonic noise from noisy speech. Our main contribution is to exploit the temporal dependence in the log-magnitude and phase spectra of the noisy speech using mutual information, and to derive a frequency-dependent step-size which detects the presence of harmonic noise in different frequency bins. Our proposed step-size control allows the suppression of harmonic noise and the preservation of speech components. The experiments are performed with different real-life acoustic noises which contain harmonic components. Using instrumental speech intelligibility and quality measures, we demonstrate that the proposed approach can outperform the conventional frequency-domain adaptive line enhancer with a fixed step-size for harmonic noise reduction.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1140–1154},
numpages = {15},
keywords = {speech enhancement, correlation, mutual information, step-size control, harmonic noise, adaptive line enhancer}
}

@article{10.1109/TASLP.2016.2540805,
author = {Zhang, Xueliang and Zhang, Hui and Nie, Shuai and Gao, Guanglai and Liu, Wenju},
title = {A Pairwise Algorithm Using the Deep Stacking Network for Speech Separation and Pitch Estimation},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2540805},
doi = {10.1109/TASLP.2016.2540805},
abstract = {Speech separation and pitch estimation in noisy conditions are considered to be a "chicken-and-egg" problem. On one hand, pitch information is an important cue for speech separation. On the other hand, speech separation makes pitch estimation easier when background noise is removed. In this paper, we propose a supervised learning architecture to solve these two problems iteratively. The proposed algorithm is based on the deep stacking network (DSN), which provides a method for stacking simple processing modules to build deep architectures. Each module is a classifier whose target is the ideal binary mask (IBM), and the input vector includes spectral features, pitch-based features and the output from the previous module. During the testing stage, we estimate the pitch using the separation results and update the pitch-based features to the next module. When embedded into the DSN, pitch estimation and speech separation each run several times. We obtain the final results from the last module. Systematic evaluations show that the proposed system results in both a high quality estimated binary mask and accurate pitch estimation and outperforms recent systems in its generalization ability.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1066–1078},
numpages = {13},
keywords = {computational auditory scene analysis, pitch estimation, speech separation, supervised learning}
}

@article{10.1109/TASLP.2016.2544660,
author = {Li, Lantian and Wang, Dong and Zhang, Chenhao and Zheng, Thomas Fang},
title = {Improving Short Utterance Speaker Recognition by Modeling Speech Unit Classes},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2544660},
doi = {10.1109/TASLP.2016.2544660},
abstract = {Short utterance speaker recognition (SUSR) is highly challenging due to the limited enrollment and/or test data. We argue that the difficulty can be largely attributed to the mismatched prior distributions of the speech data used to train the universal background model (UBM) and those for enrollment and test. This paper presents a novel solution that distributes speech signals into a multitude of acoustic subregions that are defined by speech units, and models speakers within the subregions. To avoid data sparsity, a data-driven approach is proposed to cluster speech units into speech unit classes, based on which robust subregion models can be constructed. Further more, we propose a model synthesis approach based on maximum likelihood linear regression (MLLR) to deal with no-data speech unit classes. The experiments were conducted on a publicly available database SUD12. The results demonstrated that on a text-independent speaker recognition task where the test utterances are no longer than 2 seconds and mostly shorter than 0.5 seconds, the proposed sub-region modeling offered a 21.51% relative reduction in equal error rate (EER), compared with the standard GMM-UBM baseline. In addition, with the model synthesis approach, the performance can be greatly improved in scenarios where no enrollment data are available for some speech unit classes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1129–1139},
numpages = {11},
keywords = {model synthesis, subregion model, speaker recognition, short utterance}
}

@article{10.1109/TASLP.2016.2536481,
author = {O'Leary, Se\'{a}n and R\"{o}bel, Axel},
title = {A Montage Approach to Sound Texture Synthesis},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2536481},
doi = {10.1109/TASLP.2016.2536481},
abstract = {Sound texture synthesis has applications in creating audio scenes for film and video games. In this paper, a novel algorithm for sound texture synthesis is presented. The goal of this algorithm is to produce new examples of a given sampled texture, the synthesized textures being of any desired duration. The algorithm is based on a montage approach to synthesis in that the original sample is cut into small pieces, referred to as atoms, and these atoms are concatenated together in a new sequence, preserving certain structures of the original texture. The sequence modelling of the atoms has two levels: atoms are concatenated to create segments and segments are concatenated, based on their history, to create textures. This approach deals with problems of repetition associated with sampling based sound texture synthesis techniques. Listening tests show that the results of the synthesis are very promising for a broad range of textures, including quasi-periodic and more random textures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1094–1105},
numpages = {12},
keywords = {synthesis, concatenative, audio texture, sound texture, granular}
}

@article{10.1109/TASLP.2016.2537203,
author = {Xu, Haotian and Ou, Zhijian},
title = {Scalable Discovery of Audio Fingerprint Motifs in Broadcast Streams with Determinantal Point Process Based Motif Clustering},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2537203},
doi = {10.1109/TASLP.2016.2537203},
abstract = {In this paper, we study the scalable discovery of audio repetitive patterns/motifs in long broadcast streams, where two segments are said to be repetitive if their audio fingerprints are close to each other. In this task, as we are confined to handle limited variability, we can adapt an audio hashing technique, originally proposed for searching a given music clip in music tracks, to successfully devise a linear complexity similarity matching method with a new step of repeated interval formation. This is the first contribution of this paper. As the similarity matching is super fast and thus coarse, there are false alarms in the large number of pair-wise matches generated, which constitute a major source of noise. We propose applying subset selection to the original set of pair-wise matches based on determinantal point processes (DPPs), as a filtering step, to reduce the noise. The selected subset of pair-wise matches is then subjected to motif clustering. We successfully apply DPP-based subset selection to improve motif clustering, which has a nice property that favors both quality and diversity. This is the second contribution of this paper. The proposed method is thoroughly evaluated on a 9-hour real-world audio stream and is compared with several reference methods. The bootstrap technique is used for the significance test. It is shown that the similarity matching is computationally very efficient (above 100 times faster than real time), and the filtering step with DPPs can significantly improve the precision of motif discovery, without sacrificing the recall performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {978–989},
numpages = {12},
keywords = {audio fingerprinting, motif clustering, determinantal point process, audio motif discovery}
}

@article{10.1109/TASLP.2016.2520364,
author = {Receveur, Simon and Wei\ss{}, Robin and Fingscheidt, Tim},
title = {Turbo Automatic Speech Recognition},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2520364},
doi = {10.1109/TASLP.2016.2520364},
abstract = {Performance of automatic speech recognition (ASR) systems can significantly be improved by integrating further sources of information such as additional modalities, or acoustic channels, or acoustic models. Given the arising problem of information fusion, striking parallels to problems in digital communications are exhibited, where the discovery of the turbo codes by Berrou et al. was a groundbreaking innovation. In this paper, we show ways how to successfully apply the turbo principle to the domain of ASR and thereby provide solutions to the above-mentioned information fusion problem. The contribution of our work is fourfold: First, we review the turbo decoding forward-backward algorithm (FBA), giving detailed insights into turbo ASR, and providing a new interpretation and formulation of the so-called extrinsic information being passed between the recognizers. Second, we present a real-time capable turbo-decoding Viterbi algorithm suitable for practical information fusion and recognition tasks. Then we present simulation results for a multimodal example of information fusion. Finally, we prove the suitability of both our turbo FBA and turbo Viterbi algorithm also for a single-channel multimodel recognition task obtained by using two acoustic feature extraction methods. On a small vocabulary task (challenging, since spelling is included), our proposed turbo ASR approach outperforms even the best reference system on average over all SNR conditions and investigated noise types by a relative word error rate (WER) reduction of 22.4% (audio-visual task) and 18.2% (audio-only task), respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {846–862},
numpages = {17},
keywords = {multimedia systems, speech recognition, hidden Markov models, robustness, iterative decoding}
}

@article{10.1109/TASLP.2016.2531285,
author = {Zhang, Geliang and Godsill, Simon},
title = {Fundamental Frequency Estimation in Speech Signals with Variable Rate Particle Filters},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2531285},
doi = {10.1109/TASLP.2016.2531285},
abstract = {Fundamental frequency estimation, known as pitch estimation in speech signals is of interest both to the research community and to industry. Meanwhile, the particle filter is known to be a powerful Bayesian inference method to track dynamic parameters in nonlinear state-space models. In this paper, we propose a speech model under a time-varying source-filter speech model, and use variable rate particle filters (VRPF) to develop methods for estimation of pitch periods in speech signals. A Rao--Blackwellised variable rate particle filter (RBVRPF) is also implemented. The proposed VRPF and RBVRPF are compared with a state-of-the-art pitch estimation algorithm, the YIN algorithm. Simulation results show that more accurate estimation of pitch can be obtained by VRPF and RBVRPF even under strong background noise conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {890–900},
numpages = {11},
keywords = {rao--blackwellisation, pitch estimation, variable rate particle filters, source-filter model}
}

@article{10.1109/TASLP.2016.2530409,
author = {Marxer, Ricard and Purwins, Hendrik},
title = {Unsupervised Incremental Online Learning and Prediction of Musical Audio Signals},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2530409},
doi = {10.1109/TASLP.2016.2530409},
abstract = {Guided by the idea that musical human-computer interaction may become more effective, intuitive, and creative when basing its computer part on cognitively more plausible learning principles, we employ unsupervised incremental online learning (i.e. clustering) to build a system that predicts the next event in a musical sequence, given as audio input. The flow of the system is as follows: 1) segmentation by onset detection, 2) timbre representation of each segment by Mel frequency cepstrum coefficients, 3) discretization by incremental clustering, yielding a tree of different sound classes (e.g. timbre categories/instruments) that can grow or shrink on the fly driven by the instantaneous sound events, resulting in a discrete symbol sequence, 4) extraction of statistical regularities of the symbol sequence, using hierarchical N-grams and the newly introduced conceptual Boltzmann machine that adapt to the dynamically changing clustering tree in 3), and 5) prediction of the next sound event in the sequence, given the last n previous events. The system's robustness is assessed with respect to complexity and noisiness of the signal. Clustering in isolation yields an adjusted Rand index (ARI) of 82.7%/85.7% for data sets of singing voice and drums. Onset detection jointly with clustering achieve an ARI of 81.3%/76.3% and the prediction of the entire system yields an ARI of 27.2%/39.2%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {863–874},
numpages = {12},
keywords = {adaptive algorithms, prediction algorithms, music information retrieval, unsupervised learning}
}

@article{10.1109/TASLP.2016.2536478,
author = {Zhang, Xiao-Lei and Wang, DeLiang},
title = {A Deep Ensemble Learning Method for Monaural Speech Separation},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2536478},
doi = {10.1109/TASLP.2016.2536478},
abstract = {Monaural speech separation is a fundamental problem in robust speech processing. Recently, deep neural network (DNN)-based speech separation methods, which predict either clean speech or an ideal time-frequency mask, have demonstrated remarkable performance improvement. However, a single DNN with a given window length does not leverage contextual information sufficiently, and the differences between the two optimization objectives are not well understood. In this paper, we propose a deep ensemble method, named multicontext networks, to address monaural speech separation. The first multicontext network averages the outputs of multiple DNNs whose inputs employ different window lengths. The second multicontext network is a stack of multiple DNNs. Each DNN in a module of the stack takes the concatenation of original acoustic features and expansion of the soft output of the lower module as its input, and predicts the ratio mask of the target speaker; the DNNs in the same module employ different contexts. We have conducted extensive experiments with three speech corpora. The results demonstrate the effectiveness of the proposed method. We have also compared the two optimization objectives systematically and found that predicting the ideal time-frequency mask is more efficient in utilizing clean training speech, while predicting clean speech is less sensitive to SNR variations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {967–977},
numpages = {11},
keywords = {monaural speech separation, ensemble learning, multicontext networks, mapping-based separation, masking-based separation, deep neural networks}
}

@article{10.1109/TASLP.2016.2531284,
author = {Kroher, Nadine and G\'{o}mez, Emilia},
title = {Automatic Transcription of Flamenco Singing from Polyphonic Music Recordings},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2531284},
doi = {10.1109/TASLP.2016.2531284},
abstract = {Automatic note-level transcription is considered one of the most challenging tasks in music information retrieval. The specific case of flamenco singing transcription poses a particular challenge due to its complex melodic progressions, intonation inaccuracies, the use of a high degree of ornamentation, and the presence of guitar accompaniment. In this study, we explore the limitations of existing state of the art transcription systems for the case of flamenco singing and propose a specific solution for this genre: We first extract the predominant melody and apply a novel contour filtering process to eliminate segments of the pitch contour which originate from the guitar accompaniment. We formulate a set of onset detection functions based on volume and pitch characteristics to segment the resulting vocal pitch contour into discrete note events. A quantised pitch label is assigned to each note event by combining global pitch class probabilities with local pitch contour statistics. The proposed system outperforms state of the art singing transcription systems with respect to voicing accuracy, onset detection, and overall performance when evaluated on flamenco singing datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {901–913},
numpages = {13},
keywords = {automatic music transcription, singing voice, music information retrieval, pitch contour, audio content description}
}

@article{10.1109/TASLP.2016.2533858,
author = {Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
title = {An End-to-End Neural Network for Polyphonic Piano Music Transcription},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2533858},
doi = {10.1109/TASLP.2016.2533858},
abstract = {We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network-based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yield the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {927–939},
numpages = {13},
keywords = {recurrent neural networks, deep learning, automatic music transcription, music language models}
}

@article{10.1109/TASLP.2016.2533867,
author = {Krawczyk-Becker, Martin and Gerkmann, Timo},
title = {Fundamental Frequency Informed Speech Enhancement in a Flexible Statistical Framework},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2533867},
doi = {10.1109/TASLP.2016.2533867},
abstract = {Conventional statistical clean speech estimators, like the Wiener filter, are frequently used for the spectro-temporal enhancement of noise corrupted speech. Most of these approaches estimate the clean speech independently for each time-frequency point, neglecting the structure of the underlying speech sound. In this work, we derive a statistical estimator that explicitly takes into account information about the characteristic structure of voiced speech by means of a harmonic signal model. To this end, we also present a way to estimate a harmonic model-based clean speech representation and the corresponding error variance directly in the short-time Fourier transform domain. The resulting estimator is optimal in the minimum-mean-squared error sense and can conveniently be formulated in terms of a multichannel Wiener filter. The proposed estimator outperforms several reference algorithms in terms of speech quality and intelligibility as predicted by instrumental measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {940–951},
numpages = {12},
keywords = {speech enhancement, signal reconstruction, noise reduction}
}

@article{10.1109/TASLP.2016.2535199,
author = {Szurley, Joseph and Bertrand, Alexander and Van Dijk, Bas and Moonen, Marc},
title = {Binaural Noise Cue Preservation in a Binaural Noise Reduction System with a Remote Microphone Signal},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2535199},
doi = {10.1109/TASLP.2016.2535199},
abstract = {A general binaural noise reduction system is considered that employs the multichannel Wiener filter with partial noise estimation (MWFη) allowing for an explicit tradeoff between noise reduction and binaural noise cue preservation. In this paper, it is assumed that along with the general binaural system, a remote microphone signal with a high input signal-to-noise ratio (SNR) is available for inclusion in the MWFη. The use of this remote microphone signal with a high input SNR allows for a simultaneous increase in both noise reduction performance and preservation of the binaural noise cues. To further increase the performance, a modification to the partial noise estimation (PNE) variable, η, is proposed which relies on exploiting the aforementioned trade-off by either constraining the output SNR or binaural noise cues to the same level before and after the addition of the remote microphone signal. The validity of the theoretical results are supplemented via simulations using a binaural setup with a single speech and noise source.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {952–966},
numpages = {15},
keywords = {binaural hearing aids, noise reduction, multi-channel Wiener filtering, binaural cues}
}

@article{10.1109/TASLP.2016.2531902,
author = {Winter, Fiete and Ahrens, Jens and Spors, Sascha},
title = {On Analytic Methods for 2.5-d Local Sound Field Synthesis Using Circular Distributions of Secondary Sources},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2531902},
doi = {10.1109/TASLP.2016.2531902},
abstract = {Local sound field synthesis allows for synthesizing a given desired sound field inside a limited target region such that the field is free of considerable spatial aliasing artifacts. Spatial aliasing artifacts are a consequence of overlaps due to unavoidable repetitions of the space-spectral coefficients of the secondary source driving function. We analyze various conceivable analytic ways of restricting the bandwidth of the spatial spectrum of the driving function such that considerable overlapping is prevented: local spatial bandlimitation (A), spectral windowing (B), and local spatial bandlimitation plus spectral windowing (C). While solution B is computationally significantly more efficient than A and C, it provides only limited control over the spatial location around which the aliasing-free region evolves. Solutions A and C provide more flexibility and higher accuracy whereby both achieve largely identical results so that the spectral windowing after the local spatial bandlimitation may be skipped. We present a detailed analysis of the properties of the spatial aliasing artifacts arising in the synthesis of a virtual plane wave. We establish a procedure for predicting the maximum possible size of the aliasing-free target region depending on its location and on the propagation direction of the desired sound field. The results can help reducing regularization in numerical solutions as they represent physical limitations that can be considered in the choice of parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {914–926},
numpages = {13},
keywords = {sound field synthesis, ambisonics, spatial aliasing, spherical harmonics}
}

@article{10.1109/TASLP.2016.2530405,
author = {Adeli, Mohammad and Rouat, Jean and Wood, Sean and Molotchnikoff, St\'{e}phane and Plourde, Eric},
title = {A Flexible Bio-Inspired Hierarchical Model for Analyzing Musical Timbre},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2530405},
doi = {10.1109/TASLP.2016.2530405},
abstract = {A flexible and multipurpose bio-inspired hierarchical model for analyzing musical timbre is presented in this paper. Inspired by findings in the fields of neuroscience, computational neuroscience, and psychoacoustics, not only does the model extract spectral and temporal characteristics of a signal, but it also analyzes amplitude modulations on different timescales. It uses a cochlear filter bank to resolve the spectral components of a sound, lateral inhibition to enhance spectral resolution, and a modulation filter bank to extract the global temporal envelope and roughness of the sound from amplitude modulations. The model was evaluated in three applications. First, it was used to simulate subjective data from two roughness experiments. Second, it was used for musical instrument classification using the k-NN algorithm and a Bayesian network. Third, it was applied to find the features that characterize sounds whose timbres were labeled in an audiovisual experiment. The successful application of the proposed model in these diverse tasks revealed its potential in capturing timbral information.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {875–889},
numpages = {15},
keywords = {instantaneous roughness, timbre, cochlear filter bank, temporal envelope, time-averaged spectrum, modulation filter bank, musical instrument classification, bayesian network, multimodal timbre characterization}
}

@article{10.1109/TASLP.2016.2526787,
author = {Tsai, T. J. and Stolcke, Andreas},
title = {Robust and Efficient Multiple Alignment of Unsynchronized Meeting Recordings},
year = {2016},
issue_date = {May 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2526787},
doi = {10.1109/TASLP.2016.2526787},
abstract = {This paper proposes a way to generate a single high-quality audio recording of a meeting using no equipment other than participants' personal devices. Each participant in the meeting uses their mobile device as a local recording node, and they begin recording whenever they arrive in an unsynchronized fashion. The main problem in generating a single summary recording is to temporally align the various audio recordings in a robust and efficient manner. We propose a way to do this using an adaptive audio fingerprint based on spectrotemporal eigenfilters, where the fingerprint design is learned on-the-fly in a totally unsupervised way to perform well on the data at hand. The adaptive fingerprints require only a few seconds of data to learn a robust design, and they require no tuning. Our method uses an iterative, greedy two-stage alignment algorithm which finds a rough alignment using indexing techniques, and then performs a more fine-grained alignment based on Hamming distance. Our proposed system achieves 99% alignment accuracy on challenging alignment scenarios extracted from the ICSI meeting corpus, and it outperforms five other well-known and state-of-the-art fingerprint designs. We conduct extensive analyses of the factors that affect the robustness of the adaptive fingerprints, and we provide a simple heuristic that can be used to adjust the fingerprint's robustness according to the amount of computation we are willing to perform.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {833–845},
numpages = {13},
keywords = {meetings, alignment, eigenfilter, adaptive, audio fingerprint}
}

@article{10.1109/TASLP.2016.2520371,
author = {Palangi, Hamid and Deng, Li and Shen, Yelong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Song, Xinying and Ward, Rabab},
title = {Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2520371},
doi = {10.1109/TASLP.2016.2520371},
abstract = {This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks (RNN) with Long Short-Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detect the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms Paragraph Vector method for web document retrieval task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {694–707},
numpages = {14},
keywords = {sentence embedding, deep learning, long short-term memory}
}

@article{10.1109/TASLP.2015.2499602,
author = {Liu, Jianming and Grant, Steven L.},
title = {Proportionate Adaptive Filtering for Block-Sparse System Identification},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2499602},
doi = {10.1109/TASLP.2015.2499602},
abstract = {In this paper, a new family of proportionate normalized least mean square (PNLMS) adaptive algorithms that improve the performance of identifying block-sparse systems is proposed. The main proposed algorithm, called block-sparse PNLMS (BS-PNLMS), is based on the optimization of a mixed l2,1 norm of the adaptive filter's coefficients. It is demonstrated that both the NLMS and the traditional PNLMS are special cases of BS-PNLMS. Meanwhile, a block-sparse improved PNLMS (BS-IPNLMS) is also derived for both sparse and dispersive impulse responses. Simulation results demonstrate that the proposed BS-PNLMS and BS-IPNLMS algorithms outperformed the NLMS, PNLMS and IPNLMS algorithms with only a modest increase in computational complexity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {623–630},
numpages = {8},
keywords = {proportionate adaptive algorithm, sparse system identification, block-sparse}
}

@article{10.1109/TASLP.2016.2531286,
author = {Su, Ming-Hsiang and Wu, Chung-Hsien and Zheng, Yu-Ting},
title = {Exploiting Turn-Taking Temporal Evolution for Personality Trait Perception in Dyadic Conversations},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2531286},
doi = {10.1109/TASLP.2016.2531286},
abstract = {In dyadic conversations, turn-taking is a dynamically evolving behavior strongly linked to paralinguistic communication. Turn-taking temporal evolution in a dyadic conversation is inevitable and can be incorporated into a modeling framework for characterizing and recognizing the personality traits (PTs) of two speakers. This study presents an approach to automatically predicting PTs in a dyadic conversation. First, a recurrent neural network (RNN) was used to model the relationship between Big Five Inventory 10 (BFI-10) items and linguistic features of spoken text in each turn of a speaker (speaker turn) to output a BFI-10 profile. The RNN applies a recurrent property to characterize the short-term temporal evolution of a dialog. Second, the coupled hidden Markov model (C-HMM) was employed to model the long-term turn-taking temporal evolution and cross-speaker contextual information for detecting the PTs of two individuals for the entire dialog represented by the BFI-10 profile sequence. The Mandarin Conversational Dialogue Corpus was used for evaluation. The evaluation result shows that an average perception accuracy of 79.66% for the big five traits was achieved using five-fold cross validation. Compared with conventional HMM and support vector machine-based methods, the proposed approach achieved a more favorable performance according to a statistical significance test. The encouraging results confirm the usability of this system for future applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {733–744},
numpages = {12},
keywords = {big-five personality trait, coupled hidden markov model, personality trait perception, dyadic conversation}
}

@article{10.1109/TASLP.2016.2514492,
author = {N\o{}rholm, Sidsel Marie and Jensen, Jesper Rindom and Christensen, Mads Gr\ae{}sb\o{}ll},
title = {Enhancement and Noise Statistics Estimation for Non-Stationary Voiced Speech},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2514492},
doi = {10.1109/TASLP.2016.2514492},
abstract = {In this paper, single channel speech enhancement in the time domain is considered. We address the problem of modelling non-stationary speech by describing the voiced speech parts by a harmonic linear chirp model instead of using the traditional harmonic model. This means that the speech signal is not assumed stationary, instead the fundamental frequency can vary linearly within each frame. The linearly constrained minimum variance (LCMV) filter and the amplitude and phase estimation (APES) filter are derived in this framework and compared to the harmonic versions of the same filters. It is shown through simulations on synthetic and speech signals, that the chirp versions of the filters perform better than their harmonic counterparts in terms of output signal-to-noise ratio (SNR) and signal reduction factor. For synthetic signals, the output SNR for the harmonic chirp APES based filter is increased 3 dB compared to the harmonic APES based filter at an input SNR of 10 dB, and at the same time the signal reduction factor is decreased. For speech signals, the increase is 1.5 dB along with a decrease in the signal reduction factor of 0.7. As an implicit part of the APES filter, a noise covariance matrix estimate is obtained. We suggest using this estimate in combination with other filters such as the Wiener filter. The performance of the Wiener filter and LCMV filter are compared using the APES noise covariance matrix estimate and a power spectral density (PSD) based noise covariance matrix estimate. It is shown that the APES covariance matrix works well in combination with the Wiener filter, and the PSD based covariance matrix works well in combination with the LCMV filter.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {645–658},
numpages = {14},
keywords = {non-stationary speech, speech enhancement, harmonic signal model, chirp model}
}

@article{10.1109/TASLP.2015.2505416,
author = {Jensen, Jesper Rindom and Benesty, Jacob and Christensen, Mads Gr\ae{}sb\o{}ll},
title = {Noise Reduction with Optimal Variable Span Linear Filters},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2505416},
doi = {10.1109/TASLP.2015.2505416},
abstract = {In this paper, the problem of noise reduction is addressed as a linear filtering problem in a novel way by using concepts from subspace-based enhancement methods, resulting in variable span linear filters. This is done by forming the filter coefficients as linear combinations of a number of eigenvectors stemming from a joint diagonalization of the covariance matrices of the signal of interest and the noise. The resulting filters are flexible in that it is possible to trade off distortion of the desired signal for improved noise reduction. This tradeoff is controlled by the number of eigenvectors included in forming the filter. Using these concepts, a number of different filter designs are considered, like minimum distortion, Wiener, maximum SNR, and tradeoff filters. Interestingly, all these can be expressed as special cases of variable span filters. We also derive expressions for the speech distortion and noise reduction of the various filter designs. Moreover, we consider an alternative approach, wherein the filter is designed for extracting an estimate of the noise signal, which can then be extracted from the observed signals, which is referred to as the indirect approach. Simulations demonstrate the advantages and properties of the variable span filter designs, and their potential performance gain compared to widely used speech enhancement methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {631–644},
numpages = {14},
keywords = {joint diagonalization, noise reduction, speech enhancement, optimal filters, subspace, span}
}

@article{10.1109/TASLP.2016.2530401,
author = {Phan, Huy and Hertel, Lars and Maass, Marco and Mazur, Radoslaw and Mertins, Alfred},
title = {Learning Representations for Nonspeech Audio Events through Their Similarities to Speech Patterns},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2530401},
doi = {10.1109/TASLP.2016.2530401},
abstract = {The human auditory system is very well matched to both human speech and environmental sounds. Therefore, the question arises whether human speech material may provide useful information for training systems for analyzing nonspeech audio signals, e.g., in a classification task. In order to answer this question, we consider speech patterns as basic acoustic concepts, which embody and represent the target nonspeech signal. To find out how similar the nonspeech signal is to speech, we classify it with a classifier trained on the speech patterns and use the classification posteriors to represent the closeness to the speech bases. The speech similarities are finally employed as a descriptor to represent the target signal. We further show that a better descriptor can be obtained by learning to organize the speech categories hierarchically with a tree structure. Furthermore, these descriptors are generic. That is, once the speech classifier has been learned, it can be employed as a feature extractor for different datasets without retraining. Lastly, we propose an algorithm to select a sufficient subset, which provides an approximate representation capability of the entire set of available speech patterns. We conduct experiments for the application of audio event analysis. Phone triplets from the TIMIT dataset were used as speech patterns to learn the descriptors for audio events of three different datasets with different complexity, including UPC-TALP, Freiburg-106, and NAR. The experimental results on the event classification task show that a good performance can be easily obtained even if a simple linear classifier is used. Furthermore, fusion of the learned descriptors as an additional source leads to state-of-the-art performance on all the three target datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {807–822},
numpages = {16},
keywords = {phone triplets, feature learning, speech patterns, nonspeech audio event, Representation}
}

@article{10.1109/TASLP.2016.2528171,
author = {Wang, Zhong-Qiu and Wang, DeLiang},
title = {A Joint Training Framework for Robust Automatic Speech Recognition},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2528171},
doi = {10.1109/TASLP.2016.2528171},
abstract = {Robustness against noise and reverberation is critical for ASR systems deployed in real-world environments. In robust ASR, corrupted speech is normally enhanced using speech separation or enhancement algorithms before recognition. This paper presents a novel joint training framework for speech separation and recognition. The key idea is to concatenate a deep neural network (DNN) based speech separation frontend and a DNN-based acoustic model to build a larger neural network, and jointly adjust the weights in each module. This way, the separation fron-tend is able to provide enhanced speech desired by the acoustic model and the acoustic model can guide the separation frontend to produce more discriminative enhancement. In addition, we apply sequence training to the jointly trained DNN so that the linguistic information contained in the acoustic and language models can be back-propagated to influence the separation frontend at the training stage. To further improve the robustness, we add more noise- and reverberation-robust features for acoustic modeling. At the test stage, utterance-level unsupervised adaptation is performed to adapt the jointly trained network by learning a linear transformation of the input of the separation frontend. The resulting sequence-discriminative jointly-trained multistream system with run-time adaptation achieves 10.63% average word error rate (WER) on the test set of the reverberant and noisy CHiME-2 dataset (task-2), which represents the best performance on this dataset and a 22.75% error reduction over the best existing method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {796–806},
numpages = {11},
keywords = {deep neural networks (dnn), robust automatic speech recognition, speech separation, joint training, time-frequency masking, chime-2}
}

@article{10.1109/TASLP.2015.2497148,
author = {Li, Peifeng and Zhou, Guodong},
title = {Joint Argument Inference in Chinese Event Extraction with Argument Consistency and Event Relevance},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2497148},
doi = {10.1109/TASLP.2015.2497148},
abstract = {Event extraction in Chinese suffers greatly from the frequent missing of arguments. Statistical analysis of the automatic content extraction (ACE) 2005 Chinese corpus shows that nearly 55% of arguments do not occur in their corresponding event mentions. This problem greatly hinders the wide deployment of event extraction in real applications. This paper proposes a novel joint argument inference model to recover those missing arguments in event mentions from a semantic perspective. In particular, this model employs various types of argument consistencies to reveal intra-event argument semantics in multiple dimensions, such as argument-argument, argument-role and argument-trigger. Moreover, this model explores several linguistic-driven event relevance phenomena, such as Coreference, Sequence, and Parallel, to unveil inter-event argument semantics in various layers, such as sentence, discourse, and document. An evaluation of the ACE 2005 Chinese corpus justifies the effectiveness of our joint argument inference model compared to a state-of-the-art baseline.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {612–622},
numpages = {11},
keywords = {argument inference, argument consistency, chinese event extraction, joint modeling, event relevance}
}

@article{10.1109/TASLP.2016.2522655,
author = {Takamichi, Shinnosuke and Toda, Tomoki and Black, Alan W. and Neubig, Graham and Sakti, Sakriani and Nakamura, Satoshi},
title = {Postfilters to Modify the Modulation Spectrum for Statistical Parametric Speech Synthesis},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2522655},
doi = {10.1109/TASLP.2016.2522655},
abstract = {This paper presents novel approaches based on modulation spectrum (MS) for high-quality statistical parametric speech synthesis, including text-to-speech (TTS) and voice conversion (VC). Although statistical parametric speech synthesis offers various advantages over concatenative speech synthesis, the synthetic speech quality is still not as good as that of con-catenative speech synthesis or the quality of natural speech. One of the biggest issues causing the quality degradation is the over-smoothing effect often observed in the generated speech parameter trajectories. Global variance (GV) is known as a feature well correlated with the over-smoothing effect, and the effectiveness of keeping the GV of the generated speech parameter trajectories similar to those of natural speech has been confirmed. However, the quality gap between natural speech and synthetic speech is still large. In this paper, we propose using the MS of the generated speech parameter trajectories as a new feature to effectively quantify the over-smoothing effect. Moreover, we propose post-filters to modify the MS utterance by utterance or segment by segment to make the MS of synthetic speech close to that of natural speech. The proposed postfilters are applicable to various synthesizers based on statistical parametric speech synthesis. We first perform an evaluation of the proposed method in the framework of hidden Markov model (HMM)-based TTS, examining its properties from different perspectives. Furthermore, effectiveness of the proposed postfilters are also evaluated in Gaussian mixture model (GMM)-based VC and classification and regression trees (CART)-based TTS (a.k.a., CLUSTERGEN). The experimental results demonstrate that 1) the proposed utterance-level postfilter achieves quality comparable to the conventional generation algorithm considering the GV, and yields significant improvements by applying to the GV-based generation algorithm in HMM-based TTS, 2) the proposed segment-level postfilter capable of achieving low-delay synthesis also yields significant improvements in synthetic speech quality, and 3) the proposed postfilters are also effective in not only HMM-based TTS but also GMM-based VC and CLUSTERGEN.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {755–767},
numpages = {13},
keywords = {CLUSTERGEN, statistical parametric speech synthesis, GMM-based voice conversion, HMM-based text-to-speech, global variance, modulation spectrum, over-smoothing, post-filter}
}

@article{10.1109/TASLP.2016.2522649,
author = {Jeffet, Michael and Shabtai, Noam R. and Rafaely, Boaz},
title = {Theory and Perceptual Evaluation of the Binaural Reproduction and Beamforming Tradeoff in the Generalized Spherical Array Beamformer},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2522649},
doi = {10.1109/TASLP.2016.2522649},
abstract = {Microphone arrays are widely used in speech enhancement systems for noisy and reverberant environments. Recently, a generalized spherical array beamforming approach was developed incorporating binaural sound reproduction in the beamforming process. This generalized spherical array beam-former (GSB) maintains the spatial information through the bin-aural cues and improves both the spatial realism and the speech intelligibility. In this paper, the theory of the tradeoff that arises when incorporating both beamforming and binaural reproduction in a single array is developed and investigated through a simulation study and a listening test. By representing the GSB formulation in matrix form for investigating the single plane-wave scenario, two measures are developed in order to evaluate the performance of the GSB in terms of both binaural reproduction and spatial selectivity. These measures are then employed in the evaluation of the performance of various GSB beam-patterns using simulations. A listening test experiment that validates the simulation results is then reported. Results validate the theory, i.e., the GSB can be used to integrate successfully binaural reproduction and beamforming, allowing the user to emphasize either of the two, but with a clear tradeoff; improving one is only possible at the expense of degrading the other.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {708–718},
numpages = {11},
keywords = {binaural sound reproduction, spherical microphone arrays, speech intelligibility, beamforming, spherical harmonics}
}

@article{10.1109/TASLP.2016.2526779,
author = {Andersen, Kristian Timm and Moonen, Marc},
title = {Adaptive Time-Frequency Analysis for Noise Reduction in an Audio Filter Bank with Low Delay},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2526779},
doi = {10.1109/TASLP.2016.2526779},
abstract = {In this paper, an adaptive time-frequency analysis scheme is proposed along with a synthesis scheme using an asymmetric window. The proposed scheme is suitable for audio noise reduction with a low delay in the range of 0 to 4 ms. The main novelty of the paper is the adaptive analysis scheme that can adapt to the incoming signal independently in both time and frequency by employing a complex filter on a DFT modulated filter bank. A number of adaptive time-frequency schemes are described that are suitable for low delay and low computational complexity. The adaptive time-frequency scheme is used for the computation of noise reduction gain factors, which are then adopted in a nonadaptive analysis/synthesis scheme. The synthesis scheme uses an asymmetric window to achieve a good tradeoff between low delay and a sharp frequency response. Examples are given of the adaptive analysis and measurements of the synthesis scheme are given to show that the filter bank has a gain dependent nonlinear phase response. Finally, a noise reduction task is performed that shows good performance compared to reference implementations in terms of segmental SNR and PESQ.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {784–795},
numpages = {12},
keywords = {adaptive time-frequency analysis, speech enhancement, noise reduction, low delay}
}

@article{10.1109/TASLP.2016.2521486,
author = {Parada, Pablo Peso and Sharma, Dushyant and Lainez, Jose and Barreda, Daniel and van Waterschoot, Toon and Naylor, Patrick A.},
title = {A Single-Channel Non-Intrusive C50 Estimator Correlated with Speech Recognition Performance},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2521486},
doi = {10.1109/TASLP.2016.2521486},
abstract = {Several intrusive measures of reverberation can be computed from measured and simulated room impulse responses, over the full frequency band or for each individual mel-frequency subband. It is initially shown that full-band clarity index C50 is the most correlated measure on average with reverberant speech recognition performance. This corroborates previous findings but now for the dataset to be used in this study. We extend the previous findings to show that C50 also exhibits the highest mutual information on average. Motivated by these extended findings, a nonintrusive room acoustic (NIRA) estimation method is proposed to estimate C50 from only the reverberant speech signal. The NIRA method is a data-driven approach based on computing a number of features from the speech signal and it employs these features to train a model used to perform the estimation. The choice of features and learning techniques are explored in this work using an evaluation set which comprises approximately 100 000 different reverberant signals (around 93 h of speech) including reverberation from measured and simulated room impulse responses. The feature importance of each feature with respect to the estimation of the target C50 is analysed following two different approaches. In both cases, the newly chosen set of features shows high importance for the target. The best C50 estimator provides a root-mean-square deviation around 3 dB on average for all reverberant test environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {719–732},
numpages = {14},
keywords = {room acoustic parameter estimation, reverberation, reverberant speech recognition}
}

@article{10.1109/TASLP.2016.2518804,
author = {Kodrasi, Ina and Doclo, Simon},
title = {Joint Dereverberation and Noise Reduction Based on Acoustic Multi-Channel Equalization},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2518804},
doi = {10.1109/TASLP.2016.2518804},
abstract = {Regularized acoustic multi-channel equalization techniques, such as regularized partial multi-channel equalization based on the multiple-input/output inverse theorem (RPMINT), are able to achieve a high dereverberation performance in the presence of room impulse response perturbations but may lead to amplification of the additive noise. In this paper, two time-domain techniques aiming at joint dereverberation and noise reduction based on acoustic multi-channel equalization are proposed. The first technique, namely RPMINT for joint derever-beration and noise reduction (RPM-DNR), extends RPMINT by explicitly taking the noise statistics into account. In addition to the regularization parameter used in RPMINT, the RPM-DNR technique introduces an additional weighting parameter, enabling a trade-off between dereverberation and noise reduction. The second technique, namely multi-channel Wiener filter for joint dereverberation and noise reduction (MWF-DNR), takes both the speech and the noise statistics into account and uses the RPMINT filter to compute a dereverberated reference signal for the multichannel Wiener filter. The MWF-DNR technique also introduces an additional weighting parameter, which now provides a trade-off between speech distortion and noise reduction. To automatically select the regularization and weighting parameters, for the RPM-DNR technique a novel procedure based on the L-hypersurface is proposed, whereas for the MWF-DNR technique two decoupled optimization procedures based on the L-curve are used. Extensive simulations demonstrate using instrumental measures that the RPM-DNR technique maintains the dereverberation performance of the RPMINT technique while improving its noise reduction performance. Furthermore, it is shown that the MWF-DNR technique yields a significantly better noise reduction performance than the RPM-DNR technique at the expense of a worse dereverberation performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {680–693},
numpages = {14},
keywords = {acoustic multi-channel equalization, noise reduction, automatic parameter selection, dereverberation, l-hypersurface}
}

@article{10.1109/TASLP.2016.2517318,
author = {Abdul-Rauf, Sadaf and Schwenk, Holger and Lambert, Patrik and Nawaz, Mohammad},
title = {Empirical Use of Information Retrieval to Build Synthetic Data for SMT Domain Adaptation},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2517318},
doi = {10.1109/TASLP.2016.2517318},
abstract = {In this paper, we present information retrieval as a powerful tool for addressing an imperative problem in the field of statistical machine translation, i.e., improving translation quality when not enough parallel corpora are available. We devise a framework, which uses information retrieval to create a synthetic corpus from the easily available monolingual corpora. We propose an improved unsupervised training approach with a data selection mechanism, which selects only the most appropriate sentences, thus reducing the amount of data, which is less related to the domain in the additional bitext. We also introduce a new method to choose sentences based on their relative similarity/difference from the query sentence. Using the synthetic corpus created by our method, we are able to improve state-of-the-art statistical machine translation systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {745–754},
numpages = {10},
keywords = {monolingual corpus, information retrieval, domain adaptation, statistical machine translation}
}

@article{10.1109/TASLP.2016.2526653,
author = {Wu, Zhizheng and De Leon, Phillip L. and Demiroglu, Cenk and Khodabakhsh, Ali and King, Simon and Ling, Zhen-Hua and Saito, Daisuke and Stewart, Bryan and Toda, Tomoki and Wester, Mirjam and Yamagishi, Junichi},
title = {Anti-Spoofing for Text-Independent Speaker Verification: An Initial Database, Comparison of Countermeasures, and Human Performance},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2526653},
doi = {10.1109/TASLP.2016.2526653},
abstract = {In this paper, we present a systematic study of the vulnerability of automatic speaker verification to a diverse range of spoofing attacks. We start with a thorough analysis of the spoofing effects of five speech synthesis and eight voice conversion systems, and the vulnerability of three speaker verification systems under those attacks. We then introduce a number of countermeasures to prevent spoofing attacks from both known and unknown attackers. Known attackers are spoofing systems whose output was used to train the countermeasures, while an unknown attacker is a spoofing system whose output was not available to the countermeasures during training. Finally, we benchmark automatic systems against human performance on both speaker verification and spoofing detection tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {768–783},
numpages = {16},
keywords = {speech synthesis, voice conversion, speaker verification, anti-spoofing, countermeasure, spoofing attack, security}
}

@article{10.1109/TASLP.2016.2517567,
author = {Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
title = {Unsupervised Word Segmentation and Lexicon Discovery Using Acoustic Word Embeddings},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2517567},
doi = {10.1109/TASLP.2016.2517567},
abstract = {In settings where only unlabeled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modeling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsu-pervised Bayesian model that segments unlabeled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {669–679},
numpages = {11},
keywords = {unsupervised speech processing, word acquisition, word discovery, speech segmentation, unsupervised learning}
}

@article{10.1109/TASLP.2016.2516647,
author = {Mehta, Daryush D. and Van Stan, Jarrad H. and Hillman, Robert E.},
title = {Relationships between Vocal Function Measures Derived from an Acoustic Microphone and a Subglottal Neck-Surface Accelerometer},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2516647},
doi = {10.1109/TASLP.2016.2516647},
abstract = {Monitoring subglottal neck-surface acceleration has received renewed attention due to the ability of low-profile accelerometers to confidentially and noninvasively track properties related to normal and disordered voice characteristics and behavior. This study investigated the ability of subglottal neck-surface acceleration to yield vocal function measures traditionally derived from the acoustic voice signal and help guide the development of clinically functional accelerometer-based measures from a physiological perspective. Results are reported for 82 adult speakers with voice disorders and 52 adult speakers with normal voices who produced the sustained vowels /a/, /i/, and /u/ at a comfortable pitch and loudness during the simultaneous recording of radiated acoustic pressure and subglottal neck-surface acceleration. As expected, timing-related measures of jitter exhibited the strongest correlation between acoustic and neck-surface acceleration waveforms (r ≤ 0.99), whereas amplitude-based measures of shimmer correlated less strongly (r ≤ 0.74). Additionally, weaker correlations were exhibited by spectral measures of harmonics-to-noise ratio (r ≤ 0.69) and tilt (r ≤ 0.57), whereas the cepstral peak prominence correlated more strongly (r ≤ 0.90). These empirical relationships provide evidence to support the use of accelerometers as effective complements to acoustic recordings in the assessment and monitoring of vocal function in the laboratory, clinic, and during an individual's daily activities.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {659–668},
numpages = {10},
keywords = {ambulatory voice monitoring, neck-surface accelerom-eter, vocal function analysis, vocal perturbation, harmonics-to-noise ratio, cepstral peak prominence}
}

@article{10.1109/TASLP.2015.2512499,
author = {Traa, Johannes and Wingate, David and Stein, Noah D. and Smaragdis, Paris},
title = {Robust Source Localization and Enhancement with a Probabilistic Steered Response Power Model},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2512499},
doi = {10.1109/TASLP.2015.2512499},
abstract = {Source localization and enhancement are often treated separately in the array processing literature. One can apply steered response power (SRP) localization to determine the sources' Directions-Of-Arrival (DOA) followed by beamforming and Wiener post-filtering to isolate the sources from each other and ambient interference. We show that when there is significant overlap between directional sources of interest in the time-frequency (TF) plane, traditional SRP localization breaks down. This may occur, for example, when the array is located near a reflector, significant early reflections are present, or the sources are harmonized. We propose a joint solution to the localization and enhancement problems via a probabilistic interpretation of the SRP function. We formulate optimization procedures for (1) a mixture of single-source SRP distributions (MoSRP) and (2) a multi-source SRP distribution (MultSRP). Unlike in traditional localization, the latter approach explicitly models source overlap in the TF plane. Results shows that the MultSRP model is capable of localizing sources with significant overlap in the TF domain and that either of the proposed methods out-performs standard SRP localization for multiple speakers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {493–503},
numpages = {11},
keywords = {blind source separation, steered response power, beamforming, source localization}
}

@article{10.1109/TASLP.2016.2517326,
author = {Wang, Lin and Doclo, Simon},
title = {Correlation Maximization-Based Sampling Rate Offset Estimation for Distributed Microphone Arrays},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2517326},
doi = {10.1109/TASLP.2016.2517326},
abstract = {In this paper, we investigate the sampling rate mismatch problem in distributed microphone arrays and propose a correlation maximization algorithm to blindly estimate the sampling rate offset between two asynchronously sampled microphone signals. We approximate the sampling rate offset with a linear-phase drift model in the short-time Fourier transform (STFT) domain and show that the correlation coefficient between two microphone signals tends to present the highest value when the sampling of the two microphone signals is synchronized. Based on this finding we propose the correlation maximization algorithm, which performs sampling rate compensation on two microphone signals with different possible offset values and calculates their correlation coefficient after compensation. The offset value that leads to the largest correlation coefficient is chosen as the optimal estimate. Since the precision of the STFT linear-phase drift model used in the algorithm degrades as the sampling rate offset or the signal length is increased, we further propose a two-stage exhaustive search scheme to detect the optimal sampling rate offset. This scheme is able to minimize the influence of the linear-phase drift model error in order to improve the sampling rate offset estimation accuracy. Both simulated as well as real-world experiments confirm the effectiveness of the proposed algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {571–582},
numpages = {12},
keywords = {sampling rate offset, correlation coefficient, distributed microphone array}
}

@article{10.1109/TASLP.2015.2509248,
author = {Sonnleitner, Reinhard and Widmer, Gerhard},
title = {Robust Quad-Based Audio Fingerprinting},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2509248},
doi = {10.1109/TASLP.2015.2509248},
abstract = {We propose an audio fingerprinting method that adapts findings from the field of blind astrometry to define simple, efficiently representable characteristic feature combinations called quads. Based on these, an audio identification algorithm is described that is robust to noise and severe time-frequency scale distortions and accurately identifies the underlying scale transform factors. The low number and compact representation of content features allows for efficient application of exact fixed-radius near-neighbor search methods for fingerprint matching in large audio collections. We demonstrate the practicability of the method on a collection of 100,000 songs, analyze its performance for a diverse set of noise as well as severe speed, tempo and pitch scale modifications, and identify a number of advantages of our method over two state-of-the-art distortion-robust audio identification algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {409–421},
numpages = {13},
keywords = {media monitoring, pitch shifting, tempo change, time scaling, audio fingerprinting, DJ sets}
}

@article{10.1109/TASLP.2015.2512041,
author = {Lin, Zheng and Jin, Xiaolong and Xu, Xueke and Wang, Yuanzhuo and Cheng, Xueqi and Wang, Weiping and Meng, Dan},
title = {An Unsupervised Cross-Lingual Topic Model Framework for Sentiment Classification},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2512041},
doi = {10.1109/TASLP.2015.2512041},
abstract = {Sentiment classification aims to determine the sentiment polarity expressed in a text. In online customer reviews, the sentiment polarities of words are usually dependent on the corresponding aspects. For instance, in mobile phone reviews, we may expect the long battery time but not enjoy the long response time of the operating system. Therefore, it is necessary and appealing to consider aspects when conducting sentiment classification. Probabilistic topic models that jointly detect aspects and sentiments have gained much success recently. However, most of the existing models are designed to work well in a language with rich resources. Directly applying those models on poor-quality corpora often leads to poor results. Consequently, a potential solution is to use the cross-lingual topic model to improve the sentiment classification for a target language by leveraging data and knowledge from a source language. However, the existing cross-lingual topic models are not suitable for sentiment classification because sentiment factors are not considered therein. To solve these problems, we propose for the first time a novel cross-lingual topic model framework which can be easily combined with the state-of-the-art aspect/sentiment models. Extensive experiments in different domains and multiple languages demonstrate that our model can significantly improve the accuracy of sentiment classification in the target language.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {432–444},
numpages = {13},
keywords = {cross-language, sentiment classification, topic model}
}

@article{10.1109/TASLP.2015.2512039,
author = {Leijon, Arne and Henter, Gustav Eje and Dahlquist, Martin},
title = {Bayesian Analysis of Phoneme Confusion Matrices},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2512039},
doi = {10.1109/TASLP.2015.2512039},
abstract = {This paper presents a parametric Bayesian approach to the statistical analysis of phoneme confusion matrices measured for groups of individual listeners in one or more test conditions. Two different bias problems in conventional estimation of mutual information are analyzed and explained theoretically. Evaluations with synthetic datasets indicate that the proposed Bayesian method can give satisfactory estimates of mutual information and response probabilities, even for phoneme confusion tests using a very small number of test items for each phoneme category. The proposed method can reveal overall differences in performance between two test conditions with better power than conventional Wilcoxon significance tests or conventional confidence intervals. The method can also identify sets of confusion-matrix cells that are credibly different between two test conditions, with better power than a similar approximate frequentist method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {469–482},
numpages = {14},
keywords = {bayes methods, mutual information, parameter estimation, speech recognition}
}

@article{10.1109/TASLP.2016.2518801,
author = {Radmanesh, Nasim and Burnett, Ian S. and Rao, Bhaskar D.},
title = {A Lasso-LS Optimization with a Frequency Variable Dictionary in a Multizone Sound System},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2518801},
doi = {10.1109/TASLP.2016.2518801},
abstract = {This paper presents an approach for multizone wideband sound field generation using an efficient harmonic nested (EHN) dictionary for sparse loudspeakers' placement and weight. Effectively, the nested arrays provide a priori knowledge of prospective loudspeaker locations based on the frequency bands of interest. The nested arrays are then further optimized in the Lasso stage to form an efficient loudspeakers' location dictionary. The final loudspeaker locations and weightings are estimated by a two-stage Lasso-LS pressure matching optimization. In the first stage Lasso algorithm, the center band frequencies of octave bands from 1 kHz to 8 kHz were used to select active loudspeakers. A second stage then optimizes reproduction using all selected loudspeakers on the basis of a regularized LS algorithm. The results demonstrate that the proposed approach provides a solution for the multizone sound system with the mean squared error (MSE) under -30 dB across the targeted frequency range (500 Hz-16 kHz) using a linear array e.g. 13 loudspeakers. While, the single-stage LS approach generates the MSE peaks of -10 dB and -9 dB at 9 kHz within the active and silent zones respectively using an identical number of loudspeakers and array length.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {583–593},
numpages = {11},
keywords = {least-squares, sparse optimization, loudspeaker positioning, frequency variable dictionary, wideband, multizone sound field, lasso}
}

@article{10.1109/TASLP.2016.2515514,
author = {O'Hanlon, Ken and Nagano, Hidehisa and Keriven, Nicolas and Plumbley, Mark D.},
title = {Non-Negative Group Sparsity with Subspace Note Modelling for Polyphonic Transcription},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2515514},
doi = {10.1109/TASLP.2016.2515514},
abstract = {Automatic music transcription (AMT) can be performed by deriving a pitch-time representation through decomposition of a spectrogram with a dictionary of pitch-labelled atoms. Typically, non-negative matrix factorisation (NMF) methods are used to decompose magnitude spectrograms. One atom is often used to represent each note. However, the spectrum of a note may change over time. Previous research considered this variability using different atoms to model specific parts of a note, or large dictionaries comprised of datapoints from the spectrograms of full notes. In this paper, the use of subspace modelling of note spectra is explored, with group sparsity employed as a means of coupling activations of related atoms into a pitched subspace. Stepwise and gradient-based methods for non-negative group sparse decompositions are proposed. Finally, a group sparse NMF approach is used to tune a generic harmonic subspace dictionary, leading to improved NMF-based AMT results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {530–542},
numpages = {13},
keywords = {stepwise optimal, automatic music transcription, non-negative matrix factorisation, group sparsity}
}

@article{10.1109/TASLP.2016.2515506,
author = {Shepstone, Sven Ewan and Lee, Kong Aik and Li, Haizhou and Tan, Zheng-Hua and Jensen, S\o{}ren Holdt},
title = {Total Variability Modeling Using Source-Specific Priors},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2515506},
doi = {10.1109/TASLP.2016.2515506},
abstract = {In total variability modeling, variable length speech utterances are mapped to fixed low-dimensional i-vectors. Central to computing the total variability matrix and i-vector extraction, is the computation of the posterior distribution for a latent variable conditioned on an observed feature sequence of an utterance. In both cases the prior for the latent variable is assumed to be non-informative, since for homogeneous datasets there is no gain in generality in using an informative prior. This work shows in the heterogeneous case, that using informative priors for computing the posterior, can lead to favorable results. We focus on modeling the priors using minimum divergence criterion or factor analysis techniques. Tests on the NIST 2008 and 2010 Speaker Recognition Evaluation (SRE) dataset show that our proposed method beats four baselines: For i-vector extraction using an already trained matrix, for the short2-short3 task in SRE'08, five out of eight female and four out of eight male common conditions, were improved. For the core-extended task in SRE'10, four out of nine female and six out of nine male common conditions were improved. When incorporating prior information into the training of the T matrix itself, the proposed method beats the baselines for six out of eight female and five out of eight male common conditions, for SRE'08, and five and six out of nine conditions, for the male and female case, respectively, for SRE'10. Tests using factor analysis for estimating priors show that two priors do not offer much improvement, but in the case of three separate priors (sparse data), considerable improvements were gained.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {504–517},
numpages = {14},
keywords = {total variability, source variation, i-vector, factor analysis, prior, expectation-maximization}
}

@article{10.1109/TASLP.2016.2517565,
author = {Grijalva, Felipe and Martini, Luiz and Florencio, Dinei and Goldenstein, Siome},
title = {A Manifold Learning Approach for Personalizing HRTFs from Anthropometric Features},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2517565},
doi = {10.1109/TASLP.2016.2517565},
abstract = {We present a new anthropometry-based method to personalize head-related transfer functions (HRTFs) using manifold learning in both azimuth and elevation angles with a single nonlinear regression model. The core element of our approach is a domain-specific nonlinear dimensionality reduction technique, denominated Isomap, over the intraconic component of HRTFs resulting from a spectral decomposition. HRTF intraconic components encode the most important cues for HRTF individualization, leaving out subject-independent cues. First, we modify the graph construction procedure of Isomap to integrate relevant prior knowledge of spatial audio into a single manifold for all subjects by exploiting the existing correlations among HRTFs across individuals, directions, and ears. Then, with the aim of preserving the multifactor nature of HRTFs (i.e. subject, direction and frequency), we train a single artificial neural network to predict low-dimensional HRTFs from anthropometric features. Finally, we reconstruct the HRTF from its estimated low-dimensional version using a neighborhood-based reconstruction approach. Our findings show that introducing prior knowledge in Isomap's manifold is a powerful way to capture the underlying factors of spatial hearing. Our experiments show, with p-values less than 0.05, that our approach outperforms using, either a PCA linear reduction, or the full HTRF, in its intermediate stages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {559–570},
numpages = {12},
keywords = {HRTF personalization, virtual auditory displays, spatial audio, manifold learning}
}

@article{10.1109/TASLP.2015.2511922,
author = {Tan, Tian and Qian, Yanmin and Yu, Kai},
title = {Cluster Adaptive Training for Deep Neural Network Based Acoustic Model},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2511922},
doi = {10.1109/TASLP.2015.2511922},
abstract = {Although context-dependent DNN-HMM systems have achieved significant improvements over GMM-HMM systems, significant performance degradation has been observed if the acoustic condition of the test data mismatches that of the training data. Hence, adaptation and adaptive training of DNN are of great research interest. Previous DNN adaptation works mainly focus on adapting parameters of a single DNN by applying linear transformations to feature or hidden-layer output; introducing vector representation of non-speech variability into the input. In these methods, large number of parameters are required to be estimated during adaptation. In this paper, the cluster adaptive training (CAT) framework is employed for DNN adaptive training. Here, multiple weight matrices are constructed to form the basis of a canonical parametric space. During adaptation, for a new acoustic condition, an interpolation vector is estimated to combine the weight basis into a single adapted weight matrix. Since only the interpolation vector need to be estimated during adaptation, the number of updated parameters is much smaller than existing DNN adaptation methods. The CAT-DNN approach was evaluated on an English switchboard task in unsupervised adaptation mode. It achieved significant WER reductions over the unadapted DNN-HMM, relative 7.6% to 10.6%, with only 10 parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {459–468},
numpages = {10},
keywords = {adaptation, cluster adaptive training, deep neural network}
}

@article{10.1109/TASLP.2015.2512042,
author = {Williamson, Donald S. and Wang, Yuxuan and Wang, DeLiang},
title = {Complex Ratio Masking for Monaural Speech Separation},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2512042},
doi = {10.1109/TASLP.2015.2512042},
abstract = {Speech separation systems usually operate on the short-time Fourier transform (STFT) of noisy speech, and enhance only the magnitude spectrum while leaving the phase spectrum unchanged. This is done because there was a belief that the phase spectrum is unimportant for speech enhancement. Recent studies, however, suggest that phase is important for perceptual quality, leading some researchers to consider magnitude and phase spectrum enhancements. We present a supervised monaural speech separation approach that simultaneously enhances the magnitude and phase spectra by operating in the complex domain. Our approach uses a deep neural network to estimate the real and imaginary components of the ideal ratio mask defined in the complex domain. We report separation results for the proposed method and compare them to related systems. The proposed approach improves over other methods when evaluated with several objective metrics, including the perceptual evaluation of speech quality (PESQ), and a listening test where subjects prefer the proposed approach with at least a 69% rate.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {483–492},
numpages = {10},
keywords = {speech quality, speech separation, deep neural networks, complex ideal ratio mask}
}

@article{10.1109/TASLP.2016.2519146,
author = {Liu, Xin and Bao, Changchun},
title = {Audio Bandwidth Extension Based on Ensemble Echo State Networks with Temporal Evolution},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2519146},
doi = {10.1109/TASLP.2016.2519146},
abstract = {The bandwidth limitation of wideband audio systems degrades the subjective quality and naturalness of audio signals. In this paper, a new method for blind bandwidth extension of wideband audio signals is proposed based on ensemble echo state network with temporal evolution. The high-frequency components in the band of 7 ∼ 14 kHz are artificially restored only from the information in the wideband audio. For each region in the wideband feature space, a specific echo state network with recurrent structure is explored to dynamically model the local mapping relationship between wideband audio features and high-frequency spectral envelope. The transition process among regions is modeled by a hidden Markov model, and a network ensemble technique based on temporal evolution is used to fuse multiple echo state networks such that the high-frequency spectral envelope is estimated. Combining the high-frequency fine spectrum extended by spectral translation, the proposed method can effectively extend the wideband audio to super wideband. In addition, the proposed extension method is applied to the ITU-T G.729.1 wideband audio codec and is further evaluated in comparison with the ITU-T G.729.1 Annex E super-wideband audio codec and the hidden Markov model-based reference bandwidth extension method. Objective quality evaluation results indicate that the proposed method is preferred over the hidden Markov model-based reference bandwidth extension method in terms of log spectral distortion, cosh measure, and differential log spectral distortion. Further, the proposed method improves the auditory quality of the wideband audio and also gains a good performance in the subjective listening tests.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {594–607},
numpages = {14},
keywords = {hidden Markov model, audio coding, echo state network, audio bandwidth extension}
}

@article{10.1109/TASLP.2016.2514496,
author = {Hadad, Elior and Doclo, Simon and Gannot, Sharon},
title = {The Binaural LCMV Beamformer and Its Performance Analysis},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2514496},
doi = {10.1109/TASLP.2016.2514496},
abstract = {The recently proposed binaural linearly constrained minimum variance (BLCMV) beamformer is an extension of the well-known binaural minimum variance distortionless response (MVDR) beamformer, imposing constraints for both the desired and the interfering sources. Besides its capabilities to reduce interference and noise, it also enables to preserve the binaural cues of both the desired and interfering sources, hence making it particularly suitable for binaural hearing aid applications. In this paper, a theoretical analysis of the BLCMV beamformer is presented. In order to gain insights into the performance of the BLCMV beamformer, several decompositions are introduced that reveal its capabilities in terms of interference and noise reduction, while controlling the binaural cues of the desired and the interfering sources. When setting the parameters of the BLCMV beamformer, various considerations need to be taken into account, e.g. based on the amount of interference and noise reduction and the presence of estimation errors of the required relative transfer functions (RTFs). Analytical expressions for the performance of the BLCMV beamformer in terms of noise reduction, interference reduction, and cue preservation are derived. Comprehensive simulation experiments, using measured acoustic transfer functions as well as real recordings on binaural hearing aids, demonstrate the capabilities of the BLCMV beamformer in various noise environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {543–558},
numpages = {16},
keywords = {noise reduction, LCMV beamformer, relative transfer function, binaural cues, hearing aids}
}

@article{10.1109/TASLP.2015.2511623,
author = {Nagathil, Anil and Weihs, Claus and Martin, Rainer},
title = {Spectral Complexity Reduction of Music Signals for Mitigating Effects of Cochlear Hearing Loss},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2511623},
doi = {10.1109/TASLP.2015.2511623},
abstract = {In this paper we study reduced-rank approximations of music signals in the constant-Q spectral domain as a means to reduce effects stemming from cochlear hearing loss. The rationale behind computing reduced-rank approximations is that they allow to reduce the spectral complexity of a music signal. The method is motivated by studies with cochlear implant listeners which have shown that solo instrumental music or music remixed at higher signal-to-interference ratios are preferred over complex music ensembles or orchestras. For computing the reduced-rank approximations we investigate methods based on principal component analysis and partial least squares analysis, and compare them to source separation algorithms. The strategies, which are applied to music with a predominant leading voice, are compared in terms of their ability for mitigating effects of simulated reduced frequency selectivity and with respect to source signal distortions. Established instrumental measures and a newly developed measure indicate a considerable reduction of the auditory distortion resulting from cochlear hearing loss. Furthermore, a listening test reveals a significant preference for the reduced-rank approximations in terms of melody clarity and ease of listening.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {445–458},
numpages = {14},
keywords = {dimensionality reduction, principal component analysis, cochlear hearing loss, music signal processing}
}

@article{10.1109/TASLP.2016.2515502,
author = {Schneider, Martin and Kellermann, Walter},
title = {Multichannel Acoustic Echo Cancellation in the Wave Domain with Increased Robustness to Nonuniqueness},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2515502},
doi = {10.1109/TASLP.2016.2515502},
abstract = {Acoustic echo cancellation (AEC) is significantly more challenging in the case of multichannel reproduction compared to the single-channel case. This is mainly due to the so-called nonuniqueness problem, which becomes especially severe for massive multichannel reproduction systems like, e.g., wave field synthesis systems or higher-order ambisonics, where the different loudspeaker signals are often strongly correlated. In this paper, a multichannel wave-domain AEC structure is proposed that maintains robustness without altering the loudspeaker signals, as is commonly required to preserve the reproduction quality. The proposed method exploits the dominance of certain couplings in the wave-domain representation of the loudspeaker-enclosure-microphone system. In this representation, preferring a strong weight of the dominant couplings in the identified system improves the system identification performance. A modification of the well-known generalized frequency-domain adaptive filtering algorithm is then presented to implement the proposed approach, including a self-contained description of all necessary components. An experimental evaluation compares the proposed method to competing approaches and shows that the proposed method compares well in terms of convergence behavior while avoiding the performance degradation, which is intrinsic to other approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {518–529},
numpages = {12},
keywords = {signal processing, echo cancellers, adaptive signal processing, adaptive filters, active noise reduction, acoustic signal processing}
}

@article{10.1109/TASLP.2015.2509257,
author = {Dong, Li and Wei, Furu and Xu, Ke and Liu, Shixia and Zhou, Ming},
title = {Adaptive Multi-Compositionality for Recursive Neural Network Models},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2509257},
doi = {10.1109/TASLP.2015.2509257},
abstract = {Recursive neural network models have achieved promising results in many natural language processing tasks. The main difference among these models lies in the composition function, i.e., how to obtain the vector representation for a phrase or sentence using the representations of words it contains. This paper introduces a novel Adaptive Multi-Compositionality (AdaMC) layer to recursive neural network models. The basic idea is to use more than one composition function and adaptively select them depending on input vectors. We develop a general framework to model the semantic composition as a distribution of these composition functions. The composition functions and parameters used for adaptive selection are jointly learnt from the supervision of specific tasks. We integrate AdaMC into existing recursive neural network models and conduct extensive experiments on the Stanford Sentiment Treebank and semantic relation classification task. The experimental results demonstrate that AdaMC improves the performance of recursive neural network models and outperforms the baseline methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {422–431},
numpages = {10},
keywords = {neural network, compositionality, semantic composition, recursive neural network (RNN)}
}

@article{10.1109/TASLP.2015.2504866,
author = {Pan, Chao and Benesty, Jacob and Chen, Jingdong},
title = {Design of Directivity Patterns with a Unique Null of Maximum Multiplicity},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2504866},
doi = {10.1109/TASLP.2015.2504866},
abstract = {Differential beamforming is one of the most popular beamforming approaches, which has the great potential to form frequency-invariant directivity patterns. In this paper, we study the design of beampatterns with multiple nulls in the same direction, which is clearly different from the design of beampatterns with distinct nulls. Our contributions are as follows. First, we show how to constrain multiple nulls to the same direction and design the desired beampattern with both the traditional and robust approaches. Second, we derive an explicit form of the white noise gain (WNG) of the traditional approach as a function of the frequency, interelement spacing, and null direction, which shows that the cardioid is the optimal beampattern as far as the WNG is concerned. Third, we prove that the WNG improvement of the robust approach rarely depends on the null direction at low frequencies. Finally, considering the fact that the robust differential beamforming approach may produce a frequency-dependent beampattern while improving the WNG, we develop a weighted-norm approach that can make a good compromise between the robustness of differential beamforming with respect to white noise and the frequency-invariant beampattern. The performance of the developed approach is verified by simulations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {226–235},
numpages = {10},
keywords = {differential microphone arrays, microphone arrays, beampattern design, beamforming, directivity factor, white noise gain}
}

@article{10.1109/TASLP.2015.2501724,
author = {Mohammadiha, Nasser and Doclo, Simon},
title = {Speech Dereverberation Using Non-Negative Convolutive Transfer Function and Spectro-Temporal Modeling},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2501724},
doi = {10.1109/TASLP.2015.2501724},
abstract = {This paper presents two single-channel speech dereverberation methods to enhance the quality of speech signals that have been recorded in an enclosed space. For both methods, the room acoustics are modeled using a non-negative approximation of the convolutive transfer function (N-CTF), and to additionally exploit the spectral properties of the speech signal, such as the low-rank nature of the speech spectrogram, the speech spectrogram is modeled using non-negative matrix factorization (NMF). Two methods are described to combine the N-CTF and NMF models. In the first method, referred to as the integrated method, a cost function is constructed by directly integrating the speech NMF model into the N-CTF model, while in the second method, referred to as the weighted method, the N-CTF and NMF based cost functions are weighted and summed. Efficient update rules are derived to solve both optimization problems. In addition, an extension of the integrated method is presented, which exploits the temporal dependencies of the speech signal. Several experiments are performed on reverberant speech signals with and without background noise, where the integrated method yields a considerably higher speech quality than the baseline N-CTF method and a state-of-the-art spectral enhancement method. Moreover, the experimental results indicate that the weighted method can even lead to a better performance in terms of instrumental quality measures, but that the optimal weighting parameter depends on the room acoustics and the utilized NMF model. Modeling the temporal dependencies in the integrated method was found to be useful only for highly reverberant conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {276–289},
numpages = {14},
keywords = {non-negative matrix factorization, spectral modeling, non-negative convolutive transfer function, speech dereverberation}
}

@article{10.1109/TASLP.2015.2506263,
author = {Koutrouvelis, Andreas I. and Kafentzis, George P. and Gaubitch, Nikolay D. and Heusdens, Richard},
title = {A Fast Method for High-Resolution Voiced/Unvoiced Detection and Glottal Closure/Opening Instant Estimation of Speech},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2506263},
doi = {10.1109/TASLP.2015.2506263},
abstract = {We propose a fast speech analysis method which simultaneously performs high-resolution voiced/unvoiced detection (VUD) and accurate estimation of glottal closure and glottal opening instants (GCIs and GOIs, respectively). The proposed algorithm exploits the structure of the glottal flow derivative in order to estimate GCIs and GOIs only in voiced speech using simple time-domain criteria. We compare our method with well-known GCI/GOI methods, namely, the dynamic programming projected phase-slope algorithm (DYPSA), the yet another GCI/GOI algorithm (YAGA) and the speech event detection using the residual excitation and a mean-based signal (SEDREAMS). Furthermore, we examine the performance of the aforementioned methods when combined with state-of-the-art VUD algorithms, namely, the robust algorithm for pitch tracking (RAPT) and the summation of residual harmonics (SRH). Experiments conducted on the APLAWD and SAM databases show that the proposed algorithm outperforms the state-of-the-art combinations of VUD and GCI/GOI algorithms with respect to almost all evaluation criteria for clean speech. Experiments on speech contaminated with several noise types (white Gaussian, babble, and car-interior) are also presented and discussed. The proposed algorithm outperforms the state-of-the-art combinations in most evaluation criteria for signal-to-noise ratio greater than 10 dB.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {316–328},
numpages = {13},
keywords = {pitch estimation, speech analysis, glottal opening instants (GOIs), voiced/unvoiced detection (VUD), glottal closure instants (GCIs)}
}

@article{10.1109/TASLP.2015.2507940,
author = {Schepker, Henning and Doclo, Simon},
title = {A Semidefinite Programming Approach to Min-Max Estimation of the Common Part of Acoustic Feedback Paths in Hearing Aids},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2507940},
doi = {10.1109/TASLP.2015.2507940},
abstract = {The convergence speed and the computational complexity of adaptive feedback cancellation algorithms both depend on the number of adaptive parameters used to model the acoustic feedback paths. To reduce the number of adaptive parameters it has been proposed to decompose the acoustic feedback paths as the convolution of a time-invariant common part and time-varying variable parts. Instead of estimating all parameters of the common and variable parts by minimizing the misalignment using a least-squares cost function, in this paper we propose to formulate the parameter estimation problem as a min-max optimization problem aiming to maximize the maximum stable gain (MSG). We formulate the min-max optimization problem as a semidefinite program and use a constraint based on Lyapunov theory to guarantee stability of the estimated common pole-zero filter. Experimental results using measured acoustic feedback paths show that the proposed min-max optimization outperforms least-squares optimization in terms of the MSG. Furthermore, the results indicate that the proposed common part decomposition is able to increase the MSG and reduce the number of variable part parameters even for unknown feedback paths that were not included in the optimization. Simulation results using an adaptive feedback cancellation algorithm based on the prediction-error-method show that the convergence speed can be increased by using the proposed feedback path decomposition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {366–377},
numpages = {12},
keywords = {maximum stable gain, hearing aids, min-max optimization, common part modeling, acoustic feedback cancellation}
}

@article{10.1109/TASLP.2015.2505415,
author = {Zhang, Xiao-Lei and Wang, DeLiang},
title = {Boosting Contextual Information for Deep Neural Network Based Voice Activity Detection},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2505415},
doi = {10.1109/TASLP.2015.2505415},
abstract = {Voice activity detection (VAD) is an important topic in audio signal processing. Contextual information is important for improving the performance of VAD at low signal-to-noise ratios. Here we explore contextual information by machine learning methods at three levels. At the top level, we employ an ensemble learning framework, named multi-resolution stacking (MRS), which is a stack of ensemble classifiers. Each classifier in a building block inputs the concatenation of the predictions of its lower building blocks and the expansion of the raw acoustic feature by a given window (called a resolution). At the middle level, we describe a base classifier in MRS, named boosted deep neural network (bDNN). bDNN first generates multiple base predictions from different contexts of a single frame by only one DNN and then aggregates the base predictions for a better prediction of the frame, and it is different from computationally-expensive boosting methods that train ensembles of classifiers for multiple base predictions. At the bottom level, we employ the multi-resolution cochleagram feature, which incorporates the contextual information by concatenating the cochleagram features at multiple spectrotemporal resolutions. Experimental results show that the MRS-based VAD outperforms other VADs by a considerable margin. Moreover, when trained on a large amount of noise types and a wide range of signal-to-noise ratios, the MRS-based VAD demonstrates surprisingly good generalization performance on unseen test scenarios, approaching the performance with noise-dependent training.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {252–264},
numpages = {13},
keywords = {noise-independent training, deep neural network, cochleagram, multi-resolution stacking, voice activity detection, ensemble learning}
}

@article{10.1109/TASLP.2015.2506269,
author = {Wu, Xiaoguang and Chen, Huawei},
title = {Directivity Factors of the First-Order Steerable Differential Array with Microphone Mismatches: Deterministic and Worst-Case Analysis},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2506269},
doi = {10.1109/TASLP.2015.2506269},
abstract = {It is known that differential microphone arrays are sensitive to microphone mismatches. This paper studies the effects of microphone mismatches on the directivity factor (DF) of the first-order steerable differential array (FOSDA) whose response is constructed by a linear combination of monopole and two orthogonal dipoles via a small four-microphone array. In particular, the DF of the FOSDA is studied from two perspectives, i.e., a deterministic analysis for deterministic microphone gain/phase errors, and a worst-case analysis for uncertain microphone gain/phase errors. Based on the theoretical analysis, several interesting properties are revealed, which are helpful to better understand the robustness characteristics of the FOSDA. Moreover, by worst-case analysis, it provides a useful guidance for robust FOSDA design, i.e., how to determine array size and select microphones with required characteristics such that a desired DF value can be guaranteed. In addition, the effect of sensor self-noise on the DF of the FOSDA is also discussed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {300–315},
numpages = {16},
keywords = {steer-able differential microphone array, microphone mismatch, superdirective beamforming, directivity factor}
}

@article{10.1109/TASLP.2015.2509241,
author = {Bentivogli, Luisa and Bertoldi, Nicola and Cettolo, Mauro and Federico, Marcello and Negri, Matteo and Turchi, Marco},
title = {On the Evaluation of Adaptive Machine Translation for Human Post-Editing},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2509241},
doi = {10.1109/TASLP.2015.2509241},
abstract = {We investigate adaptive machine translation (MT) as a way to reduce human workload and enhance user experience when professional translators operate in real-life conditions. A crucial aspect in our analysis is how to ensure a reliable assessment of MT technologies aimed to support human post-editing. We pay particular attention to two evaluation aspects: i) the design of a sound experimental protocol to reduce the risk of collecting biased measurements, and ii) the use of robust statistical testing methods (linear mixed-effects models) to reduce the risk of under/over-estimating the observed variations. Our adaptive MT technology is integrated in a web-based full-fledged computer-assisted translation (CAT) tool. We report on a post-editing field test that involved 16 professional translators working on two translation directions (English-Italian and English-French), with texts coming from two linguistic domains (legal, information technology). Our contrastive experiments compare user post-editing effort with static vs. adaptive MT in an end-to-end scenario where the system is evaluated as a whole. Our results evidence that adaptive MT leads to an overall reduction in post-editing effort (HTER) up to 10.6% (p &lt; 0.05). A follow-up manual evaluation of the MT outputs and their corresponding post-edits confirms that the gain in HTER corresponds to higher quality of the adaptive MT system and does not come at the expense of the final human translation quality. Indeed, adaptive MT shows to return better suggestions than static MT (p &lt; 0.01), and the resulting post-edits do not significantly differ in the two conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {388–399},
numpages = {12},
keywords = {machine translation, computer-assisted translation, post-editing, human in the loop evaluation, online adaptation, domain adaptation}
}

@article{10.1109/TASLP.2015.2506264,
author = {Sharma, Anil and Kaul, Sanjit},
title = {Two-Stage Supervised Learning-Based Method to Detect Screams and Cries in Urban Environments},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2506264},
doi = {10.1109/TASLP.2015.2506264},
abstract = {Smartphones can enable monitoring signs of distress as a human goes about his daily routine. Motivated by this possibility of 24x7 distress detection, we investigate detection of screaming and crying in urban environments, which we categorize into the contexts of indoors (home and office), outdoors, human conversation, large human gatherings, machinery, and audio from multimedia devices. Prior works are often restricted to specific environments or controlled settings. We propose a novel two-stage supervised learning based method, with tunable decision parameters for each stage, to achieve a desired true distress (scream and cry) detection rate (DR) and false alarm rate (FAR). We observe that the choice of the parameters is a function of the signal-to-noise ratio (SNR) of the distress signal, which is the ratio of the power of the distress signal to the power of the context audio. In the absence of SNR information, we show that a simple SNR estimation scheme performs well. Alternately, we show how the decision parameters can be selected based on the context estimated by the method. We show the results of testing the proposal over hundred hours of audio data recorded by the smartphones of ten volunteers as they went about their daily routines. Achieved performance is exemplified by a DR of 93.16% and a FAR of 4.76% at a SNR of 20 dB. The corresponding values for a SNR of 10 dB are 84.13% and 4.77%. Finally, we compare with, which also deals with audio event detection in noisy environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {290–299},
numpages = {10},
keywords = {scream detection, urban environment, cry detection, MFCC, smartphone, context estimation, SVM}
}

@article{10.1109/TASLP.2015.2507862,
author = {Nakamura, Tomohiko and Nakamura, Eita and Sagayama, Shigeki},
title = {Real-Time Audio-to-Score Alignment of Music Performances Containing Errors and Arbitrary Repeats and Skips},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2507862},
doi = {10.1109/TASLP.2015.2507862},
abstract = {This paper discusses real-time alignment of audio signals of music performance to the corresponding score (a.k.a. score following) which can handle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips) in performances. This type of score following is particularly useful in automatic accompaniment for practices and rehearsals, where errors and repeats/skips are often made. Simple extensions of the algorithms previously proposed in the literature are not applicable in these situations for scores of practical length due to the problem of large computational complexity. To cope with this problem, we present two hidden Markov models of monophonic performance with errors and arbitrary repeats/skips, and derive efficient score-following algorithms with an assumption that the prior probability distributions of score positions before and after repeats/skips are independent from each other. We confirmed real-time operation of the algorithms with music scores of practical length (around 10000 notes) on a modern laptop and their tracking ability to the input performance within 0.7 s on average after repeats/skips in clarinet performance data. Further improvements and extension for polyphonic signals are also discussed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {329–339},
numpages = {11},
keywords = {score following, arbitrary repeats and skips, audio-to-score alignment, fast Viterbi algorithm, hidden Markov model, music signal processing}
}

@article{10.1109/TASLP.2015.2509780,
author = {Lee, Bong-Ki and Chang, Joon-Hyuk},
title = {Packet Loss Concealment Based on Deep Neural Networks for Digital Speech Transmission},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2509780},
doi = {10.1109/TASLP.2015.2509780},
abstract = {In this paper, we propose the regression-based packet loss concealment (PLC) for digital speech transmission by using deep neural networks (DNNs) with a multiple-layer deep architecture. For the DNN training, log-power spectra and phases are employed as features in the input layer for the large training set, which ensures non-linear mapping the frames from the last correctly received frame to the missing frame. Once the training is accomplished by the restricted Boltzmann machine (RBM)-based pre-training to initialize the DNN, minimum mean square error (MMSE)-based fine tuning is then performed based on the back-propagation algorithm. In the reconstruction stage, the trained DNN model is fed with the features of the previous frames in order to estimate the log-power spectra and phases of the missing frames. Reconstruction is further improved by using the cross-fading technique to mitigate discontinuity between the reconstruction signal and good frame signal in the time-domain. To demonstrate the performance of the proposed algorithm, hidden Markov model (HMM)-based PLC algorithm and the PLC algorithm standardized in adaptive multi-rate wideband (AMR-WB) Appendix I were used for comparison. The experimental results show that the proposed approach provides better speech quality and speech recognition accuracy than the conventional approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {378–387},
numpages = {10},
keywords = {speech quality, packet loss concealment (PLC), adaptive multi-rate wideband, deep neural network (DNN), regression model, network speech recognition}
}

@article{10.1109/TASLP.2015.2496156,
author = {Bahne, Adrian and Ahl\'{e}n, Anders},
title = {Optimizing the Similarity of Loudspeaker-Room Responses in Multiple Listening Positions},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2496156},
doi = {10.1109/TASLP.2015.2496156},
abstract = {A shortcoming of multichannel sound reproduction standards, such as stereo or 5.1 surround, is their incompatibility with multiple off-axis listening positions. Accurate reproduction of virtual sound sources can only be experienced in the sweet spot, which is located equidistant to the loudspeakers. We here present a novel methodology to compensate audio systems such that the channel similarity is optimized in several listening positions simultaneously. To that end we propose a novel MIMO personal audio filter design framework based on feed-forward control. By proper design choices, filters that successfully compensate for multiple off-axis positions and irregularities in the frequency sum responses are obtained. The design choices include allpass filters with appropriate phase shifts as target for each listening position in addition to a weighted similarity requirement. Evaluations based on measurements of two four-channel car audio systems show that the proposed method significantly improves timbral sound reproduction and phantom center reproduction in several listening positions simultaneously.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {340–353},
numpages = {14},
keywords = {virtual center, psychoacoustics, audio systems, loudspeakers, multiple sweet spots, acoustic signal processing, equalizers}
}

@article{10.1109/TASLP.2015.2504781,
author = {Hung, Jeih-Weih and Hsieh, Hsin-Ju and Chen, Berlin},
title = {Robust Speech Recognition via Enhancing the Complex-Valued Acoustic Spectrum in Modulation Domain},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2504781},
doi = {10.1109/TASLP.2015.2504781},
abstract = {The purpose of this paper is to develop a novel speech feature extraction framework for independently compensating the real and imaginary acoustic spectra of speech signals in the modulation domain with the techniques of histogram equalization (HEQ) and non-negative matrix factorization (NMF). By doing so, we can enhance not only the magnitude but also the phase components of the acoustic spectra, thereby creating noise-robust speech features. More specifically, the proposed framework makes the following three major contributions: First, via either of the HEQ and NMF operations, the long-term cross-frame correlation among the acoustic spectra at the same frequency can be captured to compensate for the spectral distortion caused by noise. Second, the noise effect can be handled in a high acoustic frequency resolution. Finally, the distortion dwelt in the acoustic spectra can be more extensively mitigated due to the independent processes for the respective real and imaginary parts. The evaluation experiments were carried out on the Aurora-2 and Aurora-4 benchmark tasks, and the corresponding results suggest that our proposed methods can achieve performance competitive to or better than many widely used noise robustness methods, including the well-known advanced front-end (AFE) extraction scheme, in speech recognition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {236–251},
numpages = {16},
keywords = {histogram equalization (HEQ), feature extraction, noise robustness, modulation spectrum, non-negative matrix factorization (NMF), automatic speech recognition (ASR)}
}

@article{10.1109/TASLP.2015.2504874,
author = {Rasumow, Eugen and Hansen, Martin and van de Par, Steven and P\"{u}schel, Dirk and Mellert, Volker and Doclo, Simon and Blau, Matthias},
title = {Regularization Approaches for Synthesizing HRTF Directivity Patterns},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2504874},
doi = {10.1109/TASLP.2015.2504874},
abstract = {As an alternative to traditional artificial heads, it is possible to synthesize individual head-related transfer functions (HRTFs) using a so-called virtual artificial head (VAH), consisting of a microphone array with an appropriate topology and filter coefficients optimized using a narrowband least squares cost function. The resulting spatial directivity pattern of such a VAH is known to be sensitive to small deviations of the assumed microphone characteristics, e.g., gain, phase and/or the positions of the microphones. In many beamformer design procedures, this sensitivity is reduced by imposing a white noise gain (WNG) constraint on the filter coefficients for a single desired look direction. In this paper, this constraint is shown to be inappropriate for regularizing the HRTF synthesis with multiple desired directions and three alternative different regularization approaches are proposed and evaluated. In the first approach, the measured deviations of the microphone characteristics are taken into account in the filter design. In the second approach, the filter coefficients are regularized using the mean WNG for all directions. The third approach additionally takes into account several frequency bins into both the optimization and the regularization. The different proposed regularization approaches are compared using analytic and measured transfer functions, including random deviations. Experimental results show that the approach using multiple frequency bands mimicking the spectral resolution of the human auditory system yields the best robustness among the considered regularization approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {215–225},
numpages = {11},
keywords = {virtual artificial head, white noise gain (WNG), beamforming, head-related transfer functions (HRTFs), regularization}
}

@article{10.1109/TASLP.2015.2499040,
author = {Turan, M.A. Tu\u{g}tekin and Erzin, Engin},
title = {Source and Filter Estimation for Throat-Microphone Speech Enhancement},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2499040},
doi = {10.1109/TASLP.2015.2499040},
abstract = {In this paper, we propose a new statistical enhancement system for throat microphone recordings through source and filter separation. Throat microphones (TM) are skin-attached piezoelectric sensors that can capture speech sound signals in the form of tissue vibrations. Due to their limited bandwidth, TM recorded speech suffers from intelligibility and naturalness. In this paper, we investigate learning phone-dependent Gaussian mixture model (GMM)-based statistical mappings using parallel recordings of acoustic microphone (AM) and TM for enhancement of the spectral envelope and excitation signals of the TM speech. The proposed mappings address the phone-dependent variability of tissue conduction with TM recordings. While the spectral envelope mapping estimates the line spectral frequency (LSF) representation of AM from TM recordings, the excitation mapping is constructed based on the spectral energy difference (SED) of AM and TM excitation signals. The excitation enhancement is modeled as an estimation of the SED features from the TM signal. The proposed enhancement system is evaluated using both objective and subjective tests. Objective evaluations are performed with the log-spectral distortion (LSD), the wideband perceptual evaluation of speech quality (PESQ) and mean-squared error (MSE) metrics. Subjective evaluations are performed with an A/B comparison test. Experimental results indicate that the proposed phone-dependent mappings exhibit enhancements over phone-independent mappings. Furthermore enhancement of the TM excitation through statistical mappings of the SED features introduces significant objective and subjective performance improvements to the enhancement of TM recordings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {265–275},
numpages = {11},
keywords = {statistical mapping, throat microphone, gaussian mixture model, speech enhancement}
}

@article{10.1109/TASLP.2015.2507858,
author = {Kates, James M. and Arehart, Kathryn H.},
title = {The Hearing-Aid Audio Quality Index (HAAQI)},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2507858},
doi = {10.1109/TASLP.2015.2507858},
abstract = {This paper presents an index designed to predict music quality for individuals listening through hearing aids. The index is "intrusive," that is, it compares the degraded signal being evaluated to a reference signal. The index is based on a model of the auditory periphery that includes the effects of hearing loss. Outputs from the auditory model are used to measure changes in the signal time-frequency envelope modulation, temporal fine structure, and long-term spectrum caused by the hearing aid processing. The index is constructed by combining a term sensitive to noise and nonlinear distortion with a second term sensitive to changes in the long-term spectrum. The index is fitted to an existing database of music quality judgments made by listeners having normal or impaired hearing. The data comprise ratings for three music excerpts (classical orchestra, jazz trio, and jazz singer), each processed through 100 conditions representative of hearing-aid processing and listening situations. The overall accuracy of the index is high, with a correlation coefficient of 0.970 when computed over all of the processing conditions and averaged over the combined groups of listeners having normal and impaired hearing.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {354–365},
numpages = {12},
keywords = {objective audio quality measures, music quality measures, hearing aids, hearing loss}
}

@article{10.1109/TASLP.2015.2500732,
author = {Torbati, Amir H. Harati Nejad and Picone, Joseph},
title = {A Doubly Hierarchical Dirichlet Process Hidden Markov Model with a Non-Ergodic Structure},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2500732},
doi = {10.1109/TASLP.2015.2500732},
abstract = {Nonparametric Bayesian models use a Bayesian framework to learn model complexity automatically from the data, eliminating the need for a complex model selection process. A Hierarchical Dirichlet Process Hidden Markov Model (HDPHMM) is the nonparametric Bayesian equivalent of a hidden Markov model (HMM), but is restricted to an ergodic topology that uses a Dirichlet Process Model to achieve a mixture distribution-like model. For applications involving ordered sequences (e.g., speech recognition), it is desirable to impose a left-to-right structure on the model. In this paper, we introduce a model based on HDPHMM that: 1) shares data points between states, 2) models non-ergodic structures, and 3) models non-emitting states. The first point is particularly important because Gaussian mixture models, which support such sharing, have been very effective at modeling modalities in a signal (e.g., speaker variability). Further, sharing data points allows models to be estimated more accurately, an important consideration for applications such as speech recognition in which some mixture components occur infrequently. We demonstrate that this new model produces a 20% relative reduction in error rate for phoneme classification and an 18% relative reduction on a speech recognition task on the TIMIT Corpus compared to a baseline system consisting of a parametric HMM.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {174–184},
numpages = {11},
keywords = {nonparametric bayesian models, speech recognition, hierarchical dirichlet processes, hidden markov models}
}

@article{10.1109/TASLP.2015.2500728,
author = {Torres, Humberto and Gurlekian, Jorge},
title = {Novel Estimation Method for the Superpositional Intonation Model},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2500728},
doi = {10.1109/TASLP.2015.2500728},
abstract = {Fujisaki's intonation model parameterizes the F0's contour efficiently and because of its strong physiological basis has been successfully tested in different languages. One problem that has not been fully addressed is the extraction of the model's parameters, i.e., given a sentence, which model's parameter values best describe its intonation. Most of the proposed methods strive to optimize the parameters so as to obtain the best fit for the F0 contour globally. In this paper we propose to use text information from the sentence as the main guide or reference for adjusting the parameters. We present a method that defines a set of rules to fix and optimize the model's parameters. Optimization never loses sight of the text structure events that arouse it. When text information is not enough, the algorithm predicts parameters from F0 contour and tie them to the text. The process of parameter estimation can be seen as a way to go from text information to the F0 contour. Parameter optimization is carried out to fit the F0 contour locally. Our novel approach can be implemented manually or automatically. We present examples of manual implementation and the quantitative results of the automatic one. Tested on three corpora in Spanish, English and German, our automatic method shows a performance of 34% better than other tested methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {151–160},
numpages = {10},
keywords = {Fujisaki's intonational model, superpositional intonational model, model estimation}
}

@article{10.1109/TASLP.2015.2500028,
author = {Anderson, Craig A. and Teal, Paul D. and Poletti, Mark A.},
title = {Spatial Correlation of Radial Gaussian and Uniform Spherical Volume Near-Field Source Distributions},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2500028},
doi = {10.1109/TASLP.2015.2500028},
abstract = {In this paper, a pair of analytic expressions describing the correlation functions due to spherically symmetric radial Gaussian and uniform volume near-field source position distributions are presented. An approximate spatial correlation function solution for a radial Gaussian source location distribution is derived and compared with the existing numerical methods. An exact solution for a uniform volume source location distribution is also derived and compared with existing numerical methods. The approximate radial Gaussian solution produces a result consistent with numerical methods for compact source location distributions. The uniform volume solution matches the expected behavior. Finally, the spatial correlation functions were used to design spatially robust beamformers for compact microphone arrays. Both of the spatial correlation function solutions lead to improved spatial robustness for the applications of signal enhancement and suppression.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {143–150},
numpages = {8},
keywords = {spatial correlation functions, spatial robustness, beamforming}
}

@article{10.1109/TASLP.2015.2487051,
author = {Tahon, Marie and Devillers, Laurence},
title = {Towards a Small Set of Robust Acoustic Features for Emotion Recognition: Challenges},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2487051},
doi = {10.1109/TASLP.2015.2487051},
abstract = {The search of a small acoustic feature set for emotion recognition faces three main challenges. Such a feature set must be robust to large diversity of contexts in real-life applications; model parameters must also be optimized for reduced subsets; finally, the result of feature selection must be evaluated in cross-corpus condition. The goal of the present study is to select a consensual set of acoustic features for valence recognition using classification and non-classification based feature ranking and cross-corpus experiments, and to optimize emotional models simultaneously. Five realistic corpora are used in this study: three of them were collected in the framework of the French project on robotics ROMEO, one is a game corpus (JEMO) and one is the well-known AIBO corpus. Combinations of features found with nonclassification based methods (information gain and Gaussian mixture models with Bhattacharyya distance) through multicorpora experiments are tested under cross-corpus conditions, simultaneously with SVM parameters optimization. Reducing the number of features goes in pair with optimizing model parameters. Experiments carried on randomly selected features from two acoustic feature sets show that a feature space reduction is needed to avoid over-fitting. Since a Grid search tends to find non-standard values with small feature sets, the authors propose a multicorpus optimization method based on different corpora and acoustic feature subsets which ensures more stability. The results show that acoustic families selected with both feature ranking methods are not relevant in cross-corpus experiments. Promising results have been obtained with a small set of 24 voiced cepstral coefficients while this family was ranked in the 2nd and 5th positions with both ranking methods. The proposed optimization method is more robust than the usual Grid search for cross-corpus experiments with small feature sets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {16–28},
numpages = {13},
keywords = {information gain, Bhattacharyya distance, acoustic features, SVM parameter optimization, crosscorpus, emotion recognition}
}

@article{10.1109/TASLP.2015.2498101,
author = {Sun, Meng and Zhang, Xiongwei and Van hamme, Hugo and Zheng, Thomas Fang},
title = {Unseen Noise Estimation Using Separable Deep Auto Encoder for Speech Enhancement},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2498101},
doi = {10.1109/TASLP.2015.2498101},
abstract = {Unseen noise estimation is a key yet challenging step to make a speech enhancement algorithm work in adverse environments. At worst, the only prior knowledge we know about the encountered noise is that it is different from the involved speech. Therefore, by subtracting the components which cannot be adequately represented by a well defined speech model, the noises can be estimated and removed. Given the good performance of deep learning in signal representation, a deep auto encoder (DAE) is employed in this work for accurately modeling the clean speech spectrum. In the subsequent stage of speech enhancement, an extra DAE is introduced to represent the residual part obtained by subtracting the estimated clean speech spectrum (by using the pre-trained DAE) from the noisy speech spectrum. By adjusting the estimated clean speech spectrum and the unknown parameters of the noise DAE, one can reach a stationary point to minimize the total reconstruction error of the noisy speech spectrum. The enhanced speech signal is thus obtained by transforming the estimated clean speech spectrum back into time domain. The above proposed technique is called separable deep auto encoder (SDAE). Given the under-determined nature of the above optimization problem, the clean speech reconstruction is confined in the convex hull spanned by a pre-trained speech dictionary. New learning algorithms are investigated to respect the non-negativity of the parameters in the SDAE. Experimental results on TIMIT with 20 noise types at various noise levels demonstrate the superiority of the proposed method over the conventional baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {93–104},
numpages = {12},
keywords = {deep auto encoder, source separation, unseen noise compensation, speech enhancement}
}

@article{10.1109/TASLP.2015.2495249,
author = {Ardekani, Iman Tabatabaei and Kaipio, Jari P. and Nasiri, Alireza and Sharifzadeh, Hamid and Abdulla, Waleed H.},
title = {A Statistical Inverse Problem Approach to Online Secondary Path Modeling in Active Noise Control},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2495249},
doi = {10.1109/TASLP.2015.2495249},
abstract = {This paper recasts the problem of online secondary path modeling in the form of a statistical inverse problem. A statistical and, in particular, a Bayesian approach towards secondary path modeling is developed and the computational issues that emerge from this approach are discussed. All signals and parameters are modeled as random variables and the degree of information concerning them is coded in their probability density functions. An abstract solution is formulated in the form of a probability density function for the secondary path model. For extracting point estimates, common statistical estimation methods are investigated. It is shown that maximum likelihood estimation is not stable; however, Bayesian method of maximum a posteriori gives a reliable solution. An adaptive algorithm is then developed to compute this solution in a computationally efficient manner. This algorithm has three advantages, compared to the traditional secondary path modeling algorithms. First, it does not cause any interference with the main active noise control algorithm. Second, it does not require any additive-noise to be injected into the secondary path. Third, it does not require any off-line initiation. The convergence of the proposed algorithm is analyzed theoretically. The validity of the theoretical results is investigated by using computer simulation. Finally successful integration of the proposed algorithm into a real-time ANC system is reported.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {54–64},
numpages = {11},
keywords = {active noise control (ANC), bayesian estimation, statistical inverse problems, online secondary path modeling}
}

@article{10.1109/TASLP.2015.2497798,
author = {Adalbj\"{o}rnsson, Stefan I. and Kronvall, Ted and Burgess, Simon and \r{A}str\"{o}m, Kalle and Jakobsson, Andreas},
title = {Sparse Localization of Harmonic Audio Sources},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2497798},
doi = {10.1109/TASLP.2015.2497798},
abstract = {In this paper, we propose a novel method for estimating the locations of near- and/or far-field harmonic audio sources impinging on an arbitrary, but calibrated, sensor array. Using a joint pitch and location estimation formed in two steps, we first estimate the fundamental frequencies and complex amplitudes under a sinusoidal model assumption, whereafter the location of each source is found by utilizing both the difference in phase and the relative attenuation of the magnitude estimates. As audio recordings often consist of multi-pitch signals exhibiting some degree of reverberation, where both the number of pitches and the source locations are unknown, we propose to use sparse heuristics to avoid the necessity of detailed a priori assumptions on the spectral and spatial model orders. The method's performance is evaluated using both simulated and measured audio data, with the former showing that the proposed method achieves near-optimal performance, whereas the latter confirms the method's feasibility when used with real recordings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {117–129},
numpages = {13},
keywords = {non-convex sparsity, convex optimization, multi-pitch estimation, TDOA, block sparsity, ADMM, near- and far-field localization}
}

@article{10.1109/TASLP.2015.2497248,
author = {Stafylakis, Themos and Kenny, Patrick and Alam, Md. Jahangir and Kockmann, Marcel},
title = {Speaker and Channel Factors in Text-Dependent Speaker Recognition},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2497248},
doi = {10.1109/TASLP.2015.2497248},
abstract = {We reformulate joint factor analysis so that it can serve as a feature extractor for text-dependent speaker recognition. The new formulation is based on left-to-right modeling with tied mixture HMMs and it is designed to deal with problems such as the inadequacy of subspace methods in modeling speaker-phrase variability, UBM mismatches that arise as a result of variable phonetic content, and the need to exploit text-independent resources in text-dependent speaker recognition. We pass the features extracted by factor analysis to a trainable backend which plays a role analogous to that of PLDA in the i-vector/PLDA cascade in text-independent speaker recognition. We evaluate these methods on a proprietary dataset consisting of English and Urdu passphrases collected in Pakistan. By using both text-independent data and text-dependent data for training purposes and by fusing results obtained with multiple front ends at the score level, we achieved equal error rates of around 1.3% and 2% on the English and Urdu portions of this task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {65–78},
numpages = {14},
keywords = {factor analysis, speaker recognition}
}

@article{10.1109/TASLP.2015.2496222,
author = {He, Yanzhang and Baumann, Peter and Fang, Hao and Hutchinson, Brian and Jaech, Aaron and Ostendorf, Mari and Fosler-Lussier, Eric and Pierrehumbert, Janet},
title = {Using Pronunciation-Based Morphological Subword Units to Improve OOV Handling in Keyword Search},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2496222},
doi = {10.1109/TASLP.2015.2496222},
abstract = {Out-of-vocabulary (OOV) keywords present a challenge for keyword search (KWS) systems especially in the low-resource setting. Previous research has centered around approaches that use a variety of subword units to recover OOV words. This paper systematically investigates morphology-based subword modeling approaches on seven low-resource languages. We show that using morphological subword units (morphs) in speech recognition decoding is substantially better than expanding word-decoded lattices into subword units including phones, syllables and morphs. As alternatives to grapheme-based morphs, we apply unsupervised morphology learning to sequences of phonemes, graphones, and syllables. Using one of these phone-based morphs is almost always better than using the grapheme-based morphs, but the particular choice varies with the language. By combining the different methods, a substantial gain is obtained over the best single case for all languages, especially for OOV performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {79–92},
numpages = {14},
keywords = {keyword search, out-of-vocabulary (OOV) words, graphones, subword units, morphological analysis, speech recognition}
}

@article{10.1109/TASLP.2015.2456421,
author = {Brognaux, Sandrine and Drugman, Thomas},
title = {HMM-Based Speech Segmentation: Improvements of Fully Automatic Approaches},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2456421},
doi = {10.1109/TASLP.2015.2456421},
abstract = {Speech segmentation refers to the problem of determining the phoneme boundaries from an acoustic recording of an utterance together with its orthographic transcription. This paper focuses on a particular case of hidden Markov model (HMM)-based forced alignment in which the models are directly trained on the corpus to align. The obvious advantage of this technique is that it is applicable to any language or speaking style and does not require manually aligned data. Through a systematic step-by-step study, the role played by various training parameters (e.g. models configuration, number of training iterations) on the alignment accuracy is assessed, with corpora varying in speaking style and language. Based on a detailed analysis of the errors commonly made by this technique, we also investigate the use of additional fully automatic strategies to improve the alignment. Beside the use of supplementary acoustic features, we explore two novel approaches: an initialization of the silence models based on a voice activity detection (VAD) algorithm and the consideration of the forced alignment of the time-reversed sound. The evaluation is carried out on 12 corpora of different sizes, languages (some being under-resourced) and speaking styles. It aims at providing a comprehensive study of the alignment accuracy achieved by the different versions of the speech segmentation algorithm depending on corpus-related specificities. While the baseline method is shown to reach good alignment rates with corpora as small as 2 minutes, we also emphasize the benefit of using a few seconds of bootstrapping data. Regarding improvement methods, our results show that the insertion of additional features outperforms both other strategies. The performance of VAD, however, is shown to be notably striking on very small corpora, correcting more than 60% of the errors superior to 40 ms. Finally, the combination of the three improvement methods is also pointed out as providing the highest alignment rates, with very low variability across the corpora, regardless of their size. This combined technique is shown to outperform available speaker-independent models, improving the alignment rate by 8 to 10% absolute.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {5–15},
numpages = {11},
keywords = {corpora annotation, speech segmentation, hidden markov models, phonetic alignment}
}

@article{10.1109/TASLP.2015.2496226,
author = {Ferrer, Luciana and Lei, Yun and McLaren, Mitchell and Scheffer, Nicolas},
title = {Study of Senone-Based Deep Neural Network Approaches for Spoken Language Recognition},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2496226},
doi = {10.1109/TASLP.2015.2496226},
abstract = {This paper compares different approaches for using deep neural networks (DNNs) trained to predict senone posteriors for the task of spoken language recognition (SLR). These approaches have recently been found to outperform various baseline systems on different datasets, but they have not yet been compared to each other or to a common baseline. Two of these approaches use the DNNs to generate feature vectors which are then processed in different ways to predict the score of each language given a test sample. The features are extracted either from a bottleneck layer in the DNN or from the output layer. In the third approach, the standard i-vector extraction procedure is modified to use the senones as classes and the DNN to predict the zeroth order statistics. We compare these three approaches and conclude that the approach based on bottleneck features followed by i-vector modeling outperform the other two approaches. We also show that score-level fusion of some of these approaches leads to gains over using a single approach for short-duration test samples. Finally, we demonstrate that fusing systems that use DNNs trained with several languages leads to improvements in performance over the best single system, and we propose an adaptation procedure for DNNs trained with languages with less available data. Overall, we show improvements between 40% and 70% relative to a state-of-the-art Gaussian mixture model (GMM) i-vector system on test durations from 3 seconds to 120 seconds on two significantly different tasks: the NIST 2009 language recognition evaluation task and the DARPA RATS language identification task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {105–116},
numpages = {12},
keywords = {deep neural networks (DNNs), spoken language recognition (SLR), senones}
}

@article{10.1109/TASLP.2015.2489558,
author = {Behravan, Hamid and Hautam\"{a}ki, Ville and Siniscalchi, Sabato Marco and Kinnunen, Tomi and Lee, Chin-Hui},
title = {I-Vector Modeling of Speech Attributes for Automatic Foreign Accent Recognition},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2489558},
doi = {10.1109/TASLP.2015.2489558},
abstract = {We propose a unified approach to automatic foreign accent recognition. It takes advantage of recent technology advances in both linguistics and acoustics based modeling techniques in automatic speech recognition (ASR) while overcoming the issue of a lack of a large set of transcribed data often required in designing state-of-the-art ASR systems. The key idea lies in defining a common set of fundamental units "universally" across all spoken accents such that any given spoken utterance can be transcribed with this set of "accent-universal" units. In this study, we adopt a set of units describing manner and place of articulation as speech attributes. These units exist in most spoken languages and they can be reliably modeled and extracted to represent foreign accent cues. We also propose an i-vector representation strategy to model the feature streams formed by concatenating these units. Testing on both the Finnish national foreign language certificate (FSD) corpus and the English NIST 2008 SRE corpus, the experimental results with the proposed approach demonstrate a significant system performance improvement with p-value &lt; 0.05 over those with the conventional spectrum-based techniques. We observed up to a 15% relative error reduction over the already very strong i-vector accented recognition system when only manner information is used. Additional improvement is obtained by adding place of articulation clues along with context information. Furthermore, diagnostic information provided by the proposed approach can be useful to the designers to further enhance the system performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {29–41},
numpages = {13},
keywords = {english corpus, i-vector system, finnish corpus, attribute detectors}
}

@article{10.1109/TASLP.2015.2499038,
author = {Mak, Man-Wai and Pang, Xiaomin and Chien, Jen-Tzung},
title = {Mixture of PLDA for Noise Robust I-Vector Speaker Verification},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2499038},
doi = {10.1109/TASLP.2015.2499038},
abstract = {In real-world environments, noisy utterances with variable noise levels are recorded and then converted to i-vectors for cosine distance or PLDA scoring. This paper investigates the effect of noise-level variability on i-vectors. It demonstrates that noise-level variability causes the i-vectors to shift, causing the noise contaminated i-vectors to form clusters in the i-vector space. It also demonstrates that optimal subspaces for discriminating speakers are noise-level dependent. Based on these observations, this paper proposes using signal-to-noise ratio (SNR) of utterances as guidance for training mixture of PLDA models. To maximize the coordination among the PLDA models, mixtures of PLDA models are trained simultaneously via an EM algorithm using the utterances contaminated with noise at various levels. For scoring, given a test i-vector, the marginal likelihoods from individual PLDA models are linearly combined by the posterior probabilities of the test utterance's SNR. Verification scores are the ratio of the marginal likelihoods. Results based on NIST 2012 SRE suggest that the SNR-dependent mixture of PLDA is not only suitable for the situations where the test utterances exhibit a wide range of SNR, but also beneficial for the test utterances with unknown SNR distribution. Supplementary materials containing full derivations of the EM algorithms and scoring functions can be found in http://bioinfo.eie.polyu.edu.hk/mPLDA/SuppMaterials.pdf.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {130–142},
numpages = {13},
keywords = {speaker verification, noise robustness, mixture of PLDA, probabilistic LDA, i-vectors}
}

@article{10.1109/TASLP.2015.2500018,
author = {Bilbao, Stefan and Hamilton, Brian and Botts, Jonathan and Savioja, Lauri},
title = {Finite Volume Time Domain Room Acoustics Simulation under General Impedance Boundary Conditions},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2500018},
doi = {10.1109/TASLP.2015.2500018},
abstract = {In room acoustics simulation and virtualization applications, accurate wall termination is a perceptually crucial feature. It is particularly important in the setting of wave-based modeling of 3D spaces, using methods such as the finite difference time domain method or finite volume time domain method. In this paper, general locally reactive impedance boundary conditions are incorporated into a 3D finite volume time domain formulation, which may be specialized to the various types of finite difference time domain method under fitted boundary termination. Energy methods are used to determine stability conditions for general room geometries, under a large family of nontrivial wall impedances, for finite volume methods over unstructured grids. Simulation results are presented, highlighting in particular the need for unstructured or fitted cells at the room boundary in the case of the accurate simulation of frequency-dependent room mode decay times.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {161–173},
numpages = {13},
keywords = {room acoustics, finite volume methods, finite difference time domain method}
}

@article{10.1109/TASLP.2015.2502141,
author = {Chien, Jen-Tzung and Yang, Po-Kai},
title = {Bayesian Factorization and Learning for Monaural Source Separation},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2502141},
doi = {10.1109/TASLP.2015.2502141},
abstract = {This paper presents a new Bayesian nonnegative matrix factorization (NMF) for monaural source separation. Using this approach, the reconstruction error based on NMF is represented by a Poisson distribution, and the NMF parameters, consisting of the basis and weight matrices, are characterized by the exponential priors. A variational Bayesian inference procedure is developed to learn variational parameters and model parameters. The randomness in separation process is faithfully represented so that the system robustness to model variations in heterogeneous environments could be achieved. Importantly, the exponential prior parameters are used to impose sparseness in basis representation. The variational lower bound of log marginal likelihood is adopted as the objective to control model complexity. The dependencies of variational objective on model parameters are fully characterized in the derived closed-form solution. A clustering algorithm is performed to find the groups of bases for unsupervised source separation. The experiments on speech/music separation and singing voice separation show that the proposed Bayesian NMF (BNMF) with adaptive basis representation outperforms the NMF with fixed number of bases and the other BNMFs in terms of signal-to-distortion ratio and the global normalized source to distortion ratio.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {185–195},
numpages = {11},
keywords = {bayesian learning, monaural source separation, model complexity, nonnegative matrix factorization}
}

@article{10.1109/TASLP.2015.2502059,
author = {Alon, David Lou and Rafaely, Boaz},
title = {Beamforming with Optimal Aliasing Cancellation in Spherical Microphone Arrays},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2502059},
doi = {10.1109/TASLP.2015.2502059},
abstract = {Spherical microphone arrays facilitate three-dimensional processing and analysis of sound fields in applications such as music recording, beamforming and room acoustics. The frequency bandwidth of operation is constrained by the array configuration. At high frequencies, spatial aliasing leads to side-lobes in the array beam pattern, which limits array performance. Previous studies proposed increasing the number of microphones or changing other characteristics of the array configuration to reduce the effect of aliasing. In this paper we present a method to design beamformers that overcome the effect of spatial aliasing by suppressing the undesired side-lobes through signal processing without physically modifying the configuration of the array. This is achieved by modeling the expected aliasing pattern in a maximum-directivity beamformer design, leading to a higher directivity index at frequencies previously considered to be out of the operating bandwidth, thereby extending the microphone array frequency range of operation. Aliasing cancellation is then extended to other beamformers. A simulation example with a 32-element spherical microphone array illustrates the performance of the proposed method. An experimental example validates the theoretical results in practice.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {196–210},
numpages = {15},
keywords = {aliasing cancellation, spherical microphone arrays, white-noise gain, beamforming, optimal beamformer, array processing, directivity, spatial aliasing, maximum-directiviy beamformer, spherical harmonics}
}

@article{10.1109/TASLP.2015.2493366,
author = {Saeidi, Rahim and Alku, Paavo and B\"{a}ckstr\"{o}m, Tom},
title = {Feature Extraction Using Power-Law Adjusted Linear Prediction with Application to Speaker Recognition under Severe Vocal Effort Mismatch},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2493366},
doi = {10.1109/TASLP.2015.2493366},
abstract = {Linear prediction is one of the most established techniques in signal estimation, and it is widely utilized in speech signal processing. It has been long understood that the nerve firing rate of human auditory system can be approximated by power law nonlinearity, and this has been the motivation behind using perceptual linear prediction in extracting acoustic features in a variety of speech processing applications. In this paper, we revisit the application of power law non-linearity in speech spectrum estimation by compressing/expanding power spectrum in autocorrelation-based linear prediction. The development of so-called LP-α is motivated by a desire to obtain spectral features that present less mismatch than conventionally used spectrum estimation methods when speech of normal loudness is compared to speech under vocal effort. The effectiveness of the proposed approach is demonstrated in a speaker recognition task conducted under severe vocal effort mismatch comparing shouted versus normal speech mode.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {42–53},
numpages = {12},
keywords = {linear prediction, mismatch, speaker recognition, powerlaw, vocal effort, shouting}
}

@article{10.1109/TASLP.2015.2476755,
author = {Xiang, Yong and Natgunanathan, Iynkaran and Rong, Yue and Guo, Song},
title = {Spread Spectrum-Based High Embedding Capacity Watermarking Method for Audio Signals},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2476755},
doi = {10.1109/TASLP.2015.2476755},
abstract = {Audio watermarking is a promising technology for copyright protection of audio data. Built upon the concept of spread spectrum (SS), many SS-based audio watermarking methods have been developed, where a pseudonoise (PN) sequence is usually used to introduce security. A major drawback of the existing SS-based audio watermarking methods is their low embedding capacity. In this paper, we propose a new SS-based audio watermarking method which possesses much higher embedding capacity while ensuring satisfactory imperceptibility and robustness. The high embedding capacity is achieved through a set of mechanisms: embedding multiple watermark bits in one audio segment, reducing host signal interference on watermark extraction, and adaptively adjusting PN sequence amplitude in watermark embedding based on the property of audio segments. The effectiveness of the proposed audio watermarking method is demonstrated by simulation examples.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2228–2237},
numpages = {10},
keywords = {PN sequence, spread spectrum, audio watermarking, embedding capacity, copyright protection}
}

@article{10.1109/TASLP.2015.2479940,
author = {Marquardt, Daniel and Hadad, Elior and Gannot, Sharon and Doclo, Simon},
title = {Theoretical Analysis of Linearly Constrained Multi-Channel Wiener Filtering Algorithms for Combined Noise Reduction and Binaural Cue Preservation in Binaural Hearing Aids},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2479940},
doi = {10.1109/TASLP.2015.2479940},
abstract = {Besides noise reduction, an important objective of binaural speech enhancement algorithms is the preservation of the binaural cues of all sound sources. For the desired speech source and the interfering sources, e.g., competing speakers, this can be achieved by preserving their relative transfer functions (RTFs). It has been shown that the binaural multi-channel Wiener filter (MWF) preserves the RTF of the desired speech source, but typically distorts the RTF of the interfering sources. To this end, in this paper we propose two extensions of the binaural MWF, i.e., the binaural MWF with RTF preservation (MWF-RTF) aiming to preserve the RTF of the interfering source and the binaural MWF with interference rejection (MWF-IR) aiming to completely suppress the interfering source. Analytical expressions for the performance of the binaural MWF, MWF-RTF and MWF-IR in terms of noise reduction, speech distortion and binaural cue preservation are derived, showing that the proposed extensions yield a better performance in terms of the signal-to-interference ratio and preservation of the binaural cues of the directional interference, while the overall noise reduction performance is degraded compared to the binaural MWF. Simulation results using binaural behind-the-ear impulse responses measured in a reverberant environment validate the derived analytical expressions for the theoretically achievable performance of the binaural MWF, MWF-RTF, and MWF-IR, showing that the performance highly depends on the position of the interfering source and the number of microphones. Furthermore, the simulation results show that the MWF-RTF yields a very similar overall noise reduction performance as the binaural MWF, while preserving the binaural cues of both the speech and the interfering source.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2384–2397},
numpages = {14},
keywords = {noise reduction, multi-channel wiener filter (MWF), hearing aids, binaural cues}
}

@article{10.1109/TASLP.2015.2486381,
author = {Hadad, Elior and Marquardt, Daniel and Doclo, Simon and Gannot, Sharon},
title = {Theoretical Analysis of Binaural Transfer Function MVDR Beamformers with Interference Cue Preservation Constraints},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2486381},
doi = {10.1109/TASLP.2015.2486381},
abstract = {The objective of binaural noise reduction algorithms is not only to selectively extract the desired speaker and to suppress interfering sources (e.g., competing speakers) and ambient background noise, but also to preserve the auditory impression of the complete acoustic scene. For directional sources this can be achieved by preserving the relative transfer function (RTF) which is defined as the ratio of the acoustical transfer functions relating the source and the two ears and corresponds to the binaural cues. In this paper, we theoretically analyze the performance of three algorithms that are based on the binaural minimum variance distortionless response (BMVDR) beamformer, and hence, process the desired source without distortion. The BMVDR beamformer preserves the binaural cues of the desired source but distorts the binaural cues of the interfering source. By adding an interference reduction (IR) constraint, the recently proposed BMVDR-IR beamformer is able to preserve the binaural cues of both the desired source and the interfering source. We further propose a novel algorithm for preserving the binaural cues of both the desired source and the interfering source by adding a constraint preserving the RTF of the interfering source, which will be referred to as the BMVDR-RTF beamformer. We analytically evaluate the performance in terms of binaural signal-to-interference-and-noise ratio (SINR), signal-to-interference ratio (SIR), and signal-to-noise ratio (SNR) of the three considered beamformers. It can be shown that the BMVDR-RTF beamformer outperforms the BMVDR-IR beamformer in terms of SINR and outperforms the BMVDR beamformer in terms of SIR. Among all beamformers which are distortionless with respect to the desired source and preserve the binaural cues of the interfering source, the newly proposed BMVDR-RTF beamformer is optimal in terms of SINR. Simulations using acoustic transfer functions measured on a binaural hearing aid validate our theoretical results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2449–2464},
numpages = {16},
keywords = {minimum variance, binaural cues, hearing aids, linearly constrained minimum variance (LCMV) beamformer}
}

@article{10.1109/TASLP.2015.2479045,
author = {Vijayan, Karthika and Murty, K. Sri Rama},
title = {Analysis of Phase Spectrum of Speech Signals Using Allpass Modeling},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2479045},
doi = {10.1109/TASLP.2015.2479045},
abstract = {The phase spectrum of Fourier transform has received lesser prominence than its magnitude counterpart in speech processing. In this paper, we propose a method for parametric modeling of the phase spectrum, and discuss its applications in speech signal processing. The phase spectrum is modeled as the response of an allpass (AP) filter, whose coefficients are estimated from the knowledge of speech production process, especially the impulse-like nature of excitation source. A signal retaining only the phase spectral component of speech signal is derived by suppressing the magnitude spectral component, and is modeled as the output of an AP filter excited with a sequence of impulses. Entropy of energy of the input signal is minimized to estimate the coefficients of the AP filter. The resulting objective function, being nonconvex in nature, is minimized using particle swarm optimization. The group delay response of estimated AP filters can be used for accurate analysis of resonances of the vocal-tract system (VTS). The error signal associated with AP modeling provides unambiguous evidence about the instants of significant excitation of the VTS. The applications of the proposed AP modeling include, but not limited to, formant tracking, extraction of glottal closure instants, speaker verification and speech synthesis.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2371–2383},
numpages = {13},
keywords = {phase spectrum, particle swarm optimization, speech processing, entropy minimization, allpass modeling}
}

@article{10.1109/TASLP.2015.2464687,
author = {Graja, M. and Jaoua, M. and Belguith, L. Hadrich},
title = {Statistical Framework with Knowledge Base Integration for Robust Speech Understanding of the Tunisian Dialect},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2464687},
doi = {10.1109/TASLP.2015.2464687},
abstract = {In this paper, we propose a hybrid method for the spoken Tunisian dialect understanding within a limited task. This method couples a discriminative statistical method with a domain ontology. The statistical method is based on conditional random field (CRF) models learned from a little size corpus to perform conceptual labeling task. These models are able to detect the semantic dependency between words. However, the domain ontology is used to add prior knowledge about the task. Our experiments are based on a real spoken Tunisian dialect corpus. The obtained results show that the proposed method is able to improve the performance of CRF models for speech understanding by the integration of the domain ontology. Our method can be exploited for under-resourced languages and Arabic dialects to overcome the lack of linguistic resources.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2311–2321},
numpages = {11},
keywords = {knowledge base, statistical models, domain ontology, speech understanding, tunisian dialect (TD), conditional random field (CRF)}
}

@article{10.1109/TASLP.2015.2443982,
author = {Che, Wanxiang and Zhao, Yanyan and Guo, Honglei and Su, Zhong and Liu, Ting},
title = {Sentence Compression for Aspect-Based Sentiment Analysis},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2443982},
doi = {10.1109/TASLP.2015.2443982},
abstract = {Sentiment analysis, which addresses the computational treatment of opinion, sentiment, and subjectivity in text, has received considerable attention in recent years. In contrast to the traditional coarse-grained sentiment analysis tasks, such as document-level sentiment classification, we are interested in the fine-grained aspect-based sentiment analysis that aims to identify aspects that users comment on and these aspects' polarities. Aspect-based sentiment analysis relies heavily on syntactic features. However, the reviews that this task focuses on are natural and spontaneous, thus posing a challenge to syntactic parsers. In this paper, we address this problem by proposing a framework of adding a sentiment sentence compression (Sent_Comp) step before performing the aspect-based sentiment analysis. Different from the previous sentence compression model for common news sentences, Sent_Comp seeks to remove the sentiment-unnecessary information for sentiment analysis, thereby compressing a complicated sentiment sentence into one that is shorter and easier to parse. We apply a discriminative conditional random field model, with certain special features, to automatically compress sentiment sentences. Using the Chinese corpora of four product domains, Sent_Comp significantly improves the performance of the aspect-based sentiment analysis. The features proposed for Sent_Comp, especially the potential semantic features, are useful for sentiment sentence compression.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2111–2124},
numpages = {14},
keywords = {sentiment analysis, potential semantic features, sentence compression, aspect-based sentiment analysis}
}

@article{10.1109/TASLP.2015.2475155,
author = {Heo, Inseok and Sethares, William A.},
title = {Classification Based on Speech Rhythm via a Temporal Alignment of Spoken Sentences},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2475155},
doi = {10.1109/TASLP.2015.2475155},
abstract = {How much information is contained in the rhythm of speech? Is it possible to tell, just from the rhythm of the speech, whether the speaker is male or female? Is it possible to tell if they are a native or nonnative speaker? This paper provides a new way to address such questions. Traditional investigations into speech rhythm approach the problem by manually annotating the speech, and investigating a preselected collection of features such as the durations of vowels or inter-phoneme timings. This paper presents a method that can automatically align the audio of multiple people when speaking the same sentence. The output of the alignment procedure is a mapping (from the micro-timing of one speaker to that of another) that can be used as a surrogate for speech rhythm. The method is applied to a large online corpus of speakers and shows that it is possible to classify the speakers based on these mappings alone. Several technical aspects are discussed. First, the spectrograms switch between different-length analysis windows (based on whether the speech is voiced or unvoiced) to ameliorate the time-frequency trade-off. These variable window spectrograms are fed into a dynamic time warping algorithm to produce a timing map which represents the speech rhythm. The accuracy of the alignment is evaluated by a technique of transitive validation, and the timing maps are used to form a feature vector for the classification. The method is applied to the online Speech Accent Archive corpus. In the gender discrimination experiments, the proposed method was only about 5% worse than a state-of-the-art classifier based on spectral feature vectors. In the native speaker discrimination task, the speech rhythm was about 15% better than when using spectral information.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2209–2216},
numpages = {8},
keywords = {automated alignment, speech prosody, speech accent, transitive validation, speech rhythm, variable length windows}
}

@article{10.1109/TASLP.2015.2475173,
author = {Samarasinghe, Prasanga and Abhayapala, Thushara and Poletti, Mark and Betlehem, Terence},
title = {An Efficient Parameterization of the Room Transfer Function},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2475173},
doi = {10.1109/TASLP.2015.2475173},
abstract = {This paper proposes an efficient parameterization of the room transfer function (RTF). Typically, the RTF rapidly varies with varying source and receiver positions, hence requires an impractical number of point to point measurements to characterize a given room. Therefore, we derive a novel RTF parameterization that is robust to both receiver and source variations with the following salient features: 1) The parameterization is given in terms of a modal expansion of 3D basis functions. 2) The aforementioned modal expansion can be truncated at a finite number of modes given that the source and receiver locations are from two sizeable spatial regions, which are arbitrarily distributed. 3) The parameter weights/coefficients are independent of the source/receiver positions. Therefore, a finite set of coefficients is shown to be capable of accurately calculating the RTF between any two arbitrary points from a pre-defined spatial region where the source(s) lie and a pre-defined spatial region where the receiver(s) lie. A practical method to measure the RTF coefficients is also provided, which only requires a single microphone unit and a single loudspeaker unit, given that the room characteristics remain stationary over time. The accuracy of the above parameterization is verified using appropriate simulation examples.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2217–2227},
numpages = {11},
keywords = {microphone array processing, room transfer function (RTF), room reverberation}
}

@article{10.1109/TASLP.2015.2481179,
author = {Carlin, Michael A. and Elhilali, Mounya},
title = {A Framework for Speech Activity Detection Using Adaptive Auditory Receptive Fields},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2481179},
doi = {10.1109/TASLP.2015.2481179},
abstract = {One of the hallmarks of sound processing in the brain is the ability of the nervous system to adapt to changing behavioral demands and surrounding soundscapes. It can dynamically shift sensory and cognitive resources to focus on relevant sounds. Neurophysiological studies indicate that this ability is supported by adaptively retuning the shapes of cortical spectro-temporal receptive fields (STRFs) to enhance features of target sounds while suppressing those of task-irrelevant distractors. Because an important component of human communication is the ability of a listener to dynamically track speech in noisy environments, the solution obtained by auditory neurophysiology implies a useful adaptation strategy for speech activity detection (SAD). SAD is an important first step in a number of automated speech processing systems, and performance is often reduced in highly noisy environments. In this paper, we describe how task-driven adaptation is induced in an ensemble of neurophysiological STRFs, and show how speech-adapted STRFs reorient themselves to enhance spectro-temporal modulations of speech while suppressing those associated with a variety of nonspeech sounds. We then show how an adapted ensemble of STRFs can better detect speech in unseen noisy environments compared to an unadapted ensemble and a noise-robust baseline. Finally, we use a stimulus reconstruction task to demonstrate how the adapted STRF ensemble better captures the spectrotemporal modulations of attended speech in clean and noisy conditions. Our results suggest that a biologically plausible adaptation framework can be applied to speech processing systems to dynamically adapt feature representations for improving noise robustness.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2422–2433},
numpages = {12},
keywords = {neural plasticity, adaptive filtering, spectro-temporal receptive fields, stimulus reconstruction, speech activity detection (SAD)}
}

@article{10.1109/TASLP.2015.2473684,
author = {Anderson, Craig A. and Teal, Paul D. and Poletti, Mark A.},
title = {Spatially Robust Far-Field Beamforming Using the von Mises(-Fisher) Distribution},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2473684},
doi = {10.1109/TASLP.2015.2473684},
abstract = {This paper presents spatially robust far-field microphone beamformers and nullformers derived using the von Mises and von Mises-Fisher distributions to model the expected direction of arrival. Simple analytic expressions are presented for 2D and 3D far-field correlation functions and used to design spatially robust beamformers and nullformers. It is demonstrated that the spatially robust beamformers show a modest improvement in tolerating uncertainty in the target direction of arrival without incurring a significant penalty in terms of SINR performance compared with the MVDR beamformer. In addition, the spatially robust formulation shows significantly improved numerical robustness, indicating improved ability in tolerating intrinsic array errors.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2189–2197},
numpages = {9},
keywords = {spatial robustness, numerical robustness, robust beamforming}
}

@article{10.1109/TASLP.2015.2469634,
author = {Heidel, Aaron and Lu, Hsiang-Hung and Lee, Lin-Shan},
title = {Finding Complex Features for Guest Language Fragment Recovery in Resource-Limited Code-Mixed Speech Recognition},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2469634},
doi = {10.1109/TASLP.2015.2469634},
abstract = {The rise of mobile devices and online learning brings into sharp focus the importance of speech recognition not only for the many languages of the world but also for code-mixed speech, especially where English is the second language. The recognition of code-mixed speech, where the speaker mixes languages within a single utterance, is a challenge for both computers and humans, not least because of the limited training data. We conduct research on a Mandarin-English code-mixed lecture corpus, where Mandarin is the host language and English the guest language, and attempt to find complex features for the recovery of English segments that were misrecognized in the initial recognition pass. We propose a multi-level framework wherein both low-level and high-level cues are jointly considered; we use phonotactic, prosodic, and linguistic cues in addition to acoustic-phonetic cues to discriminate at the frame level between English- and Chinese-language segments. We develop a simple and exact method for CRF feature induction, and improved methods for using cascaded features derived from the training corpus. By additionally tuning the data imbalance ratio between English and Chinese, we demonstrate highly significant improvements over previous work in the recovery of English-language segments, and demonstrate performance superior to DNN-based methods. We demonstrate considerable performance improvements not only with the traditional GMM-HMM recognition paradigm but also with a state-of-the-art hybrid CD-HMM-DNN recognition framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2148–2161},
numpages = {14},
keywords = {speech recognition, code-mixing, language identification, bilingual}
}

@article{10.1109/TASLP.2015.2467964,
author = {Schr\"{o}der, Jens and Goetze, Stefan and Anem\"{u}ller, J\"{o}rn},
title = {Spectro-Temporal Gabor Filterbank Features for Acoustic Event Detection},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2467964},
doi = {10.1109/TASLP.2015.2467964},
abstract = {Algorithms for the automatic detection and recognition of acoustic events are increasingly gaining relevance for the reliable and robust functioning of consumer, assistive and monitoring systems. The extraction of appropriate task relevant acoustic features from the raw sound signal clearly influences performance of subsequent statistical classification, in particular in adverse acoustic situations. The present contribution investigates the use of biologically-inspired features, derived from a filter-bank of two-dimensional Gabor functions, that decompose the spectro-temporal power density into components which capture spectral, temporal and joint spectro-temporal modulation patterns. It is hypothesized that the comparably large joint spectral and temporal extent of these Gabor functions results in features that allow for robust classification. Evaluation of the proposed feature extraction scheme together with an hidden Markov model (HMM) classifier is conducted on two corpora comprising acoustic events in realistic adverse conditions from the D-CASE and CLEAR'07 evaluation campaigns. Relevance of each Gabor filter for classification is analyzed and an optimized parameter set for the Gabor filterbank (GFB) is identified. Performance of the optimized GFB is evaluated in comparison to other state-of-the-art algorithms on isolated event classification and on the full acoustic event detection (AED) including joint classification and temporal segmentation of events. Results show that Gabor features result in a signal representation that exhibits separated average class-specific patterns. An improvement in classification accuracy of up to 26% relative to the Mel-frequency cepstral coefficient (MFCC) baseline is obtained with the optimized GFB. Further experiments demonstrate that this improvement cannot be explained by purely temporal or purely spectral Gabor basis functions. Rather, a GFB with features extending in joint spectro-temporal directions is required to obtain optimum performance. Performance on AED with the D-CASE challenge dataset is shown to improve on previous algorithms from the recent literature.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2198–2208},
numpages = {11},
keywords = {gabor filter-bank, spectro-temporal filters, acoustic event detection (AED)}
}

@article{10.1109/TASLP.2015.2479041,
author = {Zhong, Xionghu and Hopgood, James R.},
title = {A Time—Frequency Masking Based Random Finite Set Particle Filtering Method for Multiple Acoustic Source Detection and Tracking},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2479041},
doi = {10.1109/TASLP.2015.2479041},
abstract = {Considering that multiple talkers may appear simultaneously, a time-frequency (TF) masking based random finite set (RFS) particle filtering (PF) method is developed for multiple acoustic source detection and tracking. The time-delay of arrival (TDOA) measurements of multiple sources are extracted by using a time-frequency masking technique, by which each source's TF bins are clustered and separated in a joint gain-ratio and time-delay histogram. Since a joint detection and tracking problem is considered, both source positions and source numbers are time-varying and need to be estimated. The tracker is built within a RFS Bayesian filtering framework. Essentially, an RFS process is used to characterize the source dynamics that include source appearance/dissappearance and motion trajectories. Latent variables are also introduced to indicate source dynamics and measurement-source associations. Subsequently, a Rao-Blackwellization PF technique is employed so that the source position state can be marginalized and only the latent variables are estimated by using the PF. The main advantage of the proposed approach is that hypothesis-pruning is formulated in a full probabilistic sense. The performance of the proposed approach is demonstrated in real speech recordings as well as in simulated room environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2356–2370},
numpages = {15},
keywords = {acoustic source tracking, particle filtering (PF), random finite set (RFS), room reverberation, time-delay of arrival}
}

@article{10.1109/TASLP.2015.2479037,
author = {Jin, Wenyu and Kleijn, W. Bastiaan},
title = {Theory and Design of Multizone Soundfield Reproduction Using Sparse Methods},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2479037},
doi = {10.1109/TASLP.2015.2479037},
abstract = {Multizone soundfield reproduction over an extended spatial region is a challenging problem in acoustic signal processing. We introduce a method of reproducing a multizone soundfield within a desired region in reverberant environments. It is based on the identification of the acoustic transfer function (ATF) from the loudspeaker over the desired reproduction region using a limited number of microphone measurements. We assume that the soundfield is sparse in the domain of planewave decomposition and identify the ATF using sparse methods. The estimates of the ATFs are then used to derive the optimal least-squares solution for the loudspeaker filters that minimize the reproduction error over the entire reproduction region. Simulations confirm that the method leads to a significantly reduced number of required microphones for accurate multizone sound reproduction, while it also facilitates the reproduction over a wide frequency range. Practical experiments are used to verify the sparse planewave representation of the reverberant soundfield in a real-world listening environment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2343–2355},
numpages = {13},
keywords = {acoustic transfer function, planewave decomposition, sparse approximation, multizone soundfield reproduction, reverberation}
}

@article{10.1109/TASLP.2015.2482118,
author = {Fang, Hao and Ostendorf, Mari and Baumann, Peter and Pierrehumbert, Janet},
title = {Exponential Language Modeling Using Morphological Features and Multi-Task Learning},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2482118},
doi = {10.1109/TASLP.2015.2482118},
abstract = {For languages with fast vocabulary growth and limited resources, data sparsity leads to challenges in training a language model. One strategy for addressing this problem is to leverage morphological structure as features in the model. This paper explores different uses of unsupervised morphological features in both the history and prediction space for three word-based exponential models (maximum entropy, logbilinear, and recurrent neural net (RNN)). Multi-task training is introduced as a regularizing mechanism to improve performance in the continuous-space approaches. The models are compared to non-parametric baselines. From using the RNN with morphological features and multi-task learning, experiments with conversational speech from four languages show we can obtain consistent gains of 7-11% in perplexity reduction in a limited-resource scenario (10 hrs speech), and 12-18% when the training size is increased (80 hrs). Results are mixed for all other approaches, compared to a modified Kneser-Ney baseline, but morphology is useful in continuous-space models compared to their word-only baseline. Multi-task learning improves both continuous-space models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2410–2421},
numpages = {12},
keywords = {limited resources, neural network, language model, morphology}
}

@article{10.1109/TASLP.2015.2470597,
author = {Yu, Kai and Sun, Kai and Chen, Lu and Zhu, Su},
title = {Constrained Markov Bayesian Polynomial for Efficient Dialogue State Tracking},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2470597},
doi = {10.1109/TASLP.2015.2470597},
abstract = {Dialogue state tracking (DST) is a process to estimate the distribution of the dialogue states at each dialogue turn given the interaction history. Although data-driven statistical approaches are of most interest, there have been attempts of using rule-based methods for DST, due to their simplicity, efficiency and portability. However, the performance of these methods are usually not competitive to data-driven tracking approaches and it is not possible to improve the DST performance when training data are available. In this paper, a novel hybrid framework, constrained Markov Bayesian polynomial (CMBP), is proposed to formulate rule-based DST in a general way and allow data-driven rule generation. Here, a DST rule is defined as a polynomial function of a set of probabilities satisfying certain linear constraints. Prior knowledge is encoded in these constraints. Under reasonable assumptions, CMBP optimization can be converted to a constrained integer linear programming problem. The integer coefficient CMBP model is further extended to CMBP with real coefficients by applying grid search. CMBP was evaluated on the data corpora of the first, the second, and the third Dialog State Tracking Challenge (DSTC-1/2/3). Experiments showed that CMBP has good generalization ability and can significantly outperform both traditional rule-based approaches and data-driven statistical approaches with similar feature set. Compared with the state-of-the-art statistical DST approaches with much richer features, CMBP is also competitive.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2177–2188},
numpages = {12},
keywords = {data-driven rule, dialogue state tracking (DST), statistical dialogue management, rule-based model}
}

@article{10.1109/TASLP.2015.2479039,
author = {Yeung, Yu Ting and Lee, Tan and Leung, Cheung-Chi},
title = {Supervised Single-Microphone Multi-Talker Speech Separation with Conditional Random Fields},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2479039},
doi = {10.1109/TASLP.2015.2479039},
abstract = {We apply conditional random field (CRF) for single-microphone speech separation in a supervised learning scenario. We train the parameters with mixture data in which the sources are competing with the same average signal power. Compared with factorial hidden Markov model (HMM) baselines, the CRF settings require fewer training mixture data to improve objective speech quality measures and speech recognition accuracy of the reconstructed sources, when mixing ratios of training and testing mixture data are matched. The CRF settings also handle minor mixing ratio mismatch after adjusting the gain factors of the sources with non-linear mappings inspired from the mixture-maximization model. When the mixing ratio mismatch further increases such that the speech mixture is dominated by only one source, factorial HMM finally catches up with and performs better than the CRF settings due to improved model accuracy. We also develop a convex statistical inference simplification based on linear-chain CRFs. The simplification achieves the same performance level as the original CRF settings after integrating additional observations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2334–2342},
numpages = {9},
keywords = {conditional random fields (CRFs), statistical model-based methods, single-microphone speech separation}
}

@article{10.1109/TASLP.2015.2479042,
author = {Bayram, Ilker},
title = {A Multichannel Audio Denoising Formulation Based on Spectral Sparsity},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2479042},
doi = {10.1109/TASLP.2015.2479042},
abstract = {We consider the estimation of an audio source from multiple noisy observations, where the correlation between noise in the different observations is low. We propose a two-stage method for this estimation problem. The method does not require any information about noise and assumes that the signal of interest has a sparse time-frequency representation. The first stage uses this assumption to obtain the best linear combination of the observations. The second stage estimates the amount of remaining noise and applies a post-filter to further enhance the reconstruction. We discuss the optimality of this method under a specific model and demonstrate its usefulness on synthetic and real data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2272–2285},
numpages = {14},
keywords = {sufficient statistic, spectrogram, post-filter, sparsity, multichannel audio denoising, beamforming, uniformly minimum variance unbiased (UMVU) estimator}
}

@article{10.1109/TASLP.2015.2479040,
author = {Percybrooks, Winston S. and Moore, Elliot},
title = {A New Hmm-Based Voice Conversion Methodology Evaluated on Monolingual and Cross-Lingual Conversion Tasks},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2479040},
doi = {10.1109/TASLP.2015.2479040},
abstract = {The work presented here proposes a new voice conversion (VC) approach based on hidden Markov models (HMMs) for spectral conversion and excitation estimation. This paper is divided in two main parts: First, an initial HMM-based VC system is presented and compared to a state-of-the-art ML-GMM VC system in a monolingual conversion scenario with parallel training data; The second part shows the necessary modifications to use the HMM VC system in a cross-lingual conversion scenario and compares it with a cross-lingual VC system based on artificial neural networks (ANNs). The results of the tests show improved performance of the proposed HMM VC system compared with both the ML-GMM and ANN-based VC alternatives, while at the same time keeping most of the flexibility afforded by the ANN approach with respect to training data requirements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2298–2310},
numpages = {13},
keywords = {hidden markov models (HMMs), excitation estimation, subjective testing, voice conversion, cross-lingual conversion}
}

@article{10.1109/TASLP.2015.2470560,
author = {Z\"{o}hrer, Matthias and Peharz, Robert and Pernkopf, Franz},
title = {Representation Learning for Single-Channel Source Separation and Bandwidth Extension},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2470560},
doi = {10.1109/TASLP.2015.2470560},
abstract = {In this paper, we use deep representation learning for model-based single-channel source separation (SCSS) and artificial bandwidth extension (ABE). Both tasks are ill-posed and source-specific prior knowledge is required. In addition to well-known generative models such as restricted Boltzmann machines and higher order contractive autoencoders two recently introduced deep models, namely generative stochastic networks (GSNs) and sum-product networks (SPNs), are used for learning spectrogram representations. For SCSS we evaluate the deep architectures on data of the 2nd CHiME speech separation challenge and provide results for a speaker dependent, a speaker independent, a matched noise condition and an unmatched noise condition task. GSNs obtain the best PESQ and overall perceptual score on average in all four tasks. Similarly, frame-wise GSNs are able to reconstruct the missing frequency bands in ABE best, measured in frequency-domain segmental SNR. They outperform SPNs embedded in hidden Markov models and the other representation models significantly.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2398–2409},
numpages = {12},
keywords = {single-channel source separation (SCSS), bandwidth extension, sum-product networks, representation learning, generative stochastic networks, deep neural networks (DNNs)}
}

@article{10.1109/TASLP.2015.2468583,
author = {Huang, Po-Sen and Kim, Minje and Hasegawa-Johnson, Mark and Smaragdis, Paris},
title = {Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2468583},
doi = {10.1109/TASLP.2015.2468583},
abstract = {Monaural source separation is important for many real world applications. It is challenging because, with only a single channel of information available, without any constraints, an infinite number of solutions are possible. In this paper, we explore joint optimization of masking functions and deep recurrent neural networks for monaural source separation tasks, including speech separation, singing voice separation, and speech denoising. The joint optimization of the deep recurrent neural networks with an extra masking layer enforces a reconstruction constraint. Moreover, we explore a discriminative criterion for training neural networks to further enhance the separation performance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT datasets for speech separation, singing voice separation, and speech denoising tasks, respectively. Our approaches achieve 2.30-4.98 dB SDR gain compared to NMF models in the speech separation task, 2.30-2.48 dB GNSDR gain and 4.32-5.42 dB GSIR gain compared to existing models in the singing voice separation task, and outperform NMF and DNN baselines in the speech denoising task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2136–2147},
numpages = {12},
keywords = {monaural source separation, deep recurrent neural network (DRNN), time-frequency masking, discriminative training}
}

@article{10.1109/TASLP.2015.2478116,
author = {Bai, Hequn and Richard, Ga\"{e}l and Daudet, Laurent},
title = {Late Reverberation Synthesis: From Radiance Transfer to Feedback Delay Networks},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2478116},
doi = {10.1109/TASLP.2015.2478116},
abstract = {In room acoustic modeling, feedback delay networks (FDN) are known to efficiently model late reverberation due to their capacity to generate exponentially decaying dense impulses. However, this method relies on a careful tuning of the different synthesis parameters, either estimated from a pre-recorded impulse response from the real acoustic scene, or set manually from experience. In this paper, we present a new method, which still inherits the efficiency of the FDN structure, but aims at linking the parameters of the FDN directly to the geometry setting. This relation is achieved by studying the sound energy exchange between each delay line using the acoustic radiance transfer method (RTM). Experimental results show that the late reverberation modeled by this method is in good agreement with the virtual geometry setting.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2260–2271},
numpages = {12},
keywords = {reverberation, room acoustics, acoustic radiance transfer, feedback delay networks (FDNs)}
}

@article{10.1109/TASLP.2015.2476762,
author = {Yoo, In-Chul and Lim, Hyeontaek and Yook, Dongsuk},
title = {Formant-Based Robust Voice Activity Detection},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2476762},
doi = {10.1109/TASLP.2015.2476762},
abstract = {Voice activity detection (VAD) can be used to distinguish human speech from other sounds, and various applications can benefit from VAD-including speech coding and speech recognition. To accurately detect voice activity, the algorithm must take into account the characteristic features of human speech and/or background noise. In many real-life applications, noise frequently occurs in an unexpected manner, and in such situations, it is difficult to determine the characteristics of noise with sufficient accuracy. As a result, robust VAD algorithms that depend less on making correct noise estimates are desirable for real-life applications. Formants are the major spectral peaks of the human voice, and these are highly useful to distinguish vowel sounds. The characteristics of the spectral peaks are such that, these peaks are likely to survive in a signal after severe corruption by noise, and so formants are attractive features for voice activity detection under low signal-to-noise ratio (SNR) conditions. However, it is difficult to accurately extract formants from noisy signals when background noise introduces unrelated spectral peaks. Therefore, this paper proposes a simple formant-based VAD algorithm to overcome the problem of detecting formants under conditions with severe noise. The proposed method achieves a much faster processing time and outperforms standard VAD algorithms under various noise conditions. The proposed method is robust against various types of noise and produces a light computational load, so it is suitable for use in various applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2238–2245},
numpages = {8},
keywords = {spectral peaks, formants, voice activity detection (VAD)}
}

@article{10.1109/TASLP.2015.2485663,
author = {Saito, Shinya and Oishi, Kunio and Furukawa, Toshihiro},
title = {Convolutive Blind Source Separation Using an Iterative Least-Squares Algorithm for Non-Orthogonal Approximate Joint Diagonalization},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2485663},
doi = {10.1109/TASLP.2015.2485663},
abstract = {In this paper, we present an approach of recovering signal waveforms of speech sources from observed signals in noisy and reverberant environments. The approach is based on approximate joint diagonalization estimate to provide interference suppression of source signals and reduce echoes and distortions of separated signals. In the proposed approach, the mixing matrix is estimated by minimizing the constrained direct least-squares (LS) criterion in direct model. Exclusively under the condition where the estimated mixing matrix is not of full rank, it is replaced by a full-rank matrix. The unmixing matrix from the estimated mixing matrix is obtained by setting the frequency response of the composite mixing-unmixing filter to identity matrix. The cross-spectral density diagonal matrices of the source signals are precisely estimated by minimizing the indirect LS criterion in indirect model. These operations are fulfilled by using alternating least-squares algorithm. The correlation between the interfrequency power ratios is used to prevent a misalignment permutation of the unmixing matrix. Finally, we compare the proposed BSS with a number of conventional BSS methods in noisy and reverberant environments under both artificial and actual conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2434–2448},
numpages = {15},
keywords = {approximate joint diagonalization (AJD), blind source separation (BSS), convolutive linear mixture, method of Lagrange multipliers, blind identification, alternating least-squares (ALS) algorithm}
}

@article{10.1109/TASLP.2015.2471096,
author = {Marquardt, Daniel and Hohmann, Volker and Doclo, Simon},
title = {Interaural Coherence Preservation in Multi-Channel Wiener Filtering-Based Noise Reduction for Binaural Hearing Aids},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2471096},
doi = {10.1109/TASLP.2015.2471096},
abstract = {Besides noise reduction an important objective of binaural speech enhancement algorithms is the preservation of the binaural cues of all sound sources. To this end, an extension of the binaural multi-channel Wiener filter (MWF), namely the MWF-ITF, has been proposed, which aims to preserve the Interaural Transfer Function (ITF) of the noise sources. However, the MWF-ITF is well-suited only for directional noise sources but not for, e.g., spatially isotropic noise, whose spatial characteristics cannot be properly described by the ITF but rather by the Interaural Coherence (IC). Hence, another extension of the binaural MWF, namely the MWF-IC, has been recently proposed, which aims to preserve the IC of the noise component. Since for the MWF-IC a substantial tradeoff between noise reduction and IC preservation exists, in this paper we propose a perceptually constrained version of the MWF-IC, where the amount of IC preservation is controlled based on the IC discrimination ability of the human auditory system. In addition, a theoretical analysis of the binaural cue preservation capabilities of the binaural MWF and the MWF-ITF for spatially isotropic noise fields is provided. Several simulations in diffuse noise scenarios show that the perceptually constrained MWF-IC yields a controllable preservation of the IC without significantly degrading the output SNR compared to the binaural MWF and the MWF-ITF. Furthermore, contrary to the binaural MWF and MWF-ITF, the proposed algorithm retains the spatial separation between the output speech and noise components while the binaural cues of the speech component are only slightly distorted, such that the binaural hearing advantage for speech intelligibility can still be exploited.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2162–2176},
numpages = {15},
keywords = {hearing aids, noise reduction, multi-channel wiener filter (MWF), binaural cues, interaural coherence}
}

@article{10.1109/TASLP.2015.2488290,
author = {Yang, Guang and Lyon, Richard F. and Drakakis, E. M.},
title = {Psychophysical Evaluation of an Ultra-Low Power, Analog Biomimetic Cochlear Implant Processor Filterbank Architecture with across Channels AGC},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2488290},
doi = {10.1109/TASLP.2015.2488290},
abstract = {This paper evaluated psychophysically an ultra-low-power, analog biomimetic cochlear-implant (CI) processor filterbank architecture, which was recently proposed and demonstrated in hardware. The architecture/strategy emulates the lateral inhibition (LI) mechanism by employing an automatic gain control (AGC) scheme that is coupled across One-Zero-Gamma-tone-Filter (OZGF) channels. The OZGF filtering and the coupled channel AGC were tested respectively in two experiments, where normal-hearing listeners were required to listen to sentences and recognise key words therein. The sentences were mixed with a steady background noise at signal-to-noise ratios (SNRs) of -6, -3 and 0 dB, and processed through a noise-excited envelope vocoder serving as the acoustic simulator of cochlear implants. In the first experiment, the OZGF filtering was compared with cascaded bandpass biquad filtering employed in recent attempts towards fully-implantable CI processing, in terms of the resulting intelligibility. The results showed that the sharply tuned OZGF response did not degrade intelligibility despite the very limited number (16) of channels used. In the second experiment, the sentences were processed in two multi-channel compression systems, one with the channel AGC coupled, whilst another uncoupled. The results showed that the AGC-coupled system was significantly advantageous, and the improvement averaged across the SNRs is 31 percentage points. Furthermore, this compressive system results in no significant decrease in intelligibility when compared to the linear filtering systems investigated in the first experiment. Thus, the coupled channel AGC may be considered as a potential solution to the limited spectral contrast of current CI systems, which may partially account for their noise-susceptibility.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2465–2473},
numpages = {9},
keywords = {spectral enhancement, cochlear implant, lateral inhibition, psychophysical evaluation}
}

@article{10.1109/TASLP.2015.2479043,
author = {Delgado, H\'{e}ctor and Anguera, Xavier and Fredouille, Corinne and Serrano, Javier},
title = {Fast Single- and Cross-Show Speaker Diarization Using Binary Key Speaker Modeling},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2479043},
doi = {10.1109/TASLP.2015.2479043},
abstract = {Speaker diarization has become a key process within other speech processing systems which take advantage of single-speaker speech signals. Furthermore, finding recurrent speakers among a set of audio recordings, known as cross-show diarization, is gaining attention in the last years. Current state-of-the-art-systems provide good performance, but usually at the cost of long processing times. This limitation may make current systems not suitable for real-life applications. In this line, the speaker diarization approach based on binary key modeling provides a very fast yet accurate alternative. In this paper, we present the last improvements applied in binary key speaker diarization with the aim of further speeding up the process and improving performance. In addition, we propose a novel method for cross-show speaker diarization based on binary keys. Experimental results show the effectiveness of the proposed improvements for single-show speaker diarization, both in terms of speed and performance, obtaining a real-time factor of 0.0354xRT and a 16.8% relative improvement in performance. Furthermore, our proposed cross-show approach provides very competitive performance, just slightly worse than its single-show diarization counterpart, and exhibits a real time factor of 0.04xRT.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2286–2297},
numpages = {12},
keywords = {cross-show speaker diarization, speaker diarization, binary key speaker modeling, within-class sum of squares}
}

@article{10.1109/TASLP.2015.2464702,
author = {Hueber, Thomas and Girin, Laurent and Alameda-Pineda, Xavier and Bailly, G\'{e}rard},
title = {Speaker-Adaptive Acoustic-Articulatory Inversion Using Cascaded Gaussian Mixture Regression},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2464702},
doi = {10.1109/TASLP.2015.2464702},
abstract = {This paper addresses the adaptation of an acoustic-articulatory model of a reference speaker to the voice of another speaker, using a limited amount of audio-only data. In the context of pronunciation training, a virtual talking head displaying the internal speech articulators (e.g., the tongue) could be automatically animated by means of such a model using only the speaker's voice. In this study, the articulatory-acoustic relationship of the reference speaker is modeled by a gaussian mixture model (GMM). To address the speaker adaptation problem, we propose a new framework called cascaded Gaussian mixture regression (C-GMR), and derive two implementations. The first one, referred to as Split-C-GMR, is a straightforward chaining of two distinct GMRs: one mapping the acoustic features of the source speaker into the acoustic space of the reference speaker, and the other estimating the articulatory trajectories with the reference model. In the second implementation, referred to as Integrated-C-GMR, the two mapping steps are tied together in a single probabilistic model. For this latter model, we present the full derivation of the exact EM training algorithm, that explicitly exploits the missing data methodology of machine learning. Other adaptation schemes based on maximum-a posteriori (MAP), maximum likelihood linear regression (MLLR) and direct cross-speaker acoustic-to-articulatory GMR are also investigated. Experiments conducted on two speakers for different amount of adaptation data show the interest of the proposed C-GMR techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2246–2259},
numpages = {14},
keywords = {speaker adaptation, talking head, pronunciation training, EM algorithm, gaussian mixture regression, speech production, acoustic-articulatory inversion}
}

@article{10.1109/TASLP.2015.2479038,
author = {Strasser, Falco and Puder, Henning},
title = {Adaptive Feedback Cancellation for Realistic Hearing Aid Applications},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2479038},
doi = {10.1109/TASLP.2015.2479038},
abstract = {Acoustic feedback is a well-known phenomenon in hearing aids. Under certain conditions it causes the so-called howling effect, which is highly annoying for the hearing aid user and limits the maximum amplification of the hearing aid. The standard adaptive feedback cancellation algorithms suffer from a biased adaptation if the input signal is spectrally colored, as it is for speech and music signals. Due to this bias distortion artifacts (entrainment) are generated. In this paper, we present a sub-band feedback cancellation system which combines decorrelation methods with a new realization of a known non-parametric variable step size. To apply this step size in the context of adaptive feedback cancellation, a method to estimate the signal power of the desired input signal, i.e., without feedback, is necessary. A major part of this paper is spent with the theoretical derivation of this estimate. Furthermore, the complete system is evaluated extensively for several speech and music signals as well as for different feedback scenarios in simulations with feedback paths measured in concrete applications as well as for real-time simulations with hearing aid dummies. Both use hearing loss compensation methods as applied in physical hearing aids. The performance is measured in terms of being able to prevent entrainment and to react to feedback path changes. For both simulation setups the system shows a good performance with respect to the two performance measures. Furthermore, the overall feedback cancellation method relies only on few parameters, shows a low computational complexity, and therefore has a strong practical relevance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2322–2333},
numpages = {12},
keywords = {normalized least mean square (NLMS) filter adaptation, hearing aids, prediction error filter, variable step size, adaptation control, adaptive feedback cancellation, frequency shift}
}

@article{10.1109/TASLP.2015.2468066,
author = {Sheaffer, Jonathan and Van Walstijn, Maarten and Rafaely, Boaz and Kowalczyk, Konrad},
title = {Binaural Reproduction of Finite Difference Simulations Using Spherical Array Processing},
year = {2015},
issue_date = {December 2015},
publisher = {IEEE Press},
volume = {23},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2468066},
doi = {10.1109/TASLP.2015.2468066},
abstract = {Due to its efficiency and simplicity, the finite-difference time-domain method is becoming a popular choice for solving wideband, transient problems in various fields of acoustics. So far, the issue of extracting a binaural response from finite difference simulations has only been discussed in the context of embedding a listener geometry in the grid. In this paper, we propose and study a method for binaural response rendering based on a spatial decomposition of the sound field. The finite difference grid is locally sampled using a volumetric array of receivers, from which a plane wave density function is computed and integrated with free-field head related transfer functions, in the spherical harmonics domain. The volumetric array is studied in terms of numerical robustness and spatial aliasing. Analytic formulas that predict the performance of the array are developed, facilitating spatial resolution analysis and numerical binaural response analysis for a number of finite difference schemes. Particular emphasis is placed on the effects of numerical dispersion on array processing and on the resulting binaural responses. Our method is compared to a binaural simulation based on the image method. Results indicate good spatial and temporal agreement between the two methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2125–2135},
numpages = {11},
keywords = {finite difference methods, room acoustics, sound reproduction, simulation, finite-difference time-do-main (FDTD), binaural processing, microphone arrays}
}

@article{10.1109/TASLP.2015.2450497,
author = {Tran, Dung T. and Vincent, Emmanuel and Jouvet, Denis},
title = {Nonparametric Uncertainty Estimation and Propagation for Noise Robust ASR},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2450497},
doi = {10.1109/TASLP.2015.2450497},
abstract = {We consider the framework of uncertainty propagation for automatic speech recognition (ASR) in highly nonstationary noise environments. Uncertainty is considered as the variance of speech distortion. Yet, its accurate estimation in the spectral domain and its propagation to the feature domain remain difficult. Existing methods typically rely on a single uncertainty estimator and propagator fixed by mathematical approximation. In this paper, we propose a new paradigm where we seek to learn more powerful mappings to predict uncertainty from data. We investigate two such possible mappings: linear fusion of multiple uncertainty estimators/propagators and nonparametric uncertainty estimation/propagation. In addition, a procedure to propagate the estimated spectral-domain uncertainty to the static Mel frequency cepstral coefficients (MFCCs), to the log-energy, and to their first- and second-order time derivatives is proposed. This results in a full uncertainty covariance matrix over both static and dynamic MFCCs. Experimental evaluation on Tracks 1 and 2 of the 2nd CHiME Challenge resulted in up to 29% and 28% relative keyword error rate reduction with respect to speech enhancement alone.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1835–1846},
numpages = {12},
keywords = {nonparametric estimation, uncertainty estimation, uncertainty decoding, robust speech recognition}
}

@article{10.1109/TASLP.2015.2456413,
author = {Tu, Mei and Zhou, Yu and Zong, Chengqing},
title = {Exploring Diverse Features for Statistical Machine Translation Model Pruning},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2456413},
doi = {10.1109/TASLP.2015.2456413},
abstract = {In phrase-based and hierarchical phrase-based statistical machine translation systems, translation performance depends heavily on the size and quality of the translation table. To meet the requirements of making a real-time response, some research has been performed to filter the translation table. However, most existing methods are always based on one or two constraints that act as hard rules, such as not allowing phrase-pairs with low translation probabilities. These approaches sometimes make constraints rigid because they consider only a single factor instead of composite factors. Based on the considerations above, in this paper, we propose a machine learning-based framework that integrates multiple features for translation model pruning. Experimental results show that our framework is effective by pruning 80% of the phrase-pairs and 70% of the hierarchical rules, while retaining the quality of the translation models when using the BLEU evaluation metric. Our study further shows that our method can select the most useful phrase-pairs and rules, including those that are low in frequency but still very useful.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1847–1857},
numpages = {11},
keywords = {syntactic constraints, statistical machine translation (SMT), classification, translation model pruning}
}

@article{10.1109/TASLP.2015.2456426,
author = {Okopal, Greg and Wisdom, Scott and Atlas, Les},
title = {Speech Analysis with the Strong Uncorrelating Transform},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2456426},
doi = {10.1109/TASLP.2015.2456426},
abstract = {The strong uncorrelating transform (SUT) provides estimates of independent components from linear mixtures using only second-order information, provided that the components have unique circularity coefficients. We propose a processing framework for generating complex-valued subbands from real-valued mixtures of speech and noise where the objective is to control the likely values of the sample circularity coefficients of the underlying speech and noise components in each subband. We show how several processing parameters affect the noncircularity of speech-like and noise components in the subband, ultimately informing parameter choices that allow for estimation of each of the components in a subband using the SUT. Additionally, because the speech and noise components will have unique sample circularity coefficients, this statistic can be used to identify time--frequency regions that contain voiced speech. We give an example of the recovery of the circularity coefficients of a real speech signal from a two-channel noisy mixture at 25 dB SNR, which demonstrates how the estimates of noncircularity can reveal the time-frequency structure of a speech signal in very high levels of noise. Finally, we present the results of a voice activity detection (VAD) experiment showing that two new circularity-based statistics, one of which is derived from the SUT processing, can achieve improved performance over state-of-the-art VADs in real-world recordings of noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1858–1868},
numpages = {11},
keywords = {short-time fourier transform (STFT), voice activity detection (VAD), noncircularity, improper, circularity coefficients, speech processing}
}

@article{10.1109/TASLP.2015.2447284,
author = {Zhao, Xiaojia and Wang, Yuxuan and Wang, DeLiang},
title = {Cochannel Speaker Identification in Anechoic and Reverberant Conditions},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2447284},
doi = {10.1109/TASLP.2015.2447284},
abstract = {Speaker identification (SID) in cochannel speech, where two speakers are talking simultaneously over a single recording channel, is a challenging problem. Previous studies address this problem in the anechoic environment under the Gaussian mixture model (GMM) framework. On the other hand, cochannel SID in reverberant conditions has not been addressed. This paper studies cochannel SID in both anechoic and reverberant conditions. We first investigate GMM-based approaches and propose a combined system that integrates two cochannel SID methods. Second, we explore deep neural networks (DNNs) for cochannel SID and propose a DNN-based recognition system. Evaluation results demonstrate that our proposed systems significantly improve SID performance over recent approaches in both anechoic and reverberant conditions and various target-to-interferer ratios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1727–1736},
numpages = {10},
keywords = {gaussian mixture model (GMM), cochannel speaker identification, reverberation, target-to-interferer ratio, deep neural network (DNN)}
}

@article{10.1109/TASLP.2015.2456423,
author = {Chen, Zhangli and Hohmann, Volker},
title = {Online Monaural Speech Enhancement Based on Periodicity Analysis and <i>a Priori</i> SNR Estimation},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2456423},
doi = {10.1109/TASLP.2015.2456423},
abstract = {This paper describes an online algorithm for enhancing monaural noisy speech. First, a novel phase-corrected low-delay gammatone filterbank is derived for signal subband decomposition and resynthesis; the subband signals are then analyzed frame by frame. Second, a novel feature named periodicity degree (PD) is proposed to be used for detecting and estimating the fundamental period (Po) in each frame and for estimating the signal-to-noise ratio (SNR) in each frame-subband signal unit. The PD is calculated in each unit as the multiplication of the normalized autocorrelation and the comb filter ratio, and shown to be robust in various low-SNR conditions. Third, the noise energy level in each signal unit is estimated recursively based on the estimated SNR for units with high PD and based on the noisy signal energy level for units with low PD. Then the a priori SNR is estimated using a decision-directed approach with the estimated noise level. Finally, a revised Wiener gain is calculated, smoothed, and applied to each unit; the processed units are summed across subbands and frames to form the enhanced signal. The Po detection accuracy of the algorithm was evaluated on two corpora and showed comparable performance on one corpus and better performance on the other corpus when compared to a recently published pitch detection algorithm. The speech enhancement effect of the algorithm was evaluated on one corpus with two objective criteria and showed better performance in one highly non-stationary noise and comparable performance in two other noises when compared to a state-of-the-art statistical-model based algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1904–1916},
numpages = {13},
keywords = {periodicity analysis, a priori signal-to-noise ratio (SNR) estimation, online implementation, monaural speech enhancement}
}

@article{10.1109/TASLP.2015.2469142,
author = {Pan, Chao and Chen, Jingdong and Benesty, Jacob},
title = {Theoretical Analysis of Differential Microphone Array Beamforming and an Improved Solution},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2469142},
doi = {10.1109/TASLP.2015.2469142},
abstract = {Differential microphone arrays (DMAs), which are responsive to the differential sound pressure field, have attracted much attention due to their properties of frequency-invariant beampatterns, small apertures, and potential of maximum directivity. Traditionally, DMAs are designed and implemented in a multistage (cascade) way, where a proper time delay is used in each stage to form a beampattern of interest. Recently, it was reported that DMAs can be designed by solving a linear system of equations formed from the information about the nulls of the desired beampattern. This paper deals with the problem of beamforming with linear DMAs. Its major contributions are as follows. 1) By using the spatial Z transform, we present some theoretical analysis of both the traditional cascade and new null-constrained DMA beamforming. It is shown that the cascade and null-constrained DMAs of the same order with the same number of sensors are theoretically identical. 2) We develop a two-stage approach to the study of the robust DMA beamformer, which is based on the principle of maximizing the white noise gain (WNG). The first-stage of this approach is in the structure of the traditional non-robust DMA while the second-stage filter is optimized for improving the WNG. 3) Using the two-stage approach, we show that the robust DMA beamformer may introduce extra nulls in the beampattern at high frequencies; particularly, it introduces M -- N -- 1 extra nulls if the interelement spacing is equal to half of the wavelength, where M and N are the number of sensors and the DMA order, respectively. 4) We develop a method that can solve the extra-null problem while maximizing the WNG in robust DMA beamforming, i.e., a robust solution with a frequency-invariant beampattern.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2093–2105},
numpages = {13},
keywords = {directivity factor, differential microphone arrays (DMAs), microphone arrays, directivity pattern, frequency-invariant beampattern, differential beamforming, robust DMAs, white noise gain}
}

@article{10.1109/TASLP.2015.2464678,
author = {Cumani, Sandro},
title = {Fast Scoring of Full Posterior PLDA Models},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2464678},
doi = {10.1109/TASLP.2015.2464678},
abstract = {A low-dimensional representation of a speech segment, the so-called i-vector, in combination with probabilistic linear discriminant analysis (PLDA) models, is the current state-of-the-art in speaker recognition. An i-vector is a compact representation of a Gaussian Mixture Model (GMM) supervector, which captures most of the GMM supervectors variability. It is usually obtained by a MAP estimate of the mean of a posterior distribution. A new PLDA model has been recently presented that, unlike the standard one, exploits the intrinsic i-vector uncertainty. This approach, referred to in this paper as Full Posterior Distribution PLDA (FP-PLDA), is particularly effective for speaker detection of short and variable duration speech segments. It is, however, computationally far more expensive than standard PLDA, making it unattractive for real applications. This paper presents three simplifications of FP-PLDA based on approximate diagonalizations of matrices involved in FP-PLDA scoring. Using in sequence these approximations allows obtaining computational costs comparable to PLDA models, with only a small performance degradation with respect to the more accurate, but less efficient, FP-PLDA models. In particular, up to 10% better performance than PLDA is obtained, with similar computational complexity, on short speech segments of variable duration, randomly extracted from the interviews and telephone conversations included in the NIST SRE 2010 extended dataset. The benefits of the proposed diagonalization approaches have also been confirmed on a short utterance text-independent verification task, where approximately 43% and 34% improvement of the EER and minimum DCF08, respectively, has been obtained with respect to PLDA.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2036–2045},
numpages = {10},
keywords = {speaker recognition, i-vector extraction, probabilistic linear discriminant analysis (PLDA), i-vectors}
}

@article{10.1109/TASLP.2015.2456428,
author = {Sim\'{o}n G\'{a}lvez, Marcos F. and Elliott, Stephen J. and Cheer, Jordan},
title = {Time Domain Optimization of Filters Used in a Loudspeaker Array for Personal Audio},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2456428},
doi = {10.1109/TASLP.2015.2456428},
abstract = {This paper describes a time domain design method for calculating the coefficients of FIR filters used to drive a loudspeaker array for personal audio. A motivating application is to boost the television audio in a certain spatial region with the aim of increasing the speech intelligibility of the hearing impaired. As the array is of small size, superdirective beamforming is applied to increase the low and mid frequency directional performance of the radiator. The filters for such arrays have previously been designed one frequency at a time, leading to non-causal filters that require a modeling delay for real time implementation. By posing the filter optimization in the time domain, the filter responses can be causally constrained, and the optimization is performed once for all frequencies. This paper considers the performance of such filters by carrying out off-line simulations, firstly using the impulse responses of point sources in the free field, and then with the measured responses of a loudspeaker array in an anechoic chamber. The simulation results show how the time domain optimization allows the creation of filters with either a low order or a low modeling delay.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1869–1878},
numpages = {10},
keywords = {personal audio, time domain optimization, array signal processing}
}

@article{10.1109/TASLP.2015.2456417,
author = {Wu, Chung-Hsien and Shen, Han-Ping and Hsu, Chun-Shan},
title = {Code-Switching Event Detection by Using a Latent Language Space Model and the Delta-Bayesian Information Criterion},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2456417},
doi = {10.1109/TASLP.2015.2456417},
abstract = {This paper proposes a new paradigm for code-switching event detection based on latent language space models (LLSMs) and the delta-Bayesian information criterion (ΔBIC). A phone-based Mandarin--English speech recognizer was first employed for obtaining the senone sequence of a speech utterance. For each senone, acoustic features and the posterior probability of the articulatory features (AFs) were extracted and applied to an eigenspace transformation, based on principal component analysis (PCA). Latent semantic analysis (LSA) was then adopted for constructing a matrix to model the importance of each principal component in the eigenspace for the senones and AFs in each language. The spatial relationships among the senones (or AFs) represented by the PCA-transformed eigenvalues in the LSA-based matrix were employed to construct an LLSM for characterizing a language. In code-switching event detection, the language likelihood between the input speech LLSM and each of the language-dependent LLSMs was estimated. The Euclidian-distance-based similarities and cosine-angle-distance-based similarities were adopted for estimating the language likelihood for senones and AFs. The ΔBIC was then used for estimating the language transition score for each hypothesized code-switching event. Finally, the dynamic programming algorithm was employed for obtaining the most likely code-switching language sequence. The proposed approach was evaluated using a Mandarin--English code-switching speech database and outperformed other conventional methods. A duration accuracy of 72.45% can be obtained from the proposed system with optimized parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1892–1903},
numpages = {12},
keywords = {delta-Bayesian information criterion (ΔBIC), articulatory features, senones, code-switching event detection, latent language space model}
}

@article{10.1109/TASLP.2015.2465150,
author = {Hirao, Tsutomu and Nishino, Masaaki and Yoshida, Yasuhisa and Suzuki, Jun and Yasuda, Norihito and Nagata, Masaaki},
title = {Summarizing a Document by Trimming the Discourse Tree},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2465150},
doi = {10.1109/TASLP.2015.2465150},
abstract = {Recent studies on extractive text summarization formulate it as a combinatorial optimization problem, extracting the optimal subset from a set of the textual units that maximizes an objective function without violating the length constraint. Although these methods successfully improve automatic evaluation scores, they do not consider the discourse structure in the source document. Thus, summaries generated by these methods may lack logical coherence. In previous work, we proposed a method that exploits a discourse tree structure to produce coherent summaries. By transforming a traditional discourse tree, namely a rhetorical structure theory-based discourse tree (RST-DT), into a dependency-based discourse tree (DEP-DT), we formulated the summarization procedure as a Tree Knapsack Problem whose tree corresponds to the DEP-DT. This paper extends the work with a detailed discussion of the approach together with a novel efficient dynamic programming algorithm for solving the Tree Knapsack Problem. Experiments show that our method not only achieved the highest score in both automatic and human evaluation, but also obtained good performance in terms of the linguistic qualities of the summaries.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2081–2092},
numpages = {12},
keywords = {tree knapsack problem, single-document summarization, discourse analysis}
}

@article{10.1109/TASLP.2015.2458585,
author = {Deng, Feng and Bao, Changchun and Kleijn, W. Bastiaan},
title = {Sparse Hidden Markov Models for Speech Enhancement in Non-Stationary Noise Environments},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2458585},
doi = {10.1109/TASLP.2015.2458585},
abstract = {We propose a sparse hidden Markov model (HMM)- based single-channel speech enhancement method that models the speech and noise gains accurately in non-stationary noise environments. Autoregressive models are employed to describe the speech and noise in a unified framework and the speech and noise gains are modeled as random processes with memory. The likelihood criterion for finding the model parameters is augmented with an lp regularization term resulting in a sparse autoregressive HMM (SARHMM) system that encourages sparsity in the speech- and noise-modeling. In the SARHMM only a small number of HMM states contribute significantly to the model of each particular observed speech segment. As it eliminates ambiguity between noise and speech spectra, the sparsity of speech and noise modeling helps to improve the tracking of the changes of both spectral shapes and power levels of non-stationary noise. Using the modeled speech and noise SARHMMs, we first construct a noise estimator to estimate the noise power spectrum. Then, a Bayesian speech estimator is derived to obtain the enhanced speech signal. The subjective and objective test results indicate that the proposed speech enhancement scheme can achieve a larger segmental SNR improvement, a lower log-spectral distortion and a better speech quality in stationary noise conditions than state-of-the-art reference methods. The advantage of the new method is largest for non-stationary noise conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1973–1987},
numpages = {15},
keywords = {non-stationary noise, sparse autoregressive hidden Markov model (ARHMM), gain modeling, speech enhancement}
}

@article{10.1109/TASLP.2015.2449088,
author = {Aragonda, Haricharan and Seelamantula, Chandra Sekhar},
title = {Demodulation of Narrowband Speech Spectrograms Using the Riesz Transform},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2449088},
doi = {10.1109/TASLP.2015.2449088},
abstract = {We propose a two-dimensional (2-D) multicomponent amplitude-modulation, frequency-modulation (AM-FM) model for a spectrogram patch corresponding to voiced speech, and develop a new demodulation algorithm to effectively separate the AM, which is related to the vocal tract response, and the carrier, which is related to the excitation. The demodulation algorithm is based on the Riesz transform and is developed along the lines of Hilbert-transform-based demodulation for 1-D AM-FM signals. We compare the performance of the Riesz transform technique with that of the sinusoidal demodulation technique on real speech data. Experimental results show that the Riesz-transform-based demodulation technique represents spectrogram patches accurately. The spectrograms reconstructed from the demodulated AM and carrier are inverted and the corresponding speech signal is synthesized. The signal-to-noise ratio (SNR) of the reconstructed speech signal, with respect to clean speech, was found to be 2 to 4 dB higher in case of the Riesz transform technique than the sinusoidal demodulation technique.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1824–1834},
numpages = {11},
keywords = {Riesz transform, grating compression transform (GCT), multiband AM-FM, sinusoidal demodulation, amplitude modulation model of spectrograms, spectro-temporal analysis}
}

@article{10.1109/TASLP.2015.2464692,
author = {Chu, Y. J. and Chan, S. C.},
title = {A New Local Polynomial Modeling-Based Variable Forgetting Factor RLS Algorithm and Its Acoustic Applications},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2464692},
doi = {10.1109/TASLP.2015.2464692},
abstract = {This paper proposes a new class of local polynomial modeling (LPM)-based variable forgetting factor (VFF) recursive least squares (RLS) algorithms called the LPM-based VFF RLS (LVFF-RLS) algorithms. It models the time-varying channel coefficients as local polynomials so as to obtain the expressions of the bias and variance terms in the mean square error (MSE) of the RLS algorithm. A new locally optimal VFF (LOVFF) is then derived by minimizing the resulting MSE and the theoretical analysis is found to be in good agreement with experimental results. Methods for estimating the parameters involved in this LOVFF are also developed, resulting in an improved RLS algorithm with VFF. The algorithm is further extended to include variable regularization and a QR decomposition (QRD) version which is numerically more stable and amenable to multiplier-less implementation using coordinate rotation digital computer (CORDIC) algorithm. Applications of these algorithms to frequency estimation and adaptive beamforming in time-varying speech and audio signals are also presented to illustrate the effectiveness of the proposed algorithms. Simulations show that the convergence and tracking performance of the proposed algorithms compare favorably with conventional algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2059–2069},
numpages = {11},
keywords = {variable forgetting factor (VFF), local polynomial modeling (LPM), recursive least square (RLS), variable regularization, mean square error (MSE) analysis, adaptive beamforming, frequency estimation}
}

@article{10.1109/TASLP.2015.2461448,
author = {Chen, Ling-Hui and Raitio, Tuomo and Valentini-Botinhao, Cassia and Ling, Zhen-Hua and Yamagishi, Junichi},
title = {A Deep Generative Architecture for Postfiltering in Statistical Parametric Speech Synthesis},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2461448},
doi = {10.1109/TASLP.2015.2461448},
abstract = {The generated speech of hidden Markov model (HMM)-based statistical parametric speech synthesis still sounds "muffled." One cause of this degradation in speech quality may be the loss of fine spectral structures. In this paper, we propose to use a deep generative architecture, a deep neural network (DNN) generatively trained, as a postfilter. The network models the conditional probability of the spectrum of natural speech given that of synthetic speech to compensate for such gap between synthetic and natural speech. The proposed probabilistic postfilter is generatively trained by cascading two restricted Boltzmann machines (RBMs) or deep belief networks (DBNs) with one bidirectional associative memory (BAM). We devised two types of DNN postfilters: one operating in the mel-cepstral domain and the other in the higher dimensional spectral domain. We compare these two new data-driven postfilters with other types of postfilters that are currently used in speech synthesis: a fixed mel-cepstral based postfilter, the global variance based parameter generation, and the modulation spectrum-based enhancement. Subjective evaluations using the synthetic voices of a male and female speaker confirmed that the proposed DNN-based postfilter in the spectral domain significantly improved the segmental quality of synthetic speech compared to that with conventional methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2003–2014},
numpages = {12},
keywords = {modulation spectrum, hidden Markov model (HMM), postfilter, deep generative architecture, segmental quality, speech synthesis}
}

@article{10.1109/TASLP.2015.2449083,
author = {Hoffmann, Falk-Martin and Fazi, Filippo Maria},
title = {Theoretical Study of Acoustic Circular Arrays with Tangential Pressure Gradient Sensors},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2449083},
doi = {10.1109/TASLP.2015.2449083},
abstract = {Microphone arrays as a means of sound field acquisition have been the topic of extensive research for more than eight decades now. A number of designs have been suggested, each trying to overcome difficulties that are inherent to either the decomposition of the sound field, the transducers in use or the presence of the array itself. This work presents a theoretical analysis of circular microphone arrays that do not measure the sound pressure but the component of its gradient that is tangential to a given boundary. Its performance is compared to that of a conventional pressure sensor array as a benchmark. The focus of the analysis and subsequent assessment lies on spatial aliasing and performance in the presence of noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1762–1774},
numpages = {13},
keywords = {microphone arrays, spatial aliasing, tangential pressure gradient, cylindrical arrays, circular arrays, transducer noise}
}

@article{10.1109/TASLP.2015.2464671,
author = {Tourbabin, Vladimir and Rafaely, Boaz},
title = {Direction of Arrival Estimation Using Microphone Array Processing for Moving Humanoid Robots},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2464671},
doi = {10.1109/TASLP.2015.2464671},
abstract = {The auditory system of humanoid robots has gained increased attention in recent years. This system typically acquires the surrounding sound field by means of a microphone array. Signals acquired by the array are then processed using various methods. One of the widely applied methods is direction of arrival estimation. The conventional direction of arrival estimation methods assume that the array is fixed at a given position during the estimation. However, this is not necessarily true for an array installed on a moving humanoid robot. The array motion, if not accounted for appropriately, can introduce a significant error in the estimated direction of arrival. The current paper presents a signal model that takes the motion into account. Based on this model, two processing methods are proposed. The first one compensates for the motion of the robot. The second method is applicable to periodic signals and utilizes the motion in order to enhance the performance to a level beyond that of a stationary array. Numerical simulations and an experimental study are provided, demonstrating that the motion compensation method almost eliminates the motion-related error. It is also demonstrated that by using the motion-based enhancement method it is possible to improve the direction of arrival estimation performance, as compared to that obtained when using a stationary array.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2046–2058},
numpages = {13},
keywords = {robot audition, rotation, microphone array, moving array, spherical harmonics, translation, direction of arrival estimation}
}

@article{10.1109/TASLP.2015.2459599,
author = {Dennis, Jonathan and Tran, Huy Dat and Li, Haizhou},
title = {Generalized Hough Transform for Speech Pattern Classification},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2459599},
doi = {10.1109/TASLP.2015.2459599},
abstract = {While typical hybrid neural network architectures for automatic speech recognition (ASR) use a context window of frame-based features, this may not be the best approach to capture the wider temporal context, which contains phonetic and linguistic information that is equally important. In this paper, we introduce a system that integrates both the spectral and geometrical shape information from the acoustic spectrum, inspired by research in the field of machine vision. In particular, we focus on the Generalized Hough Transform (GHT), which is a sophisticated technique that can model the geometrical distribution of speech information over the wider temporal context. To integrate the GHT as part of a hybrid-ASR system, we propose to use a neural network, with features derived from the probabilistic Hough voting step of the GHT, to implement an improved version of the GHT where the output of the network represents the conventional target class posteriors. A major advantage of our approach is that each step of the GHT is highly interpretable, particularly compared to deep neural network (DNN) systems which are commonly treated as powerful black-box classifiers that give little insight into how the output is achieved. Experiments are carried out on two speech pattern classification tasks. The first is the TIMIT phoneme classification, which demonstrates the performance of the approach on a standard ASR task. The second is a spoken word recognition challenge, which highlights the flexibility of the approach to capture phonetic information within a longer temporal context.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1963–1972},
numpages = {10},
keywords = {generalized hough transform, codebook activation map, TIMIT, speech pattern classification}
}

@article{10.1109/TASLP.2015.2443983,
author = {Islam, Md Tauhidul and Shahnaz, Celia and Zhu, Wei-Ping and Ahmad, M. Omair},
title = {Speech Enhancement Based on Student <i>t</i> Modeling of Teager Energy Operated Perceptual Wavelet Packet Coefficients and a Custom Thresholding Function},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2443983},
doi = {10.1109/TASLP.2015.2443983},
abstract = {This paper presents a speech enhancement approach, where an adaptive threshold is statistically determined based on Student t Modeling of Teager energy (TE) operated perceptual wavelet packet (PWP) coefficients of noisy speech. In order to obtain an enhanced speech, the threshold thus derived is applied upon the PWP coefficients by employing a Student t pdf dependent custom thresholding function, which is designed based on a combination of modified hard and semisoft thresholding functions. Extensive simulations are carried out using the NOIZEUS database to evaluate the effectiveness of the proposed method for car and multi-talker babble noise corrupted speech signals. Several standard objective measures and subjective evaluations including formal listening tests show that the proposed method outperforms some of the state-of-the-art speech enhancement methods at high as well as low levels of SNRs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1800–1811},
numpages = {12},
keywords = {student t, speech enhancement, perceptual wavelet packet transform, Teager energy (TE), Kullback-Liebler divergence}
}

@article{10.1109/TASLP.2015.2450494,
author = {Souvira\`{a}-Labastie, Nathan and Olivero, Anaik and Vincent, Emmanuel and Bimbot, Fr\'{e}d\'{e}ric},
title = {Multi-Channel Audio Source Separation Using Multiple Deformed References},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2450494},
doi = {10.1109/TASLP.2015.2450494},
abstract = {We present a general multi-channel source separation framework where additional audio references are available for one (or more) source(s) of a given mixture. Each audio reference is another mixture which is supposed to contain at least one source similar to one of the target sources. Deformations between the sources of interest and their references are modeled in a linear manner using a generic formulation. This is done by adding transformation matrices to an excitation-filter model, hence affecting different axes, namely frequency, dictionary component or time. A nonnegative matrix co-factorization algorithm and a generalized expectation-maximization algorithm are used to estimate the parameters of the model. Different model parameterizations and different combinations of algorithms are tested on music plus voice mixtures guided by music and/or voice references and on professionally-produced music recordings guided by cover references. Our algorithms improve the signal-to-distortion ratio (SDR) of the sources with the lowest intensity by 9 to 15 decibels (dB) with respect to original mixtures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1775–1787},
numpages = {13},
keywords = {generalized expectation-maximization (GEM) algorithm, source separation}
}

@article{10.1109/TASLP.2015.2449072,
author = {Ngoc Do, Quynh Thi and Bethard, Steven and Moens, Marie-Francine},
title = {Domain Adaptation in Semantic Role Labeling Using a Neural Language Model and Linguistic Resources},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2449072},
doi = {10.1109/TASLP.2015.2449072},
abstract = {We propose a method for adapting Semantic Role Labeling (SRL) systems from a source domain to a target domain by combining a neural language model and linguistic resources to generate additional training examples. We primarily aim to improve the results of Location, Time, Manner and Direction roles. In our methodology, main words of selected predicates and arguments in the source-domain training data are replaced with words from the target domain. The replacement words are generated by a language model and then filtered by several linguistic filters (including Part-Of-Speech (POS), WordNet and Predicate constraints). In experiments on the out-of-domain CoNLL 2009 data, with the Recurrent Neural Network Language Model (RNNLM) and a well-known semantic parser from Lund University, we show enhanced recall and F1 without penalizing precision on the four targeted roles. These results improve the results of the same SRL system without using the language model and the linguistic resources, and are better than the results of the same SRL system that is trained with examples that are enriched with word embeddings. We also demonstrate the importance of using a language model and the vocabulary of the target domain when generating new training examples.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1812–1823},
numpages = {12},
keywords = {open domain, semantic role labeling, language model, linguistic resources}
}

@article{10.1109/TASLP.2015.2464691,
author = {de-la-Calle-Silos, Fernando and Valverde-Albacete, Francisco J. and Gallardo-Antol\'{\i}n, Ascensi\'{o}n and Pel\'{a}ez-Moreno, Carmen},
title = {Morphologically Filtered Power-Normalized Cochleograms as Robust, Biologically Inspired Features for ASR},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2464691},
doi = {10.1109/TASLP.2015.2464691},
abstract = {In this paper, we present advances in the modeling of the masking behavior of the human auditory system (HAS) to enhance the robustness of the feature extraction stage in automatic speech recognition (ASR). The solution adopted is based on a nonlinear filtering of a spectro-temporal representation applied simultaneously to both frequency and time domains---as if it were an image---using mathematical morphology operations. A particularly important component of this architecture is the so-called structuring element (SE) that in the present contribution is designed as a single three-dimensional pattern using physiological facts, in such a way that closely resembles the masking phenomena taking place in the cochlea. A proper choice of spectro-temporal representation lends validity to the model throughout the whole frequency spectrum and intensity spans assuming the variability of the masking properties of the HAS in these two domains. The best results were achieved with the representation introduced as part of the power normalized cepstral coefficients (PNCC) together with a spectral subtraction step. This method has been tested on Aurora 2, Wall Street Journal and ISOLET databases including both classical hidden Markov model (HMM) and hybrid artificial neural networks (ANN)-HMM back-ends. In these, the proposed front-end analysis provides substantial and significant improvements compared to baseline techniques: up to 39.5% relative improvement compared to MFCC, and 18.7% compared to PNCC in the Aurora 2 database.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2070–2080},
numpages = {11},
keywords = {power normalized cepstral coefficients (PNCC), auditory-based features, cochlear masking models, spectro-temporal processing, automatic speech recognition (ASR), morphological filtering}
}

@article{10.1109/TASLP.2015.2446202,
author = {Shin, Ho Seon and Fingscheidt, Tim and Kang, Hong-Goo},
title = {<i>A Priori</i> SNR Estimation Using Air- and Bone-Conduction Microphones},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2446202},
doi = {10.1109/TASLP.2015.2446202},
abstract = {This paper proposes an a priori signal-to-noise ratio (SNR) estimator using an air-conduction (AC) and a bone-conduction (BC) microphone. Among various ways of combining AC and BC microphones for speech enhancement, it is shown that the total enhancement performance can be maximized if the BC microphone is utilized for estimating the power spectral density (PSD) of the desired speech signal. Considering the fact that a small deviation in the speech PSD estimation process brings severe spectral distortion, this paper focuses on controlling weighting factors while estimating the a priori SNR with the decision-directed approach framework. The time--frequency varying weighting factor that is determined by taking a minimum mean square error criterion improves the capability of eliminating residual noise and minimizing speech distortion. Since the weighting factors are also adjusted by measuring the usefulness of the AC and BC microphones, the proposed approach is suitable for tracking the parameter even if the characteristic of environment changes rapidly. The simulation results confirm the superiority of the proposed algorithm to conventional algorithms in high noise environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2015–2025},
numpages = {11},
keywords = {speech enhancement, bone-conduction (BC) microphone, a priori signal-to-noise ratio (SNR)}
}

@article{10.1109/TASLP.2015.2460459,
author = {Ranjan, Rishabh and Gan, Woon-Seng},
title = {Natural Listening over Headphones in Augmented Reality Using Adaptive Filtering Techniques},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2460459},
doi = {10.1109/TASLP.2015.2460459},
abstract = {Augmented reality (AR), which composes of virtual and real world environments, is becoming one of the major topics of research interest due to the advent of wearable devices. Today, AR is commonly used as assistive display to enhance the perception of reality in education, gaming, navigation, sports, entertainment, simulators, etc. However, most of the past works have mainly concentrated on the visual aspects of AR. Auditory events are one of the essential components in human perceptions in daily life but the augmented reality solutions have been lacking in this regard till now compared to visual aspects. Therefore, there is a need of natural listening in AR systems to give a holistic experience to the user. A new headphones configuration is presented in this work with two pairs of binaural microphones attached to headphones (one internal and one external microphone on each side). This paper focuses on enabling natural listening using open headphones employing adaptive filtering techniques to equalize the headset such that virtual sources are perceived as close as possible to sounds emanating from the physical sources. This would also require a superposition of virtual sources with the physical sound sources, as well as ambience. Modified versions of the filtered-x normalized least mean square algorithm (FxNLMS) are proposed in the paper to converge faster to the optimum solution as compared to the conventional FxNLMS. Measurements are carried out with open structure type headphones to evaluate their performance. Subjective test was conducted using individualized binaural room impulse responses (BRIRs) to evaluate the perceptual similarity between real and virtual sounds.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1988–2002},
numpages = {15},
keywords = {natural listening, head related transfer function (HRTF), augmented reality (AR), spatial audio, adaptive filtering}
}

@article{10.1109/TASLP.2015.2449089,
author = {Chen, Liang-Yu and Jang, Jyh-Shing Roger},
title = {Automatic Pronunciation Scoring with Score Combination by Learning to Rank and Class-Normalized DP-Based Quantization},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2449089},
doi = {10.1109/TASLP.2015.2449089},
abstract = {This paper proposes an automatic pronunciation scoring framework using learning to rank and class-normalized, dynamic-programming-based quantization. The goal is to train a model that is able to grade the pronunciation of a second language learner, such that the predicted score is as close as possible to the one given by a human teacher. Under this framework, each utterance is given a score of 1 to 5 by human raters, which is treated as a ground truth rank for the training algorithm. The corpus was rated by qualified English teachers in Taiwan (nonnative speakers). Nine phone-level scores are computed and converted into word-level scores through four conversion methods. We select the 16 best performing scores as the input features to train the learning-to-rank function. The output of the function is then quantized to a discrete rank on a 1-5 scale. The quantization is done with class normalization to alleviate the problem of data imbalance over different classes. Experimental results show that the proposed framework achieves a higher correlation to the human scores than other methods, along with higher accuracy in detecting instances of mispronunciation. We also release a new version of our nonnative corpus with human rankings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1737–1749},
numpages = {13},
keywords = {automatic pronunciation scoring, learning to rank, computer assisted language learning (CALL), computer assisted pronunciation training (CAPT)}
}

@article{10.1109/TASLP.2015.2456431,
author = {Sarreshtedari, Saeed and Akhaee, Mohammad Ali and Abbasfar, Aliazam},
title = {A Watermarking Method for Digital Speech Self-Recovery},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2456431},
doi = {10.1109/TASLP.2015.2456431},
abstract = {Authentication and tampering detection of the digital signals is one of the main applications of the digital watermarking. Recently, watermarking algorithms for digital images are developed to not only detect the image tampering, but also to recover the lost content to some extent. In this paper, a new watermarking scheme is introduced to generate digital self-embedding speech signals enjoying the self-recovery feature. For this purpose, the compressed version of the speech signal generated by a speech codec and protected against the tampering by the proper channel coding is embedded into the original speech signal. Experimental results show that the self-embedding speech signal is recoverable with proper speech quality for high tampering rates, without significant loss in the quality of the original speech signal.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1917–1925},
numpages = {9},
keywords = {digital speech self-recovery, speech authentication, digital speech watermarking, source-channel coding}
}

@article{10.1109/TASLP.2015.2449071,
author = {Tang, Duyu and Qin, Bing and Wei, Furu and Dong, Li and Liu, Ting and Zhou, Ming},
title = {A Joint Segmentation and Classification Framework for Sentence Level Sentiment Classification},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2449071},
doi = {10.1109/TASLP.2015.2449071},
abstract = {In this paper, we propose a joint segmentation and classification framework for sentence-level sentiment classification. It is widely recognized that phrasal information is crucial for sentiment classification. However, existing sentiment classification algorithms typically split a sentence as a word sequence, which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains, such as {"not bad," "bad"} and {"a great deal of," "great"}. We address this issue by developing a joint framework for sentence-level sentiment classification. It simultaneously generates useful segmentations and predicts sentence-level polarity based on the segmentation results. Specifically, we develop a candidate generation model to produce segmentation candidates of a sentence; a segmentation ranking model to score the usefulness of a segmentation candidate for sentiment classification; and a classification model for predicting the sentiment polarity of a segmentation. We train the joint framework directly from sentences annotated with only sentiment polarity, without using any syntactic or sentiment annotations in segmentation level. We conduct experiments for sentiment classification on two benchmark datasets: a tweet dataset and a review dataset. Experimental results show that: 1) our method performs comparably with state-of-the-art methods on both datasets; 2) joint modeling segmentation and classification outperforms pipelined baseline methods in various experimental settings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1750–1761},
numpages = {12},
keywords = {sentiment classification, natural language processing, joint segmentation and classification, artificial intelligence, sentiment analysis}
}

@article{10.1109/TASLP.2015.2462712,
author = {Wu, Ji and Li, Miao and Lee, Chin-Hui},
title = {A Probabilistic Framework for Representing Dialog Systems and Entropy-Based Dialog Management through Dynamic Stochastic State Evolution},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2462712},
doi = {10.1109/TASLP.2015.2462712},
abstract = {In this paper, we present a probabilistic framework for goal-driven spoken dialog systems. A new dynamic stochastic state (DS-state) is then defined to characterize the goal set of a dialog state at different stages of the dialog process. Furthermore, an entropy minimization dialog management (EMDM) strategy is also proposed to combine with the DS-states to facilitate a robust and efficient solution in reaching a user's goals. A song-on-demand task, with a total of 38 117 songs and 12 attributes corresponding to each song, is used to test the performance of the proposed approach. In an ideal simulation, assuming no errors, the EMDM strategy is the most efficient goal-seeking method among all tested approaches, returning the correct song within 3.3 dialog turns on average. Furthermore, in a practical scenario, with top five candidates to handle the unavoidable automatic speech recognition (ASR) and natural language understanding (NLU) errors, the results show that only 61.7% of the dialog goals can be successfully obtained in 6.23 dialog turns on average when random questions are asked by the system, whereas if the proposed DS-states are updated with the top five candidates from the SLU output using the proposed EMDM strategy executed at every DS-state, then a 86.7% dialog success rate can be accomplished effectively within 5.17 dialog turns on average. We also demonstrate that entropy-based DM strategies are more efficient than non-entropy based DM. Moreover, using the goal set distributions in EMDM, the results are better than those without them, such as in sate-of-the-art database summary DM.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {2026–2035},
numpages = {10},
keywords = {automatic speech recognition (ASR), dialog turns, spoken language understanding, dialog state modeling, probabilistic dialog representation, entropy minimization, dialog management, spoken dialog system}
}

@article{10.1109/TASLP.2015.2456430,
author = {Bokaei, Mohammad Hadi and Sameti, Hossein and Liu, Yang},
title = {Linear Discourse Segmentation of Multi-Party Meetings Based on Local and Global Information},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2456430},
doi = {10.1109/TASLP.2015.2456430},
abstract = {Linear segmentation of a meeting conversation is beneficial as a stand-alone system (to organize a meeting and make it easier to access) or as a preprocessing step for many other meeting related tasks. Such segmentation can be done according to two different criteria: topic in which a meeting is segmented according to the different items in its agenda, and function in which the segmentation is done according to the meeting's different events (like discussion, monologue). In this article we concentrate on the function segmentation task and propose new unsupervised methods to segment a meeting into functionally coherent parts. The first proposed method assigns a score to each possible boundary according to its local information and then selects the best ones. The second method uses a dynamic programming approach to find the global best segmentation according to a defined cost function. Since these two methods are complementary of each other, we propose the third method as a combination of the first two ones, which takes advantage of both to improve the final segmentation. In order to evaluate our proposed methods, a subset of a standard meeting dataset (AMI) is manually annotated and used as the test set. Results show that our proposed methods perform significantly better than the previous unsupervised approach according to different evaluation metrics.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1879–1891},
numpages = {13},
keywords = {meeting function segmentation, linear discourse segmentation, dynamic programming approach, unsupervised algorithm}
}

@article{10.1109/TASLP.2015.2457612,
author = {Miao, Yajie and Zhang, Hao and Metze, Florian},
title = {Speaker Adaptive Training of Deep Neural Network Acoustic Models Using I-Vectors},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2457612},
doi = {10.1109/TASLP.2015.2457612},
abstract = {In acoustic modeling, speaker adaptive training (SAT) has been a long-standing technique for the traditional Gaussian mixture models (GMMs). Acoustic models trained with SAT become independent of training speakers and generalize better to unseen testing speakers. This paper ports the idea of SAT to deep neural networks (DNNs), and proposes a framework to perform feature-space SAT for DNNs. Using i-vectors as speaker representations, our framework learns an adaptation neural network to derive speaker-normalized features. Speaker adaptive models are obtained by fine-tuning DNNs in such a feature space. This framework can be applied to various feature types and network structures, posing a very general SAT solution. In this paper, we fully investigate how to build SAT-DNN models effectively and efficiently. First, we study the optimal configurations of SAT-DNNs for large-scale acoustic modeling tasks. Then, after presenting detailed comparisons between SAT-DNNs and the existing DNN adaptation methods, we propose to combine SAT-DNNs and model-space DNN adaptation during decoding. Finally, to accelerate learning of SAT-DNNs, a simple yet effective strategy, frame skipping, is employed to reduce the size of training data. Our experiments show that compared with a strong DNN baseline, the SAT-DNN model achieves 13.5% and 17.5% relative improvement on word error rates (WERs), without and with model-space adaptation applied respectively. Data reduction based on frame skipping results in 2\texttimes{} speed-up for SAT-DNN training, while causing negligible WER loss on the testing data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1938–1949},
numpages = {12},
keywords = {speaker adaptive training (SAT), acoustic modeling, deep neural networks (DNNs)}
}

@article{10.1109/TASLP.2015.2450491,
author = {Baby, Deepak and Virtanen, Tuomas and Gemmeke, Jort F. and Van hamme, Hugo},
title = {Coupled Dictionaries for Exemplar-Based Speech Enhancement and Automatic Speech Recognition},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2450491},
doi = {10.1109/TASLP.2015.2450491},
abstract = {Exemplar-based speech enhancement systems work by decomposing the noisy speech as a weighted sum of speech and noise exemplars stored in a dictionary and use the resulting speech and noise estimates to obtain a time-varying filter in the full-resolution frequency domain to enhance the noisy speech. To obtain the decomposition, exemplars sampled in lower dimensional spaces are preferred over the full-resolution frequency domain for their reduced computational complexity and the ability to better generalize to unseen cases. But the resulting filter may be sub-optimal as the mapping of the obtained speech and noise estimates to the full-resolution frequency domain yields a low-rank approximation. This paper proposes an efficient way to directly compute the full-resolution frequency estimates of speech and noise using coupled dictionaries: an input dictionary containing atoms from the desired exemplar space to obtain the decomposition and a coupled output dictionary containing exemplars from the full-resolution frequency domain. We also introduce modulation spectrogram features for the exemplar-based tasks using this approach. The proposed system was evaluated for various choices of input exemplars and yielded improved speech enhancement performances on the AURORA-2 and AURORA-4 databases. We further show that the proposed approach also results in improved word error rates (WERs) for the speech recognition tasks using HMM-GMM and deep-neural network (DNN) based systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1788–1799},
numpages = {12},
keywords = {modulation envelope, exemplar-based, non-negative sparse coding, noise robust automatic speech recognition}
}

@article{10.1109/TASLP.2015.2447281,
author = {Sarmiento, Auxiliadora and Dur\'{a}n-D\'{\i}az, Iv\'{a}n and Cichocki, Andrzej and Cruces, Sergio},
title = {A Contrast Function Based on Generalized Divergences for Solving the Permutation Problem in Convolved Speech Mixtures},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2447281},
doi = {10.1109/TASLP.2015.2447281},
abstract = {In this paper, we propose a method for solving the permutation problem that is inherent in the separation of convolved mixtures of speech signals in the time-frequency domain. The proposed method obtains the solution through maximization of a contrast function that exploits the similarity of the temporal envelope of the speech spectrum. For this purpose, the contrast calculation uses a global measure of similarity based on the recently developed family of generalized Alpha-Beta divergences, which depend on two tuning parameters, alpha and beta. This parameterization is exploited to best measure the similarity of the speech spectrum and to obtain solutions that are robust against noise and outliers. The ability of this contrast function to solve the permutation problem is supported by a theoretical study that shows that for a simple time-frequency speech model, the contrast value reaches its maximum when the estimated components are properly aligned. Several performance studies demonstrate that the proposed method maintains a high level of permutation correction accuracy in a wide variety of acoustic environments. Moreover, it produces better results than other state-of-the-art methods for solving permutations in highly reverberant environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1713–1726},
numpages = {14},
keywords = {speech enhancement, blind source separation (BSS), permutation problem}
}

@article{10.1109/TASLP.2015.2458580,
author = {Morfi, Veronica and Degottex, Gilles and Mouchtaris, Athanasios},
title = {Speech Analysis and Synthesis with a Computationally Efficient Adaptive Harmonic Model},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2458580},
doi = {10.1109/TASLP.2015.2458580},
abstract = {Harmonic models have to be both precise and fast in order to represent the speech signal adequately and be able to process large amount of data in a reasonable amount of time. For these purposes, the full-band adaptive harmonic model (aHM) used by the adaptive iterative refinement (AIR) algorithm has been proposed in order to accurately model the perceived characteristics of a speech signal. Even though aHM-AIR is precise, it lacks the computational efficiency that would make its use convenient for large databases. The least squares (LS) solution used in the original aHM-AIR accounts for most of the computational load. In a previous paper, we suggested a peak picking (PP) approach as a substitution to the LS solution. In order to integrate the adaptivity scheme of aHM in the PP approach, an adaptive discrete Fourier transform (aDFT), whose frequency basis can fully follow the variations of the curve, was also proposed. In this paper, we complete the previous publication by evaluating the above methods for the whole analysis process of a speech signal. Evaluations have shown an average time reduction by four times using PP and aDFT compared to the LS solution. Additionally, based on formal listening tests, when using PP and aDFT, the quality of the re-synthesis is preserved compared to the original LS-based approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1950–1962},
numpages = {13},
keywords = {voice model, peak picking (PP), fundamental frequency, speech analysis/synthesis, harmonic model}
}

@article{10.1109/TASLP.2015.2456420,
author = {Moritz, Niko and Anem\"{u}ller, J\"{o}rn and Kollmeier, Birger},
title = {An Auditory Inspired Amplitude Modulation Filter Bank for Robust Feature Extraction in Automatic Speech Recognition},
year = {2015},
issue_date = {November 2015},
publisher = {IEEE Press},
volume = {23},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2456420},
doi = {10.1109/TASLP.2015.2456420},
abstract = {The human ability to classify acoustic sounds is still unmatched compared to recent methods in machine learning. Psychoacoustic and physiological studies indicate that the auditory system of mammals decomposes audio signals into their acoustic and modulation frequency components prior to further analysis. Since it is known that most linguistic information is coded in amplitude fluctuations, mimicking temporal processing strategies of the auditory system in automatic speech recognition (ASR) promises to increase recognition accuracies. We present an amplitude modulation filter bank (AMFB) that is used as a feature extraction scheme in ASR systems. The time-frequency resolution of the employed FIR filters, i.e., bandwidth and modulation frequency settings, are adopted from a psychophysically inspired model of Dau et al. (1997) that was originally proposed to describe data from human psychoacoustics. Investigations on modulation phase indicate the need for preserving such information in amplitude modulation features. We show that the filter symmetry has an important impact on ASR performance. The proposed feature extraction scheme exhibits significant word error rate (WER) reductions using the Aurora-2, Aurora-4, and REVERB ASR tasks compared to other recent feature extraction methods, such as MFCC, FDLP, and PNCC features. Thereby, AMFB features reveal high robustness against additive noise, different transmission channel characteristics, and room reverberation. Using the Aurora-4 benchmark, for instance, an average WER of 12.33% with raw and 11.31% with bottleneck transformed features is attained, which constitutes a relative improvement of 19.6% and 29.2% over raw MFCC features, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1926–1937},
numpages = {12},
keywords = {modulation phase, bottleneck features, feature extraction, amplitude modulation filter bank}
}

@article{10.1109/TASLP.2015.2439573,
author = {Tervo, Sakari and Politis, Archontis},
title = {Direction of Arrival Estimation of Reflections from Room Impulse Responses Using a Spherical Microphone Array},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2439573},
doi = {10.1109/TASLP.2015.2439573},
abstract = {This paper studies the direction of arrival estimation of reflections in short time windows of room impulse responses measured with a spherical microphone array. Spectral-based methods, such as multiple signal classification (MUSIC) and beamforming, are commonly used in the analysis of spatial room impulse responses. However, the room acoustic reflections are highly correlated or even coherent in a single analysis window and this imposes limitations on the use of spectral-based methods. Here, we apply maximum likelihood (ML) methods, which are suitable for direction of arrival estimation of coherent reflections. These methods have been earlier developed in the linear space domain and here we present the ML methods in the context of spherical microphone array processing and room impulse responses. Experiments are conducted with simulated and real data using the em32 Eigenmike. The results show that direction estimation with ML methods is more robust against noise and less biased than MUSIC or beamforming.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1539–1551},
numpages = {13},
keywords = {room impulse response, spatial, room acoustics, direction of arrival (DOA), spherical microphone arrays}
}

@article{10.1109/TASLP.2015.2438535,
author = {Wang, Jia-Ching and Chin, Yu-Hao and Chen, Bo-Wei and Lin, Chang-Hong and Wu, Chung-Hsien},
title = {Speech Emotion Verification Using Emotion Variance Modeling and Discriminant Scale-Frequency Maps},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2438535},
doi = {10.1109/TASLP.2015.2438535},
abstract = {This paper develops an approach to speech-based emotion verification based on emotion variance modeling and discriminant scale-frequency maps. The proposed system consists of two parts---feature extraction and emotion verification. In the first part, for each sound frame, important atoms from the Gabor dictionary are selected by using the matching pursuit algorithm. The scale, frequency, and magnitude of the atoms are extracted to construct a nonuniform scale-frequency map, which supports auditory discriminability by the analysis of critical bands. Next, sparse representation is used to transform scale-frequency maps into sparse coefficients to enhance the robustness against emotion variance and achieve error-tolerance improvement. In the second part, emotion verification, two scores are calculated. A novel sparse representation verification approach based on Gaussian-modeled residual errors is proposed to generate the first score from the sparse coefficients. Such a classifier can minimize emotion variance and improve recognition accuracy. The second score is calculated by using the emotional agreement index (EAI) from the same coefficients. These two scores are combined to obtain the final detection result. Experiments on an emotional database of spoken speech were conducted and indicate that the proposed approach can achieve an average equal error rate (EER) of as low as 6.61%. A comparison among different approaches reveals that the proposed method is superior to the others and confirms its feasibility.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1552–1562},
numpages = {11},
keywords = {gaussian-modeled residual error, scale-frequency map, sparse representation, emotional speech recognition}
}

@article{10.1109/TASLP.2015.2442417,
author = {Hon, Tsz-Kin and Wang, Lin and Reiss, Joshua D. and Cavallaro, Andrea},
title = {Audio Fingerprinting for Multi-Device Self-Localization},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2442417},
doi = {10.1109/TASLP.2015.2442417},
abstract = {We investigate the self-localization problem of an ad-hoc network of randomly distributed and independent devices in an open-space environment with low reverberation but heavy noise (e.g. smartphones recording videos of an outdoor event). Assuming a sufficient number of sound sources, we estimate the distance between a pair of devices from the extreme (minimum and maximum) time difference of arrivals (TDOAs) from the sources to the pair of devices without knowing the time offset. The obtained inter-device distances are then exploited to derive the geometrical configuration of the network. In particular, we propose a robust audio fingerprinting algorithm for noisy recordings and perform landmark matching to construct a histogram of the TDOAs of multiple sources. The extreme TDOAs can be estimated from this histogram. By using audio fingerprinting features, the proposed algorithm works robustly in very noisy environments. Experiments with free-field simulation and open-space recordings prove the effectiveness of the proposed algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1623–1636},
numpages = {14},
keywords = {self-localization, ad-hoc microphone array, multi-source, time difference of arrival (TDOA) estimation, audio fingerprinting}
}

@article{10.1109/TASLP.2015.2442418,
author = {Tian, Ye and Chen, Zhe and Yin, Fuliang},
title = {Distributed Imm-Unscented Kalman Filter for Speaker Tracking in Microphone Array Networks},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2442418},
doi = {10.1109/TASLP.2015.2442418},
abstract = {In this paper, we first propose a distributed unscented Kalman filter (DUKF) to overcome the nonlinearity of measurement model in speaker tracking. Next, for the different motion dynamics of a speaker in the in-door environment, we introduce the interacting multiple model (IMM) algorithm and propose a distributed interacting multiple model-unscented Kalman filter (IMM-UKF) for estimating time-varying speaker's positions in a microphone array network. In the distributed IMM-UKF based speaker tracking method, the time difference of arrival (TDOA) of the speech signals received by a pair of microphones at each node is estimated by the generalized cross-correlation (GCC) method, then the distributed IMM-UKF is used to track a speaker whose position and speed significantly vary over time in a microphone array network. The proposed method can estimate speaker's positions globally in the network and obtain a smoothed trajectory of the speaker's movement robustly in noisy and reverberant environments, and it is scalable for speaker tracking. Simulation and real-world experiment results reveal the effectiveness of the proposed speaker tracking method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1637–1647},
numpages = {11},
keywords = {interacting multiple model, microphone array network, time difference of arrival (TDOA), speaker tracking, distributed unscented kalman filter (DUKF)}
}

@article{10.1109/TASLP.2015.2425955,
author = {Shah, Pratik and Lewis, Ian and Grant, Steven and Angrignon, Sylvain},
title = {Nonlinear Acoustic Echo Cancellation Using Voltage and Current Feedback},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2425955},
doi = {10.1109/TASLP.2015.2425955},
abstract = {Acoustic echo cancellation (AEC) is a well studied problem. The underlying assumption in most echo cancellation solutions is that the echo path following the reference signal is completely linear. However, in many handheld devices, the echo path following the reference signal is nonlinear. The reason for this nonlinearity in the echo path is the use of smaller inexpensive loudspeakers and the desire for generating high sound pressure levels. This brings about the need for a nonlinear echo canceler to maintain the required echo return loss enhancement (ERLE). Many software-based solutions have been proposed to solve this problem, but the computational complexity of these solutions is prohibitively high for practical implementation. This paper analyzes the sources of nonlinearities in smartphones and proposes a simple, elegant hardware modification to significantly reduce nonlinear echo. Thorough analysis and intensive testing results show that up to 6 dB of improvement in ERLE in a real device is possible using the proposed technique.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1589–1599},
numpages = {11},
keywords = {voltage feedback, current feedback, nonlinear echo cancellation}
}

@article{10.1109/TASLP.2015.2442411,
author = {Su, Li and Yang, Yi-Hsuan},
title = {Combining Spectral and Temporal Representations for Multipitch Estimation of Polyphonic Music},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2442411},
doi = {10.1109/TASLP.2015.2442411},
abstract = {Due to the difficulty of creating pitch-labeled training data that cover the rich diversity found in music signals, unsupervised feature-based approaches derived from signal processing and feature design remain critical for multipitch estimation (MPE) of polyphonic music. While a large number of feature representations have been proposed in the literature, an effective means of combining different domains of features for MPE is still needed. In this paper, we propose a novel approach, referred to as combined frequency and periodicity (CFP), that detects pitches according to the agreement of a harmonic series in the frequency domain and a subharmonic series in the lag (quefrency) domain. This approach nicely aggregates the complementary advantages of the two feature domains in different frequency ranges, and improves the robustness of the pitch detection function to the interference of the overtones of simultaneous pitches. We report a comprehensive evaluation that compares CFP against three state-of-the-art approaches using three MPE datasets and four symphonies. The evaluation is characteristic of the coverage and complexity of music (in terms of instrument type and degree of polyphony). In addition, we also evaluate the performance of the MPE approaches when a number of audio degradations are applied. Results show that the proposed unsupervised method performs consistently well across the types of Western polyphonic music considered, and is robust to audio degradations such as high-pass filtering and MP3 compression.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1600–1612},
numpages = {13},
keywords = {unsupervised approach, generalized cepstrum, multipitch estimation, automatic music transcription}
}

@article{10.1109/TASLP.2015.2439577,
author = {He, Jianjun and Gan, Woon-Seng and Tan, Ee-Leng},
title = {Time-Shifting Based Primary-Ambient Extraction for Spatial Audio Reproduction},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2439577},
doi = {10.1109/TASLP.2015.2439577},
abstract = {One of the key issues in spatial audio analysis and reproduction is to decompose a signal into primary and ambient components based on their directional and diffuse spatial features, respectively. Existing approaches employed in primary-ambient extraction (PAE), such as principal component analysis (PCA), are mainly based on a basic stereo signal model. The performance of these PAE approaches has not been well studied for the input signals that do not satisfy all the assumptions of the stereo signal model. In practice, one such case commonly encountered is that the primary components of the stereo signal are partially correlated at zero lag, referred to as the primary-complex case. In this paper, we take PCA as a representative of existing PAE approaches and investigate the performance degradation of PAE with respect to the correlation of the primary components in the primary-complex case. A time-shifting technique is proposed in PAE to alleviate the performance degradation due to the low correlation of the primary components in such stereo signals. This technique involves time-shifting the input signal according to the estimated inter-channel time difference of the primary component prior to the signal decomposition using conventional PAE approaches. To avoid the switching artifacts caused by the varied time-shifting in successive time frames, overlapped output mapping is suggested. Based on the results from our experiments, PAE approaches with the proposed time-shifting technique are found to be superior to the conventional PAE approaches in terms of extraction accuracy and spatial accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1576–1588},
numpages = {13},
keywords = {spatial audio, spatial cues, primary-ambient extraction (PAE), principal component analysis (PCA)}
}

@article{10.1109/TASLP.2015.2442415,
author = {Fujioka, Toyota and Nagata, Yoshifumi and Abe, Masato},
title = {High-Precision Harmonic Distortion Level Measurement of a Loudspeaker Using Adaptive Filters in a Noisy Environment},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2442415},
doi = {10.1109/TASLP.2015.2442415},
abstract = {The harmonic distortion level (HDL), an important criterion used to evaluate loudspeaker performance, can usually be measured using spectral analysis. However, spectral analysis entails great computational complexity. Furthermore, long measurement times are necessary to measure HDL accurately in a noisy environment. Therefore, we proposed the HDL measurement technique for a loudspeaker using an adaptive filter. This measurement technique can measure HDL as accurately as spectral analysis using fast Fourier transform (FFT). Furthermore, its computational complexity is much less than that of spectral analysis. Nevertheless, this measurement technique requires a long measurement time to assess the convergence of filter coefficients. This paper presents a description of the new HDL measurement technique of a loudspeaker using plural adaptive filters. The proposed measurement technique can measure HDL more accurately than spectral analysis in a noisy environment. We evaluated the performance of the proposed measurement technique using computer simulations. Results based on computer simulations show that the proposed measurement technique is more effective than spectral analysis in an acoustic environment with background noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1613–1622},
numpages = {10},
keywords = {harmonic distortion, loudspeaker, adaptive filter, distortion level measurement}
}

@article{10.1109/TASLP.2015.2444664,
author = {Ruhland, Marco and Bitzer, Joerg and Brandt, Matthias and Goetze, Stefan},
title = {Reduction of Gaussian, Supergaussian, and Impulsive Noise by Interpolation of the Binary Mask Residual},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2444664},
doi = {10.1109/TASLP.2015.2444664},
abstract = {In this paper, we present a new approach for noise reduction. A binary time-frequency (T-F) masking threshold criterion is proposed and analyzed with respect to the average spectra of music and noise disturbances. Modified autoregressive (AR) detection and AR interpolation are then applied to the residual signal of the binary masking process. The proposed method is able to reduce supergaussian and impulsive noise while ensuring preservation of the desired signal, which is crucial for professional high-quality audio restoration, and it is also suitable for Gaussian noise to a certain extent. The approach is compared to a state-of-the-art restoration algorithm by means of the objective measures signal-to-noise ratio (SNR) improvement and perceptual quality, and by subjective listening tests. The objective results as well as the listening tests show that the proposed algorithm is especially suited for supergaussian, grainy-sounding noise types, e.g., optical soundtrack noise of celluloid movie footage, or rain noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1680–1691},
numpages = {12},
keywords = {optical soundtrack noise, time-frequency masking, interpolation, noise reduction}
}

@article{10.1109/TASLP.2015.2443977,
author = {Vilkamo, Juha and Delikaris-Manias, Symeon},
title = {Perceptual Reproduction of Spatial Sound Using Loudspeaker-Signal-Domain Parametrization},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2443977},
doi = {10.1109/TASLP.2015.2443977},
abstract = {Adaptive perceptual spatial sound reproduction techniques that employ a parametric model describing the properties of the sound field can reproduce spatial sound with high perceptual accuracy when compared to linear techniques. On the other hand, applying a sound-field model to control the reproduced sound may compromise the perceived quality of individual channels in cases where the model does not match the sound field. An alternative parametrization is proposed that estimates directly the perceptually relevant parameters for the target loudspeaker signals without modeling the sound field. At the synthesis stage, the loudspeaker signals with the target parametric properties are generated from the microphone signals with regularized leastsquares mixing and decorrelation. It is shown through listening experiments that the proposed method provides on average the overall perceived spatial sound reproduction quality of a state-of the-art parametric spatial sound reproduction technique, while solving the past shortcomings related to the perceived quality of the individual channels.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1660–1669},
numpages = {10},
keywords = {spatial audio reproduction, least-squares mixing, sound-field parametrization, microphone array signal processing}
}

@article{10.1109/TASLP.2015.2442757,
author = {Li, Na and Mak, Man-Wai},
title = {SNR-Invariant PLDA Modeling in Nonparametric Subspace for Robust Speaker Verification},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2442757},
doi = {10.1109/TASLP.2015.2442757},
abstract = {While i-vector/PLDA framework has achieved great success, its performance still degrades dramatically under noisy conditions. To compensate for the variability of i-vectors caused by different levels of background noise, this paper proposes an SNR-invariant PLDA framework for robust speaker verification. First, nonparametric feature analysis (NFA) is employed to suppress intra-speaker variation and emphasize the discriminative information inherited in the boundaries between speakers in the i-vector space. Then, in the NFA-projected subspace, SNR-invariant PLDA is applied to separate the SNR-specific information from speaker-specific information using an identity factor and an SNR factor. Accordingly, a projected i-vector in the NFA subspace can be represented as a linear combination of three components: speaker, SNR, and channel. During verification, the variability due to SNR and channels are integrated out when computing the marginal likelihood ratio. Experiments based on NIST 2012 SRE show that the proposed framework achieves superior performance when compared with the conventional PLDA and SNR-dependent mixture of PLDA.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1648–1659},
numpages = {12},
keywords = {probabilistic linear discriminant analysis (PLDA), speaker verification, i-vector, nonparametric feature analysis, SNR-invariant}
}

@article{10.1109/TASLP.2015.2444654,
author = {Dorfan, Yuval and Gannot, Sharon},
title = {Tree-Based Recursive Expectation-Maximization Algorithm for Localization of Acoustic Sources},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2444654},
doi = {10.1109/TASLP.2015.2444654},
abstract = {The problem of distributed localization for ad hoc wireless acoustic sensor networks (WASNs) is addressed in this paper. WASNs are characterized by low computational resources in each node and by limited connectivity between the nodes. Novel bi-directional tree-based distributed estimation--maximization (DEM) algorithms are proposed to circumvent these inherent limitations. We show that the proposed algorithms are capable of localizing static acoustic sources in reverberant enclosures without a priori information on the number of sources. Unlike serial estimation procedures (like ring-based algorithms), the new algorithms enable simultaneous computations in the nodes and exhibit greater robustness to communication failures. Specifically, the recursive distributed EM (RDEM) variant is better suited to online applications due to its recursive nature. Furthermore, the RDEM outperforms the other proposed variants in terms of convergence speed and simplicity. Performance is demonstrated by an extensive experimental study consisting of both simulated and actual environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1692–1703},
numpages = {12},
keywords = {speaker localization, recursive expectation-maximization (REM), wireless acoustic sensor networks (WASNs), bi-directional tree topologies, distributed signal processing}
}

@article{10.1109/TASLP.2015.2439040,
author = {Canclini, A. and Bestagini, P. and Antonacci, F. and Compagnoni, M. and Sarti, A. and Tubaro, S.},
title = {A Robust and Low-Complexity Source Localization Algorithm for Asynchronous Distributed Microphone Networks},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2439040},
doi = {10.1109/TASLP.2015.2439040},
abstract = {In this paper, we propose a robust and low-complexity acoustic source localization technique based on time differences of arrival (TDOA), which addresses the scenario of distributed sensor networks in 3D environments. Network nodes are assumed to be unsynchronized, i.e., TDOAs between microphones belonging to different nodes are not available. We begin with showing how to select feasible TDOAs for each sensor node, exploiting both geometrical considerations and a characterization of the overall generalized cross correlation (GCC) shape. We then show how to localize sources in the space-range reference frame, where TDOA measurements have a clear geometrical interpretation that can be fruitfully used in the scenario of unsynchronized sensors. In this framework, in fact, the source corresponds to the apex of a hypercone passing through points described by the sole microphone positions and TDOA measurements. The localization problem is therefore approached as a hypercone fitting problem. Finally, in order to improve the robustness of the estimate, we include an outlier detection procedure based on the evaluation of the hypercone fitting residuals. A refinement of source location estimate is then performed ignoring the contributions coming from outlier measurements. A set of simulations shows the performance of individual blocks of the system, with particular focus on the effect of TDOA selection on source localization and refinement steps. Experiments on real data validate the localization algorithm in an everyday scenario, proving that good accuracy can be obtained while saving computational cost in comparison with state-of-the-art techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1563–1575},
numpages = {13},
keywords = {space-range reference frame, distributed acosutic sensor networks, acoustic source localization, time difference of arrival (TDOA) selection}
}

@article{10.1109/TASLP.2015.2444659,
author = {Weng, Chao and Yu, Dong and Seltzer, Michael L. and Droppo, Jasha},
title = {Deep Neural Networks for Single-Channel Multi-Talker Speech Recognition},
year = {2015},
issue_date = {Octember 2015},
publisher = {IEEE Press},
volume = {23},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2444659},
doi = {10.1109/TASLP.2015.2444659},
abstract = {We investigate techniques based on deep neural networks (DNNs) for attacking the single-channel multi-talker speech recognition problem. Our proposed approach contains five key ingredients: a multi-style training strategy on artificially mixed speech data, a separate DNN to estimate senone posterior probabilities of the louder and softer speakers at each frame, a weighted finite-state transducer (WFST)-based two-talker decoder to jointly estimate and correlate the speaker and speech, a speaker switching penalty estimated from the energy pattern change in the mixed-speech, and a confidence based system combination strategy. Experiments on the 2006 speech separation and recognition challenge task demonstrate that our proposed DNN-based system has remarkable noise robustness to the interference of a competing speaker. The best setup of our proposed systems achieves an average word error rate (WER) of 18.8% across different SNRs and outperforms the state-of-the-art IBM superhuman system by 2.8% absolute with fewer assumptions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1670–1679},
numpages = {10},
keywords = {joint decoding, deep neural network (DNN), noise robustness, multi-talker automatic speech recognition (ASR), weighted finite-state transducer (WFST), single-channel}
}

@article{10.1109/TASLP.2015.2438543,
author = {Lee, Lin-shan and Glass, James and Lee, Hung-yi and Chan, Chun-an},
title = {Spoken Content Retrieval: Beyond Cascading Speech Recognition with Text Retrieval},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2438543},
doi = {10.1109/TASLP.2015.2438543},
abstract = {Spoken content retrieval refers to directly indexing and retrieving spoken content based on the audio rather than text descriptions. This potentially eliminates the requirement of producing text descriptions for multimedia content for indexing and retrieval purposes, and is able to precisely locate the exact time the desired information appears in the multimedia. Spoken content retrieval has been very successfully achieved with the basic approach of cascading automatic speech recognition (ASR) with text information retrieval: after the spoken content is transcribed into text or lattice format, a text retrieval engine searches over the ASR output to find desired information. This framework works well when the ASR accuracy is relatively high, but becomes less adequate when more challenging real-world scenarios are considered, since retrieval performance depends heavily on ASR accuracy. This challenge leads to the emergence of another approach to spoken content retrieval: to go beyond the basic framework of cascading ASR with text retrieval in order to have retrieval performances that are less dependent on ASR accuracy. This overview article is intended to provide a thorough overview of the concepts, principles, approaches, and achievements of major technical contributions along this line of investigation. This includes five major directions: 1) Modified ASR for Retrieval Purposes: cascading ASR with text retrieval, but the ASR is modified or optimized for spoken content retrieval purposes; 2) Exploiting the Information not present in ASR outputs: to try to utilize the information in speech signals inevitably lost when transcribed into phonemes and words; 3) Directly Matching at the Acoustic Level without ASR: for spoken queries, the signals can be directly matched at the acoustic level, rather than at the phoneme or word levels, bypassing all ASR issues; 4) Semantic Retrieval of Spoken Content: trying to retrieve spoken content that is semantically related to the query, but not necessarily including the query terms themselves; 5) Interactive Retrieval and Efficient Presentation of the Retrieved Objects: with efficient presentation of the retrieved objects, an interactive retrieval process incorporating user actions may produce better retrieval results and user experiences.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1389–1420},
numpages = {32},
keywords = {semantic retrieval, key term extraction, graph-based random walk, unsupervised acoustic pattern discovery, interactive retrieval, spoken content retrieval, summarization, pseudo-relevance feedback, query expansion, joint optimization, spoken term detection, query by example}
}

@article{10.1109/TASLP.2015.2439038,
author = {Mowlaee, Pejman and Kulmer, Josef},
title = {Harmonic Phase Estimation in Single-Channel Speech Enhancement Using Phase Decomposition and SNR Information},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2439038},
doi = {10.1109/TASLP.2015.2439038},
abstract = {In conventional single-channel speech enhancement, typically the noisy spectral amplitude is modified while the noisy phase is used to reconstruct the enhanced signal. Several recent attempts have shown the effectiveness of utilizing an improved spectral phase for phase-aware speech enhancement and consequently its positive impact on the perceived speech quality. In this paper, we present a harmonic phase estimation method relying on fundamental frequency and signal-to-noise ratio (SNR) information estimated from noisy speech. The proposed method relies on SNR-based time-frequency smoothing of the unwrapped phase obtained from the decomposition of the noisy phase. To incorporate the uncertainty in the estimated phase due to unreliable voicing decision and SNR estimate, we propose a binary hypothesis test assuming speech-present and speech-absent classes representing high and low SNRs. The effectiveness of the proposed phase estimation method is evaluated for both phase-only enhancement of noisy speech and in combination with an amplitude-only enhancement scheme. We show that by enhancing the noisy phase both perceived speech quality as well as speech intelligibility are improved as predicted by the instrumental metrics and justified by subjective listening tests.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1521–1532},
numpages = {12},
keywords = {noise reduction, harmonic model, phase decomposition, phase estimation, von Mises distribution}
}

@article{10.1109/TASLP.2015.2434272,
author = {He, Jianjun and Gan, Woon-Seng and Tan, Ee-Leng},
title = {Primary-Ambient Extraction Using Ambient Spectrum Estimation for Immersive Spatial Audio Reproduction},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2434272},
doi = {10.1109/TASLP.2015.2434272},
abstract = {The diversity of today's playback systems requires a flexible, efficient, and immersive reproduction of sound scenes in digital media. Spatial audio reproduction based on primary-ambient extraction (PAE) fulfills this objective, where accurate extraction of primary and ambient components from sound mixtures in channel-based audio is crucial. Severe extraction error was found in existing PAE approaches when dealing with sound mixtures that contain a relatively strong ambient component, a commonly encountered case in the sound scenes of digital media. In this paper, we propose a novel ambient spectrum estimation (ASE) framework to improve the performance of PAE. The ASE framework exploits the equal magnitude of the uncorrelated ambient components in two channels of a stereo signal, and reformulates the PAE problem into the problem of estimating either ambient phase or magnitude. In particular, we take advantage of the sparse characteristic of the primary components to derive sparse solutions for ASE based PAE, together with an approximate solution that can significantly reduce the computational cost. Our objective and subjective experimental results demonstrate that the proposed ASE approaches significantly outperform existing approaches, especially when the ambient component is relatively strong.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1431–1444},
numpages = {14},
keywords = {sparsity, spatial audio, ambient spectrum estimation (ASE), computational efficiency, primary-ambient extraction (PAE)}
}

@article{10.1109/TASLP.2015.2436345,
author = {Chien, Yu-Ren and Wang, Hsin-Min and Jeng, Shyh-Kang},
title = {An Acoustic-Phonetic Model of F0 Likelihood for Vocal Melody Extraction},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2436345},
doi = {10.1109/TASLP.2015.2436345},
abstract = {This paper presents a novel approach to extraction of vocal melodies from accompanied singing recordings. Central to our approach is a model of vocal fundamental frequency (F0) likelihood that integrates acoustic-phonetic knowledge and real-world data. This model consists of a timbral fitness score and a loudness measure of each F0 candidate. Timbral fitness is measured for the partial amplitudes of an F0 candidate, with respect to a small set of vocal timbre examples. This F0-specific measurement of timbral fitness depends on an acoustic-phonetic F0 modification of each timbre example. In the loudness part of the likelihood model, sinusoids are detected, tracked, and pruned to give loudness values that minimize interference from the accompaniment. A final F0 estimate is determined by a prior model of F0 sequence in addition to the likelihood model. Melody extraction is completed by detecting voiced time positions according to the singing voice loudness variations given by the estimated F0 sequence. The numerical parameters involved in our approach were optimized on three development sets from different sources before the system was evaluated on ten test sets separate from these development sets. Controlled experiments show that use of the timbral fitness score accounts for a 13% difference in overall accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1457–1468},
numpages = {12},
keywords = {acoustic phonetics, singing voice, F0 modification, melody extraction, vocal timbre examples, F0 likelihood, F0 estimation}
}

@article{10.1109/TASLP.2015.2438547,
author = {De Sena, Enzo and Hacιhabibo\u{g}lu, H\"{u}seyin and Cvetkovi\'{c}, Zoran and Smith, Julius O.},
title = {Efficient Synthesis of Room Acoustics via Scattering Delay Networks},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2438547},
doi = {10.1109/TASLP.2015.2438547},
abstract = {An acoustic reverberator consisting of a network of delay lines connected via scattering junctions is proposed. All parameters of the reverberator are derived from physical properties of the enclosure it simulates. It allows for simulation of unequal and frequency-dependent wall absorption, as well as directional sources and microphones. The reverberator renders the first-order reflections exactly, while making progressively coarser approximations of higher-order reflections. The rate of energy decay is close to that obtained with the image method (IM) and consistent with the predictions of Sabine and Eyring equations. The time evolution of the normalized echo density, which was previously shown to be correlated with the perceived texture of reverberation, is also close to that of the IM. However, its computational complexity is one to two orders of magnitude lower, comparable to the computational complexity of a feedback delay network and its memory requirements are negligible.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1478–1492},
numpages = {15},
keywords = {room acoustics, echo density, acoustic simulation, reverberation time, digital waveguide network}
}

@article{10.1109/TASLP.2015.2438542,
author = {Wang, Lin and Gerkmann, Timo and DoclO, Simon},
title = {Noise Power Spectral Density Estimation Using MaxNSR Blocking Matrix},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2438542},
doi = {10.1109/TASLP.2015.2438542},
abstract = {In this paper, a multi-microphone noise reduction system based on the generalized sidelobe canceller (GSC) structure is investigated. The system consists of a fixed beamformer providing an enhanced speech reference, a blocking matrix providing a noise reference by suppressing the target speech, and a single-channel spectral post-filter. The spectral post-filter requires the power spectral density (PSD) of the residual noise in the speech reference, which can in principle be estimated from the PSD of the noise reference. However, due to speech leakage in the noise reference, the noise PSD is overestimated, leading to target speech distortion. To minimize the influence of the speech leakage, a maximum noise-to-speech ratio (MaxNSR) blocking matrix is proposed, which maximizes the ratio between the noise and the speech leakage in the noise reference. The proposed blocking matrix can be computed from the generalized eigenvalue decomposition of the correlation matrix of the microphone signals and the noise coherence matrix, which is assumed to be time-invariant. Experimental results in both stationary and nonstationary diffuse noise fields show that the proposed algorithm outperforms existing blocking matrices in terms of target speech blocking ability, noise estimation and noise reduction performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1493–1508},
numpages = {16},
keywords = {blocking matrix, diffuse noise, speech enhancement, noise power spectral density (PSD) estimation, microphone array}
}

@article{10.1109/TASLP.2015.2434213,
author = {Jiao, Yishan and Berisha, Visar and Tu, Ming and Liss, Julie},
title = {Convex Weighting Criteria for Speaking Rate Estimation},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2434213},
doi = {10.1109/TASLP.2015.2434213},
abstract = {Speaking rate estimation directly from the speech waveform is a long-standing problem in speech signal processing. In this paper, we pose the speaking rate estimation problem as that of estimating a temporal density function whose integral over a given interval yields the speaking rate within that interval. In contrast to many existing methods, we avoid the more difficult task of detecting individual phonemes within the speech signal and we avoid heuristics such as thresholding the temporal envelope to estimate the number of vowels. Rather, the proposed method aims to learn an optimal weighting function that can be directly applied to time-frequency features in a speech signal to yield a temporal density function. We propose two convex cost functions for learning the weighting functions and an adaptation strategy to customize the approach to a particular speaker using minimal training. The algorithms are evaluated on the TIMIT corpus, on a dysarthric speech corpus, and on the ICSI Switchboard spontaneous speech corpus. Results show that the proposed methods outperform three competing methods on both healthy and dysarthric speech. In addition, for spontaneous speech rate estimation, the result show a high correlation between the estimated speaking rate and ground truth values.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1421–1430},
numpages = {10},
keywords = {speaker adaptation, vowel density function, dysarthria, convex optimization, speaking rate estimation}
}

@article{10.1109/TASLP.2015.2438549,
author = {Juki\'{c}, Ante and van Waterschoot, Toon and Gerkmann, Timo and Doclo, Simon},
title = {Multi-Channel Linear Prediction-Based Speech Dereverberation with Sparse Priors},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2438549},
doi = {10.1109/TASLP.2015.2438549},
abstract = {The quality of speech signals recorded in an enclosure can be severely degraded by room reverberation. In this paper, we focus on a class of blind batch methods for speech dereverberation in a noiseless scenario with a single source, which are based on multi-channel linear prediction in the short-time Fourier transform domain. Dereverberation is performed by maximum-likelihood estimation of the model parameters that are subsequently used to recover the desired speech signal. Contrary to the conventional method, we propose to model the desired speech signal using a general sparse prior that can be represented in a convex form as a maximization over scaled complex Gaussian distributions. The proposed model can be interpreted as a generalization of the commonly used time-varying Gaussian model. Furthermore, we reformulate both the conventional and the proposed method as an optimization problem with an ℓp-norm cost function, emphasizing the role of sparsity in the considered speech dereverberation methods. Experimental evaluation in different acoustic scenarios show that the proposed approach results in an improved performance compared to the conventional approach in terms of instrumental measures for speech quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1509–1520},
numpages = {12},
keywords = {speech dereverberation, speech enhancement, multi-channel linear prediction, sparse priors}
}

@article{10.1109/TASLP.2015.2438544,
author = {Cui, Xiaodong and Goel, Vaibhava and Kingsbury, Brian},
title = {Data Augmentation for Deep Neural Network Acoustic Modeling},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2438544},
doi = {10.1109/TASLP.2015.2438544},
abstract = {This paper investigates data augmentation for deep neural network acoustic modeling based on label-preserving transformations to deal with data sparsity. Two data augmentation approaches, vocal tract length perturbation (VTLP) and stochastic feature mapping (SFM), are investigated for both deep neural networks (DNNs) and convolutional neural networks (CNNs). The approaches are focused on increasing speaker and speech variations of the limited training data such that the acoustic models trained with the augmented data are more robust to such variations. In addition, a two-stage data augmentation scheme based on a stacked architecture is proposed to combine VTLP and SFM as complementary approaches. Experiments are conducted on Assamese and Haitian Creole, two development languages of the IARPA Babel program, and improved performance on automatic speech recognition (ASR) and keyword search (KWS) is reported.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1469–1477},
numpages = {9},
keywords = {automatic speech recognition, data augmentation, stochastic feature mapping, deep neural networks, keyword search}
}

@article{10.1109/TASLP.2015.2436214,
author = {Shen, Qing and Liu, Wei and Cui, Wei and Wu, Siliang and Zhang, Yimin D. and Amin, Moeness G.},
title = {Low-Complexity Direction-of-Arrival Estimation Based on Wideband Co-Prime Arrays},
year = {2015},
issue_date = {September 2015},
publisher = {IEEE Press},
volume = {23},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2436214},
doi = {10.1109/TASLP.2015.2436214},
abstract = {A class of low-complexity compressive sensing-based direction-of-arrival (DOA) estimation methods for wideband co-prime arrays is proposed. It is based on a recently proposed narrowband estimation method, where a virtual array model is generated by directly vectorizing the covariance matrix and then using a sparse signal recovery method to obtain the estimation result. As there are a large number of redundant entries in both the auto-correlation and cross-correlation matrices of the two sub-arrays, they can be combined together to form a model with a significantly reduced dimension, thereby leading to a solution with much lower computational complexity without sacrificing performance. A further reduction in complexity is achieved by removing noise power estimation from the formulation. Then, the two proposed low-complexity methods are extended to the wideband realm utilizing a group sparsity based signal reconstruction method. A particular advantage of group sparsity is that it allows a much larger unit inter-element spacing than the standard co-prime array and therefore leads to further improved performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1445–1453},
numpages = {9},
keywords = {co-prime, direction-of-arrival (DOA) estimation, wideband, microphone arrays, sparsity}
}

@article{10.1109/TASLP.2015.2432578,
author = {Chen, Kuan-Yu and Liu, Shih-Hung and Chen, Berlin and Wang, Hsin-Min and Jan, Ea-Ee and Hsu, Wen-Lian and Chen, Hsin-Hsi},
title = {Extractive Broadcast News Summarization Leveraging Recurrent Neural Network Language Modeling Techniques},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2432578},
doi = {10.1109/TASLP.2015.2432578},
abstract = {Extractive text or speech summarization manages to select a set of salient sentences from an original document and concatenate them to form a summary, enabling users to better browse through and understand the content of the document. A recent stream of research on extractive summarization is to employ the language modeling (LM) approach for important sentence selection, which has proven to be effective for performing speech summarization in an unsupervised fashion. However, one of the major challenges facing the LM approach is how to formulate the sentence models and accurately estimate their parameters for each sentence in the document to be summarized. In view of this, our work in this paper explores a novel use of recurrent neural network language modeling (RNNLM) framework for extractive broadcast news summarization. On top of such a framework, the deduced sentence models are able to render not only word usage cues but also long-span structural information of word co-occurrence relationships within broadcast news documents, getting around the need for the strict bag-of-words assumption. Furthermore, different model complexities and combinations are extensively analyzed and compared. Experimental results demonstrate the performance merits of our summarization methods when compared to several well-studied state-of-the-art unsupervised methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1322–1334},
numpages = {13},
keywords = {language modeling, long-span structural information, recurrent neural network, speech summarization}
}

@article{10.1109/TASLP.2015.2427522,
author = {Momeni, Hajar and Abutalebi, Hamid Reza and Tadaion, Aliakbar},
title = {Joint Detection and Estimation of Speech Spectral Amplitude Using Noncontinuous Gain Functions},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2427522},
doi = {10.1109/TASLP.2015.2427522},
abstract = {This paper addresses the joint detection and estimation approach for single-channel speech enhancement. In this approach, a detector decides on speech presence in each time-frequency unit and an estimator estimates the corresponding speech spectral amplitude. We utilize the concept of binary/continuous gain functions to study and extend the process of joint detection and estimation. The binary gains (BGs) have already shown an inferior performance compared to the continuous gains (CGs). In this paper, we propose a simultaneous detection and estimation (SDE) method where the detector structure is derived by the knowledge of the estimator. The proposed SDE method is a combination of Bayesian and Neyman-Pearson approaches and is expressed as a noncontinuous gain (NCG). It is expected that employing a superior detector, the proposed NCG improves the quality of the output speech. We concentrate on the derivation of the detector so that it minimizes the error caused by missed detection and/or wrong estimation of speech coefficients at a controlled level of falsely detecting high-energy noise as speech. Furthermore, an independent detection and estimation technique is proposed where the detector and the estimator are extracted in an independent manner. Simulation results demonstrate that the proposed SDE method minimizes the speech distortion at a controlled level of noise reduction. It is also shown that the performance of the proposed NCG is better than the CG and than the existing BGs in both noise reduction and speech distortion aspects.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1249–1258},
numpages = {10},
keywords = {joint detection and estimation, speech enhancement, spectral amplitude estimation, speech detection}
}

@article{10.1109/TASLP.2015.2430818,
author = {Fallahpour, Mehdi and Meg\'{\i}as, David},
title = {Audio Watermarking Based on Fibonacci Numbers},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2430818},
doi = {10.1109/TASLP.2015.2430818},
abstract = {This paper presents a novel high-capacity audio watermarking system to embed data and extract them in a bit-exact manner by changing some of the magnitudes of the FFT spectrum. The key idea is to divide the FFT spectrum into short frames and change the magnitude of the selected FFT samples using Fibonacci numbers. Taking advantage of Fibonacci numbers, it is possible to change the frequency samples adaptively. In fact, the suggested technique guarantees and proves, mathematically, that the maximum change is less than 61% of the related FFT sample and the average error for each sample is 25%. Using the closest Fibonacci number to FFT magnitudes results in a robust and transparent technique. On top of very remarkable capacity, transparency and robustness, this scheme provides two parameters which facilitate the regulation of these properties. The experimental results show that the method has a high capacity (700 bps to 3 kbps), without significant perceptual distortion (ODG is about 1) and provides robustness against common audio signal processing such as echo, added noise, filtering, and MPEG compression (MP3). In addition to the experimental results, the fidelity of suggested system is proved mathematically.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1273–1282},
numpages = {10},
keywords = {Fibonacci numbers, multimedia security, audio watermarking, golden ratio}
}

@article{10.1109/TASLP.2015.2428632,
author = {Chien, Jen-Tzung},
title = {Hierarchical Pitman-Yor-Dirichlet Language Model},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2428632},
doi = {10.1109/TASLP.2015.2428632},
abstract = {Probabilistic models are often viewed as insufficiently expressive because of strong limitation and assumption on the probabilistic distribution and the fixed model complexity. Bayesian nonparametric learning pursues an expressive probabilistic representation based on the nonparametric prior and posterior distributions with less assumption-laden approach to inference. This paper presents a hierarchical Pitman-Yor-Dirichlet (HPYD) process as the nonparametric priors to infer the predictive probabilities of the smoothed n-grams with the integrated topic information. A metaphor of hierarchical Chinese restaurant process is proposed to infer the HPYD language model (HPYD-LM) via Gibbs sampling. This process is equivalent to implement the hierarchical Dirichlet process-latent Dirichlet allocation (HDP-LDA) with the twisted hierarchical Pitman-Yor LM (HPY-LM) as base measures. Accordingly, we produce the power-law distributions and extract the semantic topics to reflect the properties of natural language in the estimated HPYD-LM. The superiority of HPYD-LM to HPY-LM and other language models is demonstrated by the experiments on model perplexity and speech recognition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1259–1272},
numpages = {14},
keywords = {unsupervised learning, language model, topic model, Bayesian nonparametrics, speech recognition}
}

@article{10.1109/TASLP.2015.2431854,
author = {Morchid, Mohamed and Bouallegue, Mohamed and Dufour, Richard and Linar\`{e}s, Georges and Matrouf, Driss and De Mori, Renato},
title = {Compact Multiview Representation of Documents Based on the Total Variability Space},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2431854},
doi = {10.1109/TASLP.2015.2431854},
abstract = {Mapping text documents in an LDA-based topic-space is a classical way to extract high-level representation of text documents. Unfortunately, LDA is highly sensitive to hyper-parameters related to the number of classes, or word and topic distribution, and there is no systematic way to pre-estimate optimal configurations. Moreover, various hyper-parameter configurations offer complementary views on the document. In this paper, we propose a method based on a two-step process that, first, expands the representation space by using a set of topic spaces and, second, compacts the representation space by removing poorly relevant dimensions. These two steps are based respectively on multi-view LDA-based representation spaces and factor-analysis models. This model provides a view-independent representation of documents while extracting complementary information from a massive multi-view representation. Experiments are conducted on the DECODA conversation corpus and the Reuters-21578 textual dataset. Results show the efficiency of the proposed multiview compact representation paradigm. The proposed categorization system reaches an accuracy of 86.5% with automatic transcriptions of conversations from DECODA corpus and a Macro-F1 of 80% during a classification task of the well-known Reuters-21578 corpus, with a significant gain compared to the baseline (best single topic space configuration), as well as methods and document representations previously studied.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1295–1308},
numpages = {14},
keywords = {classification, latent Dirichlet allocation, C-vector, factor analysis}
}

@article{10.5555/2876444.2876454,
author = {Wang, Xun and Yoshida, Yasuhisa and Hirao, Tsutomu and Sudoh, Katsuhito and Nagata, Masaaki},
title = {Summarization Based on Task-Oriented Discourse Parsing},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
abstract = {Previous research demonstrates that discourse relations can help generate high-quality summaries. Existing studies usually adopt existing discourse parsers directly with no modifications, hence cannot take full advantage of discourse parsing. This paper describes a new single document summarization system. In contrast to previous work, we train a discourse parser specially for summarization by using summaries. The training data are dynamically changed during the training phase to enable the parser to grab the text units that are important for summaries. A special tree-based summary extraction algorithm is designed to work with the new parser. The proposed system enables us to combine discourse parsing and summarization in a unified scheme. Experiments on both the RST-DT and DUC2001 datasets show the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1358–1367},
numpages = {10},
keywords = {discourse parsing, summarization, discourse relations}
}

@article{10.1109/TASLP.2015.2434212,
author = {Spa, Carlos and Rey, Ant\'{o}n and Hern\'{a}ndez, Erwin},
title = {A GPU Implementation of an Explicit Compact FDTD Algorithm with a Digital Impedance Filter for Room Acoustics Applications},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2434212},
doi = {10.1109/TASLP.2015.2434212},
abstract = {In recent years, computational engineering has undergone great changes due to the development of the graphics processing unit (GPU) technology. For example, in room acoustics, the wave-based methods, that formerly were considered too expensive for 3-D impulse response simulations, are now chosen to exploit the parallel nature of GPU devices considerably reducing the execution time of the simulations. There exist contributions related to this topic that have explored the performance of different GPU algorithms; however, the computational analysis of a general explicit model that incorporates algorithms with different neighboring orders and a general frequency dependent impedance boundary model has not been properly developed. In this paper, we present a GPU implementation of a complete room acoustic model based on a family of explicit finite-difference time-domain (FDTD) algorithms. We first develop a strategy for implementing a frequency independent (FI) impedance model which is free from thread divergences and then, we extend the model adding a digital impedance filter (DIF) boundary subroutine able to compute the acoustic pressure of different nodes such as corners or edges without an additional performance penalty. Both implementations are validated and deeply analyzed by performing different 3-D numerical experiments. Finally, we define a performance metric which is able to objectively measure the computing throughput of a FDTD implementation using a simple number. The robustness of this metric allows us to compare algorithms even if these have been run in different GPU cards or have been formulated with other explicit models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1368–1380},
numpages = {13},
keywords = {graphics processing unit (GPU), room acoustics, digital impedance filter, explicit compact finite-difference time-domain (FDTD)}
}

@article{10.1109/TASLP.2015.2430815,
author = {Dimitriadis, Dimitrios and Bocchieri, Enrico},
title = {Use of Micro-Modulation Features in Large Vocabulary Continuous Speech Recognition Tasks},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2430815},
doi = {10.1109/TASLP.2015.2430815},
abstract = {Most of the state-of-the-art ASR systems take as input a single type of acoustic features, dominated by the traditional feature schemes, i.e., MFCCs or PLPs. However, these features cannot model rapid, intra-frame phenomena present in the actual speech signals. On the other hand, micro-modulation components, inspired by the AM-FM speech model, can capture these important characteristics of spoken speech, resulting in significant performance improvements, as previously shown in small-vocabulary ASR tasks. Yet, they have limited use in large vocabulary ASR applications, where feature post-processing schemes are usually employed. To enable the successful application of these frequency measures in real-life tasks, we investigate their combination with the traditional Cepstral features when employing linear, e.g., HDA, and nonlinear, i.e., bottleneck neural net (BN), feature transforms. This feature combination is investigated in the context of the hybrid DNN-HMM framework, as well. The experimental results reveal that the integration of micro-modulation and Cepstral features, using neural nets, can greatly improve the ASR performance with respect to using the Cepstral features alone. We apply this novel feature extraction approach on different tasks, i.e., a clean speech task (DARPA-WSJ), the Aurora- 4 task and a real-life, open-vocabulary, mobile search task, the Speak4it, always reporting improved performance, while the obtained relative word error reduction ranges between 7%-21% depending on the task, e.g., a relative WER improvement of 18% for the Speak4it task, and similar improvements, up to 21%, for the WSJ task are reported.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1348–1357},
numpages = {10},
keywords = {robustness, neural networks, speech recognition, feature extraction, speech processing}
}

@article{10.1109/TASLP.2015.2430820,
author = {Mowlaee, Pejman and Kulmer, Josef},
title = {Phase Estimation in Single-Channel Speech Enhancement: Limits-Potential},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2430820},
doi = {10.1109/TASLP.2015.2430820},
abstract = {In this paper, we present an overview on the previous and recent methods proposed to estimate a clean spectral phase from a noisy observation in the context of single-channel speech enhancement. The importance of phase estimation in speech enhancement is inspired by the recent reports on its usefulness in finding a phase-sensitive amplitude estimation. We present a comparative study of the recent phase estimation methods and elaborate their limits. We propose a new phase enhancement method relying on phase decomposition and time-frequency smoothing filters. We demonstrate that the proposed time-frequency phase smoothing method successfully reduces the variance of the noisy phase at harmonics. Our results on different speech and noise databases and different signal-to-noise ratios show that in contrast to the existing benchmark methods only the proposed method balances a tradeoff between a joint improvement in perceived quality of 0.2 in PESQ score and speech intelligibility of 2% by phase-only enhancement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1283–1294},
numpages = {12},
keywords = {phase estimation, speech intelligibility, perceived quality, speech enhancement, signal reconstruction}
}

@article{10.1109/TASLP.2015.2431851,
author = {Sugiura, Ryosuke and Kamamoto, Yutaka and Harada, Noboru and Kameoka, Hirokazu and Moriya, Takehiro},
title = {Optimal Coding of Generalized-Gaussian-Distributed Frequency Spectra for Low-Delay Audio Coder with Powered All-Pole Spectrum Estimation},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2431851},
doi = {10.1109/TASLP.2015.2431851},
abstract = {We present an optimal coding scheme that parameterizes the maximum-likelihood estimate of variance for frequency spectra belonging to the generalized Gaussian distribution, the distribution covering the Laplacian and the Gaussian. By slightly modifying the all-pole model of the conventional linear prediction (LP), we can estimate the variance with the same method as in LP, which has low computational costs. Experimental results show that incorporating the coding scheme in a state-of-the-art wide-band audio coder enhances its objective and subjective quality in a low-bit-rate and low-delay situation by increasing the compression efficiency. Thus, this coding scheme will be useful in applications like mobile communications, which requires highly efficient compression.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1309–1321},
numpages = {13},
keywords = {linear prediction, generalized Gaussian distribution, audio compression, arithmetic coding, transform coded excitation, low delay}
}

@article{10.1109/TASLP.2015.2425213,
author = {Koldovsk\'{y}, Zbyn\v{e}k and M\'{a}lek, Ji\v{r}\'{\i} and Gannot, Sharon},
title = {Spatial Source Subtraction Based on Incomplete Measurements of Relative Transfer Function},
year = {2015},
issue_date = {August 2015},
publisher = {IEEE Press},
volume = {23},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2425213},
doi = {10.1109/TASLP.2015.2425213},
abstract = {Relative impulse responses between microphones are usually long and dense due to the reverberant acoustic environment. Estimating them from short and noisy recordings poses a long-standing challenge of audio signal processing. In this paper, we apply a novel strategy based on ideas of compressed sensing. Relative transfer function (RTF) corresponding to the relative impulse response can often be estimated accurately from noisy data but only for certain frequencies. This means that often only an incomplete measurement of the RTF is available. A complete RTF estimate can be obtained through finding its sparsest representation in the time-domain: that is, through computing the sparsest among the corresponding relative impulse responses. Based on this approach, we propose to estimate the RTF from noisy data in three steps. First, the RTF is estimated using any conventional method such as the nonstationarity-based estimator by Gannot et al. or through blind source separation. Second, frequencies are determined for which the RTF estimate appears to be accurate. Third, the RTF is reconstructed through solving a weighted l1 convex program, which we propose to solve via a computationally efficient variant of the SpaRSA (Sparse Reconstruction by Separable Approximation) algorithm. An extensive experimental study with real-world recordings has been conducted. It has been shown that the proposed method is capable of improving many conventional estimators used as the first step in most situations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1335–1347},
numpages = {13},
keywords = {l1 norm, relative impulse response, relative transfer function (RTF), sparse approximations, compressed sensing}
}

@article{10.1109/TASLP.2015.2422576,
author = {Meyer, Thomas and Hajlaoui, Najeh and Popescu-Belis, Andrei},
title = {Disambiguating Discourse Connectives for Statistical Machine Translation},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2422576},
doi = {10.1109/TASLP.2015.2422576},
abstract = {This paper shows that the automatic labeling of discourse connectives with the relations they signal, prior to machine translation (MT), can be used by phrase-based statistical MT systems to improve their translations. This improvement is demonstrated here when translating from English to four target languages-French, German, Italian and Arabic-using several test sets from recent MT evaluation campaigns. Using automatically labeled data for training, tuning and testing MT systems is beneficial on condition that labels are sufficiently accurate, typically above 70%. To reach such an accuracy, a large array of features for discourse connective labeling (morpho-syntactic, semantic and discursive) are extracted using state-of-the-art tools and exploited in factored MT models. The translation of connectives is improved significantly, between 0.7% and 10% as measured with the dedicated ACT metric. The improvements depend mainly on the level of ambiguity of the connectives in the test sets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1184–1197},
numpages = {14},
keywords = {discourse connectives, machine translation (MT)}
}

@article{10.1109/TASLP.2015.2419976,
author = {McVicar, Matt and Fukayama, Satoru and Goto, Masataka},
title = {AutoGuitarTab: Computer-Aided Composition of Rhythm and Lead Guitar Parts in the Tablature Space},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2419976},
doi = {10.1109/TASLP.2015.2419976},
abstract = {We present AutoGuitarTab, a system for generating realistic guitar tablature given an input symbolic chord and key sequence. Our system consists of two modules: AutoRhythm-Guitar and AutoLeadGuitar. The first of these generates rhythm guitar tablatures which outline the input chord sequence in a particular style (using Markov chains to ensure playability) and performs a structural analysis to produce a structurally consistent composition. AutoLeadGuitar generates lead guitar parts in distinct musical phrases, guiding the pitch classes towards chord tones and steering the evolution of the rhythmic and melodic intensity according to user preference. Experimentally, we uncover musician-specific trends in guitar playing style, and demonstrate our system's ability to produce playable, realistic and style-specific tablature using a combination of algorithmic, user-surveyed and expert evaluation techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1105–1117},
numpages = {13},
keywords = {symbolic music processing, computer-aided composition (CAC), algorithmic composition}
}

@article{10.1109/TASLP.2015.2425214,
author = {Yeh, Ching-Feng and Lee, Lin-Shan},
title = {An Improved Framework for Recognizing Highly Imbalanced Bilingual Code-Switched Lectures with Cross-Language Acoustic Modeling and Frame-Level Language Identification},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2425214},
doi = {10.1109/TASLP.2015.2425214},
abstract = {This paper considers the recognition of a widely observed type of bilingual code-switched speech: the speaker speaks primarily the host language (usually his native language), but with a few words or phrases in the guest language (usually his second language) inserted in many utterances of the host language. In this case, not only the languages are switched back and forth within an utterance so the language identification is difficult, but much less data are available for the guest language, which results in poor recognition accuracy for the guest language part. Unit merging approaches on three levels of acoustic modeling (triphone models, HMM states and Gaussians) have been proposed for cross-lingual data sharing for such highly imbalanced bilingual code-switched speech. In this paper, we present an improved overall framework on top of the previously proposed unit merging approaches for recognizing such code-switched speech. This includes unit recovery for reconstructing the identity for units of the two languages after being merged, unit occupancy ranking to offer much more flexible data sharing between units both across languages and within the language based on the accumulated occupancy of the HMM states, and estimation of frame-level language posteriors using blurred posteriorgram features (BPFs) to be used in decoding. We also present a complete set of experimental results comparing all approaches involved for a real-world application scenario under unified conditions, and show very good improvement achieved with the proposed approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1144–1159},
numpages = {16},
keywords = {speech recognition, language identification, cross-language acoustic modeling, code-switching, bilingual}
}

@article{10.1109/TASLP.2015.2425220,
author = {Wang, Rui and Zhao, Hai and Lu, Bao-Liang and Utiyama, Masao and Sumita, Eiichiro},
title = {Bilingual Continuous-Space Language Model Growing for Statistical Machine Translation},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2425220},
doi = {10.1109/TASLP.2015.2425220},
abstract = {Larger n-gram language models (LMs) perform better in statistical machine translation (SMT). However, the existing approaches have two main drawbacks for constructing larger LMs: 1) it is not convenient to obtain larger corpora in the same domain as the bilingual parallel corpora in SMT; 2) most of the previous studies focus on monolingual information from the target corpora only, and redundant n-grams have not been fully utilized in SMT. Nowadays, continuous-space language model (CSLM), especially neural network language model (NNLM), has been shown great improvement in the estimation accuracies of the probabilities for predicting the target words. However, most of these CSLM and NNLM approaches still consider monolingual information only or require additional corpus. In this paper, we propose a novel neural network based bilingual LM growing method. Compared to the existing approaches, the proposed method enables us to use bilingual parallel corpus for LM growing in SMT. The results show that our new method outperforms the existing approaches on both SMT performance and computational efficiency significantly.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1209–1220},
numpages = {12},
keywords = {statistical machine translation (SMT), language model growing (LMG), continuous-space language model, neural network language model}
}

@article{10.5555/2876428.2876438,
author = {Chong, Tze Yuang and Banchs, Rafael E. and Chng, Eng Siong and Li, Haizhou},
title = {Decoupling Word-Pair Distance and Co-Occurrence Information for Effective Long History Context Language Modeling},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
abstract = {In this paper, we propose the use of distance and co-occurrence information of word-pairs to improve language modeling. We have empirically shown that, for history-context sizes of up to ten words, the extracted information about distance and co-occurrence complements the n-gram language model well, for which learning long-history contexts is inherently difficult. Evaluated on the Wall Street Journal and the Switchboard corpora, our proposed model reduces the trigram model perplexity by up to 11.2% and 6.5%, respectively. As compared to the distant bigram model and the trigger model, our proposed model offers a more effective manner of capturing far context information, as verified in terms of perplexity and computational efficiency, i.e., fewer free parameters to be fine-tuned. Experiments using the proposed model for speech recognition, text classification and word prediction tasks showed improved performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1221–1232},
numpages = {12},
keywords = {language modeling, speech recognition, text categorization}
}

@article{10.5555/2876428.2876436,
author = {Remes, Ulpu and L\'{o}pez, Ana Ram\'{\i}rez and Palom\"{a}ki, Kalle and Kurimo, Mikko},
title = {Bounded Conditional Mean Imputation with Observation Uncertainties and Acoustic Model Adaptation},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
abstract = {Automatic speech recognition systems use noise compensation and acoustic model adaptation to increase robustness towards speaker and environmental variation. The current work focuses on noise compensation with bounded conditional mean imputation (BCMI). BCMI approaches are missing-data methods which operate on the assumption that noise-corrupted observations can be divided into reliable and unreliable components. BCMI methods substitute the unreliable components with a clean speech posterior distribution. The posterior means can be used as clean speech estimates and the posterior variances can be introduced in acoustic model likelihood calculation as observation uncertainties. In addition, we propose in the current work that similar uncertainties are introduced in acoustic model adaptation. Evaluation with speech data recorded in diverse public and car environments indicates that the proposed uncertainties improve adaptation performance. When uncertainties were used in acoustic model likelihood calculation and adaptation, the proposed imputation and adaptation system introduced 15%-84% relative error reductions to an uncompensated baseline system performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1198–1208},
numpages = {11},
keywords = {noiserobust speech recognition, acoustic model adaptation, observation uncertainties, missing data}
}

@article{10.1109/TASLP.2015.2425219,
author = {Marelli, Dami\'{a}n and Baumgartner, Robert and Majdak, Piotr},
title = {Efficient Approximation of Head-Related Transfer Functions in Subbands for Accurate Sound Localization},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2425219},
doi = {10.1109/TASLP.2015.2425219},
abstract = {Head-related transfer functions (HRTFs) describe the acoustic filtering of incoming sounds by the human morphology and are essential for listeners to localize sound sources in virtual auditory displays. Since rendering complex virtual scenes is computationally demanding, we propose four algorithms for efficiently representing HRTFs in subbands, i.e., as an analysis filterbank (FB) followed by a transfer matrix and a synthesis FB. All four algorithms use sparse approximation procedures to minimize the computational complexity while maintaining perceptually relevant HRTF properties. The first two algorithms separately optimize the complexity of the transfer matrix associated to each HRTF for fixed FBs. The other two algorithms jointly optimize the FBs and transfer matrices for complete HRTF sets by two variants. The first variant aims at minimizing the complexity of the transfer matrices, while the second one does it for the FBs. Numerical experiments investigate the latency-complexity trade-off and show that the proposed methods offer significant computational savings when compared with other available approaches. Psychoacoustic localization experiments were modeled and conducted to find a reasonable approximation tolerance so that no significant localization performance degradation was introduced by the subband representation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1130–1143},
numpages = {14},
keywords = {sparse approximation, virtual acoustics, head-related transfer functions (HRTFs), sound localization, subband signal processing}
}

@article{10.1109/TASLP.2015.2427520,
author = {Sun, Meng and Li, Yinan and Gemmeke, Jort F. and Zhang, Xiongwei},
title = {Speech Enhancement under Low SNR Conditions via Noise Estimation Using Sparse and Low-Rank NMF with Kullback–Leibler Divergence},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2427520},
doi = {10.1109/TASLP.2015.2427520},
abstract = {A key stage in speech enhancement is noise estimation which usually requires prior models for speech or noise or both. However, prior models can sometimes be difficult to obtain. In this paper, without any prior knowledge of speech and noise, sparse and low-rank nonnegative matrix factorization (NMF) with Kullback-Leibler divergence is proposed to noise and speech estimation by decomposing the input noisy magnitude spectrogram into a low-rank noise part and a sparse speech-like part. This initial unsupervised speech-noise estimation allows us to set a subsequent regularized version of NMF or convolutional NMF to reconstruct the noise and speech spectrogram, either by estimating a speech dictionary on the fly (categorized as unsupervised approaches) or by using a pre-trained speech dictionary on utterances with disjoint speakers (categorized as semi-supervised approaches). Information fusion was investigated by taking the geometric mean of the outputs from multiple enhancement algorithms. The performance of the algorithms were evaluated on five metrics (PESQ, SDR, SNR, STOI, and OVERALL) by making experiments on TIMIT with 15 noise types. The geometric means of the proposed unsupervised approaches outperformed spectral subtraction (SS), minimum mean square estimation (MMSE) under low input SNR conditions. All the proposed semi-supervised approaches showed superiority over SS and MMSE and also obtained better performance than the state-of-the-art algorithms which utilized a prior noise or speech dictionary under low SNR conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1233–1242},
numpages = {10},
keywords = {nonnegative, speech enhancement, blockwise/convolutional, matrix factorization, sparse and low-rank decomposition}
}

@article{10.1109/TASLP.2015.2422573,
author = {Chen, Dongpeng and Mak, Brian Kan-Wing},
title = {Multitask Learning of Deep Neural Networks for Low-Resource Speech Recognition},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2422573},
doi = {10.1109/TASLP.2015.2422573},
abstract = {We propose a multitask learning (MTL) approach to improve low-resource automatic speech recognition using deep neural networks (DNNs) without requiring additional language resources. We first demonstrate that the performance of the phone models of a single low-resource language can be improved by training its grapheme models in parallel under the MTL framework. If multiple low-resource languages are trained together, we investigate learning a set of universal phones (UPS) as an additional task again in the MTL framework to improve the performance of the phone models of all the involved languages. In both cases, the heuristic guideline is to select a task that may exploit extra information from the training data of the primary task(s). In the first method, the extra information is the phone-to-grapheme mappings, whereas in the second method, the UPS helps to implicitly map the phones of the multiple languages among each other. In a series of experiments using three low-resource South African languages in the Lwazi corpus, the proposed MTL methods obtain significant word recognition gains when compared with single-task learning (STL) of the corresponding DNNs or ROVER that combines results from several STL-trained DNNs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1172–1183},
numpages = {12},
keywords = {universal grapheme set, universal phone set, low-resource speech recognition, multitask learning, deep neural network (DNN)}
}

@article{10.1109/TASLP.2015.2419972,
author = {Basaran, Doundefineda\c{c} and Cemgil, Ali Taylan and Anarim, Emin},
title = {A Probabilistic Model-Based Approach for Aligning Multiple Audio Sequences},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2419972},
doi = {10.1109/TASLP.2015.2419972},
abstract = {We formulate the alignment problem of multiple and partially overlapping audio sequences in a probabilistic framework. We define and compare five generative models for several time varying features extracted from audio clips that are recorded independently and asynchronously. For each model, we derive the associated scoring function that evaluates the quality of an alignment. The matching is then achieved with a sequential algorithm. The derived score functions are also able to identify the cases where the sequences do not overlap and handle multiple sequences where no sequence is covering the entire timeline. The simulation results on real data suggest that the approach is able to handle difficult, ambiguous scenarios and partial matchings where simple baseline methods such as correlation fail.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1160–1171},
numpages = {12},
keywords = {audio synchronization, probabilistic model, fingerprinting, audio alignment, audio matching, multi sequence alignment}
}

@article{10.1109/TASLP.2015.2419978,
author = {Van Segbroeck, Maarten and Travadi, Ruchir and Narayanan, Shrikanth S.},
title = {Rapid Language Identification},
year = {2015},
issue_date = {July 2015},
publisher = {IEEE Press},
volume = {23},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2419978},
doi = {10.1109/TASLP.2015.2419978},
abstract = {A critical challenge to automatic language identification (LID) is achieving accurate performance with the shortest possible speech segment in a rapid fashion. The accuracy to correctly identify the spoken language is highly sensitive to the duration of speech and is bounded by the amount of information available. The proposed approach for rapid language identification transforms the utterances to a low dimensional i-vector representation upon which language classification methods are applied. In order to meet the challenges involved in rapidly making reliable decisions about the spoken language, a highly accurate and computationally efficient framework of i-vector extraction is proposed. The LID framework integrates the approach of universal background model (UBM) fused total variability modeling. UBM-fused modeling yields the estimation of a more discriminant, single i-vector space. This way, it is also a computationally more efficient alternative than system level fusion. A further reduction in equal error rate is achieved by training the i-vector model on long duration speech utterances and by the deployment of a robust feature extraction scheme that aims to capture the relevant language cues under various acoustic conditions. Evaluation results on the DARPA RATS data corpus suggest the potential of performing successful automated language identification at the level of one second of speech or even shorter duration.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1118–1129},
numpages = {12},
keywords = {total variability modeling, noise robustness, short-duration speech, universal background model (UBM) fusion, I-vector, rapid language identification}
}

@article{10.1109/TASLP.2015.2414820,
author = {Liu, Shih-Hung and Chen, Kuan-Yu and Chen, Berlin and Wang, Hsin-Min and Yen, Hsu-Chun and Hsu, Wen-Lian},
title = {Combining Relevance Language Modeling and Clarity Measure for Extractive Speech Summarization},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2414820},
doi = {10.1109/TASLP.2015.2414820},
abstract = {Extractive speech summarization, which purports to select an indicative set of sentences from a spoken document so as to succinctly represent the most important aspects of the document, has garnered much research over the years. In this paper, we cast extractive speech summarization as an ad-hoc information retrieval (IR) problem and investigate various language modeling (LM) methods for important sentence selection. The main contributions of this paper are four-fold. First, we explore a novel sentence modeling paradigm built on top of the notion of relevance, where the relationship between a candidate summary sentence and a spoken document to be summarized is discovered through different granularities of context for relevance modeling. Second, not only lexical but also topical cues inherent in the spoken document are exploited for sentence modeling. Third, we propose a novel clarity measure for use in important sentence selection, which can help quantify the thematic specificity of each individual sentence that is deemed to be a crucial indicator orthogonal to the relevance measure provided by the LM-based methods. Fourth, in an attempt to lessen summarization performance degradation caused by imperfect speech recognition, we investigate making use of different levels of index features for LM-based sentence modeling, including words, subword-level units, and their combination. Experiments on broadcast news summarization seem to demonstrate the performance merits of our methods when compared to several existing well-developed and/or state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {957–969},
numpages = {13},
keywords = {language modeling, relevance modeling, speech summarization, KL divergence, clarity measure}
}

@article{10.1109/TASLP.2014.2387411,
author = {Rouvier, Mickael and Oger, Stanislas and Linar\`{e}s, Georges and Matrouf, Driss and Merialdo, Bernard and Li, Yingbo},
title = {Audio-Based Video Genre Identification},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2387411},
doi = {10.1109/TASLP.2014.2387411},
abstract = {This paper presents investigations about the automatic identification of video genre by audio channel analysis. Genre refers to editorial styles such commercials, movies, sports... We propose and evaluate some methods based on both low and high level descriptors, in cepstral or time domains, but also by analyzing the global structure of the document and the linguistic contents. Then, the proposed features are combined and their complementarity is evaluated. On a database composed of single-stories web-videos, the best audio-only based system performs 9% of Classification Error Rate (CER). Finally, we evaluate the complementarity of the proposed audio features and video features that are classically used for Video Genre Identification (VGI). Results demonstrate the complementarity of the modalities for genre recognition, the final audio-video system reaching 6% CER.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1031–1041},
numpages = {11},
keywords = {linguistic feature extraction, video genre classification, automatic classification}
}

@article{10.1109/TASLP.2015.2418576,
author = {Kameoka, Hirokazu and Yoshizato, Kota and Ishihara, Tatsuma and Kadowaki, Kento and Ohishi, Yasunori and Kashino, Kunio},
title = {Generative Modeling of Voice Fundamental Frequency Contours},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2418576},
doi = {10.1109/TASLP.2015.2418576},
abstract = {This paper introduces a generative model of voice fundamental frequency (F0) contours that allows us to extract prosodic features from raw speech data. The present contour model is formulated by translating the Fujisaki model, a well-founded mathematical model representing the control mechanism of vocal fold vibration, into a probabilistic model described as a discrete-time stochastic process. There are two motivations behind this formulation. One is to derive a general parameter estimation framework for the Fujisaki model that allows the introduction of powerful statistical methods. The other is to construct an automatically trainable version of the Fujisaki model that we can incorporate into statistical-model-based text-to-speech synthesizers in such a way that the Fujisaki-model parameters can be learned from a speech corpus in a unified manner. It could also be useful for other speech applications such as emotion recognition, speaker identification, speech conversion and dialogue systems, in which prosodic information plays a significant role. We quantitatively evaluated the performance of the proposed Fujisaki model parameter extractor using real speech data. Experimental results revealed that our method was superior to a state-of-the-art Fujisaki model parameter extractor.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1042–1053},
numpages = {12},
keywords = {prosody, Fujisaki model, voice fundamental frequency contour, expectation-maximization algorithm}
}

@article{10.1109/TASLP.2015.2416655,
author = {Foster, Peter and Dixon, Simon and Klapuri, Anssi},
title = {Identifying Cover Songs Using Information-Theoretic Measures of Similarity},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2416655},
doi = {10.1109/TASLP.2015.2416655},
abstract = {This paper investigates methods for quantifying similarity between audio signals, specifically for the task of cover song detection. We consider an information-theoretic approach, where we compute pairwise measures of predictability between time series. We compare discrete-valued approaches operating on quantized audio features, to continuous-valued approaches. In the discrete case, we propose a method for computing the normalized compression distance, where we account for correlation between time series. In the continuous case, we propose to compute information-based measures of similarity as statistics of the prediction error between time series. We evaluate our methods on two cover song identification tasks using a data set comprised of 300 Jazz standards and using the Million Song Dataset. For both datasets, we observe that continuous-valued approaches outperform discrete-valued approaches. We consider approaches to estimating the normalized compression distance (NCD) based on string compression and prediction, where we observe that our proposed normalized compression distance with alignment (NCDA) improves average performance over NCD, for sequential compression algorithms. Finally, we demonstrate that continuous-valued distances may be combined to improve performance with respect to baseline approaches. Using a large-scale filter-and-refine approach, we demonstrate state-of-the-art performance for cover song identification using the Million Song Dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {993–1005},
numpages = {13},
keywords = {time series prediction, normalized compression distance, audio similarity measures, cover song identification}
}

@article{10.1109/TASLP.2015.2418577,
author = {Cernak, Milos and Garner, Philip N. and Lazaridis, Alexandros and Motlicek, Petr and Na, Xingyu},
title = {Incremental Syllable-Context Phonetic Vocoding},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2418577},
doi = {10.1109/TASLP.2015.2418577},
abstract = {Current very low bit rate speech coders are, due to complexity limitations, designed to work off-line. This paper investigates incremental speech coding that operates real-time and incrementally (i.e., encoded speech depends only on already-uttered speech without the need of future speech information). Since human speech communication is asynchronous (i.e., different information flows being simultaneously processed), we hypothesized that such an incremental speech coder should also operate asynchronously. To accomplish this task, we describe speech coding that reflects the human cortical temporal sampling that packages information into units of different temporal granularity, such as phonemes and syllables, in parallel. More specifically, a phonetic vocoder--cascaded speech recognition and synthesis systems--extended with syllable-based information transmission mechanisms is investigated. There are two main aspects evaluated in this work, the synchronous and asynchronous coding. Synchronous coding refers to the case when the phonetic vocoder and speech generation process depend on the syllable boundaries during encoding and decoding respectively. On the other hand, asynchronous coding refers to the case when the phonetic encoding and speech generation processes are done independently of the syllable boundaries. Our experiments confirmed that the asynchronous incremental speech coding performs better, in terms of intelligibility and overall speech quality, mainly due to better alignment of the segmental and prosodic information. The proposed vocoding operates at an uncompressed bit rate of 213 bits/sec and achieves an average communication delay of 243 ms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1019–1030},
numpages = {12},
keywords = {very low bit rate speech coding, parametric speech synthesis}
}

@article{10.1109/TASLP.2015.2419980,
author = {Jia, Maoshen and Yang, Ziyu and Bao, Changchun and Zheng, Xiguang and Ritz, Christian},
title = {Encoding Multiple Audio Objects Using Intra-Object Sparsity},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2419980},
doi = {10.1109/TASLP.2015.2419980},
abstract = {Preserving audio scenes in the form of audio objects has become common in recent years. Object-based audio techniques provide more flexibility for personalized rendering as well as a more accurate audio object trajectory. For encoding and transmitting multiple audio objects in a lossy manner, a new compression framework for multiple simultaneously occurring audio objects is presented in this work. The proposed encoding approach is based on the intra-object sparsity (approximate k-sparsity). After establishing a quantitative measure of approximate k-sparsity, statistical analysis is employed to validate the proposed intra-object sparsity of audio objects. By exploring this intra-object sparsity, multiple simultaneously occurring audio objects are compressed into a mono downmix signal with side information. This downmix signal can be further compressed by legacy audio codecs. Meanwhile, the side information is transmitted in a lossless manner. The objective and subjective evaluations revealed that the proposed compression framework achieved better perceptual quality compared to an existing technique where up to eight audio objects are considered. The subjective evaluations also confirmed that the proposed approach is able to achieve scalable transmission according to the bandwidth while preserving the perceptual quality of both the individual audio objects and the spatial audio scenes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1082–1095},
numpages = {14},
keywords = {sparsity, audio object coding, multichannel audio compression}
}

@article{10.5555/2876416.2876419,
author = {Han, Kun and Wang, Yuxuan and Wang, DeLiang and Woods, William S. and Merks, Ivo and Zhang, Tao},
title = {Learning Spectral Mapping for Speech Dereverberation and Denoising},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
abstract = {In real-world environments, human speech is usually distorted by both reverberation and background noise, which have negative effects on speech intelligibility and speech quality. They also cause performance degradation in many speech technology applications, such as automatic speech recognition. Therefore, the dereverberation and denoising problems must be dealt with in daily listening environments. In this paper, we propose to perform speech dereverberation using supervised learning, and the supervised approach is then extended to address both dereverberation and denoising. Deep neural networks are trained to directly learn a spectral mapping from the magnitude spectrogram of corrupted speech to that of clean speech. The proposed approach substantially attenuates the distortion caused by reverberation, as well as background noise, and is conceptually simple. Systematic experiments show that the proposed approach leads to significant improvements of predicted speech intelligibility and quality, as well as automatic speech recognition in reverberant noisy conditions. Comparisons show that our approach substantially outperforms related methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {982–992},
numpages = {11},
keywords = {dereverberation, deep neural networks (DNNs), supervised learning, spectral mapping, denoising}
}

@article{10.1109/TASLP.2015.2414823,
author = {Niedzwiecki, Maciej and Cio\l{}ek, Marcin and Cisowski, Krzysztof},
title = {Elimination of Impulsive Disturbances from Stereo Audio Recordings Using Vector Autoregressive Modeling and Variable-Order Kalman Filtering},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2414823},
doi = {10.1109/TASLP.2015.2414823},
abstract = {This paper presents a new approach to elimination of impulsive disturbances from stereo audio recordings. The proposed solution is based on vector autoregressive modeling of audio signals. Online tracking of signal model parameters is performed using the exponentially weighted least squares algorithm. Detection of noise pulses and model-based interpolation of the irrevocably distorted samples is realized using an adaptive, variable-order Kalman filter. The proposed approach is evaluated on a set of clean audio signals contaminated with real click waveforms extracted from old gramophone recordings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {970–981},
numpages = {12},
keywords = {vector autoregressive models, adaptive Kalman filtering, elimination of impulsive disturbances}
}

@article{10.1109/TASLP.2015.2418571,
author = {Schwarz, Andreas and Kellermann, Walter},
title = {Coherent-to-Diffuse Power Ratio Estimation for Dereverberation},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2418571},
doi = {10.1109/TASLP.2015.2418571},
abstract = {The estimation of the time- and frequency-dependent coherent-to-diffuse power ratio (CDR) from the measured spatial coherence between two omnidirectional microphones is investigated. Known CDR estimators are formulated in a common framework, illustrated using a geometric interpretation in the complex plane, and investigated with respect to bias and robustness towards model errors. Several novel unbiased CDR estimators are proposed, and it is shown that knowledge of either the direction of arrival (DOA) of the target source or the coherence of the noise field is sufficient for unbiased CDR estimation. The validity of the model for the application of CDR estimates to dereverberation is investigated using measured and simulated impulse responses. A CDR-based dereverberation system is presented and evaluated using signal-based quality measures as well as automatic speech recognition accuracy. The results show that the proposed unbiased estimators have a practical advantage over existing estimators, and that the proposed DOA-independent estimator can be used for effective blind dereverberation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1006–1018},
numpages = {13},
keywords = {spatial coherence, reverberation suppression, diffuseness, diffuse noise suppression, dereverberation}
}

@article{10.1109/TASLP.2015.2419971,
author = {Bates, Alice P. and Khalid, Zubair and Kennedy, Rodney A.},
title = {Novel Sampling Scheme on the Sphere for Head-Related Transfer Function Measurements},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2419971},
doi = {10.1109/TASLP.2015.2419971},
abstract = {This paper presents a novel sampling scheme on the sphere for obtaining head-related transfer function (HRTF) measurements and accurately computing the spherical harmonic transform (SHT). The scheme requires an optimal number of samples, given by the degrees of freedom in the spectral domain, for the accurate representation of the HRTF that is band-limited in the spherical harmonic domain. The proposed scheme allows for the samples to be easily taken over the sphere due to its iso-latitude structure and non-dense sampling near the poles. In addition, the scheme can be used when samples are not taken from the south polar cap region of the sphere as the HRTF measurements are not reliable in south polar cap region due to reflections from the ground. Furthermore, the scheme has a hierarchical structure, which enables the HRTF to be analyzed at different audible frequencies using the same sampling configuration. In comparison to the proposed scheme, none of the other sampling schemes on the sphere simultaneously possess all these properties. We conduct several numerical experiments to determine the accuracy of the SHT associated with the proposed sampling scheme. We show that the SHT attains accuracy on the order of numerical precision (10-14) when samples are taken over the whole sphere, both in the optimal sample placement and hierarchical configurations, and achieves an acceptable level of accuracy (10-5) when samples are not taken over the south polar cap region of the sphere for the band-limits of interest. Simulations are used to show the accurate reconstruction of the HRTF over the whole sphere, including unmeasured locations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1068–1081},
numpages = {14},
keywords = {spherical harmonics, sampling, spectral analysis, 2-sphere (unit sphere), spherical harmonic transform, head-related transfer function (HRTF) measurements}
}

@article{10.1109/TASLP.2015.2419076,
author = {Markovic, Dejan and Antonacci, Fabio and Sarti, Augusto and Tubaro, Stefano},
title = {Multiview Soundfield Imaging in the Projective Ray Space},
year = {2015},
issue_date = {June 2015},
publisher = {IEEE Press},
volume = {23},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2419076},
doi = {10.1109/TASLP.2015.2419076},
abstract = {A soundfield image is a data structure that efficiently encodes and represents the wave field as captured by a microphone array. Its representation is based on the directional plenacoustic function, which is defined as the radiance of the acoustic paths (rays) that cross the segment that the array lies upon. The soundfield image can be processed "as is" to develop a variety of applications. In its original formulation, the soundfield image is based on a Euclidean parameterization that can accommodate a limited range of rays and is suitable for managing a single array only. In this paper, we generalize this methodology to the case of multiple microphone arrays deployed in space. The use of multiple arrays allows us to capture truly global information on the sound field but requires us to rethink the ray space, and adopt a global representation of the acoustic rays based on projective geometry. After introducing the new parameterization, we present two examples of applications: the estimation of the mutual poses of two or more arrays (self-calibration); and the localization of multiple acoustic sources. The effectiveness of these applications is proven through simulations as well as real data experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1054–1067},
numpages = {14},
keywords = {acoustic measurements, acoustic signal detection, array signal processing}
}

@article{10.1109/TASLP.2015.2409780,
author = {Hendriks, Richard C. and Crespo, Jo\~{a}o B. and Jensen, Jesper and Taal, Cees H.},
title = {Optimal Near-End Speech Intelligibility Improvement Incorporating Additive Noise and Late Reverberation under an Approximation of the Short-Time SII},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2409780},
doi = {10.1109/TASLP.2015.2409780},
abstract = {The presence of environmental additive noise in the vicinity of the user typically degrades the speech intelligibility of speech processing applications. This intelligibility loss can be compensated by properly preprocessing the speech signal prior to playout, often referred to as near-end speech enhancement. Although the majority of such algorithms focus primarily on the presence of additive noise, reverberation can also severely degrade intelligibility. In this paper we investigate how late reverberation and additive noise can be jointly taken into account in the near-end speech enhancement process. For this effort we use a recently presented approximation of the speech intelligibility index under a power constraint, which we optimize for speech degraded by both additive noise and late reverberation. The algorithm results in time-frequency dependent amplification factors that depend on both the additive noise power spectral density as well as the late reverberation energy. These amplification factors redistribute speech energy across frequency and perform a dynamic range compression. Experimental results using both instrumental intelligibility measures as well as intelligibility listening tests show that the proposed approach improves speech intelligibility over state-of-the-art reference methods when speech signals are degraded simultaneously by additive noise and reverberation. Speech intelligibility improvements in the order of 20% are observed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {851–862},
numpages = {12},
keywords = {additive noise, late reverberation, speech intelligibility, approximated speech intelligibility index (SII)}
}

@article{10.1109/TASLP.2015.2412466,
author = {Chien, Jen-Tzung},
title = {Laplace Group Sensing for Acoustic Models},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2412466},
doi = {10.1109/TASLP.2015.2412466},
abstract = {This paper presents the group sparse learning for acoustic models where a sequence of acoustic features is driven by Markov chain and each feature vector is represented by groups of basis vectors. The group of common bases represents the features across Markov states within a regression class. The group of individual basis compensates the intra-state residual information. Laplace distribution is used as the sparse prior of sensing weights for group basis representation. Laplace parameter serves as regularization parameter or automatic relevance determination which controls the selection of relevant bases for acoustic modeling. The groups of regularization parameters and basis vectors are estimated from training data by maximizing the marginal likelihood over sensing weights which is implemented by Laplace approximation using the Hessian matrix and the maximum a posteriori parameters. Model uncertainty is compensated through full Bayesian treatment. The connection of Laplace group sensing to lasso regularization is illustrated. Experiments on noisy speech recognition show the robustness of group sparse acoustic models in presence of different noise types and SNRs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {909–922},
numpages = {14},
keywords = {speech recognition, Laplace distribution, acoustic model, basis representation, group sparsity}
}

@article{10.1109/TASLP.2015.2409779,
author = {Mu, Hao and Gan, Woon-Seng and Tan, Ee-Leng},
title = {An Objective Analysis Method for Perceptual Quality of a Virtual Bass System},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2409779},
doi = {10.1109/TASLP.2015.2409779},
abstract = {Due to the physical size and frequency response constraints of miniaturized and flat panel loudspeakers, low frequency reproduction from these loudspeakers is generally limited and unsatisfactory. The virtual bass system (VBS) enhances the bass performance of such loudspeakers by tricking the human brain to perceive the fundamental frequency from its higher harmonics. Problematically, the additional harmonics from VBS can also introduce perceptual distortion and reduce the audio quality. Therefore, a reliable method to assess the quality of VBS-enhanced signals is necessary in the designing of VBS. Since subjective experiments are often time-consuming and may be inconsistent, it is desirable to develop an objective assessment method for VBS. Earlier studies only utilized some simple objective metrics, which generally do not consider the human auditory model and are unable to accurately predict the perceptual quality of VBS. In this paper, we introduce a perceptual quality-assessment method for VBS based on the model output variables (MOVs) of the ITU Recommendation ITU-R BS.1387. Suitable combinations of MOVs are selected to derive perceptual quality metrics that correlate closely to the subjective quality. A verification experiment is presented to justify the accuracy of the metrics.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {840–850},
numpages = {11},
keywords = {perceptual distortion, bass enhancement, audio quality evaluation, psychoacoustic signal processing, phase vocoder}
}

@article{10.1109/TASLP.2015.2409774,
author = {Wei, Ying and Wang, Yinfeng},
title = {Design of Low Complexity Adjustable Filter Bank for Personalized Hearing Aid Solutions},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2409774},
doi = {10.1109/TASLP.2015.2409774},
abstract = {The emerging demand for personalized hearing aids requires the filter bank of a hearing aid system to be capable of decomposing the sound waves in accordance with the characteristic of the patient's hearing loss. In this paper, an efficient adjustable filter bank is proposed to achieve this goal. By careful design, the number of the subbands as well as the location of the subbands can be easily adjusted by changing a 4-bit control signal. The proposed filter bank has extremely low complexity due to the adoption of fractional interpolation and the technique of symmetric and complementary filters. Only one prototype filter is needed for each of the stages, the multiple passbands generation stage and masking stage. We show, by means of examples, that the proposed filter bank can meet different needs of hearing loss cases with acceptable delay.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {923–931},
numpages = {9},
keywords = {hearing aids, adjustable, fractional interpolation, filter bank}
}

@article{10.1109/TASLP.2015.2412462,
author = {Breebaart, Jeroen},
title = {Evaluation of Statistical Inference Tests Applied to Subjective Audio Quality Data with Small Sample Size},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2412462},
doi = {10.1109/TASLP.2015.2412462},
abstract = {Monte-Carlo simulations of statistical inference tests were performed to assess type 1 (false rejection) and type 2 (false non-rejection) error rates associated with subjective audio quality data as a function of sample size. Samples were generated by randomly drawing data from large-scale subjective audio quality tests. Null hypotheses were simulated by equalizing population means followed by pooling. The Null hypothesis rejection rates were determined for a parametric t test, as well as a non-parametric (permutation) test and compared to rejection rates based on analytical expressions and empirical distributions of the sample means and medians. The results indicated that pairwise comparisons are beneficial for high power and to obtain type I error rates that are close to the nominal value of 5%. The pairwise inferences can be realized by a parametric, pairwise t test or by a non-parametric permutation test, provided that for the latter, only pairwise permutations are executed. Although the observations from this study cannot be generalized for arbitrary data sets, the results do indicate that a pairwise, non-parametric resampling test is an interesting candidate for the statistical analysis of subjective quality data due to the absence of any requirements on data distributions and its relatively accurate performance in terms of Null hypothesis rejection rates.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {887–897},
numpages = {11},
keywords = {statistical analysis, audio compression, audio coding, Monte Carlo methods}
}

@article{10.1109/TASLP.2015.2409737,
author = {Krebs, Florian and Holzapfel, Andre and Cemgil, Ali Taylan and Widmer, Gerhard},
title = {Inferring Metrical Structure in Music Using Particle Filters},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2409737},
doi = {10.1109/TASLP.2015.2409737},
abstract = {In this paper, we propose a new state-of-the-art particle filter (PF) system to infer the metrical structure of musical audio signals. The new inference method is designed to overcome the problem of PFs in multi-modal probability distributions, which arise due to tempo and phase ambiguities in musical rhythm representations. We compare the new method with a hidden Markov model (HMM) system and several other PF schemes in terms of performance, speed and scalability on several audio datasets. We demonstrate that using the proposed system the computational complexity can be reduced drastically in comparison to the HMM while maintaining the same order of beat tracking accuracy. Therefore, for the first time, the proposed system allows fast meter inference in a high-dimensional state space, spanned by the three components of tempo, type of rhythm, and position in a metric cycle.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {817–827},
numpages = {11},
keywords = {downbeat tracking, bayesian modeling, particle filters (PFs), beat tracking, approximate inference}
}

@article{10.1109/TASLP.2015.2410140,
author = {Perez-Carrillo, Alfonso and Wanderley, Marcelo M.},
title = {Indirect Acquisition of Violin Instrumental Controls from Audio Signal with Hidden Markov Models},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2410140},
doi = {10.1109/TASLP.2015.2410140},
abstract = {Acquisition of instrumental gestures in musical performances is a field of increasing interest with applications in different areas ranging from acoustics and sound synthesis to motor learning or artistic performances. Direct acquisition approaches are based on measurements with sensors attached on the instrument or the performer, a process that usually involves the use of expensive sensing systems and complex setups that are generally intrusive in practice. An alternative is the indirect acquisition without sensors based on analysis of the audio signal. This paper reports a novel indirect acquisition method for the estimation of continuous violin controls from audio-signal analysis based on the training of statistical models with a database of previously recorded violin performances. The database contains synchronized streams of audio features and instrumental controls. Audio signal was captured with a vibration transducer built into the violin bridge, and instrumental controls were measured with sensors. The controls include bowing parameters (played string, bowing velocity, bowing force, and bowing distance to the bridge) as well as fingering position. Once the model is trained for a specific violin, we can perform indirect acquisition from analysis of the signal captured with its embedded transducer without the need for the sensors any more. The statistical methods used are Hidden Markov Models (HMM) with observation distributions parameterized as Multivariate Gaussian Mixtures (GM). HMMs provide a means for note recognition and following and parameter prediction is based on GM regression. Results show that the presented method constitutes an accurate, non-intrusive and low-cost alternative for instrumental control acquisition of a previously calibrated violin and recording device.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {932–940},
numpages = {9},
keywords = {indirect acquisition, musical gesture, information retrieval, violin instrumental controls}
}

@article{10.1109/TASLP.2015.2410139,
author = {Berkun, Reuven and Cohen, Israel and Benesty, Jacob},
title = {Combined Beamformers for Robust Broadband Regularized Superdirective Beamforming},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2410139},
doi = {10.1109/TASLP.2015.2410139},
abstract = {Superdirective fixed beamformers are known to attain high directivity factors, but are extremely sensitive to uncorrelated noise and slight errors in the array elements, which are modeled by the beamformer white noise gain measure. The delay-and-sum beamformer, on the other hand, manages to maximize the white noise gain, but suffers from a very low directivity factor. In this paper, we discuss the design of a broadband beamformer which controls both the directivity factor and the white noise gain. We combine a regularized version of the superdirective beamformer together with the delay-and-sum beamformer to create a robust regularized superdirective beamformer. We derive analytic closed-form expressions of the beamformer gain responses, and extend them to derive a beamformer with full control of the desired white noise gain or the directivity factor. The proposed approach offers a simple and robust broadband beamformer with controllable characteristics, shown here through persuasive simulation results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {877–886},
numpages = {10},
keywords = {robust superdirective beamformer, delay-and-sum beamformer, superdirective beamformer, microphone arrays, white noise gain, beamforming, directivity factor, supergain}
}

@article{10.1109/TASLP.2015.2412464,
author = {Zivanovic, Miroslav},
title = {Harmonic Bandwidth Companding for Separation of Overlapping Harmonics in Pitched Signals},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2412464},
doi = {10.1109/TASLP.2015.2412464},
abstract = {We present a novel approach to separating pitched signals from single-channel mixtures by harmonic bandwidth companding. Recalling that in a short analysis window the harmonic waveform envelope conveys information about approximate amplitude and frequency time variations, we show that proper linear waveform envelope smoothing reduces frequency spread around the harmonics and thus compresses the harmonic bandwidth. Such an action largely relaxes worst-case separation scenarios, which are caused by large proportions of overlapped harmonics. The distortion in separated signals due to harmonic bandwidth compression is compensated by an inverse system--the harmonic bandwidth expander. The benefit of such an approach is that explicit classification in overlapped and non-overlapped harmonics is no longer needed. Moreover, the underlying signal model is linear-in-parameters which allows for an efficient estimation via linear least-squares. The results show that harmonic bandwidth companding significantly outperforms two state-of-the-art separation methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {898–908},
numpages = {11},
keywords = {harmonic bandwidth, non-stationary signals, companding, signal separation}
}

@article{10.1109/TASLP.2015.2409785,
author = {Abdelaziz, Ahmed Hussen and Zeiler, Steffen and Kolossa, Dorothea},
title = {Learning Dynamic Stream Weights for Coupled-HMM-Based Audio-Visual Speech Recognition},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2409785},
doi = {10.1109/TASLP.2015.2409785},
abstract = {With the increasing use of multimedia data in communication technologies, the idea of employing visual information in automatic speech recognition (ASR) has recently gathered momentum. In conjunction with the acoustical information, the visual data enhances the recognition performance and improves the robustness of ASR systems in noisy and reverberant environments. In audio-visual systems, dynamic weighting of audio and video streams according to their instantaneous confidence is essential for reliably and systematically achieving high performance. In this paper, we present a complete framework that allows blind estimation of dynamic stream weights for audio-visual speech recognition based on coupled hidden Markov models (CHMMs). As a stream weight estimator, we consider using multilayer perceptrons and logistic functions to map multidimensional reliability measure features to audiovisual stream weights. Training the parameters of the stream weight estimator requires numerous input-output tuples of reliability measure features and their corresponding stream weights. We estimate these stream weights based on oracle knowledge using an expectation maximization algorithm. We define 31-dimensional feature vectors that combine model-based and signal-based reliability measures as inputs to the stream weight estimator. During decoding, the trained stream weight estimator is used to blindly estimate stream weights. The entire framework is evaluated using the Grid audio-visual corpus and compared to state-of-the-art stream weight estimation strategies. The proposed framework significantly enhances the performance of the audio-visual ASR system in all examined test conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {863–876},
numpages = {14},
keywords = {stream weight, audio-visual speech recognition, logistic regression, reliability measure, coupled hidden Markov model, multilayer perceptron}
}

@article{10.1109/TASLP.2015.2409778,
author = {Cho, Janghoon and Yoo, Chang D.},
title = {Underdetermined Convolutive BSS: Bayes Risk Minimization Based on a Mixture of Super-Gaussian Posterior Approximation},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2409778},
doi = {10.1109/TASLP.2015.2409778},
abstract = {This paper considers the underdetermined blind source separation (BSS) of convolutively mixed super-Gaussian signals that include speech, audio, and various other sparse signals. Here, the separation is performed in three steps. In the first and second steps, the mixing matrix and the sources at each time-frequency location are estimated by minimizing the Bayes risk (or the posterior risk) with squared loss. In the final third step, the permutation alignment is conducted by considering the correlation between adjacent spectral bins as in many conventional algorithms. To overcome any computationally intractable integrations involving a complex-valued super-Gaussian source prior, the posterior distribution of the sources is approximated as a mixture of super-Gaussians. The posterior means of the mixing matrix and the sources are obtained with Metropolis-Hastings within Gibbs sampling and the weighted sum of individual super-Gaussians, respectively. Overall, this approximation leads to a separation that is computationally lighter than and as accurate as the algorithm without the approximation. The simulation results of the synthetically generated data in a virtual room with reverberation show that the estimates of the mixing matrix in the first step and the sources in the second step are more accurate than the estimates from the state-of-the-art algorithms in terms of the mixing error ratio (MER) and the signal-to-distortion ratio (SDR). The experiment was also conducted with recorded data in a real room environment using a public benchmark dataset. Results show that the proposed algorithm gives a better performance compared to the state-of-the-art algorithms in terms of the SDR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {828–839},
numpages = {12},
keywords = {cocktail party problem, underdetermined convolutive mixture, Bayesian estimation, blind source separation (BSS)}
}

@article{10.1109/TASLP.2015.2414818,
author = {Mansikkaniemi, Andr\'{e} and Kurimo, Mikko},
title = {Adaptation of Morph-Based Speech Recognition for Foreign Names and Acronyms},
year = {2015},
issue_date = {May 2015},
publisher = {IEEE Press},
volume = {23},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2414818},
doi = {10.1109/TASLP.2015.2414818},
abstract = {In this paper, we improve morph-based speech recognition system by focusing adaptation efforts on acronyms (ACRs) and foreign proper names (FPNs). An unsupervised language model (LM) adaptation framework based on two-pass decoding is used. Vocabulary adaptation is applied alongside unsupervised LM adaptation. The aim is to improve both language and pronunciation modeling for FPNs and ACRs. A smart selection algorithm is used to find the most likely topically related foreign words and acronyms from in-domain text. New pronunciation rules are generated for the selected words. Different kinds of morpheme adaptation operations are also evaluated on the ACR and FPN candidate words, to ensure optimal results are gained from pronunciation adaptation. Statistically significant improvements in average word error rate (WER), and term error rate (TER), are achieved using a combination of unsupervised LM adaptation with vocabulary adaptation focused on ACRs and FPNs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {941–950},
numpages = {10},
keywords = {foreign word detection, morph-based speech recognition, out-of-vocabulary (OOV) recognition, unsupervised language model (LM) adaptation}
}

@article{10.1109/TASLP.2015.2404035,
author = {Aneeja, G. and Yegnanarayana, B.},
title = {Single Frequency Filtering Approach for Discriminating Speech and Nonspeech},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2404035},
doi = {10.1109/TASLP.2015.2404035},
abstract = {In this paper, a signal processing approach is proposed for speech/nonspeech discrimination. The approach is based on single frequency filtering (SFF), where the amplitude envelope of the signal is obtained at each frequency with high temporal and spectral resolution. This high resolution property helps to exploit the resulting high signal-to-noise ratio (SNR) regions in time and frequency. The variance of the spectral information across frequency is higher for speech and lower for many types of noises. The mean and variance of the noise-compensated weighted envelopes are computed across frequency at each time instant. Decision logic is applied to the feature derived from the mean and variance values on varieties of degradations, including NTIMIT, CTIMIT and distance speech, besides degradation due to standard noise types. In all cases, the proposed method gives significantly better performance than the standard Adaptive Multi-rate VAD2 (AMR2) method. AMR2 method is chosen for comparison, as the method adapts itself for different degradations, and is seen to give good performance over different SNR situations. The proposed method does not use training data to derive the characteristics of speech or noise, nor makes any assumption on the nonspeech beginning. The SFF method appears promising in other applications of speech processing, such as pitch extraction and speech enhancement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {705–717},
numpages = {13},
keywords = {single frequency filtering (SFF), temporal variance, spectral variance, voice activity detection (VAD), speech/nonspeech discrimination, weighted component envelope}
}

@article{10.5555/2876386.2876401,
author = {Huang, Hao and Xu, Haihua and Wang, Xianhui and Silamu, Wushour},
title = {Maximum F1-Score Discriminative Training Criterion for Automatic Mispronunciation Detection},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
abstract = {We carry out an in-depth investigation on a newly proposed Maximum F1-score Criterion (MFC) discriminative training objective function for Goodness of Pronunciation (GOP) based automatic mispronunciation detection that makes use of Gaussian Mixture Model-hidden Markov model (GMM-HMM) as acoustic models. The formulation of MFC seeks to directly optimize F1-score by converting the non-differentiable F1-score function into a continuous objective function to facilitate optimization. We present model-space training algorithm according to MFC using extended Baum-Welch form like update equations based on the weak-sense auxiliary function method. We then present MFC based feature-space discriminative training. We train a matrix projecting from posteriors of Gaussians to a normal size feature space, and add the projected features to traditional spectral features. Mispronunciation detection experiments show MFC based model-space training and feature-space training are effective in improving F1-score and other commonly used evaluation metrics. It is also shown MFC training in both the feature-space and model-space outperforms either model-space training or feature-space training alone, and is about 11.6% better than the maximum likelihood (ML) trained baseline in terms of F1-score. Further, we review and compare mispronunciation detection results with the use of MFC and some traditional training criteria that minimize word error rate in speech recognition. The experimental analysis and comparison provide useful insight into the correlations between F1-score maximization and optimization of these training criteria.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {787–797},
numpages = {11},
keywords = {automatic mispronunciation detection, computer-assisted language learning, discriminative training, feature extraction, F1-score}
}

@article{10.1109/TASLP.2015.2403619,
author = {Kim, Myung Jong and Kim, Younggwan and Kim, Hoirin},
title = {Automatic Intelligibility Assessment of Dysarthric Speech Using Phonologically-Structured Sparse Linear Model},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2403619},
doi = {10.1109/TASLP.2015.2403619},
abstract = {This paper presents a new method for automatically assessing the speech intelligibility of patients with dysarthria, which is a motor speech disorder impeding the physical production of speech. The proposed method consists of two main steps: feature representation and prediction. In the feature representation step, the speech utterance is converted into a phone sequence using an automatic speech recognition technique and is then aligned with a canonical phone sequence from a pronunciation dictionary using a weighted finite state transducer to capture the pronunciation mappings such as match, substitution, and deletion. The histograms of the pronunciation mappings on a pre-defined word set are used for features. Next, in the prediction step, a structured sparse linear model incorporated with phonological knowledge that simultaneously addresses phonologically structured sparse feature selection and intelligibility prediction is proposed. Evaluation of the proposed method on a database of 109 speakers consisting of 94 dysarthric and 15 control speakers yielded a root mean square error of 8.14 compared to subjectively rated scores in the range of 0 to 100. This is a promising performance in which the system can be successfully applied to help speech therapists in diagnosing the degree of speech disorder.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {694–704},
numpages = {11},
keywords = {structured sparse model, Dysarthria, pronunciation confusion network, weighted finite state transducer (WFST), speech intelligibility assessment}
}

@article{10.1109/TASLP.2014.2385478,
author = {Chen, Langzhou and Braunschweiler, Norbert and Gales, Mark J. F.},
title = {Speaker and Expression Factorization for Audiobook Data: Expressiveness and Transplantation},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2385478},
doi = {10.1109/TASLP.2014.2385478},
abstract = {Expressive synthesis from text is a challenging problem. There are two issues. First, read text is often highly expressive to convey the emotion and scenario in the text. Second, since the expressive training speech is not always available for different speakers, it is necessary to develop methods to share the expressive information over speakers. This paper investigates the approach of using very expressive, highly diverse audio book data from multiple speakers to build an expressive speech synthesis system. Both of two problems are addressed by considering a factorized framework where speaker and emotion are modeled in separate sub-spaces of a cluster adaptive training (CAT) parametric speech synthesis system. The sub-spaces for the expressive state of a speaker and the characteristics of the speaker are jointly trained using a set of audiobooks. In this work, the expressive speech synthesis system works in two distinct modes. In the first mode, the expressive information is given by audio data and the adaptation method is used to extract the expressive information in the audio data. In the second mode, the input of the synthesis system is plain text and a full expressive synthesis system is examined where the expressive state is predicted from the text. In both modes, the expressive information is shared and transplanted over different speakers. Experimental results show that in both modes, the expressive speech synthesis method proposed in this work significantly improves the expressiveness of the synthetic speech for different speakers. Finally, this paper also examines whether it is possible to predict the expressive states from text for multiple speakers using a single model, or whether the prediction process needs to be speaker specific.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {605–618},
numpages = {14},
keywords = {expressive speech synthesis, cluster adaptive training, audiobook, neural network, hidden Markov model, factorization}
}

@article{10.1109/TASLP.2015.2405481,
author = {Dov, David and Talmon, Ronen and Cohen, Israel},
title = {Audio-Visual Voice Activity Detection Using Diffusion Maps},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2405481},
doi = {10.1109/TASLP.2015.2405481},
abstract = {The performance of traditional voice activity detectors significantly deteriorates in the presence of highly nonstationary noise and transient interferences. One solution is to incorporate a video signal which is invariant to the acoustic environment. Although several voice activity detectors based on the video signal were recently presented, merely few detectors which are based on both the audio and the video signals exist in the literature to date. In this paper, we present an audio-visual voice activity detector and show that the incorporation of both audio and video signals is highly beneficial for voice activity detection. The algorithm is based on a supervised learning procedure, and a labeled training data set is considered. The algorithm comprises a feature extraction procedure, where the features are designed to separate speech from nonspeech frames. Diffusion maps is applied separately and similarly to the features of each modality and builds a low dimensional representation. Using the new representation, we propose a measure for voice activity which is based on a supervised learning procedure and the variability between adjacent frames in time. The measures of the two modalities are merged to provide voice activity detection based on both the audio and the video signals. Experimental results demonstrate the improved performance of the proposed algorithm compared to state-of-the-art detectors.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {732–745},
numpages = {14},
keywords = {audio-visual speech processing, diffusion maps, voice activity detection}
}

@article{10.5555/2876386.2876391,
author = {Kitamura, Daichi and Saruwatari, Hiroshi and Kameoka, Hirokazu and Takahashi, Yu and Kondo, Kazunobu and Nakamura, Satoshi},
title = {Multichannel Signal Separation Combining Directional Clustering and Nonnegative Matrix Factorization with Spectrogram Restoration},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
abstract = {In this paper, to address problems in multichannel music signal separation, we propose a new hybrid method that combines directional clustering and advanced nonnegative matrix factorization (NMF). The aims of multichannel music signal separation technology is to extract a specific target signal from observed multichannel signals that contain multiple instrumental sounds. In previous studies, various methods using NMF have been proposed, but many problems remain including poor separation accuracy and lack of robustness. To solve these problems, we propose a new supervised NMF (SNMF) with spectrogram restoration and a hybrid method that concatenates the proposed SNMF after directional clustering. Via the extrapolation of supervised spectral bases, the proposed SNMF attempts both target signal separation and reconstruction of the lost target components, which are generated by preceding directional clustering. In addition, we experimentally reveal the trade-off between separation and extrapolation abilities and propose a new scheme for adaptive divergence, where the optimal divergence can be automatically changed in each time frame according to the local spatial conditions. The results of an evaluation experiment show that our proposed hybrid method outperforms the conventional music signal separation methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {654–669},
numpages = {16},
keywords = {music signal processing, multichannel signal separation, nonnegative matrix factorization (NMF), spectrogram restoration}
}

@article{10.1109/TASLP.2015.2409735,
author = {Wang, Chung-Che and Jang, Jyh-Shing Roger},
title = {Improving Query-by-Singing/Humming by Combining Melody and Lyric Information},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2409735},
doi = {10.1109/TASLP.2015.2409735},
abstract = {This paper proposes a novel method for improving query-by-singing/humming systems by using both melody and lyric information. First, singing/humming discrimination is performed to distinguish between singing and humming queries, which is achieved by considering the similarity between acoustic models. For the humming queries, a pitch-only melody recognition method that was ranked first among the MIREX (Music Information Retrieval Evaluation eXchange) query-by-singing/humming task submissions is applied. For the singing queries, a lyric similarity is computed using speech recognition techniques; the computed similarity is subsequently combined with the melody distance to exploit additional information in the lyrics. Several methods for combining melody distance and lyric similarity are investigated. Under the optimal experimental settings, the proposed query-by-singing/humming system achieves 51.19% error rate reduction for the top-10 retrieved results, indicating the feasibility of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {798–806},
numpages = {9},
keywords = {singing voice recognition, combined melody distance and lyric similarity, singing/humming discrimination (SHD), query-by-singing/humming (QBSH)}
}

@article{10.1109/TASLP.2015.2405476,
author = {De Sena, Enzo and Antonello, Niccol\`{o} and Moonen, Marc and Van Waterschoot, Toon},
title = {On the Modeling of Rectangular Geometries in Room Acoustic Simulations},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2405476},
doi = {10.1109/TASLP.2015.2405476},
abstract = {This paper is concerned with an acoustical phenomenon called sweeping echo, which manifests itself in a room impulse response as a distinctive, continuous pitch increase. In this paper, it is shown that sweeping echoes are present (although to greatly varying degrees) in all perfectly rectangular rooms. The theoretical analysis is based on the rigid-wall image solution of the wave equation. Sweeping echoes are found to be caused by the orderly time-alignment of high-order reflections arriving from directions close to the three axial directions. While sweeping echoes have been previously observed in real rooms with a geometry very similar to the rectangular model (e.g., a squash court), they are not perceived in commonly encountered rooms. Room acoustic simulators such as the image method (IM) and finite difference timedomain (FDTD) correctly predict the presence of this phenomenon, which means that rectangular geometries should be used with caution when the objective is to model commonly encountered rooms. Small out-of-square asymmetries in the room geometry are shown to reduce the phenomenon significantly. Randomization of the image sources' position is shown to remove sweeping echoes without the need to model an asymmetrical geometry explicitly. Finally, the performance of three speech and audio processing algorithms is shown to be sensitive to strong sweeping echoes, thus highlighting the need to avoid their occurrence.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {774–786},
numpages = {13},
keywords = {acoustic propagation, image method, room acoustics, acoustic signal processing, computer simulation, acoustics, finite difference methods}
}

@article{10.1109/TASLP.2015.2403613,
author = {Blanco, Eduardo and Moldovan, Dan},
title = {A Semantic Logic-Based Approach to Determine Textual Similarity},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2403613},
doi = {10.1109/TASLP.2015.2403613},
abstract = {This paper presents a semantic logic-based approach to determine textual similarity. Three logic form transformations taking into account semantic structure of sentences are proposed. Logic proofs are obtained using an adapted resolution step that drops predicates when a proof cannot be found with standard resolution. Features are extracted from proofs and combined using supervised machine learning to obtain the final similarity scores. Experimental results show that taking into account semantic relations to determine textual similarity yields performance improvements with respect to both baselines and third-party state-of-the-art systems. Specific sentence pairs that benefit from considering semantic relations are discussed. Detailed results provide empirical evidence that either proof direction offers a strong baseline although considering both is beneficial, and that ignoring concepts that are not an argument of a semantic relation is not sound.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {683–693},
numpages = {11},
keywords = {semantic relations, logic prover, textual similarity}
}

@article{10.1109/TASLP.2015.2401426,
author = {Mai, Van-Khanh and Pastor, Dominique and A\"{\i}ssa-El-Bey, Abdeldjalil and Le-Bidan, Rapha\"{e}l},
title = {Robust Estimation of Non-Stationary Noise Power Spectrum for Speech Enhancement},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2401426},
doi = {10.1109/TASLP.2015.2401426},
abstract = {We propose a novel method for noise power spectrum estimation in speech enhancement. This method called extended-DATE (E-DATE) extends the -dimensional amplitude trimmed estimator (DATE), originally introduced for additive white gaussian noise power spectrum estimation in "Robust estimation of noise standard deviation in presence of signals with unknown distributions and occurrences" (D. Pastor and F. Socheleau, IEEE Trans. Signal Processing, vol. 60, no. 4, pp. 1545-1555, Apr. 2012) to the more challenging scenario of non-stationary noise. The key idea is that, in each frequency bin and within a sufficiently short time period, the noise instantaneous power spectrum can be considered as approximately constant and estimated as the variance of a complex gaussian noise process possibly observed in the presence of the signal of interest. The proposed method relies on the fact that the Short-Time Fourier Transform (STFT) of noisy speech signals is sparse in the sense that transformed speech signals can be represented by a relatively small number of coefficients with large amplitudes in the time-frequency domain. The E-DATE estimator is robust in that it does not require prior information about the signal probability distribution except for the weak-sparseness property. In comparison to other state-of-the-art methods, the E-DATE is found to require the smallest number of parameters (only two). The performance of the proposed estimator has been evaluated in combination with noise reduction and compared to alternative methods. This evaluation involves objective as well as pseudo-subjective criteria.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {670–682},
numpages = {13},
keywords = {speech enhancement, robust statistics, noise power spectrum estimation, noise reduction}
}

@article{10.1109/TASLP.2015.2396681,
author = {Hu, Ying and Liu, Guizhong},
title = {Separation of Singing Voice Using Nonnegative Matrix Partial Co-Factorization for Singer Identification},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2396681},
doi = {10.1109/TASLP.2015.2396681},
abstract = {In order to improve the performance of singer identification, we propose a system to separate singing voice from music accompaniment for monaural recordings. Our system consists of two key stages. The first stage exploits the nonnegative matrix partial co-factorization (NMPCF), which is a joint matrix decomposition integrating prior knowledge of singing voice and pure accompaniment to separate the mixture signal into singing voice portion and accompaniment portion. In the second stage, based on the separated singing voice obtained by the first stage, the pitches of singing voice are first estimated and then the harmonic components of singing voice can be distinguished. For a frame, the distinguished harmonic components are regarded as reliable while other frequency components unreliable, thus the spectrum is incomplete. With those harmonic components, the complete spectrums of singing voice can be reconstructed by a missing feature method, spectrum reconstruction, obtaining a refined signal with more clean singing voice. Experimental results demonstrate that, from the point view of source separation, the singing voice refinement can further improve ΔSNR in contrast with the singing voice separation using NMPCF, while for the point view of singer identification, the singing voice separated by NMPCF is more appropriate than the refined singing voice.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {643–653},
numpages = {11},
keywords = {singer identification, spectrum reconstruction, nonnegative matrix partial co-factorization (NMPCF), singing voice separation}
}

@article{10.1109/TASLP.2015.2405475,
author = {Deleforge, Antoine and Horaud, Radu and Schechner, Yoav Y. and Girin, Laurent},
title = {Co-Localization of Audio Sources in Images Using Binaural Features and Locally-Linear Regression},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2405475},
doi = {10.1109/TASLP.2015.2405475},
abstract = {This paper addresses the problem of localizing audio sources using binaural measurements. We propose a supervised formulation that simultaneously localizes multiple sources at different locations. The approach is intrinsically efficient because, contrary to prior work, it relies neither on source separation, nor on monaural segregation. The method starts with a training stage that establishes a locally linear Gaussian regression model between the directional coordinates of all the sources and the auditory features extracted from binaural measurements. While fixed-length wide-spectrum sounds (white noise) are used for training to reliably estimate the model parameters, we show that the testing (localization) can be extended to variable-length sparse-spectrum sounds (such as speech), thus enabling a wide range of realistic applications. Indeed, we demonstrate that the method can be used for audio-visual fusion, namely to map speech signals onto images and hence to spatially align the audio and visual modalities, thus enabling to discriminate between speaking and non-speaking faces. We release a novel corpus of real-room recordings that allow quantitative evaluation of the co-localization method in the presence of one or two sound sources. Experiments demonstrate increased accuracy and speed relative to several state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {718–731},
numpages = {14},
keywords = {mixture model, binaural hearing, regression, audio-visual fusion, sound-source localization, supervised learning}
}

@article{10.5555/2876386.2876388,
author = {Zhou, Xinjie and Wan, Xiaojun and Xiao, Jianguo},
title = {CLOpinionMiner: Opinion Target Extraction in a Cross-Language Scenario},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
abstract = {Opinion target extraction is a subtask of opinion mining which is very useful in many applications. The problem has usually been solved by training a sequence labeler on manually labeled data. However, the labeled training datasets are imbalanced in different languages, and the lack of labeled corpus in a language limits the research progress on opinion target extraction in this language. In order to address the above problem, we propose a novel system called CLOpinionMiner which investigates leveraging the rich labeled data in a source language for opinion target extraction in a different target language. In this study, we focus on English-to-Chinese cross-language opinion target extraction. Based on the English dataset, our method produces two Chinese training datasets with different features. Two labeling models for Chinese opinion target extraction are trained based on Conditional Random Fields (CRF). After that, we use a monolingual co-training algorithm to improve the performance of both models by leveraging the enormous unlabeled Chinese review texts on the web. Experimental results show the effectiveness of our proposed approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {619–630},
numpages = {12},
keywords = {cross-language information extraction, opinion mining, opinion target extraction}
}

@article{10.1109/TASLP.2015.2401513,
author = {Mamun, Nursadul and Jassim, Wissam A. and Zilany, Muhammad S. A.},
title = {Prediction of Speech Intelligibility Using a Neurogram Orthogonal Polynomial Measure (NOPM)},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2401513},
doi = {10.1109/TASLP.2015.2401513},
abstract = {Sensorineural hearing loss (SNHL) is an increasingly prevalent condition, resulting from damage to the inner ear and causing a reduction in speech intelligibility. This paper proposes a new speech intelligibility prediction metric, the neurogram orthogonal polynomial measure (NOPM). This metric applies orthogonal moments to the auditory neurogram to predict speech intelligibility for listeners with and without hearing loss. The model simulates the responses of auditory-nerve fibers to speech signals under quiet and noisy conditions. Neurograms were created using a physiologically based computational model of the auditory periphery. A well-known orthogonal polynomial measure, Krawtchouk moments, was applied to extract features from the auditory neurogram. The predicted intelligibility scores were compared to subjective results, and NOPM showed a good fit with the subjective scores for normal listeners and also for listeners with hearing loss. The proposed metric has a realistic and wider dynamic range than corresponding existing metrics, such as mean structural similarity index measure and neurogram similarity index measure, and the predicted scores are also well-separated as a function of hearing loss. The application of this metric could be extended for assessing hearing-aid and speech-enhancement algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {760–773},
numpages = {14},
keywords = {orthogonal moment, neurogram, auditory-nerve model, speech intelligibility, sensorineural hearing loss}
}

@article{10.1109/TASLP.2015.2392944,
author = {Zhou, Pan and Jiang, Hui and Dai, Li-Rong and Hu, Yu and Liu, Qing-Feng},
title = {State-Clustering Based Multiple Deep Neural Networks Modeling Approach for Speech Recognition},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2392944},
doi = {10.1109/TASLP.2015.2392944},
abstract = {The hybrid deep neural network (DNN) and hidden Markov model (HMM) has recently achieved dramatic performance gains in automatic speech recognition (ASR). The DNN-based acoustic model is very powerful but its learning process is extremely time-consuming. In this paper, we propose a novel DNN-based acoustic modeling framework for speech recognition, where the posterior probabilities of HMM states are computed from multiple DNNs (mDNN), instead of a single large DNN, for the purpose of parallel training towards faster turnaround. In the proposed mDNN method all tied HMM states are first grouped into several disjoint clusters based on data-driven methods. Next, several hierarchically structured DNNs are trained separately in parallel for these clusters using multiple computing units (e.g. GPUs). In decoding, the posterior probabilities of HMM states can be calculated by combining outputs from multiple DNNs. In this work, we have shown that the training procedure of the mDNN under popular criteria, including both frame-level cross-entropy and sequence-level discriminative training, can be parallelized efficiently to yield significant speedup. The training speedup is mainly attributed to the fact that multiple DNNs are parallelized over multiple GPUs and each DNN is smaller in size and trained by only a subset of training data. We have evaluated the proposed mDNN method on a 64-hour Mandarin transcription task and the 320-hour Switchboard task. Compared to the conventional DNN, a 4-cluster mDNN model with similar size can yield comparable recognition performance in Switchboard (only about 2% performance degradation) with a greater than 7 times speed improvement in CE training and a 2.9 times improvement in sequence training, when 4 GPUs are used.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {631–642},
numpages = {12},
keywords = {data partition, parallel training, model parallelism, cross entropy training, speech recognition, multiple DNNs (mDNN), deep neural networks (DNN), sequence training, state clustering}
}

@article{10.1109/TASLP.2015.2405482,
author = {Habibi, Maryam and Popescu-Belis, Andrei},
title = {Keyword Extraction and Clustering for Document Recommendation in Conversations},
year = {2015},
issue_date = {April 2015},
publisher = {IEEE Press},
volume = {23},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2405482},
doi = {10.1109/TASLP.2015.2405482},
abstract = {This paper addresses the problem of keyword extraction from conversations, with the goal of using these keywords to retrieve, for each short conversation fragment, a small number of potentially relevant documents, which can be recommended to participants. However, even a short fragment contains a variety of words, which are potentially related to several topics; moreover, using an automatic speech recognition (ASR) system introduces errors among them. Therefore, it is difficult to infer precisely the information needs of the conversation participants. We first propose an algorithm to extract keywords from the output of an ASR system (or a manual transcript for testing), which makes use of topic modeling techniques and of a submodular reward function which favors diversity in the keyword set, to match the potential diversity of topics and reduce ASR noise. Then, we propose a method to derive multiple topically separated queries from this keyword set, in order to maximize the chances of making at least one relevant recommendation when using these queries to search over the English Wikipedia. The proposed methods are evaluated in terms of relevance with respect to conversation fragments from the Fisher, AMI, and ELEA conversational corpora, rated by several human judges. The scores show that our proposal improves over previous methods that consider only word frequency or topic similarity, and represents a promising solution for a document recommender system to be used in conversations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {746–759},
numpages = {14},
keywords = {document recommendation, meeting analysis, keyword extraction, topic modeling, information retrieval}
}

@article{10.1109/TASLP.2015.2405131,
author = {Li, Haizhou and Federico, Marcello and He, Xiaodong and Meng, Helen and Trancoso, Isabel},
title = {Introduction to the Special Section on Continuous Space and Related Methods in Natural Language Processing},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2405131},
doi = {10.1109/TASLP.2015.2405131},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {427–430},
numpages = {4}
}

@article{10.1109/TASLP.2015.2389034,
author = {Zeng, Xiaodong and Wong, Derek F. and Chao, Lidia S. and Trancoso, Isabel},
title = {Graph-Based Lexicon Regularization for PCFG with Latent Annotations},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2389034},
doi = {10.1109/TASLP.2015.2389034},
abstract = {This paper aims at learning a better probabilistic context-free grammar with latent annotations (PCFG-LA) by using a graph propagation (GP) technique. We propose leveraging the GP to regularize the lexical model of the grammar. The proposed approach constructs k-nearest neighbor (k-NN) similarity graphs over words with identical pre-terminal (part-of-speech) tags, for propagating the probabilities of latent annotations given the words. The graphs demonstrate the relationship between words in syntactic and semantic levels, estimated by using a neural word representation method based on Recursive autoencoder (RAE). We modify the conventional PCFG-LA parameter estimation algorithm, expectation maximization (EM), by incorporating a GP process subsequent to the M-step. The GP encourages the smoothness among the graph vertices, where different words under similar syntactic and semantic environments should have approximate posterior distributions of nonterminal subcategories. The proposed PCFG-LA learning approach was evaluated together with a hierarchical split-and-merge training strategy, on parsing tasks for English, Chinese and Portuguese. The empirical results reveal two crucial findings: 1) regularizing the lexicons with GP results in positive effects to parsing accuracy; and 2) learning with unlabeled data can also expand the PCFG-LA lexicons.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {441–450},
numpages = {10},
keywords = {graph propagation, natural language processing, syntax parsing, neural word representation}
}

@article{10.1109/TASLP.2014.2365359,
author = {Chen, Wenliang and Zhang, Min and Zhang, Yue},
title = {Distributed Feature Representations for Dependency Parsing},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2365359},
doi = {10.1109/TASLP.2014.2365359},
abstract = {This paper presents an approach to automatically learning distributed representations for features to address the feature sparseness problem for dependency parsing. Borrowing terminologies from word embeddings, we call the feature representation feature embeddings. In our approach, the feature embeddings are inferred from large amounts of auto-parsed data. First, the sentences in raw data are parsed by a baseline system and we obtain dependency trees. Then, we represent each model feature using the surrounding features on the dependency trees. Based on the representation of surrounding context, we proposed two learning methods to infer feature embeddings. Finally, based on feature embeddings, we present a set of new features for graph-based dependency parsing models. The new parsers can not only make full use of well-established hand-designed features but also benefit from the hidden-class representations of features. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {451–460},
numpages = {10},
keywords = {natural language processing, feature embeddings, semi-supervised approach, dependency parsing}
}

@article{10.5555/2817174.2817183,
author = {Rashwan, Mohsen A. A. and Al Sallab, Ahmad A. and Raafat, Hazem M. and Rafea, Ahmed},
title = {Deep Learning Framework with Confused Sub-Set Resolution Architecture for Automatic Arabic Diacritization},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
abstract = {The Arabic language belongs to a group of languages that require diacritization over their characters. Modern Standard Arabic (MSA) transcripts omit the diacritics, which are essential for many machine learning tasks like Text-To-Speech (TTS) systems. In this work Arabic diacritics restoration is tackled under a deep learning framework that includes the Confused Sub-set Resolution (CSR) method to improve the classification accuracy, in addition to an Arabic Part-of-Speech (PoS) tagging framework using deep neural nets. Special focus is given to syntactic diacritization, which still suffers low accuracy as indicated in prior works. Evaluation is done versus state-of-the-art systems reported in literature, with quite challenging datasets collected from different domains. Standard datasets like the LDC Arabic Tree Bank are used in addition to custom ones we have made available online to allow other researchers to replicate these results. Results show significant improvement of the proposed techniques over other approaches, reducing the syntactic classification error to 9.9% and morphological classification error to 3% compared to 12.7% and 3.8% of the best reported results in literature, improving the error by 22% over the best reported systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {505–516},
numpages = {12},
keywords = {deep networks, arabic diacritization, part-of-speech (PoS) tagging, classifier design}
}

@article{10.5555/2817174.2817185,
author = {Mesnil, Gr\'{e}goire and Dauphin, Yann and Yao, Kaisheng and Bengio, Yoshua and Deng, Li and Hakkani-Tur, Dilek and He, Xiaodong and Heck, Larry and Tur, Gokhan and Yu, Dong and Zweig, Geoffrey},
title = {Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
abstract = {Semantic slot filling is one of the most challenging problems in spoken language understanding (SLU). In this paper, we propose to use recurrent neural networks (RNNs) for this task, and present several novel architectures designed to efficiently model past and future temporal dependencies. Specifically, we implemented and compared several important RNN architectures, including Elman, Jordan, and hybrid variants. To facilitate reproducibility, we implemented these networks with the publicly available Theano neural network toolkit and completed experiments on the well-known airline travel information system (ATIS) benchmark. In addition, we compared the approaches on two custom SLU data sets from the entertainment and movies domains. Our results show that the RNN-based models outperform the conditional random field (CRF) baseline by 2% in absolute error reduction on the ATIS benchmark. We improve the state-of-the-art by 0.5% in the Entertainment domain, and 6.7% for the movies domain.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {530–539},
numpages = {10},
keywords = {word embedding, recurrent neural network (RNN), slot filling, spoken language understanding (SLU)}
}

@article{10.1109/TASLP.2015.2389622,
author = {Adel, Heike and Vu, Ngoc Thang and Kirchhoff, Katrin and Telaar, Dominic and Schultz, Tanja},
title = {Syntactic and Semantic Features for Code-Switching Factored Language Models},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2389622},
doi = {10.1109/TASLP.2015.2389622},
abstract = {This paper presents our latest investigations on different features for factored language models for Code-Switching speech and their effect on automatic speech recognition (ASR) performance. We focus on syntactic and semantic features which can be extracted from Code-Switching text data and integrate them into factored language models. Different possible factors, such as words, part-of-speech tags, Brown word clusters, open class words and clusters of open class word embeddings are explored. The experimental results reveal that Brown word clusters, part-of-speech tags and open-class words are the most effective at reducing the perplexity of factored language models on the Mandarin-English Code-Switching corpus SEAME. In ASR experiments, the model containing Brown word clusters and part-of-speech tags and the model also including clusters of open class word embeddings yield the best mixed error rate results. In summary, the best language model can significantly reduce the perplexity on the SEAME evaluation set by up to 10.8% relative and the mixed error rate by up to 3.4% relative.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {431–440},
numpages = {10},
keywords = {recurrent neural networks, automatic speech recognition (ASR), natural language processing}
}

@article{10.1109/TASLP.2014.2379593,
author = {Hutchinson, Brian and Ostendorf, Mari and Fazel, Maryam},
title = {A Sparse plus Low-Rank Exponential Language Model for Limited Resource Scenarios},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2379593},
doi = {10.1109/TASLP.2014.2379593},
abstract = {This paper describes a new exponential language model that decomposes the model parameters into one or more low-rank matrices that learn regularities in the training data and one or more sparse matrices that learn exceptions (e.g., keywords). The low-rank matrices induce continuous-space representations of words and histories. The sparse matrices learn multiword lexical items and topic/domain idiosyncrasies. This model generalizes the standard l1-regularized exponential language model, and has an efficient accelerated first-order training algorithm. Language modeling experiments show that the approach is useful in scenarios with limited training data, including low resource languages and domain adaptation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {494–504},
numpages = {11},
keywords = {sparse, language model, exponential, low-hyphen, log bilinear}
}

@article{10.1109/TASLP.2014.2379589,
author = {Nakashika, Toru and Takiguchi, Tetsuya and Ariki, Yasuo},
title = {Voice Conversion Using RNN Pre-Trained by Recurrent Temporal Restricted Boltzmann Machines},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2379589},
doi = {10.1109/TASLP.2014.2379589},
abstract = {This paper presents a voice conversion (VC) method that utilizes the recently proposed probabilistic models called recurrent temporal restricted Boltzmann machines (RTRBMs). One RTRBM is used for each speaker, with the goal of capturing high-order temporal dependencies in an acoustic sequence. Our algorithm starts from the separate training of one RTRBM for a source speaker and another for a target speaker using speaker-dependent training data. Because each RTRBM attempts to discover abstractions to maximally express the training data at each time step, as well as the temporal dependencies in the training data, we expect that the models represent the linguistic-related latent features in high-order spaces. In our approach, we convert (match) features of emphasis for the source speaker to those of the target speaker using a neural network (NN), so that the entire network (consisting of the two RTRBMs and the NN) acts as a deep recurrent NN and can be fine-tuned. Using VC experiments, we confirm the high performance of our method, especially in terms of objective criteria, relative to conventional VC methods such as approaches based on Gaussian mixture models and on NNs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {580–587},
numpages = {8},
keywords = {speaker specific features, voice conversion, deep Learning, recurrent neural network, recurrent temporal restricted Boltzmann machine (RTRBM)}
}

@article{10.1109/TASLP.2014.2387389,
author = {Obin, Nicolas and Lanchantin, Pierre},
title = {Symbolic Modeling of Prosody: From Linguistics to Statistics},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2387389},
doi = {10.1109/TASLP.2014.2387389},
abstract = {The assignment of prosodic events (accent and phrasing) from the text is crucial in text-to-speech synthesis systems. This paper addresses the combination of linguistic and metric constraints for the assignment of prosodic events in text-to-speech synthesis. First, a linguistic processing chain is used to provide a rich linguistic description of a text. Then, a novel statistical representation based on a hierarchical HMM (HHMM) is used to model the prosodic structure of a text: the root layer represents the text, each intermediate layer a sequence of intermediate phrases, the pre-terminal layer the sequence of accents, and the terminal layer the sequence of linguistic contexts. For each intermediate layer, a segmental HMM and information fusion are used to fuse the linguistic and metric constraints for the segmentation of a text into phrases. A set of experiments conducted on multi-speaker databases with various speaking styles reports that: the rich linguistic representation improves drastically the assignment of prosodic events, and the fusion of linguistic and metric constraints significantly improves over standard methods for the segmentation of a text into phrases. These constitute substantial advances that can be further used to model the speech prosody of a speaker, a speaking style, and emotions for text-to-speech synthesis.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {588–599},
numpages = {12},
keywords = {prosodic events, hierarchical HMMs, segmental HMMs, speaking style, speech prosody, surface/deep syntactic parsing, Dempster-Shafer fusion, text-to-speech synthesis}
}

@article{10.1109/TASLP.2015.2395254,
author = {Xiong, Deyi and Zhang, Min and Wang, Xing},
title = {Topic-Based Coherence Modeling for Statistical Machine Translation},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2395254},
doi = {10.1109/TASLP.2015.2395254},
abstract = {Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose topic-based coherence models to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. We build two topic-based coherence models on the predicted target coherence chain: 1) a word level coherence model that helps the decoder select coherent word translations and 2) a phrase level coherence model that guides the decoder to select coherent phrase translations. We integrate the two models into a state-of-the-art phrase-based machine translation system. Experiments on large-scale training data show that our coherence models achieve substantial improvements over both the baseline and models that are built on either document topics or sentence topics obtained under the assumption of direct topic correspondence between the source and target side. Additionally, further evaluations on translation outputs suggest that target translations generated by our coherence models are more coherent and similar to reference translations than those generated by the baseline.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {483–493},
numpages = {11},
keywords = {coherence chain, topic modeling, text coherence, text analysis, natural, statistical machine translation (SMT), language processing}
}

@article{10.1109/TASLP.2015.2389618,
author = {McLoughlin, Ian and Zhang, Haomin and Xie, Zhipeng and Song, Yan and Xiao, Wei},
title = {Robust Sound Event Classification Using Deep Neural Networks},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2389618},
doi = {10.1109/TASLP.2015.2389618},
abstract = {The automatic recognition of sound events by computers is an important aspect of emerging applications such as automated surveillance, machine hearing and auditory scene understanding. Recent advances in machine learning, as well as in computational models of the human auditory system, have contributed to advances in this increasingly popular research field. Robust sound event classification, the ability to recognise sounds under real-world noisy conditions, is an especially challenging task. Classification methods translated from the speech recognition domain, using features such as mel-frequency cepstral coefficients, have been shown to perform reasonably well for the sound event classification task, although spectrogram-based or auditory image analysis techniques reportedly achieve superior performance in noise. This paper outlines a sound event classification framework that compares auditory image front end features with spectrogram image-based front end features, using support vector machine and deep neural network classifiers. Performance is evaluated on a standard robust classification task in different levels of corrupting noise, and with several system enhancements, and shown to compare very well with current state-of-the-art classification techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {540–552},
numpages = {13},
keywords = {auditory event detection, machine hearing}
}

@article{10.5555/2817174.2876374,
author = {Fu, Ruiji and Guo, Jiang and Qin, Bing and Che, Wanxiang and Wang, Haifeng and Liu, Ting},
title = {Learning Semantic Hierarchies: A Continuous Vector Space Approach},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
abstract = {Semantic hierarchy construction aims to build structures of concepts linked by hypernym-hyponym ("is-a") relations. A major challenge for this task is the automatic discovery of such relations. This paper proposes a novel and effective method for the construction of semantic hierarchies based on continuous vector representation of words, named word embeddings, which can be used to measure the semantic relationship between words. We identify whether a candidate word pair has hypernym-hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74%, outperforms the state-of-the-art methods on a manually labeled test dataset. Moreover, combining our method with a previous manually built hierarchy extension method can further improve F-score to 80.29%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {461–471},
numpages = {11},
keywords = {piecewise linear projections, word embedding, semantic hierarchy}
}

@article{10.1109/TASLP.2014.2387413,
author = {Wang, Yow-Bang and Lee, Lin-shan},
title = {Supervised Detection and Unsupervised Discovery of Pronunciation Error Patterns for Computer-Assisted Language Learning},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2387413},
doi = {10.1109/TASLP.2014.2387413},
abstract = {Pronunciation error patterns (EPs) are patterns of mispronunciation frequently produced by language learners, and are usually different for different pairs of target and native languages. Accurate information of EPs can offer helpful feedbacks to the learners to improve their language skills. However, the major difficulty of EP detection comes from the fact that EPs are intrinsically similar to their corresponding canonical pronunciation, and different EPs corresponding to same canonical pronunciation are also intrinsically similar to each other. As a result, distinguishing EPs from their corresponding canonical pronunciation and between different EPs of the same phoneme is a difficult task-perhaps even more difficult than distinguishing between different phonemes in one language. On the other hand, the cost of deriving all EPs for each pair of target and native languages is high, usually requiring extensive expert knowledge or high-quality annotated data. Unsupervised EP discovery from a corpus of learner recordings would thus be an attractive addition to the field. In this paper, we propose new frameworks for both supervised EP detection and unsupervised EP discovery. For supervised EP detection, we use hierarchical multi-layer perceptrons (MLPs) as the EP classifiers to be integrated with the baseline using HMM/GMM in a two-pass Viterbi decoding architecture. Experimental results show that the new framework enhances the power of EP diagnosis. For unsupervised EP discovery we propose the first known framework, using the hierarchical agglomerative clustering (HAC) algorithm to explore sub-segmental variation within phoneme segments and produce fixed-length segment-level feature vectors in order to distinguish different EPs. We tested K-means (assuming a known number of EPs) and the Gaussian mixture model with the minimum description length principle (estimating an unknown number of EPs) for EP discovery. Preliminary experiments offered very encouraging results, although there is still a long way to go to approach the performance of human experts. We also propose to use the universal phoneme posteriorgram (UPP), derived from an MLP trained on corpora of mixed languages, as frame-level features in both supervised detection and unsupervised discovery of EPs. Experimental results show that using UPP not only achieves the best performance, but also is useful in analyzing the mispronunciation produced by language learners.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {564–579},
numpages = {16},
keywords = {error pattern detection, universal phoneme posteriorgram, computer-assisted language learning, error pattern discovery, computer-aided pronunciation training}
}

@article{10.1109/TASLP.2015.2400218,
author = {Sundermeyer, Martin and Ney, Hermann and Schl\"{u}ter, Ralf},
title = {From Feedforward to Recurrent LSTM Neural Networks for Language Modeling},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2400218},
doi = {10.1109/TASLP.2015.2400218},
abstract = {Language models have traditionally been estimated based on relative frequencies, using count statistics that can be extracted from huge amounts of text data. More recently, it has been found that neural networks are particularly powerful at estimating probability distributions over word sequences, giving substantial improvements over state-of-the-art count models. However, the performance of neural network language models strongly depends on their architectural structure. This paper compares count models to feedforward, recurrent, and long short-term memory (LSTM) neural network variants on two large-vocabulary speech recognition tasks. We evaluate the models in terms of perplexity and word error rate, experimentally validating the strong correlation of the two quantities, which we find to hold regardless of the underlying type of the language model. Furthermore, neural networks incur an increased computational complexity compared to count models, and they differently model context dependences, often exceeding the number of words that are taken into account by count based approaches. These differences require efficient search methods for neural networks, and we analyze the potential improvements that can be obtained when applying advanced algorithms to the rescoring of word lattices on large-scale setups.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {517–529},
numpages = {13},
keywords = {recurrent neural network (RNN), language modeling, Kneser-Ney smoothing, feedforward neural network, long short-term memory (LSTM)}
}

@article{10.1109/TASLP.2015.2405751,
author = {Banchs, Rafael E. and D'Haro, Luis F. and Li, Haizhou},
title = {Adequacy-Fluency Metrics: Evaluating MT in the Continuous Space Model Framework},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2405751},
doi = {10.1109/TASLP.2015.2405751},
abstract = {This work extends and evaluates a two-dimensional automatic evaluation metric for machine translation, which is designed to operate at the sentence level. The metric is based on the concepts of adequacy and fluency, aiming at decoupling both semantic and syntactic components of the translation process to provide a more balanced view on translation quality. These two elements are independently evaluated by using continuous space and n-gram language modeling frameworks, respectively. Two different implementations are evaluated: a monolingual version that fully operates on the target language side, and a cross-language version that has the main advantage of not requiring reference translations. Both implementations are evaluated by comparing their performance with state-of-the-art automatic metrics over a dataset involving five different European languages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {472–482},
numpages = {11},
keywords = {machine translation, system evaluation, natural language processing}
}

@article{10.1109/TASLP.2015.2393393,
author = {Zahoransky, Du\v{s}an and Polasek, Ivan},
title = {Text Search of Surnames in Some Slavic and Other Morphologically Rich Languages Using Rule Based Phonetic Algorithms},
year = {2015},
issue_date = {March 2015},
publisher = {IEEE Press},
volume = {23},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2393393},
doi = {10.1109/TASLP.2015.2393393},
abstract = {Surnames play a key role as person natural identifiers, essentially in present information systems. This paper deals with the topic of optimizing a phonetic search algorithm as a string matching of surnames usable for communications service providers, person registries, social networks or genealogy databases. It describes a proposed solution for the phonetic searching of Slovak and (territorial) neighboring languages (Czech, Polish, Ukrainian, Russian, German, Hungarian, Jewish) surnames. This solution was designed to improve search precision and recall when searching for people by their surnames originating in these languages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {553–563},
numpages = {11},
keywords = {algorithms, natural languages, information retrieval}
}

@article{10.1109/TASLP.2014.2387385,
author = {Hua, Guang and Goh, Jonathan and Thing, Vrizlynn L. L.},
title = {Time-Spread Echo-Based Audio Watermarking with Optimized Imperceptibility and Robustness},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2387385},
doi = {10.1109/TASLP.2014.2387385},
abstract = {We present a time-spread echo-based audio watermarking scheme with optimized imperceptibility and robustness. Specifically, convex optimization based finite-impulse-response (FIR) filter design is utilized to obtain the optimal echo filter coefficients. The desired power spectrum of the echo filter is shaped by the proposed maximum power spectral margin (MPSM) and the absolute threshold of hearing (ATH) of human auditory system (HAS) to ensure the optimal imperceptibility. Meanwhile, the auto-correlation function of the echo filter coefficients is specified as the constraint in the problem formulation, which controls the robustness in terms of watermark detection. In this way, a joint optimization of imperceptibility and robustness can be quantitatively performed. As a result, the proposed watermarking scheme is superior to existing solutions such as the ones based on pseudo noise (PN) sequence or modified pseudo noise (MPN) sequence. Note that the designed echo kernel is also highly secure in that only with the same filter coefficients can one successfully detect the watermark. Experimental results are provided to evaluate the imperceptibility and robustness of the proposed watermarking scheme.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {227–239},
numpages = {13},
keywords = {time-spread echo, FIR filter design, convex optimization, audio watermarking}
}

@article{10.1109/TASLP.2014.2381881,
author = {Zhang, Mengqiu and Kennedy, Rodney A. and Abhayapala, Thushara D.},
title = {Empirical Determination of Frequency Representation in Spherical Harmonics-Based HRTF Functional Modeling},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2381881},
doi = {10.1109/TASLP.2014.2381881},
abstract = {This paper considers a functional modeling of a head-related transfer function (HRTF) where the spatial-portion is constrained to be expanded using spherical harmonics and the frequency-portion is expanded in terms of standard closed-form orthonormal functions, which may be user selected. We derive an objective evaluation metric to compare the relative energy efficiencies of candidate functional models using empirical HRTF database measurements and robust estimation techniques. Among four sets of closed-form orthonormal functions the complex exponentials are identified as the most efficient to represent the frequency-portion in the spherical harmonics-based HRTF functional model. The proposed model is evaluated across three HRTF data sets: 1) CIPIC database, 2) the MIT KEMAR (Knowles Electronics Mannequin for Acoustics Research) database, and 3) the ANU KEMAR database.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {351–360},
numpages = {10},
keywords = {functional modeling, dimensionality, series expansion theory, head-related transfer function (HRTF)}
}

@article{10.1109/TASLP.2014.2387414,
author = {Hirayama, Naoki and Yoshino, Koichiro and Itoyama, Katsutoshi and Mori, Shinsuke and Okuno, Hiroshi G.},
title = {Automatic Speech Recognition for Mixed Dialect Utterances by Mixing Dialect Language Models},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2387414},
doi = {10.1109/TASLP.2014.2387414},
abstract = {This paper presents an automatic speech recognition (ASR) system that accepts a mixture of various kinds of dialects. The system recognizes dialect utterances on the basis of the statistical simulation of vocabulary transformation and combinations of several dialect models. Previous dialect ASR systems were based on handcrafted dictionaries for several dialects, which involved costly processes. The proposed system statistically trains transformation rules between a common language and dialects, and simulates a dialect corpus for ASR on the basis of a machine translation technique. The rules are trained with small sets of parallel corpora to make up for the lack of linguistic resources on dialects. The proposed system also accepts mixed dialect utterances that contain a variety of vocabularies. In fact, spoken language is not a single dialect but a mixed dialect that is affected by the circumstances of speakers' backgrounds (e.g., native dialects of their parents or where they live). We addressed two methods to combine several dialects appropriately for each speaker. The first was recognition with language models of mixed dialects with automatically estimated weights that maximized the recognition likelihood. This method performed the best, but calculation was very expensive because it conducted grid searches of combinations of dialect mixing proportions that maximized the recognition likelihood. The second was integration of results of recognition from each single dialect language model. The improvements with this model were slightly smaller than those with the first method. Its calculation cost was, however, inexpensive and it worked in real-time on general workstations. Both methods achieved higher recognition accuracies for all speakers than those with the single dialect models and the common language model, and we could choose a suitable model for use in ASR that took into consideration the computational costs and recognition accuracies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {373–382},
numpages = {10},
keywords = {mixture of dialects, corpus simulation, speech recognition}
}

@article{10.1109/TASLP.2014.2331102,
author = {Molina, Emilio and Tard\'{o}n, Lorenzo J. and Barbancho, Ana M. and Barbancho, Isabel},
title = {SiPTH: Singing Transcription Based on Hysteresis Defined on the Pitch-Time Curve},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2331102},
doi = {10.1109/TASLP.2014.2331102},
abstract = {In this paper, we present a method for monophonic singing transcription based on hysteresis defined on the pitch-time curve. This method is designed to perform note segmentation even when the pitch evolution during the same note behaves unstably, as in the case of untrained singers. The selected approach estimates the regions in which the chroma is stable, these regions are classified as voiced or unvoiced according to a decision tree classifier using two descriptors based on aperiodicity and power. Then, a note segmentation stage based on pitch intervals of the sung signal is carried out. To this end, a dynamic averaging of the pitch curve is performed after the beginning of a note is detected in order to roughly estimate the pitch. Deviations of the actual pitch curve with respect to this average are measured to determine the next note change according to a hysteresis process defined on the pitchtime curve. Finally, each note is labeled using three single values: rounded pitch (to semitones), duration and volume. Also, a complete evaluation methodology that includes the definition of different relevant types of errors, measures and a method for the computation of the evaluation measures are presented. The proposed system improves significantly the performance of the baseline approach, and attains results similar to previous approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {252–263},
numpages = {12},
keywords = {acoustic signal processing, singing voice analysis, singing transcription, pitch, fundamental frequency}
}

@article{10.1109/TASLP.2014.2372342,
author = {Schwartz, Boaz and Gannot, Sharon and Habets, Emanu\"{e}l A. P.},
title = {Online Speech Dereverberation Using Kalman Filter and EM Algorithm},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2372342},
doi = {10.1109/TASLP.2014.2372342},
abstract = {Speech signals recorded in a room are commonly degraded by reverberation. In most cases, both the speech signal and the acoustic system of the room are unknown and time-varying. In this paper, a scenario with a single desired sound source and slowly time-varying and spatially-white noise is considered, and a multi-microphone algorithm that simultaneously estimates the clean speech signal and the time-varying acoustic system is proposed. The recursive expectation-maximization scheme is employed to obtain both the clean speech signal and the acoustic system in an online manner. In the expectation step, the Kalman filter is applied to extract a new sample of the clean signal, and in the maximization step, the system estimate is updated according to the output of the Kalman filter. Experimental results show that the proposed method is able to significantly reduce reverberation and increase the speech quality. Moreover, the tracking ability of the algorithm was validated in practical scenarios using human speakers moving in a natural manner.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {394–406},
numpages = {13},
keywords = {dereverberation, recursive parameter estimation, convolution in STFT, recursive expectation-maximization}
}

@article{10.1109/TASLP.2014.2381882,
author = {Bi\c{c}ici, Ergun and Yuret, Deniz},
title = {Optimizing Instance Selection for Statistical Machine Translation with Feature Decay Algorithms},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2381882},
doi = {10.1109/TASLP.2014.2381882},
abstract = {We introduce FDA5 for efficient parameterization, optimization, and implementation of feature decay algorithms (FDA), a class of instance selection algorithms that use feature decay. FDA increase the diversity of the selected training set by devaluing features (i.e., n-grams) that have already been included. FDA5 decides which instances to select based on three functions used for initializing and decaying feature values and scaling sentence scores controlled with five parameters. We present optimization techniques that allow FDA5 to adapt these functions to in-domain and out-of-domain translation tasks for different language pairs. In a transductive learning setting, selection of training instances relevant to the test set can improve the final translation quality. In machine translation experiments performed on the 2 million sentence English-German section of the Europarl corpus, we show that a subset of the training set selected by FDA5 can gain up to 3.22 BLEU points compared to a randomly selected subset of the same size, can gain up to 0.41 BLEU points compared to using all of the available training data using only 15% of it, and can reach within 0.5 BLEU points to the full training set result by using only 2.7% of the full training data. FDA5 peaks at around 8M words or 15% of the full training set. In an active learning setting, FDA5 minimizes the human effort by identifying the most informative sentences for translation and FDA gains up to 0.45 BLEU points using 3/5 of the available training data compared to using all of it and 1.12 BLEU points compared to random training set. In translation tasks involving English and Turkish, a morphologically rich language, FDA5 can gain up to 11.52 BLEU points compared to a randomly selected subset of the same size, can achieve the same BLEU score using as little as 4% of the data compared to random instance selection, and can exceed the full dataset result by 0.78 BLEU points. FDA5 is able to reduce the time to build a statistical machine translation system to about half with 1M words using only 3% of the space for the phrase table and 8% of the overall space when compared with a baseline system using all of the training data available yet still obtain only 0.58 BLEU points difference with the baseline system in out-of-domain translation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {339–350},
numpages = {12},
keywords = {transductive learning, information retrieval, domain adaptation, machine translation, instance selection}
}

@article{10.1109/TASLP.2014.2372335,
author = {Schwartz, Ofer and Gannot, Sharon and Habets, Emanu\"{e}l A. P.},
title = {Multi-Microphone Speech Dereverberation and Noise Reduction Using Relative Early Transfer Functions},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2372335},
doi = {10.1109/TASLP.2014.2372335},
abstract = {In speech communication systems, the microphone signals are degraded by reverberation and ambient noise. The reverberant speech can be separated into two components, namely, an early speech component that includes the direct path and some early reflections, and a late reverberant component that includes all the late reflections. In this paper, a novel algorithm to simultaneously suppress early reflections, late reverberation and ambient noise is presented. A multi-microphone minimum mean square error estimator is used to obtain a spatially filtered vaersion of the early speech component. The estimator constructed as a minimum variance distortionless response (MVDR) beam-former (BF) followed by a postfilter (PF). Three unique design features characterize the proposed method. First, the MVDR BF is implemented in a special structure, named the nonorthogonal generalized sidelobe canceller (NO-GSC). Compared with the more conventional orthogonal GSC structure, the new structure allows for a simpler implementation of the GSC blocks for various MVDR constraints. Second, In contrast to earlier works, RETFs are used in the MVDR criterion rather than either the entire RTFs or only the direct-path of the desired speech signal. An estimator of the RETFs is proposed as well. Third, the late reverberation and noise are processed by both the beamforming stage and the PF stage. Since the relative power of the noise and the late reverberation varies with the frame index, a computationally efficient method for the required matrix inversion is proposed to circumvent the cumbersome mathematical operation. The algorithm was evaluated and compared with two alternative multichannel algorithms and one single-channel algorithm using simulated data and data recorded in a room with a reverberation time of 0.5 s for various source-microphone array distances (1-4 m) and several signal-to-noise levels. The processed signals were tested using two commonly used objective measures, namely perceptual evaluation of speech quality and log-spectral distance. As an additional objective measure, the improvement in word accuracy percentage of an acoustic speech recognition system is also demonstrated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {240–251},
numpages = {12},
keywords = {relative transfer function, generalized sidelobe canceller, dereverberation, multichannel Wiener filter, minimum variance distortionless response (MVDR) beamforming}
}

@article{10.1109/TASLP.2014.2363589,
author = {Matsuyama, Yoichi and Saito, Akihiro and Fujie, Shinya and Kobayashi, Tetsunori},
title = {Automatic Expressive Opinion Sentence Generation for Enjoyable Conversational Systems},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2363589},
doi = {10.1109/TASLP.2014.2363589},
abstract = {In terms of functional conversations, Grice's Maxim of Quantity suggests that responses should contain no more information than was explicitly asked for. However, in our daily conversations, more informative response skills are usually employed in order to hold enjoyable conversations with interlocutors. These responses are usually produced as forms of one's additional opinions, which usually contain their original viewpoints as well as novel means of expression, rather than simple and common responses characteristic of the general public. In this paper, we propose automatic expressive opinion sentence generation mechanisms for enjoyable conversational systems. The generated opinions are extracted from a large number of reviews on the web, and ranked in terms of contextual relevance, length of sentences, and amount of information represented by the frequency of adjectives. The sentence generator also has an additional phrasing skill. Three controlled lab experiments were conducted, where subjects were requested to read generated sentences and watch videos filmed about conversations between the robot and a person. The results implied that mechanisms effectively promote users' enjoyment and interests.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {313–326},
numpages = {14},
keywords = {natural sentence generation, opinion generation, conversational robots, question answering}
}

@article{10.1109/TASLP.2014.2381871,
author = {Feng, Zu-Ren and Zhou, Qing and Zhang, Jun and Jiang, Ping and Yang, Xue-Wen},
title = {A Target Guided Subband Filter for Acoustic Event Detection in Noisy Environments Using Wavelet Packets},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2381871},
doi = {10.1109/TASLP.2014.2381871},
abstract = {This paper deals with acoustic event detection (AED), such as screams, gunshots, and explosions, in noisy environments. The main aim is to improve the detection performance under adverse conditions with a very low signal-to-noise ratio (SNR). A novel filtering method combined with an energy detector is presented. The wavelet packet transform (WPT) is first used for time-frequency representation of the acoustic signals. The proposed filter in the wavelet packet domain then uses a priori knowledge of the target event and an estimate of noise features to selectively suppress the background noise. It is in fact a content-aware band-pass filter which can automatically pass the frequency bands that are more significant in the target than in the noise. Theoretical analysis shows that the proposed filtering method is capable of enhancing the target content while suppressing the background noise for signals with a low SNR. A condition to increase the probability of correct detection is also obtained. Experiments have been carried out on a large dataset of acoustic events that are contaminated by different types of environmental noise and white noise with varying SNRs. Results show that the proposed method is more robust and better adapted to noise than ordinary energy detectors, and it can work even with an SNR as low as 15 dB. A practical system for real time processing and multi-target detection is also proposed in this work.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {361–372},
numpages = {12},
keywords = {filter, background noise, wavelet packets, acoustic event detection (AED)}
}

@article{10.1109/TASLP.2014.2384279,
author = {Sugiura, Ryosuke and Kamamoto, Yutaka and Harada, Noboru and Kameoka, Hirokazu and Moriya, Takehiro},
title = {Resolution Warped Spectral Representation for Low-Delay and Low-Bit-Rate Audio Coder},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2384279},
doi = {10.1109/TASLP.2014.2384279},
abstract = {We have devised a high-quality frequency-domain audio coder based on the state-of-the-art monaural wide-band coder aiming at its use in low-delay and low-bit-rate conditions. The coder efficiently represents frequency spectral envelopes of the target signals with low computational complexity using optimally prepared non-negative sparse matrices. The experimental results reveal that this representation has positive effects on the objective and subjective quality of the coder resulting in the comparable quality to the same bit rate of 3GPP Extended Adaptive Multi-Rate WideBand (AMR-WB+), a coder which permits more than four times longer delay compared with the proposed coder. Consequently, this coder is suitable for applications in mobile communications, which require low delay and low complexity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {288–299},
numpages = {12},
keywords = {line spectrum pairs, frequency warping, low delay, audio compression, transform coding excitation, linear predictive coding, non-negative matrix}
}

@article{10.1109/TASLP.2014.2365992,
author = {Schasse, Alexander and Gerkmann, Timo and Martin, Rainer and S\"{o}rgel, Wolfgang and Pilgrim, Thomas and Puder, Henning},
title = {Two-Stage Filter-Bank System for Improved Single-Channel Noise Reduction in Hearing Aids},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2365992},
doi = {10.1109/TASLP.2014.2365992},
abstract = {The filter-bank system implemented in hearing aids has to fulfill various constraints such as low latency and high stop-band attenuation, usually at the cost of low frequency resolution. In the context of frequency-domain noise-reduction algorithms, insufficient frequency resolution may lead to annoying residual noise artifacts since the spectral harmonics of the speech cannot properly be resolved. Especially in case of female speech signals, the noise between the spectral harmonics causes a distinct roughness of the processed signals. Therefore, this work proposes a two-stage filter-bank system, such that the frequency resolution can be improved for the purpose of noise reduction, while the original first-stage hearing-aid filter-bank system can still be used for compression and amplification. We also propose methods to implement the second filter-bank stage with little additional algorithmic delay. Furthermore, the computational complexity is an important design criterion. This finally leads to an application of the second filter-bank stage to lower frequency bands only, resulting in the ability to resolve the harmonics of speech. The paper presents a systematic description of the second filter-bank stage, discusses its influence on the processed signals in detail and further presents the results of a listening test which indicates the improved performance compared to the original single-stage filter-bank system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {383–393},
numpages = {11},
keywords = {single-channel noise reduction, low delay processing, cascaded filter-bank system, hearing aids}
}

@article{10.1109/TASLP.2014.2381931,
author = {Weng, Chao and Juang, Biing-Hwang},
title = {Discriminative Training Using Non-Uniform Criteria for Keyword Spotting on Spontaneous Speech},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2381931},
doi = {10.1109/TASLP.2014.2381931},
abstract = {In this work, we formulate the problem of keyword spotting as a non-uniform error automatic speech recognition (ASR) problem and propose a model training methodology based on the non-uniform minimum classification error (MCE) approach. The main idea is to adapt the fundamental MCE criteria to reflect the cost-sensitive notion in that errors on keywords are much more significant than errors on non-keywords in an automatic speech recognition task. The notion of cost sensitivity leads to emphasis of keyword models in parameter optimization. Then we present a system which takes advantage of the weighted finite-state transducer (WFST) framework to efficiently implement the non-uniform MCE. To enhance the approach of non-uniform error cost minimization for keyword spotting, we further formulate a technique called "adaptive boosted non-uniform MCE" which incorporates the idea of boosting. We validate the proposed framework on two challenging large-scale spontaneous conversational telephone speech (CTS) datasets in two different languages (English and Mandarin). Experimental results show our framework can achieve consistent and significant spotting performance gains over both the maximum likelihood estimation (MLE) baseline and conventional discriminatively-trained systems with uniform error cost.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {300–312},
numpages = {13},
keywords = {minimum classification error (MCE), discriminative training (DT), non-uniform criteria, weighted finite-state transducer (WFST), keyword spotting}
}

@article{10.1109/TASLP.2014.2387388,
author = {Arora, Vipul and Behera, Laxmidhar},
title = {Multiple F0 Estimation and Source Clustering of Polyphonic Music Audio Using PLCA and HMRFs},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2387388},
doi = {10.1109/TASLP.2014.2387388},
abstract = {Source transcription of pitched polyphonic music entails providing the pitch (F0) values corresponding to each source in a separate channel. This problem is an important step towards many important problems in music and speech processing. It involves 1) estimating the multiple F0 values in each short time frame, and 2) clustering the F0 values into streams corresponding to different sources. We address the problem in an unsupervised way, with only the total number of sources given beforehand. The framework of probabilistic latent component analysis (PLCA) is used to decompose the polyphonic short-time magnitude spectra for multiple F0 estimation and source-specific feature extraction. It is further embedded into the structure of hidden Markov random fields (HMRF) for clustering the F0s into different sources. This clustering is constrained by the cognitive grouping of continuous F0 contours as well as segregation of simultaneous F0s into different source streams. Such constraints are effectively and elegantly modeled by the HMRF's. Simulated annealing varies the degree of constraints for better clustering. The paper also proposes a novel strategy using the trade-off between precision and recall of multiple F0 estimation for better clustering. Evaluations over a variety of datasets show the efficacy of the proposed algorithm and its robustness to the presence of spurious F0s while clustering. It also outperforms a state-of-the-art unsupervised source streaming algorithm in a set of comparative experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {278–287},
numpages = {10},
keywords = {hidden Markov random fields, multiple F0 estimation, acoustic scene analysis, automatic music transcription, polyphonic instrument identification}
}

@article{10.1109/TASLP.2014.2387382,
author = {Wang, Haipeng and Lee, Tan and Leung, Cheung-Chi and Ma, Bin and Li, Haizhou},
title = {Acoustic Segment Modeling with Spectral Clustering Methods},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2387382},
doi = {10.1109/TASLP.2014.2387382},
abstract = {This paper presents a study of spectral clustering-based approaches to acoustic segment modeling (ASM). ASM aims at finding the underlying phoneme-like speech units and building the corresponding acoustic models in the unsupervised setting, where no prior linguistic knowledge and manual transcriptions are available. A typical ASM process involves three stages, namely initial segmentation, segment labeling, and iterative modeling. This work focuses on the improvement of segment labeling. Specifically, we use posterior features as the segment representations, and apply spectral clustering algorithms on the posterior representations. We propose a Gaussian component clustering (GCC) approach and a segment clustering (SC) approach. GCC applies spectral clustering on a set of Gaussian components, and SC applies spectral clustering on a large number of speech segments. Moreover, to exploit the complementary information of different posterior representations, a multiview segment clustering (MSC) approach is proposed. MSC simultaneously utilizes multiple posterior representations to cluster speech segments. To address the computational problem of spectral clustering in dealing with large numbers of speech segments, we use inner product similarity graph and make reformulations to avoid the explicit computation of the affinity matrix and Laplacian matrix. We carried out two sets of experiments for evaluation. First, we evaluated the ASM accuracy on the OGI-MTS dataset, and it was shown that our approach could yield 18.7% relative purity improvement and 15.1% relative NMI improvement compared with the baseline approach. Second, we examined the performances of our approaches in the real application of zero-resource query-by-example spoken term detection on SWS2012 dataset, and it was shown that our approaches could provide consistent improvement on four different testing scenarios with three evaluation metrics.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {264–277},
numpages = {14},
keywords = {zero-resource query-by-example spoken term detection, unsupervised training, acoustic segment modeling, sub-word unit discovery, multiview segment clustering}
}

@article{10.1109/TASLP.2014.2384271,
author = {Petkov, Petko N. and Kleijn, W. Bastiaan},
title = {Spectral Dynamics Recovery for Enhanced Speech Intelligibility in Noise},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2384271},
doi = {10.1109/TASLP.2014.2384271},
abstract = {Speech intelligibility in noisy environments decreases with an increase in the noise power. We hypothesize that the differences of subsequent short-term spectra of speech, which we collectively refer to as the speech spectral dynamics, can be used to characterize speech intelligibility. We propose a distortion measure to characterize the deviation of the dynamics of the noisy modified speech from the dynamics of natural speech. Optimizing this distortion measure, we derive a parametric relationship between the signal band-power before and after modification. The parametric nature of the solution ensures adaptation to the noise level, the speech statistics and a penalty on the power gain. A multi-band speech modification system based on the single-band optimal solution is designed under a total signal power constraint and evaluated in selected noise conditions. The results indicate that the proposed approach compares favorably to a reference method based on optimizing a measure of the speech intelligibility index. Very low computational complexity and high intelligibility gain make this an attractive approach for speech modification in a wide range of application scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {327–338},
numpages = {12},
keywords = {environment adaptation, speech intelligibility enhancement, speech modification}
}

@article{10.1109/TASLP.2014.2384274,
author = {Gerazov, Branislav and Ivanovski, Zoran},
title = {Kernel Power Flow Orientation Coefficients for Noise-Robust Speech Recognition},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2384274},
doi = {10.1109/TASLP.2014.2384274},
abstract = {Noise-robustness has become a crucial parameter in Automatic Speech Recognition (ASR) systems today with their increased use in noise-filled real-world environments. One way to address this issue is to develop features that are innately noise-robust. The Kernel Power flow Orientation Coefficients (KPOCs) are a novel feature set based on spectro-temporal analysis that uses a bank of 2D kernels to extract the dominant orientation of the power flow at each point in the auditory spectrogram of the speech signal. The collection of dominant power flow orientation angles forms a novel representation of the speech signal named the Power flow Orientation Spectrogram (POS), which is innately resistant to the spectral masking introduced by the presence of noise and reverberation. This approach not only grants KPOC its noise robustness, but also keeps the number of output coefficients inherently small, thus eliminating the need of the feature dimensionality reduction otherwise necessary in the conventional the spectro-temporal approach. KPOCs performance has been evaluated on three experimental frameworks, and the results have shown that they outperform a number of well-known noise-robust features for average and low SNRs. The relative improvement in Word Recognition Accuracy (WRA) to the classic Mel Frequency Cepstral Coefficients (MFCCs) for the Aurora 2 task goes from 32% up to 190% for SNRs in the range from 10 down to 5 dB. The experimental results also show that in clean training the performance of KPOC approaches that of the state-of-the-art noise-robust ASR frontends in all noise scenarios for small vocabulary ASR tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {407–419},
numpages = {13},
keywords = {speech recognition, spectro-temporal, 2D kernels, features, noise-robust}
}

@article{10.1109/TASLP.2014.2375558,
author = {Zhang, Zixing and Coutinho, Eduardo and Deng, Jun and Schuller, Bj\"{o}rn},
title = {Cooperative Learning and Its Application to Emotion Recognition from Speech},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2375558},
doi = {10.1109/TASLP.2014.2375558},
abstract = {In this paper, we propose a novel method for highly efficient exploitation of unlabeled data--Cooperative Learning. Our approach consists of combining Active Learning and Semi-Supervised Learning techniques, with the aim of reducing the costly effects of human annotation. The core underlying idea of Cooperative Learning is to share the labeling work between human and machine efficiently in such a way that instances predicted with insufficient confidence value are subject to human labeling, and those with high confidence values are machine labeled. We conducted various test runs on two emotion recognition tasks with a variable number of initial supervised training instances and two different feature sets. The results show that Cooperative Learning consistently outperforms individual Active and Semi-Supervised Learning techniques in all test cases. In particular, we show that our method based on the combination of Active Learning and Co-Training leads to the same performance of a model trained on the whole training set, but using 75% fewer labeled instances. Therefore, our method efficiently and robustly reduces the need for human annotations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {115–126},
numpages = {12},
keywords = {cooperative learning, supervised learning, multi-view learning, acoustics, semi-supervised learning, active learning, emotion recognition}
}

@article{10.1109/TASLP.2014.2367822,
author = {Poignant, Johann and Besacier, Laurent and Qu\'{e}not, Georges},
title = {Unsupervised Speaker Identification in TV Broadcast Based on Written Names},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2367822},
doi = {10.1109/TASLP.2014.2367822},
abstract = {Identifying speakers in TV broadcast in an unsupervised way (i.e., without biometric models) is a solution for avoiding costly annotations. Existing methods usually use pronounced names, as a source of names, for identifying speech clusters provided by a diarization step but this source is too imprecise for having sufficient confidence. To overcome this issue, another source of names can be used: the names written in a title block in the image track. We first compared these two sources of names on their abilities to provide the name of the speakers in TV broadcast. This study shows that it is more interesting to use written names for their high precision for identifying the current speaker. We also propose two approaches for finding speaker identity based only on names written in the image track. With the "late naming" approach, we propose different propagations of written names onto clusters. Our second proposition, "Early naming," modifies the speaker diarization module (agglomerative clustering) by adding constraints preventing two clusters with different associated written names to be merged together. These methods were tested on the REPERE corpus phase 1, containing 3 hours of annotated videos. Our best "late naming" system reaches an F-measure of 73.1%. "early naming" improves over this result both in terms of identification error rate and of stability of the clustering stopping criterion. By comparison, a mono-modal, supervised speaker identification system with 535 speaker models trained on matching development data and additional TV and radio data only provided a 57.2% F-measure.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {57–68},
numpages = {12},
keywords = {TV broadcast, speaker diarization, written names, multimodal fusion, speaker identification}
}

@article{10.1109/TASLP.2015.2390431,
author = {Li, Haizhou},
title = {Inaugural Editorial: Embracing New Opportunities for Growth},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2390431},
doi = {10.1109/TASLP.2015.2390431},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {5–6},
numpages = {2}
}

@article{10.1109/TASLP.2014.2367817,
author = {Wu, Yuntao and Amir, Leshem and Jensen, Jesper Rindom and Liao, Guisheng},
title = {Joint Pitch and DOA Estimation Using the ESPRIT Method},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2367817},
doi = {10.1109/TASLP.2014.2367817},
abstract = {In this paper, the problem of joint multi-pitch and direction-of-arrival (DOA) estimation for multichannel harmonic sinusoidal signals is considered. A spatio-temporal matrix signal model for a uniform linear array is defined, and then the ESPRIT method based on subspace techniques that exploits the invariance property in the time domain is first used to estimate the multi pitch frequencies of multiple harmonic signals. Followed by the estimated pitch frequencies, the DOA estimations based on the ESPRIT method are also presented by using the shift invariance structure in the spatial domain. Compared to the existing state-of-the-art algorithms, the proposed method based on ESPRIT without 2-D searching is computationally more efficient but performs similarly. An asymptotic performance analysis of the DOA and pitch estimation of the proposed method are also presented. Finally, the effectiveness of the proposed method is illustrated on a synthetic signal as well as real-life recorded data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {32–45},
numpages = {14},
keywords = {ESPRIT, pitch estimation, DOA estimation, performance analysis}
}

@article{10.1109/TASLP.2014.2377583,
author = {Jensen, Jesper Rindom and Christensen, Mads Gr\ae{}sb\o{}l and Benesty, Jacob and Jensen, S\o{}ren Holdt},
title = {Joint Spatio-Temporal Filtering Methods for DOA and Fundamental Frequency Estimation},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2377583},
doi = {10.1109/TASLP.2014.2377583},
abstract = {In this paper, spatio-temporal filtering methods are proposed for estimating the direction-of-arrival (DOA) and fundamental frequency of periodic signals, like those produced by the speech production system and many musical instruments using microphone arrays. This topic has quite recently received some attention in the community and is quite promising for several applications. The proposed methods are based on optimal, adaptive filters that leave the desired signal, having a certain DOA and fundamental frequency, undistorted and suppress everything else. The filtering methods simultaneously operate in space and time, whereby it is possible resolve cases that are otherwise problematic for pitch estimators or DOA estimators based on beamforming. Several special cases and improvements are considered, including a method for estimating the covariance matrix based on the recently proposed iterative adaptive approach (IAA). Experiments demonstrate the improved performance of the proposed methods under adverse conditions compared to the state of the art using both synthetic signals and real signals, as well as illustrate the properties of the methods and the filters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {174–185},
numpages = {12},
keywords = {fundamental frequency estimation, LCMV beamformer, DOA estimation, periodogram-based beamformer, joint estimation, 2-D filtering}
}

@article{10.1109/TASLP.2014.2364452,
author = {Xu, Yong and Du, Jun and Dai, Li-Rong and Lee, Chin-Hui},
title = {A Regression Approach to Speech Enhancement Based on Deep Neural Networks},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2364452},
doi = {10.1109/TASLP.2014.2364452},
abstract = {In contrast to the conventional minimum mean square error (MMSE)-based noise reduction techniques, we propose a supervised method to enhance speech by means of finding a mapping function between noisy and clean speech signals based on deep neural networks (DNNs). In order to be able to handle a wide range of additive noises in real-world situations, a large training set that encompasses many possible combinations of speech and noise types, is first designed. A DNN architecture is then employed as a nonlinear regression function to ensure a powerful modeling capability. Several techniques have also been proposed to improve the DNN-based speech enhancement system, including global variance equalization to alleviate the over-smoothing problem of the regression model, and the dropout and noise-aware training strategies to further improve the generalization capability of DNNs to unseen noise conditions. Experimental results demonstrate that the proposed framework can achieve significant improvements in both objective and subjective measures over the conventional MMSE based technique. It is also interesting to observe that the proposed DNN approach can well suppress highly nonstationary noise, which is tough to handle in general. Furthermore, the resulting DNN model, trained with artificial synthesized data, is also effective in dealing with noisy speech data recorded in real-world scenarios without the generation of the annoying musical artifact commonly observed in conventional enhancement methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {7–19},
numpages = {13},
keywords = {non-stationary noise, noise aware training, deep neural networks (DNNs), global variance equalization, speech enhancement, noise reduction, dropout}
}

@article{10.1109/TASLP.2014.2375575,
author = {Rakotomamonjy, Alain and Gasso, Gilles},
title = {Histogram of Gradients of Time-Frequency Representations for Audio Scene Classification},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2375575},
doi = {10.1109/TASLP.2014.2375575},
abstract = {This abstract presents our entry to the Detection and Classification of Acoustic Scenes challenge. The approach we propose for classifying acoustic scenes is based on transforming the audio signal into a time-frequency representation and then in extracting relevant features about shapes and evolutions of time-frequency structures. These features are based on histogram of gradients that are subsequently fed to amulti-class linear support vector machines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {142–153},
numpages = {12},
keywords = {histogram of gradient, constant Q transform, support vector machines}
}

@article{10.1109/TASLP.2014.2372313,
author = {Ahani, Soodeh and Ghaemmaghami, Shahrokh and Wang, Z. Jane},
title = {A Sparse Representation-Based Wavelet Domain Speech Steganography Method},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2372313},
doi = {10.1109/TASLP.2014.2372313},
abstract = {In this paper, we present a novel speech steganography method using discrete wavelet transform and sparse decomposition to address the undetectability concern in speech steganography. The proposed speech steganography method exploits the sparse representation to embed secret messages into higher semantic levels of the cover signal, resulting in increased undetectability. The proposed method also yields improvements on both stego signal quality and embedding capacity, which are the two major requirements of a steganography algorithm. Our experimental results illustrate that the stego signals generated by the proposed method are perceptually indistinguishable from the original cover signals, quantified by both SNR and PESQ quality measures. When compared with two well-known steganography methods, the proposed method is shown to be superior on addressing major requirements of a steganography algorithm, imperceptibility, undetectability, and capacity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {80–91},
numpages = {12},
keywords = {dictionary learning, speech steganography, discrete wavelet transform, data hiding, sparse representation}
}

@article{10.1109/TASLP.2014.2372901,
author = {Su, Rongfeng and Liu, Xunying and Wang, Lan},
title = {Automatic Complexity Control of Generalized Variable Parameter HMMs for Noise Robust Speech Recognition},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2372901},
doi = {10.1109/TASLP.2014.2372901},
abstract = {An important part of the acoustic modelling problem for automatic speech recognition (ASR) systems is to handle the mismatch against a target environment created by time-varying external factors such as ambient noise. One possible solution to this problem is to introduce controllability to the underlying acoustic model to allow an instantaneous adaptation to the underlying noise condition. Along this line, the continuous trajectory of optimal, well matched model parameters against the varying noise can be explicitly modelled using, for example, generalized variable parameter HMMs (GVP-HMM). In order to improve the generalization and computational efficiency of conventional GVP-HMMs, this paper investigates a novel model complexity control method for GVP-HMMs. The optimal polynomial degrees of Gaussian mean, variance and model space linear transform trajectories are automatically determined at local level. Significant error rate reductions of 20% and 28% relative were obtained over the multi-style training baseline systems on Aurora 2 and a medium vocabulary Mandarin Chinese speech recognition task respectively. Consistent performance improvements and model size compression of 60% relative were also obtained over the baseline GVP-HMM systems using a uniformly assigned polynomial degree.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {102–114},
numpages = {13},
keywords = {generalized variable parameter HMMs, robust speech recognition, complexity control, variable noise}
}

@article{10.1109/TASLP.2014.2377595,
author = {Mart\'{\i}nez-Hinarejos, Carlos D. and Bened\'{\i}, Jos\'{e}-Miguel and Tamarit, Vicent},
title = {Unsegmented Dialogue Act Annotation and Decoding with N-Gram Transducers},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2377595},
doi = {10.1109/TASLP.2014.2377595},
abstract = {Most studies on dialogue corpora, as well as most dialogue systems, employ dialogue acts as the basic units for interpreting discourse structure, user input and system actions. The definition of the discourse structure and the dialogue strategy consequently require the tagging of dialogue corpora in terms of dialogue acts. The tagging problem presents two basic variants: a batch variant (annotation of whole dialogues, in order to define dialogue strategy or study discourse structure) and an online variant (decoding of the dialogue act sequence of a given turn, in order to interpret user intentions). In the two variants is unusual having the segmentation of each turn into the dialogue meaningful units (segments) to which a dialogue act is assigned. In this paper we present the use of the N-Gram Transducer technique for tagging dialogues, without needing to provide a prior segmentation, in these two different variants (dialogue annotation and turn decoding). Experiments were performed in two corpora of different nature and results show that N-Gram Transducer models are suitable for these tasks and provide good performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {198–211},
numpages = {14},
keywords = {spoken dialogue systems, dialogue annotation, n-gram transducer}
}

@article{10.1109/TASLP.2014.2371544,
author = {Tong, Renjie and Zhou, Yingyue and Zhang, Long and Bao, Guangzhao and Ye, Zhongfu},
title = {A Robust Time-Frequency Decomposition Model for Suppression of Mixed Gaussian-Impulse Noise in Audio Signals},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2371544},
doi = {10.1109/TASLP.2014.2371544},
abstract = {In this paper, we propose a robust time-frequency decomposition (RTFD) model to restore audio signals degraded by sparse impulse noise mixed with small dense Gaussian noise. This kind of noise is very common especially in old-time recordings. The proposed RTFD model is based on the observation that these degraded audio signals mainly contain four parts, i.e., the quasi-periodic and voiced part, the aperiodic and transient part, the arbitrarily large impulse noise and the small dense Gaussian noise. Sparsity and local correlations of corresponding parts are exploited to solve the RTFD model. We also heuristically develop a discriminative orthogonal matching pursuit (DOMP) algorithm to more precisely estimate sparse representing vectors. Specifically, the DOMP algorithm divides the whole atom set into two subsets, i.e., the active subset and the passive subset. Atoms in two subsets are treated discriminatively since sparsity regularization terms are not equally weighted. Based on RTFD and DOMP, we have developed two algorithms, i.e., the fidelity-oriented algorithm and the articulation-oriented algorithm. The proposed algorithms achieve considerable performance on both synthetic and real noisy signals. Results show that the articulation-oriented algorithm using DOMP obviously outperforms other algorithms in heavier impulse noise situations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {69–79},
numpages = {11},
keywords = {Gaussian-impulse noise, articulation-oriented, robust time-frequency decomposition (RTFD), discriminative orthogonal matching pursuit (DOMP), restoration, degraded}
}

@article{10.1109/TASLP.2014.2367814,
author = {Phan, Huy and Maa\ss{}, Marco and Mazur, Radoslaw and Mertins, Alfred},
title = {Random Regression Forests for Acoustic Event Detection and Classification},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2367814},
doi = {10.1109/TASLP.2014.2367814},
abstract = {Despite the success of the automatic speech recognition framework in its own application field, its adaptation to the problem of acoustic event detection has resulted in limited success. In this paper, instead of treating the problem similar to the segmentation and classification tasks in speech recognition, we pose it as a regression task and propose an approach based on random forest regression. Furthermore, event localization in time can be efficiently handled as a joint problem. We first decompose the training audio signals into multiple interleaved superframes which are annotated with the corresponding event class labels and their displacements to the temporal onsets and offsets of the events. For a specific event category, a random-forest regression model is learned using the displacement information. Given an unseen superframe, the learned regressor will output the continuous estimates of the onset and offset locations of the events. To deal with multiple event categories, prior to the category-specific regression phase, a superframe-wise recognition phase is performed to reject the background superframes and to classify the event superframes into different event categories. While jointly posing event detection and localization as a regression problem is novel, the superior performance on two databases ITC-Irst and UPC-TALP demonstrates the efficiency and potential of the proposed approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {20–31},
numpages = {12},
keywords = {random forest, superframe, acoustic event detection, regression forest}
}

@article{10.1109/TASLP.2014.2377591,
author = {Jensen, Jesper and Tan, Zheng-Hua},
title = {Minimum Mean-Square Error Estimation of Mel-Frequency Cepstral Features-a Theoretically Consistent Approach},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2377591},
doi = {10.1109/TASLP.2014.2377591},
abstract = {In this work, we consider the problem of feature enhancement for noise-robust automatic speech recognition (ASR). We propose a method for minimum mean-square error (MMSE) estimation of mel-frequency cepstral features, which is based on a minimum number of well-established, theoretically consistent statistical assumptions. More specifically, the method belongs to the class of methods relying on the statistical framework proposed in Ephraim and Malah's original work ("Speech enhancement using a minimum mean-square error short-time spectral amplitude estimator," IEEE Trans. Acoust., Speech, Signal Process., vol. ASSP-32, no. 6, 1984). The method is general in that it allows MMSE estimation of mel-frequency cepstral coefficients (MFCC's), cepstral-mean subtracted (CMS-) MFCC's, autoregressive-moving-average (ARMA)-filtered CMS-MFCC's, velocity, and acceleration coefficients. In addition, the method is easily modified to take into account other compressive non-linearities than the logarithm traditionally used for MFCC computation. In terms of MFCC estimation performance, as measured by MFCC mean-square error, the proposed method shows performance which is identical to or better than other state-of-the-art methods. In terms of ASR performance, no statistical difference could be found between the proposed method and the state-of-the-art methods. We conclude that existing state-of-the-art MFCC feature enhancement algorithms within this class of algorithms, while theoretically suboptimal or based on theoretically inconsistent assumptions, perform close to optimally in the MMSE sense.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {186–197},
numpages = {12},
keywords = {mel-frequency cepstral coefficient (MFCC), speech enhancement, robust automatic speech recognition (ASR), minimum mean-square error (MMSE) estimation}
}

@article{10.1109/TASLP.2014.2377581,
author = {Nielsen, Jens Brehm Bagger and Nielsen, Jakob and Larsen, Jan},
title = {Perception-Based Personalization of Hearing Aids Using Gaussian Processes and Active Learning},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2377581},
doi = {10.1109/TASLP.2014.2377581},
abstract = {Personalization of multi-parameter hearing aids involves an initial fitting followed by a manual knowledge-based trial-and-error fine-tuning from ambiguous verbal user feedback. The result is an often suboptimal HA setting whereby the full potential of modern hearing aids is not utilized. This article proposes an interactive hearing-aid personalization system that obtains an optimal individual setting of the hearing aids from direct perceptual user feedback. Results obtained with ten hearing-impaired subjects show that ten to twenty pairwise user assessments between different settings--equivalent to 5-10 min--is sufficient for personalization of up to four hearing-aid parameters. A setting obtained by the system was significantly preferred by the subject over the initial fitting, and the obtained setting could be reproduced with reasonable precision. The system may have potential for clinical usage to assist both the hearing-care professional and the user.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {162–173},
numpages = {12},
keywords = {pairwise comparisons, individualization, Gaussian process (GP), hearing aids, active learning, personalization}
}

@article{10.1109/TASLP.2014.2372314,
author = {Narayanan, Arun and Wang, DeLiang},
title = {Improving Robustness of Deep Neural Network Acoustic Models via Speech Separation and Joint Adaptive Training},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2372314},
doi = {10.1109/TASLP.2014.2372314},
abstract = {Although deep neural network (DNN) acoustic models are known to be inherently noise robust, especially with matched training and testing data, the use of speech separation as a frontend and for deriving alternative feature representations has been shown to improve performance in challenging environments. We first present a supervised speech separation system that significantly improves automatic speech recognition (ASR) performance in realistic noise conditions. The system performs separation via ratio time-frequency masking; the ideal ratio mask (IRM) is estimated using DNNs. We then propose a framework that unifies separation and acoustic modeling via joint adaptive training. Since the modules for acoustic modeling and speech separation are implemented using DNNs, unification is done by introducing additional hidden layers with fixed weights and appropriate network architecture. On the CHiME-2 medium-large vocabulary ASR task, and with log mel spectral features as input to the acoustic model, an independently trained ratio masking frontend improves word error rates by 10.9% (relative) compared to the noisy baseline. In comparison, the jointly trained system improves performance by 14.4%. We also experiment with alternative feature representations to augment the standard log mel features, like the noise and speech estimates obtained from the separation module, and the standard feature set used for IRM estimation. Our best system obtains a word error rate of 15.4% (absolute), an improvement of 4.6 percentage points over the next best result on this corpus.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {92–101},
numpages = {10},
keywords = {time-frequency masking, ratio masking, CHiME-2, robust ASR, joint training}
}

@article{10.1109/TASLP.2014.2375572,
author = {Su, Pei-Hao and Wu, Chuan-Hsun and Lee, Lin-Shan},
title = {A Recursive Dialogue Game for Personalized Computer-Aided Pronunciation Training},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2375572},
doi = {10.1109/TASLP.2014.2375572},
abstract = {Learning languages in addition to the native language is very important for all people in the globalized world today, and computer-aided pronunciation training (CAPT) is attractive since the software can be used anywhere at any time, and repeated as many times as desired. In this paper, we introduce the immersive interaction scenario offered by spoken dialogues to CAPT by proposing a recursive dialogue game to make CAPT personalized. A number of tree-structured sub-dialogues are linked sequentially and recursively as the script for the game. The system policy at each dialogue turn is to select in real-time along the dialogue the best training sentence for each specific individual learner within the dialogue script, considering the learner's learning status and the future possible dialogue paths in the script, such that the learner can have the scores for all pronunciation units considered reaching a predefined standard in a minimum number of turns. The purpose here is that those pronunciation units poorly produced by the specific learner can be offered with more practice opportunities in the future sentences along the dialogue, which enables the learner to improve the pronunciation without having to repeat the same training sentences many times. This makes the learning process for each learner completely personalized. The dialogue policy is modeled by Markov decision process (MDP) with high-dimensional continuous state space, and trained with fitted value iteration using a huge number of simulated learners. These simulated leaners have the behavior similar to real learners, and were generated from a corpus of real learner data. The experiments demonstrated very promising results and a real cloud-based system is also successfully implemented.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {127–141},
numpages = {15},
keywords = {dialogue game, computer-assisted language learning, computer-aided pronunciation training (CAPT), Markov decision process, reinforcement learning}
}

@article{10.1109/TASLP.2014.2367821,
author = {Decorsi\`{e}re, R\'{e}mi and S\o{}ndergaard, Peter L. and MacDonald, Ewen N. and Dau, Torsten},
title = {Inversion of Auditory Spectrograms, Traditional Spectrograms, and Other Envelope Representations},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2367821},
doi = {10.1109/TASLP.2014.2367821},
abstract = {Envelope representations such as the auditory or traditional spectrogram can be defined by the set of envelopes from the outputs of a filterbank. Common envelope extraction methods discard information regarding the fast fluctuations, or phase, of the signal. Thus, it is difficult to invert, or reconstruct a time-domain signal from, an arbitrary envelope representation. To address this problem, a general optimization approach in the time domain is proposed here, which iteratively minimizes the distance between a target envelope representation and that of a reconstructed time-domain signal. Two implementations of this framework are presented for auditory spectrograms, where the filterbank is based on the behavior of the basilar membrane and envelope extraction is modeled on the response of inner hair cells. One implementation is direct while the other is a two-stage approach that is computationally simpler. While both can accurately invert an auditory spectrogram, the two-stage approach performs better on time-domain metrics. The same framework is applied to traditional spectrograms based on the magnitude of the short-time Fourier transform. Inspired by human perception of loudness, a modification to the framework is proposed, which leads to a more accurate inversion of traditional spectrograms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {46–56},
numpages = {11},
keywords = {short-time Fourier transform, gradient methods, spectrogram inversion, auditory spectrogram}
}

@article{10.1109/TASLP.2014.2380352,
author = {Wang, Lin and Chen, Zhe and Yin, Fuliang},
title = {A Novel Hierarchical Decomposition Vector Quantization Method for High-Order LPC Parameters},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2380352},
doi = {10.1109/TASLP.2014.2380352},
abstract = {The paper investigates vector quantization coding of high-order (e.g., 20th-50th order) linear prediction coding (LPC) parameters, and proposes a novel hierarchical decomposition vector quantization method for a scalable speech coding framework with variable orders of LPC analysis. Instead of vector quantizing the whole group of LPC parameters in the linear spectral frequency (LSF) domain directly, the proposed method decomposes the high-order LPC model into several low-order (e.g., 10th-order) LPC models, and vector quantizes them in the LSF domain separately. For the decomposition, the high-order LPC model is converted into a group of reflection coefficients at first, and then the group is split into several subgroups and converted into multiple low-order LPC models. It is shown that the proposed method is naturally suitable for a scalable coding framework where the information of the decomposed low-order LPC models can be encoded into a multi-layered bitstream and can be combined in a progressive way to recover the high-order LPC information. Experiments in a scalable coding framework with variable LPC analysis orders (10-50) reveal that, compared to a direct vector quantization scheme, the proposed method can reduce the size of the codebook and the number of coding bits significantly, and can also efficiently reduce the computation cost.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {212–221},
numpages = {10},
keywords = {scalable coding, linear prediction coding (LPC), vector quantization, reflection coefficient, line spectral frequency}
}

@article{10.1109/TASLP.2014.2377575,
author = {Khoubrouy, Soudeh A. and Panahi, Issa M. S. and Hansen, John H. L.},
title = {Howling Detection in Hearing Aids Based on Generalized Teager-Kaiser Operator},
year = {2015},
issue_date = {January 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2377575},
doi = {10.1109/TASLP.2014.2377575},
abstract = {With the ongoing miniaturization in the hearing aid industry, acoustical coupling between the loudspeaker and the microphone(s) of the hearing aid causes a major problem to users. Howling is one of the most severe and annoying consequences of this acoustical coupling. This study presents a howling detection method using the Generalized Teager-Kaiser Operator (GTKO). Since the GTKO is both time and frequency sensitive, its resolution parameter must be assigned properly to ensure satisfactory performance of this operator in the frequency range of the input signal for the hearing aid. In order to cover the entire band of the input signal with appropriate resolution parameters, the input signal is decomposed into a filterbank (i.e., uniform and nonuniform filterbanks). GTKO is applied to the output of each band to detect the howling, and the resolution parameter of the GTKO block is selected depending on the central frequency of that particular band. Experimental results compare the performance of each proposed method with two known howling detection approaches, Peak-to-harmonic power ratio (PHPR) approach and a multiple-feature approach. The proposed method has high detection probability and short detection time. It is also shown that considering a hybrid algorithm which includes the PHPR approach with each of the proposed methods (i.e. combination of GTKO blocks with different types of filterbanks) results in lower false alarm probability.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {154–161},
numpages = {8},
keywords = {uniform filterbank, howling detection, non-uniform filterbank, acoustic feedback, hearing aid, Teager-Kaiser operator}
}

@article{10.1109/TASLP.2014.2360461,
author = {Yan, Su and Wan, Xiaojun},
title = {SRRank: Leveraging Semantic Roles for Extractive Multi-Document Summarization},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2360461},
doi = {10.1109/TASLP.2014.2360461},
abstract = {Extractive multi-document summarization systems usually rank sentences in a document set with some ranking strategy and then select a few highly ranked sentences into the summary. One of the most popular ranking algorithms is the graph-based ranking algorithm. In this paper, we investigate making use of semantic role information to enhance the graph-based ranking algorithm for multi-document summarization. We first parse the sentences and obtain the semantic roles, and then propose a novel SRRank algorithm and two extensions to make better use of the semantic role information. Our proposed algorithms can simultaneously rank the sentences, semantic roles and words in a heterogeneous ranking process. Experimental results on two DUC datasets demonstrate that our proposed algorithms significantly outperform a few baselines, and the semantic role information is validated to be very helpful for multidocument summarization.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2048–2058},
numpages = {11},
keywords = {multi-document summarization, semantic roles, graph-based ranking algorithm}
}

@article{10.1109/TASLP.2014.2348916,
author = {Percival, Graham and Tzanetakis, George},
title = {Streamlined Tempo Estimation Based on Autocorrelation and Cross-Correlation with Pulses},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2348916},
doi = {10.1109/TASLP.2014.2348916},
abstract = {Algorithms for musical tempo estimation have become increasingly complicated in recent years. These algorithms typically utilize two fundamental properties of musical rhythm: some features of the audio signal are self-similar at periods related to the underlying rhythmic structure, and rhythmic events tend to be spaced regularly in time. We present a streamlined tempo estimation method (stem) that distills ideas from previous work by reducing the number of steps, parameters, and modeling assumptions while retaining good accuracy. This method is designed for music with a constant or near-constant tempo. The proposed method either outperforms or has similar performance to many existing state-of-the-art algorithms. Self-similarity is captured through autocorrelation of the onset strength signal (OSS), and time regularity is captured through cross-correlation of the OSS with regularly spaced pulses. Our findings are supported by the most comprehensive evaluation of tempo estimation algorithms to date in terms of the number of datasets and tracks considered. During the process we have also corrected ground truth annotations for the datasets considered. All the data, the annotations, the evaluation code, and three different implementations (C++, Python, MATLAB) of the proposed algorithm are provided in order to support reproducibility.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1765–1776},
numpages = {12},
keywords = {rhythm analysis, music information retrieval, tempo induction, audio signal processing}
}

@article{10.1109/TASLP.2014.2354236,
author = {Krawczyk, Martin and Gerkmann, Timo},
title = {STFT Phase Reconstruction in Voiced Speech for an Improved Single-Channel Speech Enhancement},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2354236},
doi = {10.1109/TASLP.2014.2354236},
abstract = {The enhancement of speech which is corrupted by noise is commonly performed in the short-time discrete Fourier transform domain. In case only a single microphone signal is available, typically only the spectral amplitude is modified. However, it has recently been shown that an improved spectral phase can as well be utilized for speech enhancement, e.g., for phase-sensitive amplitude estimation. In this paper, we therefore present a method to reconstruct the spectral phase of voiced speech from only the fundamental frequency and the noisy observation. The importance of the spectral phase is highlighted and we elaborate on the reason why noise reduction can be achieved by modifications of the spectral phase. We show that, when the noisy phase is enhanced using the proposed phase reconstruction, instrumental measures predict an increase of speech quality over a range of signal to noise ratios, even without explicit amplitude enhancement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1931–1940},
numpages = {10},
keywords = {speech enhancement, noise reduction, phase estimation, signal reconstruction}
}

@article{10.1109/TASLP.2014.2346314,
author = {Chivukula, Ravi K. and Reznik, Yuriy A. and Hu, Yanyan and Devarajan, Venkat and Jayendra-Lakshman, Mythreya},
title = {Fast Algorithms for Low-Delay TDAC Filterbanks in MPEG-4 AAC-ELD},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2346314},
doi = {10.1109/TASLP.2014.2346314},
abstract = {The MPEG committee has completed development of a new audio coding standard called "MPEG-4 advanced audio coding-enhanced low delay" (AAC-ELD). AAC-ELD uses low delay spectral band replication (LD-SBR) technology together with a low delay time domain alias cancellation (LD TDAC) filterbank in the encoder to achieve both high coding efficiency and low algorithmic delay. In this paper, we present fast algorithms for implementing LD-TDAC filterbanks in AAC-ELD. Two types of fast algorithms are presented. In the first, we map LD-TDAC analysis and synthesis filterbanks to modified discrete cosine transform (MDCT) and inverse modified discrete cosine transform (IMDCT), respectively. Since MDCT/IMDCT are already extensively used in AAC and they have many fast algorithms, this mapping not only provides a fast implementation but also allows a common implementation of the filterbanks in AAC Low Complexity (AAC-LC), AAC Low Delay (AAC-LD) and AAC-ELD codecs. In the second algorithm, we provide a mapping to discrete Cosine transform of type II. The mapping to DCT-II allows the merger of the matrix operations with the windowing stage that precedes or follows them. This further reduces the number of multiplications and leads to an algorithm with the lowest known arithmetic complexity. For filterbanks of lengths 1024 and 960, we also present a new fast factorization of 15-point DCT-II that requires only 14 irrational multiplications, 3 dyadic rational multiplications and 67 additions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1701–1712},
numpages = {12},
keywords = {AAC, MDCT, filterbanks, DCT, audio coding, low delay, time domain alias cancellation, MPEG, speech coding, fast algorithms, factorization}
}

@article{10.1109/TASLP.2014.2351131,
author = {Tachibana, Hideyuki and Ono, Nobutaka and Kameoka, Hirokazu and Sagayama, Shigeki},
title = {Harmonic/Percussive Sound Separation Based on Anisotropic Smoothness of Spectrograms},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2351131},
doi = {10.1109/TASLP.2014.2351131},
abstract = {This paper describes a method to separate a monaural music signal into harmonic components e.g., a guitar and percussive components, e.g., a snare drum. Separation of these two components is a useful preprocessing for many music information retrieval applications, and in addition, it can be used as a new kind of music equalizer in itself, which enables a music listener to adjust the ratio of the volume of the guitar and the drum freely by themselves. Because of these potential applications, there have been many attempts to develop such a technique, especially in the last decade. However, some of the state-of-the-art techniques have a drawback that they are based on costly operations, such as the multiplications of large-sized matrix, Monte Carlo method, etc., which may constitute barriers to the practical use on some small computers such as smart phones. In this paper, an efficient method that does not depend on these costly operations is described. In formulating the methods, the authors basically assumed only the "anisotropic smoothness" of music spectrogram, which can be one of the minimalistic model that reflects the natures of these instruments. To be specific, the authors just assumed that harmonic instruments are smooth in time, while the percussive instruments are smooth in frequency on a music spectrogram. In this paper, on the basis of the assumption, source separation methods are formulated as optimization problems that optimize the "anisotropic smoothness" under some conditions. Because of the simplicity of the model, the derived algorithms are quite simple. Experimental results show that the methods were effective compared to a state-of-the-art technique, and the computation time was much shorter than an existing method; specifically, it can process a three-minute song in around 4-20 seconds on a laptop PC.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2059–2073},
numpages = {15},
keywords = {percussion, music signal processing, harmonic instruments, audio source separation}
}

@article{10.1109/TASLP.2014.2365971,
author = {FanChiang, Yi and Wei, Cheng-Wen and Meng, Yi-Le and Lin, Yu-Wen and Jou, Shyh-Jye and Chang, Tian-Sheuan},
title = {Correction to "Low Complexity Formant Estimation Adaptive Feedback Cancellation for Hearing Aids Using Pitch Based Processing"},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2365971},
doi = {10.1109/TASLP.2014.2365971},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2256},
numpages = {1}
}

@article{10.1109/TASLP.2014.2346313,
author = {Xue, Shaofei and Abdel-Hamid, Ossama and Jiang, Hui and Dai, Lirong and Liu, Qingfeng},
title = {Fast Adaptation of Deep Neural Network Based on Discriminant Codes for Speech Recognition},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2346313},
doi = {10.1109/TASLP.2014.2346313},
abstract = {Fast adaptation of deep neural networks (DNN) is an important research topic in deep learning. In this paper, we have proposed a general adaptation scheme for DNN based on discriminant condition codes, which are directly fed to various layers of a pre-trained DNN through a new set of connection weights. Moreover, we present several training methods to learn connection weights from training data as well as the corresponding adaptation methods to learn new condition code from adaptation data for each new test condition. In this work, the fast adaptation scheme is applied to supervised speaker adaptation in speech recognition based on either frame-level cross-entropy or sequence-level maximum mutual information training criterion. We have proposed three different ways to apply this adaptation scheme based on the so-called speaker codes: i) Nonlinear feature normalization in feature space; ii) Direct model adaptation of DNN based on speaker codes; iii) Joint speaker adaptive training with speaker codes. We have evaluated the proposed adaptation methods in two standard speech recognition tasks, namely TIMIT phone recognition and large vocabulary speech recognition in the Switchboard task. Experimental results have shown that all three methods are quite effective to adapt large DNN models using only a small amount of adaptation data. For example, the Switchboard results have shown that the proposed speaker-code-based adaptation methods may achieve up to 8-10% relative error reduction using only a few dozens of adaptation utterances per speaker. Finally, we have achieved very good performance in Switchboard (12.1% in WER) after speaker adaptation using sequence training criterion, which is very close to the best performance reported in this task ("Deep convolutional neural networks for LVCSR," T. N. Sainath et al., Proc. IEEE Acoust., Speech, Signal Process., 2013).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1713–1725},
numpages = {13},
keywords = {speaker code, cross entropy (CE), maximum mutual information (MMI), deep neural network (DNN), condition code, fast adaptation}
}

@article{10.1109/TASLP.2014.2352495,
author = {D'Angelo, Stefano and V\"{a}lim\"{a}ki, Vesa},
title = {Generalized Moog Ladder Filter: Part I-Linear Analysis and Parameterization},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2352495},
doi = {10.1109/TASLP.2014.2352495},
abstract = {The Moog ladder filter, which consists of four cascaded first-order ladder stages in a feedback loop, falls within the class of devices that have attracted greatest interest in virtual analog research. On one hand, this work confirms that the presence of exactly four stages in the original analog circuit is motivated by specific filter control issues and, on the other, that such a limitation can be overcome in the digital domain with relative ease. First, a continuous-time large-signal model is defined for a version of the circuit that is generalized to an arbitrary number of ladder stages. Then, the linear behavior of the filter around its natural operating point and the effect of control parameters on the resulting frequency response are studied in depth, to obtain exact analytical expressions for the position of poles in the transfer function and for the dc gain of the filter, as well as a parameterization strategy that is consistent for any number of ladder stages. A previously-introduced linear digital model of the device suggested by Smith is eventually generalized based on these general results, which remain, however, relevant and similarly applicable to other discretizations of the filter. The proposed model faithfully reproduces the linear behavior of the generalized device while providing sensible parametric control for any number of ladder stages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1825–1832},
numpages = {8},
keywords = {IIR filters, resonator filters, music, acoustic signal processing, circuit simulation}
}

@article{10.1109/TASLP.2014.2352456,
author = {Zakharov, Yuriy and Nascimento, V\'{\i}tor H.},
title = {Sliding-Window RLS Low-Cost Implementation of Proportionate Affine Projection Algorithms},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2352456},
doi = {10.1109/TASLP.2014.2352456},
abstract = {This paper addresses adaptive filtering for sparse identification. Proportionate affine projection algorithms (PAPAs) are known to be efficient techniques for this purpose. We show that the PAPA performance may improve with an increase in the projection order M (for example, such as M = 512 ), which, however, also results in an increased complexity; the complexity is in general O(M2N) or at least O(M N) operations per sample, where N is the filter length. We show that PAPAs are equivalent to specific sliding-window recursive least squares (SRLS) adaptive algorithms with time-varying and tap-varying diagonal loading (SRLS-VDLs). We then propose an approximation to the SRLS-VDLs based on dichotomous coordinate descent (DCD) iterations with a complexity of O(NuN), which does not depend on M; it depends on the number of DCD iterations Nu, which as we show can be significantly smaller than M, thus allowing a low-complexity implementation of PAPA adaptive filters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1815–1824},
numpages = {10},
keywords = {adaptive filter, sliding window, DCD, dichotomous coordinate descent, diagonal loading, PAPA, sparse identification, RLS, affine projection}
}

@article{10.1109/TASLP.2014.2357677,
author = {Wang, Qi and Woo, W. L. and Dlay, S. S.},
title = {Informed Single-Channel Speech Separation Using HMM-GMM User-Generated Exemplar Source},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2357677},
doi = {10.1109/TASLP.2014.2357677},
abstract = {We present a new approach for solving the single channel speech separation with the aid of an user-generated exemplar source that is recorded from a microphone. Our method deviates from the conventional model-based methods, which highly rely on speaker dependent training data. We readdress the problem by offering a new approach based on utterance dependent patterns extracted from the user-generated exemplar source. Our proposed approach is less restrictive, and does not require speaker dependent information and yet exceeds the performance of conventional model-based separation methods in separating male and male speech mixtures. We combine general speaker-independent (SI) features with specifically generated utterance-dependent (UD) features in a joint probability model. The UD features are initially extracted from the user-generated exemplar source and represented as statistical estimates. These estimates are calibrated based on information extracted from the mixture source to statistically represent the target source. The UD probability model is subsequently generated to target problems of ambiguity and to offer better cues for separation. The proposed algorithm is tested and compared with recent method using the GRID database and the Mocha-TIMIT database.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2087–2100},
numpages = {14},
keywords = {exemplar assistance, Gaussian mixture model (GMM), single-channel source separation (SCSS), speaker-assisted source separation, factorial hidden Markov model(FHMM), concurrent pitch tracking, informed source separation (ISS)}
}

@article{10.1109/TASLP.2014.2353991,
author = {Chen, Ling-Hui and Ling, Zhen-Hua and Liu, Li-Juan and Dai, Li-Rong},
title = {Voice Conversion Using Deep Neural Networks with Layer-Wise Generative Training},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2353991},
doi = {10.1109/TASLP.2014.2353991},
abstract = {This paper presents a new spectral envelope conversion method using deep neural networks (DNNs). The conventional joint density Gaussian mixture model (JDGMM) based spectral conversion methods perform stably and effectively. However, the speech generated by these methods suffer severe quality degradation due to the following two factors: 1) inadequacy of JDGMM in modeling the distribution of spectral features as well as the non-linear mapping relationship between the source and target speakers, 2) spectral detail loss caused by the use of high-level spectral features such as mel-cepstra. Previously, we have proposed to use the mixture of restricted Boltzmann machines (MoRBM) and the mixture of Gaussian bidirectional associative memories (MoGBAM) to cope with these problems. In this paper, we propose to use a DNN to construct a global non-linear mapping relationship between the spectral envelopes of two speakers. The proposed DNN is generatively trained by cascading two RBMs, which model the distributions of spectral envelopes of source and target speakers respectively, using a Bernoulli BAM (BBAM). Therefore, the proposed training method takes the advantage of the strong modeling ability of RBMs in modeling the distribution of spectral envelopes and the superiority of BAMs in deriving the conditional distributions for conversion. Careful comparisons and analysis among the proposed method and some conventional methods are presented in this paper. The subjective results show that the proposed method can significantly improve the performance in terms of both similarity and naturalness compared to conventional methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1859–1872},
numpages = {14},
keywords = {voice conversion, Gaussian mixture model, bidirectional associative memory, spectral envelope conversion, deep neural network, restricted Boltzmann machine}
}

@article{10.1109/TASLP.2014.2354241,
author = {R\"{a}m\"{o}, Jussi and V\"{a}lim\"{a}ki, Vesa and Bank, Bal\'{a}zs},
title = {High-Precision Parallel Graphic Equalizer},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2354241},
doi = {10.1109/TASLP.2014.2354241},
abstract = {This paper proposes a high-precision graphic equalizer based on second-order parallel filters. Previous graphic equalizers suffer from interaction between adjacent band filters, especially at high gain values, which can lead to substantial errors in the magnitude response. The fixed-pole design of the proposed parallel graphic equalizer avoids this problem, since the parallel second-order filters are optimized jointly. When the number of pole frequencies is twice the number of command points of the graphic equalizer, the proposed non-iterative design matches the target curve with high precision. In the three example cases presented in this paper, the proposed parallel equalizer clearly outperforms other non-iterative graphic equalizer designs, and its maximum global error is as low as 0.00-0.75 dB when compared to the target curve. While the proposed design has superior accuracy, the number of operations in the filter structure is increased only by 23% when compared to the second-order Regalia-Mitra structure. The parallel structure also enables the utilization of parallel computing hardware, which can nowadays easily outperform the traditional serial processing. The proposed graphic equalizer can be widely used in audio signal processing applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1894–1904},
numpages = {11},
keywords = {audio systems, digital signal processing, infinite impulse response (IIR) filters, acoustic signal processing, equalizers}
}

@article{10.1109/TASLP.2014.2362006,
author = {Su, Li and Lin, Hsin-Ming and Yang, Yi-Hsuan},
title = {Sparse Modeling of Magnitude and Phase-Derived Spectra for Playing Technique Classification},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2362006},
doi = {10.1109/TASLP.2014.2362006},
abstract = {Computational modeling of musical timbre is important for a variety of music information retrieval applications. While considerable progress has been made to recognize musical genres and instruments, relatively little attention has been paid to modeling playing techniques, which affect timbre in more subtle ways. In this paper, we contribute to this area of research by systematically evaluating various audio features and processing methods for multi-class playing technique classification, considering up to nine distinct playing techniques of bowed string instruments. Specifically, a collection of 6,759 chamber-recorded single notes of four bowed string instruments and a collection of 33 real-world solo violin recordings are used in the evaluation. Our evaluation shows that using sparse features extracted from the magnitude spectra and phase derivatives including group delay function (GDF) and instantaneous frequency deviation (IFD) leads to significantly better performance than using a combination of state-of-the-art temporal, spectral, cepstral and harmonic feature descriptors. For playing technique classification of violin singe notes, the former approach attains 0.915 macro-average F-score under a tenfold cross validation setting, while the latter only attains 0.835. Moreover, sparse modeling of magnitude and phase-derived spectra also performs well for single-note joint instrument-technique classification (F-score 0.770) and for playing technique classification of real-world violin solos (F-score 0.547). We find that phase information is particularly important in discriminating playing techniques with subtle differences, such as playing with different bowing positions (i.e., normal, sul tasto, and sul ponticello). A systematic investigation of the effect of parameters such as window sizes, hop factors, window types for phase-derived features is also reported to provide more insights.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2122–2132},
numpages = {11},
keywords = {instantaneous frequency deviation, sparse coding, phase, group delay function, playing technique classification}
}

@article{10.1109/TASLP.2014.2363407,
author = {Thiergart, Oliver and Taseska, Maja and Habets, Emanu\"{e}l A. P.},
title = {An Informed Parametric Spatial Filter Based on Instantaneous Direction-of-Arrival Estimates},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2363407},
doi = {10.1109/TASLP.2014.2363407},
abstract = {Extracting desired source signals in noisy and reverberant environments is required in many hands-free communication systems. In practical situations, where the position and number of active sources may be unknown and time-varying, conventional implementations of spatial filters do not provide sufficiently good performance. Recently, informed spatial filters have been introduced that incorporate almost instantaneous parametric information on the sound field, thereby enabling adaptation to new acoustic conditions and moving sources. In this contribution, we propose a spatial filter which generalizes the recently proposed informed linearly constrained minimum variance filter and informed minimum mean square error filter. The proposed filter uses multiple direction-of-arrival estimates and second-order statistics of the noise and diffuse sound. To determine those statistics, an optimal diffuse power estimator is proposed that outperforms state-of-the-art estimators. Extensive performance evaluation demonstrates the effectiveness of the proposed filter in dynamic acoustic conditions. For this purpose, we have considered a challenging scenario which consists of quickly moving sound sources during double-talk. The performance of the proposed spatial filter was evaluated in terms of objective measures including segmental signal-to-reverberation ratio and log spectral distance, and by means of a listening test confirming the objective results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2182–2196},
numpages = {15},
keywords = {microphone array processing, dereverberation, interference reduction, optimal beamforming}
}

@article{10.1109/TASLP.2014.2370434,
author = {Deng, Li},
title = {Farewell Editorial: Keeping up the Momentum of Innovations},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2370434},
doi = {10.1109/TASLP.2014.2370434},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1687},
numpages = {1}
}

@article{10.1109/TASLP.2014.2355821,
author = {Venturini, A. and Z\~{a}o, L. and Coelho, R.},
title = {On Speech Features Fusion, α-Integration Gaussian Modeling and Multi-Style Training for Noise Robust Speaker Classification},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2355821},
doi = {10.1109/TASLP.2014.2355821},
abstract = {This paper investigates the fusion of Mel-frequency cepstral coefficients (MFCC) and statistical pH features to improve the performance of speaker verification (SV) in non-stationary noise conditions. The α-integrated Gaussian Mixture Model (α-GMM) classifier is adopted for speaker modeling. Two different approaches are applied to reduce the effects of noise corruption in the SV task: speech enhancement and multi-style training (MT). The spectral subtraction with minimum statistics (MS/SS) and the optimally-modified log-spectral amplitude with improved minima controlled recursive averaging (IMCRA/OMLSA) are examined for the speech enhancement procedure. The MT techniques are based on colored (Colored-MT), white (White-MT) and narrow-band (Narrow-MT) noises. Six real non-stationary noises, collected from different acoustic sources, are used to corrupt the TIMIT speech database in four different signal-to-noise ratios (SNR). The index of non-stationarity (INS) is chosen for the stationarity tests of the acoustic noises. Complementary SV experiments are conducted in realistic noisy conditions using the MIT database. The results show that the best SV accuracy was obtained with the MFCC + pH features fusion, the MS/SS and the Colored-MT.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1951–1964},
numpages = {14},
keywords = {features fusion, non-stationary acoustic noise, hurst exponent, speech enhancement, speaker verification, multi-style training, α-GMM}
}

@article{10.1109/TASLP.2014.2352451,
author = {Khanagha, Vahid and Daoudi, Khalid and Yahia, Hussein M.},
title = {Detection of Glottal Closure Instants Based on the Microcanonical Multiscale Formalism},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2352451},
doi = {10.1109/TASLP.2014.2352451},
abstract = {This paper presents a novel algorithm for automatic detection of Glottal Closure Instants (GCI) from the speech signal. Our approach is based on a novel multiscale method that relies on precise estimation of a multiscale parameter at each time instant in the signal domain. This parameter quantifies the degree of signal singularity at each sample from a multi-scale point of view and thus its value can be used to classify signal samples accordingly. We use this property to develop a simple algorithm for detection of GCIs and we show that for the case of clean speech, our algorithm performs almost as well as a recent state-of-the-art method. Next, by performing a comprehensive comparison in presence of 14 different types of noises, we show that our method is more accurate (particularly for very low SNRs). Our method has lower computational times compared to others and does not rely on an estimate of pitch period or any critical choice of parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1941–1950},
numpages = {10},
keywords = {nonlinear speech analysis, multiscale signal processing, detection of glottal closure instant}
}

@article{10.1109/TASLP.2014.2349856,
author = {Barkefors, Annea and Sternad, Mikael and Br\"{a}nnmark, Lars-Johan},
title = {Design and Analysis of Linear Quadratic Gaussian Feedforward Controllers for Active Noise Control},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2349856},
doi = {10.1109/TASLP.2014.2349856},
abstract = {A method for sound field control applied to active noise control is presented and evaluated. The method uses Linear Quadratic Gaussian (LQG) feedforward control to find a Minimal Mean Square Error (MMSE)-optimal linear sound field controller under a causality constraint. It is obtained by solving a polynomial matrix spectral factorization and a linear (Diophantine) polynomial matrix equation. An important component in the design is the control signal penalty term of the criterion. Its use and influence is here discussed and evaluated using measured room impulse responses. The results indicate that the use of a relatively simple, frequency-weighted penalty on individual control signals provides most of the benefits obtainable by the considered more advanced alternative. We also introduce and illustrate several tools for performance analysis. An analytical expression for the attainable performance clearly reveals the performance loss generated by having to use a causal controller instead of the ideal noncausal controller. This loss is largest at low frequencies. Furthermore, we introduce a measure of the reproducibility of the target noise sound field with given control loudspeaker setups and room transfer functions. It describes how well a controller that uses an input subspace of dimension equal to the effective rank of the system is able to reproduce a target sound field. This performance measure can e.g. be used to support the selection of good combinations of placements of control loudspeakers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1777–1791},
numpages = {15},
keywords = {sound field reproducibility, sound field control, linear quadratic control, active noise reduction, feedforward control, effective rank, causality constraints}
}

@article{10.1109/TASLP.2014.2365701,
author = {Traa, Johannes and Smaragdis, Paris},
title = {Multichannel Source Separation and Tracking with RANSAC and Directional Statistics},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2365701},
doi = {10.1109/TASLP.2014.2365701},
abstract = {We describe multichannel blind source separation and tracking algorithms based on clustering wrapped interchannel phase difference (IPD) features. We pose the clustering problem as one of multimodal circular-linear regression and present its probabilistic formulation. Phase wrapping due to spatial aliasing is explicitly incorporated by modeling the IPD features as circular variables. We present two methods based on Expectation-Maximization (EM) and a sequential variant of RANdom SAmple Consensus (RANSAC). We show that their strengths can be combined by using RANSAC to initialize EM. The IPD clustering algorithm is applied to separate stationary speakers from a multi-channel mixture. We then extend it to the case of moving speakers by tracking their directions-of-arrival with the Factorial Wrapped Kalman Filter (FWKF) using RANSAC as a data preprocessor. Experimental results demonstrate that the proposed methods perform well in the presence of reverberant babble noise and spatial aliasing. The FWKF successfully tracks and separates moving speakers with separation quality comparable to that for stationary speakers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2233–2243},
numpages = {11},
keywords = {blind source separation (BSS), interchannel phase difference (IPD), directional statistics, wrapped Kalman filter}
}

@article{10.1109/TASLP.2014.2363790,
author = {Otsuka, Takuma and Ishiguro, Katsuhiko and Yoshioka, Takuya and Sawada, Hiroshi and Okuno, Hiroshi G.},
title = {Multichannel Sound Source Dereverberation and Separation for Arbitrary Number of Sources Based on Bayesian Nonparametrics},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2363790},
doi = {10.1109/TASLP.2014.2363790},
abstract = {Multichannel signal processing using a microphone array provides fundamental functions for coping with multisource situations, such as sound source localization and separation, that are needed to extract the auditory information for each source. Auditory uncertainties about the degree of reverberation and the number of sources are known to degrade performance or limit the practical application of microphone array processing. Such uncertainties must therefore be overcome to realize general and robust microphone array processing. These uncertainty issues have been partly addressed--existing methods focus on either source number uncertainty or the reverberation issue, where joint separation and dereverberation has been achieved only for the overdetermined conditions. This paper presents an all-round method that achieves source separation and dereverberation for an arbitrary number of sources including underdetermined conditions. Our method uses Bayesian nonparametrics that realize an infinitely extensible modeling flexibility so as to bypass the model selection in the separation and dereverberation problem, which is caused by the source number uncertainty. Evaluation using a dereverberation and separation task with various numbers of sources including underdetermined conditions demonstrates that (1) our method is applicable to the separation and dereverberation of underdetermined mixtures, and that (2) the source extraction performance is comparable to that of a state-of-the-art method suitable only for overdetermined conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2218–2232},
numpages = {15},
keywords = {blind dereverberation, Bayesian nonparametrics, underdetermined mixtures, microphone array processing, blind source separation, Markov chain Monte Carlo method}
}

@article{10.1109/TASLP.2014.2355772,
author = {Maezawa, Akira and Itoyama, Katsutoshi and Yoshii, Kazuyoshi and Okuno, Hiroshi G.},
title = {Nonparametric Bayesian Dereverberation of Power Spectrograms Based on Infinite-Order Autoregressive Processes},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2355772},
doi = {10.1109/TASLP.2014.2355772},
abstract = {This paper describes a monaural audio dereverberation method that operates in the power spectrogram domain. The method is robust to different kinds of source signals such as speech or music. Moreover, it requires little manual intervention, including the complexity of room acoustics. The method is based on a non-conjugate Bayesian model of the power spectrogram. It extends the idea of multi-channel linear prediction to the power spectrogram domain, and formulates a model of reverberation as a non-negative, infinite-order autoregressive process. To this end, the power spectrogram is interpreted as a histogram count data, which allows a nonparametric Bayesian model to be used as the prior for the autoregressive process, allowing the effective number of active components to grow, without bound, with the complexity of data. In order to determine the marginal posterior distribution, a convergent algorithm, inspired by the variational Bayes method, is formulated. It employs the minorization-maximization technique to arrive at an iterative, convergent algorithm that approximates the marginal posterior distribution. Both objective and subjective evaluations show advantage over other methods based on the power spectrum. We also apply the method to a music information retrieval task and demonstrate its effectiveness.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1918–1930},
numpages = {13},
keywords = {nonparameteric Bayes, dereverberation, minorization maximization}
}

@article{10.1109/TASLP.2014.2363414,
author = {Mu, Yongsheng and Ji, Peifeng and Ji, Wei and Wu, Ming and Yang, Jun},
title = {Modeling and Compensation for the Distortion of Parametric Loudspeakers Using a One-Dimension Volterra Filter},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2363414},
doi = {10.1109/TASLP.2014.2363414},
abstract = {Recently, the general Volterra filter (VF) has been adopted for the modeling of parametric loudspeakers. However, the computation complexity of the VF is too high for real-time implementation. In this paper, a one-dimension Volterra filter (ODVF) with much lower complexity is introduced to model and compensate for the nonlinearity of parametric loudspeakers. A theoretical framework for ODVF model identification is established and a method of measuring the ODVF kernels using the exponential swept-sine signal is provided. The validity of modeling the nonlinearity of the parametric loudspeaker using the ODVF is verified theoretically and experimentally. Based on the established ODVF model, an inverse filter is designed to compensate for the 2nd harmonic distortion of the parametric loudspeakers. To further reduce the 3rd harmonic distortion, an improved compensation method is also proposed. Experimental results show that the performance of the ODVF-based compensation is comparable to that of the Volterra-filter based compensation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2169–2181},
numpages = {13},
keywords = {pth-order inverse, one-dimension Volterra filter (ODVF), parametric loudspeaker, Volterra filter (VF)}
}

@article{10.1109/TASLP.2014.2351133,
author = {Tourbabin, Vladimir and Rafaely, Boaz},
title = {Theoretical Framework for the Optimization of Microphone Array Configuration for Humanoid Robot Audition},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2351133},
doi = {10.1109/TASLP.2014.2351133},
abstract = {An important aspect of a humanoid robot is audition. Previous work has presented robot systems capable of sound localization and source segregation based on microphone arrays with various configurations. However, no theoretical framework for the design of these arrays has been presented. In the current paper, a design framework is proposed based on a novel array quality measure. The measure is based on the effective rank of a matrix composed of the generalized head related transfer functions (GHRTFs) that account for microphone positions other than the ears. The measure is shown to be theoretically related to standard array performance measures such as beamforming robustness and DOA estimation accuracy. Then, the measure is applied to produce sample designs of microphone arrays. Their performance is investigated numerically, verifying the advantages of array design based on the proposed theoretical framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1803–1814},
numpages = {12},
keywords = {robot audition, generalized HRTF, microphone array, DOA estimation, beamforming}
}

@article{10.1109/TASLP.2014.2352154,
author = {Liu, Gang and Hansen, John H. L.},
title = {An Investigation into Back-End Advancements for Speaker Recognition in Multi-Session and Noisy Enrollment Scenarios},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2352154},
doi = {10.1109/TASLP.2014.2352154},
abstract = {This study aims to explore the case of robust speaker recognition with multi-session enrollments and noise, with an emphasis on optimal organization and utilization of speaker information presented in the enrollment and development data. This study has two core objectives. First, we investigate more robust back-ends to address noisy multi-session enrollment data for speaker recognition. This task is achieved by proposing novel back-end algorithms. Second, we construct a highly discriminative speaker verification framework. This task is achieved through intrinsic and extrinsic back-end algorithm modification, resulting in complementary sub-systems. Evaluation of the proposed framework is performed on the NIST SRE2012 corpus. Results not only confirm individual sub-system advancements over an established baseline, the final grand fusion solution also represents a comprehensive overall advancement for the NIST SRE2012 core tasks. Compared with state-of-the-art SID systems on the NIST SRE2012, the novel parts of this study are: 1) exploring a more diverse set of solutions for low-dimensional i-Vector based modeling; and 2) diversifying the information configuration before modeling. All these two parts work together, resulting in very competitive performance with reasonable computational cost.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1978–1992},
numpages = {15},
keywords = {GCDS, speaker recognition, universal background support, classification algorithms, PLDA}
}

@article{10.1109/TASLP.2014.2352453,
author = {Yang, Na and Ba, He and Cai, Weiyang and Demirkol, Ilker and Heinzelman, Wendi},
title = {BaNa: A Noise Resilient Fundamental Frequency Detection Algorithm for Speech and Music},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2352453},
doi = {10.1109/TASLP.2014.2352453},
abstract = {Fundamental frequency (F0) is one of the essential features in many acoustic related applications. Although numerous F0 detection algorithms have been developed, the detection accuracy in noisy environments still needs improvement. We present a hybrid noise resilient F0 detection algorithm named BaNa that combines the approaches of harmonic ratios and Cepstrum analysis. A Viterbi algorithm with a cost function is used to identify the F0 value among several F0 candidates. Speech and music databases with eight different types of additive noise are used to evaluate the performance of the BaNa algorithm and several classic and state-of-the-art F0 detection algorithms. Results show that for almost all types of noise and signal-to-noise ratio (SNR) values investigated, BaNa achieves the lowest Gross Pitch Error (GPE) rate among all the algorithms. Moreover, for the 0 dB SNR scenarios, the BaNa algorithm is shown to achieve 20% to 35% GPE rate for speech and 12% to 39% GPE rate for music. We also describe implementation issues that must be addressed to run the BaNa algorithm as a real-time application on a smartphone platform.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1833–1848},
numpages = {16},
keywords = {noise resilience, viterbi algorithm, harmonics, fundamental frequency detection, cepstrum}
}

@article{10.1109/TASLP.2014.2352935,
author = {Wang, Yuxuan and Narayanan, Arun and Wang, DeLiang},
title = {On Training Targets for Supervised Speech Separation},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2352935},
doi = {10.1109/TASLP.2014.2352935},
abstract = {Formulation of speech separation as a supervised learning problem has shown considerable promise. In its simplest form, a supervised learning algorithm, typically a deep neural network, is trained to learn a mapping from noisy features to a time-frequency representation of the target of interest. Traditionally, the ideal binary mask (IBM) is used as the target because of its simplicity and large speech intelligibility gains. The supervised learning framework, however, is not restricted to the use of binary targets. In this study, we evaluate and compare separation results by using different training targets, including the IBM, the target binary mask, the ideal ratio mask (IRM), the short-time Fourier transform spectral magnitude and its corresponding mask (FFT-MASK), and the Gammatone frequency power spectrum. Our results in various test conditions reveal that the two ratio mask targets, the IRM and the FFT-MASK, outperform the other targets in terms of objective intelligibility and quality metrics. In addition, we find that masking based targets, in general, are significantly better than spectral envelope based targets. We also present comparisons with recent methods in non-negative matrix factorization and speech enhancement, which show clear performance advantages of supervised speech separation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1849–1858},
numpages = {10},
keywords = {speech separation, deep neural networks, supervised learning, training targets}
}

@article{10.1109/TASLP.2014.2354242,
author = {Rafii, Zafar and Duan, Zhiyao and Pardo, Bryan},
title = {Combining Rhythm-Based and Pitch-Based Methods for Background and Melody Separation},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2354242},
doi = {10.1109/TASLP.2014.2354242},
abstract = {Musical works are often composed of two characteristic components: the background (typically the musical accompaniment), which generally exhibits a strong rhythmic structure with distinctive repeating time elements, and the melody (typically the singing voice or a solo instrument), which generally exhibits a strong harmonic structure with a distinctive predominant pitch contour. Drawing from findings in cognitive psychology, we propose to investigate the simple combination of two dedicated approaches for separating those two components: a rhythm-based method that focuses on extracting the background via a rhythmic mask derived from identifying the repeating time elements in the mixture and a pitch-based method that focuses on extracting the melody via a harmonic mask derived from identifying the predominant pitch contour in the mixture. Evaluation on a data set of song clips showed that combining such two contrasting yet complementary methods can help to improve separation performance--from the point of view of both components--compared with using only one of those methods, and also compared with two other state-of-the-art approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1884–1893},
numpages = {10},
keywords = {background, pitch, separation, melody, rhythm}
}

@article{10.1109/TASLP.2014.2348913,
author = {Wachowski, Neil and Azimi-Sadjadi, Mahmood R.},
title = {Detection and Classification of Nonstationary Transient Signals Using Sparse Approximations and Bayesian Networks},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2348913},
doi = {10.1109/TASLP.2014.2348913},
abstract = {This paper considers sequential detection and classification of multiple transient signals from vector observations corrupted with additive noise and multiple types of structured interference. Sparse approximations of observations are found to facilitate computation of the likelihood of each signal model without relying on restrictive assumptions concerning the distribution of observations. Robustness to interference may be incorporated by virtue of the inherent separation capabilities of sparse coding. Each signal model is characterized by a Bayesian Network, which captures the temporal dependency structure among coefficients in successive sparse approximations under the associated hypothesis. Generalized likelihood ratios tests may then be used to perform signal detection and classification during quiescent periods, and quiescent detection whenever a signal is present. The results of applying the proposed method to a national park soundscape analysis problem demonstrate its practical utility for detecting and classifying real acoustical sources present in complex sonic environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1750–1764},
numpages = {15},
keywords = {sparse representations, signal classification, multivariate analysis, transient detection}
}

@article{10.1109/TASLP.2014.2341913,
author = {Van Mourik, Jelle and Murphy, Damian},
title = {Explicit Higher-Order FDTD Schemes for 3D Room Acoustic Simulation},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2341913},
doi = {10.1109/TASLP.2014.2341913},
abstract = {The Finite Difference Time Domain method is gaining popularity as a means to simulate and solve room acoustical problems. In this paper, a new set of stencils is defined that approximate the wave equation with a high degree of accuracy and lower dispersion error. Compared to the previously presented optimal scheme, the Interpolated Wideband scheme, our schemes are computationally less demanding and more practical to implement. They use at least 8 times less memory for the same audio rate and are an order of magnitude faster, although the former has a higher valid bandwidth. Despite their larger computational expense per node update, it is shown that our schemes on the whole use less memory and computation time than the Standard Rectilinear stencil, particularly when GPU implementations are employed. Lastly, a new way of visualizing and comparing valid bandwidth is recommended.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2003–2011},
numpages = {9},
keywords = {wave propagation, finite difference time domain (FDTD), numerical methods, acoustics}
}

@article{10.1109/TASLP.2014.2346315,
author = {Yella, Sree Harsha and Bourlard, Herv\'{e}},
title = {Overlapping Speech Detection Using Long-Term Conversational Features for Speaker Diarization in Meeting Room Conversations},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2346315},
doi = {10.1109/TASLP.2014.2346315},
abstract = {Overlapping speech has been identified as one of the main sources of errors in diarization of meeting room conversations. Therefore, overlap detection has become an important step prior to speaker diarization. Studies on conversational analysis have shown that overlapping speech is more likely to occur at specific parts of a conversation. They have also shown that overlap occurrence is correlated with various conversational features such as speech, silence patterns and speaker turn changes. We use features capturing this higher level information from structure of a conversation such as silence and speaker change statistics to improve acoustic feature based classifier of overlapping and single-speaker speech classes. The silence and speaker change statistics are computed over a long-term window (around 3-4 seconds) and are used to predict the probability of overlap in the window. These estimates are then incorporated into a acoustic feature based classifier as prior probabilities of the classes. Experiments conducted on three corpora (AMI, NIST-RT and ICSI) have shown that the proposed method improves the performance of acoustic feature-based overlap detector on all the corpora. They also reveal that the model based on long-term conversational features used to estimate probability of overlap which is learned from AMI corpus generalizes to meetings from other corpora (NIST-RT and ICSI). Moreover, experiments on ICSI corpus reveal that the proposed method also improves laughter overlap detection. Consequently, applying overlap handling techniques to speaker diarization using the detected overlap results in reduction of diarization error rate (DER) on all the three corpora.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1688–1700},
numpages = {13},
keywords = {simultaneous speakers, spontaneous conversations, meeting room recordings, speaker diarization, spontaneous overlapping speech}
}

@article{10.1109/TASLP.2014.2361022,
author = {Erro, Daniel and Zorila, Tudor-Catalin and Stylianou, Yannis},
title = {Enhancing the Intelligibility of Statistically Generated Synthetic Speech by Means of Noise-Independent Modifications},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2361022},
doi = {10.1109/TASLP.2014.2361022},
abstract = {When speaking devices such as smartphones, tablet-PCs, or GPS systems are used in noisy outdoor environments, the intelligibility of speech significantly drops. This is even more pronounced when synthetic speech is used. This article describes how a statistical parametric speech synthesis system trained on an ordinary synthesis database can be designed to generate highly intelligible speech, even at very low signal-to-noise ratios. By using a simple and flexible vocoder based on a full-band harmonic model, the proposed system applies deterministic noise-independent modifications at several levels: speaking rate, average fundamental frequency level and range, energy contour over time, formant sharpness, and intensity of specific spectral bands. The degree of intelligibility achieved by the system has been evaluated by means of a large-scale subjective test, the results of which show that the suggested approach clearly outperforms a reference state-of-the-art TTS system and also unmodified natural speech in some conditions. In comparison with alternative systems evaluated in the same framework, the proposed one exhibits the best performance in the scenarios with lowest signal-to-noise ratio. Finally, the impact of the suggested modifications on naturalness, quality and similarity to the original natural voice is quantified by means of a subjective test.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2101–2111},
numpages = {11},
keywords = {harmonic model, speech enhancement, voice transformation, speech intelligibility in noise, statistical parametric speech synthesis}
}

@article{10.1109/TASLP.2014.2362009,
author = {Mohammadi, Amir and Sarfjoo, Seyyed Saeed and Demiro\u{g}lu, Cenk},
title = {Eigenvoice Speaker Adaptation with Minimal Data for Statistical Speech Synthesis Systems Using a MAP Approach and Nearest-Neighbors},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2362009},
doi = {10.1109/TASLP.2014.2362009},
abstract = {Statistical speech synthesis (SSS) systems have the ability to adapt to a target speaker with a couple of minutes of adaptation data. Developing adaptation algorithms to further reduce the number of adaptation utterances to a few seconds of data can have substantial effect on the deployment of the technology in real-life applications such as consumer electronics devices. The traditional way to achieve such rapid adaptation is the eigenvoice technique which works well in speech recognition but known to generate perceptual artifacts in statistical speech synthesis. Here, we propose three methods to alleviate the quality problems of the baseline eigenvoice adaptation algorithm while allowing speaker adaptation with minimal data. Our first method is based on using a Bayesian eigenvoice approach for constraining the adaptation algorithm to move in realistic directions in the speaker space to reduce artifacts. Our second method is based on finding pretrained reference speakers that are close to the target speaker and utilizing only those reference speaker models in a second eigenvoice adaptation iteration. Both techniques performed significantly better than the baseline eigenvoice method in the objective tests. Similarly, they both improved the speech quality in subjective tests compared to the baseline eigenvoice method. In the third method, tandem use of the proposed eigenvoice method with a state-of-the-art linear regression based adaptation technique is found to improve adaptation of excitation features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2146–2157},
numpages = {12},
keywords = {nearest neighbor, eigenvoice adaptation, speaker adaptation, cluster adaptive training, statistical speech synthesis}
}

@article{10.1109/TASLP.2014.2364130,
author = {Li, Weifeng and Wang, Longbiao and Zhou, Yicong and Dines, John and Magimai.-Doss, Mathew and Bourlard, Herv\'{e} and Liao, Qingmin},
title = {Feature Mapping of Multiple Beamformed Sources for Robust Overlapping Speech Recognition Using a Microphone Array},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2364130},
doi = {10.1109/TASLP.2014.2364130},
abstract = {This paper introduces a nonlinear vector-based feature mapping approach to extract robust features for automatic speech recognition (ASR) of overlapping speech using a microphone array. We explore different configurations and additional sources of information to improve the effectiveness of the feature mapping. First, we investigate the full-vector based mapping of different sources in a log mel-filterbank energy (log MFBE) domain, and demonstrate that retraining the acoustic model using the generated training data can help improve the recognition performance. Then we investigate the feature mapping between different domains. Finally in order to improve the qualities of the mapping inputs we propose a nonlinear mapping of the features from multiple beamformed sources, which are directed at the target and interfering speakers, respectively. We demonstrate the effectiveness of the proposed approach through extensive evaluations on the MONC corpus, which includes non-overlapping single speaker and overlapping multi-speaker conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2244–2255},
numpages = {12},
keywords = {neural network, speech recognition, beamforming, speech separation, microphone array}
}

@article{10.1109/TASLP.2014.2359626,
author = {Yong, Pei Chee and Nordholm, Sven and Dam, Hai Huyen},
title = {Effective Binaural Multi-Channel Processing Algorithm for Improved Environmental Presence},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2359626},
doi = {10.1109/TASLP.2014.2359626},
abstract = {Binaural noise-reduction algorithms based on multi-channel Wiener filter (MWF) are promising techniques to be used in binaural assistive listening devices. The real-time implementation of the existing binaural MWF methods, however, involves challenges to increase the amount of noise reduction without imposing speech distortion, and at the same time preserving the binaural cues of both speech and noise components. Although significant efforts have been made in the literature, most developed methods so far have focused only on either the former or latter problem. This paper proposes an alternative binaural MWF algorithm that incorporates the nonstationarity of the signal components into the framework. The main objective is to design an algorithm that would be able to select the sources that are present in the environment. To achieve this, a modified speech presence probability (SPP) and a single-channel speech enhancement algorithm are utilized in the formulation. The resulting optimal filter also avoids the poor estimation of the second-order clean speech statistics, which is normally done by simple subtraction. Theoretical analysis and performance evaluation using realistic recorded data shows the advantage of the proposed method over the reference MWF solution in terms of the binaural cues preservation, as well as the noise reduction and speech distortion.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2012–2024},
numpages = {13},
keywords = {single-channel noise reduction, multi-channel wiener filter, binaural cues, speech enhancement, modified sigmoid function}
}

@article{10.1109/TASLP.2014.2347133,
author = {Weng, Chao and Thomson, David L. and Haffner, Patrick and Juang, Biing-Hwang},
title = {Latent Semantic Rational Kernels for Topic Spotting on Conversational Speech},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2347133},
doi = {10.1109/TASLP.2014.2347133},
abstract = {In this work, we propose latent semantic rational kernels (LSRK) for topic spotting on conversational speech. Rather than mapping the input weighted finite-state transducers (WFSTs) onto a high dimensional n-gram feature space as in n-gram rational kernels, the proposed LSRK maps the WFSTs onto a latent semantic space. With the proposed LSRK, all available external knowledge and techniques can be flexibly integrated into a unified WFST based framework to boost the topic spotting performance. We present how to generalize the LSRK using tf-idf weighting, latent semantic analysis, WordNet and probabilistic topic models. To validate the proposed LSRK framework, we conduct the topic spotting experiments on two datasets, Switchboard and AT&amp;T HMIHY0300 initial collection. The experimental results show that with the proposed LSRK we can achieve significant and consistent topic spotting performance gains over the n-gram rational kernels.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1738–1749},
numpages = {12},
keywords = {topic spotting, tf-idf, LSA, WFSTs, LDA, PLSA, rational kernels}
}

@article{10.1109/TASLP.2014.2352556,
author = {D'Angelo, Stefano and V\"{a}lim\"{a}ki, Vesa},
title = {Generalized Moog Ladder Filter: Part II-Explicit Nonlinear Model through a Novel Delay-Free Loop Implementation Method},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2352556},
doi = {10.1109/TASLP.2014.2352556},
abstract = {One of the most critical aspects of virtual analog simulation of circuits for music production consists in accurate reproduction of their nonlinear behavior, yet this goal is in many cases difficult to achieve due to the presence of implicit differential equations in circuit models, since they naturally map to delay-free loops in the digital domain. This paper presents a novel and general method for non-iteratively implementing these loops in such a way that the linear response around a chosen operating point is preserved, the topology is minimally affected, and transformation of nonlinearities is not required. This technique is then applied to a generalized model of the Moog ladder filter, resulting in an implementation that outperforms its predecessors with only a modest computational load penalty. This digital version of the filter is shown to offer strong stability guarantees w.r.t. parameter variation, allows the extraction of different frequency response modes by simple mixing of individual ladder stage outputs, and is suitable for real-time sound synthesis and audio effects processing.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1873–1883},
numpages = {11},
keywords = {circuit simulation, music, resonator filters, IIR filters, acoustic signal processing}
}

@article{10.1109/TASLP.2014.2347135,
author = {Davies, Matthew E. P. and Hamel, Philippe and Yoshii, Kazuyoshi and Goto, Masataka},
title = {AutoMashUpper: Automatic Creation of Multi-Song Music Mashups},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2347135},
doi = {10.1109/TASLP.2014.2347135},
abstract = {In this paper we present a system, AutoMashUpper, for making multi-song music mashups. Central to our system is a measure of "mashability" calculated between phrase sections of an input song and songs in a music collection. We define mashability in terms of harmonic and rhythmic similarity and a measure of spectral balance. The principal novelty in our approach centres on the determination of how elements of songs can be made fit together using key transposition and tempo modification, rather than based on their unaltered properties. In this way, the properties of two songs used to model their mashability can be altered with respect to transformations performed to maximize their perceptual compatibility. AutoMashUpper has a user interface to allow users to control the parameterization of the mashability estimation. It allows users to define ranges for key shifts and tempo as well as adding, changing or removing elements from the created mashups. We evaluate AutoMashUpper by its ability to reliably segment music signals into phrase sections, and also via a listening test to examine the relationship between estimated mashability and user enjoyment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1726–1737},
numpages = {12},
keywords = {music signal processing, music remixing, audio user interfaces, creative MIR}
}

@article{10.1109/TASLP.2014.2359159,
author = {Chen, Jitong and Wang, Yuxuan and Wang, DeLiang},
title = {A Feature Study for Classification-Based Speech Separation at Low Signal-to-Noise Ratios},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2359159},
doi = {10.1109/TASLP.2014.2359159},
abstract = {Speech separation can be formulated as a classification problem. In classification-based speech separation, supervised learning is employed to classify time-frequency units as either speech-dominant or noise-dominant. In very low signal-to-noise ratio (SNR) conditions, acoustic features extracted from a mixture are crucial for correct classification. In this study, we systematically evaluate a range of promising features for classification-based separation using six nonstationary noises at the low SNR level of -5 dB, which is chosen with the goal of improving human speech intelligibility in mind. In addition, we propose a new feature called multi-resolution cochleagram (MRCG). The new feature is constructed by combining four cochleagrams at different spectrotemporal resolutions in order to capture both the local and contextual information. Experimental results show that MRCG gives the best classification results among all evaluated features. In addition, our results indicate that auto-regressive moving average (ARMA) filtering, a post-processing technique for improving automatic speech recognition features, also improves many acoustic features for speech separation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1993–2002},
numpages = {10},
keywords = {speech separation, ARMA filtering, multi-resolution cochleagram, classification}
}

@article{10.1109/TASLP.2014.2357676,
author = {Foster, Peter and Mauch, Matthias and Dixon, Simon},
title = {Sequential Complexity as a Descriptor for Musical Similarity},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2357676},
doi = {10.1109/TASLP.2014.2357676},
abstract = {We propose string compressibility as a descriptor of temporal structure in audio, for the purpose of determining musical similarity. Our descriptors are based on computing trackwise compression rates of quantized audio features, using multiple temporal resolutions and quantization granularities. To verify that our descriptors capture musically relevant information, we incorporate our descriptors into similarity rating prediction and song year prediction tasks. We base our evaluation on a dataset of 15500 track excerpts of Western popular music, for which we obtain 7800 web-sourced pairwise similarity ratings. To assess the agreement among similarity ratings, we perform an evaluation under controlled conditions, obtaining a rank correlation of 0.33 between intersected sets of ratings. Combined with bag-of-features descriptors, we obtain performance gains of 31.1% and 10.9% for similarity rating prediction and song year prediction. For both tasks, analysis of selected descriptors reveals that representing features at multiple time scales benefits prediction accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1965–1977},
numpages = {13},
keywords = {musical similarity measures, music content analysis, time series complexity}
}

@article{10.1109/TASLP.2014.2344856,
author = {Reddy, Vinod Veera and Khong, Andy W. H. and Ng, Boon Poh},
title = {Unambiguous Speech DOA Estimation under Spatial Aliasing Conditions},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2344856},
doi = {10.1109/TASLP.2014.2344856},
abstract = {With the bandwidth of speech signals extending over several octaves, the spatial Nyquist criterion constrains the microphone array design. Violating this criterion by increasing microphone spacing in order to achieve high resolution introduces ambiguity in identifying the source directions due to the aliasing components. In this work, we investigate the effect of spatial aliasing on the direction-of-arrival (DOA) spectrum due to wideband sources. Noting that the extent of aliasing is frequency dependent, we propose a multi-stage scheme for speech DOA estimation following a subband decomposition. To observe the advantage of this scheme, we verify it with the steered minimum variance distortionless response (STMV) and approximate kernel density estimators. The performance is evaluated with simulations and recorded room impulse responses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2133–2145},
numpages = {13},
keywords = {spatial aliasing, DOA estimation}
}

@article{10.1109/TASLP.2014.2363410,
author = {Han, Kun and Wang, DeLiang},
title = {Neural Network Based Pitch Tracking in Very Noisy Speech},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2363410},
doi = {10.1109/TASLP.2014.2363410},
abstract = {Pitch determination is a fundamental problem in speech processing, which has been studied for decades. However, it is challenging to determinate pitch in strong noise because the harmonic structure is corrupted. In this paper, we estimate pitch using supervised learning, where the probabilistic pitch states are directly learned from noisy speech data. We investigate two alternative neural networks modeling pitch state distribution given observations. The first one is a feedforward deep neural network (DNN), which is trained on static frame-level acoustic features. The second one is a recurrent deep neural network (RNN) which is trained on sequential frame-level features and capable of learning temporal dynamics. Both DNNs and RNNs produce accurate probabilistic outputs of pitch states, which are then connected into pitch contours by Viterbi decoding. Our systematic evaluation shows that the proposed pitch tracking algorithms are robust to different noise conditions and can even be applied to reverberant speech. The proposed approach also significantly outperforms other state-of-the-art pitch tracking algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2158–2168},
numpages = {11},
keywords = {recurrent neural networks (RNNs), deep neural networks (DNNs), pitch estimation, supervised learning, viterbi decoding}
}

@article{10.1109/TASLP.2014.2355774,
author = {Panagakis, Yannis and Kotropoulos, Constantine L. and Arce, Gonzalo R.},
title = {Music Genre Classification via Joint Sparse Low-Rank Representation of Audio Features},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2355774},
doi = {10.1109/TASLP.2014.2355774},
abstract = {A novel framework for music genre classification, namely the joint sparse low-rank representation (JSLRR) is proposed in order to: 1) smooth the noise in the test samples, and 2) identify the subspaces that the test samples lie onto. An efficient algorithm is proposed for obtaining the JSLRR and a novel classifier is developed, which is referred to as the JSLRR-based classifier. Special cases of the JSLRR-based classifier are the joint sparse representation-based classifier and the low-rank representation-based one. The performance of the three aforementioned classifiers is compared against that of the sparse representation-based classifier, the nearest subspace classifier, the support vector machines, and the nearest neighbor classifier for music genre classification on six manually annotated benchmark datasets. The best classification results reported here are comparable with or slightly superior than those obtained by the state-of-the-art music genre classification methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1905–1917},
numpages = {13},
keywords = {music genre classification, nuclear norm minimization, l1 norm minimization, low-rank representation, auditory representations, sparse representation}
}

@article{10.1109/TASLP.2014.2351614,
author = {Gil-Cacho, Jose M. and Van Waterschoot, Toon and Moonen, Marc and Jensen, S\o{}ren Holdt},
title = {A Frequency-Domain Adaptive Filter (FDAF) Prediction Error Method (PEM) Framework for Double-Talk-Robust Acoustic Echo Cancellation},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2351614},
doi = {10.1109/TASLP.2014.2351614},
abstract = {In this paper, we propose a new framework to tackle the double-talk (DT) problem in acoustic echo cancellation (AEC). It is based on a frequency-domain adaptive filter (FDAF) implementation of the so-called prediction error method adaptive filtering using row operations (PEM-AFROW) leading to the FDAF-PEM-AFROW algorithm. We show that FDAF-PEM-AFROW is by construction related to the best linear unbiased estimate (BLUE) of the echo path. We depart from this framework to show an improvement in performance with respect to other adaptive filters minimizing the BLUE criterion, namely the PEM-AFROW and the FDAF-NLMS with near-end signal normalization. One of the contributions is to propose the instantaneous pseudo-correlation (IPC) measure between the near-end signal and the loudspeaker signal. The IPC measure serves as an indication of the effect of a DT situation occurring during adaptation. We motivate the choice of FDAF-PEM-AFROW over PEM-AFROW and FDAF-NLMS with near-end signal normalization, based on performance, computational complexity and related IPC measure values. Moreover, we use the FDAF-PEM-AFROW framework to improve several state-of-the-art variable step-size (VSS) and variable regularization (VR) algorithms. The FDAF-PEM-AFROW versions significantly outperform the original versions in every simulation. In terms of computational complexity, the FDAF-PEM-AFROW versions are themselves about two orders of magnitude cheaper than the original versions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2074–2086},
numpages = {13},
keywords = {acoustic echo cancellation, double-talk-robust acoustic echo cancellation, double-talk, variable step size, prediction error method, frequency-domain adaptive filters}
}

@article{10.1109/TASLP.2014.2360646,
author = {Kim, Seon Man and Kim, Hong Kook},
title = {Direction-of-Arrival Based SNR Estimation for Dual-Microphone Speech Enhancement},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2360646},
doi = {10.1109/TASLP.2014.2360646},
abstract = {In this paper, we propose a method for estimating target speech by exploring the spatial cues in adverse noise environments. This method is able to reliably estimate the signal-to-noise ratio (SNR) using the phase difference obtained from dual-microphone signals. To this end, spatial cues such as the phase difference are used to estimate the target-to-non-target directional signal ratio (TNR). Based on the estimated TNR, a direction-of-arrival (DOA)-based SNR is then estimated by using a statistical model-based log-likelihood ratio test for the target speech activity decision followed by a decision-directed approach. The estimate is then incorporated into a Wiener filter in order to obtain a spectral-gain attenuator. The perceptual evaluation of speech quality shows that the performance of a dual-microphone speech enhancement system employing the proposed estimation method outperforms single-and dual-microphone speech enhancement systems that use conventional methods such as Wiener filtering, beamforming, or phase-error-based filtering under noise conditions whose SNR ranges from 0 to 20 dB.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2207–2217},
numpages = {11},
keywords = {signal-to-noise ratio, spatial cue, direction-of-arrival, dual-microphone signal, speech enhancement, target-to-non-target directional signal ratio}
}

@article{10.1109/TASLP.2014.2360643,
author = {Huang, Gongping and Benesty, Jacob and Long, Tao and Chen, Jingdong},
title = {A Family of Maximum SNR Filters for Noise Reduction},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2360643},
doi = {10.1109/TASLP.2014.2360643},
abstract = {This paper is devoted to the study and analysis of the maximum signal-to-noise ratio (SNR) filters for noise reduction both in the time and short-time Fourier transform (STFT) domains with one single microphone and multiple microphones. In the time domain, we show that the maximum SNR filters can significantly increase the SNR but at the expense of tremendous speech distortion. As a consequence, the speech quality improvement, measured by the perceptual evaluation of speech quality (PESQ) algorithm, is marginal if any, regardless of the number of microphones used. In the STFT domain, the maximum SNR filters are formulated by considering the interframe information in every frequency band. It is found that these filters not only improve the SNR, but also improve the speech quality significantly. As the number of input channels increases so is the gain in SNR as well as the speech quality. This demonstrates that the maximum SNR filters, particularly the multichannel ones, in the STFT domain may be of great practical value.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2034–2047},
numpages = {14},
keywords = {short-time Fourier transform (STFT) domain, time domain, multichannel, maximum SNR filter, speech enhancement, noise reduction, single channel}
}

@article{10.1109/TASLP.2014.2363788,
author = {Santos, Jo\~{a}o F. and Falk, Tiago H.},
title = {Updating the SRMR-CI Metric for Improved Intelligibility Prediction for Cochlear Implant Users},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2363788},
doi = {10.1109/TASLP.2014.2363788},
abstract = {When compared to intrusive speech intelligibility metrics, non-intrusive ones show a stronger dependency on speech content, given the lack of a reference signal for distortion level computation. Reduction of this dependency is an important step needed to develop reliable metrics. In this paper, two different updates to SRMR-CI, a recently-proposed speech intelligibility metric tailored for cochlear implant users, are applied. First, modulation energy thresholding is proposed to reduce the variability caused by the differences in modulation spectral representations for different phonemes and speakers, as well as speech enhancement algorithm artifacts. Second, a narrower range of modulation filters is employed to reduce fundamental frequency effects. Experimental results show that the updated metric outperforms two benchmark metrics, namely ModA and ANIQUE , by as much as 15% in terms of correlation between objective and subjective ratings, and a relative decrease of 47% in root mean square error compared to the previously-proposed SRMR-CI metric.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2197–2206},
numpages = {10},
keywords = {non-intrusive, speech intelligibility, modulation spectrum, objective metrics, cochlear implants}
}

@article{10.1109/TASLP.2014.2351132,
author = {Cobos, Maximo and Perez-Solano, Juan J. and Felici-Castell, Santiago and Segura, Jaume and Navarro, Juan M.},
title = {Cumulative-Sum-Based Localization of Sound Events in Low-Cost Wireless Acoustic Sensor Networks},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2351132},
doi = {10.1109/TASLP.2014.2351132},
abstract = {Wireless acoustic sensor networks (WASNs) are known for their potential applications in multiple areas, such as audio-based surveillance, binaural hearing aids or advanced acoustic monitoring. The knowledge of the spatial position of a source of interest is usually a requirement for many of these applications. Therefore, source localization is an important problem to be addressed in WASNs. Unfortunately, most localization algorithms need costly signal processing stages that prevent them from being implemented in low-cost sensor networks, requiring additional modules for signal acquisition and processing. This paper presents a low-complexity method for acoustic event detection and localization considering a change detection statistical framework. Two possible implementation approaches based on the efficient cumulative sum (CUSUM) algorithm are presented and discussed. Results from simulations and a real deployment show that the proposed techniques can be easily implemented in low-cost sensor networks, providing good localization accuracy and making good use of the available node resources.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1792–1802},
numpages = {11},
keywords = {wireless sensor networks, source localization, cumulative sum}
}

@article{10.1109/TASLP.2014.2359628,
author = {Chen, Austin and Hasegawa-Johnson, Mark A.},
title = {Mixed Stereo Audio Classification Using a Stereo-Input Mixed-to-Panned Level Feature},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2359628},
doi = {10.1109/TASLP.2014.2359628},
abstract = {Many past studies have been conducted on speech/music discrimination due to the potential applications for broadcast and other media; however, it remains possible to expand the experimental scope to include samples of speech with varying amounts of background music. This paper focuses on the development and evaluation of two measures of the ratio between speech energy and music energy: a reference measure called speech-to-music ratio (SMR), which is known objectively only prior to mixing, and a feature called the stereo-input mix-to-peripheral level feature (SIMPL), which is computed from the stereo mixed signal as an imprecise estimate of SMR. SIMPL is an objective signal measure calculated by taking advantage of broadcast mixing techniques in which vocals are typically placed at stereo center, unlike most instruments. Conversely, SMR is a hidden variable defined by the relationship between the powers of portions of audio attributed to speech and music. It is shown that SIMPL is predictive of SMR and can be combined with state-of-the-art features in order to improve performance. For evaluation, this new metric is applied in speech/music (binary) classification, speech/music/mixed (trinary) classification, and a new speech-to-music ratio estimation problem. Promising results are achieved, including 93.06% accuracy for trinary classification and 3.86 dB RMSE for estimation of the SMR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2025–2033},
numpages = {9},
keywords = {mel-frequency cepstral coefficients, music processing, speech/music discrimination, speech processing, audio segmentation, Gaussian mixture model, classification algorithms, music information retrieval, audio classification, audio processing}
}

@article{10.1109/TASLP.2014.2361023,
author = {Jiang, Yi and Wang, DeLiang and Liu, RunSheng and Feng, ZhenMing},
title = {Binaural Classification for Reverberant Speech Segregation Using Deep Neural Networks},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2361023},
doi = {10.1109/TASLP.2014.2361023},
abstract = {Speech signal degradation in real environments mainly results from room reverberation and concurrent noise. While human listening is robust in complex auditory scenes, current speech segregation algorithms do not perform well in noisy and reverberant environments. We treat the binaural segregation problem as binary classification, and employ deep neural networks (DNNs) for the classification task. The binaural features of the interaural time difference and interaural level difference are used as the main auditory features for classification. The monaural feature of gammatone frequency cepstral coefficients is also used to improve classification performance, especially when interference and target speech are collocated or very close to one another. We systematically examine DNN generalization to untrained spatial configurations. Evaluations and comparisons show that DNN-based binaural classification produces superior segregation performance in a variety of multisource and reverberant conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2112–2121},
numpages = {10},
keywords = {room reverberation, computational auditory scene analysis (CASA), speech segregation, deep neural networks (DNNs), binary classification}
}

@article{10.1109/TASLP.2014.2344862,
author = {Defraene, Bruno and Van Waterschoot, Toon and Diehl, Moritz and Moonen, Marc},
title = {Embedded-Optimization-Based Loudspeaker Precompensation Using a Hammerstein Loudspeaker Model},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2344862},
doi = {10.1109/TASLP.2014.2344862},
abstract = {This paper presents an embedded-optimization-based loudspeaker precompensation algorithm using a Hammerstein loudspeaker model, i.e. a cascade of a memoryless nonlinearity and a linear finite impulse response filter. The loudspeaker precompensation consists in a per-frame signal optimization. In order to minimize the perceptible distortion incurred in the loudspeaker, a psychoacoustically motivated optimization criterion is proposed. The resulting per-frame signal optimization problems are solved efficiently using first-order optimization methods. Depending on the invertibility and the smoothness of the memoryless nonlinearity, different first-order optimization methods are proposed and their convergence properties are analyzed. Objective evaluation experiments using synthetic loudspeaker models and real loudspeakers show that the proposed loudspeaker precompensation algorithm provides a significant audio quality improvement, especially so at high playback levels.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1648–1659},
numpages = {12},
keywords = {loudspeaker precompensation, sound perception, gradient optimization, hammerstein model, embedded optimization}
}

@article{10.1109/TASLP.2014.2341918,
author = {Togami, Masahito and Kawaguchi, Yohei},
title = {Simultaneous Optimization of Acoustic Echo Reduction, Speech Dereverberation, and Noise Reduction against Mutual Interference},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2341918},
doi = {10.1109/TASLP.2014.2341918},
abstract = {We propose an optimized speech enhancement method that combines acoustic echo reduction, speech dereverberation, and noise reduction in a unified framework. Normally, partial optimization of acoustic echo reduction, speech dereverberation, and noise reduction does not lead to total optimization. A cascade method of multiple functions causes mutual interference between these functions and degrades eventual speech enhancement performance. Unlike cascade methods, the proposed method combines all functions to optimize eventual speech enhancement performance based on a unified framework, which is also robust against the mutual interference problem. With the proposed method, in addition to time-invariant linear filters, time-varying filters are used to reduce residual reverberation, residual acoustic echo signal, and background noise signal which cannot be reduced using time-invariant filters. These time-invariant filters and time-varying filters are also optimized based on a unified likelihood function to avoid the mutual interference problem. By combining the time-invariant linear filters and the time-varying filters, the proposed method uses a local Gaussian model with a full-rank covariance matrix and a non-zero average vector as a probabilistic model of the microphone input signal. In the local Gaussian model, non-stationary characteristics of speech sources are considered to effectively enhance speech sources. Under this probabilistic model, all the parameters are optimized simultaneously based on the expectation-maximization algorithm and calculates a minimum mean squared error estimate of a desired signal. The experimental results show that the proposed method is superior to the cascade methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1612–1623},
numpages = {12},
keywords = {simultaneous optimization, echo reduction, dereverberation, local Gaussian modeling}
}

@article{10.1109/TASLP.2014.2341920,
author = {Badeau, Roland and Plumbley, Mark D.},
title = {Multichannel High-Resolution NMF for Modeling Convolutive Mixtures of Non-Stationary Signals in the Time-Frequency Domain},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2341920},
doi = {10.1109/TASLP.2014.2341920},
abstract = {Several probabilistic models involving latent components have been proposed for modeling time-frequency (TF) representations of audio signals such as spectrograms, notably in the nonnegative matrix factorization (NMF) literature. Among them, the recent high-resolution NMF (HR-NMF) model is able to take both phases and local correlations in each frequency band into account, and its potential has been illustrated in applications such as source separation and audio inpainting. In this paper, HR-NMF is extended to multichannel signals and to convolutive mixtures. The new model can represent a variety of stationary and non-stationary signals, including autoregressive moving average (ARMA) processes and mixtures of damped sinusoids. A fast variational expectation-maximization (EM) algorithm is proposed to estimate the enhanced model. This algorithm is applied to piano signals, and proves capable of accurately modeling reverberation, restoring missing observations, and separating pure tones with close frequencies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1670–1680},
numpages = {11},
keywords = {non-stationary signal modeling, multichannel signal analysis, variational EM algorithm, nonnegative matrix factorization, time-frequency analysis}
}

@article{10.1109/TASLP.2014.2341911,
author = {Xu, Jian and Yan, Zhi-Jie and Huo, Qiang},
title = {An Unsupervised Adaptation Approach to Leveraging Feedback Loop Data by Using I-Vector for Data Clustering and Selection},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2341911},
doi = {10.1109/TASLP.2014.2341911},
abstract = {We present a study of using unsupervised adaptation approaches to improve speech recognition accuracy of a deployed speech service by leveraging large-scale untranscribed speech data collected from a feedback loop (FBL). For a regular user with lots of adaptation utterances, conventional CMLLR-based adaptation can be used for personalization directly. For a casual user with a few adaptation utterances, we propose to use CMLLR-based adaptation by augmenting his / her adaptation utterances with utterances acoustically close to the user, which are selected from the FBL data by an i-vector based approach. For a new user, we propose to perform a CMLLR-based recognition of an unknown utterance by selecting a set of CMLLR transforms from the most similar cluster, which are pre-trained by using the utterances from the corresponding cluster generated by an i-vector based utterance clustering method from the FBL data. The effectiveness of the above approaches are confirmed by our experiments on a short message dictation task on smart phones.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1581–1589},
numpages = {9},
keywords = {feedback loop, unsupervised adaptation, personalization, data augmentation, i-vector, data clustering}
}

@article{10.1109/TASLP.2014.2341914,
author = {Cumani, Sandro and Laface, Pietro},
title = {Large-Scale Training of Pairwise Support Vector Machines for Speaker Recognition},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2341914},
doi = {10.1109/TASLP.2014.2341914},
abstract = {State-of-the-art systems for text-independent speaker recognition use as their features a compact representation of a speaker utterance, known as "i-vector." We recently presented an efficient approach for training a Pairwise Support Vector Machine (PSVM) with a suitable kernel for i-vector pairs for a quite large speaker recognition task. Rather than estimating an SVM model per speaker, according to the "one versus all" discriminative paradigm, the PSVM approach classifies a trial, consisting of a pair of i-vectors, as belonging or not to the same speaker class. Training a PSVM with large amount of data, however, is a memory and computational expensive task, because the number of training pairs grows quadratically with the number of training i-vectors. This paper demonstrates that a very small subset of the training pairs is necessary to train the original PSVM model, and proposes two approaches that allow discarding most of the training pairs that are not essential, without harming the accuracy of the model. This allows dramatically reducing the memory and computational resources needed for training, which becomes feasible with large datasets including many speakers. We have assessed these approaches on the extended core conditions of the NIST 2012 Speaker Recognition Evaluation. Our results show that the accuracy of the PSVM trained with a sufficient number of speakers is 10%-30% better compared to the one obtained by a PLDA model, depending on the testing conditions. Since the PSVM accuracy increases with the training set size, but PSVM training does not scale well for large numbers of speakers, our selection techniques become relevant for training accurate discriminative classifiers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1590–1600},
numpages = {11},
keywords = {PLDA, support vectors, speaker recognition, i-vector, pairwise support vector machines}
}

@article{10.1109/TASLP.2014.2344852,
author = {Lorente, Jorge and Ferrer, Miguel and De Diego, Maria and Gonz\'{a}lez, Alberto},
title = {GPU Implementation of Multichannel Adaptive Algorithms for Local Active Noise Control},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2344852},
doi = {10.1109/TASLP.2014.2344852},
abstract = {Multichannel active noise control (ANC) systems are commonly based on adaptive signal processing algorithms that require high computational capacity, which constrains their practical implementation. Graphics Processing Units (GPUs) are well known for their potential for highly parallel data processing. Therefore, GPUs seem to be a suitable platform for multichannel scenarios. However, efficient use of parallel computation in the adaptive filtering context is not straightforward due to the feedback loops. This paper compares two GPU implementations of a multichannel feedforward local ANC system working as a real-time prototype. Both GPU implementations are based on the filtered-x Least Mean Square algorithms; one is based on the conventional filtered-x scheme and the other is based on the modified filtered-x scheme. Details regarding the parallelization of the algorithms are given. Finally, experimental results are presented to compare the performance of both multichannel ANC GPU implementations. The results show the usefulness of many-core devices for developing versatile, scalable, and low-cost multichannel ANC systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1624–1635},
numpages = {12},
keywords = {filtered-x least mean square, active noise control, graphics processing unit}
}

@article{10.1109/TASLP.2014.2344855,
author = {Wang, Guangsen and Sim, Khe Chai},
title = {Regression-Based Context-Dependent Modeling of Deep Neural Networks for Speech Recognition},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2344855},
doi = {10.1109/TASLP.2014.2344855},
abstract = {The data sparsity problem is addressed by using the decision tree state clusters as the training targets for the state-of-the-art context-dependent (CD) deep neural network (DNN) systems. The CD states within a cluster cannot be distinguished at the frame level. We surmise that the state clustering may cause an issue for the standard CD-DNNs, which has so far not been addressed in the literature. In this paper, a logistic regression framework is proposed for the CD-DNNs based on a set of broad phone classes to address both the data sparsity and the clustering problems. To address the data sparsity issue, the triphones are clustered into shorter biphones with broad phone contexts under multiple articulatory categories. A DNN is trained to discriminate the disjoint biphone clusters within each articulatory category. The regression bases are formed by the concatenated log posterior probabilities of all the broad phone DNNs. Logistic regression is used to transform the regression bases into the triphone state posteriors. Clustering of the regression parameters is used to reduce the regression model complexity while still achieving unique acoustic scores for all possible triphones. Based on some approximations, the regression model can be trained as a sparse softmax layer and its parameters can be learned by optimizing the cross-entropy criterion. The experimental results on a broadcast news transcription task reveal that the proposed regression-based CD-DNN significantly outperforms the standard CD-DNN. The best system provides a 1.3% absolute word error rate reduction compared to the best standard CD-DNN system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1660–1669},
numpages = {10},
keywords = {context dependent modeling, deep neural network, logistic regression, articulatory features}
}

@article{10.1109/TASLP.2014.2341912,
author = {Du, Jun and Huo, Qiang},
title = {An Improved VTS Feature Compensation Using Mixture Models of Distortion and IVN Training for Noisy Speech Recognition},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2341912},
doi = {10.1109/TASLP.2014.2341912},
abstract = {In our previous work, we proposed a feature compensation approach using high-order vector Taylor series (VTS) approximation for noisy speech recognition. In this paper, we report new progress on making it more powerful and practical in real applications. First, mixtures of densities are used to enhance the distortion models of both additive noise and convolutional distortion. New formulations for maximum likelihood (ML) estimation of distortion model parameters, and minimum mean squared error (MMSE) estimation of clean speech are derived and presented. Second, we improve the feature compensation in both efficiency and accuracy by applying higher order information of VTS approximation only to the noisy speech mean parameters, and a temporal smoothing operation for the posterior probability of Gaussian mixture components in clean speech estimation. Finally, we design a procedure to perform irrelevant variability normalization (IVN) based joint training of a reference Gaussian mixture model (GMM) for feature compensation and hidden Markov models (HMMs) for acoustic modeling using VTS-based feature compensation. The effectiveness of our proposed approach is confirmed by experiments on Aurora3 benchmark database for a real-world in-vehicle connected digits recognition task. Compared with ETSI advanced front-end, our approach achieves significant recognition accuracy improvement across three "training-testing" conditions for four languages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1601–1611},
numpages = {11},
keywords = {irrelevant variability normalization, mixture model of distortion, noisy speech recognition, vector Taylor series, feature compensation}
}

@article{10.1109/TASLP.2014.2323715,
author = {H\'{e}lie, Thomas},
title = {Simulation of Fractional-Order Low-Pass Filters},
year = {2014},
issue_date = {November 2014},
publisher = {IEEE Press},
volume = {22},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2323715},
doi = {10.1109/TASLP.2014.2323715},
abstract = {The attenuation of standard analog low-pass filters corresponds to a multiple value of decibels per octave. This quantified value is related to the order of the filter. The issue addressed here is concerned with the extension of integer orders to non integer orders, such that the attenuation of a low-pass filter can be continuously adjusted. Fractional differential systems are known to provide such asymptotic behaviors and many results about their simulation are available. But even for a fixed cutoff frequency, their combination does not generate an additive group with respect to the order and they involve stability problems. In this paper, a class of low-pass filters with orders between 0 (the filter is a unit gain) and 1 (standard one-pole filter) is defined to restore these properties. These infinite dimensional filters are not fractional differential but admit some well-posed representations into weighted integrals of standard one-pole filters. Based on this, finite dimensional approximations are proposed and recast into the framework of state-space representations. A special care is given to reduce the computational complexity, through the dimension of the state. In practice, this objective is reached for the complete family, without damaging the perceptive quality, with dimension 13. Then, an accurate low-cost digital version of this family is built in the time-domain. The accuracy of the digital filters is verified on the complete range of parameters (cutoff frequencies and fractional orders). Moreover, the stability is guaranteed, even for time-varying parameters. As an application, a plugin has been implemented which provides a new audio tool for tuning the cutoff frequency and the asymptotic slope in a continuous way. As a very special application, choosing a one-half order combined with a low cutoff frequency (20 Hz or less), the filter fed with a white noise provides a pink noise generator.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {1636–1647},
numpages = {12},
keywords = {irrational transfer function, simulation, fractional order, signal synthesis, state-space methods}
}

@article{10.1109/TASLP.2014.2339736,
author = {Abdel-Hamid, Ossama and Mohamed, Abdel-Rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
title = {Convolutional Neural Networks for Speech Recognition},
year = {2014},
issue_date = {October 2014},
publisher = {IEEE Press},
volume = {22},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2339736},
doi = {10.1109/TASLP.2014.2339736},
abstract = {Recently, the hybrid deep neural network (DNN)- hidden Markov model (HMM) has been shown to significantly improve speech recognition performance over the conventional Gaussian mixture model (GMM)-HMM. The performance improvement is partially attributed to the ability of the DNN to model complex correlations in speech features. In this paper, we show that further error rate reduction can be obtained by using convolutional neural networks (CNNs). We first present a concise description of the basic CNN and explain how it can be used for speech recognition. We further propose a limited-weight-sharing scheme that can better model speech features. The special structure such as local connectivity, weight sharing, and pooling in CNNs exhibits some degree of invariance to small shifts of speech features along the frequency axis, which is important to deal with speaker and environment variations. Experimental results show that CNNs reduce the error rate by 6%-10% compared with DNNs on the TIMIT phone recognition and the voice search large vocabulary speech recognition tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1533–1545},
numpages = {13},
keywords = {pooling, convolution, convolutional neural networks, limited weight sharing (LWS) scheme}
}

@article{10.1109/TASLP.2014.2337844,
author = {Zhao, Liheng and Benesty, Jacob and Chen, Jingdong},
title = {Design of Robust Differential Microphone Arrays},
year = {2014},
issue_date = {October 2014},
publisher = {IEEE Press},
volume = {22},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2337844},
doi = {10.1109/TASLP.2014.2337844},
abstract = {Differential microphone arrays (DMAs), due to their small size and enhanced directivity, are quite promising in speech enhancement applications. However, it is well known that differential beamformers have the drawback of white noise amplification, which is a major issue in the processing of wideband signals such as speech. In this paper, we focus on the design of robust DMAs. Based on the Maclaurin's series approximation and frequency-independent beampatterns, the robust first-, second-, and third-order DMAs are proposed by using more microphones than the order plus one, and the corresponding minimum-norm filters are derived. Compared to the traditional DMAs, the proposed designs are more robust with respect to white noise amplification while they are capable of achieving similar directional gains.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1455–1466},
numpages = {12},
keywords = {beamforming, directivity factor, second-order DMA, white noise gain, beampattern, first-order DMA, differential microphone arrays (DMAs), third-order DMA, robust DMAs}
}

@article{10.1109/TASLP.2014.2339738,
author = {Chen, Chia-Ping and Huang, Yi-Chin and Wu, Chung-Hsien and Lee, Kuan-De},
title = {Polyglot Speech Synthesis Based on Cross-Lingual Frame Selection Using Auditory and Articulatory Features},
year = {2014},
issue_date = {October 2014},
publisher = {IEEE Press},
volume = {22},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2339738},
doi = {10.1109/TASLP.2014.2339738},
abstract = {In this paper, an approach for polyglot speech synthesis based on cross-lingual frame selection is proposed. This method requires only mono-lingual speech data of different speakers in different languages for building a polyglot synthesis system, thus reducing the burden of data collection. Essentially, a set of artificial utterances in the second language for a target speaker is constructed based on the proposed cross-lingual frame-selection process, and this data set is used to adapt a synthesis model in the second language to the speaker. In the cross-lingual frame-selection process, we propose to use auditory and articulatory features to improve the quality of the synthesized polyglot speech. For evaluation, a Mandarin-English polyglot system is implemented where the target speaker only speaks Mandarin. The results show that decent performance regarding voice identity and speech quality can be achieved with the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1558–1570},
numpages = {13},
keywords = {cross-lingual frame selection, auditory features, articulatory features, polyglot speech synthesis}
}

@article{10.1109/TASLP.2014.2339735,
author = {Koyama, Shoichi and Furuya, Ken'ichi and Hiwasaki, Yusuke and Haneda, Yoichi and Suzuki, Y\^{o}iti},
title = {Wave Field Reconstruction Filtering in Cylindrical Harmonic Domain for With-Height Recording and Reproduction},
year = {2014},
issue_date = {October 2014},
publisher = {IEEE Press},
volume = {22},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2339735},
doi = {10.1109/TASLP.2014.2339735},
abstract = {For sound field reproduction that includes height (with-height reproduction), it is more efficient to record and reproduce the sound field with lower resolution in elevation than in azimuth due to the spatial abilities of human auditory perception. We propose a sound field reproduction method using horizontally arranged cylindrical arrays of microphones and loudspeakers, which is based on the wave field reconstruction (WFR) filter. With the use of cylindrical array configurations, it is possible to reproduce sound waves arriving from upper and lower directions with a smaller number of array elements at angular positions. The WFR filter is analytically derived in the cylindrical harmonic domain and allows direct transformation from the received signals of the microphones into the driving signals of the loudspeakers. A model in which microphones are mounted on a rigid cylindrical baffle is introduced to stabilize the WFR filter. Numerical simulation results indicated that the reproduction accuracy in the neighboring region along the central axis of the cylindrical array was better preserved when using the proposed method than when the method with planar arrays was used.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1546–1557},
numpages = {12},
keywords = {cylindrical harmonics, helical wave spectrum, higher order ambisonics, wave field reconstruction filter, sound field reproduction, wave field synthesis, Fourier transform}
}

@article{10.1109/TASLP.2014.2337846,
author = {Nadiri, O. and Rafaely, B.},
title = {Localization of Multiple Speakers under High Reverberation Using a Spherical Microphone Array and the Direct-Path Dominance Test},
year = {2014},
issue_date = {October 2014},
publisher = {IEEE Press},
volume = {22},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2337846},
doi = {10.1109/TASLP.2014.2337846},
abstract = {One of the major challenges encountered when localizing multiple speakers in real world environments is the need to overcome the effect of multipath distortion due to room reverberation. A wide range of methods has been proposed for speaker localization, many based on microphone array processing. Some of these methods are designed for the localization of coherent sources, typical of multipath environments, and some have even reported limited robustness to reverberation. Nevertheless, speaker localization under conditions of high reverberation still remains a challenging task. This paper proposes a novel multiple-speaker localization technique suitable for environments with high reverberation, based on a spherical microphone array and processing in the spherical harmonics (SH) domain. The non-stationarity and sparsity of speech, as well as frequency smoothing in the SH domain, are exploited in the development of a direct-path dominance test. This test can identify time-frequency (TF) bins that contain contributions from only one significant source and no significant contribution from room reflections, such that localization based on these selected TF-bins is performed accurately, avoiding the potential distortion due to other sources and reverberation. Computer simulations and an experiment in a real reverberant room validate the robustness of the proposed method in the presence of high reverberation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1494–1505},
numpages = {12},
keywords = {time-frequency analysis, speaker localization, spherical array, direction-of-arrival estimation, room reverberation}
}

@article{10.1109/TASLP.2014.2339195,
author = {Talagala, Dumidu S. and Zhang, Wen and Abhayapala, Thushara D.},
title = {Efficient Multi-Channel Adaptive Room Compensation for Spatial Soundfield Reproduction Using a Modal Decomposition},
year = {2014},
issue_date = {October 2014},
publisher = {IEEE Press},
volume = {22},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2339195},
doi = {10.1109/TASLP.2014.2339195},
abstract = {Mitigating the effects of reverberation is a significant challenge for real-world spatial soundfield reproduction, but the necessity of a large number of reproduction channels increases the complexity and presents several challenges to existing listening room compensation techniques. In this paper, we present an adaptive room compensation method to overcome the effects of reverberation within a region, using a model description of the reverberant soundfield. We propose the reverberant channel estimation and compensation be carried out in a single step using completely decoupled adaptive filters; thus, reducing the complexity of the overall process. We compare the soundfield reproduction performance with existing adaptive and nonadaptive room compensation methods through several simulation examples. The performance of the proposed method is comparable to existing techniques, and achieves a normalized wideband region reproduction error of 1% at a signal-to-noise ratio of 50 dB, within a 1 m radius region of interest using 60 loudspeakers and 55 microphones at frequencies below 1 kHz. Robust behavior of the room compensator is demonstrated down to direct-to-reverberant-path power ratios of -5 dB. Overall, the results suggest that the proposed method can diagonalize the room compensation system, leading to amore robust and parallel implementation for spatial soundfield reproduction.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1522–1532},
numpages = {11},
keywords = {modal decomposition, soundfield reproduction, reverberation, decoupled adaptive filtering, room equalization, multi-channel audio, adaptive channel estimation, room compensation}
}

@article{10.1109/TASLP.2014.2335056,
author = {Jain, Pooja and Pachori, Ram Bilas},
title = {Event-Based Method for Instantaneous Fundamental Frequency Estimation from Voiced Speech Based on Eigenvalue Decomposition of the Hankel Matrix},
year = {2014},
issue_date = {October 2014},
publisher = {IEEE Press},
volume = {22},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2335056},
doi = {10.1109/TASLP.2014.2335056},
abstract = {We propose a robust event-based method for estimation of the instantaneous fundamental frequency of a voiced speech signal. The amplitude and frequency modulated (AM-FM) signal model of voiced speech in the low frequency range (LFR) indicates the presence of energy only around its instantaneous fundamental frequency (F0) and its few harmonics. The time-varying F0 component of a voiced speech signal is extracted by a robust algorithm which iteratively performs eigenvalue decomposition (EVD) of the Hankel matrix, initially constructed from samples of the LFR filtered voiced speech signal. The negative cycles of the extracted time-varying F0 component provide a reliable coarse estimate of intervals where glottal closure instants (GCIs) may be present. The negative cycles of the LFR filtered voiced speech signal occurring within these intervals are isolated. There is a sudden decrease in the glottal impedance at GCIs resulting in high signal strength. Therefore, GCIs are detected as local minima in the derivative of the falling edges of the isolated negative cycles of the LFR filtered voiced speech signal, followed by a selection criterion to discard false GCI candidates. The instantaneous F0 is estimated as the inverse of the time interval between two consecutive GCIs. Experiments were performed on the Keele and CSTR speech databases in white and babble noise environments at various levels of degradation to assess the performance of the proposed method. The proposed method substantially reduces the gross F0 estimation errors in comparison to some state of the art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1467–1482},
numpages = {16},
keywords = {Hankel matrix, eigenvalue decomposition, instantaneous fundamental frequency, speech signal processing}
}

@article{10.1109/TASLP.2014.2333242,
author = {Wu, Zhizheng and Virtanen, Tuomas and Chng, Eng Siong and Li, Haizhou},
title = {Exemplar-Based Sparse Representation with Residual Compensation for Voice Conversion},
year = {2014},
issue_date = {October 2014},
publisher = {IEEE Press},
volume = {22},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2333242},
doi = {10.1109/TASLP.2014.2333242},
abstract = {We propose a nonparametric framework for voice conversion, that is, exemplar-based sparse representation with residual compensation. In this framework, a spectrogram is reconstructed as a weighted linear combination of speech segments, called exemplars, which span multiple consecutive frames. The linear combination weights are constrained to be sparse to avoid over-smoothing, and high-resolution spectra are employed in the exemplars directly without dimensionality reduction to maintain spectral details. In addition, a spectral compression factor and a residual compensation technique are included in the framework to enhance the conversion performances. We conducted experiments on the VOICES database to compare the proposed method with a large set of state-of-the-art baseline methods, including the maximum likelihood Gaussian mixture model (ML-GMM) with dynamic feature constraint and the partial least squares (PLS) regression based methods. The experimental results show that the objective spectral distortion of ML-GMM is reduced from 5.19 dB to 4.92 dB, and both the subjective mean opinion score and the speaker identification rate are increased from 2.49 and 73.50% to 3.15 and 79.50%, respectively, by the proposed method. The results also show the superiority of our method over PLS-based methods. In addition, the subjective listening tests indicate that the naturalness of the converted speech by our proposed method is comparable with that by the ML-GMM method with global variance constraint.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1506–1521},
numpages = {16},
keywords = {sparse representation, voice conversion, residual compensation, nonnegative matrix factorization, exemplar}
}

@article{10.1109/TASLP.2014.2337842,
author = {Vaizman, Yonatan and McFee, Brian and Lanckriet, Gert},
title = {Codebook-Based Audio Feature Representation for Music Information Retrieval},
year = {2014},
issue_date = {October 2014},
publisher = {IEEE Press},
volume = {22},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2337842},
doi = {10.1109/TASLP.2014.2337842},
abstract = {Digital music has become prolific in the web in recent decades. Automated recommendation systems are essential for users to discover music they love and for artists to reach appropriate audience. When manual annotations and user preference data is lacking (e.g. for new artists) these systems must rely on content based methods. Besides powerful machine learning tools for classification and retrieval, a key component for successful recommendation is the audio content representation. Good representations should capture informative musical patterns in the audio signal of songs. These representations should be concise, to enable efficient (low storage, easy indexing, fast search) management of huge music repositories, and should also be easy and fast to compute, to enable real-time interaction with a user supplying new songs to the system. Before designing new audio features, we explore the usage of traditional local features, while adding a stage of encoding with a pre-computed codebook and a stage of pooling to get compact vectorial representations. We experiment with different encoding methods, namely the LASSO, vector quantization (VQ) and cosine similarity (CS). We evaluate the representations' quality in two music information retrieval applications: query-by-tag and query-by-example. Our results show that concise representations can be used for successful performance in both applications. We recommend using top-τ VQ encoding, which consistently performs well in both applications, and requires much less computation time than the LASSO.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1483–1493},
numpages = {11},
keywords = {vector quantization, sparse coding, audio content representations, music recommendation, music information retrieval}
}

@article{10.1109/TASLP.2014.2328175,
author = {Xiang, Yong and Natgunanathan, Iynkaran and Guo, Song and Zhou, Wanlei and Nahavandi, Saeid},
title = {Patchwork-Based Audio Watermarking Method Robust to de-Synchronization Attacks},
year = {2014},
issue_date = {September 2014},
publisher = {IEEE Press},
volume = {22},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2328175},
doi = {10.1109/TASLP.2014.2328175},
abstract = {This paper presents a patchwork-based audio watermarking method to resist de-synchronization attacks such as pitch-scaling, time-scaling, and jitter attacks. At the embedding stage, the watermarks are embedded into the host audio signal in the discrete cosine transform (DCT) domain. Then, a set of synchronization bits are implanted into the watermarked signal in the logarithmic DCT (LDCT) domain. At the decoding stage, we analyze the received audio signal in the LDCT domain to find the scaling factor imposed by an attack. Then, we modify the received signal to remove the scaling effect, together with the embedded synchronization bits. After that, watermarks are extracted from the modified signal. Simulation results show that at the embedding rate of 10 bps, the proposed method achieves 98.9% detection rate on average under the considered de-synchronization attacks. At the embedding rate of 16 bps, it can still obtain 94.7% detection rate on average. So, the proposed method is much more robust to de-synchronization attacks than other patchwork watermarking methods. Compared with the audio watermarking methods designed for tackling de-synchronization attacks, our method has much higher embedding capacity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1413–1423},
numpages = {11},
keywords = {audio watermarking, discrete cosine transform, patchwork, de-synchronization attack, synchronization bits}
}

@article{10.1109/TASLP.2014.2335055,
author = {McLoughlin, Ian Vince},
title = {Super-Audible Voice Activity Detection},
year = {2014},
issue_date = {September 2014},
publisher = {IEEE Press},
volume = {22},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2335055},
doi = {10.1109/TASLP.2014.2335055},
abstract = {In this paper, reflected sound of frequency just above the audible range is used to detect speech activity. The active signal used is inaudible to humans, readily generated by the typical audio circuitry and components found in mobile telephones, and is robust to background sounds such as nearby voices. In use, the system relies upon a wideband excitation signal emitted from a loudspeaker located near the lips, which reflects from the mouth region and is then captured by a nearby microphone. The state of the lip opening is evaluated periodically by tracking the resonance patterns in the reflected excitation signal. When the lips are open, deep and complex resonances are formed as energy propagates into and then reflects out from the open mouth and vocal tract, with resonance depth being related to the open lip area. When the lips are closed, these resonance patterns are absent. The presence of the resonances can thus serve as a low complexity detection measure. The technique is evaluated for multiple users in terms of sensitivity to source placement and sensor placement. Voice activity detection performance using this measure is further evaluated in the presence of realistic wideband acoustic background noise, as well as artificially added noise. The system is shown to be relatively insensitive to sensor placement, highly insensitive to background noise, and able to achieve greater than 90% voice activity detection accuracy. The technique is even suitable when a subject is whispering in the presence of much louder multi-speaker babble. The technique has potential for speech-based systems operating in high noise environments as well as in silent speech interfaces, whisper-input systems and voice prostheses for speech-impaired users.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1424–1433},
numpages = {10},
keywords = {voice activity detection, silent speech interfaces, mouth state detection, speech activity detection, lip state detection, voice operated switch}
}

@article{10.1109/TASLP.2014.2332043,
author = {Ghalehjegh, Sina Hamidi and Rose, Richard C.},
title = {Linear Regression Based Acoustic Adaptation for the Subspace Gaussian Mixture Model},
year = {2014},
issue_date = {September 2014},
publisher = {IEEE Press},
volume = {22},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2332043},
doi = {10.1109/TASLP.2014.2332043},
abstract = {This paper presents a study of two acoustic speaker adaptation techniques applied in the context of the subspace Gaussian mixture model (SGMM) for automatic speech recognition (ASR). First, a model space linear regression based approach is presented for adaptation of SGMM state projection vectors and is referred to as subspace vector adaptation (SVA). Second, an easy to implement realization of constrained maximum likelihood linear regression (CMLLR) is presented for feature space adaptation in the SGMM. Numerically stable procedures for row-by-row estimation of the regression based transformation matrices are presented for both SVA and CMLLR adaptation. These approaches are applied to SGMM models that are estimated using speaker adaptive training (SAT), a technique for estimating more compact speaker independent acoustic models. Unsupervised speaker adaptation performance is evaluated on conversational and read speech task domains and compared to unsupervised adaptation performance obtained using the hidden Markov model-Gaussian mixture model (HMM-GMM) in ASR. It is shown that the feature space and model space adaptation approaches applied to the SGMM provide complementary reductions in word error rate (WER) and provide lower WERs than that obtained using CMLLR adaptation for the HMM-GMM.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1391–1402},
numpages = {12},
keywords = {speaker adaptation, subspace modeling, constrained maximum likelihood linear regression, automatic speech recognition}
}

@article{10.1109/TASLP.2014.2329734,
author = {Novotn\'{y}, Michal and Rusz, Jan and \v{C}mejla, Roman and R\r{u}\v{z}i\v{c}ka, Ev\v{z}en},
title = {Automatic Evaluation of Articulatory Disorders in Parkinson's Disease},
year = {2014},
issue_date = {September 2014},
publisher = {IEEE Press},
volume = {22},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2329734},
doi = {10.1109/TASLP.2014.2329734},
abstract = {Although articulatory deficits represent an important manifestation of dysarthria in Parkinson's disease (PD), the most widely used methods currently available for the automatic evaluation of speech performance are focused on the assessment of dysphonia. The aim of the present study was to design a reliable automatic approach for the precise estimation of articulatory deficits in PD. Twenty-four individuals diagnosed with de novo PD and twenty-two age-matched healthy controls were recruited. Each participant performed diadochokinetic tasks based upon the fast repetition of /pa/-/ta/-/ka/ syllables. All phonemes were manually labeled and an algorithm for their automatic detection was designed. Subsequently, 13 features describing six different articulatory aspects of speech including vowel quality, coordination of laryngeal and supralaryngeal activity, precision of consonant articulation, tongue movement, occlusion weakening, and speech timing were analyzed. In addition, a classification experiment using a support vector machine based on articulatory features was proposed to differentiate between PD patients and healthy controls. The proposed detection algorithm reached approximately 80% accuracy for a 5 ms threshold of absolute difference between manually labeled references and automatically detected positions. When compared to controls, PD patients showed impaired articulatory performance in all investigated speech dimensions (p &lt; 0.05). Moreover, using the six features representing different aspects of articulation, the best overall classification result attained a success rate of 88% in separating PD from controls. Imprecise consonant articulation was found to be the most powerful indicator of PD-related dysarthria. We envisage our approach as the first step towards development of acoustic methods allowing the automated assessment of articulatory features in dysarthrias.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1366–1378},
numpages = {13},
keywords = {speech disorders, diadochokinetic task, hypokinetic dysarthria, acoustic analysis, automatic segmentation, Parkinson's disease}
}

@article{10.1109/TASLP.2014.2332045,
author = {Botts, Jonathan and Savioja, Lauri},
title = {Spectral and Pseudospectral Properties of Finite Difference Models Used in Audio and Room Acoustics},
year = {2014},
issue_date = {September 2014},
publisher = {IEEE Press},
volume = {22},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2332045},
doi = {10.1109/TASLP.2014.2332045},
abstract = {Finite difference solutions to the wave equation are simple and flexible modeling tools for approximating physical systems in audio and room acoustics. Each model is characterized by a matrix operator and the time-stepping solution by a sequence of powers of the matrix. Spectral decomposition of representative matrices provide some practical insight into solution behavior and in some cases stability. In addition to computed eigenvalue spectra, pseudospectra provide a description of numerical amplification due to rounding errors in floating point arithmetic. The matrix analysis also shows that certain boundary implementations in non-cuboid geometries can be unstable despite satisfying conditions derived from von Neumann and normal mode analyses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1403–1412},
numpages = {10},
keywords = {eigenvalues and eigenfunctions, room acoustics, pseudospectra, operator spectra, finite difference}
}

@article{10.1109/TASLP.2014.2329184,
author = {Masiero, Bruno and Vorl\"{a}nder, Michael},
title = {A Framework for the Calculation of Dynamic Crosstalk Cancellation Filters},
year = {2014},
issue_date = {September 2014},
publisher = {IEEE Press},
volume = {22},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2329184},
doi = {10.1109/TASLP.2014.2329184},
abstract = {Dynamic crosstalk cancellation (CTC) systems commonly find use in immersive virtual reality (VR) applications. Such dynamic setups require extremely high filter update rates, so filter calculation is usually performed in the frequency-domain for higher efficiency. This paper proposes a general framework for the calculation of dynamic CTC filters to be used in immersive VR applications. Within this framework, we introduce a causality constraint to the frequency-domain calculation to avoid undesirable wrap-around effects and echo artifacts. Furthermore, when regularization is applied to the CTC filter calculation, in order to limit the output levels at the loudspeakers, noncausal artifacts appear at the CTC filters and the resulting ear signals. We propose a global minimum-phase regularization to convert these anti-causal ringing artifacts into causal artifacts. Finally, an aspect that is especially critical for dynamic CTC systems is the filter switch between active loudspeakers distributed in a surround audio-visual display system with 360° of freedom of operator orientation. Within this framework we apply a weighted filter calculation to control the filter switch, which allows the loudspeakers' contribution to be windowed in space, resulting in a smooth filter transition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1345–1354},
numpages = {10},
keywords = {minimum-phase regularization, binaural technique, dynamic crosstalk cancellation, causal implementation}
}

@article{10.1109/TASLP.2014.2329632,
author = {Lim, Felicia and Zhang, Wancheng and Habets, Emanu\"{e}l A. P. and Naylor, Patrick A.},
title = {Robust Multichannel Dereverberation Using Relaxed Multichannel Least Squares},
year = {2014},
issue_date = {September 2014},
publisher = {IEEE Press},
volume = {22},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2329632},
doi = {10.1109/TASLP.2014.2329632},
abstract = {A novel approach is proposed for robust multichannel dereverberation in the presence of system identification error (SIEs), based on channel shortening. A mathematical link is derived between the well known multiple-input/output inverse theorem (MINT) algorithm and channel shortening. The relaxed multichannel least squares (RMCLS) algorithm is then proposed as an efficient realization within the channel shortening paradigm and is shown through experimental results to outperform MINT in the presence of SIEs. While the RMCLS is robust to SIEs, the coloration of the output cannot be controlled. Two extensions to RMCLS are proposed to control the level of coloration and the performances of both extensions are evaluated comparatively. It is shown that both substantially maintain the dereverberation performance and robustness to SIEs obtained from RMCLS while effectively controlling the level of coloration introduced.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1379–1390},
numpages = {12},
keywords = {dereverberation, multichannel equalization, system identification errors, acoustic signal processing}
}

@article{10.1109/TASLP.2014.2329633,
author = {Schasse, Alexander and Martin, Rainer},
title = {Estimation of Subband Speech Correlations for Noise Reduction via MVDR Processing},
year = {2014},
issue_date = {September 2014},
publisher = {IEEE Press},
volume = {22},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2329633},
doi = {10.1109/TASLP.2014.2329633},
abstract = {Recently, it has been proposed to use the minimum-variance distortionless-response (MVDR) approach in single-channel speech enhancement in the short-time frequency domain. By applying optimal FIR filters to each subband signal, these filters reduce additive noise components with less speech distortion compared to conventional approaches. An important ingredient to these filters is the temporal correlation of the speech signals. We derive algorithms to provide a blind estimation of this quantity based on a maximum-likelihood and maximum a-posteriori estimation. To derive proper models for the inter-frame correlation of the speech and noise signals, we investigate their statistics on a large dataset. If the speech correlation is properly estimated, the previously derived subband filters discussed in this work show significantly less speech distortion compared to conventional noise reduction algorithms. Therefore, the focus of the experimental parts of this work lies on the quality and intelligibility of the processed signals. To evaluate the performance of the subband filters in combination with the clean speech inter-frame correlation estimators, we predict the speech quality and intelligibility by objective measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1355–1365},
numpages = {11},
keywords = {speech quality, Wiener filter, speech intelligibility, speech enhancement, subband filtering, noise reduction}
}

@article{10.1109/TASLP.2014.2320637,
author = {Alinaghi, Atiyeh and Jackson, Philip J. B. and Liu, Qingju and Wang, Wenwu},
title = {Joint Mixing Vector and Binaural Model Based Stereo Source Separation},
year = {2014},
issue_date = {September 2014},
publisher = {IEEE Press},
volume = {22},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2320637},
doi = {10.1109/TASLP.2014.2320637},
abstract = {In this paper the mixing vector (MV) in the statistical mixing model is compared to the binaural cues represented by interaural level and phase differences (ILD and IPD). It is shown that the MV distributions are quite distinct while binaural models overlap when the sources are close to each other. On the other hand, the binaural cues are more robust to high reverberation than MV models. According to this complementary behavior we introduce a new robust algorithm for stereo speech separation which considers both additive and convolutive noise signals to model the MV and binaural cues in parallel and estimate probabilistic time-frequency masks. The contribution of each cue to the final decision is also adjusted by weighting the log-likelihoods of the cues empirically. Furthermore, the permutation problem of the frequency domain blind source separation (BSS) is addressed by initializing the MVs based on binaural cues. Experiments are performed systematically on determined and underdetermined speech mixtures in five rooms with various acoustic properties including anechoic, highly reverberant, and spatially-diffuse noise conditions. The results in terms of signal-to-distortion-ratio (SDR) confirm the benefits of integrating the MV and binaural cues, as compared with two state-of-the-art baseline algorithms which only use MV or the binaural cues.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {1434–1448},
numpages = {15},
keywords = {blind source separation, computational auditory scene analysis, reverberation, time-frequency masking}
}

@article{10.1109/TASLP.2014.2327299,
author = {Li, Zhibao and Yiu, Ka Fai Cedric and Nordholm, Sven},
title = {On the Indoor Beamformer Design with Reverberation},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2327299},
doi = {10.1109/TASLP.2014.2327299},
abstract = {Beamforming remains to be an important technique for signal enhancement. For applications in open space, the transfer function describing waves propagation has an explicit expression, which can be employed for beamformer design. However, the function becomes very complex in an indoor environment due to the effects of reverberation. In this paper, this problem is discussed. A method based on the image source method (ISM) is applied to model the room impulse responses (RIRs), which will act as the transfer function between source and sensor. The indoor beamformer design problem is formulated as a minimax optimization problem. We propose and study several optimization models based on the L1-norm to design the beamformer. We found that it is advantageous to separate early and late reverberations in the design process and better designs can be achieved. Several numerical experiments are presented using both simulated data and real recordings to evaluate the proposed methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1225–1235},
numpages = {11},
keywords = {beamformer design, reverberation, signal enhancement, dereverberation}
}

@article{10.1109/TASLP.2014.2329732,
author = {Schmid, Dominic and Enzner, Gerald and Malik, Sarmad and Kolossa, Dorothea and Martin, Rainer},
title = {Variational Bayesian Inference for Multichannel Dereverberation and Noise Reduction},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2329732},
doi = {10.1109/TASLP.2014.2329732},
abstract = {Room reverberation and background noise severely degrade the quality of hands-free speech communication systems. In this work, we address the problem of combined speech dereverberation and noise reduction using a variational Bayesian (VB) inference approach. Our method relies on a multichannel state-space model for the acoustic channels that combines frame-based observation equations in the frequency domain with a first-order Markov model to describe the time-varying nature of the room impulse responses. By modeling the channels and the source signal as latent random variables, we formulate a lower bound on the log-likelihood function of the model parameters given the observed microphone signals and iteratively maximize it using an online expectation-maximization approach. Our derivation yields update equations to jointly estimate the channel and source posterior distributions and the remaining model parameters. An inspection of the resulting VB algorithmfor blind equalization and channel identification (VB-BENCH) reveals that the presented framework includes previously proposed methods as special cases. Finally, we evaluate the performance of our approach in terms of speech quality, adaptation times, and speech recognition results to demonstrate its effectiveness for a wide range of reverberation and noise conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1320–1335},
numpages = {16},
keywords = {expectation-maximization algorithm, dereverberation, noise reduction, variational bayes, state-space model}
}

@article{10.1109/TASLP.2014.2328174,
author = {Sch\"{u}ldt, Christian and H\"{a}ndel, Peter},
title = {Decay Rate Estimators and Their Performance for Blind Reverberation Time Estimation},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2328174},
doi = {10.1109/TASLP.2014.2328174},
abstract = {Several approaches for blind estimation of reverberation time have been presented in the literature and decay rate estimation is an integral part of many, if not all, of such approaches. This paper provides both an analytical and experimental comparison, in terms of the bias and variance of three common decay rate estimators; a straight-forward linear regression approach as well as two maximum-likelihood based methods. Situations with and without interfering additive noise are considered. It is shown that the linear regression based approach is unbiased if no smoothing is applied, and that the estimation variance in the absence of noise is constantly about twice that of the maximum-likelihood based methods. It is shown that the methods that do not take possible noise into account suffer from similar estimation bias in the presence of noise. Further, a hybrid method, combining the noise robustness and low computational complexity advantages of the two different maximum-likelihood based methods, is presented.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1274–1284},
numpages = {11},
keywords = {reverberation time, blind estimation, decay rate, maximum-likelihood estimation}
}

@article{10.1109/TASLP.2014.2329188,
author = {Y\'{\i}lmaz, Emre and Gemmeke, Jort Florent and Van Hamme, Hugo},
title = {Noise Robust Exemplar Matching Using Sparse Representations of Speech},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2329188},
doi = {10.1109/TASLP.2014.2329188},
abstract = {Performing automatic speech recognition using exemplars (templates) holds the promise to provide a better duration and coarticulation modeling compared to conventional approaches such as hidden Markov models (HMMs). Exemplars are spectrographic representations of speech segments extracted from the training data, each associated with a speech unit, e.g. phones, syllables, half-words or words, and preserve the complete spectro-temporal content of the speech. Conventional exemplar-matching approaches to automatic speech recognition systems, such as those based on dynamic time warping, have typically focused on evaluation in clean conditions. In this paper, we propose a novel noise robust exemplar matching framework for automatic speech recognition. This recognizer approximates noisy speech segments as a weighted sum of speech and noise exemplars and performs recognition by comparing the reconstruction errors of different classes with respect to a divergence measure. We evaluate the system performance in keyword recognition on the small vocabulary track of the 2nd CHiME Challenge and connected digit recognition on the AURORA-2 database. The results show that the proposed system achieves comparable results with state-of-the-art noise robust recognition systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1306–1319},
numpages = {14},
keywords = {sparse representations, reconstruction error, automatic speech recognition, noise robustness, exemplar-based}
}

@article{10.1109/TASLP.2014.2327300,
author = {FanChiang, Yi and Wei, Cheng-Wen and Meng, Yi-Le and Lin, Yu-Wen and Jou, Shyh-Jye and Chang, Tian-Sheuan},
title = {Low Complexity Formant Estimation Adaptive Feedback Cancellation for Hearing Aids Using Pitch Based Processing},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2327300},
doi = {10.1109/TASLP.2014.2327300},
abstract = {This paper proposes a novel algorithm and architecture for the adaptive feedback cancellation (AFC) based on the pitch and the formant information for hearing aid (HA) applications. The proposed method, named as Pitch based Formant Estimation (PFE-AFC), has significantly low complexity compared to Prediction Error Method AFC (PEM-AFC). The proposed PFE-AFC consists of a forward and a backward path processing. The forward path processing includes a low complexity pitch based formant estimator for decorrelation filter coefficients update and a pitch based voice activity detector for speech detection, which facilitates the feedback cancellation filter in the backward path to reduce feedback component and maintain speech quality. From system point of view, the PFE-AFC has low complexity overhead since it is easy to share computation resource with other components in the HA system, such as noise reduction and auditory compensation. In addition, the PFE-AFC is suitable for hardware implementation owing to its regular structure. Complexity evaluations show that the PFE-AFC has four orders lower complexity than the PEM-AFC. Simulation results show that the PFE-AFC and the PEM-AFC can achieve similar PESQ (perceptual evaluation speech quality) and ASG (added stable gain). Moreover, the proposed PFE-AFC can outperform the conventional AFC.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1248–1259},
numpages = {12},
keywords = {hearing aid application, adaptive feedback cancellation, voice activity detection}
}

@article{10.1109/TASLP.2014.2327297,
author = {Conan, Simon and Derrien, Olivier and Aramaki, Mitsuko and Ystad, S\o{}lvi and Kronland-Martinet, Richard},
title = {A Synthesis Model with Intuitive Control Capabilities for Rolling Sounds},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2327297},
doi = {10.1109/TASLP.2014.2327297},
abstract = {This paper presents a physically inspired source-filter model for rolling sound synthesis. The model, which is suitable for real-time implementation, is based on qualitative and quantitative observations obtained from a physics-based model described in the literature. In the first part of the paper, the physics-based model is presented, followed by a perceptual experiment, whose aim is to identify the perceptually relevant information characterizing the rolling interaction. On the basis of this experiment, we hypothesize that the particular pattern of the interaction force is responsible for the perception of a rolling object. A complete analysis-synthesis scheme of this interaction force is then provided, along with a description of the calibration of the proposed source-filter sound synthesis process. Finally, a mapping strategy for intuitive control of the proposed synthesis process (i.e. size and velocity of the rolling object and roughness of the surface) is proposed and validated by a listening test.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1260–1273},
numpages = {14},
keywords = {sound synthesis, audio virtual reality, physics-based model, rolling sounds, sound analysis, source-filter model}
}

@article{10.1109/TASLP.2014.2329190,
author = {Ganapathy, Sriram and Mallidi, Sri Harish and Hermansky, Hynek},
title = {Robust Feature Extraction Using Modulation Filtering of Autoregressive Models},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2329190},
doi = {10.1109/TASLP.2014.2329190},
abstract = {Speaker and language recognition in noisy and degraded channel conditions continue to be a challenging problem mainly due to the mismatch between clean training and noisy test conditions. In the presence of noise, the most reliable portions of the signal are the high energy regions which can be used for robust feature extraction. In this paper, we propose a front end processing scheme based on autoregressive (AR) models that represent the high energy regions with good accuracy followed by a modulation filtering process. The AR model of the spectrogram is derived using two separable time and frequency AR transforms. The first AR model (temporal AR model) of the sub-band Hilbert envelopes is derived using frequency domain linear prediction (FDLP). This is followed by a spectral AR model applied on the FDLP envelopes. The output 2-D AR model represents a low-pass modulation filtered spectrogram of the speech signal. The band-pass modulation filtered spectrograms can further be derived by dividing two AR models with different model orders (cut-off frequencies). The modulation filtered spectrograms are converted to cepstral coefficients and are used for a speaker recognition task in noisy and reverberant conditions. Various speaker recognition experiments are performed with clean and noisy versions of the NIST-2010 speaker recognition evaluation (SRE) database using the state-of-the-art speaker recognition system. In these experiments, the proposed front-end analysis provides substantial improvements (relative improvements of up to 25%) compared to baseline techniques. Furthermore, we also illustrate the generalizability of the proposed methods using language identification (LID) experiments on highly degraded high-frequency (HF) radio channels and speech recognition experiments on noisy data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1285–1295},
numpages = {11},
keywords = {feature extraction, modulation filtering, speaker and language recognition, autoregressive modeling}
}

@article{10.1109/TASLP.2014.2329237,
author = {Li, Bo and Sim, Khe Chai},
title = {A Spectral Masking Approach to Noise-Robust Speech Recognition Using Deep Neural Networks},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2329237},
doi = {10.1109/TASLP.2014.2329237},
abstract = {Improving the noise robustness of automatic speech recognition systems has been a challenging task for many years. Recently, it was found that Deep Neural Networks (DNNs) yield large performance gains over conventional GMM-HMM systems, when used in both hybrid and tandem systems. However, they are still far from the level of human expectations especially under adverse environments. Motivated by the separation-prior-to-recognition process of the human auditory system, we propose a robust spectral masking system where power spectral domain masks are predicted using a DNN trained on the same filter-bank features used for acoustic modeling. To further improve performance, Linear Input Network (LIN) adaptation is applied to both the mask estimator and the acoustic model DNNs. Since the estimation of LINs for the mask estimator requires stereo data, which is not available during testing, we proposed using the LINs estimated for the acoustic model DNNs to adapt the mask estimators. Furthermore, we used the same set of weights obtained from pretraining for the input layers of both the mask estimator and the acoustic model DNNs to ensure a better consistency for sharing LINs. Experimental results on benchmark Aurora2 and Aurora4 tasks demonstrated the effectiveness of our system, which yielded Word Error Rates (WERs) of 4.6% and 11.8% respectively. Furthermore, the simple averaging of posteriors from systems with and without spectral masking can further reduce the WERs to 4.3% on Aurora2 and 11.4% on Aurora4.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1296–1305},
numpages = {10},
keywords = {deep neural network, spectral masking, noise robustness}
}

@article{10.1109/TASLP.2014.2327298,
author = {Hawes, Matthew B. and Liu, Wei},
title = {Sparse Array Design for Wideband Beamforming with Reduced Complexity in Tapped Delay-Lines},
year = {2014},
issue_date = {August 2014},
publisher = {IEEE Press},
volume = {22},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2327298},
doi = {10.1109/TASLP.2014.2327298},
abstract = {Sparse wideband array design for sensor location optimization is highly nonlinear and it is traditionally solved by genetic algorithms (GAs) or other similar optimization methods. This is an extremely time-consuming process and an optimum solution is not always guaranteed. In this work, this problem is studied from the viewpoint of compressive sensing (CS). Although there have been CS-based methods proposed for the design of sparse narrowband arrays, its extension to the wideband case is not straightforward, as there are multiple coefficients associated with each sensor and they have to be simultaneously minimized in order to discard the corresponding sensor locations. At first, sensor location optimization for both general wideband beamforming and frequency invariant beamforming is considered. Then, sparsity in the tapped delay-line (TDL) coefficients associated with each sensor is considered in order to reduce the implementation complexity of each TDL. Finally, design of robust wideband arrays against norm-bounded steering vector errors is addressed. Design examples are provided to verify the effectiveness of the proposed methods, with comparisons drawn with a GA-based design method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1236–1247},
numpages = {12},
keywords = {compressive sensing, frequency invariant beamforming, robust beamforming, implementation complexity, wideband beamforming, sparse array}
}

@article{10.1109/TASLP.2014.2320575,
author = {Bao, Guangzhao and Xu, Yangfei and Ye, Zhongfu},
title = {Learning a Discriminative Dictionary for Single-Channel Speech Separation},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Press},
volume = {22},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2320575},
doi = {10.1109/TASLP.2014.2320575},
abstract = {This paper presents a novel dictionary learning (DL) method to improve the performance of sparsity based single-channel speech separation (SCSS). The conventional approaches regard the sub-dictionaries as independent units and learn sub-dictionaries separately in the short-time Fourier transform (STFT) domain using their corresponding training sets respectively. However, we take the relationship between the sub-dictionaries into account and optimize the sub-dictionaries jointly in the time domain. By satisfying a designed discrimination constraint, a structured dictionary, whose atoms have better correspondences to the speaker labels, is learned so that the sources can be recovered by the corresponding reconstruction after sparse coding. An algorithm, which consists of sparse coding stage and dictionary updating stage, is proposed to deal with this DL optimization problem. Two strategies, i.e., direct learning and adaptive learning, are presented to select the training sets which are used to learn the discriminative dictionary. Experimental results show that the proposed SCSS algorithms have superior performance compared with other tested approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1130–1138},
numpages = {9},
keywords = {dictionary learning, single-channel speech separation, sparsity, discriminative dictionary}
}

@article{10.1109/TASLP.2014.2319159,
author = {Bahari, Mohamad Hasan and Dehak, Najim and Van Hamme, Hugo and Burget, Lukas and Ali, Ahmed M. and Glass, Jim},
title = {Non-Negative Factor Analysis of Gaussian Mixture Model Weight Adaptation for Language and Dialect Recognition},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Press},
volume = {22},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2319159},
doi = {10.1109/TASLP.2014.2319159},
abstract = {Recent studies show that Gaussian mixture model (GMM) weights carry less, yet complimentary, information to GMM means for language and dialect recognition. However, state-of-the-art language recognition systems usually do not use this information. In this research, a non-negative factor analysis (NFA) approach is developed for GMM weight decomposition and adaptation. This modeling, which is conceptually simple and computationally inexpensive, suggests a new low-dimensional utterance representation method using a factor analysis similar to that of the i-vector framework. The obtained subspace vectors are then applied in conjunction with i-vectors to the language/dialect recognition problem. The suggested approach is evaluated on the NIST 2011 and RATS language recognition evaluation (LRE) corpora and on the QCRI Arabic dialect recognition evaluation (DRE) corpus. The assessment results show that the proposed adaptation method yields more accurate recognition results compared to three conventional weight adaptation approaches, namely maximum likelihood re-estimation, non-negative matrix factorization, and a subspace multinomial model. Experimental results also show that the intermediate-level fusion of i-vectors and NFA subspace vectors improves the performance of the state-of-the-art i-vector framework especially for the case of short utterances.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1117–1129},
numpages = {13},
keywords = {model adaptation, non-negative factor analysis, dialect recognition, language recognition, Gaussian mixture model weight}
}

@article{10.1109/TASLP.2014.2327295,
author = {Shen, Mo and Kawahara, Daisuke and Kurohashi, Sadao},
title = {Dependency Parse Reranking with Rich Subtree Features},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Press},
volume = {22},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2327295},
doi = {10.1109/TASLP.2014.2327295},
abstract = {In pursuing machine understanding of human language, highly accurate syntactic analysis is a crucial step. In this work, we focus on dependency grammar, which models syntax by encoding transparent predicate-argument structures. Recent advances in dependency parsing have shown that employing higher-order subtree structures in graph-based parsers can substantially improve the parsing accuracy. However, the inefficiency of this approach increases with the order of the subtrees. This work explores a new reranking approach for dependency parsing that can utilize complex subtree representations by applying efficient sub-tree selection methods. We demonstrate the effectiveness of the approach in experiments conducted on the Penn Treebank and the Chinese Treebank. Our system achieves the best performance among known supervised systems evaluated on these datasets, improving the baseline accuracy from 91.88% to 93.42% for English, and from 87.39% to 89.25% for Chinese.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1208–1218},
numpages = {11},
keywords = {dependency parsing, parse reranking, multilingual parsing}
}

@article{10.1109/TASLP.2014.2321482,
author = {Chen, Sin-Horng and Hsieh, Chiao-Hua and Chiang, Chen-Yu and Hsiao, Hsi-Chun and Wang, Yih-Ru and Liao, Yuan-Fu and Yu, Hsiu-Min},
title = {Modeling of Speaking Rate Influences on Mandarin Speech Prosody and Its Application to Speaking Rate-Controlled TTS},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Press},
volume = {22},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2321482},
doi = {10.1109/TASLP.2014.2321482},
abstract = {A new data-driven approach to building a speaking rate-dependent hierarchical prosodic model (SR-HPM), directly from a large prosody-unlabeled speech database containing utterances of various speaking rates, to describe the influences of speaking rate on Mandarin speech prosody is proposed. It is an extended version of the existing HPM model which contains 12 sub-models to describe various relationships of prosodic-acoustic features of speech signal, linguistic features of the associated text, and prosodic tags representing the prosodic structure of speech. Two main modifications are suggested. One is designing proper normalization functions from the statistics of the whole database to compensate the influences of speaking rate on all prosodic-acoustic features. Another is modifying the HPM training to let its parameters be speaking-rate dependent. Experimental results on a large Mandarin read speech corpus showed that the parameters of the SR-HPM together with these feature normalization functions interpreted the effects of speaking rate onMandarin speech prosody very well. An application of the SR-HPM to design and implement a speaking rate-controlled Mandarin TTS system is demonstrated. The system can generate natural synthetic speech for any given speaking rate in awide range of 3.4-6.8 syllables/sec. Two subjective tests, MOS and preference test, were conducted to compare the proposed system with the popular HTS system. The MOS scores of the proposed system were in the range of 3.58-3.83 for eight different speaking rates, while they were in 3.09-3.43 for HTS. Besides, the proposed system had higher preference scores (49.8%-79.6%) than those (9.8%-30.7%) of HTS. This confirmed the effectiveness of the speaking rate control method of the proposed TTS system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1158–1171},
numpages = {14},
keywords = {speaking rate modeling, speaking rate-controlled TTS, Mandarin prosody modeling}
}

@article{10.1109/TASLP.2014.2321472,
author = {Kelly, Ian J. and Boland, Francis M.},
title = {Detecting Arrivals in Room Impulse Responses with Dynamic Time Warping},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Press},
volume = {22},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2321472},
doi = {10.1109/TASLP.2014.2321472},
abstract = {The detection of early reflections in room impulse responses is of importance to many algorithms including room geometry inference, mixing time determination and speech dereverberation. The detection of early reflections can be hampered by increasing reflection width, as the direct sound undergoes reflection and by overlapping of the reflections, as the reflection density grows. This paper investigates the use of Dynamic Time Warping upon a direct sound pulse to estimate the temporal distribution of arrivals in room impulse responses. Bounded Dynamic Time Warping is performed, after an initial correlation of the direct sound with the remaining signal, to further refine the arrival's location and duration and to find arrivals which may otherwise not correlate well with the un-warped direct sound due to a change in the reflection's shape. Dynamic Time Warping upon concatenated versions of the direct sound is also used to help find overlapping reflections which may otherwise go undetected. Warping is performed via a set of warp matrices which can be combined together and can also be inverted via a quickly calculated left pseudo-inverse. The algorithm presented is compared to two current methods of arrival detection and is shown to detect a lesser number of spurious arrivals than either of these algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1139–1147},
numpages = {9},
keywords = {early reflections, dynamic time warping, room impulse response}
}

@article{10.1109/TASLP.2014.2324182,
author = {Zhang, Wen and Abhayapala, Thushara D.},
title = {Three Dimensional Sound Field Reproduction Using Multiple Circular Loudspeaker Arrays: Functional Analysis Guided Approach},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Press},
volume = {22},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2324182},
doi = {10.1109/TASLP.2014.2324182},
abstract = {Three dimensional sound field reproduction based on higher order Ambisonics requires the placement of loudspeakers on a sphere that surrounds the target reproduction region. The deployment of a spherical array is not trivial especially for implementation in real rooms where the placement flexibility is highly desirable. This paper proposes a design of multiple circular loudspeaker arrays for reproducing three dimensional sound fields originating from a limited region of interest. We apply a functional analysis framework to formulate the sound field reproduction problem in a closed form. Secondary source distributions and target sound fields are modeled as two Hilbert spaces and mapped by an integral operator and its adjoint operator, from which a self-adjoint operator is constructed and the singular value decomposition is applied to represent source distributions and sound fields with two sets of interrelated singular functions. We derive the solutions for a circular secondary source arrangement and propose the design of placing multiple circular loudspeaker arrays only over the limited region of interest. Such a design allows for non-spherical and non-uniform loudspeaker placement and thus provides a flexible array arrangement. The reproduction accuracy of the proposed method is verified through numerical simulations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1184–1194},
numpages = {11},
keywords = {spherical harmonics, 3D audio, sound field reproduction, circular loudspeaker arrays, ambisonics}
}

@article{10.1109/TASLP.2014.2327294,
author = {Taseska, Maja and Habets, Emanu\"{e}l A. P.},
title = {Informed Spatial Filtering for Sound Extraction Using Distributed Microphone Arrays},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Press},
volume = {22},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2327294},
doi = {10.1109/TASLP.2014.2327294},
abstract = {Hands-free acquisition of speech is required in many human-machine interfaces and communication systems. The signals received by integrated microphones contain a desired speech signal, spatially coherent interfering signals, and background noise. In order to enhance the desired speech signal, state-of-the-art techniques apply data-dependent spatial filters which require the second order statistics (SOS) of the desired signal, the interfering signals and the background noise. As the number of sources and the reverberation time increase, the estimation accuracy of the SOS deteriorates, often resulting in insufficient noise and interference reduction. In this paper, a signal extraction framework with distributed microphone arrays is developed. An expectation maximization (EM)-based algorithm detects the number of coherent speech sources and estimates source clusters using time-frequency (TF) bin-wise position estimates. Subsequently, the second order statistics (SOS) are estimated using bin-wise speech presence probability (SPP) and a source probability for each source. Finally, a desired source is extracted using a minimum variance distortionless response (MVDR) filter, a multichannel Wiener filter (MWF) and a parametric multichannel Wiener filter (PMWF). The same framework can be employed for source separation, where a spatial filter is computed for each source considering the remaining sources as interferers. Evaluation using simulated and measured data demonstrates the effectiveness of the framework in estimating the number of sources, clustering, signal enhancement, and source separation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1195–1207},
numpages = {13},
keywords = {spatial filtering, EM algorithm, source extraction, distributed arrays, PSD matrix estimation}
}

@article{10.1109/TASLP.2014.2324175,
author = {Comminiello, Danilo and Scarpiniti, Michele and Azpicueta-Ruiz, Luis A. and Arenas-Garc\'{\i}a, Jer\'{o}nimo and Uncini, Aurelio},
title = {Nonlinear Acoustic Echo Cancellation Based on Sparse Functional Link Representations},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Press},
volume = {22},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2324175},
doi = {10.1109/TASLP.2014.2324175},
abstract = {Recently, a new class of nonlinear adaptive filtering architectures has been introduced based on the functional link adaptive filter (FLAF) model. Here we focus specifically on the split FLAF (SFLAF) architecture, which separates the adaptation of linear and nonlinear coefficients using two different adaptive filters in parallel. This property makes the SFLAF a well-suited method for problems like nonlinear acoustic echo cancellation (NAEC), in which the separation of filtering tasks brings some performance improvement. Although flexibility is one of the main features of the SFLAF, some problem may occur when the nonlinearity degree of the input signal is not known a priori. This implies a non-optimal choice of the number of coefficients to be adapted in the nonlinear path of the SFLAF. In order to tackle this problem, we propose a proportionate FLAF (PFLAF), which is based on sparse representations of functional links, thus giving less importance to those coefficients that do not actively contribute to the nonlinear modeling. Experimental results show that the proposed PFLAF achieves performance improvement with respect to the SFLAF in several nonlinear scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1172–1183},
numpages = {12},
keywords = {sparse adaptive filters, nonlinear modeling, proportionate adaptive filters, functional link adaptive filters, nonlinear acoustic echo cancellation}
}

@article{10.1109/TASLP.2014.2321475,
author = {Guldenschuh, Markus and De Callafon, Raymond},
title = {Detection of Secondary-Path Irregularities in Active Noise Control Headphones},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Press},
volume = {22},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2321475},
doi = {10.1109/TASLP.2014.2321475},
abstract = {Headphones with adaptive feedback ANC show a good performance if the secondary-path is well known. The secondary-path however changes considerably if the headphones are lifted. In this paper, it is shown that these changes mainly affect the low frequencies of the secondary-path and that the resulting low-frequency poles of the system cause instabilities. However, the changes in the secondary-path can be detected via low-frequency changes of the adaptive filter. A cost-efficient algorithm in the time-domain is developed to detect and react to these changes in order to keep the ANC system stable. Experimental results show that the algorithm yields the desired ANC performance in the regular use case and still avoids instabilities even during sudden changes in the secondary-path.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1148–1157},
numpages = {10},
keywords = {adaptive filters, noise cancellation, robust stability, active noise reduction, adaptive signal processing, feedback, adaptive control}
}

@article{10.1109/TASLP.2014.2315044,
author = {Nongpiur, R. C.},
title = {Design of Minimax Broadband Beamformers That Are Robust to Microphone Gain, Phase and Position Errors},
year = {2014},
issue_date = {June 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2315044},
doi = {10.1109/TASLP.2014.2315044},
abstract = {Broadband beamformers with small-size microphone arrays are known to be highly sensitive to microphone imperfections. A new method for the design of minimax broadband beamformers that are robust to microphone gain, phase, and position errors is proposed. In the method, the maximum variations in the microphone errors are used in formulating a convex optimization problem where the worst-case passband error is minimized under the constraint that the worst-case stopband error is below a prescribed level. To include the microphone imperfections in the optimization problem, we developed a suitable model that incorporates the variations due to the microphone errors and at the same time is efficient to compute. An important advantage of the proposed method is the availability of corresponding worst-case passband - and stopband-error bounds for the beamformer that has been designed; a second advantage is that it does not require the probability distributions of microphone errors. We then describe a two-phase method where the proposed method is used in the first phase to derive the passband and stopband error constraints for solving an optimization problem in the second phase where the white noise gain (WNG) of the beamformer is maximized. In our experiments, we compare beamformers designed using the proposed method, the two-phase method and a modified version of a competing method. Experimental results show that beamformers designed using the proposed method have much better performance than those of the modified competing method and comparable performance with those of the two-phase method; however, unlike the two-phase method, the proposed method provides the additional guarantee that the errors will always lie within the worst-case error bounds.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1013–1022},
numpages = {10},
keywords = {speech enhancement, acoustic beamforming, constrained optimization, broadband beamformer}
}

@article{10.1109/TASLP.2014.2319155,
author = {Reindl, Klaus and Meier, Stefan and Barfuss, Hendrik and Kellermann, Walter},
title = {Minimum Mutual Information-Based Linearly Constrained Broadband Signal Extraction},
year = {2014},
issue_date = {June 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2319155},
doi = {10.1109/TASLP.2014.2319155},
abstract = {In this contribution, the problem of broadband acoustic signal extraction is treated as a specific source separation problem, where the desired signal components are to be separated from all remaining undesired components. For this, we exploit the generic TRIple-N Independent component analysis for CONvolutive mixtures (TRINICON) framework. The TRINICON optimization criterion is complemented with linear constraints leading to the Linearly Constrained Minimum Mutual Information (LCMMI) criterion for desired signal extraction. A general linearly constrained update rule for iterative filter optimization is derived, which can efficiently be realized in a novel Minimum Mutual Information (MMI)-Generalized Sidelobe Canceler (GSC). The general treatment of the signal extraction problem using an MMI criterion provides several advantages: Firstly, new insights into the signal extraction problem can be derived by establishing links to both the original GSC and the Multichannel Wiener Filter (MWF). Secondly, by exploiting fundamental properties characteristic for speech and audio signals, complicated and often unreliable Voice Activity Detection (VAD)-based control mechanisms become unnecessary. Thirdly, the overall realization requires only prior information of the desired source position. An evaluation of the MMI-GSC for the double-talk situation with two concurrently active speech sources under reverberant and noisy conditions demonstrates the effectiveness of this novel approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1096–1108},
numpages = {13},
keywords = {wiener filter, linearly constrained minimum mutual information (LCMMI), generalized sidelobe canceler (GSC), interference cancellation, linearly constrained minimum variance (LCMV)}
}

@article{10.1109/TASLP.2014.2319157,
author = {Gangeh, Mehrdad J. and Fewzee, Pouria and Ghodsi, Ali and Kamel, Mohamed S. and Karray, Fakhri},
title = {Multiview Supervised Dictionary Learning in Speech Emotion Recognition},
year = {2014},
issue_date = {June 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2319157},
doi = {10.1109/TASLP.2014.2319157},
abstract = {Recently, a supervised dictionary learning (SDL) approach based on the Hilbert-Schmidt independence criterion (HSIC) has been proposed that learns the dictionary and the corresponding sparse coefficients in a space where the dependency between the data and the corresponding labels is maximized. In this paper, two multiview dictionary learning techniques are proposed based on this HSIC-based SDL. While one of these two techniques learns one dictionary and the corresponding coefficients in the space of fused features in all views, the other learns one dictionary in each view and subsequently fuses the sparse coefficients in the spaces of learned dictionaries. The effectiveness of the proposed multiview learning techniques in using the complementary information of single views is demonstrated in the application of speech emotion recognition (SER). The fully-continuous sub-challenge (FCSC) of the AVEC 2012 dataset is used in two different views: baseline and spectral energy distribution (SED) feature sets. Four dimensional affects, i.e., arousal, expectation, power, and valence are predicted using the proposed multiview methods as the continuous response variables. The results are compared with the single views, AVEC 2012 baseline system, and also other supervised and unsupervised multiview learning approaches in the literature. Using correlation coefficient as the performance measure in predicting the continuous dimensional affects, it is shown that the proposed approach achieves the highest performance among the rivals. The relative performance of the two proposed multiview techniques and their relationship are also discussed. Particularly, it is shown that by providing an additional constraint on the dictionary of one of these approaches, it becomes the same as the other.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1056–1068},
numpages = {13},
keywords = {sparse representation, emotion recognition, supervised learning, multiview representation, dictionary learning}
}

@article{10.1109/TASLP.2014.2318514,
author = {Geiger, J\"{u}rgen T. and Weninger, Felix and Gemmeke, Jort F. and W\"{o}llmer, Martin and Schuller, Bj\"{o}rn and Rigoll, Gerhard},
title = {Memory-Enhanced Neural Networks and NMF for Robust ASR},
year = {2014},
issue_date = {June 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2318514},
doi = {10.1109/TASLP.2014.2318514},
abstract = {In this article we address the problem of distant speech recognition for reverberant noisy environments. Speech enhancement methods, e. g., using non-negative matrix factorization (NMF), are succesful in improving the robustness of ASR systems. Furthermore, discriminative training and feature transformations are employed to increase the robustness of traditional systems using Gaussian mixture models (GMM). On the other hand, acoustic models based on deep neural networks (DNN) were recently shown to outperform GMMs. In this work, we combine a state-of-the art GMM system with a deep Long Short-Term Memory (LSTM) recurrent neural network in a double-stream architecture. Such networks use memory cells in the hidden units, enabling them to learn long-range temporal context, and thus increasing the robustness against noise and reverberation. The network is trained to predict frame-wise phoneme estimates, which are converted into observation likelihoods to be used as an acoustic model. It is of particular interest whether the LSTM system is capable of improving a robust state-of-the-art GMM system, which is confirmed in the experimental results. In addition, we investigate the efficiency of NMF for speech enhancement on the front-end side. Experiments are conducted on the medium-vocabulary task of the 2nd 'CHiME' Speech Separation and Recognition Challenge, which includes reverberation and highly variable noise. Experimental results show that the average word error rate of the challenge baseline is reduced by 64% relative. The best challenge entry, a noise-robust state-of-the-art recognition system, is outperformed by 25% relative.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1037–1046},
numpages = {10},
keywords = {long short-term memory, non-negative matrix factorization, noise robust speech recognition, multi-stream recognition}
}

@article{10.1109/TASLP.2014.2318519,
author = {Zhao, Haiquan and Yu, Yi and Gao, Shibin and Zeng, Xiangping and He, Zhengyou},
title = {Memory Proportionate APA with Individual Activation Factors for Acoustic Echo Cancellation},
year = {2014},
issue_date = {June 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2318519},
doi = {10.1109/TASLP.2014.2318519},
abstract = {An individual-activation-factor memory proportionate affine projection algorithm (IAF-MPAPA) is proposed for sparse system identification in acoustic echo cancellation (AEC) scenarios. By utilizing an individual activation factor for each adaptive filter coefficient instead of a global activation factor, as in the standard proportionate affine projection algorithm (PAPA), the adaptation energy over the coefficients of the proposed IAF-MPAPA can achieve a better distribution, which leads to an improvement of the convergence performance. Moreover, benefiting from the memory characteristics of the proportionate coefficients, its computational complexity is less than the PAPA and improved PAPA (IPAPA). In the context of AEC and stereophonic AEC (SAEC) for highly sparse impulse responses, simulation results indicate that the proposed IAF-MPAPA outperforms the PAPA, IPAPA, and memory IPAPA (MIPAPA) in terms of the convergence rate and tracking capability when the unknown impulse response suddenly changes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1047–1055},
numpages = {9},
keywords = {sparse system identification, individual activation factor, adaptive filtering, sparse impulse response, proportionate affine projection algorithm}
}

@article{10.1109/TASLP.2014.2316376,
author = {Venkitaraman, Arun and Seelamantula, Chandra Sekhar},
title = {Binaural Signal Processing Motivated Generalized Analytic Signal Construction and AM-FM Demodulation},
year = {2014},
issue_date = {June 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2316376},
doi = {10.1109/TASLP.2014.2316376},
abstract = {Binaural hearing studies show that the auditory system uses the phase-difference information in the auditory stimuli for localization of a sound source. Motivated by this finding, we present a method for demodulation of amplitude-modulated-frequency-modulated (AM-FM) signals using a signal and its arbitrary phase-shifted version. The demodulation is achieved using two allpass filters, whose impulse responses are related through the fractional Hilbert transform (FrHT). The allpass filters are obtained by cosine-modulation of a zero-phase flat-top prototype halfband lowpass filter. The outputs of the filters are combined to construct an analytic signal (AS) from which the AM and FM are estimated. We show that, under certain assumptions on the signal and the filter structures, the AM and FM can be obtained exactly. The AM-FM calculations are based on the quasi-eigenfunction approximation. We then extend the concept to the demodulation of multicomponent signals using uniform and non-uniform cosine-modulated filterbank (FB) structures consisting of flat bandpass filters, including the uniform cosine-modulated, equivalent rectangular bandwidth (ERB), and constant-Q filterbanks. We validate the theoretical calculations by considering application on synthesized AM-FM signals and compare the performance in presence of noise with three other multiband demodulation techniques, namely, the Teager-energy-based approach, the Gabor's AS approach, and the linear transduction filter approach. We also show demodulation results for real signals.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1023–1036},
numpages = {14},
keywords = {cosine-modulated filterbank, fractional Hilbert transform, demodulation, binaural processing, quasi-eigenfunction approximation, equivalent rectangular bandwidth}
}

@article{10.1109/TASLP.2014.2313917,
author = {Choi, Jae-Hun and Chang, Joon-Hyuk},
title = {Dual-Microphone Voice Activity Detection Technique Based on Two-Step Power Level Difference Ratio},
year = {2014},
issue_date = {June 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2313917},
doi = {10.1109/TASLP.2014.2313917},
abstract = {In this paper, we propose a novel dual-microphone voice activity detection (VAD) technique based on the two-step power level difference (PLD) ratio. This technique basically exploits the PLD between the primary microphone and the secondary microphone in a mobile device when the distance between the microphones and the sound source is relatively short. Based on the PLD, we propose the use of the PLD ratio (PLDR) instead of the original PLD to take advantage of the relative difference between the PLD of speech and the PLD of noise. Indeed, the PLDR is obtained by estimating the ratio of the PLD between the input signals and the PLD between the two channel noises during periods without speech. The proposed technique offers a two-step algorithm using the PLDRs including long-term PLDR (LT-PLDR), which characterizes long-term evolution and short-term PLDR (ST-PLDR), which characterizes short-time variation during the first step. LT-PLDR-based and ST-PLDR-based VAD decision are performed using the maximum a posteriori (MAP) probability derived from the model-trust algorithm and combined at the second step to reach a superior VAD decision for both long-term and short-term situations. Extensive experimental results show that the proposed dual-microphone VAD technique outperforms the conventional two-channel VAD method as well as most standardized VAD algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1069–1081},
numpages = {13},
keywords = {dual-microphone, two-step, power level difference ratio, voice activity detection}
}

@article{10.1109/TASLP.2014.2317989,
author = {Alameda-Pineda, Xavier and Horaud, Radu},
title = {A Geometric Approach to Sound Source Localization from Time-Delay Estimates},
year = {2014},
issue_date = {June 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2317989},
doi = {10.1109/TASLP.2014.2317989},
abstract = {This paper addresses the problem of sound-source localization from time-delay estimates using arbitrarily-shaped non-coplanar microphone arrays. A novel geometric formulation is proposed, together with a thorough algebraic analysis and a global optimization solver. The proposed model is thoroughly described and evaluated. The geometric analysis, stemming from the direct acoustic propagation model, leads to necessary and sufficient conditions for a set of time delays to correspond to a unique position in the source space. Such sets of time delays are referred to as feasible sets. We formally prove that every feasible set corresponds to exactly one position in the source space, whose value can be recovered using a closed-form localization mapping. Therefore we seek for the optimal feasible set of time delays given, as input, the received microphone signals. This time delay estimation problem is naturally cast into a programming task, constrained by the feasibility conditions derived from the geometric analysis. A global branch-and-bound optimization technique is proposed to solve the problem at hand, hence estimating the best set of feasible time delays and, subsequently, localizing the sound source. Extensive experiments with both simulated and real data are reported; we compare our methodology to four state-of-the-art techniques. This comparison shows that the proposed method combined with the branch-and-bound algorithm outperforms existing methods. These in-depth geometric understanding, practical algorithms, and encouraging results, open several opportunities for future work.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1082–1095},
numpages = {14},
keywords = {constrained multivariate nonlinear programming, time delay estimates, geometric sound source localization}
}

@article{10.1109/TASLP.2014.2313404,
author = {Arora, Vipul and Behera, Laxmidhar},
title = {Musical Source Clustering and Identification in Polyphonic Audio},
year = {2014},
issue_date = {June 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2313404},
doi = {10.1109/TASLP.2014.2313404},
abstract = {For music transcription or musical source separation, apart from knowing the multi-F0 contours, it is also important to know which F0 has been played by which instrument. This paper focuses on this aspect, i.e. given the polyphonic audio along with its multiple F0 contours, the proposed system clusters them so as to decide 'which instrument played when.' For the task of identifying the instrument or singers in the polyphonic audio, there are many supervised methods available. But many times individual source audio is not available for training. To address this problem, this paper proposes novel schemes using semi-supervised as well as unsupervised approach to source clustering. The proposed theoretical framework is based on auditory perception theory and is implemented using various tools like probabilistic latent component analysis and graph clustering, while taking into account various perceptual cues for characterizing a source. Experiments have been carried out over a wide variety of datasets - ranging from vocal to instrumental as well as from synthetic to real world music. The proposed scheme significantly outperforms a state of the art unsupervised scheme, which does not make use of the given F0 contours. The proposed semi-supervised approach also performs better than another semi-supervised scheme, which makes use of the given F0 information, in terms of computations as well as accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {1003–1012},
numpages = {10},
keywords = {acoustic scene analysis, music information retrieval, polyphonic instrument identification}
}

@article{10.1109/TASLP.2014.2311299,
author = {Lacouture-Parodi, Yesenia and Habets, Emanu\"{e}l A. P. and Chen, Jingdong and Benesty, Jacob},
title = {Multichannel Noise Reduction in the Karhunen-Lo\`{e}ve Expansion Domain},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2311299},
doi = {10.1109/TASLP.2014.2311299},
abstract = {The noise reduction problem is traditionally approached in the time, frequency, or transform domain. Having a signal dependent transform has shown some advantages over the traditional signal independent transform. Recently, the single-channel noise reduction problem in the Karhunen-Lo\`{e}ve expansion (KLE) domain has received special attention. In this paper, the noise reduction problem in the KLE domain is studied from a multichannel perspective. We present a new formulation of the problem, in which inter-channel and inter-mode correlations are optimally exploited. We derive different optimal noise reduction filters and present a set of useful performance measures within this framework. The performance of the different filters is then evaluated through experiments in which not only noise but also competing speech sources are present. It is shown that the proposed multichannel formulation is more robust to competing speech sources than the single-channel approach and that a better compromise between noise reduction and speech distortion can be obtained.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {923–936},
numpages = {14},
keywords = {noise reduction, Karhunen-Lo\`{e}ve expansion (KLE), maximum snr filter, multichannel, tradeoff filter, speech enhancement, minimum variance distortionless response (MVDR) filter, wiener filter}
}

@article{10.1109/TASLP.2014.2311329,
author = {Sadjadi, Seyed Omid and Hansen, John H. L.},
title = {Blind Spectral Weighting for Robust Speaker Identification under Reverberation Mismatch},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2311329},
doi = {10.1109/TASLP.2014.2311329},
abstract = {Room reverberation poses various deleterious effects on performance of automatic speech systems. Speaker identification (SID) performance, in particular, degrades rapidly as reverberation time increases. Reverberation causes two forms of spectro-temporal distortions on speech signals: i) self-masking which is due to early reflections and ii) overlap-masking which is due to late reverberation. Overlap-masking effect of reverberation has been shown to have a greater adverse impact on performance of speech systems. Motivated by this fact, this study proposes a blind spectral weighting (BSW) technique for suppressing the reverberation overlap-masking effect on SID systems. The technique is blind in the sense that prior knowledge of neither the anechoic signal nor the room impulse response is required. Performance of the proposed technique is evaluated on speaker verification tasks under simulated and actual reverberant mismatched conditions. Evaluations are conducted in the context of the conventional GMM-UBM as well as the state-of-the-art i-vector based systems. The GMM-UBM experiments are performed using speech material from a new data corpus well suited for speaker verification experiments under actual reverberant mismatched conditions, entitled MultiRoom8. The i-vector experiments are carried out with microphone (interview and phonecall) data from the NIST SRE 2010 extended evaluation set which are digitally convolved with three different measured room impulse responses extracted from the Aachen impulse response (AIR) database. Experimental results prove that incorporating the proposed blind technique into the standard MFCC feature extraction framework yields significant improvement in SID performance under reverberation mismatch.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {937–945},
numpages = {9},
keywords = {speaker verification, reverberation, mismatch conditions, overlap-masking effect, NIST SRE}
}

@article{10.1109/TASLP.2014.2311925,
author = {Jarrett, Daniel P. and Taseska, Maja and Habets, Emanu\"{e}l A. P. and Naylor, Patrick A.},
title = {Noise Reduction in the Spherical Harmonic Domain Using a Tradeoff Beamformer and Narrowband DOA Estimates},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2311925},
doi = {10.1109/TASLP.2014.2311925},
abstract = {In noise reduction, a common approach is to use a microphone array with a beamformer that combines the individual microphone signals to extract a desired speech signal. The beamformer weights usually depend on the statistics of the noise and desired speech signals, which cannot be directly observed and must be estimated. Estimators based on the speech presence probability (SPP) seek to update the statistics estimates only when desired speech is known to be absent or present. However, they do not normally distinguish between desired and undesired speech sources. In this contribution, an algorithm is proposed to distinguish between these two types of sources using additional spatial information, by estimating a desired speech presence probability based on the combination of a multichannel SPP and a direction of arrival (DOA) based probability. The DOA-based probability is computed using DOA estimates for each time-frequency bin. The estimated statistics are then used to compute the weights of a spherical harmonic domain tradeoff beamformer, which achieves a balance between noise reduction and speech distortion. The performance evaluation demonstrates the effectiveness of the proposed approach at suppressing both background noise and spatially coherent noise. A number of audio examples and sample spectrograms are also provided.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {967–978},
numpages = {12},
keywords = {noise reduction, speech enhancement, speech presence probability, spherical harmonic domain}
}

@article{10.1109/TASLP.2014.2312541,
author = {Z\~{a}o, L. and Coelho, R. and Flandrin, P.},
title = {Speech Enhancement with EMD and Hurst-Based Mode Selection},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2312541},
doi = {10.1109/TASLP.2014.2312541},
abstract = {This paper presents a speech enhancement technique for signals corrupted by nonstationary acoustic noises. The proposed approach applies the empirical mode decomposition (EMD) to the noisy speech signal and obtains a set of intrinsic mode functions (IMF). The main contribution of the proposed procedure is the adoption of the Hurst exponent in the selection of IMFs to reconstruct the speech. This EMD and Hurst-based (EMDH) approach is evaluated in speech enhancement experiments considering environmental acoustic noises with different indices of nonstationarity. The results show that the EMDH improves the segmental signal-to-noise ratio and an overall quality composite measure, encompassing the perceptual evaluation of speech quality (PESQ). Moreover, the short-time objective intelligibility (STOI) measure reinforces the superior performance of EMDH. Finally, the EMDH is also examined in a speaker identification task in noisy conditions. The proposed technique leads to the highest speaker identification rates when compared to the baseline speech enhancement algorithms and also to a multicondition training procedure.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {899–911},
numpages = {13},
keywords = {hurst exponent, empirical mode decomposition, index of nonstationarity, speech enhancement, speaker identification}
}

@article{10.1109/TASLP.2014.2310993,
author = {Lee, Hung-Yi and Shiang, Sz-Rung and Yeh, Ching-Feng and Chen, Yun-Nung and Huang, Yu and Kong, Sheng-Yi and Lee, Lin-Shan},
title = {Spoken Knowledge Organization by Semantic Structuring and a Prototype Course Lecture System for Personalized Learning},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2310993},
doi = {10.1109/TASLP.2014.2310993},
abstract = {It takes very long time to go through a complete online course. Without proper background, it is also difficult to understand retrieved spoken paragraphs. This paper therefore presents a new approach of spoken knowledge organization for course lectures for efficient personalized learning. Automatically extracted key terms are taken as the fundamental elements of the semantics of the course. Key term graph constructed by connecting related key terms forms the backbone of the global semantic structure. Audio/video signals are divided into multi-layer temporal structure including paragraphs, sections and chapters, each of which includes a summary as the local semantic structure. The interconnection between semantic structure and temporal structure together with spoken term detection jointly offer to the learners efficient ways to navigate across the course knowledge with personalized learning paths considering their personal interests, available time and background knowledge. A preliminary prototype system has also been successfully developed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {883–898},
numpages = {16},
keywords = {spoken content retrieval, speech summarization, keyterm extraction, course lectures}
}

@article{10.1109/TASL.2014.2315271,
author = {Rieser, Verena and Lemon, Oliver and Keizer, Simon},
title = {Natural Language Generation as Incremental Planning under Uncertainty: Adaptive Information Presentation for Statistical Dialogue Systems},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASL.2014.2315271},
doi = {10.1109/TASL.2014.2315271},
abstract = {We present and evaluate a novel approach to natural language generation (NLG) in statistical spoken dialogue systems (SDS) using a data-driven statistical optimization framework for incremental information presentation (IP), where there is a trade-off to be solved between presenting "enough" information to the user while keeping the utterances short and understandable. The trained IP model is adaptive to variation from the current generation context (e.g. a user and a non-deterministic sentence planner), and it incrementally adapts the IP policy at the turn level. Reinforcement learning is used to automatically optimize the IP policy with respect to a data-driven objective function. In a case study on presenting restaurant information, we show that an optimized IP strategy trained on Wizard-of-Oz data outperforms a baseline mimicking the wizard behavior in terms of total reward gained. The policy is then also tested with real users, and improves on a conventional hand-coded IP strategy used in a deployed SDS in terms of overall task success. The evaluation found that the trained IP strategy significantly improves dialogue task completion for real users, with up to a 8.2% increase in task success. This methodology also provides new insights into the nature of the IP problem, which has previously been treated as a module following dialogue management with no access to lower-level context features (e.g. from a surface realizer and/or speech synthesizer).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {979–994},
numpages = {16},
keywords = {natural language user interfaces, reinforcement learning, information presentation, natural language generation}
}

@article{10.1109/TASLP.2014.2311324,
author = {Giacobello, Daniele and Christensen, Mads Gr\ae{}sb\o{}ll and Jensen, Tobias Lindstr\o{}m T. L. and Murthi, Manohar N. and Jensen, S\o{}ren Holdt and Moonen, Marc},
title = {Stable 1-Norm Error Minimization Based Linear Predictors for Speech Modeling},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2311324},
doi = {10.1109/TASLP.2014.2311324},
abstract = {In linear prediction of speech, the 1-norm error minimization criterion has been shown to provide a valid alternative to the 2-norm minimization criterion. However, unlike 2-norm minimization, 1-norm minimization does not guarantee the stability of the corresponding all-pole filter and can generate saturations when this is used to synthesize speech. In this paper, we introduce two new methods to obtain intrinsically stable predictors with the 1-norm minimization. The first method is based on constraining the roots of the predictor to lie within the unit circle by reducing the numerical range of the shift operator associated with the particular prediction problem considered. The second method uses the alternative Cauchy bound to impose a convex constraint on the predictor in the 1-norm error minimization. These methods are compared with two existing methods: the Burg method, based on the 1-norm minimization of the forward and backward prediction error, and the iteratively reweighted 2-norm minimization known to converge to the 1-norm minimization with an appropriate selection of weights. The evaluation gives proof of the effectiveness of the new methods, performing as well as unconstrained 1-norm based linear prediction for modeling and coding of speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {912–922},
numpages = {11},
keywords = {sparse linear prediction, all-pole modeling, linear prediction, autoregressive modeling, convex optimization}
}

@article{10.1109/TASLP.2014.2311319,
author = {Cheer, Jordan and Elliott, Stephen J.},
title = {Comments on "Complete Parallel Narrowband Active Noise Control Systems"},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2311319},
doi = {10.1109/TASLP.2014.2311319},
abstract = {In the above paper Chang and Kuo show that the convergence of parallel narrowband active noise control systems is limited by the interaction between the multiple narrowband adaptive filters. To overcome this problem they propose a new algorithm that provides separate error signals to each of the narrowband adaptive filters, generated by filtering the error signal using bandpass filters tuned to the control frequency of each of the adaptive filters. Although these bandpass filters do not introduce an additional phase shift at the control frequency they do introduce an additional group delay into the secondary path. It is shown in this correspondence that although introducing these bandpass filters allows an increase in the step size it may also limit the convergence speed compared to the conventional algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {995–996},
numpages = {2},
keywords = {adaptive algorithms, active noise control, convergence rate}
}

@article{10.1109/TASLP.2014.2312548,
author = {Zhang, Weibin and Fung, Pascale},
title = {Discriminatively Trained Sparse Inverse Covariance Matrices for Speech Recognition},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2312548},
doi = {10.1109/TASLP.2014.2312548},
abstract = {We propose to use acoustic models with sparse inverse covariance matrices to deal with the well-known over-fitting problem of discriminative training, especially when training data are limited. Compared with traditional diagonal or full covariance models, significant improvement by using sparse inverse covariance matrices has been achieved with maximum likelihood training. In state-of-the-art large vocabulary continuous speech recognition systems, discriminative training is commonly employed to achieve the best system performance. This paper investigates training acoustic models with sparse inverse covariance matrices using one of the most widely used discriminative training criteria-maximum mutual information (MMI). A lasso regularization term is added to the traditional objective function for MMI to automatically sparsify the inverse covariance matrices. The whole training process is then derived by maximizing the new objective function. This is achieved through iteratively maximizing a weak-sense auxiliary function. The final problem is shown to be a convex optimization problem and can be efficiently solved. Experimental results on the published Wall Street Journal and our collected Mandarin data sets show that the acoustic models with sparse inverse covariance matrices consistently outperform the conventional diagonal and full covariance models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {873–882},
numpages = {10},
keywords = {discriminative training, speech recognition, sparse inverse covariance matrix}
}

@article{10.1109/TASLP.2014.2307166,
author = {Osterwise, Christopher and Grant, Steven L.},
title = {On Over-Determined Frequency Domain BSS},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2307166},
doi = {10.1109/TASLP.2014.2307166},
abstract = {This paper introduces two new frequency domain overdetermined blind source separation (BSS) algorithms: Inter-frequency Correlation with Microphone Diversity (ICMD), and ICA with Triggered Principal component analysis (ITP). In the first, we consider different sets of microphones, where in each set the number of microphones and sources are equal. In the second, we extract principal components from an overdetermined mixture to form a determined mixture for separation. Both techniques utilize inter-frequency correlation to align permutations via energy profiles. Both monitor the condition number of an inter-frequency cross-correlation matrix of the normalized de-mixed signals' envelopes to determine if separation has failed for the current ICA input configuration; if so, the input configuration is revised and efficiently realigned to produce a better mixture for separation. The complexities and performances of these algorithms are examined in both simulations and a real-room measurement, with three and five sources. They are also compared to other recent frequency domain BSS algorithms for benchmarking purposes. Results show that generally, ICMD and ITP show similar performance with each other and with one of the benchmarking algorithms. However, ICMD is more computationally efficient.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {956–966},
numpages = {11},
keywords = {blind source separation, independent component analysis, frequency domain, over-determined BSS}
}

@article{10.1109/TASLP.2014.2311322,
author = {Mantena, Gautam and Achanta, Sivanand and Prahallad, Kishore},
title = {Query-by-Example Spoken Term Detection Using Frequency Domain Linear Prediction and Non-Segmental Dynamic Time Warping},
year = {2014},
issue_date = {May 2014},
publisher = {IEEE Press},
volume = {22},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2311322},
doi = {10.1109/TASLP.2014.2311322},
abstract = {The task of query-by-example spoken term detection (QbE-STD) is to find a spoken query within spoken audio data. Current state-of-the-art techniques assume zero prior knowledge about the language of the audio data, and thus explore dynamic time warping (DTW) based techniques for the QbE-STD task. In this paper, we use a variant of DTW based algorithm referred to as non-segmental DTW (NS-DTW), with a computational upper bound of O(mn) and analyze the performance of QbE-STD with Gaussian posteriorgrams obtained from spectral and temporal features of the speech signal. The results show that frequency domain linear prediction cepstral coefficients, which capture the temporal dynamics of the speech signal, can be used as an alternative to traditional spectral parameters such as linear prediction cepstral coefficients, perceptual linear prediction cepstral coefficients and Mel-frequency cepstral coefficients. We also introduce another variant of NS-DTW called fast NS-DTW (FNS-DTW) which uses reduced feature vectors for search. With a reduction factor of α ∈ N, we show that the computational upper bound for FNS-DTW is O(mn/α2) which is faster than NS-DTW.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {946–955},
numpages = {10},
keywords = {query-by-example spoken term detection, frequency domain linear prediction, fast search, dynamic time warping}
}

@article{10.1109/TASLP.2014.2304635,
author = {Crocco, Marco and Trucco, Andrea},
title = {Design of Superdirective Planar Arrays with Sparse Aperiodic Layouts for Processing Broadband Signals via 3-D Beamforming},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2304635},
doi = {10.1109/TASLP.2014.2304635},
abstract = {Planar arrays are used jointly with filter-and-sum beamforming to achieve 3-D spatial discrimination in processing broadband signals. In these systems, the beams are steered in various directions to investigate a given portion of space. The band can be so wide as to require both superdirective performance (to increase directivity at low frequencies) and sparse aperiodic layouts (to avoid grating lobes at high frequencies). We propose an original method to simultaneously optimize the transducer positions and the coefficients of the Finite Impulse Response FIR filters, providing a solution that maintains its validity for whatever steering direction inside a predefined region of interest. A hybrid strategy, analytical for the coefficients and stochastic for the positions, is devised to minimize the beam pattern (BP) energy while maintaining an unaltered signal from the steering direction and controlling the side lobes. The robustness of the superdirectivity is achieved by taking into account the probability density functions for the characteristics of realistic transducers. A distinctive feature of our method is its ability to maintain the computational tractability of the addressed optimization problem by drastically reducing the burden of evaluating the cost function. The obtained results, addressing tens of transducers and several octaves of band, demonstrate the effectiveness of the proposed method in terms of directivity, contrast, and robustness.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {800–815},
numpages = {16},
keywords = {microphone arrays, filter-and-sum beamforming, aperiodic arrays, acoustic imaging, 3-D beamforming, sparse arrays, robust superdirective beamforming, planar arrays}
}

@article{10.1109/TASLP.2014.2305252,
author = {Zapata, Jos\'{e} R. and Davies, Matthew E. P. and G\'{o}mez, Emilia},
title = {Multi-Feature Beat Tracking},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2305252},
doi = {10.1109/TASLP.2014.2305252},
abstract = {A recent trend in the field of beat tracking for musical audio signals has been to explore techniques for measuring the level of agreement and disagreement between a committee of beat tracking algorithms. By using beat tracking evaluation methods to compare all pairwise combinations of beat tracker outputs, it has been shown that selecting the beat tracker which most agrees with the remainder of the committee, on a song-by-song basis, leads to improved performance which surpasses the accuracy of any individual beat tracker used on its own. In this paper we extend this idea towards presenting a single, standalone beat tracking solution which can exploit the benefit ofmutual agreement without the need to run multiple separate beat tracking algorithms. In contrast to existing work, we re-cast the problem as one of selecting between the beat outputs resulting from a single beat tracking model with multiple, diverse input features. Through extended evaluation on a large annotated database, we show that our multi-feature beat tracker can outperform the state of the art, and thereby demonstrate that there is sufficient diversity in input features for beat tracking, without the need for multiple tracking models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {816–825},
numpages = {10},
keywords = {music information retrieval, beat tracking, music signal processing, evaluation}
}

@article{10.1109/TASLP.2014.2304240,
author = {Serizel, Romain and Moonen, Marc and Van Dijk, Bas and Wouters, Jan},
title = {Low-Rank Approximation Based Multichannel Wiener Filter Algorithms for Noise Reduction with Application in Cochlear Implants},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2304240},
doi = {10.1109/TASLP.2014.2304240},
abstract = {This paper presents low-rank approximation based multichannel Wiener filter algorithms for noise reduction in speech plus noise scenarios, with application in cochlear implants. In a single speech source scenario, the frequency-domain autocorrelation matrix of the speech signal is often assumed to be a rank-1 matrix, which then allows to derive different rank-1 approximation based noise reduction filters. In practice, however, the rank of the autocorrelation matrix of the speech signal is usually greater than one. Firstly, the link between the different rank-1 approximation based noise reduction filters and the original speech distortion weighted multichannel Wiener filter is investigated when the rank of the autocorrelation matrix of the speech signal is indeed greater than one. Secondly, in low input signal-to-noise-ratio scenarios, due to noise non-stationarity, the estimation of the auto-correlation matrix of the speech signal can be problematic and the noise reduction filters can deliver unpredictable noise reduction performance. An eigenvalue decomposition based filter and a generalized eigenvalue decomposition based filter are introduced that include a more robust rank-1, or more generally rank-R, approximation of the autocorrelation matrix of the speech signal. These noise reduction filters are demonstrated to deliver a better noise reduction performance especially in low input signal-to-noise-ratio scenarios. The filters are especially useful in cochlear implants, where more speech distortion and hence a more aggressive noise reduction can be tolerated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {785–799},
numpages = {15}
}

@article{10.1109/TASLP.2014.2308398,
author = {Zhao, Xiaojia and Wang, Yuxuan and Wang, DeLiang},
title = {Robust Speaker Identification in Noisy and Reverberant Conditions},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2308398},
doi = {10.1109/TASLP.2014.2308398},
abstract = {Robustness of speaker recognition systems is crucial for real-world applications, which typically contain both additive noise and room reverberation. However, the combined effects of additive noise and convolutive reverberation have been rarely studied in speaker identification (SID). This paper addresses this issue in two phases. We first remove background noise through binary masking using a deep neural network classifier. Then we perform robust SID with speaker models trained in selected reverberant conditions, on the basis of bounded marginalization and direct masking. Evaluation results show that the proposed system substantially improves SID performance over related systems in a wide range of reverberation time and signal-to-noise ratios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {836–845},
numpages = {10},
keywords = {noise, reverberation, ideal binary mask, deep neural network, robust speaker identification}
}

@article{10.1109/TASLP.2014.2308473,
author = {Cumani, Sandro and Plchot, Old\v{r}ich and Laface, Pietro},
title = {On the Use of I-Vector Posterior Distributions in Probabilistic Linear Discriminant Analysis},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2308473},
doi = {10.1109/TASLP.2014.2308473},
abstract = {The i-vector extraction process is affected by several factors such as the noise level, the acoustic content of the observed features, the channel mismatch between the training conditions and the test data, and the duration of the analyzed speech segment. These factors influence both the i-vector estimate and its uncertainty, represented by the i-vector posterior covariance. This paper presents a new PLDA model that, unlike the standard one, exploits the intrinsic i-vector uncertainty. Since the recognition accuracy is known to decrease for short speech segments, and their length is one of the main factors affecting the i-vector covariance, we designed a set of experiments aiming at comparing the standard and the new PLDA models on short speech cuts of variable duration, randomly extracted from the conversations included in the NIST SRE 2010 extended dataset, both from interviews and telephone conversations. Our results on NIST SRE 2010 evaluation data show that in different conditions the new model outperforms the standard PLDA by more than 10% relative when tested on short segments with duration mismatches, and is able to keep the accuracy of the standard model for long enough speaker segments. This technique has also been successfully tested in the NIST SRE 2012 evaluation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {846–857},
numpages = {12},
keywords = {I-vector extraction, I-vectors, probabilistic linear discriminant analysis, speaker recognition}
}

@article{10.1109/TASLP.2014.2305833,
author = {Narayanan, Arun and Wang, DeLiang},
title = {Investigation of Speech Separation as a Front-End for Noise Robust Speech Recognition},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2305833},
doi = {10.1109/TASLP.2014.2305833},
abstract = {Recently, supervised classification has been shown to work well for the task of speech separation. We perform an in-depth evaluation of such techniques as a front-end for noise-robust automatic speech recognition (ASR). The proposed separation front-end consists of two stages. The first stage removes additive noise via time-frequency masking. The second stage addresses channel mismatch and the distortions introduced by the first stage; a non-linear function is learned that maps the masked spectral features to their clean counterpart. Results show that the proposed front-end substantially improves ASR performance when the acoustic models are trained in clean conditions. We also propose a diagonal feature discriminant linear regression (dFDLR) adaptation that can be performed on a per-utterance basis for ASR systems employing deep neural networks and HMM. Results show that dFDLR consistently improves performance in all test conditions. Surprisingly, the best average results are obtained when dFDLR is applied to models trained using noisy log-Mel spectral features from the multi-condition training set. With no channel mismatch, the best results are obtained when the proposed speech separation front-end is used along with multi-condition training using log-Mel features followed by dFDLR adaptation. Both these results are among the best on the Aurora-4 dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {826–835},
numpages = {10},
keywords = {deep neural networks, feature mapping, robust ASR, time-frequency masking, Aurora-4}
}

@article{10.1109/TASLP.2014.2304637,
author = {Li, Jinyu and Deng, Li and Gong, Yifan and Haeb-Umbach, Reinhold},
title = {An Overview of Noise-Robust Automatic Speech Recognition},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2304637},
doi = {10.1109/TASLP.2014.2304637},
abstract = {New waves of consumer-centric applications, such as voice search and voice interaction with mobile devices and home entertainment systems, increasingly require automatic speech recognition (ASR) to be robust to the full range of real-world noise and other acoustic distorting conditions. Despite its practical importance, however, the inherent links between and distinctions among the myriad of methods for noise-robust ASR have yet to be carefully studied in order to advance the field further. To this end, it is critical to establish a solid, consistent, and common mathematical foundation for noise-robust ASR, which is lacking at present. This article is intended to fill this gap and to provide a thorough overview of modern noise-robust techniques for ASR developed over the past 30 years. We emphasize methods that are proven to be successful and that are likely to sustain or expand their future applicability. We distill key insights from our comprehensive overview in this field and take a fresh look at a few old problems, which nevertheless are still highly relevant today. Specifically, we have analyzed and categorized a wide range of noise-robust techniques using five different criteria: 1) feature-domain vs. model-domain processing, 2) the use of prior knowledge about the acoustic environment distortion, 3) the use of explicit environment-distortion models, 4) deterministic vs. uncertainty processing, and 5) the use of acoustic models trained jointly with the same feature enhancement or model adaptation process used in the testing stage. With this taxonomy-oriented review, we equip the reader with the insight to choose among techniques and with the awareness of the performance-complexity tradeoffs. The pros and cons of using different noise-robust ASR techniques in practical application scenarios are provided as a guide to interested practitioners. The current challenges and future research directions in this field is also carefully analyzed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {745–777},
numpages = {33},
keywords = {robustness, distortion modeling, compensation, uncertainty processing, noise, speech recognition, joint model training}
}

@article{10.1109/TASLP.2014.2310353,
author = {Wu, Chung-Hsien and Shen, Han-Ping and Yang, Yan-Ting},
title = {Chinese-English Phone Set Construction for Code-Switching ASR Using Acoustic and DNN-Extracted Articulatory Features},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2310353},
doi = {10.1109/TASLP.2014.2310353},
abstract = {This study proposes a data-driven approach to phone set construction for code-switching automatic speech recognition (ASR). Acoustic and context-dependent cross-lingual articulatory features (AFs) are incorporated into the estimation of the distance between triphone units for constructing a Chinese-English phone set. The acoustic features of each triphone in the training corpus are extracted for constructing an acoustic triphone HMM. Furthermore, the articulatory features of the "last/first" state of the corresponding preceding/succeeding triphone in the training corpus are used to construct an AF-based GMM. The AFs, extracted using a deep neural network (DNN), are used for code-switching articulation modeling to alleviate the data sparseness problem due to the diverse context-dependent phone combinations in intra-sentential code-switching. The triphones are then clustered to obtain a Chinese-English phone set based on the acoustic HMMs and the AF-based GMMs using a hierarchical triphone clustering algorithm. Experimental results on code-switching ASR show that the proposed method for phone set construction outperformed other traditional methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {858–862},
numpages = {5},
keywords = {phone set construction, code-switching, articulatory features, speech recognition}
}

@article{10.1109/TASLP.2014.2303296,
author = {Sarikaya, Ruhi and Hinton, Geoffrey E. and Deoras, Anoop},
title = {Application of Deep Belief Networks for Natural Language Understanding},
year = {2014},
issue_date = {April 2014},
publisher = {IEEE Press},
volume = {22},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2303296},
doi = {10.1109/TASLP.2014.2303296},
abstract = {Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {778–784},
numpages = {7},
keywords = {DBN, deep learning, natural language understanding, call-routing, deep neural nets, RBM}
}

@article{10.1109/TASLP.2014.2300344,
author = {Raczynski, Stanislaw A. and Vincent, Emmanuel},
title = {Genre-Based Music Language Modeling with Latent Hierarchical Pitman-Yor Process Allocation},
year = {2014},
issue_date = {March 2014},
publisher = {IEEE Press},
volume = {22},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2300344},
doi = {10.1109/TASLP.2014.2300344},
abstract = {In this work we present a new Bayesian topic model: latent hierarchical Pitman-Yor process allocation (LHPYA), which uses hierarchical Pitman-Yor process priors for both word and topic distributions, and generalizes a few of the existing topic models, including the latent Dirichlet allocation (LDA), the bigram topic model and the hierarchical Pitman-Yor topic model. Using such priors allows for integration of  $n$-grams with a topic model, while smoothing them with the state-of-the-art method. Our model is evaluated by measuring its perplexity on a dataset of musical genre and harmony annotations 3 Genre Database (3GDB) and by measuring its ability to predict musical genre from chord sequences. In terms of perplexity, for a 262-chord dictionary we achieve a value of 2.74, compared to 18.05 for trigrams and 7.73 for a unigram topic model. In terms of genre prediction accuracy with 9 genres, the proposed approach performs about 33% better in relative terms than genre-dependent $n$ -grams, achieving 60.4% of accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {672–681},
numpages = {10}
}

@article{10.1109/TASLP.2014.2300339,
author = {Wei, Wen-Li and Wu, Chung-Hsien and Lin, Jen-Chun and Li, Han},
title = {Exploiting Psychological Factors for Interaction Style Recognition in Spoken Conversation},
year = {2014},
issue_date = {March 2014},
publisher = {IEEE Press},
volume = {22},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2300339},
doi = {10.1109/TASLP.2014.2300339},
abstract = {Determining how a speaker is engaged in a conversation is crucial for achieving harmonious interaction between computers and humans. In this study, a fusion approach was developed based on psychological factors to recognize Interaction Style ($IS$ ) in spoken conversation, which plays a key role in creating natural dialogue agents. The proposed Fused Cross-Correlation Model (FCCM) provides a unified probabilistic framework to model the relationships among the psychological factors of emotion, personality trait ($PT$), transient  $IS$, and  $IS$ history, for recognizing $IS$. An emotional arousal-dependent speech recognizer was used to obtain the recognized spoken text for extracting linguistic features to estimate transient  $IS$ likelihood and recognize $PT$. A temporal course modeling approach and an emotional sub-state language model, based on the temporal phases of an emotional expression, were employed to obtain a better emotion recognition result. The experimental results indicate that the proposed FCCM yields satisfactory results in  $IS$ recognition and also demonstrate that combining psychological factors effectively improves  $IS$ recognition accuracy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {659–671},
numpages = {13}
}

@article{10.1109/TASLP.2013.2297012,
author = {Asaei, Afsaneh and Golbabaee, Mohammad and Bourlard, Herve and Cevher, Volkan},
title = {Structured Sparsity Models for Reverberant Speech Separation},
year = {2014},
issue_date = {March 2014},
publisher = {IEEE Press},
volume = {22},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2297012},
doi = {10.1109/TASLP.2013.2297012},
abstract = {We tackle the speech separation problem through modeling the acoustics of the reverberant chambers. Our approach exploits structured sparsity models to perform speech recovery and room acoustic modeling from recordings of concurrent unknown sources. The speakers are assumed to lie on a two-dimensional plane and the multipath channel is characterized using the image model. We propose an algorithm for room geometry estimation relying on localization of the early images of the speakers by sparse approximation of the spatial spectrum of the virtual sources in a free-space model. The images are then clustered exploiting the low-rank structure of the spectro-temporal components belonging to each source. This enables us to identify the early support of the room impulse response function and its unique map to the room geometry. To further tackle the ambiguity of the reflection ratios, we propose a novel formulation of the reverberation model and estimate the absorption coefficients through a convex optimization exploiting joint sparsity model formulated upon spatio-spectral sparsity of concurrent speech representation. The acoustic parameters are then incorporated for separating individual speech signals through either structured sparse recovery or inverse filtering the acoustic channels. The experiments conducted on real data recordings of spatially stationary sources demonstrate the effectiveness of the proposed approach for speech separation and recognition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {620–633},
numpages = {14}
}

@article{10.1109/TASLP.2013.2297013,
author = {Rashobh, Rajan S. and Khong, Andy W. H. and Liu, Di},
title = {Multichannel Equalization in the KLT and Frequency Domains With Application to Speech Dereverberation},
year = {2014},
issue_date = {March 2014},
publisher = {IEEE Press},
volume = {22},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2297013},
doi = {10.1109/TASLP.2013.2297013},
abstract = {Equalization of acoustic channels usually involves inversion of acoustic impulse responses (AIRs), and generally employs multichannel techniques. In this paper, we propose three equalization algorithms, one in the Karhunen-Lo\`{e}ve transform (KLT) domain and the other two in the frequency domain. Our proposed algorithm in the KLT domain provides a platform to achieve equalization in conjunction with denoising. Existing multiple-input/output inverse theorem (MINT)-based non-adaptive algorithms require the inversion of a matrix with dimension that is proportional to the AIR length, and is computationally expensive. To overcome this limitation, we propose the frequency-domain algorithm which is computationally very efficient and thus can be employed for the equalization of high-order AIRs in practical applications. In addition, the frequency-domain method is more robust to AIR estimation errors. To achieve further reduction in the complexity without significant performance degradation, we then propose a modified version of the frequency-domain algorithm.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {634–646},
numpages = {13}
}

@article{10.1109/TASLP.2013.2297018,
author = {Wu, Chung-Hsien and Huang, Yi-Chin and Lee, Chung-Han and Guo, Jun-Cheng},
title = {Synthesis of Spontaneous Speech With Syllable Contraction Using State-Based Context-Dependent Voice Transformation},
year = {2014},
issue_date = {March 2014},
publisher = {IEEE Press},
volume = {22},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2297018},
doi = {10.1109/TASLP.2013.2297018},
abstract = {Pronunciation normally varies in spontaneous speech, and is an integral aspect of spontaneous expression. This study describes a voice transformation-based approach to generating spontaneous speech with syllable contractions for Hidden Markov Model (HMM)-based speech synthesis. A multi-dimensional linear regression model is adopted as the context-dependent, state-based transformation function to convert the feature sequence of read speech to that of spontaneous speech with syllable contraction. With insufficient number of training data, the obtained transformation functions are categorized using a decision tree based on linguistic and articulatory features for better and efficient selection of suitable transformation functions. Furthermore, to cope with the problem of small parallel corpus, cross-validation of trained transformation function is performed to ensure correct transformation functions are obtained and prevent over-fitting. Consequently, pronunciation variations of syllable contraction for the trained and the unseen syllable-contracted words are generated from the transformation function retrieved from the decision tree using linguistic and articulatory features. Objective and subjective tests were used to evaluate the performance of the proposed approach. Evaluation results demonstrate that the proposed transformation function substantially improves apparent spontaneity of the synthesized speech compared to the conventional methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {585–595},
numpages = {11}
}

@article{10.1109/TASLP.2014.2300341,
author = {Samarasinghe, Prasanga and Abhayapala, Thushara and Poletti, Mark},
title = {Wavefield Analysis Over Large Areas Using Distributed Higher Order Microphones},
year = {2014},
issue_date = {March 2014},
publisher = {IEEE Press},
volume = {22},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2014.2300341},
doi = {10.1109/TASLP.2014.2300341},
abstract = {Successful recording of large spatial soundfields is a prevailing challenge in acoustic signal processing due to the enormous numbers of microphones required. This paper presents the design and analysis of an array of higher order microphones that uses 2D wavefield translation to provide a mode matching solution to the height invariant recording problem. It is shown that the use of $M$th order microphones significantly reduces the number of microphone units by a factor of $1/(2M + 1)$ at the expense of increased complexity at each microphone unit. Robustness of the proposed array is also analyzed based on the condition number of the translation matrix while discussing array configurations that result in low condition numbers. The white-noise gain (WNG) of the array is then derived to verify that improved WNG can be achieved when the translation matrix is well conditioned. Furthermore, the array’s performance is studied for interior soundfield recording as well as exterior soundfield recording using appropriate simulation examples.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {647–658},
numpages = {12}
}

@article{10.1109/TASLP.2013.2294578,
author = {Yang, Jae-Mo and Kang, Hong-Goo},
title = {Online Speech Dereverberation Algorithm Based on Adaptive Multichannel Linear Prediction},
year = {2014},
issue_date = {March 2014},
publisher = {IEEE Press},
volume = {22},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2294578},
doi = {10.1109/TASLP.2013.2294578},
abstract = {This paper proposes a real-time acoustic channel equalization method that uses an adaptive multichannel linear prediction technique. In general, multichannel equalization algorithms can eliminate reverberation if they meet the following specific conditions including: the co-primeness between channels and sufficient filter length. It also requires the characteristic of correct channel information, however, it is difficult to estimate accurate acoustic channels in a practical system. The proposed method utilizes a theoretically perfect channel equalization algorithm and considers problems that may arise in the actual system. Linear-predictive multi-input equalization (LIME) is also an appropriate attempt at blind dereverberation by assuring the theoretical basis. However, a huge computational cost is incurred by calculating the large dimensions of a covariance matrix and its inversion. The proposed equalizer is developed as a multichannel linear prediction (MLP) oriented structure with a new formula that is optimized to time-varying acoustical room environments. Moreover, experimental results show that the proposed method works well even if the channel characteristics of each microphone are similar. The results of experiments using various room impulse response (RIR) models, including both the synthesized and real room environments, show that the proposed method is superior to conventional methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {608–619},
numpages = {12}
}

@article{10.1109/TASLP.2013.2294585,
author = {Airaksinen, Manu and Raitio, Tuomo and Story, Brad and Alku, Paavo},
title = {Quasi Closed Phase Glottal Inverse Filtering Analysis With Weighted Linear Prediction},
year = {2014},
issue_date = {March 2014},
publisher = {IEEE Press},
volume = {22},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2294585},
doi = {10.1109/TASLP.2013.2294585},
abstract = {This study presents a new glottal inverse filtering (GIF) technique based on closed phase analysis over multiple fundamental periods. The proposed quasi closed phase (QCP) analysis method utilizes weighted linear prediction (WLP) with a specific attenuated main excitation (AME) weight function that attenuates the contribution of the glottal source in the linear prediction model optimization. This enables the use of the autocorrelation criterion in linear prediction in contrast to the covariance criterion used in conventional closed phase analysis. The QCP method was compared to previously developed methods by using synthetic vowels produced with the conventional source-filter model as well as with a physical modeling approach. The obtained objective measures show that the QCP method improves the GIF performance in terms of errors in typical glottal source parametrizations for both low- and high-pitched vowels. Additionally, QCP was tested in a physiologically oriented vocoder, where the analysis/synthesis quality was evaluated with a subjective listening test indicating improved perceived quality for normal speaking style.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {596–607},
numpages = {12}
}

@article{10.1109/TASLP.2013.2292356,
author = {Hasan, Taufiq and Hansen, John H. L.},
title = {Maximum Likelihood Acoustic Factor Analysis Models for Robust Speaker Verification in Noise},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2292356},
doi = {10.1109/TASLP.2013.2292356},
abstract = {Recent speaker recognition/verification systems generally utilize an utterance dependent fixed dimensional vector as features to Bayesian classifiers. These vectors, known as i-Vectors, are lower dimensional representations of Gaussian Mixture Model (GMM) mean super-vectors adapted from a Universal Background Model (UBM) using speech utterance features, and extracted utilizing a Factor Analysis (FA) framework. This method is based on the assumption that the speaker dependent information resides in a lower dimensional sub-space. In this study, we utilize a mixture of Acoustic Factor Analyzers (AFA) to model the acoustic features instead of a GMM-UBM. Following our previously proposed AFA technique (“Acoustic factor analysis for robust speaker verification,” by Hasan and Hansen, IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 4, April 2013), this model is based on the assumption that the speaker relevant information lies in a lower dimensional subspace in the multi-dimensional feature space localized by the mixture components. Unlike our previous method, here we train the AFA-UBM model directly from the data using an Expectation-Maximization (EM) algorithm. This method shows improved robustness to noise as the nuisance dimensions are removed in each EM iteration. Two variants of the AFA model are considered utilizing an isotropic and diagonal covariance residual term. The method is integrated within a standard i-Vector system where the hidden variables of the model, termed as acoustic factors, are utilized as the input for total variability modeling. Experimental results obtained on the 2012 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) core-extended trials indicate the effectiveness of the proposed strategy in both clean and noisy conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {381–391},
numpages = {11}
}

@article{10.1109/TASL.2013.2282191,
author = {Dehong Gao and Wenjie Li and Xiaoyan Cai and Renxian Zhang and You Ouyang},
title = {Sequential Summarization: A Full View of Twitter Trending Topics},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASL.2013.2282191},
doi = {10.1109/TASL.2013.2282191},
abstract = {As an information delivering platform, Twitter collects millions of tweets every day. However, some users, especially new users, often find it difficult to understand trending topics in Twitter when confronting the overwhelming and unorganized tweets. Existing work has attempted to provide a short snippet to explain a topic, but this only provides limited benefits and cannot satisfy the users' expectations. In this paper, we propose a new summarization task, namely sequential summarization, which aims to provide a serial of chronologically ordered short sub-summaries for a trending topic in order to provide a complete story about the development of the topic while retaining the order of information presentation. Different from the traditional summarization task, the numbers of sub-summaries for different topics are not fixed. Two approaches, i.e., stream-based and semantic-based approaches, are developed to detect the important subtopics within a trending topic. Then a short sub-summary is generated for each subtopic. In addition, we propose three new measures to evaluate the position-aware coverage, sequential novelty and sequence correlation of the system-generated summaries. The experimental results based on the proposed evaluation criteria have demonstrated the effectiveness of the proposed approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {293–302},
numpages = {10}
}

@article{10.1109/TASLP.2013.2294584,
author = {Min Zhang and Xiangyu Duan and Wenliang Chen},
title = {Bayesian Constituent Context Model for Grammar Induction},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2294584},
doi = {10.1109/TASLP.2013.2294584},
abstract = {Constituent Context Model (CCM) is an effective generative model for grammar induction, the aim of which is to induce hierarchical syntactic structure from natural text. The CCM simply defines the Multinomial distribution over constituents, which leads to a severe data sparse problem because long constituents are unlikely to appear in unseen data sets. This paper proposes a Bayesian method for constituent smoothing by defining two kinds of prior distributions over constituents: the Dirichlet prior and the Pitman-Yor Process prior. The Dirichlet prior functions as an additive smoothing method, and the PYP prior functions as a back-off smoothing method. Furthermore, a modified CCM is proposed to differentiate left constituents and right constituents in binary branching trees. Experiments show that both the proposed Bayesian smoothing method and the modified CCM are effective, and combining them attains or significantly improves the state-of-the-art performance of grammar induction evaluated on standard treebanks of various languages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {531–541},
numpages = {11}
}

@article{10.1109/TASLP.2013.2294581,
author = {Hacihabiboglu, Huseyin},
title = {Theoretical Analysis of Open Spherical Microphone Arrays for Acoustic Intensity Measurements},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2294581},
doi = {10.1109/TASLP.2013.2294581},
abstract = {Acoustic intensity is a vectorial measure of acoustic energy flow through a given region of interest. Three-dimensional measurement of acoustic intensity requires special microphone array configurations. This paper provides a theoretical analysis of open spherical microphone arrays for the 3-D measurement of acoustic intensity. The calculations of the pressure and the particle velocity components of the sound field inside a closed volume are expressed using the Kirchhoff-Helmholtz integral equation. The conditions which simplify the calculation are identified. This calculation is then constrained to a finite set of microphones positioned at prescribed points on an open sphere. Several open spherical array topologies are proposed. Their magnitude and directional errors and measurement bandwidths are investigated via numerical simulations. A comparison with conventional open-sphere 3-D intensity probes is presented.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {465–476},
numpages = {12}
}

@article{10.1109/TASLP.2013.2297016,
author = {Dah-Chung Chang and Fei-Tao Chu},
title = {Feedforward Active Noise Control With a New Variable Tap-Length and Step-Size Filtered-X LMS Algorithm},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2297016},
doi = {10.1109/TASLP.2013.2297016},
abstract = {The fixed tap-length and step-size filtered-X least mean-square (FxLMS) algorithm is conventionally used in active noise control (ANC) systems. A tradeoff between the performance and the convergence rate is a well-known problem due to the choice of the step size. Although the variable-step-size FxLMS algorithms have been proposed for fast convergence, a long tap-length filter is frequently required in order to deal with different environments such that the convergence rate is still subject to a small step size for the long tap length. In this paper, we study a new ANC system with a variable tap-length and step-size FxLMS algorithm. Based on the assumption of an unsymmetric and two-sided exponential decay response model for the ANC control filter, the new FxLMS algorithm has the minimum mean-square deviation for the optimal filter coefficients. In the online secondary path modeling ANC system, simulation results show that the new algorithm with different kind of variable step sizes can provide significant improvements of convergence rate and noise reduction ratio, compared to the fixed-tap-length FxLMS algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {542–555},
numpages = {14}
}

@article{10.1109/TASLP.2013.2295918,
author = {Gonzalez, Sira and Brookes, Mike},
title = {PEFAC - A Pitch Estimation Algorithm Robust to High Levels of Noise},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2295918},
doi = {10.1109/TASLP.2013.2295918},
abstract = {We present PEFAC, a fundamental frequency estimation algorithm for speech that is able to identify voiced frames and estimate pitch reliably even at negative signal-to-noise ratios. The algorithm combines a normalization stage, to remove channel dependency and to attenuate strong noise components, with a harmonic summing filter applied in the log-frequency power spectral domain, the impulse response of which is chosen to sum the energy of the fundamental frequency harmonics while attenuating smoothly-varying noise components. Temporal continuity constraints are applied to the selected pitch candidates and a voiced speech probability is computed from the likelihood ratio of two classifiers, one for voiced speech and one for unvoiced speech/silence. We compare the performance of our algorithm with that of other widely used algorithms and demonstrate that it performs well in both high and low levels of additive noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {518–530},
numpages = {13}
}

@article{10.1109/TASLP.2013.2294586,
author = {Pui-Yu Hui and Meng, Helen},
title = {Latent Semantic Analysis for Multimodal User Input With Speech and Gestures},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2294586},
doi = {10.1109/TASLP.2013.2294586},
abstract = {This paper describes our work in semantic interpretation of a “multimodal language” with speech and gestures using latent semantic analysis (LSA). Our aim is to infer the domain-specific informational goal of multimodal inputs. The informational goal is characterized by lexical terms used in the spoken modality, partial semantics of gestures in the pen modality, as well as term co-occurrence patterns across modalities, leading to “multimodal terms.” We designed and collected a multimodal corpus of navigational inquiries. We also obtained perfect (i.e. manual) and imperfect (i.e. automatic via recognition) transcriptions for these. We automatically align parsed spoken locative references (SLRs) with their corresponding pen gesture(s) using the Viterbi alignment, according to their numeric and location type features. Then, we characterize each cross-modal integration pattern as a 3-tuple multimodal term with SLR, pen gesture type and their temporal relationship. We propose to use latent semantic analysis (LSA) to derive the latent semantics from manual (i.e. perfect) and automatic (i.e. imperfect) transcriptions of the collected multimodal inputs. In order to achieve this, both multimodal and lexical terms are used to compose an inquiry-term matrix, which is then factorized using singular value decomposition (SVD) to derive the latent semantics automatically. Informational goal inference based on the latent semantics shows that the informational goal inference accuracy of a disjoint test set is 99% and 84% when a perfect and imperfect projection model is used respectively, which performs significantly better than (at least 9.9% absolute) the baseline performance using vector-space model (VSM).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {417–429},
numpages = {13}
}

@article{10.1109/TASLP.2013.2292308,
author = {Souden, Mehrez and Kinoshita, Keisuke and Delcroix, Marc and Nakatani, Tomohiro},
title = {Location Feature Integration for Clustering-Based Speech Separation in Distributed Microphone Arrays},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2292308},
doi = {10.1109/TASLP.2013.2292308},
abstract = {In distributed microphone arrays (DMAs) the source location information can be defined at the intra and inter-node levels. Indeed, while the first type of information results from the diversity of acoustic channels recorded by microphones embedded in the same node, the second is attributed to the differences between the acoustic channels observed by spatially distributed nodes. Both cues are very useful in DMA processing, and the aim of this paper is to utilize both of them to cluster and separate multiple competing speech signals. To capture the intra-node information, we employ the normalized recording vector, while at the inter-node level, we consider different features including the energy level differences with and without the phase differences between nodes. We model the intra-node information using the Watson mixture model (WMM), and propose using the Gamma mixture model (GaMM), Dirichlet mixture model (DMM), and WMM to model different inter-node location features. Furthermore, we propose several integrations of the intra-node and inter-node feature contributions to cluster speech recordings using the expectation maximization algorithm. Finally, simulation results are provided to demonstrate the performance of all ensuing methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {354–367},
numpages = {14}
}

@article{10.1109/TASLP.2013.2292362,
author = {Yu Tsao and Matsuda, Shigeki and Hori, Chiori and Kashioka, Hideki and Chin-Hui Lee},
title = {A MAP-Based Online Estimation Approach to Ensemble Speaker and Speaking Environment Modeling},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2292362},
doi = {10.1109/TASLP.2013.2292362},
abstract = {An ensemble speaker and speaking environment modeling (ESSEM) approach was recently developed. This ESSEM process consists of offline and online phases. The offline phase establishes an environment structure using speech data collected under a wide range of acoustic conditions, whereas the online phase estimates a set of acoustic models that matches the testing environment based on the established environment structure. Since the estimated acoustic models accurately characterize particular testing conditions, ESSEM can improve the speech recognition performance under adverse conditions. In this work, we propose two maximum a posteriori (MAP) based algorithms to improve the online estimation part of the original ESSEM framework. We first develop MAP-based environment structure adaptation to refine the original environment structure. Next, we propose to utilize the MAP criterion to estimate the mapping function of ESSEM and enhance the environment modeling capability. For the MAP estimation, three types of priors are derived; they are the clustered prior (CP), the sequential prior (SP), and the hierarchical prior (HP) densities. Since each prior density is able to characterize specific acoustic knowledge, we further derive a combination mechanism to integrate the three priors. Based on the experimental results on the Aurora-2 task, we verify that using the MAP-based online mapping function estimation can enable ESSEM to achieve better performance than using the maximum-likelihood (ML) based counterpart. Moreover, by using an integration of the online environment structuring adaptation and mapping function estimation, the proposed MAP-based ESSEM framework is found to provide the best performance. Compared with our baseline results, MAP-based ESSEM achieves an average word error rate reduction of 15.53% (5.41 to 4.57%) under 50 testing conditions at a signal-to-noise ratio (SNR) of 0 to 20 dB over the three standardized testing sets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {403–416},
numpages = {14}
}

@article{10.1109/TASLP.2013.2292361,
author = {Schwartz, Ofer and Gannot, Sharon},
title = {Speaker Tracking Using Recursive EM Algorithms},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2292361},
doi = {10.1109/TASLP.2013.2292361},
abstract = {The problem of localizing and tracking a known number of concurrent speakers in noisy and reverberant enclosures is addressed in this paper. We formulate the localization task as a maximum likelihood (ML) parameter estimation problem, and solve it by utilizing the expectation-maximization (EM) procedure. For the tracking scenario, we propose to adapt two recursive EM (REM) variants. The first, based on Titterington's scheme, is a Newton-based recursion. In this work we also extend Titterington's method to deal with constrained maximization, encountered in the problem at hand. The second is based on Capp\'{e} and Moulines' scheme. We discuss the similarities and dissimilarities of these two variants and show their applicability to the tracking problem by a simulated experimental study.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {392–402},
numpages = {11}
}

@article{10.1109/TASLP.2013.2290497,
author = {Lun, Daniel P. K. and Tak-Wai Shen and Ho, K. C.},
title = {A Novel Expectation-Maximization Framework for Speech Enhancement in Non-Stationary Noise Environments},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2290497},
doi = {10.1109/TASLP.2013.2290497},
abstract = {Voiced speeches have a quasi-periodic nature that allows them to be compactly represented in the cepstral domain. It is a distinctive feature compared with noises. Recently, the temporal cepstrum smoothing (TCS) algorithm was proposed and was shown to be effective for speech enhancement in non-stationary noise environments. However, the missing of an automatic parameter updating mechanism limits its adaptability to noisy speeches with abrupt changes in SNR across time frames or frequency components. In this paper, an improved speech enhancement algorithm based on a novel expectation-maximization (EM) framework is proposed. The new algorithm starts with the traditional TCS method which gives the initial guess of the periodogram of the clean speech. It is then applied to an L1 norm regularizer in the M-step of the EM framework to estimate the true power spectrum of the original speech. It in turn enables the estimation of the a-priori SNR and is used in the E-step, which is indeed a logmmse gain function, to refine the estimation of the clean speech periodogram. The M-step and E-step iterate alternately until converged. A notable improvement of the proposed algorithm over the traditional TCS method is its adaptability to the changes (even abrupt changes) in SNR of the noisy speech. Performance of the proposed algorithm is evaluated using standard measures based on a large set of speech and noise signals. Evaluation results show that a significant improvement is achieved compared to conventional approaches especially in non-stationary noise environment where most conventional algorithms fail to perform.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {335–346},
numpages = {12}
}

@article{10.1109/TASLP.2013.2294580,
author = {McVicar, Matt and Santos-Rodriguez, Raul and Yizhao Ni and Tijl De Bie},
title = {Automatic Chord Estimation from Audio: A Review of the State of the Art},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2294580},
doi = {10.1109/TASLP.2013.2294580},
abstract = {In this overview article, we review research on the task of Automatic Chord Estimation (ACE). The major contributions from the last 14 years of research are summarized, with detailed discussions of the following topics: feature extraction, modeling strategies, model training and datasets, and evaluation strategies. Results from the annual benchmarking evaluation Music Information Retrieval Evaluation eXchange (MIREX) are also discussed as well as developments in software implementations and the impact of ACE within MIR. We conclude with possible directions for future research.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {556–575},
numpages = {20}
}

@article{10.1109/TASLP.2013.2295926,
author = {Taemin Cho and Bello, Juan P.},
title = {On the Relative Importance of Individual Components of Chord Recognition Systems},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2295926},
doi = {10.1109/TASLP.2013.2295926},
abstract = {Most chord recognition systems share a common architecture comprising two main stages: feature extraction and pattern matching, and two optional sub stages: pre-filtering and post-filtering. Understanding the interaction between these basic components is very important not only for achieving optimal performance, but also for assessing the potential and limitations of the system. Unfortunately, there are no studies that sufficiently evaluate the effects of the different approaches to each processing step and the interactions between these steps. In this paper we attempt to remedy this deficiency by performing a systematic evaluation encompassing a wide variety of techniques used for each processing step. In our study we find that filtering has a significant impact on performance, but providing musical context information in the transition matrix is rendered moot by the need to enforce continuity in the estimations. We discovered that the benefits of using complex chord models can be largely offset by an appropriate choice of features. In addition, the initial performance gap between different features were not fully compensated by any subsequent processing stages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {477–492},
numpages = {16}
}

@article{10.1109/TASLP.2013.2292328,
author = {Kallasjoki, Heikki and Gemmeke, Jort F. and Palomaki, Kalle J.},
title = {Estimating Uncertainty to Improve Exemplar-Based Feature Enhancement for Noise Robust Speech Recognition},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2292328},
doi = {10.1109/TASLP.2013.2292328},
abstract = {We present a method of improving automatic speech recognition performance under noisy conditions by using a source separation approach to extract the underlying clean speech signal. The feature enhancement processing is complemented with heuristic estimates of the uncertainty of the source separation, that are used to further assist the recognition. The uncertainty heuristics are converted to estimates of variance for the extracted clean speech using a Gaussian Mixture Model based mapping, and applied in the decoding stage under the observation uncertainty framework. We propose six heuristics, and evaluate them using both artificial and real-world noisy data, and with acoustic models trained on clean speech, a multi-condition noisy data set, and the multi-condition set processed with the source separation front-end. Taking the uncertainty of the enhanced features into account is shown to improve recognition performance when the acoustic models are trained on unenhanced data, while training on enhanced noisy data yields the lowest error rates.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {368–380},
numpages = {13}
}

@article{10.1109/TASL.2013.2283105,
author = {van Hengel, P. W. J. and Krijnders, J. D.},
title = {A Comparison of Spectro-Temporal Representations of Audio Signals},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASL.2013.2283105},
doi = {10.1109/TASL.2013.2283105},
abstract = {This article compares methods for the conversion of timeseries into a spectro-temporal representation. These methods are designed based on a resemblance with the auditory processing of sound in the mammalian inner ear, or on mathematical principles related to, for example, Fourier analysis. This study provides a comparison between several of these methods. Two tests were devised for this comparison: one based on susceptibility to noise and one on the expression of spectro-temporal detail. These two aspects were considered of importance for real world applications. While some methods produced good results on one of the two tests, others produced good results on both. Overall the transmission line model using an impedance function suggested by Zweig (“Finding the impedance of the organ of Corti,” J. Acoust. Soc. Amer., vol. 89, no. 3, pp. 1229-1254, 1991) provided the best results, though not significantly. Also a larger computational load may hinder application in some domains. The gammatone filterbank and straightforward spectrogram provide good alternatives with less computational load. The introduction of nonlinearity was shown to deteriorate performance on both tests, in both the filterbank and in the transmission line model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {303–313},
numpages = {11}
}

@article{10.1109/TASLP.2013.2287056,
author = {Molina, Emilio and Barbancho, Ana M. and Tardon, Lorenzo J. and Barbancho, Isabel},
title = {Dissonance Reduction In Polyphonic Audio Using Harmonic Reorganization},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2287056},
doi = {10.1109/TASLP.2013.2287056},
abstract = {In this paper, a method for automatic reduction of dissonance in recorded isolated chords is proposed. Previous approaches address this problem using source separation and note-level processing. In our approach, we manipulate the harmonic structure as a whole in order to avoid beating partials which, according to prior research on dissonance perception, typically produce an unpleasant sound. The proposed system firstly performs a sinusoidal plus residual modeling of the input and analyses the various fundamental frequencies present in the chord. This information is used to create a symbolic representation of the in-tune version of the input according to some musical rules. Then, the partials of the signals are shifted in order to fit the in-tune harmonic structure of the input chord. The input is assumed to contain one isolated chord, with relatively stable fundamental frequencies belonging to the Western chromatic scale. The evaluation has been performed by 31 expert musicians, which have quantified the perceived consonance of six varied, out-of-tune chords in three variants: unprocessed, processed with our system and processed by a state-of-the-art commercial tool (Melodyne Editor). The proposed approach attains an important reduction of the perceived dissonance, showing better performance than Melodyne Editor for most of the cases evaluated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {325–334},
numpages = {10}
}

@article{10.1109/TASLP.2013.2290502,
author = {Cosentino, Stefano and Falk, Tiago H. and McAlpine, David and Marquardt, Torsten},
title = {Cochlear Implant Filterbank Design and Optimization: A Simulation Study},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2290502},
doi = {10.1109/TASLP.2013.2290502},
abstract = {Cochlear implants (CIs) are devices capable of restoring hearing function in profoundly-deaf patients to an acceptable degree of performance. An essential processing step in any cochlear implant is frequency analysis, which is usually performed via banks of filters. Here, we simulate and test the suitability of different filters and filterbank architectures for CIs with respect to their performance in speech intelligibility. Four different filters were implemented in an established model of CI hearing, the tone-excited vocoder, namely: GTF (Gammatone Filter), DAPGF (Differentiated All-Pole GTF), OZGF (One-Zero GTF) and BUTF (Butterworth). Three filterbank parameters, the filter order ( N), the filter quality factor ( Q) and the number of channels ( Ch), and their combinations were tested using objective and subjective metrics. Simulation results show that all filters tested are suitable for CI implementation, but that the choice of Q and N parameter values is crucial. For most conditions, optimal ( N,Q) combinations were within few units away from the combination (2, 4).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {347–353},
numpages = {7}
}

@article{10.1109/TASLP.2013.2294582,
author = {Otsuka, Takuma and Ishiguro, Katsuhiko and Sawada, Hiroshi and Okuno, Hiroshi G.},
title = {Bayesian Nonparametrics for Microphone Array Processing},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2294582},
doi = {10.1109/TASLP.2013.2294582},
abstract = {Sound source localization and separation from a mixture of sounds are essential functions for computational auditory scene analysis. The main challenges are designing a unified framework for joint optimization and estimating the sound sources under auditory uncertainties such as reverberation or unknown number of sounds. Since sound source localization and separation are mutually dependent, their simultaneous estimation is required for better and more robust performance. A unified model is presented for sound source localization and separation based on Bayesian nonparametrics. Experiments using simulated and recorded audio mixtures show that a method based on this model achieves state-of-the-art sound source separation quality and has more robust performance on the source number estimation under reverberant environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {493–504},
numpages = {12}
}

@article{10.1109/TASLP.2013.2287055,
author = {Zitouni, Imed and Benajiba, Yassine},
title = {Aligned-Parallel-Corpora Based Semi-Supervised Learning for Arabic Mention Detection},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2287055},
doi = {10.1109/TASLP.2013.2287055},
abstract = {In the last two decades, significant effort has been put into annotating linguistic resources in several languages. Despite this valiant effort, there are still many languages left that have only small amounts of such resources. The goal of this article is to present and investigate a method of propagating information (specifically mentions) from a resource-rich language such as English into a relatively less-resource language such as Arabic. We compare also this approach to its equivalent counterpart using monolingual resources. Part of the investigation is to quantify the contribution of propagating information in different conditions - based on the availability of resources in the target language. Experiments on the language pair Arabic-English show that one can achieve relatively decent performance by propagating information from a language with richer resources such as English into Arabic alone (no resources or models in the source language Arabic). Furthermore, results show that propagated features from English do help improve the Arabic system performance even when used in conjunction with all feature types built from the source language. Experiments also show that using propagated features in conjunction with lexically-derived features only (as can be obtained directly from a mention annotated corpus) brings the system performance at the one obtained in the target language by using feature derived from many linguistic resources, therefore improving the system when such resources are not available.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {314–324},
numpages = {11}
}

@article{10.1109/TASLP.2013.2295925,
author = {Primavera, Andrea and Cecchi, Stefania and Junfeng Li and Piazza, Francesco},
title = {Objective and Subjective Investigation on a Novel Method for Digital Reverberator Parameters Estimation},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2295925},
doi = {10.1109/TASLP.2013.2295925},
abstract = {Reverberation is a well known effect that has an important role in our listening experience. A great deal of research has been devoted in the last decades aiming to artificially reproduce the reverberation effect exploiting a hybrid reverberation structure. In this context, several automatic procedures have been presented in the literature in order to derive the reverberator structure considering the mixing time evaluation and the minimization functions definition for the late reverberation device. Taking into consideration these aspects, a deep analysis of hybrid digital reverberator audio quality is here proposed, introducing a new parameter for the definition of the mixing time and two new cost functions for the definition of the late reverberation parameters. More in detail, starting from the considerations derived from a previous accurate approach based on the mel frequency cepstral coefficients, the new cost functions are based on the evaluation of the perceptual linear predictive and power normalized cepstral coefficients. Several results are reported, in terms of objective measure, performance analysis and subjective measures, taking into consideration different real impulse responses and various input stimuli and making a comparison with the state of the art. In particular, the obtained results show that a good accuracy can be achieved also considering a low number of coefficients, therefore improving the computational performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {441–452},
numpages = {12}
}

@article{10.1109/TASLP.2013.2297015,
author = {Jianjun He and Ee-Leng Tan and Woon-Seng Gan},
title = {Linear Estimation Based Primary-Ambient Extraction for Stereo Audio Signals},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2297015},
doi = {10.1109/TASLP.2013.2297015},
abstract = {Audio signals for moving pictures and video games are often linear combinations of primary and ambient components. In spatial audio analysis-synthesis, these mixed signals are usually decomposed into primary and ambient components to facilitate flexible spatial rendering and enhancement. Existing approaches such as principal component analysis (PCA) and least squares (LS) are widely used to perform this decomposition from stereo signals. However, the performance of these approaches in primary-ambient extraction (PAE) has not been well studied and no comparative analysis among the existing approaches has been carried out so far. In this paper, we generalize the existing approaches into a linear estimation framework. Under this framework, we propose a series of performance measures to identify the components that contribute to the extraction error. Based on the generalized linear estimation framework and our proposed performance measures, a comparative study and experimental testing of the linear estimation based PAE approaches including existing PCA, LS, and three proposed variant LS approaches are presented.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {505–517},
numpages = {13}
}

@article{10.1109/TASLP.2013.2294579,
author = {Speed, Matt and Murphy, Damian and Howard, David},
title = {Modeling the Vocal Tract Transfer Function Using a 3D Digital Waveguide Mesh},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2294579},
doi = {10.1109/TASLP.2013.2294579},
abstract = {The digital waveguide mesh has been shown to be capable of reproducing the acoustic impulse response of cylindrical vocal tract analogs. This study extends the same methodology to three-dimensional simulation of the acoustic response of graphical models of the vocal tract obtained from magnetic resonance imaging for a group of trained subjects. By such simulation of the vocal tract transfer function and convolution with an appropriate source waveform, basic phonemes are resynthesized and compared with benchmark audio recordings. The technologies and techniques used for simulation are described, alongside the protocol for image capture and the process for collection of benchmark audio. The results of simulation and acoustic recording are then evaluated and compared. The value of three-dimensional simulation in comparison to existing lower-dimensionality equivalents is assessed. It is found that while three-dimensional simulation provides a strong representation of the low frequency vocal tract transfer function, at higher frequencies its performance becomes geometry-dependent. MRI imaging and benchmark audio is provided for future studies and to permit comparison with comparable means of acoustic simulation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {453–464},
numpages = {12}
}

@article{10.1109/TASLP.2013.2295914,
author = {Jensen, Jesper and Taal, Cees H.},
title = {Speech Intelligibility Prediction Based on Mutual Information},
year = {2014},
issue_date = {February 2014},
publisher = {IEEE Press},
volume = {22},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2295914},
doi = {10.1109/TASLP.2013.2295914},
abstract = {This paper deals with the problem of predicting the average intelligibility of noisy and potentially processed speech signals, as observed by a group of normal hearing listeners. We propose a model which performs this prediction based on the hypothesis that intelligibility is monotonically related to the mutual information between critical-band amplitude envelopes of the clean signal and the corresponding noisy/processed signal. The resulting intelligibility predictor turns out to be a simple function of the mean-square error (mse) that arises when estimating a clean critical-band amplitude using a minimum mean-square error (mmse) estimator based on the noisy/processed amplitude. The proposed model predicts that speech intelligibility cannot be improved by any processing of noisy critical-band amplitudes. Furthermore, the proposed intelligibility predictor performs well ( ρ &gt; 0.95) in predicting the intelligibility of speech signals contaminated by additive noise and potentially non-linearly processed using time-frequency weighting.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {430–440},
numpages = {11}
}

@article{10.1109/TASLP.2013.2290172,
author = {Deng, Li and Renals, Steve and Federico, Marcello and Ostendorf, Mari},
title = {Editorial: Expanding the Technical Reach of Our Transactions},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2290172},
doi = {10.1109/TASLP.2013.2290172},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {5},
numpages = {1}
}

@article{10.1109/TASLP.2013.2285469,
author = {Hung-Yi Lee and Lin-Shan Lee},
title = {Improved Semantic Retrieval of Spoken Content by Document/Query Expansion with Random Walk Over Acoustic Similarity Graphs},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2285469},
doi = {10.1109/TASLP.2013.2285469},
abstract = {In a text context, document/query expansion has proven very useful in retrieving objects semantically related to the query. However, when applying text-based techniques on spoken content, the inevitable recognition errors seriously degrade performance even when the retrieval process is performed over lattices. We propose the estimation of more accurate term distributions (or unigram language models) for the spoken documents by acoustic similarity graphs. In this approach, a graph is constructed for each term describing the acoustic similarity among all signal regions hypothesized to be the considered term. Score propagation based on a random walk over the graph offers more reliable scores of the term hypotheses, which in turn yield more accurate term distributions (or unigram language models). This approach was applied with the language modeling retrieval approach, including using document expansion based on latent topic analysis and query expansion with a query-regularized mixture model. We extend these approaches from words to subword n-grams, and the query expansion from document-level to utterance-level and from term-based to topic-based. Experiments performed on Mandarin broadcast news showed improved performance under almost all tested conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {80–94},
numpages = {15}
}

@article{10.1109/TASL.2013.2281575,
author = {Liang Lu and Ghoshal, Arnab and Renals, Steve},
title = {Cross-Lingual Subspace Gaussian Mixture Models for Low-Resource Speech Recognition},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASL.2013.2281575},
doi = {10.1109/TASL.2013.2281575},
abstract = {This paper studies cross-lingual acoustic modeling in the context of subspace Gaussian mixture models (SGMMs). SGMMs factorize the acoustic model parameters into a set that is globally shared between all the states of a hidden Markov model (HMM) and another that is specific to the HMM states. We demonstrate that the SGMM global parameters are transferable between languages, particularly when the parameters are trained multilingually. As a result, acoustic models may be trained using limited amounts of transcribed audio by borrowing the SGMM global parameters from one or more source languages, and only training the state-specific parameters on the target language audio. Model regularization using ℓ1-norm penalty is shown to be particularly effective at avoiding overtraining and leading to lower word error rates. We investigate maximum a posteriori (MAP) adaptation of subspace parameters in order to reduce the mismatch between the SGMM global parameters of the source and target languages. In addition, monolingual and cross-lingual speaker adaptive training is used to reduce the model variance introduced by speakers. We have systematically evaluated these techniques by experiments on the GlobalPhone corpus.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {17–27},
numpages = {11}
}

@article{10.1109/TASLP.2013.2285480,
author = {Leutnant, Volker and Krueger, Alexander and Haeb-Umbach, Reinhold},
title = {A New Observation Model in the Logarithmic Mel Power Spectral Domain for the Automatic Recognition of Noisy Reverberant Speech},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2285480},
doi = {10.1109/TASLP.2013.2285480},
abstract = {In this contribution we present a theoretical and experimental investigation into the effects of reverberation and noise on features in the logarithmic mel power spectral domain, an intermediate stage in the computation of the mel frequency cepstral coefficients, prevalent in automatic speech recognition (ASR). Gaining insight into the complex interaction between clean speech, noise, and noisy reverberant speech features is essential for any ASR system to be robust against noise and reverberation present in distant microphone input signals. The findings are gathered in a probabilistic formulation of an observation model which may be used in model-based feature compensation schemes. The proposed observation model extends previous models in three major directions: First, the contribution of additive background noise to the observation error is explicitly taken into account. Second, an energy compensation constant is introduced which ensures an unbiased estimate of the reverberant speech features, and, third, a recursive variant of the observation model is developed resulting in reduced computational complexity when used in model-based feature compensation. The experimental section is used to evaluate the accuracy of the model and to describe how its parameters can be determined from test data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {95–109},
numpages = {15}
}

@article{10.1109/TASLP.2013.2290499,
author = {Shabtai, Noam R. and Rafaely, Boaz},
title = {Generalized Spherical Array Beamforming for Binaural Speech Reproduction},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2290499},
doi = {10.1109/TASLP.2013.2290499},
abstract = {Microphone arrays are used in speech signal processing applications such as teleconferencing and telepresence, in order to enhance a desired speech signal in the presence of speech signals from other speakers, reverberation and background noise. These arrays usually provide a single-channel output, so that no spatial information is available in the output signal. However, spatial information on the sound sources may increase the intelligibility of a speech signal perceived by a human listener. This work presents a mathematical framework for generalized spherical array beamforming that in addition to suppressing noise and reverberation, is aiming to preserve spatial information on the sources in the recording venue. The generalized beamforming, formulated in the spherical harmonics domain, is based on binaural sound reproduction where the head-related transfer functions are incorporated into a headphones presentation. The performance of the proposed generalized beamformer is compared to that of a single-channel output maximum-directivity beamformer. Listening tests with human subjects show that when the generalized beamformer is used the intelligibility is improved at low input SNRs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {238–247},
numpages = {10}
}

@article{10.1109/TASL.2013.2283104,
author = {Chao Pan and Jingdong Chen and Benesty, Jacob},
title = {Performance Study of the MVDR Beamformer as a Function of the Source Incidence Angle},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASL.2013.2283104},
doi = {10.1109/TASL.2013.2283104},
abstract = {Linear microphone arrays combined with the minimum variance distortionless response (MVDR) beamformer have been widely studied in various applications to acquire desired signals and reduce the unwanted noise. Most of the existing array systems assume that the desired sources are in the broadside direction. In this paper, we study and analyze the performance of the MVDR beamformer as a function of the source incidence angle. Using the signal-to-noise ratio (SNR) and beampattern as the criteria, we investigate its performance in four different scenarios: spatially white noise, diffuse noise, diffuse-plus-white noise, and point-source-plus-white noise. The results demonstrate that the optimal performance of the MVDR beamformer occurs when the source is in the endfire directions for diffuse noise and point-source noise while its SNR gain does not depend on the signal incidence angle in spatially white noise. This indicates that most current systems may not fully exploit the potential of the MVDR beamformer. This analysis does not only help us better understand this algorithm, but also helps us design better array systems for practical applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {67–79},
numpages = {13}
}

@article{10.1109/TASL.2013.2283100,
author = {Crespo, Joao B. and Hendriks, Richard C.},
title = {Multizone Speech Reinforcement},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASL.2013.2283100},
doi = {10.1109/TASL.2013.2283100},
abstract = {In this article, we address speech reinforcement (near-end listening enhancement) for a scenario where there are several playback zones. In such a framework, signals from one zone can leak into other zones (crosstalk), causing intelligibility and/or quality degradation. An optimization framework is built by exploring a signal model where effects of noise, reverberation and zone crosstalk are taken into account simultaneously. Through the symbolic usage of a general smooth distortion measure, necessary optimality conditions are derived in terms of distortion measure gradients and the signal model. Subsequently, as an illustrative example of the framework, the conditions are applied for the mean-square error (MSE) expected distortion under a hybrid stochastic-deterministic model for the corruptions. A crosstalk cancellation algorithm follows, which depends on diffuse reverberation and across zone direct path components. Simulations validate the optimality of the algorithm and show a clear benefit in multizone processing, as opposed to the iterated application of a single-zone algorithm. Also, comparisons with least-squares crosstalk cancellers in literature show the profit of using a hybrid model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {54–66},
numpages = {13}
}

@article{10.1109/TASLP.2013.2290505,
author = {Cumani, Sandro and Laface, Pietro},
title = {Factorized Sub-Space Estimation for Fast and Memory Effective I-Vector Extraction},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2290505},
doi = {10.1109/TASLP.2013.2290505},
abstract = {Most of the state-of-the-art speaker recognition systems use a compact representation of spoken utterances referred to as i-vector. Since the “standard” i-vector extraction procedure requires large memory structures and is relatively slow, new approaches have recently been proposed that are able to obtain either accurate solutions at the expense of an increase of the computational load, or fast approximate solutions, which are traded for lower memory costs. We propose a new approach particularly useful for applications that need to minimize their memory requirements. Our solution not only dramatically reduces the memory needs for i-vector extraction, but is also fast and accurate compared to recently proposed approaches. Tested on the female part of the tel-tel extended NIST 2010 evaluation trials, our approach substantially improves the performance with respect to the fastest but inaccurate eigen-decomposition approach, using much less memory than other methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {248–259},
numpages = {12}
}

@article{10.1109/TASLP.2013.2285482,
author = {Chen, Nancy F. and Tam, Sharon W. and Wade Shen and Campbell, Joseph P.},
title = {Characterizing Phonetic Transformations and Acoustic Differences Across English Dialects},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2285482},
doi = {10.1109/TASLP.2013.2285482},
abstract = {In this work, we propose a framework that automatically discovers dialect-specific phonetic rules. These rules characterize when certain phonetic or acoustic transformations occur across dialects. To explicitly characterize these dialect-specific rules, we adapt the conventional hidden Markov model to handle insertion and deletion transformations. The proposed framework is able to convert pronunciation of one dialect to another using learned rules, recognize dialects using learned rules, retrieve dialect-specific regions, and refine linguistic rules. Potential applications of our proposed framework include computer-assisted language learning, sociolinguistics, and diagnosis tools for phonological disorders.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {110–124},
numpages = {15}
}

@article{10.1109/TASLP.2013.2288081,
author = {Zhenghua Li and Min Zhang and Wanxiang Che and Ting Liu and Wenliang Chen},
title = {Joint Optimization for Chinese POS Tagging and Dependency Parsing},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2288081},
doi = {10.1109/TASLP.2013.2288081},
abstract = {Dependency parsing has gained more and more interest in natural language processing in recent years due to its simplicity and general applicability for diverse languages. Previous work demonstrates that part-of-speech (POS) is an indispensable feature in dependency parsing since pure lexical features suffer from serious data sparseness problem. However, due to little morphological changes, Chinese POS tagging has proven to be much more challenging than morphology-richer languages such as English (94% vs. 97% on POS tagging accuracy). This leads to severe error propagation for Chinese dependency parsing. Our experiments show that parsing accuracy drops by about 6% when replacing manual POS tags of the input sentence with automatic ones generated by a state-of-the-art statistical POS tagger. To address this issue, this paper proposes a solution by jointly optimizing POS tagging and dependency parsing in a unique model. We propose for our joint models several dynamic programming based decoding algorithms which can incorporate rich POS tagging and syntactic features. Then we present an effective pruning strategy to reduce the search space of candidate POS tags, leading to significant improvement of parsing speed. Experimental results on two Chinese data sets, i.e. Penn Chinese Treebank 5.1 and Penn Chinese Treebank 7, demonstrate that our joint models significantly improve both the state-of-the-art tagging and parsing accuracies. Detailed analysis shows that the joint method can help resolve syntax-sensitive POS ambiguities {ssrNN,ssrVV}. In return, the POS tags become more reliable and helpful for parsing since the syntactic features are used in POS tagging. This is the fundamental reason for the performance improvement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {274–286},
numpages = {13}
}

@article{10.1109/TASLP.2013.2286906,
author = {Tomar, Vikrant Singh and Rose, Richard C.},
title = {A Family of Discriminative Manifold Learning Algorithms and Their Application to Speech Recognition},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2286906},
doi = {10.1109/TASLP.2013.2286906},
abstract = {This paper presents a family of discriminative manifold learning approaches to feature space dimensionality reduction in noise robust automatic speech recognition (ASR). The specific goal of these techniques is to preserve local manifold structure in feature space while at the same time maximizing the separability between classes of feature vectors. In the manifold space, the relationships among the feature vectors are defined using nonlinear kernels. Two separate distance measures are used to characterize the kernels, namely the conventional Euclidean distance and a cosine-correlation based distance. The performance of the proposed techniques is evaluated on two task domains involving noise corrupted utterances of connected digits and read newspaper text. Performance is compared to existing approaches used for feature space transformations, including linear discriminant analysis (LDA) and locality preserving linear projections (LPP). The proposed approaches are found to provide a significant reduction in word error rate (WER) with respect to the more well-known techniques for a variety of noise conditions. Another contribution of the paper is to quantify the interaction between acoustic noise conditions and the shape and size of local neighborhoods which are used in manifold learning to define local relationships among feature vectors. Based on this analysis, a procedure for reducing the impact of varying acoustic conditions on manifold learning is proposed .},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {161–171},
numpages = {11}
}

@article{10.1109/TASL.2013.2281574,
author = {Taghia, Jalal and Martin, Rainer},
title = {Objective Intelligibility Measures Based on Mutual Information for Speech Subjected to Speech Enhancement Processing},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASL.2013.2281574},
doi = {10.1109/TASL.2013.2281574},
abstract = {We propose a novel method for objective speech intelligibility prediction which can be useful in many application domains such as hearing instruments and forensics. Most objective intelligibility measures available in the literature employ some kind of signal-to-noise ratio (SNR) or a correlation-based comparison between the spectro-temporal representations of clean and processed speech. In this paper, we investigate the speech intelligibility prediction from the viewpoint of information theory and introduce novel objective intelligibility measures based on the estimated mutual information between the temporal envelopes of clean speech and processed speech in the subband domain. Mutual information allows to account for higher order statistics and hence to consider dependencies beyond the conventional second order statistics. Using data from three different listening tests it is shown that the proposed objective intelligibility measures provide promising results for speech intelligibility prediction in different scenarios of speech enhancement where speech is processed by non-linear modification strategies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {6–16},
numpages = {11}
}

@article{10.1109/TASLP.2013.2285484,
author = {Zhiyao Duan and Jinyu Han and Pardo, Bryan},
title = {Multi-Pitch Streaming of Harmonic Sound Mixtures},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2285484},
doi = {10.1109/TASLP.2013.2285484},
abstract = {Multi-pitch analysis of concurrent sound sources is an important but challenging problem. It requires estimating pitch values of all harmonic sources in individual frames and streaming the pitch estimates into trajectories, each of which corresponds to a source. We address the streaming problem for monophonic sound sources. We take the original audio, plus frame-level pitch estimates from any multi-pitch estimation algorithm as inputs, and output a pitch trajectory for each source. Our approach does not require pre-training of source models from isolated recordings. Instead, it casts the problem as a constrained clustering problem, where each cluster corresponds to a source. The clustering objective is to minimize the timbre inconsistency within each cluster. We explore different timbre features for music and speech. For music, harmonic structure and a newly proposed feature called uniform discrete cepstrum (UDC) are found effective; while for speech, MFCC and UDC works well. We also show that timbre-consistency is insufficient for effective streaming. Constraints are imposed on pairs of pitch estimates according to their time-frequency relationships. We propose a new constrained clustering algorithm that satisfies as many constraints as possible while optimizing the clustering objective. We compare the proposed approach with other state-of-the-art supervised and unsupervised multi-pitch streaming approaches that are specifically designed for music or speech. Better or comparable results are shown.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {138–150},
numpages = {13}
}

@article{10.1109/TASLP.2013.2285474,
author = {Senoussaoui, Mohammed and Kenny, Patrick and Stafylakis, Themos and Dumouchel, Pierre},
title = {A Study of the Cosine Distance-Based Mean Shift for Telephone Speech Diarization},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2285474},
doi = {10.1109/TASLP.2013.2285474},
abstract = {Speaker clustering is a crucial step for speaker diarization. The short duration of speech segments in telephone speech dialogue and the absence of prior information on the number of clusters dramatically increase the difficulty of this problem in diarizing spontaneous telephone speech conversations. We propose a simple iterative Mean Shift algorithm based on the cosine distance to perform speaker clustering under these conditions. Two variants of the cosine distance Mean Shift are compared in an exhaustive practical study. We report state of the art results as measured by the Diarization Error Rate and the Number of Detected Speakers on the LDC CallHome telephone corpus.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {217–227},
numpages = {11}
}

@article{10.1109/TASL.2013.2282190,
author = {Gasic, Milica and Young, Steve},
title = {Gaussian Processes for POMDP-Based Dialogue Manager Optimization},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASL.2013.2282190},
doi = {10.1109/TASL.2013.2282190},
abstract = {A partially observable Markov decision process (POMDP) has been proposed as a dialog model that enables automatic optimization of the dialog policy and provides robustness to speech understanding errors. Various approximations allow such a model to be used for building real-world dialog systems. However, they require a large number of dialogs to train the dialog policy and hence they typically rely on the availability of a user simulator. They also require significant designer effort to hand-craft the policy representation. We investigate the use of Gaussian processes (GPs) in policy modeling to overcome these problems. We show that GP policy optimization can be implemented for a real world POMDP dialog manager, and in particular: 1) we examine different formulations of a GP policy to minimize variability in the learning process; 2) we find that the use of GP increases the learning rate by an order of magnitude thereby allowing learning by direct interaction with human users; and 3) we demonstrate that designer effort can be substantially reduced by basing the policy directly on the full belief space thereby avoiding ad hoc feature space modeling. Overall, the GP approach represents an important step forward towards fully automatic dialog policy optimization in real world systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {28–40},
numpages = {13}
}

@article{10.1109/TASLP.2013.2286919,
author = {Arisoy, Ebru and Chen, Stanley F. and Ramabhadran, Bhuvana and Sethy, Abhinav},
title = {Converting Neural Network Language Models into Back-off Language Models for Efficient Decoding in Automatic Speech Recognition},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2286919},
doi = {10.1109/TASLP.2013.2286919},
abstract = {Neural network language models (NNLMs) have achieved very good performance in large-vocabulary continuous speech recognition (LVCSR) systems. Because decoding with NNLMs is computationally expensive, there is interest in developing methods to approximate NNLMs with simpler language models that are suitable for fast decoding. In this work, we propose an approximate method for converting a feedforward NNLM into a back-off n-gram language model that can be used directly in existing LVCSR decoders. We convert NNLMs of increasing order to pruned back-off language models, using lower-order models to constrain the n-grams allowed in higher-order models. In experiments on Broadcast News data, we find that the resulting back-off models retain the bulk of the gain achieved by NNLMs over conventional n-gram language models, and give accuracy improvements as compared to existing methods for converting NNLMs to back-off models. In addition, the proposed approach can be applied to any type of non-back-off language model to enable efficient decoding.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {184–192},
numpages = {9}
}

@article{10.1109/TASLP.2013.2286917,
author = {Doi, Hironori and Toda, Tomoki and Nakamura, Keigo and Saruwatari, Hiroshi and Shikano, Kiyohiro},
title = {Alaryngeal Speech Enhancement Based on One-to-Many Eigenvoice Conversion},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2286917},
doi = {10.1109/TASLP.2013.2286917},
abstract = {In this paper, we present novel speaking-aid systems based on one-to-many eigenvoice conversion (EVC) to enhance three types of alaryngeal speech: esophageal speech, electrolaryngeal speech, and body-conducted silent electrolaryngeal speech. Although alaryngeal speech allows laryngectomees to utter speech sounds, it suffers from the lack of speech quality and speaker individuality. To improve the speech quality of alaryngeal speech, alaryngeal-speech-to-speech (AL-to-Speech) methods based on statistical voice conversion have been proposed. In this paper, one-to-many EVC capable of flexibly controlling the converted voice quality by adapting the conversion model to given target natural voices is further implemented for the AL-to-Speech methods to effectively recover speaker individuality of each type of alaryngeal speech. These proposed systems are compared with each other from various perspectives. The experimental results demonstrate that our proposed systems are capable of effectively addressing the issues of alaryngeal speech, e.g., yielding significant improvements in speech quality of each type of alaryngeal speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {172–183},
numpages = {12}
}

@article{10.1109/TASL.2013.2282214,
author = {Mezghani-Marrakchi, Imen and Mahe, Gael and Djaziri-Larbi, Sonia and Jaidane, Meriem and Turki-Hadj Alouane, Monia},
title = {Nonlinear Audio Systems Identification Through Audio Input Gaussianization},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASL.2013.2282214},
doi = {10.1109/TASL.2013.2282214},
abstract = {Nonlinear audio system identification generally relies on Gaussianity, whiteness and stationarity hypothesis on the input signal, although audio signals are non-Gaussian, highly correlated and non-stationary. However, since the physical behavior of nonlinear audio systems is input-dependent, they should be identified using natural audio signals (speech or music) as input, instead of artificial signals (sweeps or noise) as usually done. We propose an identification scheme that conditions audio signals to fit the desired properties for an efficient identification. The identification system consists in (1) a Gaussianization step that makes the signal near-Gaussian under a perceptual constraint; (2) a predictor filterbank that whitens the signal; (3) an orthonormalization step that enhances the statistical properties of the input vector of the last step, under a Gaussianity hypothesis; (4) an adaptive nonlinear model. The proposed scheme enhances the convergence rate of the identification and reduces the steady state identification error, compared to other schemes, for example the classical adaptive nonlinear identification.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {41–53},
numpages = {13}
}

@article{10.1109/TASLP.2013.2287052,
author = {Tachibana, Hideyuki and Ono, Nobutaka and Sagayama, Shigeki},
title = {Singing Voice Enhancement in Monaural Music Signals Based on Two-Stage Harmonic/Percussive Sound Separation on Multiple Resolution Spectrograms},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2287052},
doi = {10.1109/TASLP.2013.2287052},
abstract = {We propose a novel singing voice enhancement technique for monaural music audio signals, which is a quite challenging problem. Many singing voice enhancement techniques have been proposed recently. However, our approach is based on a quite different idea from these existing methods. We focused on the fluctuation of a singing voice and considered to detect it by exploiting two differently resolved spectrograms, one has rich temporal resolution and poor frequency resolution, while the other has rich frequency resolution and poor temporal resolution. On such two spectrograms, the shapes of fluctuating components are quite different. Based on this idea, we propose a singing voice enhancement technique that we call two-stage harmonic/percussive sound separation (HPSS). In this paper, we describe the details of two-stage HPSS and evaluate the performance of the method. The experimental results show that SDR, a commonly-used criterion on the task, was improved by around 4 dB, which is a considerably higher level than existing methods. In addition, we also evaluated the performance of the method as a preprocessing for melody estimation in music. The experimental results show that our singing voice enhancement technique considerably improved the performance of a simple pitch estimation technique. These results prove the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {228–237},
numpages = {10}
}

@article{10.1109/TASLP.2013.2285487,
author = {Shilin Liu and Khe Chai Sim},
title = {Temporally Varying Weight Regression: A Semi-Parametric Trajectory Model for Automatic Speech Recognition},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2285487},
doi = {10.1109/TASLP.2013.2285487},
abstract = {Standard Hidden Markov Model (HMM) assumes that successive observations are independent to one another given the state sequence. This leads to a poor trajectory model for speech. Many explicit trajectory modeling techniques have been studied in the past to improve trajectory modeling for HMM. However, these techniques do not yield promising improvements over conventional HMM systems where differential parameters and Gaussian Mixture Model have been used implicitly to circumvent the poor trajectory modeling issue of HMM. Recently, semi-parametric trajectory modeling techniques based on temporally varying model parameters such as fMPE and pMPE have been shown to yield promising improvements over state-of-the-art systems on large vocabulary continuous speech recognition tasks. These techniques use high dimensional posterior features derived from a long span of acoustic features to model temporally varying attributes of the speech signal. Bases corresponding to these posterior features are then discriminatively estimated to yield temporally varying mean (fMPE) and precision matrix (pMPE) parameters. Motivated by the success of fMPE and pMPE, Temporally Varying Weight Regression (TVWR) was recently proposed to model HMM trajectory implicitly using time-varying Gaussian weights. In this paper, a complete formulation of TVWR is given based on a probabilistic modeling framework. Parameter estimation formulae using both maximum likelihood (ML) and minimum phone error (MPE) criteria are derived. Experimental results based on the Wall Street Journal ( CSR-WSJ0 + WSJ1) and Aurora 4 corpora show that consistent promising improvements over the standard HMM systems can be obtained in both the 20 k open vocabulary recognition task (NIST Nov'92 WSJ0) and 5 k closed vocabulary noisy speech recognition for both ML and MPE criteria.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {151–160},
numpages = {10}
}

@article{10.1109/TASLP.2013.2286920,
author = {Jin, Craig T. and Epain, Nicolas and Parthy, Abhaya},
title = {Design, Optimization and Evaluation of a Dual-Radius Spherical Microphone Array},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2286920},
doi = {10.1109/TASLP.2013.2286920},
abstract = {Spherical Microphone Arrays (SMAs) constitute a powerful tool for analyzing the spatial properties of sound fields. However, the performance of SMA-based signal processing algorithms ultimately depends on the physical characteristics of the array. In particular, the range of frequencies over which an SMA provide rich spatial information is conditioned by the size of the array, the angular position of the sensors and other factors. In this work, we investigate the design of SMAs offering a wider frequency range of operation than that offered by conventional designs. To achieve this goal, microphones are distributed both on and at a distance from the surface of a rigid spherical baffle. The contributions of the paper are as follows. First, we present a general framework for modeling SMAs whose sensors are located at different distances from the array center and calculating optimal filters for the decomposition of the sound field into spherical harmonic modes. Second, we present an optimization method to design multi-radius SMAs with an optimally wide frequency range of operation given the total number of sensors available and target spatial resolution. Lastly, based on the optimization results, we built a prototype dual-radius SMA with 64 microphones. We present measurement results for the prototype microphone array and compare these results with theory.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {193–204},
numpages = {12}
}

@article{10.1109/TASLP.2013.2286922,
author = {Mignot, Remi and Chardon, Gilles and Daudet, Laurent},
title = {Low Frequency Interpolation of Room Impulse Responses Using Compressed Sensing},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2286922},
doi = {10.1109/TASLP.2013.2286922},
abstract = {Measuring the Room Impulse Responses within a finite 3D spatial domain can require a very large number of measurements with standard uniform sampling. In this paper, we show that, at low frequencies, this sampling can be done with significantly less measurements, using some modal properties of the room. At a given temporal frequency, a plane wave approximation of the acoustic field leads to a sparse approximation, and therefore a compressed sensing framework can be used for its acquisition. This paper describes three different sparse models that can be constructed, and the corresponding estimation algorithms: two models that exploit the structured sparsity of the soundfield, with projections of the modes onto plane waves sharing the same wavenumber, and one that computes a sparse decomposition on a dictionary of independent plane waves with time / space variable separation. These models are compared numerically and experimentally, with an array of 120 microphones irregularly placed within a 2 \texttimes{}2 \texttimes{}2 m volume inside a room, with an approximate uniform distribution. One of the most challenging part is the design of estimation algorithms whose computational complexity remains tractable.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {205–216},
numpages = {12}
}

@article{10.1109/TASLP.2013.2285483,
author = {Markovic, D. and Kowalczyk, K. and Antonacci, F. and Hofmann, C. and Sarti, A. and Kellermann, W.},
title = {Estimation of Acoustic Reflection Coefficients Through Pseudospectrum Matching},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2285483},
doi = {10.1109/TASLP.2013.2285483},
abstract = {Estimating the geometric and reflective properties of the environment is important for a wide range of applications of space-time audio processing, from acoustic scene analysis to room equalization and spatial audio rendering. In this manuscript, we propose a methodology for frequency-subband in-situ estimation of the reflection coefficients of planar surfaces. This is a rather challenging task, as the reflection coefficients depend on the frequency and the angle of incidence and their estimate is highly sensitive to background noise and interfering sources. Our method is based on the assumption that we know the geometry of the reflectors; the position and the radiation pattern of the source; the position and the spatial response of the array. Applying beamforming algorithms on a single set of measured sensor data, we estimate the angular distribution of the acoustic energy (angular pseudospectrum) that impinges on a microphone array. We then apply a two-step iterative estimation technique based on an Expectation-Maximization (EM) algorithm. The first step estimates the scaling factors. The second one infers the reflection coefficients from the scaling factors. Under the assumption of additive white Gaussian noise, we finally determine the reflection coefficients with a Maximum Likelihood (ML) estimation method. The effectiveness and the accuracy of the proposed technique are assessed through experiments based on measured data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {125–137},
numpages = {13}
}

@article{10.1109/TASLP.2013.2290861,
author = {Yuan Zeng and Hendriks, Richard C.},
title = {Distributed Delay and Sum Beamformer for Speech Enhancement via Randomized Gossip},
year = {2014},
issue_date = {January 2014},
publisher = {IEEE Press},
volume = {22},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2013.2290861},
doi = {10.1109/TASLP.2013.2290861},
abstract = {In this paper, we investigate the use of randomized gossip for distributed speech enhancement and present a distributed delay and sum beamformer (DDSB). In a randomly connected wireless acoustic sensor network, the DDSB estimates the desired signal at each node by communicating only with its neighbors. We first provide the asynchronous DDSB (ADDSB) where each pair of neighboring nodes updates its data asynchronously. Then, we introduce an improved general distributed synchronous averaging (IGDSA) algorithm, which can be used in any connected network, and combine that with the DDSB algorithm where multiple node pairs can update their estimates simultaneously. For convergence analysis, we first provide bounds for the worst case averaging time of the ADDSB for the best and worst connected networks, and then we compare the convergence rate of the ADDSB with the original synchronous DDSB (OSDDSB) and the improved synchronous DDSB (ISDDSB) in regular networks. This convergence rate comparison is extended to randomly connected non-regular networks using simulations. The simulation results show that the DDSB using the different updating schemes converges to the optimal estimates of the centralized beamformer and that the proposed IGDSA algorithm converges much faster than the original synchronous communication scheme, in particular for non-regular networks. Moreover, comparisons are performed with several existing distributed speech enhancement methods from literature, assuming that the steering vector is given. In the simulated scenario, the proposed method leads to a slight performance improvement at the expense of a higher communication cost. The presented method is not constrained to a certain network topology (e.g., tree connected or fully connected), while this is the case for many of the reference methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {260–273},
numpages = {14}
}

