@article{10.1145/3417334,
author = {Mcdonald, Graham and Macdonald, Craig and Ounis, Iadh},
title = {How the Accuracy and Confidence of Sensitivity Classification Affects Digital Sensitivity Review},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3417334},
doi = {10.1145/3417334},
abstract = {Government documents must be manually reviewed to identify any sensitive information, e.g., confidential information, before being publicly archived. However, human-only sensitivity review is not practical for born-digital documents due to, for example, the volume of documents that are to be reviewed. In this work, we conduct a user study to evaluate the effectiveness of sensitivity classification for assisting human sensitivity reviewers. We evaluate how the accuracy and confidence levels of sensitivity classification affects the number of documents that are correctly judged as being sensitive (reviewer accuracy) and the time that it takes to sensitivity review a document (reviewing speed). In our within-subject study, the participants review government documents to identify real sensitivities while being assisted by three sensitivity classification treatments, namely None (no classification predictions), Medium (sensitivity predictions from a simulated classifier with a balanced accuracy (BAC) of 0.7), and Perfect (sensitivity predictions from a classifier with an accuracy of 1.0). Our results show that sensitivity classification leads to significant improvements (ANOVA, p &lt; 0.05) in reviewer accuracy in terms of BAC (+37.9% Medium, +60.0% Perfect) and also in terms of F2 (+40.8% Medium, +44.9% Perfect). Moreover, we show that assisting reviewers with sensitivity classification predictions leads to significantly increased (ANOVA, p &lt; 0.05) mean reviewing speeds (+72.2% Medium, +61.6% Perfect). We find that reviewers do not agree with the classifier significantly more as the classifier’s confidence increases. However, reviewing speed is significantly increased when the reviewers agree with the classifier (ANOVA, p &lt; 0.05). Our in-depth analysis shows that when the reviewers are not assisted with sensitivity predictions, mean reviewing speeds are 40.5% slower for sensitive judgements compared to not-sensitive judgements. However, when the reviewers are assisted with sensitivity predictions, the difference in reviewing speeds between sensitive and not-sensitive judgements is reduced by ˜10%, from 40.5% to 30.8%. We also find that, for sensitive judgements, sensitivity classification predictions significantly increase mean reviewing speeds by 37.7% when the reviewers agree with the classifier’s predictions (t-test, p &lt; 0.05). Overall, our findings demonstrate that sensitivity classification is a viable technology for assisting human reviewers with the sensitivity review of digital documents.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {4},
numpages = {34},
keywords = {user study, freedom of information, document classification, Technology-assisted review}
}

@article{10.1145/3419972,
author = {Liu, Bulou and Li, Chenliang and Zhou, Wei and Ji, Feng and Duan, Yu and Chen, Haiqing},
title = {An Attention-Based Deep Relevance Model for Few-Shot Document Filtering},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3419972},
doi = {10.1145/3419972},
abstract = {With the large quantity of textual information produced on the Internet, a critical necessity is to filter out the irrelevant information and organize the rest into categories of interest (e.g., an emerging event). However, supervised-learning document filtering methods heavily rely on a large number of labeled documents for model training. Manually identifying plenty of positive examples for each category is expensive and time-consuming. Also, it is unrealistic to cover all the categories from an evolving text source that covers diverse kinds of events, user opinions, and daily life activities. In this article, we propose a novel attention-based deep relevance model for few-shot document filtering (named ADRM), inspired by the relevance feedback methodology proposed for ad hoc retrieval. ADRM calculates the relevance score between a document and a category by taking a set of seed words and a few seed documents relevant to the category. It constructs the category-specific conceptual representation of the document based on the corresponding seed words and seed documents. Specifically, to filter irrelevant yet noisy information in the seed documents, ADRM employs two types of attention mechanisms (namely whole-match attention and max-match attention) and generates category-specific representations for them. Then ADRM is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process, a self-attention layer, and a relevance aggregation layer. Extensive experiments on three real-world datasets show that ADRM consistently outperforms the existing technical alternatives, including the conventional classification and retrieval baselines, and the state-of-the-art deep relevance ranking models for few-shot document filtering. We also perform an ablation study to demonstrate that each component in ADRM is effective for enhancing filtering performance. Further analysis shows that ADRM is robust under varying parameter settings.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {6},
numpages = {35},
keywords = {document filtering, Few-shot learning, deep learning}
}

@article{10.1145/3411755,
author = {Li, Dan and Kanoulas, Evangelos},
title = {When to Stop Reviewing in Technology-Assisted Reviews: Sampling from an Adaptive Distribution to Estimate Residual Relevant Documents},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3411755},
doi = {10.1145/3411755},
abstract = {Technology-Assisted Reviews (TAR) aim to expedite document reviewing (e.g., medical articles or legal documents) by iteratively incorporating machine learning algorithms and human feedback on document relevance. Continuous Active Learning (CAL) algorithms have demonstrated superior performance compared to other methods in efficiently identifying relevant documents. One of the key challenges for CAL algorithms is deciding when to stop displaying documents to reviewers. Existing work either lacks transparency—it provides an ad-hoc stopping point, without indicating how many relevant documents are still not found, or lacks efficiency by paying an extra cost to estimate the total number of relevant documents in the collection prior to the actual review.In this article, we handle the problem of deciding the stopping point of TAR under the continuous active learning framework by jointly training a ranking model to rank documents, and by conducting a “greedy” sampling to estimate the total number of relevant documents in the collection. We prove the unbiasedness of the proposed estimators under a with-replacement sampling design, while experimental results demonstrate that the proposed approach, similar to CAL, effectively retrieves relevant documents; but it also provides a transparent, accurate, and effective stopping point.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {41},
numpages = {36},
keywords = {Total recall, active sampling, unbiased estimator}
}

@article{10.1145/3418052,
author = {Kim, Youngwoo and Jang, Myungha and Allan, James},
title = {Explaining Text Matching on Neural Natural Language Inference},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3418052},
doi = {10.1145/3418052},
abstract = {Natural language inference (NLI) is the task of detecting the existence of entailment or contradiction in a given sentence pair. Although NLI techniques could help numerous information retrieval tasks, most solutions for NLI are neural approaches whose lack of interpretability prohibits both straightforward integration and diagnosis for further improvement. We target the task of generating token-level explanations for NLI from a neural model. Many existing approaches for token-level explanation are either computationally costly or require additional annotations for training. In this article, we first introduce a novel method for training an explanation generator that does not require additional human labels. Instead, the explanation generator is trained with the objective of predicting how the model’s classification output will change when parts of the inputs are modified. Second, we propose to build an explanation generator in a multi-task learning setting along with the original NLI task so the explanation generator can utilize the model’s internal behavior. The experiment results suggest that the proposed explanation generator outperforms numerous strong baselines. In addition, our method does not require excessive additional computation at prediction time, which renders it an order of magnitude faster than the best-performing baseline.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {39},
numpages = {23},
keywords = {neural network explanation, Natural language inference, rationale, interpretable machine learning}
}

@article{10.1145/3406109,
author = {Chen, Xiaolin and Song, Xuemeng and Ren, Ruiyang and Zhu, Lei and Cheng, Zhiyong and Nie, Liqiang},
title = {Fine-Grained Privacy Detection with Graph-Regularized Hierarchical Attentive Representation Learning},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3406109},
doi = {10.1145/3406109},
abstract = {Due to the complex and dynamic environment of social media, user generated contents (UGCs) may inadvertently leak users’ personal aspects, such as the personal attributes, relationships and even the health condition, and thus place users at high privacy risks. Limited research efforts, thus far, have been dedicated to the privacy detection from users’ unstructured data (i.e., UGCs). Moreover, existing efforts mainly focus on applying conventional machine learning techniques directly to traditional hand-crafted privacy-oriented features, ignoring the powerful representing capability of the advanced neural networks. In light of this, in this article, we present a fine-grained privacy detection network&nbsp;(GrHA) equipped with graph-regularized hierarchical attentive representation learning. In particular, the proposed GrHA explores the semantic correlations among personal aspects with graph convolutional networks to enhance the regularization for the UGC representation learning, and, hence, fulfil effective fine-grained privacy detection. Extensive experiments on a real-world dataset demonstrate the superiority of the proposed model over state-of-the-art competitors in terms of eight standard metrics. As a byproduct, we have released the codes and involved parameters to facilitate the research community.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {37},
numpages = {26},
keywords = {hierarchical attention mechanism, graph convolutional networks, Fine-grained privacy detection}
}

@article{10.1145/3412362,
author = {Zhao, Guangzhen and Yang, Peng},
title = {Pretrained Embeddings for Stance Detection with Hierarchical Capsule Network on Social Media},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3412362},
doi = {10.1145/3412362},
abstract = {Stance detection on social media aims to identify the stance of social media users toward a topic or claim, which can provide powerful information for various downstream tasks. Many existing stance detection approaches neglect to model the deep semantic representation information in tweets and do not explore aggregating the hierarchical features among words, thus degrading performance. To address these issues, this article proposes a novel deep learning approach Pretrained Embeddings for Stance Detection with Hierarchical Capsule Network (PE-HCN) without complicated preprocessing. Specifically, PE-HCN first adopts a pretrained language model and then uses a related textual entailment task for fine-tuning to obtain the deep textual representations of tweets. The PE-HCN approach extends the dynamic routing scheme to cope with these deep textual representations by utilizing primary capsules for routing the information among words in each tweet and applying secondary capsules to transmit the aggregated features to each category capsule accordingly. Moreover, to improve the confidences of the category capsules, we design an adaptive feedback mechanism to dynamically strengthen the routing signals. Through experiments on three benchmark datasets, compared with the state-of-the-art baselines, the extensive results exhibit that PE-HCN achieves competitive improvements of up to 6.32%, 2.09%, and 1.8%, respectively.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {1},
numpages = {32},
keywords = {Language model, capsule network, stance detection}
}

@article{10.1145/3417996,
author = {Agosti, Maristella and Marchesin, Stefano and Silvello, Gianmaria},
title = {Learning Unsupervised Knowledge-Enhanced Representations to Reduce the Semantic Gap in Information Retrieval},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3417996},
doi = {10.1145/3417996},
abstract = {The semantic mismatch between query and document terms—i.e., the semantic gap—is a long-standing problem in Information Retrieval (IR). Two main linguistic features related to the semantic gap that can be exploited to improve retrieval are synonymy and polysemy. Recent works integrate knowledge from curated external resources into the learning process of neural language models to reduce the effect of the semantic gap. However, these knowledge-enhanced language models have been used in IR mostly for re-ranking and not directly for document retrieval.We propose the Semantic-Aware Neural Framework for IR (SAFIR), an unsupervised knowledge-enhanced neural framework explicitly tailored for IR. SAFIR jointly learns word, concept, and document representations from scratch. The learned representations encode both polysemy and synonymy to address the semantic gap. SAFIR can be employed in any domain where external knowledge resources are available. We investigate its application in the medical domain where the semantic gap is prominent and there are many specialized and manually curated knowledge resources. The evaluation on shared test collections for medical literature retrieval shows the effectiveness of SAFIR in terms of retrieving and ranking relevant documents most affected by the semantic gap.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {38},
numpages = {48},
keywords = {Knowledge-enhanced retrieval, representation learning, medical literature, semantic gap}
}

@article{10.1145/3411754,
author = {Chen, Yifan and Wang, Yang and Zhao, Xiang and Zou, Jie and Rijke, Maarten De},
title = {Block-Aware Item Similarity Models for Top-<i>N</i> Recommendation},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3411754},
doi = {10.1145/3411754},
abstract = {Top-N recommendations have been studied extensively. Promising results have been achieved by recent item-based collaborative filtering (ICF) methods. The key to ICF lies in the estimation of item similarities. Observing the block-diagonal structure of the item similarities in practice, we propose a block-diagonal regularization (BDR) over item similarities for ICF. The intuitions behind BDR are as follows: (1) with BDR, item clustering is embedded into the learning of ICF methods; (2) BDR induces sparsity of item similarities, which guarantees recommendation efficiency; and (3) BDR captures in-block transitivity to overcome rating sparsity. By regularizing the item similarity matrix of item similarity models with BDR, we obtain a block-aware item similarity model. Our experimental evaluations on a large number of datasets show that the block-diagonal structure is crucial to the performance of top-N recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {42},
numpages = {26},
keywords = {Item collaborative filtering, top-N recommendation, item similarity model}
}

@article{10.1145/3411753,
author = {Li, Chang and Markov, Ilya and Rijke, Maarten De and Zoghi, Masrour},
title = {MergeDTS: A Method for Effective Large-Scale Online Ranker Evaluation},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3411753},
doi = {10.1145/3411753},
abstract = {Online ranker evaluation is one of the key challenges in information retrieval. Although the preferences of rankers can be inferred by interleaving methods, the problem of how to effectively choose the ranker pair that generates the interleaved list without degrading the user experience too much is still challenging. On the one hand, if two rankers have not been compared enough, the inferred preference can be noisy and inaccurate. On the other hand, if two rankers are compared too many times, the interleaving process inevitably hurts the user experience too much. This dilemma is known as the exploration versus exploitation tradeoff. It is captured by the K-armed dueling bandit problem, which is a variant of the K-armed bandit problem, where the feedback comes in the form of pairwise preferences. Today’s deployed search systems can evaluate a large number of rankers concurrently, and scaling effectively in the presence of numerous rankers is a critical aspect of K-armed dueling bandit problems.In this article, we focus on solving the large-scale online ranker evaluation problem under the so-called Condorcet assumption, where there exists an optimal ranker that is preferred to all other rankers. We propose Merge Double Thompson Sampling (MergeDTS), which first utilizes a divide-and-conquer strategy that localizes the comparisons carried out by the algorithm to small batches of rankers, and then employs Thompson Sampling to reduce the comparisons between suboptimal rankers inside these small batches. The effectiveness (regret) and efficiency (time complexity) of MergeDTS are extensively evaluated using examples from the domain of online evaluation for web search. Our main finding is that for large-scale Condorcet ranker evaluation problems, MergeDTS outperforms the state-of-the-art dueling bandit algorithms.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {40},
numpages = {28},
keywords = {preference learning, implicit feedback, dueling bandits, Online evaluation}
}

@article{10.1145/3415149,
author = {Mousset, Paul and Pitarch, Yoann and Tamine, Lynda},
title = {End-to-End Neural Matching for Semantic Location Prediction of Tweets},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3415149},
doi = {10.1145/3415149},
abstract = {The impressive increasing availability of social media posts has given rise to considerable research challenges. This article is concerned with the problem of semantic location prediction of geotagged tweets. The underlying task is to associate to a social media post, the focal spatial object, if any (e.g., Place Of Interest POI), it topically focuses on. Although relevant for a number of applications such as POI recommendation, this problem has not so far received the attention it deserves. In previous work, the problem has mainly been tackled by means of language models that rely on costly probability estimation of word relevance across spatial regions. We propose the Spatially-aware Geotext Matching (SGM) model, which relies on a neural network learning framework. The model combines exact word-word-local interaction matching signals with semantic global tweet-POI interaction matching signals. The local interactions are built over kernel spatial word distributions that allow revealing spatially driven word pair similarity patterns. The global interactions consider the strength of the interaction between the tweet and the POI from both the spatial and semantic perspectives. Experimental results on two real-world datasets demonstrate the effectiveness of our proposed SGM model compared to state-of-the-art baselines including language models and traditional neural interaction-based models.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {3},
numpages = {35},
keywords = {neural text matching, Semantic location prediction, point of interest, tweet}
}

@article{10.1145/3414067,
author = {Zhang, Yuan and Sun, Fei and Yang, Xiaoyong and Xu, Chen and Ou, Wenwu and Zhang, Yan},
title = {Graph-Based Regularization on Embedding Layers for Recommendation},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3414067},
doi = {10.1145/3414067},
abstract = {Neural networks have been extensively used in recommender systems. Embedding layers are not only necessary but also crucial for neural models in recommendation as a typical discrete task. In this article, we argue that the widely used l2 regularization for normal neural layers (e.g., fully connected layers) is not ideal for embedding layers from the perspective of regularization theory in Reproducing Kernel Hilbert Space. More specifically, the l2 regularization corresponds to the inner product and the distance in the Euclidean space where correlations between discrete objects (e.g., items) are not well captured. Inspired by this observation, we propose a graph-based regularization approach to serve as a counterpart of the l2 regularization for embedding layers. The proposed regularization incurs almost no extra computational overhead especially when being trained with mini-batches. We also discuss its relationships to other approaches (namely, data augmentation, graph convolution, and joint learning) theoretically. We conducted extensive experiments on five publicly available datasets from various domains with two state-of-the-art recommendation models. Results show that given a kNN (k-nearest neighbor) graph constructed directly from training data without external information, the proposed approach significantly outperforms the l2 regularization on all the datasets and achieves more notable improvements for long-tail users and items.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {2},
numpages = {27},
keywords = {Embedding, neural recommender system, graph-based regularization}
}

@article{10.1145/3406116,
author = {Zhang, Richong and Mensah, Samuel and Kong, Fanshuang and Hu, Zhiyuan and Mao, Yongyi and Liu, Xudong},
title = {Pairwise Link Prediction Model for Out of Vocabulary Knowledge Base Entities},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3406116},
doi = {10.1145/3406116},
abstract = {Real-world knowledge bases such as DBPedia, Yago, and Freebase contain sparse linkage connectivity, which poses a severe challenge to link prediction between entities. To cope with such data scarcity issues, recent models have focused on learning interactions between entity pairs by means of relations that exist between them. However promising, some relations are associated with very few tail entities or head entities, resulting in poor estimation of the relation interaction between entities. In this article, we break the sole dependency of modeling relation interactions between entity pairs by associating a triple with pairwise embeddings, i.e., distributed vector representations for pairs of word-based entities and relation of a triple. We capture the interactions that exist between pairwise embeddings by means of a Pairwise Factorization Model that employs a factorization machine with relation attention. This approach allows parameters for related interactions to be estimated efficiently, ensuring that the pairwise embeddings are discriminative, providing strong supervisory signals for the decoding task of link prediction. The Pairwise Factorization Model we propose exploits a neural bag-of-words model as the encoder, which effectively encodes word-based entities into distributed vector representations for the decoder. The proposed model is simple and enjoys efficiency and capability, showing superior link prediction performance over state-of-the-art complex models on benchmark datasets DBPedia50K and FB15K-237.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {36},
numpages = {28},
keywords = {graph convolutional networks, representation learning, Knowledge bases}
}

@article{10.1145/3402521,
author = {Chen, Xiancong and Li, Lin and Pan, Weike and Ming, Zhong},
title = {A Survey on Heterogeneous One-Class Collaborative Filtering},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3402521},
doi = {10.1145/3402521},
abstract = {Recommender systems play an important role in providing personalized services for users in the context of information overload. Generally, users’ feedback toward items often contain the most significant information reflecting their preferences, which enables accurate personalized recommendation. In real applications, users’ feedback are usually heterogeneous (rather than homogeneous) such as purchases and examinations in e-commerce, which reflects users’ preferences in different degrees. Effective modeling of such heterogeneous one-class feedback is challenging compared with that of homogeneous feedback of ratings. As a response, heterogeneous one-class collaborative filtering (HOCCF) is proposed, which often converts the heterogeneous feedback into two parts (i.e., target feedback and auxiliary feedback), aiming to care more about the target feedback (e.g., purchases) with the assistance of the auxiliary feedback (e.g., examinations). In this survey, we provide an overview of the representative HOCCF methods from the perspective of factorization-based methods, transfer learning-based methods, and deep learning-based methods. First, we review the factorization-based methods according to different strategies. Second, we describe the transfer learning-based methods with different knowledge sharing manners. Third, we discuss the deep learning-based methods according to the neural architectures. Moreover, we include some important example applications, describe the empirical studies, and discuss some promising future directions.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {35},
numpages = {54},
keywords = {matrix factorization, Heterogeneous one-class collaborative filtering, transfer learning, deep learning}
}

@article{10.1145/3397175,
author = {Moffat, Alistair and Petri, Matthias},
title = {Large-Alphabet Semi-Static Entropy Coding Via Asymmetric Numeral Systems},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3397175},
doi = {10.1145/3397175},
abstract = {An entropy coder takes as input a sequence of symbol identifiers over some specified alphabet and represents that sequence as a bitstring using as few bits as possible, typically assuming that the elements of the sequence are independent of each other. Previous entropy coding methods include the well-known Huffman and arithmetic approaches. Here we examine the newer asymmetric numeral systems (ANS) technique for entropy coding and develop mechanisms that allow it to be efficiently used when the size of the source alphabet is large—thousands or millions of symbols. In particular, we examine different ways in which probability distributions over large alphabets can be approximated and in doing so infer techniques that allow the ANS mechanism to be extended to support large-alphabet entropy coding. As well as providing a full description of ANS, we also present detailed experiments using several different types of input, including data streams arising as typical output from the modeling stages of text compression software, and compare theproposed ANS variants with Huffman and arithmetic coding baselines, measuring both compression effectiveness and also encoding and decoding throughput. We demonstrate that in applications in which semi-static compression is appropriate, ANS-based coders can provide an excellent balance between compression effectiveness and speed, even when the alphabet is large.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {33},
numpages = {33},
keywords = {Huffman code, compression, Burrows-Wheeler transform, entropy coder, arithmetic code, Asymmetric numeral systems}
}

@article{10.1145/3394592,
author = {Ren, Xuhui and Yin, Hongzhi and Chen, Tong and Wang, Hao and Hung, Nguyen Quoc Viet and Huang, Zi and Zhang, Xiangliang},
title = {CRSAL: Conversational Recommender Systems with Adversarial Learning},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3394592},
doi = {10.1145/3394592},
abstract = {Recommender systems have been attracting much attention from both academia and industry because of their ability to capture user interests and generate personalized item recommendations. As the life pace in contemporary society speeds up, traditional recommender systems are inevitably limited by their disconnected interaction styles and low adaptivity to users’ evolving demands. Consequently, conversational recommender systems emerge as a prospective research area, where an intelligent dialogue agent is integrated with a recommender system. Conversational recommender systems possess the ability to accurately understand end-users’ intent or request and generate human-like dialogue responses when performing recommendations. However, existing conversational recommender systems only allow the systems to ask users for more preference information, while users’ further questions and concerns about the recommended items (e.g., enquiring the location of a recommended restaurant) can hardly be addressed. Though the recent task-oriented dialogue systems allow for two-way communications, they are not easy to train because of their high dependence on human guidance in terms of user intent recognition and system response generation. Hence, to enable two-way human-machine communications and tackle the challenges brought by manually crafted rules, we propose Conversational Recommender System with Adversarial Learning (CRSAL), a novel end-to-end system to tackle the task of conversational recommendation. In CRSAL, we innovatively design a fully statistical dialogue state tracker coupled with a neural policy agent to precisely capture each user’s intent from limited dialogue data and generate conversational recommendation actions. We further develop an adversarial Actor-Critic reinforcement learning approach to adaptively refine the quality of generated system actions, thus ensuring coherent human-like dialogue responses. Extensive experiments on two benchmark datasets fully demonstrate the superiority of CRSAL on conversational recommendation tasks.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {34},
numpages = {40},
keywords = {Conversational recommender systems, dialogue systems, adversarial learning, deep neural networks}
}

@article{10.1145/3394052,
author = {Zhang, Ruqing and Guo, Jiafeng and Fan, Yixing and Lan, Yanyan and Cheng, Xueqi},
title = {Dual-Factor Generation Model for Conversation},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3394052},
doi = {10.1145/3394052},
abstract = {The conversation task is usually formulated as a conditional generation problem, i.e., to generate a natural and meaningful response given the input utterance. Generally speaking, this formulation is apparently based on an oversimplified assumption that the response is solely dependent on the input utterance. It ignores the subjective factor of the responder, e.g., his/her emotion or knowledge state, which is a major factor that affects the response in practice. Without explicitly differentiating such subjective factor behind the response, existing generation models can only learn the general shape of conversations, leading to the blandness problem of the response. Moreover, there is no intervention mechanism within the existing generation process, since the response is fully decided by the input utterance. In this work, we propose to view the conversation task as a dual-factor generation problem, including an objective factor denoting the input utterance and a subjective factor denoting the responder state. We extend the existing neural sequence-to-sequence (Seq2Seq) model to accommodate the responder state modeling. We introduce two types of responder state, i.e., discrete and continuous state, to model emotion state and topic preference state, respectively. We show that with our dual-factor generation model, we can not only better fit the conversation data, but also actively control the generation of the response with respect to sentiment or topic specificity.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {31},
numpages = {31},
keywords = {Conversation, dual-factor generation, responder state modeling}
}

@article{10.1145/3389547,
author = {Cheng, Miaomiao and Jing, Liping and Ng, Michael K.},
title = {Robust Unsupervised Cross-Modal Hashing for Multimedia Retrieval},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3389547},
doi = {10.1145/3389547},
abstract = {With the quick development of social websites, there are more opportunities to have different media types (such as text, image, video, etc.) describing the same topic from large-scale heterogeneous data sources. To efficiently identify the inter-media correlations for multimedia retrieval, unsupervised cross-modal hashing (UCMH) has gained increased interest due to the significant reduction in computation and storage. However, most UCMH methods assume that the data from different modalities are well paired. As a result, existing UCMH methods may not achieve satisfactory performance when partially paired data are given only. In this article, we propose a new-type of UCMH method called robust unsupervised cross-modal hashing (RUCMH). The major contribution lies in jointly learning modal-specific hash function, exploring the correlations among modalities with partial or even without any pairwise correspondence, and preserving the information of original features as much as possible. The learning process can be modeled via a joint minimization problem, and the corresponding optimization algorithm is presented. A series of experiments is conducted on four real-world datasets (Wiki, MIRFlickr, NUS-WIDE, and MS-COCO). The results demonstrate that RUCMH can significantly outperform the state-of-the-art unsupervised cross-modal hashing methods, especially for the partially paired case, which validates the effectiveness of RUCMH.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {30},
numpages = {25},
keywords = {Multimedia retrieval, unsupervised learning, cross-modal hashing, partially paired data}
}

@article{10.1145/3392734,
author = {Lin, Hao and Zhu, Hengshu and Wu, Junjie and Zuo, Yuan and Zhu, Chen and Xiong, Hui},
title = {Enhancing Employer Brand Evaluation with Collaborative Topic Regression Models},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3392734},
doi = {10.1145/3392734},
abstract = {Employer Brand Evaluation (EBE) is to understand an employer’s unique characteristics to identify competitive edges. Traditional approaches rely heavily on employers’ financial information, including financial reports and filings submitted to the Securities and Exchange Commission (SEC), which may not be readily available for private companies. Fortunately, online recruitment services provide a variety of employers’ information from their employees’ online ratings and comments, which enables EBE from an employee’s perspective. To this end, in this article, we propose a method named Company Profiling–based Collaborative Topic Regression (CPCTR) to collaboratively model both textual (i.e., reviews) and numerical information (i.e., salaries and ratings) for learning latent structural patterns of employer brands. With identified patterns, we can effectively conduct both qualitative opinion analysis and quantitative salary benchmarking. Moreover, a Gaussian processes--based extension, GPCTR, is proposed to capture the complex correlation among heterogeneous information. Extensive experiments are conducted on three real-world datasets to validate the effectiveness and generalizability of our methods in real-life applications. The results clearly show that our methods outperform state-of-the-art baselines and enable a comprehensive understanding of EBE.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {32},
numpages = {33},
keywords = {salary benchmarking, Gaussian processes, collaborative topic regression, Employer brand evaluation}
}

@article{10.1145/3382764,
author = {Qiu, Ruihong and Huang, Zi and Li, Jingjing and Yin, Hongzhi},
title = {Exploiting Cross-Session Information for Session-Based Recommendation with Graph Neural Networks},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3382764},
doi = {10.1145/3382764},
abstract = {Different from the traditional recommender system, the session-based recommender system introduces the concept of the session, i.e., a sequence of interactions between a user and multiple items within a period, to preserve the user’s recent interest. The existing work on the session-based recommender system mainly relies on mining sequential patterns within individual sessions, which are not expressive enough to capture more complicated dependency relationships among items. In addition, it does not consider the cross-session information due to the anonymity of the session data, where the linkage between different sessions is prevented. In this article, we solve these problems with the graph neural networks technique. First, each session is represented as a graph rather than a linear sequence structure, based on which a novel Full Graph Neural Network (FGNN) is proposed to learn complicated item dependency. To exploit and incorporate cross-session information in the individual session’s representation learning, we further construct a Broadly Connected Session (BCS) graph to link different sessions and a novel Mask-Readout function to improve session embedding based on the BCS graph. Extensive experiments have been conducted on two e-commerce benchmark datasets, i.e., Yoochoose and Diginetica, and the experimental results demonstrate the superiority of our proposal through comparisons with state-of-the-art session-based recommender models.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {22},
numpages = {23},
keywords = {Recommender system, graph neural networks, session-based recommendation}
}

@article{10.1145/3389795,
author = {Tonellotto, Nicola and Macdonald, Craig},
title = {Using an Inverted Index Synopsis for Query Latency and Performance Prediction},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3389795},
doi = {10.1145/3389795},
abstract = {Predicting the query latency by a search engine has important benefits, for instance, in allowing the search engine to adjust its configuration to address long-running queries without unnecessarily sacrificing its effectiveness. However, for the dynamic pruning techniques that underlie many commercial search engines, achieving accurate predictions of query latencies is difficult. We propose the use of index synopses—which are stochastic samples of the full index—for attaining accurate timing predictions. Indeed, we experiment using the TREC ClueWeb09 collection, and a large set of real user queries, and find that using small index synopses it is possible to very accurately estimate properties of the larger index, including sizes of posting list unions and intersections. Thereafter, we demonstrate that index synopses facilitate two key use cases: first, for query efficiency prediction, we show that predicting the query latencies on the full index and classifying long-running queries can be accurately achieved using index synopses; second, for query performance prediction, we show that the effectiveness of queries can be estimated more accurately using a synopsis index post-retrieval predictor than a pre-retrieval predictor. Overall, our experiments demonstrate the value of such a stochastic sample of a larger index at predicting the properties of the larger index.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {29},
numpages = {33},
keywords = {query performance prediction, dynamic pruning, Inverted index synopsis, query efficiency prediction}
}

@article{10.1145/3388870,
author = {Alserafi, Ayman and Abell\'{o}, Alberto and Romero, Oscar and Calders, Toon},
title = {Keeping the Data Lake in Form: Proximity Mining for Pre-Filtering Schema Matching},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3388870},
doi = {10.1145/3388870},
abstract = {Data lakes (DLs) are large repositories of raw datasets from disparate sources. As more datasets are ingested into a DL, there is an increasing need for efficient techniques to profile them and to detect the relationships among their schemata, commonly known as holistic schema matching. Schema matching detects similarity between the information stored in the datasets to support information discovery and retrieval. Currently, this is computationally expensive with the volume of state-of-the-art DLs. To handle this challenge, we propose a novel early-pruning approach to improve efficiency, where we collect different types of content metadata and schema metadata about the datasets, and then use this metadata in early-pruning steps to pre-filter the schema matching comparisons. This involves computing proximities between datasets based on their metadata, discovering their relationships based on overall proximities and proposing similar dataset pairs for schema matching. We improve the effectiveness of this task by introducing a supervised mining approach for effectively detecting similar datasets that are proposed for further schema matching. We conduct extensive experiments on a real-world DL that proves the success of our approach in effectively detecting similar datasets for schema matching, with recall rates of more than 85% and efficiency improvements above 70%. We empirically show the computational cost saving in space and time by applying our approach in comparison to instance-based schema matching techniques.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {26},
numpages = {30},
keywords = {data governance, content metadata management, Data lake, holistic schema matching, dataset similarity mining, early pruning}
}

@article{10.1145/3388640,
author = {Zou, Jie and Kanoulas, Evangelos},
title = {Towards Question-Based High-Recall Information Retrieval: Locating the Last Few Relevant Documents for Technology-Assisted Reviews},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3388640},
doi = {10.1145/3388640},
abstract = {While continuous active learning algorithms have proven effective in finding most of the relevant documents in a collection, the cost for locating the last few remains high for applications such as Technology-assisted Reviews (TAR). To locate these last few but significant documents efficiently, Zou et&nbsp;al. [2018] have proposed a novel interactive algorithm. The algorithm is based on constructing questions about the presence or absence of entities in the missing relevant documents. The hypothesis made is that entities play a central role in documents carrying key information and that the users are able to answer questions about the presence or absence of an entity in the missing relevance documents. Based on this, a Sequential Bayesian Search-based approach that selects the optimal sequence of questions to ask was devised. In this work, we extend Zou et&nbsp;al. [2018] by (a) investigating the noise tolerance of the proposed algorithm; (b) proposing an alternative objective function to optimize, which accounts for user “erroneous” answers; (c) proposing a method that sequentially decides the best point to stop asking questions to the user; and (d) conducting a small user study to validate some of the assumptions made by Zou et&nbsp;al. [2018]. Furthermore, all experiments are extended to demonstrate the effectiveness of the proposed algorithms not only in the phase of abstract appraisal (i.e., finding the abstracts of potentially relevant documents in a collection) but also finding the documents to be included in the review (i.e., finding the subset of those relevant abstracts for which the article remains relevant). The experimental results demonstrate that the proposed algorithms can greatly improve performance, requiring reviewing fewer irrelevant documents to find the last relevant ones compared to state-of-the-art methods, even in the case of noisy answers. Further, they show that our algorithm learns to stop asking questions at the right time. Last, we conduct a small user study involving an expert reviewer. The user study validates some of the assumptions made in this work regarding the user’s willingness to answer the system questions and the extent of it, as well as the ability of the user to answer these questions.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {27},
numpages = {35},
keywords = {Technology-assisted reviews, interactive search, SBSTARext, SBSTAR, asking questions}
}

@article{10.1145/3381926,
author = {Hauff, Claudia and Kiseleva, Julia and Sanderson, Mark and Zamani, Hamed and Zhang, Yongfeng},
title = {Special Issue Proposal: Conversational Search and Recommendation},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {1046-8188},
url = {https://doi.org/10.1145/3381926},
doi = {10.1145/3381926},
abstract = {The rapid growth in speech and small screen interfaces, particularly on mobile devices, has significantly influenced the way users interact with intelligent systems to satisfy their information needs. The growing interest in personal digital assistants, such as Amazon Alexa, Apple Siri, Google  Assistant, and Microsoft Cortana, demonstrates the willingness of users to employ conversational interactions. In this special issue, we focus on interactions with information seeking goals. This includes conversational search and recommendation. Given the importance of the topic to both academia and industry and the recent availability of multiple public datasets in this area, we believe that the time is right to propose a special issue on this topic, and ACM Transactions on Information Systems is the perfect venue for it.},
journal = {ACM Trans. Inf. Syst.},
numpages = {1}
}

@article{10.1145/3388924,
author = {Huang, Jie and Chen, Chuan and Ye, Fanghua and Hu, Weibo and Zheng, Zibin},
title = {Nonuniform Hyper-Network Embedding with Dual Mechanism},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3388924},
doi = {10.1145/3388924},
abstract = {Network embedding which aims to learn the low-dimensional representations for vertices in networks has been extensively studied in recent years. Although there are various models designed for networks with different properties and different structures for different tasks, most of them are only applied to normal networks which only contain pairwise relationships between vertices. In many realistic cases, relationships among objects are not pairwise and such relationships can be better modeled by a hyper-network in which each edge can connect an uncertain number of vertices. In this article, we focus on two properties of hyper-networks: nonuniform and dual property. In order to make full use of these two properties, we firstly propose a flexible model called Hyper2vec to learn the embeddings of hyper-networks by applying a biased second order random walk strategy to hyper-networks in the framework of Skip-gram. Then, we combine the features of hyperedges by considering the dual hyper-networks to build a further model called NHNE based on 1D convolutional neural networks, and train a tuplewise similarity function for the nonuniform relationships in hyper-networks. Extensive experiments demonstrate the significant effectiveness of our methods for hyper-network embedding.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {28},
numpages = {18},
keywords = {dual network, link prediction, hyper-network, Network embedding}
}

@article{10.1145/3386253,
author = {Liu, Yang and Wu, Yi-Fang Brook},
title = {FNED: A Deep Network for Fake News Early Detection on Social Media},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3386253},
doi = {10.1145/3386253},
abstract = {The fast spreading of fake news stories on social media can cause inestimable social harm. Developing effective methods to detect them early is of paramount importance. A major challenge of fake news early detection is fully utilizing the limited data observed at the early stage of news propagation and then learning useful patterns from it for identifying fake news. In this article, we propose a novel deep neural network to detect fake news early. It has three novel components: (1) a status-sensitive crowd response feature extractor that extracts both text features and user features from combinations of users’ text response and their corresponding user profiles, (2) a position-aware attention mechanism that highlights important user responses at specific ranking positions, and (3) a multi-region mean-pooling mechanism to perform feature aggregation based on multiple window sizes. Experimental results on two real-world datasets demonstrate that our proposed model can detect fake news with greater than 90% accuracy within 5 minutes after it starts to spread and before it is retweeted 50 times, which is significantly faster than state-of-the-art baselines. Most importantly, our approach requires only 10% labeled fake news samples to achieve this effectiveness under PU-Learning settings.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {25},
numpages = {33},
keywords = {deep learning, Fake news detection, social media}
}

@article{10.1145/3385670,
author = {Jagerman, Rolf and Markov, Ilya and Rijke, Maarten De},
title = {Safe Exploration for Optimizing Contextual Bandits},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3385670},
doi = {10.1145/3385670},
abstract = {Contextual bandit problems are a natural fit for many information retrieval tasks, such as learning to rank, text classification, recommendation, and so on. However, existing learning methods for contextual bandit problems have one of two drawbacks: They either do not explore the space of all possible document rankings (i.e., actions) and, thus, may miss the optimal ranking, or they present suboptimal rankings to a user and, thus, may harm the user experience. We introduce a new learning method for contextual bandit problems, Safe Exploration Algorithm (SEA), which overcomes the above drawbacks. SEA starts by using a baseline (or production) ranking system (i.e., policy), which does not harm the user experience and, thus, is safe to execute but has suboptimal performance and, thus, needs to be improved. Then SEA uses counterfactual learning to learn a new policy based on the behavior of the baseline policy. SEA also uses high-confidence off-policy evaluation to estimate the performance of the newly learned policy. Once the performance of the newly learned policy is at least as good as the performance of the baseline policy, SEA starts using the new policy to execute new actions, allowing it to actively explore favorable regions of the action space. This way, SEA never performs worse than the baseline policy and, thus, does not harm the user experience, while still exploring the action space and, thus, being able to find an optimal policy. Our experiments using text classification and document retrieval confirm the above by comparing SEA (and a boundless variant called BSEA) to online and offline learning methods for contextual bandit problems.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {24},
numpages = {23},
keywords = {Counterfactual learning, Exploration, Learning to rank}
}

@article{10.1145/3385186,
author = {Oliveira, Wyverson Bonasoli de and Dorini, Leyza Baldo and Minetto, Rodrigo and Silva, Thiago H.},
title = {OutdoorSent: Sentiment Analysis of Urban Outdoor Images by Using Semantic and Deep Features},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3385186},
doi = {10.1145/3385186},
abstract = {Opinion mining in outdoor images posted by users during different activities can provide valuable information to better understand urban areas. In this regard, we propose a framework to classify the sentiment of outdoor images shared by users on social networks. We compare the performance of state-of-the-art ConvNet architectures and one specifically designed for sentiment analysis. We also evaluate how the merging of deep features and semantic information derived from the scene attributes can improve classification and cross-dataset generalization performance. The evaluation explores a novel dataset—namely, OutdoorSent—and other publicly available datasets. We observe that the incorporation of knowledge about semantic attributes improves the accuracy of all ConvNet architectures studied. Besides, we found that exploring only images related to the context of the study—outdoor, in our case—is recommended, i.e., indoor images were not significantly helpful. Furthermore, we demonstrated the applicability of our results in the United States city of Chicago, Illinois, showing that they can help to improve the knowledge of subjective characteristics of different areas of the city. For instance, particular areas of the city tend to concentrate more images of a specific class of sentiment, which are also correlated with median income, opening up opportunities in different fields.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {23},
numpages = {28},
keywords = {information retrieval, location-based social networks, deep learning, image processing, Sentiment analysis}
}

@article{10.1145/3383123,
author = {Huang, Minlie and Zhu, Xiaoyan and Gao, Jianfeng},
title = {Challenges in Building Intelligent Open-Domain Dialog Systems},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3383123},
doi = {10.1145/3383123},
abstract = {There is a resurgent interest in developing intelligent open-domain dialog systems due to the availability of large amounts of conversational data and the recent progress on neural approaches to conversational AI [33]. Unlike traditional task-oriented bots, an open-domain dialog system aims to establish long-term connections with users by satisfying the human need for communication, affection, and social belonging. This article reviews the recent work on neural approaches that are devoted to addressing three challenges in developing such systems: semantics, consistency, and interactiveness. Semantics requires a dialog system to not only understand the content of the dialog but also identify users’ emotional and social needs during the conversation. Consistency requires the system to demonstrate a consistent personality to win users’ trust and gain their long-term confidence. Interactiveness refers to the system’s ability to generate interpersonal responses to achieve particular social goals such as entertainment and conforming. The studies we select to present in this survey are based on our unique views and are by no means complete. Nevertheless, we hope that the discussion will inspire new research in developing more intelligent open-domain dialog systems.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {21},
numpages = {32},
keywords = {chatbot, conversation generation, Dialog system, social bot, conversational AI, response generation}
}

@article{10.1145/3377550,
author = {Zhao, Wayne Xin and Hou, Yupeng and Chen, Junhua and Zhu, Jonathan J. H. and Yin, Eddy Jing and Su, Hanting and Wen, Ji-Rong},
title = {Learning Semantic Representations from Directed Social Links to Tag Microblog Users at Scale},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3377550},
doi = {10.1145/3377550},
abstract = {This article presents a network embedding approach to automatically generate tags for microblog users. Instead of using text data, we aim to annotate microblog users with meaningful tags by leveraging rich social link data. To utilize directed social links, we use two kinds of node representations for modeling user interest in terms of their followers and followees, respectively. To alleviate the sparsity problem, we propose a novel method based on two transformation functions for capturing implicit interest similarity. Different from previous works on capturing high-order proximity, our model is able to directly characterize the effect of the context user on the proximity of node pairs. Another novelty of our model is that the importance scores of users learned from the classic PageRank algorithm are utilized to set the link weights. By using such weights, our model is more capable of disentangling the interest similarity evidence of a link. We jointly consider the above factors when designing the final objective function.We construct a very large evaluation set consisting of 2.6M users, 0.5M tags, and 0.8B following links. To our knowledge, it is the largest reported dataset for microblog user tagging in the literature. Extensive experiments on this dataset demonstrate the effectiveness of the proposed approach. We implement this approach with several optimization techniques, which makes our model easy to scale to very large social networks. Ubiquitous social links provide important data resources to understand user interests. Our work provides an effective and efficient solution to annotate user interests solely using the link data, which has important practical value in industry. To illustrate the use of our models, we implement a demonstration system for visualizing, navigating, and searching microblog users.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {17},
numpages = {30},
keywords = {Microblog user tagging, Social importance, Network embedding}
}

@article{10.1145/3379340,
author = {Naskar, Debashis and Singh, Sanasam Ranbir and Kumar, Durgesh and Nandi, Sukumar and Rivaherrera, Eva Onaindia de la},
title = {Emotion Dynamics of Public Opinions on Twitter},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3379340},
doi = {10.1145/3379340},
abstract = {Recently, social media has been considered the fastest medium for information broadcasting and sharing. Considering the wide range of applications such as viral marketing, political campaigns, social advertisement, and so on, influencing characteristics of users or tweets have attracted several researchers. It is observed from various studies that influential messages or users create a high impact on a social ecosystem. In this study, we assume that public opinion on a social issue on Twitter carries a certain degree of emotion, and there is an emotion flow underneath the Twitter network. In this article, we investigate social dynamics of emotion present in users’ opinions and attempt to understand (i) changing characteristics of users’ emotions toward a social issue over time, (ii) influence of public emotions on individuals’ emotions, (iii) cause of changing opinion by social factors, and so on. We study users’ emotion dynamics over a collection of 17.65M tweets with 69.36K users and observe 63% of the users are likely to change their emotional state against the topic into their subsequent tweets. Tweets were coming from the member community shows higher influencing capability than the other community sources. It is also observed that retweets influence users more than hashtags, mentions, and replies.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {18},
numpages = {24},
keywords = {social dynamics, opinion discussion, Emotion transition, social agreement, influence measure}
}

@article{10.1145/3380954,
author = {Nie, Liqiang and Li, Yongqi and Feng, Fuli and Song, Xuemeng and Wang, Meng and Wang, Yinglong},
title = {Large-Scale Question Tagging via Joint Question-Topic Embedding Learning},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3380954},
doi = {10.1145/3380954},
abstract = {Recent years have witnessed a flourishing of community-driven question answering (cQA), like Yahoo! Answers and AnswerBag, where people can seek precise information. After 2010, some novel cQA systems, including Quora and Zhihu, gained momentum. Besides interactions, the latter enables users to label the questions with topic tags that highlight the key points conveyed in the questions. In this article, we shed light on automatically annotating a newly posted question with topic tags that are predefined and preorganized into a directed acyclic graph. To accomplish this task, we present an end-to-end deep interactive embedding model to jointly learn the embeddings of questions and topics by projecting them into the same space for a similarity measure. In particular, we first learn the embeddings of questions and topic tags by two deep parallel models. Thereinto, we regularize the embeddings of topic tags via fully exploring their hierarchical structures, which is able to alleviate the problem of imbalanced topic distribution. Thereafter, we interact each question embedding with the topic tag matrix, i.e., all the topic tag embeddings. Following that, a sigmoid cross-entropy loss is appended to reward the positive question-topic pairs and penalize the negative ones. To justify our model, we have conducted extensive experiments on an unprecedented large-scale social QA dataset obtained from Zhihu.com, and the experimental results demonstrate that our model achieves superior performance to several state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {20},
numpages = {23},
keywords = {topic hierarchy, Question tagging, embedding learning, CQA}
}

@article{10.1145/3379507,
author = {Huang, Zhenya and Liu, Qi and Chen, Yuying and Wu, Le and Xiao, Keli and Chen, Enhong and Ma, Haiping and Hu, Guoping},
title = {Learning or Forgetting? A Dynamic Approach for Tracking the Knowledge Proficiency of Students},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3379507},
doi = {10.1145/3379507},
abstract = {The rapid development of the technologies for online learning provides students with extensive resources for self-learning and brings new opportunities for data-driven research on educational management. An important issue of online learning is to diagnose the knowledge proficiency (i.e., the mastery level of a certain knowledge concept) of each student. Considering that it is a common case that students inevitably learn and forget knowledge from time to time, it is necessary to track the change of their knowledge proficiency during the learning process. Existing approaches either relied on static scenarios or ignored the interpretability of diagnosis results. To address these problems, in this article, we present a focused study on diagnosing the knowledge proficiency of students, where the goal is to track and explain their evolutions simultaneously. Specifically, we first devise an explanatory probabilistic matrix factorization model, Knowledge Proficiency Tracing (KPT), by leveraging educational priors. KPT model first associates each exercise with a knowledge vector in which each element represents a specific knowledge concept with the help of Q-matrix. Correspondingly, at each time, each student can be represented as a proficiency vector in the same knowledge space. Then, our KPT model jointly applies two classical educational theories (i.e., learning curve and forgetting curve) to capture the change of students’ proficiency level on concepts over time. Furthermore, for improving the predictive performance, we develop an improved version of KPT, named Exercise-correlated Knowledge Proficiency Tracing (EKPT), by considering the connectivity among exercises with the same knowledge concepts. Finally, we apply our KPT and EKPT models to three important diagnostic tasks, including knowledge estimation, score prediction, and diagnosis result visualization. Extensive experiments on four real-world datasets demonstrate that both of our models could track the knowledge proficiency of students effectively and interpretatively.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {19},
numpages = {33},
keywords = {Diagnosis, knowledge proficiency levels, educational theories}
}

@article{10.1145/3376927,
author = {Qin, Chuan and Zhu, Hengshu and Xu, Tong and Zhu, Chen and Ma, Chao and Chen, Enhong and Xiong, Hui},
title = {An Enhanced Neural Network Approach to Person-Job Fit in Talent Recruitment},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3376927},
doi = {10.1145/3376927},
abstract = {The widespread use of online recruitment services has led to an information explosion in the job market. As a result, recruiters have to seek intelligent ways for Person-Job Fit, which is the bridge for adapting the right candidates to the right positions. Existing studies on Person-Job Fit usually focus on measuring the matching degree between talent qualification and job requirements mainly based on the manual inspection of human resource experts, which could be easily misguided by the subjective, incomplete, and inefficient nature of human judgment. To that end, in this article, we propose a novel end-to-end Topic-based Ability-aware Person-Job Fit Neural Network (TAPJFNN) framework, which has a goal of reducing the dependence on manual labor and can provide better interpretability about the fitting results. The key idea is to exploit the rich information available in abundant historical job application data. Specifically, we propose a word-level semantic representation for both job requirements and job seekers’ experiences based on Recurrent Neural Network (RNN). Along this line, two hierarchical topic-based ability-aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measure the different contribution of each job experience to a specific ability requirement. In addition, we design a refinement strategy for Person-Job Fit prediction based on historical recruitment records. Furthermore, we introduce how to exploit our TAPJFNN framework for enabling two specific applications in talent recruitment: talent sourcing and job recommendation. Particularly, in the application of job recommendation, a novel training mechanism is designed for addressing the challenge of biased negative labels. Finally, extensive experiments on a large-scale real-world dataset clearly validate the effectiveness and interpretability of the TAPJFNN and its variants compared with several baselines.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {15},
numpages = {33},
keywords = {Recruitment analysis, person-job fit, neural network}
}

@article{10.1145/3372154,
author = {Chen, Yifan and Wang, Yang and Zhao, Xiang and Yin, Hongzhi and Markov, Ilya and Rijke, MAARTEN De},
title = {Local Variational Feature-Based Similarity Models for Recommending Top-<i>N</i> New Items},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3372154},
doi = {10.1145/3372154},
abstract = {The top-N recommendation problem has been studied extensively. Item-based collaborative filtering recommendation algorithms show promising results for the problem. They predict a user’s preferences by estimating similarities between a target and user-rated items. Top-N recommendation remains a challenging task in scenarios where there is a lack of preference history for new items. Feature-based Similarity Models (FSMs) address this particular problem by extending item-based collaborative filtering by estimating similarity functions of item features. The quality of the estimated similarity function determines the accuracy of the recommendation. However, existing FSMs only estimate global similarity functions; i.e., they estimate using preference information across all users. Moreover, the estimated similarity functions are linear; hence, they may fail to capture the complex structure underlying item features.In this article, we propose to improve FSMs by estimating local similarity functions, where each function is estimated for a subset of like-minded users. To capture global preference patterns, we extend the global similarity function from linear to nonlinear, based on the effectiveness of variational autoencoders. We propose a Bayesian generative model, called the Local Variational Feature-based Similarity Model, to encapsulate local and global similarity functions. We present a variational Expectation Minimization algorithm for efficient approximate inference. Extensive experiments on a large number of real-world datasets demonstrate the effectiveness of our proposed model.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {12},
numpages = {33},
keywords = {item feature, deep generative model, item cold-start, Top-N recommendation}
}

@article{10.1145/3372338,
author = {Ding, Jingtao and Yu, Guanghui and Li, Yong and He, Xiangnan and Jin, Depeng},
title = {Improving Implicit Recommender Systems with Auxiliary Data},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3372338},
doi = {10.1145/3372338},
abstract = {Most existing recommender systems leverage the primary feedback only, despite the fact that users also generate a large amount of auxiliary feedback. These feedback usually indicate different user preferences when comparing to the primary feedback directly used to optimize the system performance. For example, in E-commerce sites, view data is easily accessible, which provides a valuable yet weaker signal than the primary feedback of purchase. In this work, we improve implicit feedback-based recommender systems (dubbed Implicit Recommender Systems) by integrating auxiliary view data into matrix factorization (MF). To exploit different preference levels, we propose both pointwise and pairwise models in terms of how to leverage users’ viewing behaviors. The latter model learns the pairwise ranking relations among purchased, viewed, and non-viewed interactions, being more effective and flexible than the former pointwise MF method. However, such a pairwise formulation poses a computational efficiency problem in learning the model. To address this problem, we design a new learning algorithm based on the element-wise Alternating Least Squares&nbsp;(eALS) learner. Notably, our designed algorithm can efficiently learn model parameters from the whole user-item matrix (including all missing data), with a rather low time complexity that is dependent on the observed data only. Extensive experiments on two real-world datasets demonstrate that our method outperforms several state-of-the-art MF methods by 6.43%∼ 6.75%. Our implementation is available at&nbsp;https://github.com/dingjingtao/Auxiliary_enhanced_ALS.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {11},
numpages = {27},
keywords = {matrix factorization, implicit feedback, Auxiliary feedback, item recommendation, eALS}
}

@article{10.1145/3365211,
author = {Ahmad, Faizan and Abbasi, Ahmed and Li, Jingjing and Dobolyi, David G. and Netemeyer, Richard G. and Clifford, Gari D. and Chen, Hsinchun},
title = {A Deep Learning Architecture for Psychometric Natural Language Processing},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3365211},
doi = {10.1145/3365211},
abstract = {Psychometric measures reflecting people’s knowledge, ability, attitudes, and personality traits are critical for many real-world applications, such as e-commerce, health care, and cybersecurity. However, traditional methods cannot collect and measure rich psychometric dimensions in a timely and unobtrusive manner. Consequently, despite their importance, psychometric dimensions have received limited attention from the natural language processing and information retrieval communities. In this article, we propose a deep learning architecture, PyNDA, to extract psychometric dimensions from user-generated texts. PyNDA contains a novel representation embedding, a demographic embedding, a structural equation model (SEM) encoder, and a multitask learning mechanism designed to work in unison to address the unique challenges associated with extracting rich, sophisticated, and user-centric psychometric dimensions. Our experiments on three real-world datasets encompassing 11 psychometric dimensions, including trust, anxiety, and literacy, show that PyNDA markedly outperforms traditional feature-based classifiers as well as the state-of-the-art deep learning architectures. Ablation analysis reveals that each component of PyNDA significantly contributes to its overall performance. Collectively, the results demonstrate the efficacy of the proposed architecture for facilitating rich psychometric analysis. Our results have important implications for user-centric information extraction and retrieval systems looking to measure and incorporate psychometric dimensions.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {6},
numpages = {29},
keywords = {Deep learning, psychometric measures, natural language processing, text classification}
}

@article{10.1145/3361719,
author = {Zhang, Dong and Zhao, Shu and Duan, Zhen and Chen, Jie and Zhang, Yanping and Tang, Jie},
title = {A Multi-Label Classification Method Using a Hierarchical and Transparent Representation for Paper-Reviewer Recommendation},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3361719},
doi = {10.1145/3361719},
abstract = {The paper-reviewer recommendation task is of significant academic importance for conference chairs and journal editors. It aims to recommend appropriate experts in a discipline to comment on the quality of papers of others in that discipline. How to effectively and accurately recommend reviewers for the submitted papers is a meaningful and still tough task. Generally, the relationship between a paper and a reviewer often depends on the semantic expressions of them. Creating a more expressive representation can make the peer-review process more robust and less arbitrary. So the representations of a paper and a reviewer are very important for the paper-reviewer recommendation. Actually, a reviewer or a paper often belongs to multiple research fields, which increases difficulty in paper-reviewer recommendation. In this article, we propose a Multi-Label Classification method using a HIErarchical and transPArent Representation named Hiepar-MLC. First, we introduce HIErarchical and transPArent Representation (Hiepar) to express the semantic information of the reviewer and the paper. Hiepar is learned from a two-level bidirectional gated recurrent unit based network applying the attention mechanism. It is capable of capturing the two-level hierarchical information (word-sentence-document) and highlighting the elements in reviewers or papers to support the labels. This word-sentence-document information mirrors the hierarchical structure of a reviewer or a paper and captures the exact semantics of them. Then we transform the paper-reviewer recommendation problem into a multi-level classification issue, whose multiple research labels exactly guide the learning process. It is flexible in that we can select any multi-label classification method to solve the paper-reviewer recommendation problem. Further, we propose a simple multi-label-based reviewer assignment (MLBRA) strategy to select the appropriate reviewers. It is interesting in that we also explore the paper-reviewer recommendation in the coarse-grain granularity. Extensive experiments on the real-world dataset consisting of the papers in the ACM Digital Library show that Hiepar-MLC achieves better label prediction performance than the existing representation alternatives. In addition, with the MLBRA strategy, we show the effectiveness and the feasibility of our transformation from paper-reviewer recommendation to multi-label classification.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {5},
numpages = {20},
keywords = {multi-label classification, transparent, ACM Digital Library, Paper-reviewer recommendation, hierarchical}
}

@article{10.1145/3377850,
author = {Meng, Zaiqiao and Liang, Shangsong and Zhang, Xiangliang and McCreadie, Richard and Ounis, Iadh},
title = {Jointly Learning Representations of Nodes and Attributes for Attributed Networks},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3377850},
doi = {10.1145/3377850},
abstract = {Previous embedding methods for attributed networks aim at learning low-dimensional vector representations only for nodes but not for both nodes and attributes, resulting in the fact that node embeddings cannot be directly used to recover the correlations between nodes and attributes. However, capturing such correlations by embeddings is of great importance for many real-world applications, such as attribute inference and user profiling. Moreover, in real-world scenarios, many attributed networks evolve over time, with their nodes, links, and attributes changing from time to time. In this article, we study the problem of jointly learning low-dimensional representations of both nodes and attributes for static and dynamic attributed networks. To address this problem, we propose a Co-embedding model for Static Attributed Networks (CSAN), which jointly learns low-dimensional representations of both attributes and nodes in the same semantic space such that their affinities can be effectively captured and measured, and a Co-embedding model for Dynamic Attributed Networks (CDAN) to dynamically track low-dimensional representations of nodes and attributes over time. To obtain effective embeddings, both our co-embedding models, CSAN and CDAN, embed each node and attribute with means and variances of Gaussian distributions via variational auto-encoders. Our CDAN model formulates the dynamic changes of a dynamic attributed network by aggregating perturbation features from the nodes’ local neighborhoods as well as attributes’ associations such that the evolving patterns of the given network can be tracked. Experimental results on real-world networks demonstrate that our proposed embedding models outperform state-of-the-art non-dynamic and dynamic embedding models.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {16},
numpages = {32},
keywords = {dynamic embedding, Attributed network, variational auto-encoder, network embedding}
}

@article{10.1145/3374210,
author = {Arapakis, Ioannis and Penta, Antonio and Joho, Hideo and Leiva, Luis A.},
title = {A Price-per-Attention Auction Scheme Using Mouse Cursor Information},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3374210},
doi = {10.1145/3374210},
abstract = {Payments in online ad auctions are typically derived from click-through rates, so that advertisers do not pay for ineffective ads. But advertisers often care about more than just clicks. That is, for example, if they aim to raise brand awareness or visibility. There is thus an opportunity to devise a more effective ad pricing paradigm, in which ads are paid only if they are actually noticed. This article contributes a novel auction format based on a pay-per-attention (PPA) scheme. We show that the PPA auction inherits the desirable properties (strategy-proofness and efficiency) as its pay-per-impression and pay-per-click counterparts, and that it also compares favourably in terms of revenues. To make the PPA format feasible, we also contribute a scalable diagnostic technology to predict user attention to ads in sponsored search using raw mouse cursor coordinates only, regardless of the page content and structure. We use the user attention predictions in numerical simulations to evaluate the PPA auction scheme. Our results show that, in relevant economic settings, the PPA revenues would be strictly higher than the existing auction payment schemes.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {13},
numpages = {30},
keywords = {direct displays, online advertising, auctions, mouse cursor analysis, user attention, Sponsored search}
}

@article{10.1145/3373807,
author = {Chen, Chong and Zhang, Min and Zhang, Yongfeng and Liu, Yiqun and Ma, Shaoping},
title = {Efficient Neural Matrix Factorization without Sampling for Recommendation},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3373807},
doi = {10.1145/3373807},
abstract = {Recommendation systems play a vital role to keep users engaged with personalized contents in modern online platforms. Recently, deep learning has revolutionized many research fields and there is a surge of interest in applying it for recommendation. However, existing studies have largely focused on exploring complex deep-learning architectures for recommendation task, while typically applying the negative sampling strategy for model learning. Despite effectiveness, we argue that these methods suffer from two important limitations: (1) the methods with complex network structures have a substantial number of parameters, and require expensive computations even with a sampling-based learning strategy; (2) the negative sampling strategy is not robust, making sampling-based methods difficult to achieve the optimal performance in practical applications.In this work, we propose to learn neural recommendation models from the whole training data without sampling. However, such a non-sampling strategy poses strong challenges to learning efficiency. To address this, we derive three new optimization methods through rigorous mathematical reasoning, which can efficiently learn model parameters from the whole data (including all missing data) with a rather low time complexity. Moreover, based on a simple Neural Matrix Factorization architecture, we present a general framework named ENMF, short for Efficient Neural Matrix Factorization. Extensive experiments on three real-world public datasets indicate that the proposed ENMF framework consistently and significantly outperforms the state-of-the-art methods on the Top-K recommendation task. Remarkably, ENMF also shows significant advantages in training efficiency, which makes it more applicable to real-world large-scale systems.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {14},
numpages = {28},
keywords = {efficient learning, neural networks, Matrix factorization, recommendation system, implicit feedback}
}

@article{10.1145/3371707,
author = {Choi, Bogeum and Ward, Austin and Li, Yuan and Arguello, Jaime and Capra, Robert},
title = {The Effects of Task Complexity on the Use of Different Types of Information in a Search Assistance Tool},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3371707},
doi = {10.1145/3371707},
abstract = {In interactive information retrieval, an important research question is: How do task characteristics influence users’ needs and behaviors? We report on a laboratory study (N=32) that investigated the effects of task complexity on the types of information used by participants while searching. Participants completed tasks of four complexity levels and had access to four different types of information provided through a search-assistance tool referred to as the InfoBoxes (IB). The IB tool presented the following types of task-related information (info-types) on different tabs: (1) facts, (2) concepts, (3) opinions, and (4) insights. Facts (and opinions) were defined as objective (and subjective) statements relevant to the task. Concepts were defined as important ideas, principles, or entities related to the task. Insights were defined as tips or advice about the task. The study investigated six research questions that considered the effects of task complexity on: (RQ1) participants’ pre-/post-task perceptions about useful info-types; (RQ2) use of different info-types during the task; (RQ3) motivations for engaging with the IB; (RQ4) gains from using it; (RQ5) the search stage participants were in while engaging with the IB; and (RQ6) motivations for sometimes avoiding the IB. Our results suggest that task complexity influenced all six types of outcomes. We discuss implications of our results for designing search assistance tools and systems that favor certain types of content based on task characteristics.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {9},
numpages = {28},
keywords = {search behaviors, Cognitive task complexity, search assistance}
}

@article{10.1145/3371390,
author = {Thomas, Paul and Billerbeck, Bodo and Craswell, Nick and White, Ryen W.},
title = {Investigating Searchers’ Mental Models to Inform Search Explanations},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3371390},
doi = {10.1145/3371390},
abstract = {Modern web search engines use many signals to select and rank results in response to queries. However, searchers’ mental models of search are relatively unsophisticated, hindering their ability to use search engines efficiently and effectively. Annotating results with more in-depth explanations could help, but search engine providers need to know what to explain. To this end, we report on a study of searchers’ mental models of web selection and ranking, with more than 400&nbsp;respondents to an online survey and 11&nbsp;face-to-face interviews. Participants volunteered a range of factors and showed good understanding of important concepts such as popularity, wording, and personalization. However, they showed little understanding of recency or diversity and incorrect ideas of payment for ranking. Where there are already explanatory annotations on the results page—such as “ad” markers and keyword highlighting—participants were familiar with ranking concepts. This suggests that further explanatory annotations may be useful.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {10},
numpages = {25},
keywords = {explanation, web search, Mental models, ranking}
}

@article{10.1145/3366172,
author = {Zeng, Zijie and Lin, Jing and Li, Lin and Pan, Weike and Ming, Zhong},
title = {Next-Item Recommendation via Collaborative Filtering with Bidirectional Item Similarity},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3366172},
doi = {10.1145/3366172},
abstract = {Exploiting temporal effect has empirically been recognized as a promising way to improve recommendation performance in recent years. In real-world applications, one-class data in the form of (user, item, timestamp) are usually more accessible and abundant than numerical ratings. In this article, we focus on exploiting such one-class data in order to provide personalized next-item recommendation services. Specifically, we base our work on the framework of time-aware item-based collaborative filtering and propose a simple yet effective similarity measurement called bidirectional item similarity (BIS) that is able to capture sequential patterns even from noisy data. Furthermore, we extend BIS via some factorization techniques and obtain an adaptive version, i.e., adaptive BIS (ABIS), in order to better fit the behavioral data. We also design a compound weighting function that leverages the complementarity between two well-known time-aware weighting functions. With the proposed similarity measurements and weighting function, we obtain two novel collaborative filtering methods that are able to achieve significantly better performance than the state-of-the-art methods, showcasing their effectiveness for next-item recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {7},
numpages = {22},
keywords = {collaborative filtering, matrix factorization, next-item recommendation, Bidirectional item similarity}
}

@article{10.1145/3368960,
author = {Sun, Xiao and Li, Jia and Wei, Xing and Li, Changliang and Tao, Jianhua},
title = {Emotional Conversation Generation Based on a Bayesian Deep Neural Network},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3368960},
doi = {10.1145/3368960},
abstract = {The field of conversation generation using neural networks has attracted increasing attention from researchers for several years. However, traditional neural language models tend to generate a generic reply with poor semantic logic and no emotion. This article proposes an emotional conversation generation model based on a Bayesian deep neural network that can generate replies with rich emotions, clear themes, and diverse sentences. The topic and emotional keywords of the replies are pregenerated by introducing commonsense knowledge in the model. The reply is divided into multiple clauses, and then a multidimensional generator based on the transformer mechanism proposed in this article is used to iteratively generate clauses from two dimensions: sentence granularity and sentence structure. Subjective and objective experiments prove that compared with existing models, the proposed model effectively improves the semantic logic and emotional accuracy of replies. This model also significantly enhances the diversity of replies, largely overcoming the shortcomings of traditional models that generate safe replies.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {8},
numpages = {24},
keywords = {deep learning, Bayesian neural network, affective computing, natural language processing, Emotional conversation generation}
}

@article{10.1145/3365367,
author = {Rijke, Maarten de},
title = {Reviewers for <i>ACM Transactions on Information Systems</i> Volume 37},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3365367},
doi = {10.1145/3365367},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {49},
numpages = {6}
}

@article{10.1145/3352592,
author = {Lin, Xiao and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
title = {Enhancing Personalized Recommendation by Implicit Preference Communities Modeling},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3352592},
doi = {10.1145/3352592},
abstract = {Recommender systems aim to capture user preferences and provide accurate recommendations to users accordingly. For each user, there usually exist others with similar preferences, and a collection of users may also have similar preferences with each other, thus forming a community. However, such communities may not necessarily be explicitly given, and the users inside the same communities may not know each other; they are formally defined and named Implicit Preference Communities (IPCs) in this article. By enriching user preferences with the information of other users in the communities, the performance of recommender systems can also be enhanced.Historical explicit ratings are a good resource to construct the IPCs of users but is usually sparse. Meanwhile, user preferences are easily affected by their social connections, which can be jointly used for IPC modeling with the ratings. However, this imposes two challenges for model design. First, the rating and social domains are heterogeneous; thus, it is challenging to coordinate social information and rating behaviors for a same learning task. Therefore, transfer learning is a good strategy for IPC modeling. Second, the communities are not explicitly labeled, and existing supervised learning approaches do not fit the requirement of IPC modeling. As co-clustering is an effective unsupervised learning approach for discovering block structures in high-dimensional data, it is a cornerstone for discovering the structure of IPCs.In this article, we propose a recommendation model with Implicit Preference Communities from user ratings and social connections. To tackle the unsupervised learning limitation, we design a Bayesian probabilistic graphical model to capture the IPC structure for recommendation. Meanwhile, following the spirit of transfer learning, both rating behaviors and social connections are introduced into the model by parameter sharing. Moreover, Gibbs sampling-based algorithms are proposed for parameter inferences of the models. Furthermore, to meet the need for online scenarios when the data arrive sequentially as a stream, a novel online sampling-based parameter inference algorithm for recommendation is proposed. To the best of our knowledge, this is the first attempt to propose and formally define the concept of IPC.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {48},
numpages = {32},
keywords = {transfer learning, Bayesian inference, online learning, implicit preference community, Recommendation}
}

@article{10.1145/3361738,
author = {Ai, Qingyao and Zhang, Yongfeng and Bi, Keping and Croft, W. Bruce},
title = {Explainable Product Search with a Dynamic Relation Embedding Model},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3361738},
doi = {10.1145/3361738},
abstract = {Product search is one of the most popular methods for customers to discover products online. Most existing studies on product search focus on developing effective retrieval models that rank items by their likelihood to be purchased. However, they ignore the problem that there is a gap between how systems and customers perceive the relevance of items. Without explanations, users may not understand why product search engines retrieve certain items for them, which consequentially leads to imperfect user experience and suboptimal system performance in practice. In this work, we tackle this problem by constructing explainable retrieval models for product search. Specifically, we propose to model the “search and purchase” behavior as a dynamic relation between users and items, and create a dynamic knowledge graph based on both the multi-relational product data and the context of the search session. Ranking is conducted based on the relationship between users and items in the latent space, and explanations are generated with logic inferences and entity soft matching on the knowledge graph. Empirical experiments show that our model, which we refer to as the Dynamic Relation Embedding Model (DREM), significantly outperforms the state-of-the-art baselines and has the ability to produce reasonable explanations for search results.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {4},
numpages = {29},
keywords = {relation embedding, Product search, knowledge graph, explainable model}
}

@article{10.1145/3362651,
author = {Zhang, Xiaoying and Xie, Hong and Zhao, Junzhou and Lui, John C. S.},
title = {Understanding Assimilation-Contrast Effects in Online Rating Systems: Modelling, Debiasing, and Applications},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3362651},
doi = {10.1145/3362651},
abstract = {“Unbiasedness,” which is an important property to ensure that users’ ratings indeed reflect their true evaluations of products, is vital both in shaping consumer purchase decisions and providing reliable recommendations in online rating systems. Recent experimental studies showed that distortions from historical ratings would ruin the unbiasedness of subsequent ratings. How to “discover” historical distortions in each single rating (or at the micro-level), and perform the “debiasing operations” are our main objective. Using 42M real customer ratings, we first show that users either “assimilate” or “contrast” to historical ratings under different scenarios, which can be further explained by a well-known psychological argument: the “Assimilate-Contrast” theory. This motivates us to propose the Historical Influence Aware Latent Factor Model (HIALF), the “first” model for real rating systems to capture and mitigate historical distortions in each single rating. HIALF allows us to study the influence patterns of historical ratings from a modelling perspective, which perfectly matches the assimilation and contrast effects observed in experiments. Moreover, HIALF achieves significant improvements in predicting subsequent ratings and characterizing relationships in ratings. It also contributes to better recommendations, wiser consumer purchase decisions, and deeper understanding of historical distortions in both honest rating and misbehaving rating settings.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {2},
numpages = {25},
keywords = {Modelling and debiasing historical ratings’ influence, recommender systems}
}

@article{10.1145/3362505,
author = {Han, Jungkyu and Yamana, Hayato},
title = {Geographic Diversification of Recommended POIs in Frequently Visited Areas},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3362505},
doi = {10.1145/3362505},
abstract = {In the personalized Point-Of-Interest (POI) (or venue) recommendation, the diversity of recommended POIs is an important aspect. Diversity is especially important when POIs are recommended in the target users’ frequently visited areas, because users are likely to revisit such areas. In addition to the (POI) category diversity that is a popular diversification objective in recommendation domains, diversification of recommended POI locations is an interesting subject itself. Despite its importance, existing POI recommender studies generally focus on and evaluate prediction accuracy. In this article, geographical diversification&nbsp;(geo-diversification), a novel diversification concept that aims to increase recommendation coverage for a target users’ geographic areas of interest, is introduced, from which a method that improves geo-diversity as an addition to existing state-of-the-art POI recommenders is proposed. In experiments with the datasets from two real Location Based Social Networks&nbsp;(LSBNs), we first analyze the performance of four state-of-the-art POI recommenders from various evaluation perspectives including category diversity and geo-diversity that have not been examined previously. The proposed method consistently improves geo-diversity&nbsp;(CPR(geo)@20) by 5 to 12% when combined with four state-of-the-art POI recommenders with negligible prediction accuracy&nbsp;(Recall@20) loss and provides 6 to 18% geo-diversity improvement with tolerable prediction accuracy loss&nbsp;&nbsp;(up to 2.4%).},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {1},
numpages = {39},
keywords = {LBSN, POI, POI recommendation, diversity, geographical diversity, recommendation}
}

@article{10.1145/3361217,
author = {Lv, Pengtao and Meng, Xiangwu and Zhang, Yujie},
title = {BoRe: Adapting to Reader Consumption Behavior Instability for News Recommendation},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3361217},
doi = {10.1145/3361217},
abstract = {News recommendation has become an essential way to help readers discover interesting stories. While a growing line of research has focused on modeling reading preferences for news recommendation, they neglect the instability of reader consumption behaviors, i.e., consumption behaviors of readers may be influenced by other factors in addition to user interests, which degrades the recommendation effectiveness of existing methods. In this article, we propose a probabilistic generative model, BoRe, where user interests and crowd effects are used to adapt to the instability of reader consumption behaviors, and reading sequences are utilized to adapt user interests evolving over time. Further, the extreme sparsity problem in the domain of news severely hinders accurately modeling user interests and reading sequences, which discounts BoRe’s ability to adapt to the instability. Accordingly, we leverage domain-specific features to model user interests in the situation of extreme sparsity. Meanwhile, we consider groups of users instead of individuals to capture reading sequences. Besides, we study how to reduce the computation to allow online application. Extensive experiments have been conducted to evaluate the effectiveness and efficiency of BoRe on real-world datasets. The experimental results show the superiority of BoRe, compared with the state-of-the-art competing methods.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {3},
numpages = {33},
keywords = {instability, probabilistic generative model, News recommendation, domain-specific feature, reader consumption behavior}
}

@article{10.1145/3360488,
author = {Liu, Huafeng and Jing, Liping and Qian, Yuhua and Yu, Jian},
title = {Adaptive Local Low-Rank Matrix Approximation for Recommendation},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3360488},
doi = {10.1145/3360488},
abstract = {Low-rank matrix approximation (LRMA) has attracted more and more attention in the community of recommendation. Even though LRMA-based recommendation methods (including Global LRMA and Local LRMA) obtain promising results, they suffer from the complicated structure of the large-scale and sparse rating matrix, especially when the underlying system includes a large set of items with various types and a huge amount of users with diverse interests. Thus, they have to predefine the important parameters, such as the rank of the rating matrix and the number of submatrices. Moreover, most existing Local LRMA methods are usually designed in a two-phase separated framework and do not consider the missing mechanisms of rating matrix. In this article, a non-parametric unified Bayesian graphical model is proposed for Adaptive Local low-rank Matrix Approximation (ALoMA). ALoMA has ability to simultaneously identify rating submatrices, determine the optimal rank for each submatrix, and learn the submatrix-specific user/item latent factors. Meanwhile, the missing mechanism is adopted to characterize the whole rating matrix. These four parts are seamlessly integrated and enhance each other in a unified framework. Specifically, the user-item rating matrix is adaptively divided into proper number of submatrices in ALoMA by exploiting the Chinese Restaurant Process. For each submatrix, by considering both global/local structure information and missing mechanisms, the latent user/item factors are identified in an optimal latent space by adopting automatic relevance determination technique. We theoretically analyze the model’s generalization error bounds and give an approximation guarantee. Furthermore, an efficient Gibbs sampling-based algorithm is designed to infer the proposed model. A series of experiments have been conducted on six real-world datasets (Epinions, Douban, Dianping, Yelp, Movielens (10M), and Netflix). The results demonstrate that ALoMA outperforms the state-of-the-art LRMA-based methods and can easily provide interpretable recommendation results.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {45},
numpages = {34},
keywords = {Recommendation system, probabilistic graphical model, clustering}
}

@article{10.1145/3360487,
author = {Raiber, Fiana and Kurland, Oren},
title = {Relevance Feedback: The Whole Is Inferior to the Sum of Its Parts},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3360487},
doi = {10.1145/3360487},
abstract = {Document retrieval methods that utilize relevance feedback often induce a single query model from the set of feedback documents, specifically, the relevant documents. We empirically show that for a few state-of-the-art query-model induction methods, retrieval performance can be significantly improved by constructing the query model from a subset of the relevant documents rather than from all of them. Motivated by this finding, we propose a new approach for relevance-feedback-based retrieval. The approach, derived from the risk minimization framework, is based on utilizing multiple query models induced from all subsets of the given relevant documents. Empirical evaluation shows that the approach posts performance that is statistically significantly better than that of applying the standard practice of utilizing a single query model induced from the relevant documents. While the average relative improvements are small to moderate, the robustness of the approach is substantially higher than that of a variety of reference comparison methods that address various challenges in using relevance feedback.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {44},
numpages = {28},
keywords = {relevance feedback, Ad hoc retrieval}
}

@article{10.1145/3345001,
author = {Benham, Rodger and Mackenzie, Joel and Moffat, Alistair and Culpepper, J. Shane},
title = {Boosting Search Performance Using Query Variations},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3345001},
doi = {10.1145/3345001},
abstract = {Rank fusion is a powerful technique that allows multiple sources of information to be combined into a single result set. Query variations covering the same information need represent one way in which different sources of information might arise. However, when implemented in the obvious manner, fusion over query variations is not cost-effective, at odds with the usual web-search requirement for strict per-query efficiency guarantees. In this work, we propose a novel solution to query fusion by splitting the computation into two parts: one phase that is carried out offline, to generate pre-computed centroid answers for queries addressing broadly similar information needs, and then a second online phase that uses the corresponding topic centroid to compute a result page for each query. To achieve this, we make use of score-based fusion algorithms whose costs can be amortized via the pre-processing step and that can then be efficiently combined during subsequent per-query re-ranking operations. Experimental results using the ClueWeb12B collection and the UQV100 query variations demonstrate that centroid-based approaches allow improved retrieval effectiveness at little or no loss in query throughput or latency and within reasonable pre-processing requirements. We additionally show that queries that do not match any of the pre-computed clusters can be accurately identified and efficiently processed in our proposed ranking pipeline.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {41},
numpages = {25},
keywords = {Rank fusion, effectiveness, experimentation, efficiency, dynamic pruning}
}

@article{10.1145/3360486,
author = {Zheng, Yukun and Mao, Jiaxin and Liu, Yiqun and Luo, Cheng and Zhang, Min and Ma, Shaoping},
title = {Constructing Click Model for Mobile Search with Viewport Time},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3360486},
doi = {10.1145/3360486},
abstract = {A series of click models has been proposed to extract accurate and unbiased relevance feedback from valuable yet noisy click-through data in search logs. Previous works have shown that users search behavior in mobile and desktop scenarios are rather different in many aspects, therefore, the click models designed for desktop search may not be effective in the mobile context. To address this problem, we propose two novel click models for mobile search: (1) Mobile Click Model (MCM), which models click necessity bias and examination satisfaction bias; (2) Viewport Time Click Model (VTCM), which further extends MCM by utilizing the viewport time. Extensive experiments on large-scale real mobile search logs show that: (1) MCM and VTCM outperform existing models in predicting users’ clicks and estimating result relevance; (2) MCM and VTCM can extract richer information, such as the click necessity of search results and the probability of user satisfaction, from mobile click logs; (3) By modeling the viewport time distributions of heterogeneous results, VTCM can bring a significant improvement over MCM in click prediction and relevance estimation tasks. Our proposed click models can help better understand user behavior patterns in mobile search and improve the ranking performance of mobile search engines.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {43},
numpages = {34},
keywords = {mobile search, Click model, web search, viewport time}
}

@article{10.1145/3345557,
author = {Zhang, Richong and Wang, Yue and Mao, Yongyi and Huai, Jinpeng},
title = {Question Answering in Knowledge Bases: A Verification Assisted Model with Iterative Training},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3345557},
doi = {10.1145/3345557},
abstract = {Question answering over knowledge bases aims to take full advantage of the information in knowledge bases with the ultimate purpose of returning answers to questions. To access the substantial knowledge within the KB, many model architectures are hindered by the bottleneck of accurately predicting relations that connect subject entities in questions to object entities in the knowledge base. To break the bottleneck, this article presents a novel model architecture, APVA, which includes a verification mechanism to check the correctness of predicted relations. Specifically, APVA takes advantage of KB-based information to improve relation prediction but verifies the correctness of the predicted relation by means of simple negative sampling in a logistic regression framework. The APVA architecture offers a natural way to integrate an iterative training procedure, which we call turbo training. Accordingly, we introduce APVA-TURBO to perform question answering over knowledge bases. We demonstrate extensive experiments to show that APVA-TURBO outperforms existing approaches on question answering.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {40},
numpages = {26},
keywords = {Knowledge base, question answering}
}

@article{10.1145/3354187,
author = {Li, Xin and Han, Dongcheng and He, Jing and Liao, Lejian and Wang, Mingzhong},
title = {Next and Next New POI Recommendation via Latent Behavior Pattern Inference},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3354187},
doi = {10.1145/3354187},
abstract = {Next and next new point-of-interest (POI) recommendation are essential instruments in promoting customer experiences and business operations related to locations. However, due to the sparsity of the check-in records, they still remain insufficiently studied. In this article, we propose to utilize personalized latent behavior patterns learned from contextual features, e.g., time of day, day of week, and location category, to improve the effectiveness of the recommendations. Two variations of models are developed, including GPDM, which learns a fixed pattern distribution for all users; and PPDM, which learns personalized pattern distribution for each user. In both models, a soft-max function is applied to integrate the personalized Markov chain with the latent patterns, and a sequential Bayesian Personalized Ranking (S-BPR) is applied as the optimization criterion. Then, Expectation Maximization (EM) is in charge of finding optimized model parameters. Extensive experiments on three large-scale commonly adopted real-world LBSN data sets prove that the inclusion of location category and latent patterns helps to boost the performance of POI recommendations. Specifically, our models in general significantly outperform other state-of-the-art methods for both next and next new POI recommendation tasks. Moreover, our models are capable of making accurate recommendations regardless of the short/long duration or distance.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {46},
numpages = {28},
keywords = {latent behavior patterns, Next POI recommendation, next new POI recommendation}
}

@article{10.1145/3349527,
author = {Tamine, Lynda and Soulier, Laure and Nguyen, Gia-Hung and Souf, Nathalie},
title = {Offline versus Online Representation Learning of Documents Using External Knowledge},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3349527},
doi = {10.1145/3349527},
abstract = {An intensive recent research work investigated the combined use of hand-curated knowledge resources and corpus-driven resources to learn effective text representations. The overall learning process could be run by online revising the learning objective or by offline refining an original learned representation. The differentiated impact of each of the learning approaches on the quality of the learned representations has not been studied so far in the literature. This article focuses on the design of comparable offline vs. online knowledge-enhanced document representation learning models and the comparison of their effectiveness using a set of standard IR and NLP downstream tasks. The results of quantitative and qualitative analyses show that (1) offline vs. online learning approaches have dissimilar result trends regarding the task as well as the dataset distribution counts with regard to domain application; (2) while considering external knowledge resources is undoubtedly beneficial, the way used to express relational constraints could affect semantic inference effectiveness. The findings of this work present opportunities for the design of future representation learning models, but also for providing insights about the evaluation of such models.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {42},
numpages = {34},
keywords = {information retrieval, knowledge resources, Representation learning, natural language processing}
}

@article{10.1145/3357154,
author = {Du, Xiaoyu and He, Xiangnan and Yuan, Fajie and Tang, Jinhui and Qin, Zhiguang and Chua, Tat-Seng},
title = {Modeling Embedding Dimension Correlations via Convolutional Neural Collaborative Filtering},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3357154},
doi = {10.1145/3357154},
abstract = {As the core of recommender systems, collaborative filtering (CF) models the affinity between a user and an item from historical user-item interactions, such as clicks, purchases, and so on. Benefiting from the strong representation power, neural networks have recently revolutionized the recommendation research, setting up a new standard for CF. However, existing neural recommender models do not explicitly consider the correlations among embedding dimensions, making them less effective in modeling the interaction function between users and items. In this work, we emphasize on modeling the correlations among embedding dimensions in neural networks to pursue higher effectiveness for CF. We propose a novel and general neural collaborative filtering framework—namely, ConvNCF, which is featured with two designs: (1) applying outer product on user embedding and item embedding to explicitly model the pairwise correlations between embedding dimensions, and (2) employing convolutional neural network above the outer product to learn the high-order correlations among embedding dimensions. To justify our proposal, we present three instantiations of ConvNCF by using different inputs to represent a user and conduct experiments on two real-world datasets. Extensive results verify the utility of modeling embedding dimension correlations with ConvNCF, which outperforms several competitive CF methods.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {47},
numpages = {22},
keywords = {Neural collaborative filtering, convolutional neural network, recommender system, embedding dimension correlation}
}

@article{10.1145/3343117,
author = {Chen, Wanyu and Cai, Fei and Chen, Honghui and Rijke, Maarten De},
title = {Joint Neural Collaborative Filtering for Recommender Systems},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3343117},
doi = {10.1145/3343117},
abstract = {We propose a Joint Neural Collaborative Filtering (J-NCF) method for recommender systems. The J-NCF model applies a joint neural network that couples deep feature learning and deep interaction modeling with a rating matrix. Deep feature learning extracts feature representations of users and items with a deep learning architecture based on a user-item rating matrix. Deep interaction modeling captures non-linear user-item interactions with a deep neural network using the feature representations generated by the deep feature learning process as input. J-NCF enables the deep feature learning and deep interaction modeling processes to optimize each other through joint training, which leads to improved recommendation performance. In addition, we design a new loss function for optimization that takes both implicit and explicit feedback, point-wise and pair-wise loss into account.Experiments on several real-world datasets show significant improvements of J-NCF over state-of-the-art methods, with improvements of up to 8.24% on the MovieLens 100K dataset, 10.81% on the MovieLens 1M dataset, and 10.21% on the Amazon Movies dataset in terms of HR@10. NDCG@10 improvements are 12.42%, 14.24%, and 15.06%, respectively. We also conduct experiments to evaluate the scalability and sensitivity of J-NCF. Our experiments show that the J-NCF model has a competitive recommendation performance with inactive users and different degrees of data sparsity when compared to state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {39},
numpages = {30},
keywords = {Neural recommendation, collaborative filtering}
}

@article{10.1145/3317612,
author = {Zhang, Zheng and Huang, Minlie and Zhao, Zhongzhou and Ji, Feng and Chen, Haiqing and Zhu, Xiaoyan},
title = {Memory-Augmented Dialogue Management for Task-Oriented Dialogue Systems},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3317612},
doi = {10.1145/3317612},
abstract = {Dialogue management (DM) is responsible for predicting the next action of a dialogue system according to the current dialogue state and thus plays a central role in task-oriented dialogue systems. Since DM requires having access not only to local utterances but also to the global semantics of the entire dialogue session, modeling the long-range history information is a critical issue. To this end, we propose MAD, a novel memory-augmented dialogue management model that employs a memory controller and two additional memory structures (i.e., a slot-value memory and an external memory). The slot-value memory tracks the dialogue state by memorizing and updating the values of semantic slots (i.e., cuisine, price, and location), and the external memory augments the representation of hidden states of traditional recurrent neural networks by storing more context information. To update the dialogue state efficiently, we also propose slot-level attention on user utterances to extract specific semantic information for each slot. Experiments show that our model can obtain state-of-the-art performance and outperforms existing baselines.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {34},
numpages = {30},
keywords = {dialogue state, attention, neural network, Dialogue management, memory network}
}

@article{10.1145/3329188,
author = {Liu, Yiqun and Zhang, Junqi and Mao, Jiaxin and Zhang, Min and Ma, Shaoping and Tian, Qi and Lu, Yanxiong and Lin, Leyu},
title = {Search Result Reranking with Visual and Structure Information Sources},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3329188},
doi = {10.1145/3329188},
abstract = {Relevance estimation is among the most important tasks in the ranking of search results. Current methodologies mainly concentrate on text matching, link analysis, and user behavior models. However, users judge the relevance of search results directly from Search Engine Result Pages (SERPs), which provide valuable signals for reranking. In this article, we propose two different approaches to aggregate the visual, structure, as well as textual information sources of search results in relevance estimation. The first one is a late-fusion framework named Joint Relevance Estimation model (JRE). JRE estimates the relevance independently from screenshots, textual contents, and HTML source codes of search results and jointly makes the final decision through an inter-modality attention mechanism. The second one is an early-fusion framework named Tree-based Deep Neural Network (TreeNN), which embeds the texts and images into the HTML parse tree through a recursive process. To evaluate the performance of the proposed models, we construct a large-scale practical Search Result Relevance (SRR) dataset that consists of multiple information sources and relevance labels of over 60,000 search results. Experimental results show that the proposed two models achieve better performance than state-of-the-art ranking solutions as well as the original rankings of commercial search engines.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {38},
numpages = {38},
keywords = {Multimodal, information retrieval, ranking, relevance}
}

@article{10.1145/3326065,
author = {Esuli, Andrea and Moreo, Alejandro and Sebastiani, Fabrizio},
title = {Funnelling: A New Ensemble Method for Heterogeneous Transfer Learning and Its Application to Cross-Lingual Text Classification},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3326065},
doi = {10.1145/3326065},
abstract = {Cross-lingual Text Classification (CLC) consists of automatically classifying, according to a common set C of classes, documents each written in one of a set of languages L, and doing so more accurately than when “na\"{\i}vely” classifying each document via its corresponding language-specific classifier. To obtain an increase in the classification accuracy for a given language, the system thus needs to also leverage the training examples written in the other languages. We tackle “multilabel” CLC via funnelling, a new ensemble learning method that we propose here. Funnelling consists of generating a two-tier classification system where all documents, irrespective of language, are classified by the same (second-tier) classifier. For this classifier, all documents are represented in a common, language-independent feature space consisting of the posterior probabilities generated by first-tier, language-dependent classifiers. This allows the classification of all test documents, of any language, to benefit from the information present in all training documents, of any language. We present substantial experiments, run on publicly available multilingual text collections, in which funnelling is shown to significantly outperform a number of state-of-the-art baselines. All code and datasets (in vector form) are made publicly available.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {37},
numpages = {30},
keywords = {cross-lingual text classification, Funnelling, transfer learning, heterogeneous transfer learning}
}

@article{10.1145/3322128,
author = {Arguello, Jaime and Choi, Bogeum},
title = {The Effects of Working Memory, Perceptual Speed, and Inhibition in Aggregated Search},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3322128},
doi = {10.1145/3322128},
abstract = {Prior work has studied how different characteristics of individual users (e.g., personality traits and cognitive abilities) can impact search behaviors and outcomes. We report on a laboratory study (N = 32) that investigated the effects of three different cognitive abilities (perceptual speed, working memory, and inhibition) in the context of aggregated search. Aggregated search systems combine results from multiple heterogeneous sources (or verticals) in a unified presentation. Participants in our study interacted with two different aggregated search interfaces (a within-subjects design) that differed based on the extent to which the layout distinguished between results originating from different verticals. The interleaved interface merged results from different verticals in a fairly unconstrained fashion. Conversely, the blocked interface displayed results from the same vertical as a group, displayed each group of vertical results in the same region on the SERP for every query, and used a border around each group of vertical results to help distinguish among results from different sources. We investigated three research questions (RQ1--RQ3). Specifically, we investigated the effects of the interface condition and each cognitive ability on three types of outcomes: (RQ1) participants’ levels of workload, (RQ2) participants’ levels of user engagement, and (RQ3) participants’ search behaviors. Our results found different main and interaction effects. Perceptual speed and inhibition did not significantly affect participants’ workload and user engagement but significantly affected their search behaviors. Specifically, with the interleaved interface, participants with lower perceptual speed had more difficulty finding relevant results on the SERP, and participants with lower inhibitory attention control searched at a slower pace. Working memory did not have a strong effect on participants’ behaviors but had several significant effects on the levels of workload and user engagement reported by participants. Specifically, participants with lower working memory reported higher levels of workload and lower levels of user engagement. We discuss implications of our results for designing aggregated search interfaces that are well suited for users with different cognitive abilities.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {36},
numpages = {34},
keywords = {perceptual speed, Cognitive abilities, search behaviors, workload, inhibition, user engagement, aggregated search, working memory}
}

@article{10.1145/3320118,
author = {Wu, Zhijing and Zhou, Ke and Liu, Yiqun and Zhang, Min and Ma, Shaoping},
title = {Does Diversity Affect User Satisfaction in Image Search},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3320118},
doi = {10.1145/3320118},
abstract = {Diversity has been taken into consideration by existing Web image search engines in ranking search results. However, there is no thorough investigation of how diversity affects user satisfaction in image search. In this article, we address the following questions: (1) How do different factors, such as content and visual presentations, affect users’ perception of diversity? (2) How does search result diversity affect user satisfaction with different search intents? To answer those questions, we conduct a set of laboratory user studies to collect users’ perceived diversity annotations and search satisfaction. We find that the existence of nearly duplicated image results has the largest impact on users’ perceived diversity, followed by the similarity in content and visual presentations. Besides these findings, we also investigate the relationship between diversity and satisfaction in image search. Specifically, we find that users’ preference for diversity varies across different search intents. When users want to collect information or save images for further usage (the Locate search tasks), more diversified result lists lead to higher satisfaction levels. The insights may help commercial image search engines to design better result ranking strategies and evaluation metrics.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {35},
numpages = {30},
keywords = {Image search, user satisfaction, image diversity}
}

@article{10.1145/3314578,
author = {Xue, Feng and He, Xiangnan and Wang, Xiang and Xu, Jiandong and Liu, Kai and Hong, Richang},
title = {Deep Item-Based Collaborative Filtering for Top-N Recommendation},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3314578},
doi = {10.1145/3314578},
abstract = {Item-based Collaborative Filtering (ICF) has been widely adopted in recommender systems in industry, owing to its strength in user interest modeling and ease in online personalization. By constructing a user’s profile with the items that the user has consumed, ICF recommends items that are similar to the user’s profile. With the prevalence of machine learning in recent years, significant processes have been made for ICF by learning item similarity (or representation) from data. Nevertheless, we argue that most existing works have only considered linear and shallow relationships between items, which are insufficient to capture the complicated decision-making process of users.In this article, we propose a more expressive ICF solution by accounting for the nonlinear and higher-order relationships among items. Going beyond modeling only the second-order interaction (e.g., similarity) between two items, we additionally consider the interaction among all interacted item pairs by using nonlinear neural networks. By doing this, we can effectively model the higher-order relationship among items, capturing more complicated effects in user decision-making. For example, it can differentiate which historical itemsets in a user’s profile are more important in affecting the user to make a purchase decision on an item. We treat this solution as a deep variant of ICF, thus term it as DeepICF. To justify our proposal, we perform empirical studies on two public datasets from MovieLens and Pinterest. Extensive experiments verify the highly positive effect of higher-order item interaction modeling with nonlinear neural networks. Moreover, we demonstrate that by more fine-grained second-order interaction modeling with attention network, the performance of our DeepICF method can be further improved.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {33},
numpages = {25},
keywords = {implicit feedback, Collaborative filtering, item-based CF, deep learning, neural networks}
}

@article{10.1145/3309994,
author = {Shao, Yunqiu and Liu, Yiqun and Zhang, Fan and Zhang, Min and Ma, Shaoping},
title = {On Annotation Methodologies for Image Search Evaluation},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3309994},
doi = {10.1145/3309994},
abstract = {Image search engines differ significantly from general web search engines in the way of presenting search results. The difference leads to different interaction and examination behavior patterns, and therefore requires changes in evaluation methodologies. However, evaluation of image search still utilizes the methods for general web search. In particular, offline metrics are calculated based on coarse-fine topical relevance judgments with the assumption that users examine results in a sequential manner.In this article, we investigate annotation methods via crowdsourcing for image search evaluation based on a lab-based user study. Using user satisfaction as the golden standard, we make several interesting findings. First, instead of item-based annotation, annotating relevance in a row-based way is more efficient without hurting performance. Second, besides topical relevance, image quality plays a crucial role when evaluating the image search results, and the importance of image quality changes with search intent. Third, compared to traditional four-level scales, the fine-grain annotation method outperforms significantly. To our best knowledge, our work is the first to systematically study how diverse factors in data annotation impact image search evaluation. Our results suggest different strategies for exploiting the crowdsourcing to get data annotated under different conditions.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {29},
numpages = {32},
keywords = {user satisfaction, offline evaluation, Image search, crowdsourcing annotation}
}

@article{10.1145/3309546,
author = {Guan, Xinyu and Cheng, Zhiyong and He, Xiangnan and Zhang, Yongfeng and Zhu, Zhibo and Peng, Qinke and Chua, Tat-Seng},
title = {Attentive Aspect Modeling for Review-Aware Recommendation},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3309546},
doi = {10.1145/3309546},
abstract = {In recent years, many studies extract aspects from user reviews and integrate them with ratings for improving the recommendation performance. The common aspects mentioned in a user’s reviews and a product’s reviews indicate indirect connections between the user and product. However, these aspect-based methods suffer from two problems. First, the common aspects are usually very sparse, which is caused by the sparsity of user-product interactions and the diversity of individual users’ vocabularies. Second, a user’s interests on aspects could be different with respect to different products, which are usually assumed to be static in existing methods. In this article, we propose an Attentive Aspect-based Recommendation Model (AARM) to tackle these challenges. For the first problem, to enrich the aspect connections between user and product, besides common aspects, AARM also models the interactions between synonymous and similar aspects. For the second problem, a neural attention network which simultaneously considers user, product, and aspect information is constructed to capture a user’s attention toward aspects when examining different products. Extensive quantitative and qualitative experiments show that AARM can effectively alleviate the two aforementioned problems and significantly outperforms several state-of-the-art recommendation methods on the top-N recommendation task.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {28},
numpages = {27},
keywords = {Top-N recommendation, attention mechanism, aspects, neural network}
}

@article{10.1145/3310364,
author = {Ferro, Nicola and Kim, Yubin and Sanderson, Mark},
title = {Using Collection Shards to Study Retrieval Performance Effect Sizes},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3310364},
doi = {10.1145/3310364},
abstract = {Despite the bulk of research studying how to more accurately compare the performance of IR systems, less attention is devoted to better understanding the different factors that play a role in such performance and how they interact. This is the case of shards, i.e., partitioning a document collection into sub-parts, which are used for many different purposes, ranging from efficiency to selective search or making test collection evaluation more accurate. In all these cases, there is empirical knowledge supporting the importance of shards, but we lack actual models that allow us to measure the impact of shards on system performance and how they interact with topics and systems. We use the general linear mixed model framework and present a model that encompasses the experimental factors of system, topic, shard, and their interaction effects. This detailed model allows us to more accurately estimate differences between the effect of various factors. We study shards created by a range of methods used in prior work and better explain observations noted in prior work in a principled setting and offer new insights. Notably, we discover that the topic*shard interaction effect, in particular, is a large effect almost globally across all datasets, an observation that, to our knowledge, has not been measured before.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {30},
numpages = {40},
keywords = {effectiveness model, ANOVA, Shard effect, GLMM}
}

@article{10.1145/3312738,
author = {Wang, Hongwei and Zhang, Fuzheng and Wang, Jialin and Zhao, Miao and Li, Wenjie and Xie, Xing and Guo, Minyi},
title = {Exploring High-Order User Preference on the Knowledge Graph for Recommender Systems},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3312738},
doi = {10.1145/3312738},
abstract = {To address the sparsity and cold-start problem of collaborative filtering, researchers usually make use of side information, such as social networks or item attributes, to improve the performance of recommendation. In this article, we consider the knowledge graph (KG) as the source of side information. To address the limitations of existing embedding-based and path-based methods for KG-aware recommendation, we propose RippleNet, an end-to-end framework that naturally incorporates the KG into recommender systems. RippleNet has two versions: (1) The outward propagation version, which is analogous to the actual ripples on water, stimulates the propagation of user preferences over the set of knowledge entities by automatically and iteratively extending a user’s potential interests along links in the KG. The multiple “ripples” activated by a user’s historically clicked items are thus superposed to form the preference distribution of the user with respect to a candidate item. (2) The inward aggregation version aggregates and incorporates the neighborhood information biasedly when computing the representation of a given entity. The neighborhood can be extended to multiple hops away to model high-order proximity and capture users’ long-distance interests. In addition, we intuitively demonstrate how a KG assists with recommender systems in RippleNet, and we also find that RippleNet provides a new perspective of explainability for the recommended results in terms of the KG. Through extensive experiments on real-world datasets, we demonstrate that both versions of RippleNet achieve substantial gains in a variety of scenarios, including movie, book, and news recommendations, over several state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {32},
numpages = {26},
keywords = {inward aggregation, Recommender systems, outward propagation, knowledge graph}
}

@article{10.1145/3312528,
author = {Li, Xinyi and Chen, Yifan and Pettit, Benjamin and Rijke, Maarten De},
title = {Personalised Reranking of Paper Recommendations Using Paper Content and User Behavior},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3312528},
doi = {10.1145/3312528},
abstract = {Academic search engines have been widely used to access academic papers, where users’ information needs are explicitly represented as search queries. Some modern recommender systems have taken one step further by predicting users’ information needs without the presence of an explicit query. In this article, we examine an academic paper recommender that sends out paper recommendations in email newsletters, based on the users’ browsing history on the academic search engine. Specifically, we look at users who regularly browse papers on the search engine, and we sign up for the recommendation newsletters for the first time. We address the task of reranking the recommendation candidates that are generated by a production system for such users.We face the challenge that the users on whom we focus have not interacted with the recommender system before, which is a common scenario that every recommender system encounters when new users sign up. We propose an approach to reranking candidate recommendations that utilizes both paper content and user behavior. The approach is designed to suit the characteristics unique to our academic recommendation setting. For instance, content similarity measures can be used to find the closest match between candidate recommendations and the papers previously browsed by the user. To this end, we use a knowledge graph derived from paper metadata to compare entity similarities (papers, authors, and journals) in the embedding space. Since the users on whom we focus have no prior interactions with the recommender system, we propose a model to learn a mapping from users’ browsed articles to user clicks on the recommendations. We combine both content and behavior into a hybrid reranking model that outperforms the production baseline significantly, providing a relative 13% increase in Mean Average Precision and 28% in Precision@1.Moreover, we provide a detailed analysis of the model components, highlighting where the performance boost comes from. The obtained insights reveal useful components for the reranking process and can be generalized to other academic recommendation settings as well, such as the utility of graph embedding similarity. Also, recent papers browsed by users provide stronger evidence for recommendation than historical ones.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {31},
numpages = {23},
keywords = {Academic search, reranking, paper recommendation}
}

@article{10.1145/3309547,
author = {Feng, Fuli and He, Xiangnan and Wang, Xiang and Luo, Cheng and Liu, Yiqun and Chua, Tat-Seng},
title = {Temporal Relational Ranking for Stock Prediction},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3309547},
doi = {10.1145/3309547},
abstract = {Stock prediction aims to predict the future trends of a stock in order to help investors make good investment decisions. Traditional solutions for stock prediction are based on time-series models. With the recent success of deep neural networks in modeling sequential data, deep learning has become a promising choice for stock prediction.However, most existing deep learning solutions are not optimized toward the target of investment, i.e., selecting the best stock with the highest expected revenue. Specifically, they typically formulate stock prediction as a classification (to predict stock trends) or a regression problem (to predict stock prices). More importantly, they largely treat the stocks as independent of each other. The valuable signal in the rich relations between stocks (or companies), such as two stocks are in the same sector and two companies have a supplier-customer relation, is not considered.In this work, we contribute a new deep learning solution, named Relational Stock Ranking (RSR), for stock prediction. Our RSR method advances existing solutions in two major aspects: (1) tailoring the deep learning models for stock ranking, and (2) capturing the stock relations in a time-sensitive manner. The key novelty of our work is the proposal of a new component in neural network modeling, named Temporal Graph Convolution, which jointly models the temporal evolution and relation network of stocks. To validate our method, we perform back-testing on the historical data of two stock markets, NYSE and NASDAQ. Extensive experiments demonstrate the superiority of our RSR method. It outperforms state-of-the-art stock prediction solutions achieving an average return ratio of 98% and 71% on NYSE and NASDAQ, respectively.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {27},
numpages = {30},
keywords = {learning to rank, graph-based learning, Stock prediction}
}

@article{10.1145/3309543,
author = {Zhan, Xueying and Wang, Yaowei and Rao, Yanghui and Li, Qing},
title = {Learning from Multi-Annotator Data: A Noise-Aware Classification Framework},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3309543},
doi = {10.1145/3309543},
abstract = {In the field of sentiment analysis and emotion detection in social media, or other tasks such as text classification involving supervised learning, researchers rely more heavily on large and accurate labelled training datasets. However, obtaining large-scale labelled datasets is time-consuming and high-quality labelled datasets are expensive and scarce. To deal with these problems, online crowdsourcing systems provide us an efficient way to accelerate the process of collecting training data via distributing the enormous tasks to various annotators to help create large amounts of labelled data at an affordable cost. Nowadays, these crowdsourcing platforms are heavily needed in dealing with social media text, since the social network platforms (e.g., Twitter) generate huge amounts of data in textual form everyday. However, people from different social and knowledge backgrounds have different views on various texts, which may lead to noisy labels. The existing noisy label aggregation/refinement algorithms mostly focus on aggregating labels from noisy annotations, which would not guarantee their effectiveness on the subsequent classification/ranking tasks. In this article, we propose a noise-aware classification framework that integrates the steps of noisy label aggregation and classification. The aggregated noisy crowd labels are fed into a classifier for training, while the predicted labels are employed as feedback for adjusting the parameters at the label aggregating stage. The classification framework is suitable for directly running on crowdsourcing datasets and applies to various kinds of classification algorithms. The feedback strategy makes it possible for us to find optimal parameters instead of using known data for parameter selection. Simulation experiments demonstrate that our method provide significant label aggregation performance for both binary and multiple classification tasks under various noisy environments. Experimenting on real-world data validates the feasibility of our framework in real noise data and helps us verify the reasonableness of the simulated experiment settings.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {26},
numpages = {28},
keywords = {crowdsourcing, Social media, emotion detection, sentiment analysis}
}

@article{10.1145/3300196,
author = {Sousa, Daniel Xavier and Canuto, S\'{e}rgio and Gon\c{c}alves, Marcos Andr\'{e} and Rosa, Thierson Couto and Martins, Wellington Santos},
title = {Risk-Sensitive Learning to Rank with Evolutionary Multi-Objective Feature Selection},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3300196},
doi = {10.1145/3300196},
abstract = {Learning to Rank (L2R) is one of the main research lines in Information Retrieval. Risk-sensitive L2R is a sub-area of L2R that tries to learn models that are good on average while at the same time reducing the risk of performing poorly in a few but important queries (e.g., medical or legal queries). One way of reducing risk in learned models is by selecting and removing noisy, redundant features, or features that promote some queries to the detriment of others. This is exacerbated by learning methods that usually maximize an average metric (e.g., mean average precision (MAP) or Normalized Discounted Cumulative Gain (NDCG)). However, historically, feature selection (FS) methods have focused only on effectiveness and feature reduction as the main objectives. Accordingly, in this work, we propose to evaluate FS for L2R with an additional objective in mind, namely risk-sensitiveness. We present novel single and multi-objective criteria to optimize feature reduction, effectiveness, and risk-sensitiveness, all at the same time. We also introduce a new methodology to explore the search space, suggesting effective and efficient extensions of a well-known Evolutionary Algorithm (SPEA2) for FS applied to L2R. Our experiments show that explicitly including risk as an objective criterion is crucial to achieving a more effective and risk-sensitive performance. We also provide a thorough analysis of our methodology and experimental results.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {24},
numpages = {34},
keywords = {feature selection, Learning to rank, risk-sensitiveness}
}

@article{10.1145/3295823,
author = {Lukasik, Michal and Bontcheva, Kalina and Cohn, Trevor and Zubiaga, Arkaitz and Liakata, Maria and Procter, Rob},
title = {Gaussian Processes for Rumour Stance Classification in Social Media},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3295823},
doi = {10.1145/3295823},
abstract = {Social media tend to be rife with rumours while new reports are released piecemeal during breaking news. Interestingly, one can mine multiple reactions expressed by social media users in those situations, exploring their stance towards rumours, ultimately enabling the flagging of highly disputed rumours as being potentially false. In this work, we set out to develop an automated, supervised classifier that uses multi-task learning to classify the stance expressed in each individual tweet in a conversation around a rumour as either supporting, denying or questioning the rumour. Using a Gaussian Process classifier, and exploring its effectiveness on two datasets with very different characteristics and varying distributions of stances, we show that our approach consistently outperforms competitive baseline classifiers. Our classifier is especially effective in estimating the distribution of different types of stance associated with a given rumour, which we set forth as a desired characteristic for a rumour-tracking system that will show both ordinary users of Twitter and professional news practitioners how others orient to the disputed veracity of a rumour, with the final aim of establishing its actual truth value.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {20},
numpages = {24},
keywords = {breaking news, machine learning, rumours, stance classification, veracity classification, Social media}
}

@article{10.1145/3291756,
author = {Loni, Babak and Pagano, Roberto and Larson, Martha and Hanjalic, Alan},
title = {Top-N Recommendation with Multi-Channel Positive Feedback Using Factorization Machines},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3291756},
doi = {10.1145/3291756},
abstract = {User interactions can be considered to constitute different feedback channels, for example, view, click, like or follow, that provide implicit information on users’ preferences. Each implicit feedback channel typically carries a unary, positive-only signal that can be exploited by collaborative filtering models to generate lists of personalized recommendations. This article investigates how a learning-to-rank recommender system can best take advantage of implicit feedback signals from multiple channels. We focus on Factorization Machines (FMs) with Bayesian Personalized Ranking (BPR), a pairwise learning-to-rank method, that allows us to experiment with different forms of exploitation. We perform extensive experiments on three datasets with multiple types of feedback to arrive at a series of insights. We compare conventional, direct integration of feedback types with our proposed method, which exploits multiple feedback channels during the sampling process of training. We refer to our method as multi-channel sampling. Our results show that multi-channel sampling outperforms conventional integration, and that sampling with the relative “level” of feedback is always superior to a level-blind sampling approach. We evaluate our method experimentally on three datasets in different domains and observe that with our multi-channel sampler the accuracy of recommendations can be improved considerably compared to the state-of-the-art models. Further experiments reveal that the appropriate sampling method depends on particular properties of datasets such as popularity skewness.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {15},
numpages = {23},
keywords = {learning-to-rank, implicit feedback, Factorizaion machines, multi-channel feedback}
}

@article{10.1145/3302913,
author = {Pibiri, Giulio Ermanno and Venturini, Rossano},
title = {Handling Massive <i>N</i>-Gram Datasets Efficiently},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3302913},
doi = {10.1145/3302913},
abstract = {Two fundamental problems concern the handling of large n-gram language models: indexing, that is, compressing the n-grams and associated satellite values without compromising their retrieval speed, and estimation, that is, computing the probability distribution of the n-grams extracted from a large textual source.Performing these two tasks efficiently is vital for several applications in the fields of Information Retrieval, Natural Language Processing, and Machine Learning, such as auto-completion in search engines and machine translation.Regarding the problem of indexing, we describe compressed, exact, and lossless data structures that simultaneously achieve high space reductions and no time degradation with respect to the state-of-the-art solutions and related software packages. In particular, we present a compressed trie data structure in which each word of an n-gram following a context of fixed length k, that is, its preceding k words, is encoded as an integer whose value is proportional to the number of words that follow such context. Since the number of words following a given context is typically very small in natural languages, we lower the space of representation to compression levels that were never achieved before, allowing the indexing of billions of strings. Despite the significant savings in space, our technique introduces a negligible penalty at query time.Specifically, the most space-efficient competitors in the literature, which are both quantized and lossy, do not take less than our trie data structure and are up to 5 times slower. Conversely, our trie is as fast as the fastest competitor but also retains an advantage of up to 65% in absolute space.Regarding the problem of estimation, we present a novel algorithm for estimating modified Kneser-Ney language models that have emerged as the de-facto choice for language modeling in both academia and industry thanks to their relatively low perplexity performance. Estimating such models from large textual sources poses the challenge of devising algorithms that make a parsimonious use of the disk.The state-of-the-art algorithm uses three sorting steps in external memory: we show an improved construction that requires only one sorting step by exploiting the properties of the extracted n-gram strings. With an extensive experimental analysis performed on billions of n-grams, we show an average improvement of 4.5 times on the total runtime of the previous approach.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {25},
numpages = {41},
keywords = {Efficiency, scalability, algorithm engineering}
}

@article{10.1145/3300197,
author = {Liu, Ming and Gong, Gu and Qin, Bing and Liu, Ting},
title = {A Multi-View–Based Collective Entity Linking Method},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3300197},
doi = {10.1145/3300197},
abstract = {Facing lots of name mentions appearing on the web, entity linking is essential for many information processing applications. To improve linking accuracy, the relations between entities are usually considered in the linking process. This kind of method is called collective entity linking and can obtain high-quality results. There are two kinds of information helpful to reveal the relations between entities, i.e., contextual information and structural information of entities. Most traditional collective entity linking methods consider them separately. In fact, these two kinds of information represent entities from specific and diverse views and can enhance each other, respectively. Besides, if we look into each view closely, it can be separated into sub-views that are more meaningful. For this reason, this article proposes a multi-view–based collective entity linking algorithm, which combines several views of entities into an objective function for entity linking. The importance of each view can be valued and the linking results can be obtained along with resolving this objective function. Experimental results demonstrate that our linking algorithm can acquire higher accuracy than many state-of-the-art entity linking methods. Besides, since we simplify the entity's structure and change the entity linking to a sub-matrix searching problem, our algorithm also obtains high efficiency.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {23},
numpages = {29},
keywords = {contextual information, structural information, weighing process, Multi-view–based entity linking, gradient-descent}
}

@article{10.1145/3298988,
author = {Wu, Libing and Quan, Cong and Li, Chenliang and Wang, Qian and Zheng, Bolong and Luo, Xiangyang},
title = {A Context-Aware User-Item Representation Learning for Item Recommendation},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3298988},
doi = {10.1145/3298988},
abstract = {Both reviews and user-item interactions (i.e., rating scores) have been widely adopted for user rating prediction. However, these existing techniques mainly extract the latent representations for users and items in an independent and static manner. That is, a single static feature vector is derived to encode user preference without considering the particular characteristics of each candidate item. We argue that this static encoding scheme is incapable of fully capturing users’ preferences, because users usually exhibit different preferences when interacting with different items. In this article, we propose a novel context-aware user-item representation learning model for rating prediction, named CARL. CARL derives a joint representation for a given user-item pair based on their individual latent features and latent feature interactions. Then, CARL adopts Factorization Machines to further model higher order feature interactions on the basis of the user-item pair for rating prediction. Specifically, two separate learning components are devised in CARL to exploit review data and interaction data, respectively: review-based feature learning and interaction-based feature learning. In the review-based learning component, with convolution operations and attention mechanism, the pair-based relevant features for the given user-item pair are extracted by jointly considering their corresponding reviews. However, these features are only reivew-driven and may not be comprehensive. Hence, an interaction-based learning component further extracts complementary features from interaction data alone, also on the basis of user-item pairs. The final rating score is then derived with a dynamic linear fusion mechanism. Experiments on seven real-world datasets show that CARL achieves significantly better rating prediction accuracy than existing state-of-the-art alternatives. Also, with the attention mechanism, we show that the pair-based relevant information (i.e., context-aware information) in reviews can be highlighted to interpret the rating prediction for different user-item pairs.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {22},
numpages = {29},
keywords = {recommendation systems, neural networks, Rating prediction}
}

@article{10.1145/3295499,
author = {Qian, Tieyun and Liu, Bei and Nguyen, Quoc Viet Hung and Yin, Hongzhi},
title = {Spatiotemporal Representation Learning for Translation-Based POI Recommendation},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3295499},
doi = {10.1145/3295499},
abstract = {The increasing proliferation of location-based social networks brings about a huge volume of user check-in data, which facilitates the recommendation of points of interest (POIs). Time and location are the two most important contextual factors in the user’s decision-making for choosing a POI to visit. In this article, we focus on the spatiotemporal context-aware POI recommendation, which considers the joint effect of time and location for POI recommendation. Inspired by the recent advances in knowledge graph embedding, we propose a spatiotemporal context-aware and translation-based recommender framework (STA) to model the third-order relationship among users, POIs, and spatiotemporal contexts for large-scale POI recommendation. Specifically, we embed both users and POIs into a “transition space” where spatiotemporal contexts (i.e., a <time, location> pair) are modeled as translation vectors operating on users and POIs. We further develop a series of strategies to exploit various correlation information to address the data sparsity and cold-start issues for new spatiotemporal contexts, new users, and new POIs. We conduct extensive experiments on two real-world datasets. The experimental results demonstrate that our STA framework achieves the superior performance in terms of high recommendation accuracy, robustness to data sparsity, and effectiveness in handling the cold-start problem.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {18},
numpages = {24},
keywords = {POI recommendation, spatiotemporal aware, location-based social networks, contextual modeling}
}

@article{10.1145/3298987,
author = {Cagliero, Luca and Garza, Paolo and Baralis, Elena},
title = {ELSA: A Multilingual Document Summarization Algorithm Based on Frequent Itemsets and Latent Semantic Analysis},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3298987},
doi = {10.1145/3298987},
abstract = {Sentence-based summarization aims at extracting concise summaries of collections of textual documents. Summaries consist of a worthwhile subset of document sentences. The most effective multilingual strategies rely on Latent Semantic Analysis (LSA) and on frequent itemset mining, respectively. LSA-based summarizers pick the document sentences that cover the most important concepts. Concepts are modeled as combinations of single-document terms and are derived from a term-by-sentence matrix by exploiting Singular Value Decomposition (SVD). Itemset-based summarizers pick the sentences that contain the largest number of frequent itemsets, which represent combinations of frequently co-occurring terms. The main drawbacks of existing approaches are (i)&nbsp;the inability of LSA to consider the correlation between combinations of multiple-document terms and the underlying concepts, (ii)&nbsp;the inherent redundancy of frequent itemsets because similar itemsets may be related to the same concept, and (iii)&nbsp;the inability of itemset-based summarizers to correlate itemsets with the underlying document concepts. To overcome the issues of both of the abovementioned algorithms, we propose a new summarization approach that exploits frequent itemsets to describe all of the latent concepts covered by the documents under analysis and LSA to reduce the potentially redundant set of itemsets to a compact set of uncorrelated concepts. The summarizer selects the sentences that cover the latent concepts with minimal redundancy. We tested the summarization algorithm on both multilingual and English-language benchmark document collections. The proposed approach performed significantly better than both itemset- and LSA-based summarizers, and better than most of the other state-of-the-art approaches.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {21},
numpages = {33},
keywords = {frequent weighted itemset mining, text mining, Multilingual summarization}
}

@article{10.1145/3243652,
author = {Pan, Weike and Yang, Qiang and Cai, Wanling and Chen, Yaofeng and Zhang, Qing and Peng, Xiaogang and Ming, Zhong},
title = {Transfer to Rank for Heterogeneous One-Class Collaborative Filtering},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3243652},
doi = {10.1145/3243652},
abstract = {Heterogeneous one-class collaborative filtering is an emerging and important problem in recommender systems, where two different types of one-class feedback, i.e., purchases and browses, are available as input data. The associated challenges include ambiguity of browses, scarcity of purchases, and heterogeneity arising from different feedback. In this article, we propose to model purchases and browses from a new perspective, i.e., users’ roles of mixer, browser and purchaser. Specifically, we design a novel transfer learning solution termed role-based transfer to rank (RoToR), which contains two variants, i.e., integrative RoToR and sequential RoToR. In integrative RoToR, we leverage browses into the preference learning task of purchases, in which we take each user as a sophisticated customer (i.e., mixer) that is able to take different types of feedback into consideration. In sequential RoToR, we aim to simplify the integrative one by decomposing it into two dependent phases according to a typical shopping process. Furthermore, we instantiate both variants using different preference learning paradigms such as pointwise preference learning and pairwise preference learning. Finally, we conduct extensive empirical studies with various baseline methods on three large public datasets and find that our RoToR can perform significantly more accurate than the state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {10},
numpages = {20},
keywords = {Heterogeneous one-class collaborative filtering, role-based recommendation, one-class feedback, transfer to rank}
}

@article{10.1145/3295822,
author = {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and Wang, Yinglong and Ma, Jun and Kankanhalli, Mohan},
title = {Attentive Long Short-Term Preference Modeling for Personalized Product Search},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3295822},
doi = {10.1145/3295822},
abstract = {E-commerce users may expect different products even for the same query, due to their diverse personal preferences. It is well known that there are two types of preferences: long-term ones and short-term ones. The former refers to users’ inherent purchasing bias and evolves slowly. By contrast, the latter reflects users’ purchasing inclination in a relatively short period. They both affect users’ current purchasing intentions. However, few research efforts have been dedicated to jointly model them for the personalized product search. To this end, we propose a novel Attentive Long Short-Term Preference model, dubbed as ALSTP, for personalized product search. Our model adopts the neural networks approach to learn and integrate the long- and short-term user preferences with the current query for the personalized product search. In particular, two attention networks are designed to distinguish which factors in the short-term as well as long-term user preferences are more relevant to the current query. This unique design enables our model to capture users’ current search intentions more accurately. Our work is the first to apply attention mechanisms to integrate both long- and short-term user preferences with the given query for the personalized search. Extensive experiments over four Amazon product datasets show that our model significantly outperforms several state-of-the-art product search methods in terms of different evaluation metrics.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {19},
numpages = {27},
keywords = {long short-term preference, attention mechanism, Personalized product search}
}

@article{10.1145/3291060,
author = {Cheng, Zhiyong and Chang, Xiaojun and Zhu, Lei and Kanjirathinkal, Rose C. and Kankanhalli, Mohan},
title = {MMALFM: Explainable Recommendation by Leveraging Reviews and Images},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3291060},
doi = {10.1145/3291060},
abstract = {Personalized rating prediction is an important research problem in recommender systems. Although the latent factor model (e.g., matrix factorization) achieves good accuracy in rating prediction, it suffers from many problems including cold-start, non-transparency, and suboptimal results for individual user-item pairs. In this article, we exploit textual reviews and item images together with ratings to tackle these limitations. Specifically, we first apply a proposed multi-modal aspect-aware topic model (MATM) on text reviews and item images to model users’ preferences and items’ features from different aspects, and also estimate the aspect importance of a user toward an item. Then, the aspect importance is integrated into a novel aspect-aware latent factor model (ALFM), which learns user’s and item’s latent factors based on ratings. In particular, ALFM introduces a weight matrix to associate those latent factors with the same set of aspects in MATM, such that the latent factors could be used to estimate aspect ratings. Finally, the overall rating is computed via a linear combination of the aspect ratings, which are weighted by the corresponding aspect importance. To this end, our model could alleviate the data sparsity problem and gain good interpretability for recommendation. Besides, every aspect rating is weighted by its aspect importance, which is dependent on the targeted user’s preferences and the targeted item’s features. Therefore, it is expected that the proposed method can model a user’s preferences on an item more accurately for each user-item pair. Comprehensive experimental studies have been conducted on the Yelp 2017 Challenge dataset and Amazon product datasets. Results show that (1) our method achieves significant improvement compared to strong baseline methods, especially for users with only few ratings; (2) item visual features can improve the prediction performance—the effects of item image features on improving the prediction results depend on the importance of the visual features for the items; and (3) our model can explicitly interpret the predicted results in great detail.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {16},
numpages = {28},
keywords = {rating prediction, latent factor model, explainable recommendation, multi-modal, Aspect}
}

@article{10.1145/3291059,
author = {Chong, Wen-Haw and Lim, Ee-Peng},
title = {Fine-Grained Geolocation of Tweets in Temporal Proximity},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3291059},
doi = {10.1145/3291059},
abstract = {In fine-grained tweet geolocation, tweets are linked to the specific venues (e.g., restaurants, shops) from which they were posted. This explicitly recovers the venue context that is essential for applications such as location-based advertising or user profiling. For this geolocation task, we focus on geolocating tweets that are contained in tweet sequences. In a tweet sequence, tweets are posted from some latent venue(s) by the same user and within a short time interval. This scenario arises from two observations: (1) It is quite common that users post multiple tweets in a short time and (2) most tweets are not geocoded. To more accurately geolocate a tweet, we propose a model that performs query expansion on the tweet (query) using two novel approaches. The first approach temporal query expansion considers users’ staying behavior around venues. The second approach visitation query expansion leverages on user revisiting the same or similar venues in the past. We combine both query expansion approaches via a novel fusion framework and overlay them on a Hidden Markov Model to account for sequential information. In our comprehensive experiments across multiple datasets and metrics, we show our proposed model to be more robust and accurate than other baselines.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {17},
numpages = {33},
keywords = {Tweet geolocation, temporal proximity, staying behavior}
}

@article{10.1145/3284101,
author = {Niu, Xi and Fan, Xiangyu and Zhang, Tao},
title = {Understanding Faceted Search from Data Science and Human Factor Perspectives},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3284101},
doi = {10.1145/3284101},
abstract = {Faceted search has become a common feature on most search interfaces in e-commerce websites, digital libraries, government’s open information portals, and so on. Beyond the existing studies on developing algorithms for faceted search and empirical studies on facet usage, this study investigated user real-time interactions with facets over the course of a search from both data science and human factor perspectives. It adopted a Random Forest (RF) model to successfully predict facet use using search dynamic variables. In addition, the RF model provided a ranking of variables by their predictive power, which suggests that the search process follows rhythmic flow of a sequence within which facet addition is mostly influenced by its immediately preceding action. In the follow-up user study, we found that participants used facets at critical points from the beginning to end of search sessions. Participants used facets for distinctive reasons at different stages. They also used facets implicitly without applying the facets to their search. Most participants liked the faceted search, although a few participants were concerned about the choice overload introduced by facets. The results of this research can be used to understand information seekers and propose or refine a set of practical design guidelines for faceted search.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {14},
numpages = {27},
keywords = {predictive analytics, server logs, random forest, user study, Faceted search}
}

@article{10.1145/3281659,
author = {Chen, Xu and Zhang, Yongfeng and Xu, Hongteng and Qin, Zheng and Zha, Hongyuan},
title = {Adversarial Distillation for Efficient Recommendation with External Knowledge},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3281659},
doi = {10.1145/3281659},
abstract = {Integrating external knowledge into the recommendation system has attracted increasing attention in both industry and academic communities. Recent methods mostly take the power of neural network for effective knowledge representation to improve the recommendation performance. However, the heavy deep architectures in existing models are usually incorporated in an embedded manner, which may greatly increase the model complexity and lower the runtime efficiency.To simultaneously take the power of deep learning for external knowledge modeling as well as maintaining the model efficiency at test time, we reformulate the problem of recommendation with external knowledge into a generalized distillation framework. The general idea is to free the complex deep architecture into a separate model, which is only used in the training phrase, while abandoned at test time. In particular, in the training phrase, the external knowledge is processed by a comprehensive teacher model to produce valuable information to teach a simple and efficient student model. Once the framework is learned, the teacher model is abandoned, and only the succinct yet enhanced student model is used to make fast predictions at test time. In this article, we specify the external knowledge as user review, and to leverage it in an effective manner, we further extend the traditional generalized distillation framework by designing a Selective Distillation Network (SDNet) with adversarial adaption and orthogonality constraint strategies to make it more robust to noise information.Extensive experiments verify that our model can not only improve the performance of rating prediction, but also can significantly reduce time consumption when making predictions as compared with several state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {12},
numpages = {28},
keywords = {personalization, distillation network, external knowledge, adversarial training, Recommendation system}
}

@article{10.1145/3233773,
author = {Lu, Wei and Chung, Fu-Lai and Jiang, Wenhao and Ester, Martin and Liu, Wei},
title = {A Deep Bayesian Tensor-Based System for Video Recommendation},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3233773},
doi = {10.1145/3233773},
abstract = {With the availability of abundant online multi-relational video information, recommender systems that can effectively exploit these sorts of data and suggest creatively interesting items will become increasingly important. Recent research illustrates that tensor models offer effective approaches for complex multi-relational data learning and missing element completion. So far, most tensor-based user clustering models have focused on the accuracy of recommendation. Given the dynamic nature of online media, recommendation in this setting is more challenging as it is difficult to capture the users’ dynamic topic distributions in sparse data settings as well as to identify unseen items as candidates of recommendation. Targeting at constructing a recommender system that can encourage more creativity, a deep Bayesian probabilistic tensor framework for tag and item recommendation is proposed. During the score ranking processes, a metric called Bayesian surprise is incorporated to increase the creativity of the recommended candidates. The new algorithm, called Deep Canonical PARAFAC Factorization (DCPF), is evaluated on both synthetic and large-scale real-world problems. An empirical study for video recommendation demonstrates the superiority of the proposed model and indicates that it can better capture the latent patterns of interactions and generates interesting recommendations based on creative tag combinations.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {7},
numpages = {22},
keywords = {tensor decomposition, Computational creativity, Bayesian methods}
}

@article{10.1145/3284102,
author = {Cornolti, Marco and Ferragina, Paolo and Ciaramita, Massimiliano and R\"{u}d, Stefan and Sch\"{u}tze, Hinrich},
title = {SMAPH: A Piggyback Approach for Entity-Linking in Web Queries},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3284102},
doi = {10.1145/3284102},
abstract = {We study the problem of linking the terms of a web-search query to a semantic representation given by the set of entities (a.k.a. concepts) mentioned in it. We introduce SMAPH, a system that performs this task using the information coming from a web search engine, an approach we call “piggybacking.” We employ search engines to alleviate the noise and irregularities that characterize the language of queries. Snippets returned as search results also provide a context for the query that makes it easier to disambiguate the meaning of the query. From the search results, SMAPH builds a set of candidate entities with high coverage. This set is filtered by linking back the candidate entities to the terms occurring in the input query, ensuring high precision. A greedy disambiguation algorithm performs this filtering; it maximizes the coherence of the solution by iteratively discovering the pertinent entities mentioned in the query. We propose three versions of SMAPH that outperform state-of-the-art solutions on the known benchmarks and on the GERDAQ dataset, a novel dataset that we have built specifically for this problem via crowd-sourcing and that we make publicly available.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {13},
numpages = {42},
keywords = {query annotation, piggyback, Entity-linking, ERD}
}

@article{10.1145/3238250,
author = {Li, Chenliang and Chen, Shiqian and Xing, Jian and Sun, Aixin and Ma, Zongyang},
title = {Seed-Guided Topic Model for Document Filtering and Classification},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3238250},
doi = {10.1145/3238250},
abstract = {One important necessity is to filter out the irrelevant information and organize the relevant information into meaningful categories. However, developing text classifiers often requires a large number of labeled documents as training examples. Manually labeling documents is costly and time-consuming. More importantly, it becomes unrealistic to know all the categories covered by the documents beforehand. Recently, a few methods have been proposed to label documents by using a small set of relevant keywords for each category, known as dataless text classification. In this article, we propose a seed-guided topic model for the dataless text filtering and classification (named DFC). Given a collection of unlabeled documents, and for each specified category a small set of seed words that are relevant to the semantic meaning of the category, DFC filters out the irrelevant documents and classifies the relevant documents into the corresponding categories through topic influence. DFC models two kinds of topics: category-topics and general-topics. Also, there are two kinds of category-topics: relevant-topics and irrelevant-topics. Each relevant-topic is associated with one specific category, representing its semantic meaning. The irrelevant-topics represent the semantics of the unknown categories covered by the document collection. And the general-topics capture the global semantic information. DFC assumes that each document is associated with a single category-topic and a mixture of general-topics. A novelty of the model is that DFC learns the topics by exploiting the explicit word co-occurrence patterns between the seed words and regular words (i.e., non-seed words) in the document collection. A document is then filtered, or classified, based on its posterior category-topic assignment. Experiments on two widely used datasets show that DFC consistently outperforms the state-of-the-art dataless text classifiers for both classification with filtering and classification without filtering. In many tasks, DFC can also achieve comparable or even better classification accuracy than the state-of-the-art supervised learning solutions. Our experimental results further show that DFC is insensitive to the tuning parameters. Moreover, we conduct a thorough study about the impact of seed words for existing dataless text classification techniques. The results reveal that it is not using more seed words but the document coverage of the seed words for the corresponding category that affects the dataless classification performance.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {9},
numpages = {37},
keywords = {document filtering, dataless classification, Topic model}
}

@article{10.1145/3231936,
author = {Mic, Vladimir and Novak, David and Zezula, Pavel},
title = {Binary Sketches for Secondary Filtering},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3231936},
doi = {10.1145/3231936},
abstract = {This article addresses the problem of matching the most similar data objects to a given query object. We adopt a generic model of similarity that involves the domain of objects and metric distance functions only. We examine the case of a large dataset in a complex data space, which makes this problem inherently difficult. Many indexing and searching approaches have been proposed, but they have often failed to efficiently prune complex search spaces and access large portions of the dataset when evaluating queries. We propose an approach to enhancing the existing search techniques to significantly reduce the number of accessed data objects while preserving the quality of the search results. In particular, we extend each data object with its sketch, a short binary string in Hamming space. These sketches approximate the similarity relationships in the original search space, and we use them to filter out non-relevant objects not pruned by the original search technique. We provide a&nbsp;probabilistic model to tune the parameters of the sketch-based filtering separately for each query object. Experiments conducted with different similarity search techniques and real-life datasets demonstrate that the secondary filtering can speed-up similarity search several times.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {1},
numpages = {28},
keywords = {filter and refine, binary sketch, Similarity search, Hamming space}
}

@article{10.1145/3231935,
author = {Alakuijala, Jyrki and Farruggia, Andrea and Ferragina, Paolo and Kliuchnikov, Eugene and Obryk, Robert and Szabadka, Zoltan and Vandevenne, Lode},
title = {Brotli: A General-Purpose Data Compressor},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3231935},
doi = {10.1145/3231935},
abstract = {Brotli is an open source general-purpose data compressor introduced by Google in late 2013 and now adopted in most known browsers and Web servers. It is publicly available on GitHub and its data format was submitted as RFC 7932 in July 2016. Brotli is based on the Lempel-Ziv compression scheme and planned as a generic replacement of Gzip and ZLib. The main goal in its design was to compress data on the Internet, which meant optimizing the resources used at decoding time, while achieving maximal compression density.This article is intended to provide the first thorough, systematic description of the Brotli format as well as a detailed computational and experimental analysis of the main algorithmic blocks underlying the current encoder implementation, together with a comparison against compressors of different families constituting the state-of-the-art either in practice or in theory. This treatment will allow us to raise a set of new algorithmic and software engineering problems that deserve further attention from the scientific community.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {4},
numpages = {30},
keywords = {experiments, NP-completeness, Treaps, Lempel-Ziv parsing, shortest paths, Data compression}
}

@article{10.1145/3268928,
author = {Oard, Douglas W. and Sebastiani, Fabrizio and Vinjumur, Jyothi K.},
title = {Jointly Minimizing the Expected Costs of Review for Responsiveness and Privilege in E-Discovery},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3268928},
doi = {10.1145/3268928},
abstract = {Discovery is an important aspect of the civil litigation process in the United States of America, in which all parties to a lawsuit are permitted to request relevant evidence from other parties. With the rapid growth of digital content, the emerging need for “e-discovery” has created a strong demand for techniques that can be used to review massive collections both for “responsiveness” (i.e., relevance) to the request and for “privilege” (i.e., presence of legally protected content that the party performing the review may have a right to withhold). In this process, the party performing the review may incur costs of two types, namely, annotation costs (deriving from the fact that human reviewers need to be paid for their work) and misclassification costs (deriving from the fact that failing to correctly determine the responsiveness or privilege of a document may adversely affect the interests of the parties in various ways). Relying exclusively on automatic classification would minimize annotation costs but could result in substantial misclassification costs, while relying exclusively on manual classification could generate the opposite consequences. This article proposes a risk minimization framework (called MINECORE, for “<underline>min</underline>imizing the <underline>e</underline>xpected <underline>co</underline>sts of <underline>re</underline>view”) that seeks to strike an optimal balance between these two extreme stands. In MINECORE (a) the documents are first automatically classified for both responsiveness and privilege, and then (b) some of the automatically classified documents are annotated by human reviewers for responsiveness (typically by junior reviewers) and/or, in cascade, for privilege (typically by senior reviewers), with the overall goal of minimizing the expected cost (i.e., the risk) of the entire process. Risk minimization is achieved by optimizing, for both responsiveness and privilege, the choice of which documents to manually review. We present a simulation study in which classes from a standard text classification test collection (RCV1-v2) are used as surrogates for responsiveness and privilege. The results indicate that MINECORE can yield substantially lower total cost than any of a set of strong baselines.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {11},
numpages = {35},
keywords = {technology-assisted review, semi-automated text classification, E-discovery, utility theory}
}

@article{10.1145/3233772,
author = {Tymoshenko, Kateryna and Moschitti, Alessandro},
title = {Shallow and Deep Syntactic/Semantic Structures for Passage Reranking in Question-Answering Systems},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3233772},
doi = {10.1145/3233772},
abstract = {In this article, we extensively study the use of syntactic and semantic structures obtained with shallow and full syntactic parsers for answer passage reranking. We propose several dependency and constituent-based structures, also enriched with Linked Open Data (LD) knowledge to represent pairs of questions and answer passages. We encode such tree structures in learning-to-rank (L2R) algorithms using tree kernels, which can project them in tree substructure spaces, where each dimension represents a powerful syntactic/semantic feature. Additionally, since we define links between question and passage structures, our tree kernel spaces also include relational structural features. We carried out an extensive comparative experimentation of our models for automatic answer selection benchmarks on different TREC QA corpora as well as the newer Wikipedia-based dataset, namely WikiQA, which has been widely used to test sentence rerankers. The results consistently demonstrate that our structural semantic models achieve the state of the art in passage reranking. In particular, we derived the following important findings: (i) relational syntactic structures are essential to achieve superior results; (ii) models trained with dependency trees can outperform those trained with shallow trees, e.g., in case of sentence reranking; (iii) external knowledge automatically generated with focus and question classifiers is very effective; and (iv) the semantic information derived by LD and incorporated in syntactic structures can be used to replace the knowledge provided by the above-mentioned classifiers. This is a remarkable advantage as it enables our models to increase coverage and portability over new domains.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {8},
numpages = {38},
keywords = {learning to rank, linked data, question answering, structural kernels, Kernel methods}
}

@article{10.1145/3233771,
author = {Huang, Heyan and Wei, Xiaochi and Nie, Liqiang and Mao, Xianling and Xu, Xin-Shun},
title = {From Question to Text: Question-Oriented Feature Attention for Answer Selection},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3233771},
doi = {10.1145/3233771},
abstract = {Understanding unstructured texts is an essential skill for human beings as it enables knowledge acquisition. Although understanding unstructured texts is easy for we human beings with good education, it is a great challenge for machines. Recently, with the rapid development of artificial intelligence techniques, researchers put efforts to teach machines to understand texts and justify the educated machines by letting them solve the questions upon the given unstructured texts, inspired by the reading comprehension test as we humans do. However, feature effectiveness with respect to different questions significantly hinders the performance of answer selection, because different questions may focus on various aspects of the given text and answer candidates. To solve this problem, we propose a question-oriented feature attention (QFA) mechanism, which learns to weight different engineering features according to the given question, so that important features with respect to the specific question is emphasized accordingly. Experiments on MCTest dataset have well-validated the effectiveness of the proposed method. Additionally, the proposed QFA is applicable to various IR tasks, such as question answering and answer selection. We have verified the applicability on a crawled community-based question-answering dataset.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {6},
numpages = {33},
keywords = {Question answering, attention method, answer selection}
}

@article{10.1145/3233770,
author = {Qu, Yanru and Fang, Bohui and Zhang, Weinan and Tang, Ruiming and Niu, Minzhe and Guo, Huifeng and Yu, Yong and He, Xiuqiang},
title = {Product-Based Neural Networks for User Response Prediction over Multi-Field Categorical Data},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3233770},
doi = {10.1145/3233770},
abstract = {User response prediction is a crucial component for personalized information retrieval and filtering scenarios, such as recommender system and web search. The data in user response prediction is mostly in a multi-field categorical format and transformed into sparse representations via one-hot encoding. Due to the sparsity problems in representation and optimization, most research focuses on feature engineering and shallow modeling. Recently, deep neural networks have attracted research attention on such a problem for their high capacity and end-to-end training scheme. In this article, we study user response prediction in the scenario of click prediction. We first analyze a coupled gradient issue in latent vector-based models and propose kernel product to learn field-aware feature interactions. Then, we discuss an insensitive gradient issue in DNN-based models and propose Product-based Neural Network, which adopts a feature extractor to explore feature interactions. Generalizing the kernel product to a net-in-net architecture, we further propose Product-network in Network (PIN), which can generalize previous models. Extensive experiments on four industrial datasets and one contest dataset demonstrate that our models consistently outperform eight baselines on both area under curve and log loss. Besides, PIN makes great click-through rate improvement (relatively 34.67%) in online A/B test.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {5},
numpages = {35},
keywords = {Deep learning, recommender system, product-based neural network}
}

@article{10.1145/3231937,
author = {Deveaud, Romain and Mothe, Josiane and Ullah, Md Zia and Nie, Jian-Yun},
title = {Learning to Adaptively Rank Document Retrieval System Configurations},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3231937},
doi = {10.1145/3231937},
abstract = {Modern Information Retrieval (IR) systems have become more and more complex, involving a large number of parameters. For example, a system may choose from a set of possible retrieval models (BM25, language model, etc.), or various query expansion parameters, whose values greatly influence the overall retrieval effectiveness. Traditionally, these parameters are set at a system level based on training queries, and the same parameters are then used for different queries. We observe that it may not be easy to set all these parameters separately, since they can be dependent. In addition, a global setting for all queries may not best fit all individual queries with different characteristics. The parameters should be set according to these characteristics. In this article, we propose a novel approach to tackle this problem by dealing with the entire system configurations (i.e., a set of parameters representing an IR system behaviour) instead of selecting a single parameter at a time. The selection of the best configuration is cast as a problem of ranking different possible configurations given a query. We apply learning-to-rank approaches for this task. We exploit both the query features and the system configuration features in the learning-to-rank method so that the selection of configuration is query dependent. The experiments we conducted on four TREC ad hoc collections show that this approach can significantly outperform the traditional method to tune system configuration globally (i.e., grid search) and leads to higher effectiveness than the top performing systems of the TREC tracks. We also perform an ablation analysis on the impact of different features on the model learning capability and show that query expansion features are among the most important for adaptive systems.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {3},
numpages = {41},
keywords = {Information systems, data analytics, query features, learning to rank, retrieval system parameters, adaptive information retrieval, information retrieval}
}

@article{10.1145/3231934,
author = {Safran, Mejdl and Che, Dunren},
title = {Efficient Learning-Based Recommendation Algorithms for Top-<i>N</i> Tasks and Top-<i>N</i> Workers in Large-Scale Crowdsourcing Systems},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3231934},
doi = {10.1145/3231934},
abstract = {The task and worker recommendation problems in crowdsourcing systems have brought up unique characteristics that are not present in traditional recommendation scenarios, i.e., the huge flow of tasks with short lifespans, the importance of workers’ capabilities, and the quality of the completed tasks. These unique features make traditional recommendation approaches no longer satisfactory for task and worker recommendation in crowdsourcing systems. In this article, we propose a two-tier data representation scheme (defining a worker--category suitability score and a worker--task attractiveness score) to support personalized task and worker recommendations. We also extend two optimization methods, namely least mean square error and Bayesian personalized rank, to better fit the characteristics of task/worker recommendation in crowdsourcing systems. We then integrate the proposed representation scheme and the extended optimization methods along with the two adapted popular learning models, i.e., matrix factorization and kNN, and result in two lines of top-N recommendation algorithms for crowdsourcing systems: (1) Top-N-Tasks recommendation algorithms for discovering the top-N most suitable tasks for a given worker and (2) Top-N-Workers recommendation algorithms for identifying the top-N best workers for a task requester. An extensive experimental study is conducted that validates the effectiveness and efficiency of a broad spectrum of algorithms, accompanied by our analysis and the insights gained.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {2},
numpages = {46},
keywords = {crowdsourcing, task recommendation, Crowd computing, ranking algorithms, machine learning}
}

@article{10.1145/3231933,
author = {Aliannejadi, Mohammad and Crestani, Fabio},
title = {Personalized Context-Aware Point of Interest Recommendation},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3231933},
doi = {10.1145/3231933},
abstract = {Personalized recommendation of Points of Interest (POIs) plays a key role in satisfying users on Location-Based Social Networks (LBSNs). In this article, we propose a probabilistic model to find the mapping between user-annotated tags and locations’ taste keywords. Furthermore, we introduce a dataset on locations’ contextual appropriateness and demonstrate its usefulness in predicting the contextual relevance of locations. We investigate four approaches to use our proposed mapping for addressing the data sparsity problem: one model to reduce the dimensionality of location taste keywords and three models to predict user tags for a new location. Moreover, we present different scores calculated from multiple LBSNs and show how we incorporate new information from the mapping into a POI recommendation approach. Then, the computed scores are integrated using learning to rank techniques. The experiments on two TREC datasets show the effectiveness of our approach, beating state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {45},
numpages = {28},
keywords = {point of interest recommendation, location-based social networks, contextual suggestion, User modeling, content-based recommendation}
}

@article{10.1145/3231593,
author = {Ruotsalo, Tuukka and Peltonen, Jaakko and Eugster, Manuel J. A. and G\l{}owacka, Dorota and Flor\'{e}en, Patrik and Myllym\"{a}ki, Petri and Jacucci, Giulio and Kaski, Samuel},
title = {Interactive Intent Modeling for Exploratory Search},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3231593},
doi = {10.1145/3231593},
abstract = {Exploratory search requires the system to assist the user in comprehending the information space and expressing evolving search intents for iterative exploration and retrieval of information. We introduce interactive intent modeling, a technique that models a user’s evolving search intents and visualizes them as keywords for interaction. The user can provide feedback on the keywords, from which the system learns and visualizes an improved intent estimate and retrieves information. We report experiments comparing variants of a system implementing interactive intent modeling to a control system. Data comprising search logs, interaction logs, essay answers, and questionnaires indicate significant improvements in task performance, information retrieval performance over the session, information comprehension performance, and user experience. The improvements in retrieval effectiveness can be attributed to the intent modeling and the effect on users’ task performance, breadth of information comprehension, and user experience are shown to be dependent on a richer visualization. Our results demonstrate the utility of combining interactive modeling of search intentions with interactive visualization of the models that can benefit both directing the exploratory search process and making sense of the information space. Our findings can help design personalized systems that support exploratory information seeking and discovery of novel information.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {44},
numpages = {46},
keywords = {user intent modeling, Proactive search}
}

@article{10.1145/3230871,
author = {Wang, Yanhao and Li, Yuchen and Fan, Ju and Tan, Kian-Lee},
title = {Location-Aware Influence Maximization over Dynamic Social Streams},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3230871},
doi = {10.1145/3230871},
abstract = {Influence maximization (IM), which selects a set of k seed users (a.k.a., a seed set) to maximize the influence spread over a social network, is a fundamental problem in a wide range of applications. However, most existing IM algorithms are static and location-unaware. They fail to provide high-quality seed sets efficiently when the social network evolves rapidly and IM queries are location-aware. In this article, we first define two IM queries, namely Stream Influence Maximization (SIM) and Location-aware SIM (LSIM), to track influential users over social streams. Technically, SIM adopts the sliding window model and maintains a seed set with the maximum influence value collectively over the most recent social actions. LSIM further considers social actions are associated with geo-tags and identifies a seed set that maximizes the influence value in a query region over a location-aware social stream. Then, we propose the Sparse Influential Checkpoints (SIC) framework for efficient SIM query processing. SIC maintains a sequence of influential checkpoints over the sliding window and each checkpoint maintains a partial solution for SIM in an append-only substream of social actions. Theoretically, SIC keeps a logarithmic number of checkpoints w.r.t.&nbsp;the size of the sliding window and always returns an approximate solution from one of the checkpoint for the SIM query at any time. Furthermore, we propose the Location-based SIC (LSIC) framework and its improved version LSIC+, both of which process LSIM queries by integrating the SIC framework with a Quadtree spatial index. LSIC can provide approximate solutions for both ad hoc and continuous LSIM queries in real time, while LSIC+ further improves the solution quality of LSIC. Experimental results on real-world datasets demonstrate the effectiveness and efficiency of the proposed frameworks against the state-of-the-art IM algorithms.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {43},
numpages = {35},
keywords = {submodular optimization, Influence maximization, spatial index, data stream, social network, region query}
}

@article{10.1145/3223045,
author = {Mao, Jiaxin and Liu, Yiqun and Kando, Noriko and Zhang, Min and Ma, Shaoping},
title = {How Does Domain Expertise Affect Users’ Search Interaction and Outcome in Exploratory Search?},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3223045},
doi = {10.1145/3223045},
abstract = {People often conduct exploratory search to explore unfamiliar information space and learn new knowledge. While supporting the highly dynamic and interactive exploratory search is still challenging for the search system, we want to investigate which factors can make the exploratory search successful and satisfying from the user’s perspective. Previous research suggests that domain experts have different search strategies and are more successful in finding domain-specific information, but how the domain expertise level will influence users’ interaction and search outcomes in exploratory search, especially in different knowledge domains, is still unclear. In this work, via a carefully designed user study that involves 30 participants, we investigate the influence of domain expertise levels on the interaction and outcome of exploratory search in three different domains: environment, medicine, and politics. We record participants’ search behaviors, including their explicit feedback and eye fixation sequences, in a laboratory setting. With this dataset, we identify both domain-independent and domain-dependent effects on user behaviors and search outcomes. Our results extend existing research on the effect of domain expertise in search and suggest different strategies for exploiting domain expertise to support exploratory search in different knowledge domains.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {42},
numpages = {30},
keywords = {Exploratory search, user behavior analysis, domain expertise}
}

@article{10.1145/3209624,
author = {Arguello, Jaime and Choi, Bogeum and Capra, Robert},
title = {Factors Influencing Users’ Information Requests: Medium, Target, and Extra-Topical Dimension},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3209624},
doi = {10.1145/3209624},
abstract = {We report on a crowdsourced study that investigated how two factors influence the way people formulate information requests. Our first factor, medium, considers whether the request is produced using text or voice. Our second factor, target, considers whether the request is intended for a search engine or a human intermediary (i.e., someone who will search on the user’s behalf). In particular, we study how these two factors influence the way people formulate requests in situations where the information need has a specific type of extra-topical dimension (i.e., a type of constraint that is independent from the information need’s topic). We focus on six extra-topical dimensions: (1) domain knowledge, (2) viewpoint, (3) experiential, (4) venue location, (5) source location, and (6) temporal. The extra-topical dimension was manipulated by giving participants carefully constructed search tasks. We analyzed a large number of information requests produced by study participants, and address three research questions. We study the effects of our two factors (medium and target) on (RQ1) participants’ perceptions about their own information requests, (RQ2) the different characteristics of their information requests (e.g., natural language structure, retrieval performance), and (RQ3) participants’ strategies for requesting information when the search task has a specific type of extra-topical dimension. Our results found that both factors influenced participants’ perceptions about their own information requests, the characteristics of participants’ requests, and the strategies adopted by participants to request information matching the extra-topical dimension. Our results have implications for future research on methods that can harness (rather than ignore) extra-topical query terms to retrieve relevant information.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {41},
numpages = {37},
keywords = {relevance criteria, spoken search, query formulation, Information requests, search performance}
}

@article{10.1145/3196826,
author = {Gysel, Christophe Van and de Rijke, Maarten and Kanoulas, Evangelos},
title = {Neural Vector Spaces for Unsupervised Information Retrieval},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3196826},
doi = {10.1145/3196826},
abstract = {We propose the Neural Vector Space Model (NVSM), a method that learns representations of documents in an unsupervised manner for news article retrieval. In the NVSM paradigm, we learn low-dimensional representations of words and documents from scratch using gradient descent and rank documents according to their similarity with query representations that are composed from word representations. We show that NVSM performs better at document ranking than existing latent semantic vector space methods. The addition of NVSM to a mixture of lexical language models and a state-of-the-art baseline vector space model yields a statistically significant increase in retrieval effectiveness. Consequently, NVSM adds a complementary relevance signal. Next to semantic matching, we find that NVSM performs well in cases where lexical matching is needed. NVSM learns a notion of term specificity directly from the document collection without feature engineering. We also show that NVSM learns regularities related to Luhn significance. Finally, we give advice on how to deploy NVSM in situations where model selection (e.g., cross-validation) is infeasible. We find that an unsupervised ensemble of multiple models trained with different hyperparameter values performs better than a single cross-validated model. Therefore, NVSM can safely be used for ranking documents without supervised relevance judgments.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {38},
numpages = {25},
keywords = {semantic matching, Ad-hoc retrieval, latent vector spaces, document retrieval, representation learning}
}

@article{10.1145/3202662,
author = {Middleton, Stuart E. and Kordopatis-Zilos, Giorgos and Papadopoulos, Symeon and Kompatsiaris, Yiannis},
title = {Location Extraction from Social Media: Geoparsing, Location Disambiguation, and Geotagging},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3202662},
doi = {10.1145/3202662},
abstract = {Location extraction, also called “toponym extraction,” is a field covering geoparsing, extracting spatial representations from location mentions in text, and geotagging, assigning spatial coordinates to content items. This article evaluates five “best-of-class” location extraction algorithms. We develop a geoparsing algorithm using an OpenStreetMap database, and a geotagging algorithm using a language model constructed from social media tags and multiple gazetteers. Third-party work evaluated includes a DBpedia-based entity recognition and disambiguation approach, a named entity recognition and Geonames gazetteer approach, and a Google Geocoder API approach. We perform two quantitative benchmark evaluations, one geoparsing tweets and one geotagging Flickr posts, to compare all approaches. We also perform a qualitative evaluation recalling top N location mentions from tweets during major news events. The OpenStreetMap approach was best (F1 0.90+) for geoparsing English, and the language model approach was best (F1 0.66) for Turkish. The language model was best (F1@1km 0.49) for the geotagging evaluation. The map database was best (R@20 0.60+) in the qualitative evaluation. We report on strengths, weaknesses, and a detailed failure analysis for the approaches and suggest concrete areas for further research.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {40},
numpages = {27},
keywords = {toponym, location, benchmark, disambiguation, geoparsing, Location extraction, social media, information extraction, geocoding, toponym extraction, geotagging}
}

@article{10.1145/3185153,
author = {Nelissen, Klaas and Snoeck, Monique and Broucke, Seppe Vanden and Baesens, Bart},
title = {Swipe and Tell: Using Implicit Feedback to Predict User Engagement on Tablets},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3185153},
doi = {10.1145/3185153},
abstract = {When content consumers explicitly judge content positively, we consider them to be engaged. Unfortunately, explicit user evaluations are difficult to collect, as they require user effort. Therefore, we propose to use device interactions as implicit feedback to detect engagement.We assess the usefulness of swipe interactions on tablets for predicting engagement and make the comparison with using traditional features based on time spent.We gathered two unique datasets of more than 250,000 swipes, 100,000 unique article visits, and over 35,000 explicitly judged news articles by modifying two commonly used tablet apps of two newspapers. We tracked all device interactions of 407 experiment participants during one month of habitual news reading.We employed a behavioral metric as a proxy for engagement, because our analysis needed to be scalable to many users, and scanning behavior required us to allow users to indicate engagement quickly.We point out the importance of taking into account content ordering, report the most predictive features, zoom in on briefly read content and on the most frequently read articles.Our findings demonstrate that fine-grained tablet interactions are useful indicators of engagement for newsreaders on tablets. The best features successfully combine both time-based aspects and swipe interactions.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {35},
numpages = {36},
keywords = {tablets, frequently read content, briefly read content, online news, newspaper, dwell time, touch interactions, User engagement, implicit feedback, content ordering}
}

@article{10.1145/3190784,
author = {Bekhet, Saddam and Ahmed, Amr},
title = {An Integrated Signature-Based Framework for Efficient Visual Similarity Detection and Measurement in Video Shots},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3190784},
doi = {10.1145/3190784},
abstract = {This article presents a framework for speedy video matching and retrieval through detection and measurement of visual similarity. The framework’s efficiency stems from its power to encode a given shot content into a compact fixed-length signature that helps in robust real-time matching. Separate scene and motion signatures are developed and fused together to fully represent and match respective video shots. Scene information is captured through the Statistical Dominant Color Profile (SDCP), while motion information is captured through a graph-based signature called the Dominant Color Graph Profile (DCGP). The SDCP is a fixed-length compact signature that statistically encodes the colors’ spatiotemporal patterns across video frames. The DCGP is a fixed-length signature that records and tracks the gray levels across subsampled video frames, where the graph structural properties are used to extract the signature values. Finally, the overall video signature is generated by fusing the individual scene and motion signatures. The signature-based aspect of the proposed framework is the key to its high matching speed (&gt; 2000 fps) compared to current techniques that rely on exhaustive processing. To maximize the benefit of the framework, compressed-domain videos are utilized as a case study following their wide availability. However, the framework avoids full video decompression and operates on tiny frames rather than full-size decompressed frames. Experiments on various standard and challenging dataset groups show the framework’s robust performance in terms of both retrieval and computational performance.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {37},
numpages = {38},
keywords = {MPEG, Video retrieval, signature, compressed video, graph, video similarity}
}

@article{10.1145/3200864,
author = {Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun and Wei, Furu and Nie, Liqiang and Ma, Jun and de Rijke, Maarten},
title = {Sentence Relations for Extractive Summarization with Deep Neural Networks},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3200864},
doi = {10.1145/3200864},
abstract = {Sentence regression is a type of extractive summarization that achieves state-of-the-art performance and is commonly used in practical systems. The most challenging task within the sentence regression framework is to identify discriminative features to represent each sentence. In this article, we study the use of sentence relations, e.g., Contextual Sentence Relations (CSR), Title Sentence Relations (TSR), and Query Sentence Relations (QSR), so as to improve the performance of sentence regression. CSR, TSR, and QSR refer to the relations between a main body sentence and its local context, its document title, and a given query, respectively.We propose a deep neural network model, Sentence Relation-based Summarization (SRSum), that consists of five sub-models, PriorSum, CSRSum, TSRSum, QSRSum, and SFSum. PriorSum encodes the latent semantic meaning of a sentence using a bi-gram convolutional neural network. SFSum encodes the surface information of a sentence, e.g., sentence length, sentence position, and so on. CSRSum, TSRSum, and QSRSum are three sentence relation sub-models corresponding to CSR, TSR, and QSR, respectively. CSRSum evaluates the ability of each sentence to summarize its local contexts. Specifically, CSRSum applies a CSR-based word-level and sentence-level attention mechanism to simulate the context-aware reading of a human reader, where words and sentences that have anaphoric relations or local summarization abilities are easily remembered and paid attention to. TSRSum evaluates the semantic closeness of each sentence with respect to its title, which usually reflects the main ideas of a document. TSRSum applies a TSR-based attention mechanism to simulate people’s reading ability with the main idea (title) in mind. QSRSum evaluates the relevance of each sentence with given queries for the query-focused summarization. QSRSum applies a QSR-based attention mechanism to simulate the attentive reading of a human reader with some queries in mind. The mechanism can recognize which parts of the given queries are more likely answered by a sentence under consideration. Finally as a whole, SRSum automatically learns useful latent features by jointly learning representations of query sentences, content sentences, and title sentences as well as their relations.We conduct extensive experiments on six benchmark datasets, including generic multi-document summarization and query-focused multi-document summarization. On both tasks, SRSum achieves comparable or superior performance compared with state-of-the-art approaches in terms of multiple ROUGE metrics.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {39},
numpages = {32},
keywords = {sentence relations, attentive pooling, neural network, Extractive summarization}
}

@article{10.1145/3186195,
author = {Goldberg, David and Trotman, Andrew and Wang, Xiao and Min, Wei and Wan, Zongru},
title = {Further Insights on Drawing Sound Conclusions from Noisy Judgments},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3186195},
doi = {10.1145/3186195},
abstract = {The effectiveness of a search engine is typically evaluated using hand-labeled datasets, where the labels indicate the relevance of documents to queries. Often the number of labels needed is too large to be created by the best annotators, and so less expensive labels (e.g., from crowdsourcing) are used. This introduces errors in the labels, and thus errors in standard effectiveness metrics (such as P@k and DCG). These errors must be taken into consideration when using the metrics. Previous work has approached assessor error by taking aggregates over multiple inexpensive assessors. We take a different approach and introduce equations and algorithms that can adjust the metrics to the values they would have had if there were no annotation errors.This is especially important when two search engines are compared on their metrics. We give examples where one engine appeared to be statistically significantly better than the other, but the effect disappeared after the metrics were corrected for annotation error. In other words, the evidence supporting a statistical difference was illusory and caused by a failure to account for annotation error.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {36},
numpages = {31},
keywords = {standard error, statistical significance, Precision}
}

@article{10.1145/3183370,
author = {Tan, Jiwei and Wan, Xiaojun and Liu, Hui and Xiao, Jianguo},
title = {QuoteRec: Toward Quote Recommendation for Writing},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3183370},
doi = {10.1145/3183370},
abstract = {Quote is a language phenomenon of transcribing the statement of someone else, such as a proverb and a famous saying. An appropriate usage of quote usually equips the expression with more elegance and credibility. However, there are times when we are eager to stress our idea by citing a quote, while nothing relevant comes to mind. Therefore, it is exciting to have a recommender system which provides quote recommendations while we are writing. This article extends previous study of quote recommendation, the task that recommends the appropriate quote according to the context (i.e., the content occurring before and after the quote). In this article, a quote recommender system called QuoteRec is presented to tackle the task. We investigate two models to learn the vector representations of quotes and contexts, and then rank the candidate quotes based on the representations. The first model learns the quote representation according to the contexts of a quote. The second model is an extension of the neural network model in previous study, which learns the representation of a quote by concerning both its content and contexts. Experimental results demonstrate the effectiveness of the two models in learning the semantic representations of quotes, and the neural network model achieves state-of-the-art results on the quote recommendation task.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {34},
numpages = {36},
keywords = {Deep learning, quote recommendation, LSTM, document recommendation}
}

@article{10.1145/3182166,
author = {Lian, Defu and Zheng, Kai and Ge, Yong and Cao, Longbing and Chen, Enhong and Xie, Xing},
title = {GeoMF++: Scalable Location Recommendation via Joint Geographical Modeling and Matrix Factorization},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3182166},
doi = {10.1145/3182166},
abstract = {Location recommendation is an important means to help people discover attractive locations. However, extreme sparsity of user-location matrices leads to a severe challenge, so it is necessary to take implicit feedback characteristics of user mobility data into account and leverage the location’s spatial information. To this end, based on previously developed GeoMF, we propose a scalable and flexible framework, dubbed GeoMF++, for joint geographical modeling and implicit feedback-based matrix factorization. We then develop an efficient optimization algorithm for parameter learning, which scales linearly with data size and the total number of neighbor grids of all locations. GeoMF++ can be well explained from two perspectives. First, it subsumes two-dimensional kernel density estimation so that it captures spatial clustering phenomenon in user mobility data; Second, it is strongly connected with widely used neighbor additive models, graph Laplacian regularized models, and collective matrix factorization. Finally, we extensively evaluate GeoMF++ on two large-scale LBSN datasets. The experimental results show that GeoMF++ consistently outperforms the state-of-the-art and other competing baselines on both datasets in terms of NDCG and Recall. Besides, the efficiency studies show that GeoMF++ is much more scalable with the increase of data size and the dimension of latent space.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {33},
numpages = {29},
keywords = {LBSNs, geographical modeling, Location recommendation}
}

@article{10.1145/3183712,
author = {Liao, Yi and Lam, Wai and Bing, Lidong and Shen, Xin},
title = {Joint Modeling of Participant Influence and Latent Topics for Recommendation in Event-Based Social Networks},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3183712},
doi = {10.1145/3183712},
abstract = {Event-based social networks (EBSNs) are becoming popular in recent years. Users can publish a planned event on an EBSN website, calling for other users to participate in the event. When a user is making a decision on whether to participate in an event in EBSNs, one aspect for consideration is existing participants defined as users who have agreed to join this event. Existing participants of the event may affect the decision of the user, to which we refer as participant influence. However, participant influence is not well studied by previous works. In this article, we propose an event recommendation model that considers participant influence, and exploits the influence of existing participants on the decisions of new participants based on Poisson factorization. The effect of participant influence is associated with the target event, the host group of the event, and the location of the event. Furthermore, our proposed model can extract latent event topics from event text descriptions, and characterize events, groups, and locations by distributions of event topics. Associations between latent event topics and participant influence are exploited for improving event recommendation. Besides making event recommendation, the proposed model is able to reveal the semantic properties of the participant influence between two users semantically. We have conducted extensive experiments on some datasets extracted from a real-world EBSN. Our proposed model achieves superior event recommendation performance over several state-of-the-art models. The results demonstrate that the consideration of participant influence can improve event recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {29},
numpages = {31},
keywords = {Event-based social networks, Poisson factorization}
}

@article{10.1145/3182165,
author = {Yang, Jing and Eickhoff, Carsten},
title = {Unsupervised Learning of Parsimonious General-Purpose Embeddings for User and Location Modeling},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3182165},
doi = {10.1145/3182165},
abstract = {Many social network applications depend on robust representations of spatio-temporal data. In this work, we present an embedding model based on feed-forward neural networks which transforms social media check-ins into dense feature vectors encoding geographic, temporal, and functional aspects for modeling places, neighborhoods, and users. We employ the embedding model in a variety of applications including location recommendation, urban functional zone study, and crime prediction. For location recommendation, we propose a Spatio-Temporal Embedding Similarity algorithm (STES) based on the embedding model.In a range of experiments on real life data collected from Foursquare, we demonstrate our model’s effectiveness at characterizing places and people and its applicability in aforementioned problem domains. Finally, we select eight major cities around the globe and verify the robustness and generality of our model by porting pre-trained models from one city to another, thereby alleviating the need for costly local training.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {32},
numpages = {33},
keywords = {crime prediction, Social networks, check-in embedding, personalized location recommendation, urban functional zone study}
}

@article{10.1145/3182164,
author = {Guo, Long and Zhang, Dongxiang and Wang, Yuan and Wu, Huayu and Cui, Bin and Tan, Kian-Lee},
title = {CO<sup>2</sup>: Inferring Personal Interests From Raw Footprints by Connecting the Offline World with the Online World},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3182164},
doi = {10.1145/3182164},
abstract = {User-generated trajectories (UGTs), such as travel records from bus companies, capture rich information of human mobility in the offline world. However, some interesting applications of these raw footprints have not been exploited well due to the lack of textual information to infer the subject’s personal interests. Although there is rich semantic information contained in the spatial- and temporal-aware user-generated contents (STUGC) published in the online world, such as Twitter, less effort has been made to utilize this information to facilitate the interest discovery process. In this article, we design an effective probabilistic framework named CO2 to <underline>c</underline>onnect the <underline>o</underline>ffline world with the <underline>o</underline>nline world in order to discover users’ interests directly from their raw footprints in UGT. CO2 first infers trip intentions by utilizing the semantic information in STUGC and then discovers user interests by aggregating the intentions. To evaluate the effectiveness of CO2, we use two large-scale real-world datasets as a case study and further conduct a questionnaire survey to show the superior performance of CO2.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {31},
numpages = {29},
keywords = {Personal interests, user-generated trajectories, raw footprints, trip intention, user-generated contents}
}

@article{10.1145/3182163,
author = {Guy, Ido},
title = {The Characteristics of Voice Search: Comparing Spoken with Typed-in Mobile Web Search Queries},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3182163},
doi = {10.1145/3182163},
abstract = {The growing popularity of mobile search and the advancement in voice recognition technologies have opened the door for web search users to speak their queries rather than type them. While this kind of voice search is still in its infancy, it is gradually becoming more widespread. In this article, we report a comprehensive voice search query log analysis of a commercial web search engine’s mobile application. We compare voice and text search by various aspects, with special focus on the semantic and syntactic characteristics of the queries. Our analysis suggests that voice queries focus more on audio-visual content and question answering and less on social networking and adult domains. In addition, voice queries are more commonly submitted on the go. We also conduct an empirical evaluation showing that the language of voice queries is closer to natural language than the language of text queries. Our analysis points out further differences between voice and text search. We discuss the implications of these differences for the design of future voice-enabled web search tools.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {30},
numpages = {28},
keywords = {spoken search, voice queries, mobile search, query log analysis, voice search, Conversational search}
}

@article{10.1145/3158670,
author = {Zhao, Wayne Xin and Zhang, Wenhui and He, Yulan and Xie, Xing and Wen, Ji-Rong},
title = {Automatically Learning Topics and Difficulty Levels of Problems in Online Judge Systems},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3158670},
doi = {10.1145/3158670},
abstract = {Online Judge (OJ) systems have been widely used in many areas, including programming, mathematical problems solving, and job interviews. Unlike other online learning systems, such as Massive Open Online Course, most OJ systems are designed for self-directed learning without the intervention of teachers. Also, in most OJ systems, problems are simply listed in volumes and there is no clear organization of them by topics or difficulty levels. As such, problems in the same volume are mixed in terms of topics or difficulty levels. By analyzing large-scale users’ learning traces, we observe that there are two major learning modes (or patterns). Users either practice problems in a sequential manner from the same volume regardless of their topics or they attempt problems about the same topic, which may spread across multiple volumes. Our observation is consistent with the findings in classic educational psychology. Based on our observation, we propose a novel two-mode Markov topic model to automatically detect the topics of online problems by jointly characterizing the two learning modes. For further predicting the difficulty level of online problems, we propose a competition-based expertise model using the learned topic information. Extensive experiments on three large OJ datasets have demonstrated the effectiveness of our approach in three different tasks, including skill topic extraction, expertise competition prediction and problem recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {27},
numpages = {33},
keywords = {expertise learning, online judge systems, Topic models}
}

@article{10.1145/3158672,
author = {Levi, Or and Guy, Ido and Raiber, Fiana and Kurland, Oren},
title = {Selective Cluster Presentation on the Search Results Page},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3158672},
doi = {10.1145/3158672},
abstract = {Web search engines present, for some queries, a cluster of results from the same specialized domain (“vertical”) on the search results page (SERP). We introduce a comprehensive analysis of the presentation of such clusters from seven different verticals based on the logs of a commercial Web search engine. This analysis reveals several unique characteristics—such as size, rank, and clicks—of result clusters from community question-and-answer websites. The study of properties of this result cluster—specifically as part of the SERP—has received little attention in previous work. Our analysis also motivates the pursuit of a long-standing challenge in ad hoc retrieval, namely, selective cluster retrieval. In our setting, the specific challenge is to select for presentation the documents most highly ranked either by a cluster-based approach (those in the top-retrieved cluster) or by a document-based approach. We address this classification task by representing queries with features based on those utilized for ranking the clusters, query-performance predictors, and properties of the document-clustering structure. Empirical evaluation performed with TREC data shows that our approach outperforms a recently proposed state-of-the-art cluster-based document-retrieval method as well as state-of-the-art document-retrieval methods that do not account for inter-document similarities.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {28},
numpages = {42},
keywords = {Cluster-based retrieval, aggregated search}
}

@article{10.1145/3158671,
author = {McCreadie, Richard and Santos, Rodrygo L. T. and Macdonald, Craig and Ounis, Iadh},
title = {Explicit Diversification of Event Aspects for Temporal Summarization},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3158671},
doi = {10.1145/3158671},
abstract = {During major events, such as emergencies and disasters, a large volume of information is reported on newswire and social media platforms. Temporal summarization (TS) approaches are used to automatically produce concise overviews of such events by extracting text snippets from related articles over time. Current TS approaches rely on a combination of event relevance and textual novelty for snippet selection. However, for events that span multiple days, textual novelty is often a poor criterion for selecting snippets, since many snippets are textually unique but are semantically redundant or non-informative. In this article, we propose a framework for the diversification of snippets using explicit event aspects, building on recent works in search result diversification. In particular, we first propose two techniques to identify explicit aspects that a user might want to see covered in a summary for different types of event. We then extend a state-of-the-art explicit diversification framework to maximize the coverage of these aspects when selecting summary snippets for unseen events. Through experimentation over the TREC TS 2013, 2014, and 2015 datasets, we show that explicit diversification for temporal summarization significantly outperforms classical novelty-based diversification, as the use of explicit event aspects reduces the amount of redundant and off-topic snippets returned, while also increasing summary timeliness.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {25},
numpages = {31},
keywords = {xQuAD, Temporal summarization, explicit diversification}
}

@article{10.1145/3156667,
author = {Chong, Wen-Haw and Lim, Ee-Peng},
title = {Exploiting User and Venue Characteristics for Fine-Grained Tweet Geolocation},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3156667},
doi = {10.1145/3156667},
abstract = {Which venue is a tweet posted from? We call this a fine-grained geolocation problem. Given an observed tweet, the task is to infer its discrete posting venue, e.g., a specific restaurant. This recovers the venue context and differs from prior work, which geolocats tweets to location coordinates or cities/neighborhoods.First, we conduct empirical analysis to uncover venue and user characteristics for improving geolocation. For venues, we observe spatial homophily, in which venues near each other have more similar tweet content (i.e., text representations) compared to venues further apart. For users, we observe that they are spatially focused and more likely to visit venues near their previous visits. We also find that a substantial proportion of users post one or more geocoded tweet(s), thus providing their location history data. We then propose geolocation models that exploit spatial homophily and spatial focus characteristics plus posting time information. Our models rank candidate venues of test tweets such that the actual posting venue is ranked high. To better tune model parameters, we introduce a learning-to-rank framework. Our best model significantly outperforms state-of-the-art baselines. Furthermore, we show that tweets without any location-indicative words can be geolocated meaningfully as well.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {26},
numpages = {34},
keywords = {spatial homophily, spatial focus, learning to rank, Tweet geolocation}
}

@article{10.1145/3143802,
author = {Zhao, Jingwen and Gao, Yunjun and Chen, Gang and Chen, Rui},
title = {Towards Efficient Framework for Time-Aware Spatial Keyword Queries on Road Networks},
year = {2017},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3143802},
doi = {10.1145/3143802},
abstract = {The spatial keyword query takes as inputs a query location and a set of query keywords and returns the answer objects by considering both their spatial distances to the query location and textual similarity with the query keywords. However, temporal information plays an important role in the spatial keyword query (where there is, to our knowledge, no prior work considering temporal information of the objects), since objects are not always valid. For instance, visitors may plan their trips according to the opening hours of attractions. Moreover, in real-life applications, objects are located on a predefined road network, and the spatial proximity of two objects is measured by the shortest path distance or travelling time between them. In this article, we study the problem of time-aware spatial keyword (TSK) query, which assumes that objects are located on the road network, and finds the k objects satisfying users’ spatio-temporal description and textual constraint. We first present the pruning strategy and algorithm based on an existing index. Then, we design an efficient index structure called TG index and propose several algorithms using the TG index that can prune the search space with both spatio-temporal and textual information simultaneously. Further, we show that the TG index technique can also be applied to improve the performance of time-travel text search and spatial keyword query. Extensive experiments using both real and synthetic datasets demonstrate the effectiveness and efficiency of the presented index and algorithms.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {24},
numpages = {48},
keywords = {temporal information retrieval, road network, spatial keyword query, Indexing technique, query processing}
}

@article{10.1145/3125620,
author = {Arampatzis, Avi and Kalamatianos, Georgios},
title = {Suggesting Points-of-Interest via Content-Based, Collaborative, and Hybrid Fusion Methods In&nbsp;Mobile&nbsp;Devices},
year = {2017},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3125620},
doi = {10.1145/3125620},
abstract = {Recommending venues or points-of-interest (POIs) is a hot topic in recent years, especially for tourism applications and mobile users. We propose and evaluate several suggestion methods, taking an effectiveness, feasibility, efficiency, and privacy perspective. The task is addressed by two content-based methods (a Weighted kNN classifier and a Rated Rocchio personalized query), Collaborative Filtering methods, as well as several (rank-based or rating-based) methods of merging results of different systems. Effectiveness is evaluated on two standard benchmark datasets, provided and used by TREC’s Contextual Suggestion Tracks in 2015 and 2016. First, we enrich these datasets with more information on venues, collected from web services like Foursquare and Yelp; we make this extra data available for future experimentation. Then, we find that the content-based methods provide state-of-the-art effectiveness, the collaborative filtering variants mostly suffer from data sparsity problems in the current datasets, and the merging methods further improve results by mainly promoting the first relevant suggestion. Concerning mobile feasibility, efficiency, and user privacy, the content-based methods, especially Rated Rocchio, are the best. Collaborative filtering has the worst efficiency and privacy leaks. Our findings can be very useful for developing effective and efficient operational systems, respecting user privacy. Last, our experiments indicate that better benchmark datasets would be welcome, and the use of additional evaluation measures—more sensitive in recall—is recommended.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {23},
numpages = {28},
keywords = {recommender systems, privacy, Contextual suggestion}
}

@article{10.1145/3091995,
author = {Zhou, Guang-You and Huang, Jimmy Xiangji},
title = {Modeling and Mining Domain Shared Knowledge for Sentiment Analysis},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3091995},
doi = {10.1145/3091995},
abstract = {Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of user generated sentiment data (e.g., reviews, blogs). In real applications, these user-generated sentiment data can span so many different domains that it is difficult to label the training data for all of them. Therefore, we study the problem of sentiment classification adaptation task in this article. That is, a system is trained to label reviews from one source domain but is meant to be used on the target domain. One of the biggest challenges for sentiment classification adaptation task is how to deal with the problem when two data distributions between the source domain and target domain are significantly different from one another. However, our observation is that there might exist some domain shared knowledge among certain input dimensions of different domains. In this article, we present a novel method for modeling and mining the domain shared knowledge from different sentiment review domains via a joint non-negative matrix factorization–based framework. In this proposed framework, we attempt to learn the domain shared knowledge and the domain-specific information from different sentiment review domains with several various regularization constraints. The advantage of the proposed method can promote the correspondence under the topic space between the source domain and the target domain, which can significantly reduce the data distribution gap across two domains. We conduct extensive experiments on two real-world balanced data sets from Amazon product reviews for sentence-level and document-level binary sentiment classification. Experimental results show that our proposed approach significantly outperforms several strong baselines and achieves an accuracy that is competitive with the most well-known methods for sentiment classification adaptation.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {18},
numpages = {36},
keywords = {Natural Language Processing, Sentiment Analysis, Information Retrieval}
}

@article{10.1145/3086820,
author = {Cai, Wenbin and Zhang, Yexun and Zhang, Ya and Zhou, Siyuan and Wang, Wenquan and Chen, Zhuoxiang and Ding, Chris},
title = {Active Learning for Classification with Maximum Model Change},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086820},
doi = {10.1145/3086820},
abstract = {Most existing active learning studies focus on designing sample selection algorithms. However, several fundamental problems deserve investigation to provide deep insight into active learning. In this article, we conduct an in-depth investigation on active learning for classification from the perspective of model change. We derive a general active learning framework for classification called maximum model change (MMC), which aims at querying the influential examples. The model change is quantified as the difference between the model parameters before and after training with the expanded training set. Inspired by the stochastic gradient update rule, the gradient of the loss with respect to a given candidate example is adopted to approximate the model change. This framework is applied to two popular classifiers: support vector machines and logistic regression. We analyze the convergence property of MMC and theoretically justify it. We explore the connection between MMC and uncertainty-based sampling to provide a uniform view. In addition, we discuss its potential usability to other learning models and show its applicability in a wide range of applications. We validate the MMC strategy on two kinds of benchmark datasets, the UCI repository and ImageNet, and show that it outperforms many state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {15},
numpages = {28},
keywords = {classification, logistic regression, maximum model change, support vector machines, Active learning}
}

@article{10.1145/3086703,
author = {Liu, Ming and Chen, Lei and Liu, Bingquan and Zheng, Guidong and Zhang, Xiaoming},
title = {DBpedia-Based Entity Linking via Greedy Search and Adjusted Monte Carlo Random Walk},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086703},
doi = {10.1145/3086703},
abstract = {Facing a large amount of entities appearing on the web, entity linking has recently become useful. It assigns an entity from a resource to one name mention to help users grasp the meaning of this name mention. Unfortunately, many possible entities can be assigned to one name mention. Apparently, the usually co-occurring name mentions are related and can be considered together to determine their best assignments. This approach is called collective entity linking and is often conducted based on entity graph. However, traditional collective entity linking methods either consume much time due to the large scale of entity graph or obtain low accuracy due to simplifying graph. To improve both accuracy and efficiency, this article proposes a novel collective entity linking algorithm. It first constructs an entity graph by connecting any two related entities, and then a probability-based objective function is proposed on this graph to ensure the high accuracy of the linking result. Via this function, we convert entity linking to the process of finding the nodes with the highest PageRank Values. Greedy search and an adjusted Monte Carlo random walk are proposed to fulfill this work. Experimental results demonstrate that our algorithm performs much better than traditional linking methods.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {16},
numpages = {34},
keywords = {multi-core, adjusted monte carlo random walk, pagerank, greedy search, Collective entity linking}
}

@article{10.1145/3110217,
author = {Ferrante, Marco and Ferro, Nicola and Maistro, Maria},
title = {AWARE: Exploiting Evaluation Measures to Combine Multiple Assessors},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3110217},
doi = {10.1145/3110217},
abstract = {We propose the Assessor-driven Weighted Averages for Retrieval Evaluation (AWARE) probabilistic framework, a novel methodology for dealing with multiple crowd assessors that may be contradictory and/or noisy. By modeling relevance judgements and crowd assessors as sources of uncertainty, AWARE takes the expectation of a generic performance measure, like Average Precision, composed with these random variables. In this way, it approaches the problem of aggregating different crowd assessors from a new perspective, that is, directly combining the performance measures computed on the ground truth generated by the crowd assessors instead of adopting some classification technique to merge the labels produced by them. We propose several unsupervised estimators that instantiate the AWARE framework and we compare them with state-of-the-art approaches, that is,Majoriity Vote and Expectation Maximization, on TREC collections. We found that AWARE approaches improve in terms of their capability of correctly ranking systems and predicting their actual performance scores.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {20},
numpages = {38},
keywords = {weighted average, AWARE, Crowdsourcing, performance measure, unsupervised estimators}
}

@article{10.1145/3108148,
author = {Shi, Lei and Zhao, Wayne Xin and Shen, Yi-Dong},
title = {Local Representative-Based Matrix Factorization for Cold-Start Recommendation},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3108148},
doi = {10.1145/3108148},
abstract = {Cold-start recommendation is one of the most challenging problems in recommender systems. An important approach to cold-start recommendation is to conduct an interview for new users, called the interview-based approach. Among the interview-based methods, Representative-Based Matrix Factorization (RBMF) [24] provides an effective solution with appealing merits: it represents users over selected representative items, which makes the recommendations highly intuitive and interpretable. However, RBMF only utilizes a global set of representative items to model all users. Such a representation is somehow too strict and may not be flexible enough to capture varying users’ interests. To address this problem, we propose a novel interview-based model to dynamically create meaningful user groups using decision trees and then select local representative items for different groups. A two-round interview is performed for a new user. In the first round, l1 global questions are issued for group division, while in the second round, l2 local-group-specific questions are given to derive local representation. We collect the feedback on the (l1+l2) items to learn the user representations. By putting these steps together, we develop a joint optimization model, named local representative-based matrix factorization, for new user recommendations. Extensive experiments on three public datasets have demonstrated the effectiveness of the proposed model compared with several competitive baselines.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {22},
numpages = {28},
keywords = {Cold start recommendation, matrix factorization}
}

@article{10.1145/3106372,
author = {Bai, Xiao and Arapakis, Ioannis and Cambazoglu, B. Barla and Freire, Ana},
title = {Understanding and Leveraging the Impact of Response Latency on User Behaviour in Web Search},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3106372},
doi = {10.1145/3106372},
abstract = {The interplay between the response latency of web search systems and users’ search experience has only recently started to attract research attention, despite the important implications of response latency on monetisation of such systems. In this work, we carry out two complementary studies to investigate the impact of response latency on users’ searching behaviour in web search engines. We first conduct a controlled user study to investigate the sensitivity of users to increasing delays in response latency. This study shows that the users of a fast search system are more sensitive to delays than the users of a slow search system. Moreover, the study finds that users are more likely to notice the response latency delays beyond a certain latency threshold, their search experience potentially being affected. We then analyse a large number of search queries obtained from Yahoo Web Search to investigate the impact of response latency on users’ click behaviour. This analysis demonstrates the significant change in click behaviour as the response latency increases. We also find that certain user, context, and query attributes play a role in the way increasing response latency affects the click behaviour. To demonstrate a possible use case for our findings, we devise a machine-learning framework that leverages the latency impact, together with other features, to predict whether a user will issue any clicks on web search results. As a further extension of this use case, we investigate whether this machine-learning framework can be exploited to help search engines reduce their energy consumption during query processing.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {21},
numpages = {42},
keywords = {click prediction, user engagement, Web search engine, energy consumption, user behaviour, response latency, search experience, green information retrieval}
}

@article{10.1145/3106371,
author = {Ferro, Nicola},
title = {What Does Affect the Correlation Among Evaluation Measures?},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3106371},
doi = {10.1145/3106371},
abstract = {Information Retrieval (IR) is well-known for the great number of adopted evaluation measures, with new ones popping up more and more frequently. In this context, correlation analysis is the tool used to study the evaluation measures and to let us understand if two measures rank systems similarly, if they grasp different aspects of system performances or actually reflect different user models, if a new measure is well motivated or not. To this end, the two most commonly used correlation coefficients are the Kendall’s τ correlation and the AP correlation τAP.The goal of the article is to investigate the properties of the tool, that is, correlation analysis, we use to study evaluation measures. In particular, we investigate three research questions about these two correlation coefficients: (i) what is the effect of the number of systems and topics? (ii) what is the effect of removing low-performing systems? (iii) what is the effect of the experimental collections?To answer these research questions, we propose a methodology based on General Linear Mixed Model (GLMM) and ANalysis Of VAriance (ANOVA) to isolate the effects of the number of topics, number of systems, and experimental collections and to let us observe expected correlation values, net from these effects, which are stable and reliable.We learned that the effect of the number of topics is more prominent than the effect of the number of systems. Even if it produces different absolute values, the effect of removing low-performing systems does not seem to provide information substantially different from not removing them, especially when comparing a whole set of evaluation measures. Finally, we found out that both document corpora and topic sets affect the correlation among evaluation measures, the effect of the latter being more prominent. Moreover, there is a substantial interaction between evaluation measures, corpora and topic sets, meaning that the correlation between different evaluation measures can be substantially increased or decreased depending on the different corpora and topics at hand.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {19},
numpages = {40},
keywords = {Correlation analysis, general linear mixed models (GLMM), analysis of variance (ANOVA), evaluation measures, AP correlation, Kendall’s tau correlation, grid of points (GoP)}
}

@article{10.1145/3086701,
author = {Voorhees, Ellen M. and Samarov, Daniel and Soboroff, Ian},
title = {Using Replicates in Information Retrieval Evaluation},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086701},
doi = {10.1145/3086701},
abstract = {This article explores a method for more accurately estimating the main effect of the system in a typical test-collection-based evaluation of information retrieval systems, thus increasing the sensitivity of system comparisons. Randomly partitioning the test document collection allows for multiple tests of a given system and topic (replicates). Bootstrap ANOVA can use these replicates to extract system-topic interactions—something not possible without replicates—yielding a more precise value for the system effect and a narrower confidence interval around that value. Experiments using multiple TREC collections demonstrate that removing the topic-system interactions substantially reduces the confidence intervals around the system effect as well as increases the number of significant pairwise differences found. Further, the method is robust against small changes in the number of partitions used, against variability in the documents that constitute the partitions, and the measure of effectiveness used to quantify system effectiveness.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {12},
numpages = {21},
keywords = {statistical analysis, test collections, Information retrieval, topic variance}
}

@article{10.1145/3086665,
author = {Wang, Hongning and Li, Rui and Shokouhi, Milad and Li, Hang and Chang, Yi},
title = {Search, Mining, and Their Applications on Mobile Devices: Introduction to the Special Issue},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086665},
doi = {10.1145/3086665},
abstract = {In recent years, mobile devices have become the most popular interface for users to retrieve and access information: recent reports show that users spend significantly more time and issue more search queries on mobile devices than on desktops in the United States.1 The accelerated growth of mobile usage brings unique opportunities to the information retrieval and data mining research communities.Mobile devices capture rich contextual and personal signals that can be leveraged to accurately predict users’ intent for serving more relevant content and can even proactively provide novel zero-query recommendations. Apple Siri, Google Now, and Microsoft Cortana are recent examples of such emerging systems. Furthermore, mobile devices constantly generate a huge amount of sensor footprints (e.g., GPS, motion sensors) and user activity data (e.g., used apps) that are often missing from their desktop counterparts. These new sources of implicit and explicit user feedback are valuable for discovering actionable knowledge, and designing better systems that serve each individual the right content at the right time and location. In addition, by aggregating mobile interactions across individuals, one can infer interesting conclusions beyond search and recommendation. Generating real-time traffic estimates is one example of such applications.This special issue focuses on research problems of search, mining, and their applications in mobile devices. Topics of interest in this special issue include but are not limited to mobile data mining and management, mobile search, personalization and recommendation, mobile user interfaces and human-computer interaction, and new applications in the mobile environment. The aim of this special issue is to bring together top experts across multiple disciplines, including information retrieval, data mining, mobile computing, and cyberphysical systems, such that academic and industrial researchers can exchange ideas and share the latest developments on the state of the art and practice of mobile search and mobile data mining.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {29},
numpages = {17},
keywords = {mobile user interfaces, Personalization, recommendation, new applications in mobile environment}
}

@article{10.1145/3091108,
author = {Li, Chenliang and Duan, Yu and Wang, Haoran and Zhang, Zhiqian and Sun, Aixin and Ma, Zongyang},
title = {Enhancing Topic Modeling for Short Texts with Auxiliary Word Embeddings},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3091108},
doi = {10.1145/3091108},
abstract = {Many applications require semantic understanding of short texts, and inferring discriminative and coherent latent topics is a critical and fundamental task in these applications. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Recent studies show that the Dirichlet Multinomial Mixture (DMM) model is effective for topic inference over short texts by assuming that each piece of short text is generated by a single topic. However, DMM has two main limitations. First, even though it seems reasonable to assume that each short text has only one topic because of its shortness, the definition of “shortness” is subjective and the length of the short texts is dataset dependent. That is, the single-topic assumption may be too strong for some datasets. To address this limitation, we propose to model the topic number as a Poisson distribution, allowing each short text to be associated with a small number of topics (e.g., one to three topics). This model is named PDMM. Second, DMM (and also PDMM) does not have access to background knowledge (e.g., semantic relations between words) when modeling short texts. When a human being interprets a piece of short text, the understanding is not solely based on its content words, but also their semantic relations. Recent advances in word embeddings offer effective learning of word semantic relations from a large corpus. Such auxiliary word embeddings enable us to address the second limitation. To this end, we propose to promote the semantically related words under the same topic during the sampling process, by using the generalized P\'{o}lya urn (GPU) model. Through the GPU model, background knowledge about word semantic relations learned from millions of external documents can be easily exploited to improve topic modeling for short texts. By directly extending the PDMM model with the GPU model, we propose two more effective topic models for short texts, named GPU-DMM and GPU-PDMM. Through extensive experiments on two real-world short text collections in two languages, we demonstrate that PDMM achieves better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to better accuracy in a text classification task, as an indirect evaluation. Both GPU-DMM and GPU-PDMM further improve topic coherence and text classification accuracy. GPU-PDMM outperforms GPU-DMM at the price of higher computational costs.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {11},
numpages = {30},
keywords = {short texts, Topic model, word embeddings}
}

@article{10.1145/3086702,
author = {Peng, Min and Gao, Wang and Wang, Hua and Zhang, Yanchun and Huang, Jiajia and Xie, Qianqian and Hu, Gang and Tian, Gang},
title = {Parallelization of Massive Textstream Compression Based on Compressed Sensing},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086702},
doi = {10.1145/3086702},
abstract = {Compressing textstreams generated by social networks can both reduce storage consumption and improve efficiency such as fast searching. However, the compression process is a challenge due to the large scale of textstreams. In this article, we propose a textstream compression framework based on compressed sensing theory and design a series of matching parallel procedures. The new approach uses a linear projection technique in the textstream compression process, achieving fast compression speed and low compression ratio. Two processes are executed by designing elaborated parallel procedures for efficient compressing and decompressing of large-scale textstreams. The decompression process is implemented for approximate solutions of underdetermined linear systems. Experimental results show that the new method can efficiently achieve the compression and decompression tasks on a large amount of text generated by social networks.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {17},
numpages = {18},
keywords = {Text stream compression, compressed sensing, parallelization}
}

@article{10.1145/3086700,
author = {Nguyen, Hung T. and Ghosh, Preetam and Mayo, Michael L. and Dinh, Thang N.},
title = {Social Influence Spectrum at Scale: Near-Optimal Solutions for Multiple Budgets at Once},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086700},
doi = {10.1145/3086700},
abstract = {Given a social network, the Influence Maximization (InfMax) problem seeks a seed set of k people that maximizes the expected influence for a viral marketing campaign. However, a solution for a particular seed size k is often not enough to make an informed choice regarding budget and cost-effectiveness.In this article, we propose the computation of Influence Spectrum (InfSpec), the maximum influence at each possible seed set size k within a given range [klower,kupper], thus providing optimal decision making for any availability of budget or influence requirements. As none of the existing methods for InfMax are efficient enough for the task in large networks, we propose LISA (sub-Linear Influence Spectrum Approximation), an efficient approximation algorithm for InfSpec (and also InfMax) with the best-known worst-case guarantees for billion-scale networks. LISA returns an (1-1/e -ϵ)-approximate influence spectrum with high probability (1-δ), where ϵ, δ are precision parameters provided by users. Using statistical decision theory, LISA has an asymptotic optimal running time (in addition to optimal approximation guarantee). In practice, LISA surpasses the state-of-the-art InfMax methods, taking less than 15 minutes to process a network of 41.7 million nodes and 1.5 billions edges.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {14},
numpages = {26},
keywords = {approximation algorithms, Influence maximization, influence spectrum}
}

@article{10.1145/3086695,
author = {Zhang, Jing and Tang, Jie and Ma, Cong and Tong, Hanghang and Jing, Yu and Li, Juanzi and Luyten, Walter and Moens, Marie-Francine},
title = {Fast and Flexible Top-<i>k</i> Similarity Search on Large Networks},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086695},
doi = {10.1145/3086695},
abstract = {Similarity search is a fundamental problem in network analysis and can be applied in many applications, such as collaborator recommendation in coauthor networks, friend recommendation in social networks, and relation prediction in medical information networks. In this article, we propose a sampling-based method using random paths to estimate the similarities based on both common neighbors and structural contexts efficiently in very large homogeneous or heterogeneous information networks. We give a theoretical guarantee that the sampling size depends on the error-bound ε, the confidence level (1-δ), and the path length T of each random walk. We perform an extensive empirical study on a Tencent microblogging network of 1,000,000,000 edges. We show that our algorithm can return top-k similar vertices for any vertex in a network 300\texttimes{} faster than the state-of-the-art methods. We develop a prototype system of recommending similar authors to demonstrate the effectiveness of our method.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {13},
numpages = {30},
keywords = {heterogeneous information network, similarity search, random path, Vertex similarity, social network}
}

@article{10.1145/3086676,
author = {Farseev, Aleksandr and Chua, Tat-Seng},
title = {Tweet Can Be Fit: Integrating Data from Wearable Sensors and Multiple Social Networks for Wellness Profile Learning},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086676},
doi = {10.1145/3086676},
abstract = {Wellness is a widely popular concept that is commonly applied to fitness and self-help products or services. Inference of personal wellness--related attributes, such as body mass index (BMI) category or disease tendency, as well as understanding of global dependencies between wellness attributes and users’ behavior, is of crucial importance to various applications in personal and public wellness domains. At the same time, the emergence of social media platforms and wearable sensors makes it feasible to perform wellness profiling for users from multiple perspectives. However, research efforts on wellness profiling and integration of social media and sensor data are relatively sparse. This study represents one of the first attempts in this direction. Specifically, we infer personal wellness attributes by utilizing our proposed multisource multitask wellness profile learning framework—WellMTL—which can handle data incompleteness and perform wellness attributes inference from sensor and social media data simultaneously. To gain insights into the data at a global level, we also examine correlations between first-order data representations and personal wellness attributes. Our experimental results show that the integration of sensor data and multiple social media sources can substantially boost the performance of individual wellness profiling.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {42},
numpages = {34},
keywords = {wellness profile learning, multitask learning, wearable sensors, Multiple sources integration, personal lifestyle assistance}
}

@article{10.1145/3072652,
author = {Vardasbi, Ali and Faili, Heshaam and Asadpour, Masoud},
title = {SWIM: Stepped Weighted Shell Decomposition Influence Maximization for Large-Scale Networks},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3072652},
doi = {10.1145/3072652},
abstract = {A considerable amount of research has been devoted to the proposition of scalable algorithms for influence maximization. A number of such scalable algorithms exploit the community structure of the network. Besides the community structure, real-world social networks possess a different property, known as the layer structure. In this article, we propose a method based on the layer structure to maximize the influence in huge networks. Conducting experiments on a number of real-world networks, we will show that our method outperforms the state-of-the-art algorithms by its time complexity while having similar or slightly better final influence spread. Furthermore, unlike its predecessors, our method is able to show a high entanglement between structure and dynamics by giving insight on the reason why different networks have two contrasting behaviors in their saturation. By “saturation,” we mean a state during the seed selection process after which adjoining new nodes to the initial set will have a negligible effect on increasing the influence spread. We will demonstrate that how our method can predict the saturation dynamics in the networks. This prediction can be used to identify the network structures that are more vulnerable to the fast spread of the rumors.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {6},
numpages = {33},
keywords = {Influence maximization, independent cascade model, weighted k-shell}
}

@article{10.1145/3072588,
author = {He, Jiangning and Liu, Hongyan},
title = {Mining Exploratory Behavior to Improve Mobile App Recommendations},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3072588},
doi = {10.1145/3072588},
abstract = {With the widespread usage of smart phones, more and more mobile apps are developed every day, playing an increasingly important role in changing our lifestyles and business models. In this trend, it becomes a hot research topic for developing effective mobile app recommender systems in both industry and academia. Compared with existing studies about mobile app recommendations, our research aims to improve the recommendation effectiveness based on analyzing a psychological trait of human beings, exploratory behavior, which refers to a type of variety-seeking behavior in unfamiliar domains. To this end, we propose a novel probabilistic model named Goal-oriented Exploratory Model (GEM), integrating exploratory behavior identification with personalized item recommendation. An algorithm combining collapsed Gibbs sampling and Expectation Maximization is developed for model learning and inference. Through extensive experiments conducted on a real dataset, the proposed model demonstrates superior recommendation performances and good interpretability compared with state-of-art recommendation methods. Moreover, empirical analyses on exploratory behavior find that individuals with a strong exploratory tendency exhibit behavioral patterns of variety seeking, risk taking, and higher involvement. Besides, mobile apps that are less popular or in the long tail possess greater potential of arousing exploratory behavior in individuals.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {32},
numpages = {37},
keywords = {personalized item recommendation, topic model, Mobile app recommendation, probabilistic generative model, exploratory behavior}
}

@article{10.1145/3086686,
author = {Brade\v{s}ko, Luka and Witbrock, Michael and Starc, Janez and Herga, Zala and Grobelnik, Marko and Mladeni\'{c}, Dunja},
title = {Curious Cat--Mobile, Context-Aware Conversational Crowdsourcing Knowledge Acquisition},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086686},
doi = {10.1145/3086686},
abstract = {Scaled acquisition of high-quality structured knowledge has been a longstanding goal of Artificial Intelligence research. Recent advances in crowdsourcing, the sheer number of Internet and mobile users, and the commercial availability of supporting platforms offer new tools for knowledge acquisition. This article applies context-aware knowledge acquisition that simultaneously satisfies users’ immediate information needs while extending its own knowledge using crowdsourcing. The focus is on knowledge acquisition on a mobile device, which makes the approach practical and scalable; in this context, we propose and implement a new KA approach that exploits an existing knowledge base to drive the KA process, communicate with the right people, and check for consistency of the user-provided answers. We tested the viability of the approach in experiments using our platform with real users around the world, and an existing large source of common-sense background knowledge. These experiments show that the approach is promising: the knowledge is estimated to be true and useful for users 95% of the time. Using context to proactively drive knowledge acquisition increased engagement and effectiveness (the number of new assertions/day/user increased for 175%). Using pre-existing and newly acquired knowledge also proved beneficial.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {33},
numpages = {46},
keywords = {crowdsourcing, location and context based knowledge acquisition, reasoning, dialogue systems, Sensor and location mining, chatbots, knowledge systems}
}

@article{10.1145/3041659,
author = {Sun, Yu and Yuan, Nicholas Jing and Xie, Xing and McDonald, Kieran and Zhang, Rui},
title = {Collaborative Intent Prediction with Real-Time Contextual Data},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3041659},
doi = {10.1145/3041659},
abstract = {Intelligent personal assistants on mobile devices such as Apple’s Siri and Microsoft Cortana are increasingly important. Instead of passively reacting to queries, they provide users with brand new proactive experiences that aim to offer the right information at the right time. It is, therefore, crucial for personal assistants to understand users’ intent, that is, what information users need now. Intent is closely related to context. Various contextual signals, including spatio-temporal information and users’ activities, can signify users’ intent. It is, however, challenging to model the correlation between intent and context. Intent and context are highly dynamic and often sequentially correlated. Contextual signals are usually sparse, heterogeneous, and not simultaneously available. We propose an innovative collaborative nowcasting model to jointly address all these issues. The model effectively addresses the complex sequential and concurring correlation between context and intent and recognizes users’ real-time intent with continuously arrived contextual signals. We extensively evaluate the proposed model with real-world data sets from a commercial personal assistant. The results validate the effectiveness the proposed model, and demonstrate its capability of handling the real-time flow of contextual signals. The studied problem and model also provide inspiring implications for new paradigms of recommendation on mobile intelligent devices.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {30},
numpages = {33},
keywords = {personal assistants, streaming context, Proactive experiences, nowcasting}
}

@article{10.1145/3041658,
author = {Yang, Cheng and Sun, Maosong and Zhao, Wayne Xin and Liu, Zhiyuan and Chang, Edward Y.},
title = {A Neural Network Approach to Jointly Modeling Social Networks and Mobile Trajectories},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3041658},
doi = {10.1145/3041658},
abstract = {Two characteristics of location-based services are mobile trajectories and the ability to facilitate social networking. The recording of trajectory data contributes valuable resources towards understanding users’ geographical movement behaviors. Social networking is possible when users are able to quickly connect to anyone nearby. A social network with location based services is known as location-based social network (LBSN). As shown in Cho et al. [2013], locations that are frequently visited by socially related persons tend to be correlated, which indicates the close association between social connections and trajectory behaviors of users in LBSNs. To better analyze and mine LBSN data, we need to have a comprehensive view of each of these two aspects, i.e., the mobile trajectory data and the social network.Specifically, we present a novel neural network model that can jointly model both social networks and mobile trajectories. Our model consists of two components: the construction of social networks and the generation of mobile trajectories. First we adopt a network embedding method for the construction of social networks: a networking representation can be derived for a user. The key to our model lies in generating mobile trajectories. Second, we consider four factors that influence the generation process of mobile trajectories: user visit preference, influence of friends, short-term sequential contexts, and long-term sequential contexts. To characterize the last two contexts, we employ the RNN and GRU models to capture the sequential relatedness in mobile trajectories at the short or long term levels. Finally, the two components are tied by sharing the user network representations. Experimental results on two important applications demonstrate the effectiveness of our model. In particular, the improvement over baselines is more significant when either network structure or trajectory data is sparse.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {36},
numpages = {28},
keywords = {Link prediction, friend recommendation, next-location recommendation, recurrent neural network}
}

@article{10.1145/3072614,
author = {Yang, Longqi and Hsieh, Cheng-Kang and Yang, Hongjian and Pollak, John P. and Dell, Nicola and Belongie, Serge and Cole, Curtis and Estrin, Deborah},
title = {Yum-Me: A Personalized Nutrient-Based Meal Recommender System},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3072614},
doi = {10.1145/3072614},
abstract = {Nutrient-based meal recommendations have the potential to help individuals prevent or manage conditions such as diabetes and obesity. However, learning people’s food preferences and making recommendations that simultaneously appeal to their palate and satisfy nutritional expectations are challenging. Existing approaches either only learn high-level preferences or require a prolonged learning period. We propose Yum-me, a personalized nutrient-based meal recommender system designed to meet individuals’ nutritional expectations, dietary restrictions, and fine-grained food preferences. Yum-me enables a simple and accurate food preference profiling procedure via a visual quiz-based user interface and projects the learned profile into the domain of nutritionally appropriate food options to find ones that will appeal to the user. We present the design and implementation of Yum-me and further describe and evaluate two innovative contributions. The first contriution is an open source state-of-the-art food image analysis model, named FoodDist. We demonstrate FoodDist’s superior performance through careful benchmarking and discuss its applicability across a wide array of dietary applications. The second contribution is a novel online learning framework that learns food preference from itemwise and pairwise image comparisons. We evaluate the framework in a field study of 227 anonymous users and demonstrate that it outperforms other baselines by a significant margin. We further conducted an end-to-end validation of the feasibility and effectiveness of Yum-me through a 60-person user study, in which Yum-me improves the recommendation acceptance rate by 42.63%.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {7},
numpages = {31},
keywords = {online learning, personalization, visual interface, Nutrient-based meal recommendation, food preferences}
}

@article{10.1145/3072606,
author = {Liang, Shangsong and Ren, Zhaochun and Zhao, Yukun and Ma, Jun and Yilmaz, Emine and Rijke, Maarten De},
title = {Inferring Dynamic User Interests in Streams of Short Texts for User Clustering},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3072606},
doi = {10.1145/3072606},
abstract = {User clustering has been studied from different angles. In order to identify shared interests, behavior-based methods consider similar browsing or search patterns of users, whereas content-based methods use information from the contents of the documents visited by the users. So far, content-based user clustering has mostly focused on static sets of relatively long documents. Given the dynamic nature of social media, there is a need to dynamically cluster users in the context of streams of short texts. User clustering in this setting is more challenging than in the case of long documents, as it is difficult to capture the users’ dynamic topic distributions in sparse data settings. To address this problem, we propose a dynamic user clustering topic model (UCT). UCT adaptively tracks changes of each user’s time-varying topic distributions based both on the short texts the user posts during a given time period and on previously estimated distributions. To infer changes, we propose a Gibbs sampling algorithm where a set of word pairs from each user is constructed for sampling. UCT can be used in two ways: (1) as a short-term dependency model that infers a user’s current topic distribution based on the user’s topic distributions during the previous time period only, and (2) as a long-term dependency model that infers a user’s current topic distributions based on the user’s topic distributions during multiple time periods in the past. The clustering results are explainable and human-understandable, in contrast to many other clustering algorithms. For evaluation purposes, we work with a dataset consisting of users and tweets from each user. Experimental results demonstrate the effectiveness of our proposed short-term and long-term dependency user clustering models compared to state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {10},
numpages = {37},
keywords = {Diversity, data streams, ad hoc retrieval}
}

@article{10.1145/3072591,
author = {Hou, Lei and Li, Juanzi and Li, Xiao-Li and Tang, Jie and Guo, Xiaofei},
title = {Learning to Align Comments to News Topics},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3072591},
doi = {10.1145/3072591},
abstract = {With the rapid proliferation of social media, increasingly more people express their opinions and reviews (user-generated content (UGC)) on recent news articles through various online services, such as news portals, forums, discussion groups, and microblogs. Clearly, identifying hot topics that users greatly care about can improve readers’ news browsing experience and facilitate research into interaction analysis between news and UGC. Furthermore, it is of great benefit to public opinion monitoring and management for both industry and government agencies. However, it is extremely time consuming, if not impossible, to manually examine the large amount of available social content. In this article, we formally define the news comment alignment problem and propose a novel framework that: (1) automatically extracts topics from a given news article and its associated comments, (2) identifies and extends positive examples with different degrees of confidence using three methods (i.e., hypersphere, density, and cluster chain), and (3) completes the alignment between news sentences and comments through a weighted-SVM classifier. Extensive experiments show that our proposed framework significantly outperforms state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {9},
numpages = {31},
keywords = {alignment, density, pu learning, User-generated content, cluster chain, dependent topic model}
}

@article{10.1145/3057282,
author = {Liang, Shangsong and Yilmaz, Emine and Shen, Hong and Rijke, Maarten De and Croft, W. Bruce},
title = {Search Result Diversification in Short Text Streams},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3057282},
doi = {10.1145/3057282},
abstract = {We consider the problem of search result diversification for streams of short texts. Diversifying search results in short text streams is more challenging than in the case of long documents, as it is difficult to capture the latent topics of short documents. To capture the changes of topics and the probabilities of documents for a given query at a specific time in a short text stream, we propose a dynamic Dirichlet multinomial mixture topic model, called D2M3, as well as a Gibbs sampling algorithm for the inference. We also propose a streaming diversification algorithm, SDA, that integrates the information captured by D2M3 with our proposed modified version of the PM-2 (Proportionality-based diversification Method -- second version) diversification algorithm. We conduct experiments on a Twitter dataset and find that SDA statistically significantly outperforms state-of-the-art non-streaming retrieval methods, plain streaming retrieval methods, as well as streaming diversification methods that use other dynamic topic models.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {8},
numpages = {35},
keywords = {data streams, Diversity, ad hoc retrieval}
}

@article{10.1145/3086677,
author = {Wang, Haoyu and Li, Yuanchun and Guo, Yao and Agarwal, Yuvraj and Hong, Jason I.},
title = {Understanding the Purpose of Permission Use in Mobile Apps},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086677},
doi = {10.1145/3086677},
abstract = {Mobile apps frequently request access to sensitive data, such as location and contacts. Understanding the purpose of why sensitive data is accessed could help improve privacy as well as enable new kinds of access control. In this article, we propose a text mining based method to infer the purpose of sensitive data access by Android apps. The key idea we propose is to extract multiple features from app code and then use those features to train a machine learning classifier for purpose inference. We present the design, implementation, and evaluation of two complementary approaches to infer the purpose of permission use, first using purely static analysis, and then using primarily dynamic analysis. We also discuss the pros and cons of both approaches and the trade-offs involved.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {43},
numpages = {40},
keywords = {Android, access control, purpose, Permission, privacy, mobile applications}
}

@article{10.1145/3057281,
author = {Wang, Senzhang and Zhang, Xiaoming and Cao, Jianping and He, Lifang and Stenneth, Leon and Yu, Philip S. and Li, Zhoujun and Huang, Zhiqiu},
title = {Computing Urban Traffic Congestions by Incorporating Sparse GPS Probe Data and Social Media Data},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3057281},
doi = {10.1145/3057281},
abstract = {Estimating urban traffic conditions of an arterial network with GPS probe data is a practically important while substantially challenging problem, and has attracted increasing research interests recently. Although GPS probe data is becoming a ubiquitous data source for various traffic related applications currently, they are usually insufficient for fully estimating traffic conditions of a large arterial network due to the low sampling frequency. To explore other data sources for more effectively computing urban traffic conditions, we propose to collect various traffic events such as traffic accident and jam from social media as complementary information. In addition, to further explore other factors that might affect traffic conditions, we also extract rich auxiliary information including social events, road features, Point of Interest (POI), and weather. With the enriched traffic data and auxiliary information collected from different sources, we first study the traffic co-congestion pattern mining problem with the aim of discovering which road segments geographically close to each other are likely to co-occur traffic congestion. A search tree based approach is proposed to efficiently discover the co-congestion patterns. These patterns are then used to help estimate traffic congestions and detect anomalies in a transportation network. To fuse the multisourced data, we finally propose a coupled matrix and tensor factorization model named TCE_R to more accurately complete the sparse traffic congestion matrix by collaboratively factorizing it with other matrices and tensors formed by other data. We evaluate the proposed model on the arterial network of downtown Chicago with 1,257 road segments whose total length is nearly 700 miles. The results demonstrate the superior performance of TCE_R by comprehensive comparison with existing approaches.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {40},
numpages = {30},
keywords = {data fusion, Social media, matrix factorization, traffic congestion}
}

@article{10.1145/3057278,
author = {Dong, Yuxiao and Chawla, Nitesh V. and Tang, Jie and Yang, Yang and Yang, Yang},
title = {User Modeling on Demographic Attributes in Big Mobile Social Networks},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3057278},
doi = {10.1145/3057278},
abstract = {Users with demographic profiles in social networks offer the potential to understand the social principles that underpin our highly connected world, from individuals, to groups, to societies. In this article, we harness the power of network and data sciences to model the interplay between user demographics and social behavior and further study to what extent users’ demographic profiles can be inferred from their mobile communication patterns. By modeling over 7 million users and 1 billion mobile communication records, we find that during the active dating period (i.e., 18--35 years old), users are active in broadening social connections with males and females alike, while after reaching 35 years of age people tend to keep small, closed, and same-gender social circles. Further, we formalize the demographic prediction problem of inferring users’ gender and age simultaneously. We propose a factor graph-based WhoAmI method to address the problem by leveraging not only the correlations between network features and users’ gender/age, but also the interrelations between gender and age. In addition, we identify a new problem—coupled network demographic prediction across multiple mobile operators—and present a coupled variant of the WhoAmI method to address its unique challenges. Our extensive experiments demonstrate the effectiveness, scalability, and applicability of the WhoAmI methods. Finally, our study finds a greater than 80% potential predictability for inferring users’ gender from phone call behavior and 73% for users’ age from text messaging interactions.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {35},
numpages = {33},
keywords = {ego networks, computational social science, social tie and triad, node attributes, mobile communication, demographic prediction, mobile phone data, Gender and age}
}

@article{10.1145/3017429,
author = {Cao, Da and He, Xiangnan and Nie, Liqiang and Wei, Xiaochi and Hu, Xia and Wu, Shunxiang and Chua, Tat-Seng},
title = {Cross-Platform App Recommendation by Jointly Modeling Ratings and Texts},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3017429},
doi = {10.1145/3017429},
abstract = {Over the last decade, the renaissance of Web technologies has transformed the online world into an application (App) driven society. While the abundant Apps have provided great convenience, their sheer number also leads to severe information overload, making it difficult for users to identify desired Apps. To alleviate the information overloading issue, recommender systems have been proposed and deployed for the App domain. However, existing work on App recommendation has largely focused on one single platform (e.g., smartphones), while it ignores the rich data of other relevant platforms (e.g., tablets and computers).In this article, we tackle the problem of cross-platform App recommendation, aiming at leveraging users’ and Apps’ data on multiple platforms to enhance the recommendation accuracy. The key advantage of our proposal is that by leveraging multiplatform data, the perpetual issues in personalized recommender systems—data sparsity and cold-start—can be largely alleviated. To this end, we propose a hybrid solution, STAR (short for “croSs-plaTform App Recommendation”) that integrates both numerical ratings and textual content from multiple platforms. In STAR, we innovatively represent an App as an aggregation of common features across platforms (e.g., App’s functionalities) and specific features that are dependent on the resided platform. In light of this, STAR can discriminate a user’s preference on an App by separating the user’s interest into two parts (either in the App’s inherent factors or platform-aware features). To evaluate our proposal, we construct two real-world datasets that are crawled from the App stores of iPhone, iPad, and iMac. Through extensive experiments, we show that our STAR method consistently outperforms highly competitive recommendation methods, justifying the rationality of our cross-platform App recommendation proposal and the effectiveness of our solution.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {37},
numpages = {27},
keywords = {cold-start, hybrid system, App recommendation, cross-platform}
}

@article{10.1145/3015462,
author = {Liu, Xuanzhe and Ai, Wei and Li, Huoran and Tang, Jian and Huang, Gang and Feng, Feng and Mei, Qiaozhu},
title = {Deriving User Preferences of Mobile Apps from Their Management Activities},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3015462},
doi = {10.1145/3015462},
abstract = {App marketplaces host millions of mobile apps that are downloaded billions of times. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behavior and preferences of mobile device users, infer the quality of apps, and improve user experience. Existing literature provides very limited knowledge about app management activities, due to the lack of app usage data at scale. This article takes the initiative to analyze a very large app management log collected through a leading Android app marketplace. The dataset covers 5 months of detailed downloading, updating, and uninstallation activities, which involve 17 million anonymized users and 1 million apps. We present a surprising finding that the metrics commonly used to rank apps in app stores do not truly reflect the users’ real attitudes. We then identify behavioral patterns from the app management activities that more accurately indicate user preferences of an app even when no explicit rating is available. A systematic statistical analysis is designed to evaluate machine learning models that are trained to predict user preferences using these behavioral patterns, which features an inverse probability weighting method to correct the selection biases in the training process.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {39},
numpages = {32},
keywords = {behavior analysis, Mobile apps, app management activities}
}

@article{10.1145/3091107,
author = {Umemoto, Kazutoshi and Song, Ruihua and Nie, Jian-Yun and Xie, Xing and Tanaka, Katsumi and Rui, Yong},
title = {Search by Screenshots for Universal Article Clipping in Mobile Apps},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3091107},
doi = {10.1145/3091107},
abstract = {To address the difficulty in clipping articles from various mobile applications (apps), we propose a novel framework called UniClip, which allows a user to snap a screen of an article to save the whole article in one place. The key task of the framework is search by screenshots, which has three challenges: (1) how to represent a screenshot; (2) how to formulate queries for effective article retrieval; and (3) how to identify the article from search results. We solve these by (1) segmenting a screenshot into structural units called blocks, (2) formulating effective search queries by considering the role of each block, and (3) aggregating the search result lists of multiple queries. To improve efficiency, we also extend our approach with learning-to-rank techniques so that we can find the desired article with only one query. Experimental results show that our approach achieves high retrieval performance (F1 = 0.868), which outperforms baselines based on keyword extraction and chunking methods. Learning-to-rank models improve our approach without learning by about 6%. A user study conducted to investigate the usability of UniClip reveals that ours is preferred by 21 out of 22 participants for its simplicity and effectiveness.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {34},
numpages = {29},
keywords = {search by screenshots, article clipping, mobile apps, Universal clipping}
}

@article{10.1145/3057283,
author = {Li, Xin and Jiang, Mingming and Hong, Huiting and Liao, Lejian},
title = {A Time-Aware Personalized Point-of-Interest Recommendation via High-Order Tensor Factorization},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3057283},
doi = {10.1145/3057283},
abstract = {Recently, location-based services (LBSs) have been increasingly popular for people to experience new possibilities, for example, personalized point-of-interest (POI) recommendations that leverage on the overlapping of user trajectories to recommend POI collaboratively. POI recommendation is yet challenging as it suffers from the problems known for the conventional recommendation tasks such as data sparsity and cold start, and to a much greater extent. In the literature, most of the related works apply collaborate filtering to POI recommendation while overlooking the personalized time-variant human behavioral tendency. In this article, we put forward a fourth-order tensor factorization-based ranking methodology to recommend users their interested locations by considering their time-varying behavioral trends while capturing their long-term preferences and short-term preferences simultaneously. We also propose to categorize the locations to alleviate data sparsity and cold-start issues, and accordingly new POIs that users have not visited can thus be bubbled up during the category ranking process. The tensor factorization is carefully studied to prune the irrelevant factors to the ranking results to achieve efficient POI recommendations. The experimental results validate the efficacy of our proposed mechanism, which outperforms the state-of-the-art approaches significantly.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {31},
numpages = {23},
keywords = {HITS algorithm, tensor factorization, Time-aware POI recommendation}
}

@article{10.1145/3057280,
author = {Song, Xuan and Shibasaki, Ryosuke and Yuan, Nicholos Jing and Xie, Xing and Li, Tao and Adachi, Ryutaro},
title = {DeepMob: Learning Deep Knowledge of Human Emergency Behavior and Mobility from Big and Heterogeneous Data},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3057280},
doi = {10.1145/3057280},
abstract = {The frequency and intensity of natural disasters has increased significantly in recent decades, and this trend is expected to continue. Hence, understanding and predicting human evacuation behavior and mobility will play a vital role in planning effective humanitarian relief, disaster management, and long-term societal reconstruction. However, existing models are shallow models, and it is difficult to apply them for understanding the “deep knowledge” of human mobility. Therefore, in this study, we collect big and heterogeneous data (e.g., GPS records of 1.6 million users over 3 years, data on earthquakes that have occurred in Japan over 4 years, news report data, and transportation network data), and we build an intelligent system, namely, DeepMob, for understanding and predicting human evacuation behavior and mobility following different types of natural disasters. The key component of DeepMob is based on a deep learning architecture that aims to understand the basic laws that govern human behavior and mobility following natural disasters, from big and heterogeneous data. Furthermore, based on the deep learning model, DeepMob can accurately predict or simulate a person’s future evacuation behaviors or evacuation routes under different disaster conditions. Experimental results and validations demonstrate the efficiency and superior performance of our system, and suggest that human mobility following disasters may be predicted and simulated more easily than previously thought.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {41},
numpages = {19},
keywords = {urban computing, spatiotemporal data mining, disaster informatics, Human mobility}
}

@article{10.1145/3015458,
author = {Yao, Yuan and Zhao, Wayne Xin and Wang, Yaojing and Tong, Hanghang and Xu, Feng and Lu, Jian},
title = {Version-Aware Rating Prediction for Mobile App Recommendation},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3015458},
doi = {10.1145/3015458},
abstract = {With the great popularity of mobile devices, the amount of mobile apps has grown at a more dramatic rate than ever expected. A technical challenge is how to recommend suitable apps to mobile users. In this work, we identify and focus on a unique characteristic that exists in mobile app recommendation—that is, an app usually corresponds to multiple release versions. Based on this characteristic, we propose a fine-grain version-aware app recommendation problem. Instead of directly learning the users’ preferences over the apps, we aim to infer the ratings of users on a specific version of an app. However, the user-version rating matrix will be sparser than the corresponding user-app rating matrix, making existing recommendation methods less effective. In view of this, our approach has made two major extensions. First, we leverage the review text that is associated with each rating record; more importantly, we consider two types of version-based correlations. The first type is to capture the temporal correlations between multiple versions within the same app, and the second type of correlation is to capture the aggregation correlations between similar apps. Experimental results on a large dataset demonstrate the superiority of our approach over several competitive methods.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {38},
numpages = {33},
keywords = {version correlation, recommender systems, App rating prediction}
}

@article{10.1145/3052769,
author = {Hu, Liang and Cao, Longbing and Cao, Jian and Gu, Zhiping and Xu, Guandong and Wang, Jie},
title = {Improving the Quality of Recommendations for Users and Items in the Tail of Distribution},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3052769},
doi = {10.1145/3052769},
abstract = {Short-head and long-tail distributed data are widely observed in the real world. The same is true of recommender systems (RSs), where a small number of popular items dominate the choices and feedback data while the rest only account for a small amount of feedback. As a result, most RS methods tend to learn user preferences from popular items since they account for most data. However, recent research in e-commerce and marketing has shown that future businesses will obtain greater profit from long-tail selling. Yet, although the number of long-tail items and users is much larger than that of short-head items and users, in reality, the amount of data associated with long-tail items and users is much less. As a result, user preferences tend to be popularity-biased. Furthermore, insufficient data makes long-tail items and users more vulnerable to shilling attack. To improve the quality of recommendations for items and users in the tail of distribution, we propose a coupled regularization approach that consists of two latent factor models: C-HMF, for enhancing credibility, and S-HMF, for emphasizing specialty on user choices. Specifically, the estimates learned from C-HMF and S-HMF recurrently serve as the empirical priors to regularize one another. Such coupled regularization leads to the comprehensive effects of final estimates, which produce more qualitative predictions for both tail users and tail items. To assess the effectiveness of our model, we conduct empirical evaluations on large real-world datasets with various metrics. The results prove that our approach significantly outperforms the compared methods.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {25},
numpages = {37},
keywords = {recurrent mutual regularization, trust and reputation systems, long tail, multi-objective learning, Recommender systems}
}

@article{10.1145/3052775,
author = {Shirakawa, Masumi and Hara, Takahiro and Nishio, Shojiro},
title = {IDF for Word N-Grams},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3052775},
doi = {10.1145/3052775},
abstract = {Inverse Document Frequency (IDF) is widely accepted term weighting scheme whose robustness is supported by many theoretical justifications. However, applying IDF to word N-grams (or simply N-grams) of any length without relying on heuristics has remained a challenging issue. This article describes a theoretical extension of IDF to handle N-grams. First, we elucidate the theoretical relationship between IDF and information distance, a universal metric defined by the Kolmogorov complexity. Based on our understanding of this relationship, we propose N-gram IDF, a new IDF family that gives fair weights to words and phrases of any length. Based only on the magnitude relation of N-gram IDF weights, dominant N-grams among overlapping N-grams can be determined. We also propose an efficient method to compute the N-gram IDF weights of all N-grams by leveraging the enhanced suffix array and wavelet tree. Because the exact computation of N-gram IDF provably requires significant computational cost, we modify it to a fast approximation method that can estimate weight errors analytically and maintain application-level performance. Empirical evaluations with unsupervised/supervised key term extraction and web search query segmentation with various experimental settings demonstrate the robustness and language-independent nature of the proposed N-gram IDF.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {5},
numpages = {38},
keywords = {wavelet tree, Poisson distribution, Term weighting, multiword expression, information distance, Kolmogorov complexity}
}

@article{10.1145/3052770,
author = {Huang, Minlie and Qian, Qiao and Zhu, Xiaoyan},
title = {Encoding Syntactic Knowledge in Neural Networks for Sentiment Classification},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3052770},
doi = {10.1145/3052770},
abstract = {Phrase/Sentence representation is one of the most important problems in natural language processing. Many neural network models such as Convolutional Neural Network (CNN), Recursive Neural Network (RNN), and Long Short-Term Memory (LSTM) have been proposed to learn representations of phrase/sentence, however, rich syntactic knowledge has not been fully explored when composing a longer text from its shorter constituent words. In most traditional models, only word embeddings are utilized to compose phrase/sentence representations, while the syntactic information of words is yet to be explored. In this article, we discover that encoding syntactic knowledge (part-of-speech tag) in neural networks can enhance sentence/phrase representation. Specifically, we propose to learn tag-specific composition functions and tag embeddings in recursive neural networks, and propose to utilize POS tags to control the gates of tree-structured LSTM networks. We evaluate these models on two benchmark datasets for sentiment classification, and demonstrate that improvements can be obtained with such syntactic knowledge encoded.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {26},
numpages = {27},
keywords = {Neural networks, recursive neural network, sentiment analysis, deep learning, sentiment classification, representation learning, long short-term memory}
}

@article{10.1145/3052768,
author = {Moffat, Alistair and Bailey, Peter and Scholer, Falk and Thomas, Paul},
title = {Incorporating User Expectations and Behavior into the Measurement of Search Effectiveness},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3052768},
doi = {10.1145/3052768},
abstract = {Information retrieval systems aim to help users satisfy information needs. We argue that the goal of the person using the system, and the pattern of behavior that they exhibit as they proceed to attain that goal, should be incorporated into the methods and techniques used to evaluate the effectiveness of IR systems, so that the resulting effectiveness scores have a useful interpretation that corresponds to the users’ search experience. In particular, we investigate the role of search task complexity, and show that it has a direct bearing on the number of relevant answer documents sought by users in response to an information need, suggesting that useful effectiveness metrics must be goal sensitive. We further suggest that user behavior while scanning results listings is affected by the rate at which their goal is being realized, and hence that appropriate effectiveness metrics must be adaptive to the presence (or not) of relevant documents in the ranking. In response to these two observations, we present a new effectiveness metric, INST, that has both of the desired properties: INST employs a parameter T, a direct measure of the user’s search goal that adjusts the top-weightedness of the evaluation score; moreover, as progress towards the target T is made, the modeled user behavior is adapted, to reflect the remaining expectations. INST is experimentally compared to previous effectiveness metrics, including Average Precision (AP), Normalized Discounted Cumulative Gain (NDCG), and Rank-Biased Precision (RBP), demonstrating our claims as to INST’s usefulness. Like RBP, INST is a weighted-precision metric, meaning that each score can be accompanied by a residual that quantifies the extent of the score uncertainty caused by unjudged documents. As part of our experimentation, we use crowd-sourced data and score residuals to demonstrate that a wide range of queries arise for even quite specific information needs, and that these variant queries introduce significant levels of residual uncertainty into typical experimental evaluations. These causes of variability have wide-reaching implications for experiment design, and for the construction of test collections.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {24},
numpages = {38},
keywords = {query, search, relevance measures, test collections, effectiveness metric, User behavior}
}

@article{10.1145/3041657,
author = {Jiang, Jiawei and Tong, Yunhai and Lu, Hua and Cui, Bin and Lei, Kai and Yu, Lele},
title = {GVoS: A General System for Near-Duplicate Video-Related Applications on Storm},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3041657},
doi = {10.1145/3041657},
abstract = {The exponential increase of online videos greatly enriches the life of users but also brings huge numbers of near-duplicate videos (NDVs) that seriously challenge the video websites. The video websites entail NDV-related applications such as detection of copyright violation, video monitoring, video re-ranking, and video recommendation. Since these applications adopt different features and different processing procedures due to diverse scenarios, constructing separate and special-purpose systems for them incurs considerable costs on design, implementation, and maintenance. In this article, we propose a general NDV system on Storm (GVoS)—a popular distributed real-time stream processing platform—to simultaneously support a wide variety of video applications. The generality of GVoS is achieved in two aspects. First, we extract the reusable components from various applications. Second, we conduct the communication between components via a mechanism called Stream Shared Message (SSM) that contains the video-related data. Furthermore, we present an algorithm to reduce the size of SSM in order to avoid the data explosion and decrease the network latency. The experimental results demonstrate that GVoS can achieve performance almost the same as the customized systems. Meanwhile, GVoS accomplishes remarkably higher systematic versatility and efficiently facilitates the development of various NDV-related applications.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {3},
numpages = {36},
keywords = {retrieval and detection, real-time processing, Near duplicate video, general system}
}

@article{10.1145/3041656,
author = {Alkwai, Lulwah M. and Nelson, Michael L. and Weigle, Michele C.},
title = {Comparing the Archival Rate of Arabic, English, Danish, and Korean Language Web Pages},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3041656},
doi = {10.1145/3041656},
abstract = {It has long been suspected that web archives and search engines favor Western and English language webpages. In this article, we quantitatively explore how well indexed and archived Arabic language webpages are as compared to those from other languages. We began by sampling 15,092 unique URIs from three different website directories: DMOZ (multilingual), Raddadi, and Star28 (the last two primarily Arabic language). Using language identification tools, we eliminated pages not in the Arabic language (e.g., English-language versions of Aljazeera pages) and culled the collection to 7,976 Arabic language webpages. We then used these 7,976 pages and crawled the live web and web archives to produce a collection of 300,646 Arabic language pages. We compared the analysis of Arabic language pages with that of English, Danish, and Korean language pages. First, for each language, we sampled unique URIs from DMOZ; then, using language identification tools, we kept only pages in the desired language. Finally, we crawled the archived and live web to collect a larger sample of pages in English, Danish, or Korean. In total for the four languages, we analyzed over 500,000 webpages. We discovered: (1) English has a higher archiving rate than Arabic, with 72.04% archived. However, Arabic has a higher archiving rate than Danish and Korean, with 53.36% of Arabic URIs archived, followed by Danish and Korean with 35.89% and 32.81% archived, respectively. (2) Most Arabic and English language pages are located in the United States; only 14.84% of the Arabic URIs had an Arabic country code top-level domain (e.g., sa) and only 10.53% had a GeoIP in an Arabic country. Most Danish-language pages were located in Denmark, and most Korean-language pages were located in South Korea. (3) The presence of a webpage in a directory positively impacts indexing and presence in the DMOZ directory, specifically, positively impacts archiving in all four languages. In this work, we show that web archives and search engines favor English pages. However, it is not universally true for all Western-language webpages because, in this work, we show that Arabic webpages have a higher archival rate than Danish language webpages.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {1},
numpages = {34},
keywords = {indexing, Arabic web, Korean web, digital preservation, English web, Web archiving, Danish web}
}

@article{10.1145/2975590,
author = {Sadeghi, Seyedeh Sargol and Blanco, Roi and Mika, Peter and Sanderson, Mark and Scholer, Falk and Vallet, David},
title = {Re-Finding Behaviour in Vertical Domains},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2975590},
doi = {10.1145/2975590},
abstract = {Re-finding is the process of searching for information that a user has previously encountered and is a common activity carried out with information retrieval systems. In this work, we investigate re-finding in the context of vertical search, differentiating and modeling user re-finding behavior within different media and topic domains, including images, news, reference material, and movies. We distinguish the re-finding behavior in vertical domains from re-finding in a general search context and engineer features that are effective in differentiating re-finding across the domains. The features are then used to build machine-learned models, achieving an accuracy of re-finding detection in verticals of 85.7% on average. Our results demonstrate that detecting re-finding in specific verticals is more difficult than examining re-finding for general search tasks. We then investigate the effectiveness of differentiating re-finding behavior in two restricted contexts: We consider the case where the history of a searcher’s interactions with the search system is not available. In this scenario, our features and models achieve an average accuracy of 77.5% across the domains. We then examine the detection of re-finding during the early part of a search session. Both of these restrictions represent potential real-world search scenarios, where a system is attempting to learn about a user but may have limited information available. Finally, we investigate in which types of domains re-finding is most difficult. Here, it would appear that re-finding images is particularly challenging for users. This research has implications for search engine design, in terms of adapting search results by predicting the type of user tasks and potentially enabling the presentation of vertical-specific results when re-finding is identified. To the best of our knowledge, this is the first work to investigate the issue of vertical re-finding.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {21},
numpages = {30},
keywords = {predictive models, vertical, difficulty, search feature, Re-finding behavior}
}

@article{10.1145/3052772,
author = {Zhang, Dongxiang and Li, Yuchen and Fan, Ju and Gao, Lianli and Shen, Fumin and Shen, Heng Tao},
title = {Processing Long Queries Against Short Text: Top-<i>k</i> Advertisement Matching in News Stream Applications},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3052772},
doi = {10.1145/3052772},
abstract = {Many real applications in real-time news stream advertising call for efficient processing of long queries against short text. In such applications, dynamic news feeds are regarded as queries to match against an advertisement (ad) database for retrieving the k most relevant ads. The existing approaches to keyword retrieval cannot work well in this search scenario when queries are triggered at a very high frequency. To address the problem, we introduce new techniques to significantly improve search performance. First, we devise a two-level partitioning for tight upper bound estimation and a lazy evaluation scheme to delay full evaluation of unpromising candidates, which can bring three to four times performance boosting in a database with 7 million ads. Second, we propose a novel rank-aware block-oriented inverted index to further improve performance. In this index scheme, each entry in an inverted list is assigned a rank according to its importance in the ad. Then, we introduce a block-at-a-time search strategy based on the index scheme to support a much tighter upper bound estimation and a very early termination. We have conducted experiments with real datasets, and the results show that the rank-aware method can further improve performance by an order of magnitude.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {28},
numpages = {27},
keywords = {short text, Long queries, top-k retrieval, inverted index, rank-aware partitioning}
}

@article{10.1145/3052771,
author = {Zhang, Dongxiang and Nie, Liqiang and Luan, Huanbo and Tan, Kian-Lee and Chua, Tat-Seng and Shen, Heng Tao},
title = {Compact Indexing and Judicious Searching for Billion-Scale Microblog Retrieval},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3052771},
doi = {10.1145/3052771},
abstract = {In this article, we study the problem of efficient top-k disjunctive query processing in a huge microblog dataset. In terms of compact indexing, we categorize the keywords into rare terms and common terms based on inverse document frequency (idf) and propose tailored block-oriented organization to save memory consumption. In terms of fast searching, we classify the queries into three types based on term category and judiciously design an efficient search algorithm for each type. We conducted extensive experiments on a billion-scale Twitter dataset and examined the performance with both simple and more advanced ranking functions. The results showed that with much smaller index size, our search algorithm achieves a factor of 2--3 times faster speedup over state-of-the-art solutions in both ranking scenarios.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {27},
numpages = {24},
keywords = {disjunctive keyword search, Top-k, billion-scale, microblg}
}

@article{10.1145/3015466,
author = {White, Ryen W. and Diaz, Fernando and Guo, Qi},
title = {Search Result Prefetching on Desktop and Mobile},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3015466},
doi = {10.1145/3015466},
abstract = {Search result examination is an important part of searching. High page load latency for landing pages (clicked search results) can reduce the efficiency of the search process. Proactively prefetching landing pages in advance of clickthrough can save searchers valuable time. However, prefetching consumes resources (primarily bandwidth and battery) that are wasted unless the prefetched results are requested by searchers. Balancing the costs in prefetching particular results against the benefits in reduced latency to searchers represents the search result prefetching challenge. In this article, we introduce this challenge and present methods to address it in both desktop and mobile settings. Our methods leverage searchers’ cursor movements (on desktop) and viewport-based viewing behavior (on mobile) on search engine result pages (SERPs) in real time to dynamically estimate the result that searchers will request next. We demonstrate through large-scale log analysis that our approach significantly outperforms three strong baselines that prefetch results based on (i) the search engine result ranking (prefetch top-ranked results), (ii) past SERP clicks from all searchers for the query (prefetch popular results), or (iii) past SERP clicks from the current searcher for the query (prefetch results that the searcher prefers). Our promising findings have implications for the design of search support in desktop and mobile settings that makes the search process more efficient.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {23},
numpages = {34},
keywords = {Search result prefetching, mobile, viewport, mouse cursor, attention modeling}
}

@article{10.1145/3052774,
author = {Wang, Xiang and Nie, Liqiang and Song, Xuemeng and Zhang, Dongxiang and Chua, Tat-Seng},
title = {Unifying Virtual and Physical Worlds: Learning Toward Local and Global Consistency},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3052774},
doi = {10.1145/3052774},
abstract = {Event-based social networking services, such as Meetup, are capable of linking online virtual interactions to offline physical activities. Compared to mono online social networking services (e.g., Twitter and Google+), such dual networks provide a complete picture of users’ online and offline behaviors that more often than not are compatible and complementary. In the light of this, we argue that joint learning over dual networks offers us a better way to comprehensively understand user behaviors and their underlying organizational principles. Despite its value, few efforts have been dedicated to jointly considering the following factors within a unified model: (1) local user contextualization, (2) global structure coherence, and (3) effectiveness evaluation. Toward this end, we propose a novel dual clustering model for community detection over dual networks to jointly model local consistency for a specific user and global consistency of partitioning results across networks. We theoretically derived its solution. In addition, we verified our model regarding multiple metrics from different aspects and applied it to the application of event attendance prediction.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {4},
numpages = {26},
keywords = {Event-based social networks, global consistency, local consistency}
}

@article{10.1145/3052773,
author = {Pibiri, Giulio Ermanno and Venturini, Rossano},
title = {Clustered Elias-Fano Indexes},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3052773},
doi = {10.1145/3052773},
abstract = {State-of-the-art encoders for inverted indexes compress each posting list individually. Encoding clusters of posting lists offers the possibility of reducing the redundancy of the lists while maintaining a noticeable query processing speed.In this article, we propose a new index representation based on clustering the collection of posting lists and, for each created cluster, building an ad hoc reference list with respect to which all lists in the cluster are encoded with Elias-Fano. We describe a posting lists clustering algorithm tailored for our encoder and two methods for building the reference list for a cluster. Both approaches are heuristic and differ in the way postings are added to the reference list: according to their frequency in the cluster or according to the number of bits necessary for their representation.The extensive experimental analysis indicates that significant space reductions are indeed possible, beating the best state-of-the-art encoders.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {2},
numpages = {33},
keywords = {performance, Elias-Fano encoding, inverted indexes}
}

@article{10.1145/3007186,
author = {Konow, Roberto and Navarro, Gonzalo and Clarke, Charles L. A. and L\'{o}pez-Ort\'{\i}z, Alejandro},
title = {Inverted Treaps},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3007186},
doi = {10.1145/3007186},
abstract = {We introduce a new representation of the inverted index that performs faster ranked unions and intersections while using similar space. Our index is based on the treap data structure, which allows us to intersect/merge the document identifiers while simultaneously thresholding by frequency, instead of the costlier two-step classical processing methods. To achieve compression, we represent the treap topology using different alternative compact data structures. Further, the treap invariants allow us to elegantly encode differentially both document identifiers and frequencies. We also show how to extend this representation to support incremental updates over the index. Results show that, under the tf-idf scoring scheme, our index uses about the same space as state-of-the-art compact representations, while performing up to 2--20 times faster on ranked single-word, union, or intersection queries. Under the BM25 scoring scheme, our index may use up to 40% more space than the others and outperforms them less frequently but still reaches improvement factors of 2--20 in the best cases. The index supporting incremental updates poses an overhead of 50%--100% over the static variants in terms of space, construction, and query time.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {22},
numpages = {45},
keywords = {Compact data structure, top-k document retrieval}
}

@article{10.1145/3003725,
author = {Zhang, Dongxiang and Guo, Long and Nie, Liqiang and Shao, Jie and Wu, Sai and Shen, Heng Tao},
title = {Targeted Advertising in Public Transportation Systems with Quantitative Evaluation},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3003725},
doi = {10.1145/3003725},
abstract = {In spite of vast business potential, targeted advertising in public transportation systems is a grossly unexplored research area. For instance, SBS Transit in Singapore can reach 1 billion passengers per year but the annual advertising revenue contributes less than $35 million. To bridge the gap, we propose a probabilistic data model that captures the motion patterns and user interests so as to quantitatively evaluate the impact of an advertisement among the passengers. In particular, we leverage hundreds of millions of bus/train boarding transaction records to quantitatively estimate the probability as well as the extent of a user being influenced by an ad. Based on the influence model, we study a top-k retrieval problem for bus/train ad recommendation, which acts as a primitive operator to support various advanced applications. We solve the retrieval problem efficiently to support real-time decision making. In the experimental study, we use the dataset from SBS Transit as a case study to verify the effectiveness and efficiency of our proposed methodologies.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {20},
numpages = {29},
keywords = {user interest, probabilistic data model, Targeted advertising, quantitative evaluation, public transportation systems, mobility pattern}
}

@article{10.1145/3002172,
author = {Maddalena, Eddy and Mizzaro, Stefano and Scholer, Falk and Turpin, Andrew},
title = {On Crowdsourcing Relevance Magnitudes for Information Retrieval Evaluation},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3002172},
doi = {10.1145/3002172},
abstract = {Magnitude estimation is a psychophysical scaling technique for the measurement of sensation, where observers assign numbers to stimuli in response to their perceived intensity. We investigate the use of magnitude estimation for judging the relevance of documents for information retrieval evaluation, carrying out a large-scale user study across 18 TREC topics and collecting over 50,000 magnitude estimation judgments using crowdsourcing. Our analysis shows that magnitude estimation judgments can be reliably collected using crowdsourcing, are competitive in terms of assessor cost, and are, on average, rank-aligned with ordinal judgments made by expert relevance assessors.We explore the application of magnitude estimation for IR evaluation, calibrating two gain-based effectiveness metrics, nDCG and ERR, directly from user-reported perceptions of relevance. A comparison of TREC system effectiveness rankings based on binary, ordinal, and magnitude estimation relevance shows substantial variation; in particular, the top systems ranked using magnitude estimation and ordinal judgments differ substantially. Analysis of the magnitude estimation scores shows that this effect is due in part to varying perceptions of relevance: different users have different perceptions of the impact of relative differences in document relevance. These results have direct implications for IR evaluation, suggesting that current assumptions about a single view of relevance being sufficient to represent a population of users are unlikely to hold.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {19},
numpages = {32},
keywords = {relevance, Magnitude estimation, relevance assessments, evaluation}
}

@article{10.1145/3001833,
author = {Miao, Zhongchen and Chen, Kai and Fang, Yi and He, Jianhua and Zhou, Yi and Zhang, Wenjun and Zha, Hongyuan},
title = {Cost-Effective Online Trending Topic Detection and Popularity Prediction in Microblogging},
year = {2016},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3001833},
doi = {10.1145/3001833},
abstract = {Identifying topic trends on microblogging services such as Twitter and estimating those topics’ future popularity have great academic and business value, especially when the operations can be done in real time. For any third party, however, capturing and processing such huge volumes of real-time data in microblogs are almost infeasible tasks, as there always exist API (Application Program Interface) request limits, monitoring and computing budgets, as well as timeliness requirements. To deal with these challenges, we propose a cost-effective system framework with algorithms that can automatically select a subset of representative users in microblogging networks in offline, under given cost constraints. Then the proposed system can online monitor and utilize only these selected users’ real-time microposts to detect the overall trending topics and predict their future popularity among the whole microblogging network. Therefore, our proposed system framework is practical for real-time usage as it avoids the high cost in capturing and processing full real-time data, while not compromising detection and prediction performance under given cost constraints. Experiments with real microblogs dataset show that by tracking only 500 users out of 0.6 million users and processing no more than 30,000 microposts daily, about 92% trending topics could be detected and predicted by the proposed system and, on average, more than 10 hours earlier than they appear in official trends lists.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {18},
numpages = {36},
keywords = {prediction, microblogging, Topic detection, cost}
}

@article{10.1145/3001583,
author = {Connor, Richard and Cardillo, Franco Alberto and Vadicamo, Lucia and Rabitti, Fausto},
title = {Hilbert Exclusion: Improved Metric Search through Finite Isometric Embeddings},
year = {2016},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3001583},
doi = {10.1145/3001583},
abstract = {Most research into similarity search in metric spaces relies on the triangle inequality property. This property allows the space to be arranged according to relative distances to avoid searching some subspaces. We show that many common metric spaces, notably including those using Euclidean and Jensen-Shannon distances, also have a stronger property, sometimes called the four-point property: In essence, these spaces allow an isometric embedding of any four points in three-dimensional Euclidean space, as well as any three points in two-dimensional Euclidean space. In fact, we show that any space that is isometrically embeddable in Hilbert space has the stronger property. This property gives stronger geometric guarantees, and one in particular, which we name the Hilbert Exclusion property, allows any indexing mechanism which uses hyperplane partitioning to perform better. One outcome of this observation is that a number of state-of-the-art indexing mechanisms over high-dimensional spaces can be easily refined to give a significant increase in performance; furthermore, the improvement given is greater in higher dimensions. This therefore leads to a significant improvement in the cost of metric search in these spaces.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {17},
numpages = {27},
keywords = {Similarity search, four-point property, metric indexing, metric space, Hilbert embedding}
}

@article{10.1145/2988230,
author = {Liu, Yiqun and Xie, Xiaohui and Wang, Chao and Nie, Jian-Yun and Zhang, Min and Ma, Shaoping},
title = {Time-Aware Click Model},
year = {2016},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2988230},
doi = {10.1145/2988230},
abstract = {Click-through information is considered as a valuable source of users’ implicit relevance feedback for commercial search engines. As existing studies have shown that the search result position in a search engine result page (SERP) has a very strong influence on users’ examination behavior, most existing click models are position based, assuming that users examine results from top to bottom in a linear fashion. Although these click models have been successful, most do not take temporal information into account. As many existing studies have shown, click dwell time and click sequence information are strongly correlated with users’ perceived relevance and search satisfaction. Incorporating temporal information may be important to improve performance of user click models for Web searches. In this article, we investigate the problem of properly incorporating temporal information into click models. We first carry out a laboratory eye-tracking study to analyze users’ examination behavior in different click sequences and find that the user common examination path among adjacent clicks is linear. Next, we analyze the user dwell time distribution in different search logs and find that we cannot simply use a click dwell time threshold (e.g., 30 seconds) to distinguish relevant/irrelevant results. Finally, we propose a novel time-aware click model (TACM), which captures the temporal information of user behavior. We compare the TACM to several existing click models using two real-world search engine logs. Experimental results show that the TACM outperforms other click models in terms of both predicting click behavior (perplexity) and estimating result relevance (NDCG).},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {24},
keywords = {click sequence, click dwell time, Click model}
}

@article{10.1145/2987380,
author = {Dato, Domenico and Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Perego, Raffaele and Tonellotto, Nicola and Venturini, Rossano},
title = {Fast Ranking with Additive Ensembles of Oblivious and Non-Oblivious Regression Trees},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2987380},
doi = {10.1145/2987380},
abstract = {Learning-to-Rank models based on additive ensembles of regression trees have been proven to be very effective for scoring query results returned by large-scale Web search engines. Unfortunately, the computational cost of scoring thousands of candidate documents by traversing large ensembles of trees is high. Thus, several works have investigated solutions aimed at improving the efficiency of document scoring by exploiting advanced features of modern CPUs and memory hierarchies. In this article, we present QuickScorer, a new algorithm that adopts a novel cache-efficient representation of a given tree ensemble, performs an interleaved traversal by means of fast bitwise operations, and supports ensembles of oblivious trees. An extensive and detailed test assessment is conducted on two standard Learning-to-Rank datasets and on a novel very large dataset we made publicly available for conducting significant efficiency tests. The experiments show unprecedented speedups over the best state-of-the-art baselines ranging from 1.9 \texttimes{} to 6.6 \texttimes{} . The analysis of low-level profiling traces shows that QuickScorer efficiency is due to its cache-aware approach in terms of both data layout and access patterns and to a control flow that entails very low branch mis-prediction rates.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {15},
numpages = {31},
keywords = {cache-awareness, additive ensembles of regression trees, document scoring, efficiency, Learning to rank}
}

@article{10.1145/2976737,
author = {Hu, Liang and Cao, Longbing and Cao, Jian and Gu, Zhiping and Xu, Guandong and Yang, Dingyu},
title = {Learning Informative Priors from Heterogeneous Domains to Improve Recommendation in Cold-Start User Domains},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2976737},
doi = {10.1145/2976737},
abstract = {In the real-world environment, users have sufficient experience in their focused domains but lack experience in other domains. Recommender systems are very helpful for recommending potentially desirable items to users in unfamiliar domains, and cross-domain collaborative filtering is therefore an important emerging research topic. However, it is inevitable that the cold-start issue will be encountered in unfamiliar domains due to the lack of feedback data. The Bayesian approach shows that priors play an important role when there are insufficient data, which implies that recommendation performance can be significantly improved in cold-start domains if informative priors can be provided. Based on this idea, we propose a Weighted Irregular Tensor Factorization (WITF) model to leverage multi-domain feedback data across all users to learn the cross-domain priors w.r.t. both users and items. The features learned from WITF serve as the informative priors on the latent factors of users and items in terms of weighted matrix factorization models. Moreover, WITF is a unified framework for dealing with both explicit feedback and implicit feedback. To prove the effectiveness of our approach, we studied three typical real-world cases in which a collection of empirical evaluations were conducted on real-world datasets to compare the performance of our model and other state-of-the-art approaches. The results show the superiority of our model over comparison models.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {13},
numpages = {37},
keywords = {multi-task learning, Recommender systems, probabilistic matrix factorization, cross-domain collaborative filtering, weighted irregular tensor factorization}
}

@article{10.1145/2978579,
author = {Chen, Jia and Jin, Qin and Zhao, Shiwan and Bao, Shenghua and Zhang, Li and Su, Zhong and Yu, Yong},
title = {Boosting Recommendation in Unexplored Categories by User Price Preference},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2978579},
doi = {10.1145/2978579},
abstract = {State-of-the-art methods for product recommendation encounter a significant performance drop in categories where a user has no purchase history. This problem needs to be addressed since current online retailers are moving beyond single category and attempting to be diversified. In this article, we investigate the challenging problem of product recommendation in unexplored categories and discover that the price, a factor comparable across categories, can improve the recommendation performance significantly. We introduce the price utility concept to characterize users’ sense of price and propose three different utility functions. We show that user price preference in a category is a distribution and we mine typical user price preference patterns based on three different types of distance between distributions. We fuse user price preference through regularization and joint factorization to boost recommendation performance in both browsing and buying shopping orientations. Experimental results show that fusing user price preference improves performance in a series of recommendation tasks: unexplored category recommendation, product recommendation under a given unexplored category, and product recommendation under generic unexplored categories.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {12},
numpages = {27},
keywords = {Product recommendation, price, unexplored category}
}

@article{10.1145/2978578,
author = {Thomason, Alasdair and Griffiths, Nathan and Sanchez, Victor},
title = {Context Trees: Augmenting Geospatial Trajectories with Context},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2978578},
doi = {10.1145/2978578},
abstract = {Exposing latent knowledge in geospatial trajectories has the potential to provide a better understanding of the movements of individuals and groups. Motivated by such a desire, this work presents the context tree, a new hierarchical data structure that summarises the context behind user actions in a single model. We propose a method for context tree construction that augments geospatial trajectories with land usage data to identify such contexts. Through evaluation of the construction method and analysis of the properties of generated context trees, we demonstrate the foundation for understanding and modelling behaviour afforded. Summarising user contexts into a single data structure gives easy access to information that would otherwise remain latent, providing the basis for better understanding and predicting the actions and behaviours of individuals and groups. Finally, we also present a method for pruning context trees for use in applications where it is desirable to reduce the size of the tree while retaining useful information.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {14},
numpages = {37},
keywords = {trajectories, Clustering, context, land usage, spatiotemporal data}
}

@article{10.1145/2873055,
author = {Yin, Hongzhi and Cui, Bin and Zhou, Xiaofang and Wang, Weiqing and Huang, Zi and Sadiq, Shazia},
title = {Joint Modeling of User Check-in Behaviors for Real-Time Point-of-Interest Recommendation},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2873055},
doi = {10.1145/2873055},
abstract = {Point-of-Interest (POI) recommendation has become an important means to help people discover attractive and interesting places, especially when users travel out of town. However, the extreme sparsity of a user-POI matrix creates a severe challenge. To cope with this challenge, we propose a unified probabilistic generative model, the Topic-Region Model (TRM), to simultaneously discover the semantic, temporal, and spatial patterns of users’ check-in activities, and to model their joint effect on users’ decision making for selection of POIs to visit. To demonstrate the applicability and flexibility of TRM, we investigate how it supports two recommendation scenarios in a unified way, that is, hometown recommendation and out-of-town recommendation. TRM effectively overcomes data sparsity by the complementarity and mutual enhancement of the diverse information associated with users’ check-in activities (e.g., check-in content, time, and location) in the processes of discovering heterogeneous patterns and producing recommendations. To support real-time POI recommendations, we further extend the TRM model to an online learning model, TRM-Online, to track changing user interests and speed up the model training. In addition, based on the learned model, we propose a clustering-based branch and bound algorithm (CBB) to prune the POI search space and facilitate fast retrieval of the top-k recommendations.We conduct extensive experiments to evaluate the performance of our proposals on two real-world datasets, including recommendation effectiveness, overcoming the cold-start problem, recommendation efficiency, and model-training efficiency. The experimental results demonstrate the superiority of our TRM models, especially TRM-Online, compared with state-of-the-art competitive methods, by making more effective and efficient mobile recommendations. In addition, we study the importance of each type of pattern in the two recommendation scenarios, respectively, and find that exploiting temporal patterns is most important for the hometown recommendation scenario, while the semantic patterns play a dominant role in improving the recommendation effectiveness for out-of-town users.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {11},
numpages = {44},
keywords = {efficient retrieval algorithm, location-based service, POI, real-time recommendation, online learning}
}

@article{10.1145/2935747,
author = {Arguello, Jaime and Capra, Rob},
title = {The Effects of Aggregated Search Coherence on Search Behavior},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2935747},
doi = {10.1145/2935747},
abstract = {Aggregated search is the task of combining results from multiple independent search systems in a single Search Engine Results Page (SERP). Aggregated search coherence refers to the extent to which different sources on the SERP focus on similar senses of an ambiguous or underspecified query. In previous studies, we found that the query senses in a set of vertical results can influence user engagement with the web results (the so-called “spillover” effect). In this work, we investigate five research questions (RQ1--RQ5) that extend our prior work. First, we investigate the extent to which results from different sources focus on different senses of an ambiguous query (RQ1). Second, we investigate how the vertical-to-web spillover effect varies across different verticals (RQ2). Then, we examine whether the level of spillover depends on the vertical position (RQ3) and on whether the vertical results are displayed with a border and different-colored background to distinguish them from the web results (RQ4). Finally, we propose a new method for displaying results from a particular vertical that are more consistent with the query senses in the web results (RQ5). We evaluate this new method based on how it influences users to make more correct decisions with respect to the web results—to engage with the web results when at least one of them is relevant and to avoid engaging with the web results otherwise. Our results show the following trends. In terms of RQ1, our analysis suggests that the top results from the web search engine are more diversified than the top results from our four different verticals considered (images, news, shopping, and video). In terms of RQ2, we found a stronger spillover effect for the images vertical than the news, shopping, and video verticals. In terms of RQ3, we found a stronger level of spillover when the vertical was positioned at the top of the SERP versus to the right side of the web results. In terms of RQ4, we found an interesting additive effect between the vertical’s position and displaying the vertical results enclosed in a border and with a different-colored background—the image vertical had no spillover when presented to the right side of the web results and with a border and background. Finally, in terms of RQ5, we found that our proposed vertical results selection approach can influence users to make more correct predictions about their level of engagement with the web results.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {2},
numpages = {30},
keywords = {search behavior, aggregated search evaluation, vertical results selection, aggregated search coherence, Aggregated search}
}

@article{10.1145/2960408,
author = {Wang, Shuaiqiang and Huang, Shanshan and Liu, Tie-Yan and Ma, Jun and Chen, Zhumin and Veijalainen, Jari},
title = {Ranking-Oriented Collaborative Filtering: A Listwise Approach},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2960408},
doi = {10.1145/2960408},
abstract = {Collaborative filtering (CF) is one of the most effective techniques in recommender systems, which can be either rating oriented or ranking oriented. Ranking-oriented CF algorithms demonstrated significant performance gains in terms of ranking accuracy, being able to estimate a precise preference ranking of items for each user rather than the absolute ratings (as rating-oriented CF algorithms do). Conventional memory-based ranking-oriented CF can be referred to as pairwise algorithms. They represent each user as a set of preferences on each pair of items for similarity calculations and predictions. In this study, we propose ListCF, a novel listwise CF paradigm that seeks improvement in both accuracy and efficiency in comparison with pairwise CF. In ListCF, each user is represented as a probability distribution of the permutations over rated items based on the Plackett-Luce model, and the similarity between users is measured based on the Kullback--Leibler divergence between their probability distributions over the set of commonly rated items. Given a target user and the most similar users, ListCF directly predicts a total order of items for each user based on similar users’ probability distributions over permutations of the items. Besides, we also reveal insightful connections among pointwise, pairwise, and listwise CF algorithms from the perspective of the matrix representations. In addition, to make our algorithm more scalable and adaptive, we present an incremental algorithm for ListCF, which allows incrementally updating the similarities between users when certain user submits a new rating or updates an existing rating. Extensive experiments on benchmark datasets in comparison with the state-of-the-art approaches demonstrate the promise of our approach.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {10},
numpages = {28},
keywords = {ranking-oriented collaborative filtering, Collaborative filtering, recommender systems}
}

@article{10.1145/2956235,
author = {Jiang, Di and Tong, Yongxin and Song, Yuanfeng},
title = {Cross-Lingual Topic Discovery From Multilingual Search Engine Query Log},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2956235},
doi = {10.1145/2956235},
abstract = {Today, major commercial search engines are operating in a multinational fashion to provide web search services for millions of users who compose search queries by different languages. Hence, the search engine query log, which serves as the backbone of many search engine applications, records millions of users’ search history in a wide spectrum of human languages and demonstrates a strong multilingual phenomenon. However, with its salience, the multilingual nature of a search engine query log is usually ignored by existing works, which usually consider query log entries of different languages as being orthogonal and independent. This kind of oversimplified assumption heavily distorts the underlying structure of web search data. In this article, we pioneer in recognition of the multilingual nature of a query log and make the first attempt to cross the language barrier in query logs. We propose a novel model named Cross-Lingual Query Log Topic Model (CL-QLTM) to analyze query logs from a cross-lingual perspective and derive the latent topics of web search data. The CL-QLTM comprehensively integrates web search data in different languages by collectively utilizing cross-lingual dictionaries, as well as the co-occurrence relations in the query log. In order to relieve the efficiency bottleneck of applying the CL-QLTM on voluminous query logs, we propose an efficient parameter inference algorithm based on the MapReduce computing paradigm. Both qualitative and quantitative experimental results show that the CL-QLTM is able to effectively derive cross-lingual topics from multilingual query logs and spawn a wide spectrum of new search engine applications.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {9},
numpages = {28},
keywords = {probabilistic topic model, query log, Search engine}
}

@article{10.1145/2948065,
author = {Zhang, Chenyi and Liang, Hongwei and Wang, Ke},
title = {Trip Recommendation Meets Real-World Constraints: POI Availability, Diversity, and Traveling Time Uncertainty},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2948065},
doi = {10.1145/2948065},
abstract = {As location-based social network (LBSN) services become increasingly popular, trip recommendation that recommends a sequence of points of interest (POIs) to visit for a user emerges as one of many important applications of LBSNs. Personalized trip recommendation tailors to users’ specific tastes by learning from past check-in behaviors of users and their peers. Finding the optimal trip that maximizes user’s experiences for a given time budget constraint is an NP-hard problem and previous solutions do not consider three practical and important constraints. One constraint is POI availability, where a POI may be only available during a certain time window. Another constraint is uncertain traveling time, where the traveling time between two POIs is uncertain. In addition, the diversity of the POIs included in the trip plays an important role in user’s final adoptions. This work presents efficient solutions to personalized trip recommendation by incorporating these constraints and leveraging them to prune the search space. We evaluated the efficiency and effectiveness of our solutions on real-life LBSN datasets.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {5},
numpages = {28},
keywords = {recommender systems, Trip plan, location-based social network}
}

@article{10.1145/2950049,
author = {Azmi, Aqil M. and Alshenaifi, Nouf A.},
title = {Answering Arabic Why-Questions: Baseline vs. RST-Based Approach},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2950049},
doi = {10.1145/2950049},
abstract = {A Question Answering (QA) system is concerned with building a system that automatically answer questions posed by humans in a natural language. Compared to other languages, little effort was directed towards QA systems for Arabic. Due to the difficulty of handling why-questions, most Arabic QA systems tend to ignore it. In this article, we specifically address the why-question for Arabic using two different approaches and compare their performance and the quality of their answer. The first is the baseline approach, a generic method that is used to answer all types of questions, including factoid; and for the second approach, we use Rhetorical Structure Theory (RST). We evaluate both schemes using a corpus of 700 textual documents in different genres collected from Open Source Arabic Corpora (OSAC), and a set of 100 question-answer pairs. Overall, the performance measures of recall, precision, and c@1 was 68% (all three measures) for the baseline approach, and 71%, 78%, and 77.4%, respectively, for the RST-based approach. The recently introduced extension of the accuracy, the c@1 measure, rewards unanswered questions over those wrongly answered.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {6},
numpages = {19},
keywords = {why question-answer, baseline, RST, Arabic question-answer}
}

@article{10.1145/2948064,
author = {Zhao, Wayne Xin and Zhou, Ningnan and Zhang, Wenhui and Wen, Ji-Rong and Wang, Shan and Chang, Edward Y.},
title = {A Probabilistic Lifestyle-Based Trajectory Model for Social Strength Inference from Human Trajectory Data},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2948064},
doi = {10.1145/2948064},
abstract = {With the pervasiveness of location-based social networks, it becomes increasingly important to consider the social characteristics of locations shared among persons. Several studies have been proposed to infer social strength by using trajectory similarity. However, these studies have two major shortcomings. First, they rely on the explicit co-occurrence of check-in locations. In this situation, a user pair of two friends who seldom share common locations or a user pair of two strangers who heavily share common visited locations will receive an unreliable estimation of the real social strength between them. Second, these studies do not consider how the overall trajectory patterns of users change with the varying of living styles.In this article, we propose a probabilistic generative model to mine latent lifestyle-related patterns from human trajectory data for inferring social strength. It can automatically learn functionality topics consisting of locations with similar service functions and transition probabilities over the set of functionality topics. Furthermore, a lifestyle is modeled as a unique transition probability matrix over the set of functionality topics. A user has a preference distribution over the set of lifestyles, and he or she is able to select over multiple lifestyles to adapt to different living contexts. The learned lifestyle-related patterns are subsequently used as features in a supervised learner for both strength estimation and link prediction. We conduct extensive experiments to evaluate the performance of the proposed method on two real-world datasets. The experimental results demonstrate the effectiveness of our proposed method.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {8},
numpages = {28},
keywords = {trajectory data, probabilistic generative model, link prediction, Social strength inference}
}

@article{10.1145/2948063,
author = {Molino, Piero and Aiello, Luca Maria and Lops, Pasquale},
title = {Social Question Answering: Textual, User, and Network Features for Best Answer Prediction},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2948063},
doi = {10.1145/2948063},
abstract = {Community question answering (CQA) sites use a collaborative paradigm to satisfy complex information needs. Although the task of matching questions to their best answers has been tackled for more than a decade, the social question-answering practice is a complex process. The factors influencing the accuracy of question-answer matching are many and hard to disentangle. We approach the task from an application-oriented perspective, probing the space of several dimensions relevant to this problem: features, algorithms, and topics. We gather under a learning to rank framework the most extensive feature set used in literature to date, including 225 features from five different families. We test the power of such features in predicting the best answer to a question on the largest dataset from Yahoo Answers used for this task so far (40M answers) and provide a faceted analysis of the results along different topical areas and question types. We propose a novel family of distributional semantics measures that most of the time can seamlessly replace widely used linguistic similarity features, being more than one order of magnitude faster to compute and providing greater predictive power. The best feature set reaches an improvement between 11% and 26% in P@1 compared to recent well-established state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {4},
numpages = {40},
keywords = {expertise networks, distributional semantics, expert finding, Yahoo Answers, best answer prediction, Community question answering}
}

@article{10.1145/2934671,
author = {Morsy, Sara and Karypis, George},
title = {Accounting for Language Changes Over Time in Document Similarity Search},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2934671},
doi = {10.1145/2934671},
abstract = {Given a query document, ranking the documents in a collection based on how similar they are to the query is an essential task with extensive applications. For collections that contain documents whose creation dates span several decades, this task is further complicated by the fact that the language changes over time. For example, many terms add or lose one or more senses to meet people’s evolving needs. To address this problem, we present methods that take advantage of two types of information to account for the language change. The first is the citation network that often exists within the collection, which can be used to link related documents with significantly different creation dates (and hence different language use). The second is the changes in the usage frequency of terms that occur over time, which can indicate changes in their senses and uses. These methods utilize the preceding information while estimating the representation of both documents and terms within the context of nonprobabilistic static and dynamic topic models. Our experiments on two real-world datasets that span more than 40 years show that our proposed methods improve the retrieval performance of existing models and that these improvements are statistically significant.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {1},
numpages = {26},
keywords = {terms usage frequency changes, language change, regularization, similarity search, longitudinal document collections, Citation network}
}

@article{10.1145/2956234,
author = {Miao, Jun and Huang, Jimmy Xiangji and Zhao, Jiashu},
title = {TopPRF: A Probabilistic Framework for Integrating Topic Space into Pseudo Relevance Feedback},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2956234},
doi = {10.1145/2956234},
abstract = {Traditional pseudo relevance feedback (PRF) models choose top k feedback documents for query expansion and treat those documents equally. When k is determined, feedback terms are selected without considering the reliability of these documents for relevance. Because the performance of PRF is sensitive to the selection of feedback terms, noisy terms imported from these irrelevant documents or partially relevant documents will harm the final results extensively. Intuitively, terms in these documents should be considered less important for feedback term selection. Nonetheless, how to measure the reliability of feedback documents is a difficult problem.Recently, topic modeling has become more and more popular in the information retrieval (IR) area. In order to identify how reliable a feedback document is to be relevant, we attempt to adapt the topical information into PRF. However, topics are hard to be quantified and therefore the identification of topic is usually fuzzy. It is very challenging for integrating the obtained topical information effectively into IR and other text-processing-related areas. Current research work mainly focuses on mining relevant information from particular topics. This is extremely difficult when the boundaries of different topics are hard to define. In this article, we investigate a key factor of this problem, the topic number for topic modeling and how it makes topics “fuzzy.” To effectively and efficiently apply topical information, we propose a new probabilistic framework, “TopPRF,” and three models, TS-COS, TS-EU, and TS-Entropy, via integrating “Topic Space” (TS) information into pseudo relevance feedback. These methods discover how reliable a document is to be relevant through both term and topical information. When selecting feedback terms, candidate terms in more reliable feedback documents should obtain extra weights. Experimental results on various public collections justify that our proposed methods can significantly reduce the influence of “fuzzy topics” and obtain stable, good results over the strong baseline models. Our proposed probabilistic framework, TopPRF, and three topic-space-based models are capable of searching documents beyond traditional term matching only and provide a promising avenue for constructing better topic-space-based IR systems. Moreover, in-depth discussions and conclusions are made to help other researchers apply topical information effectively.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {22},
numpages = {36},
keywords = {Pseudo relevance feedback, topic modeling, text mining}
}

@article{10.1145/2866571,
author = {Ibrahim, Muhammad and Carman, Mark},
title = {Comparing Pointwise and Listwise Objective Functions for Random-Forest-Based Learning-to-Rank},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2866571},
doi = {10.1145/2866571},
abstract = {Current random-forest (RF)-based learning-to-rank (LtR) algorithms use a classification or regression framework to solve the ranking problem in a pointwise manner. The success of this simple yet effective approach coupled with the inherent parallelizability of the learning algorithm makes it a strong candidate for widespread adoption. In this article, we aim to better understand the effectiveness of RF-based rank-learning algorithms with a focus on the comparison between pointwise and listwise approaches.We introduce what we believe to be the first listwise version of an RF-based LtR algorithm. The algorithm directly optimizes an information retrieval metric of choice (in our case, NDCG) in a greedy manner. Direct optimization of the listwise objective functions is computationally prohibitive for most learning algorithms, but possible in RF since each tree maximizes the objective in a coordinate-wise fashion. Computational complexity of the listwise approach is higher than the pointwise counterpart; hence for larger datasets, we design a hybrid algorithm that combines a listwise objective in the early stages of tree construction and a pointwise objective in the latter stages. We also study the effect of the discount function of NDCG on the listwise algorithm.Experimental results on several publicly available LtR datasets reveal that the listwise/hybrid algorithm outperforms the pointwise approach on the majority (but not all) of the datasets. We then investigate several aspects of the two algorithms to better understand the inevitable performance tradeoffs. The aspects include examining an RF-based unsupervised LtR algorithm and comparing individual tree strength. Finally, we compare the the investigated RF-based algorithms with several other LtR algorithms.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {20},
numpages = {38},
keywords = {objective function, computational complexity, random forest, splitting criterion, evaluation metrics, Learning-to-rank}
}

@article{10.1145/2950050,
author = {Baly, Ramy and Hobeica, Roula and Hajj, Hazem and El-Hajj, Wassim and Shaban, Khaled Bashir and Al-Sallab, Ahmad},
title = {A Meta-Framework for Modeling the Human Reading Process in Sentiment Analysis},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2950050},
doi = {10.1145/2950050},
abstract = {This article introduces a sentiment analysis approach that adopts the way humans read, interpret, and extract sentiment from text. Our motivation builds on the assumption that human interpretation should lead to the most accurate assessment of sentiment in text. We call this automated process Human Reading for Sentiment (HRS). Previous research in sentiment analysis has produced many frameworks that can fit one or more of the HRS aspects; however, none of these methods has addressed them all in one approach. HRS provides a meta-framework for developing new sentiment analysis methods or improving existing ones. The proposed framework provides a theoretical lens for zooming in and evaluating aspects of any sentiment analysis method to identify gaps for improvements towards matching the human reading process. Key steps in HRS include the automation of humans low-level and high-level cognitive text processing. This methodology paves the way towards the integration of psychology with computational linguistics and machine learning to employ models of pragmatics and discourse analysis for sentiment analysis. HRS is tested with two state-of-the-art methods; one is based on feature engineering, and the other is based on deep learning. HRS highlighted the gaps in both methods and showed improvements for both.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {7},
numpages = {21},
keywords = {supervised learning and notions, human reading, psychology, Sentiment analysis}
}

@article{10.1145/2937752,
author = {Zhang, Yating and Jatowt, Adam and Tanaka, Katsumi},
title = {Causal Relationship Detection in Archival Collections of Product Reviews for Understanding Technology Evolution},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2937752},
doi = {10.1145/2937752},
abstract = {Technology progress is one of the key reasons behind today's rapid changes in lifestyles. Knowing how products and objects evolve can not only help with understanding the evolutionary patterns in our society but can also provide clues on effective product design and can offer support for predicting the future. We propose a general framework for analyzing technology's impact on our lives through detecting cause--effect relationships, where causes represent changes in technology while effects are changes in social life, such as new activities or new ways of using products. We address the challenge of viewing technology evolution through the “social impact lens” by mining causal relationships from the long-term collections of product reviews. In particular, we first propose dividing vocabulary into two groups: terms describing product features (called physical terms) and terms representing product usage (called conceptual terms). We then search for two kinds of changes related to the appearance of terms: frequency-based and context-based changes. The former indicate periods when a word was significantly more frequently used, whereas the latter indicate periods of high change in the word's context. Based on the detected changes, we then search for causal term pairs such that the change in the physical term triggers the change in the conceptual term. We next extend our approach to finding causal relationships between word groups such as a group of words representing the same technology and causing a given conceptual change or group of words representing two different technologies that simultaneously “co-cause” a conceptual change. We conduct experiments on different product types using the Amazon Product Review Dataset, which spans 1995 to 2013, and we demonstrate that our approaches outperform state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {3},
numpages = {41},
keywords = {product evolution analysis, social influence, Technology evolution analysis, causality detection}
}

@article{10.1145/2926790,
author = {Shtok, Anna and Kurland, Oren and Carmel, David},
title = {Query Performance Prediction Using Reference Lists},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2926790},
doi = {10.1145/2926790},
abstract = {The task of query performance prediction is to estimate the effectiveness of search performed in response to a query when no relevance judgments are available. We present a novel probabilistic analysis of the performance prediction task. The analysis gives rise to a general prediction framework that uses pseudo-effective or ineffective document lists that are retrieved in response to the query. These lists serve as reference to the result list at hand, the effectiveness of which we want to predict. We show that many previously proposed prediction methods can be explained using our framework. More generally, we shed new light on existing prediction methods and establish formal common grounds to seemingly different prediction approaches. In addition, we formally demonstrate the connection between prediction using reference lists and fusion of retrieved lists, and provide empirical support to this connection. Through an extensive empirical exploration, we study various factors that affect the quality of prediction using reference lists.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {19},
numpages = {34},
keywords = {reference lists, Query performance prediction}
}

@article{10.1145/2910579,
author = {Cai, Fei and Reinanda, Ridho and Rijke, Maarten De},
title = {Diversifying Query Auto-Completion},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2910579},
doi = {10.1145/2910579},
abstract = {Query auto-completion assists web search users in formulating queries with a few keystrokes, helping them to avoid spelling mistakes and to produce clear query expressions, and so on. Previous work on query auto-completion mainly centers around returning a list of completions to users, aiming to push queries that are most likely intended by the user to the top positions but ignoring the redundancy among the query candidates in the list. Thus, semantically related queries matching the input prefix are often returned together. This may push valuable suggestions out of the list, given that only a limited number of candidates can be shown to the user, which may result in a less than optimal search experience.In this article, we consider the task of diversifying query auto-completion, which aims to return the correct query completions early in a ranked list of candidate completions and at the same time reduce the redundancy among query auto-completion candidates. We develop a greedy query selection approach that predicts query completions based on the current search popularity of candidate completions and on the aspects of previous queries in the same search session. The popularity of completion candidates at query time can be directly aggregated from query logs. However, query aspects are implicitly expressed by previous clicked documents in the search context. To determine the query aspect, we categorize clicked documents of a query using a hierarchy based on the open directory project. Bayesian probabilistic matrix factorization is applied to derive the distribution of queries over all aspects. We quantify the improvement of our greedy query selection model against a state-of-the-art baseline using two large-scale, real-world query logs and show that it beats the baseline in terms of well-known metrics used in query auto-completion and diversification. In addition, we conduct a side-by-side experiment to verify the effectiveness of our proposal.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {25},
numpages = {33},
keywords = {diversification, Query auto-completion, query suggestion, web search}
}

@article{10.1145/2903719,
author = {Luo, Xiangfeng and Xuan, Junyu and Lu, Jie and Zhang, Guangquan},
title = {Measuring the Semantic Uncertainty of News Events for Evolution Potential Estimation},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2903719},
doi = {10.1145/2903719},
abstract = {The evolution potential estimation of news events can support the decision making of both corporations and governments. For example, a corporation could manage its public relations crisis in a timely manner if a negative news event about this corporation is known with large evolution potential in advance. However, existing state-of-the-art methods are mainly based on time series historical data, which are not suitable for the news events with limited historical data and bursty properties. In this article, we propose a purely content-based method to estimate the evolution potential of the news events. The proposed method considers a news event at a given time point as a system composed of different keywords, and the uncertainty of this system is defined and measured as the Semantic Uncertainty of this news event. At the same time, an uncertainty space is constructed with two extreme states: the most uncertain state and the most certain state. We believe that the Semantic Uncertainty has correlation with the content evolution of the news events, so it can be used to estimate the evolution potential of the news events. In order to verify the proposed method, we present detailed experimental setups and results measuring the correlation of the Semantic Uncertainty with the Content Change of news events using collected news events data. The results show that the correlation does exist and is stronger than the correlation of value from the time-series-based method with the Content Change. Therefore, we can use the Semantic Uncertainty to estimate the evolution potential of news events.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {24},
numpages = {25},
keywords = {natural language processing, Information search and retrieval, semantic analysis, news event, text mining}
}

@article{10.1145/2882782,
author = {Kharazmi, Sadegh and Scholer, Falk and Vallet, David and Sanderson, Mark},
title = {Examining Additivity and Weak Baselines},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2882782},
doi = {10.1145/2882782},
abstract = {We present a study of which baseline to use when testing a new retrieval technique. In contrast to past work, we show that measuring a statistically significant improvement over a weak baseline is not a good predictor of whether a similar improvement will be measured on a strong baseline. Sometimes strong baselines are made worse when a new technique is applied. We investigate whether conducting comparisons against a range of weaker baselines can increase confidence that an observed effect will also show improvements on a stronger baseline. Our results indicate that this is not the case -- at best, testing against a range of baselines means that an experimenter can be more confident that the new technique is unlikely to significantly harm a strong baseline. Examining recent past work, we present evidence that the information retrieval (IR) community continues to test against weak baselines. This is unfortunate as, in light of our experiments, we conclude that the only way to be confident that a new technique is a contribution is to compare it against nothing less than the state of the art.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {23},
numpages = {18},
keywords = {Baselines, information retrieval, evaluation}
}

@article{10.1145/2854147,
author = {Do, Loc and Lauw, Hady W.},
title = {Probabilistic Models for Contextual Agreement in Preferences},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2854147},
doi = {10.1145/2854147},
abstract = {The long-tail theory for consumer demand implies the need for more accurate personalization technologies to target items to the users who most desire them. A key tenet of personalization is the capacity to model user preferences. Most of the previous work on recommendation and personalization has focused primarily on individual preferences. While some focus on shared preferences between pairs of users, they assume that the same similarity value applies to all items. Here we investigate the notion of “context,” hypothesizing that while two users may agree on their preferences on some items, they may also disagree on other items. To model this, we design probabilistic models for the generation of rating differences between pairs of users across different items. Since this model also involves the estimation of rating differences on unseen items for the purpose of prediction, we further conduct a systematic analysis of matrix factorization and tensor factorization methods in this estimation, and propose a factorization model with a novel objective function of minimizing error in rating differences. Experiments on several real-life rating datasets show that our proposed model consistently yields context-specific similarity values that perform better on a prediction task than models relying on shared preferences.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {21},
numpages = {33},
keywords = {User preference, generative model, contextual agreement}
}

@article{10.1145/2858791,
author = {Hamdi, Sana and Gancarski, Alda Lopes and Bouzeghoub, Amel and Yahia, Sadok Ben},
title = {TISoN: Trust Inference in Trust-Oriented Social Networks},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2858791},
doi = {10.1145/2858791},
abstract = {Trust systems represent a significant trend in decision support for social networks’ service provision. The basic idea is to allow users to rate each other even without being direct neighbours. In this case, the purpose is to derive a trust score for a given user, which could be of help to decide whether to trust other users or not. In this article, we investigate the properties of trust propagation within social networks, based on the notion of transitivity, and we introduce the TISoN model to generate and evaluate Trust Inference within online Social Networks. To do so, (i) we develop a novel TPS algorithm for Trust Path Searching where we define neighbours’ priority based on their direct trust degrees, and then select trusted paths while controlling the path length; and, (ii) we develop different TIM algorithms for Trust Inference Measuring and build a trust network. In addition, we analyse existing algorithms and we demonstrate that our proposed model better computes transitive trust values than do the existing models. We conduct extensive experiments on a real online social network dataset, Advogato. Experimental results show that our work is scalable and generates better results than do the pioneering approaches of the literature.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {17},
numpages = {32},
keywords = {Indirect trust, transitivity, trust paths}
}

@article{10.1145/2893478,
author = {Webb, Helena and Burnap, Pete and Procter, Rob and Rana, Omer and Stahl, Bernd Carsten and Williams, Matthew and Housley, William and Edwards, Adam and Jirotka, Marina},
title = {Digital Wildfires: Propagation, Verification, Regulation, and Responsible Innovation},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2893478},
doi = {10.1145/2893478},
abstract = {Social media platforms provide an increasingly popular means for individuals to share content online. Whilst this produces undoubted societal benefits, the ability for content to be spontaneously posted and reposted creates an ideal environment for rumour and false/malicious information to spread rapidly. When this occurs it can cause significant harm and can be characterised as a “digital wildfire.” In this article, we demonstrate that the propagation and regulation of digital wildfires form important topics for research and conduct an overview of existing work in this area. We outline the relevance of a range of work from the computational and social sciences, including a series of insights into the propagation of rumour and false/malicious information. We argue that significant research gaps remain—for instance, there is an absence of systematic studies on the effects of digital wildfires and there is a need to combine empirical research with a consideration of how the responsible governance of social media can be determined. We propose an agenda for research that establishes a methodology to explore in full the propagation and regulation of unverified content on social media. This agenda promotes high-quality interdisciplinary research that will also inform policy debates.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {15},
numpages = {23},
keywords = {Social media, rumour}
}

@article{10.1145/2746230,
author = {Tang, Jie and Lou, Tiancheng and Kleinberg, Jon and Wu, Sen},
title = {Transfer Learning to Infer Social Ties across Heterogeneous Networks},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2746230},
doi = {10.1145/2746230},
abstract = {Interpersonal ties are responsible for the structure of social networks and the transmission of information through these networks. Different types of social ties have essentially different influences on people. Awareness of the types of social ties can benefit many applications, such as recommendation and community detection. For example, our close friends tend to move in the same circles that we do, while our classmates may be distributed into different communities. Though a bulk of research has focused on inferring particular types of relationships in a specific social network, few publications systematically study the generalization of the problem of predicting social ties across multiple heterogeneous networks.In this work, we develop a framework referred to as TranFG for classifying the type of social relationships by learning across heterogeneous networks. The framework incorporates social theories into a factor graph model, which effectively improves the accuracy of predicting the types of social relationships in a target network by borrowing knowledge from a different source network. We also present several active learning strategies to further enhance the inferring performance. To scale up the model to handle really large networks, we design a distributed learning algorithm for the proposed model.We evaluate the proposed framework (TranFG) on six different networks and compare with several existing methods. TranFG clearly outperforms the existing methods on multiple metrics. For example, by leveraging information from a coauthor network with labeled advisor-advisee relationships, TranFG is able to obtain an F1-score of 90% (8%--28% improvements over alternative methods) for predicting manager-subordinate relationships in an enterprise email network. The proposed model is efficient. It takes only a few minutes to train the proposed transfer model on large networks containing tens of thousands of nodes.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {7},
numpages = {43},
keywords = {social influence, Social ties, social network, predictive model}
}

@article{10.1145/2885494,
author = {Zhang, Huiling and Alim, Md Abdul and Li, Xiang and Thai, My T. and Nguyen, Hien T.},
title = {Misinformation in Online Social Networks: Detect Them All with a Limited Budget},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2885494},
doi = {10.1145/2885494},
abstract = {Online social networks have become an effective and important social platform for communication, opinions exchange, and information sharing. However, they also make it possible for rapid and wide misinformation diffusion, which may lead to pernicious influences on individuals or society. Hence, it is extremely important and necessary to detect the misinformation propagation by placing monitors.In this article, we first define a general misinformation-detection problem for the case where the knowledge about misinformation sources is lacking, and show its equivalence to the influence-maximization problem in the reverse graph. Furthermore, considering node vulnerability, we aim to detect the misinformation reaching to a specific user. Therefore, we study a τ-Monitor Placement problem for cases where partial knowledge of misinformation sources is available and prove its #P complexity. We formulate a corresponding integer program, tackle exponential constraints, and propose a Minimum Monitor Set Construction (MMSC) algorithm, in which the cut-set2 has been exploited in the estimation of reachability of node pairs. Moreover, we generalize the problem from a single target to multiple central nodes and propose another algorithm based on a Monte Carlo sampling technique. Extensive experiments on real-world networks show the effectiveness of proposed algorithms with respect to minimizing the number of monitors.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {18},
numpages = {24},
keywords = {monitor placement, Misinformation detection, online social networks}
}

@article{10.1145/2870630,
author = {Papadopoulos, Symeon and Bontcheva, Kalina and Jaho, Eva and Lupu, Mihai and Castillo, Carlos},
title = {Overview of the Special Issue on Trust and Veracity of Information in Social Media},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2870630},
doi = {10.1145/2870630},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {14},
numpages = {5},
keywords = {fake content, social media, Information veracity, rumour propagation, news verification}
}

@article{10.1145/2842604,
author = {Middleton, Stuart E. and Krivcovs, Vadims},
title = {Geoparsing and Geosemantics for Social Media: Spatiotemporal Grounding of Content Propagating Rumors to Support Trust and Veracity Analysis during Breaking News},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2842604},
doi = {10.1145/2842604},
abstract = {In recent years, there has been a growing trend to use publicly available social media sources within the field of journalism. Breaking news has tight reporting deadlines, measured in minutes not days, but content must still be checked and rumors verified. As such, journalists are looking at automated content analysis to prefilter large volumes of social media content prior to manual verification. This article describes a real-time social media analytics framework for journalists. We extend our previously published geoparsing approach to improve its scalability and efficiency. We develop and evaluate a novel approach to geosemantic feature extraction, classifying evidence in terms of situatedness, timeliness, confirmation, and validity. Our approach works for new unseen news topics. We report results from four experiments using five Twitter datasets crawled during different English-language news events. One of our datasets is the standard TREC 2012 microblog corpus. Our classification results are promising, with F1 scores varying by class from 0.64 to 0.92 for unseen event types. We lastly report results from two case studies during real-world news stories, showcasing different ways our system can assist journalists filter and cross-check content as they examine the trust and veracity of content and sources.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {16},
numpages = {26},
keywords = {rumors, news, social media, credibility, Geosemantics, journalism, veracity, geoparsing, trust, breaking news}
}

@article{10.1145/2846092,
author = {Cheng, Zhiyong and Shen, Jialie},
title = {On Effective Location-Aware Music Recommendation},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2846092},
doi = {10.1145/2846092},
abstract = {Rapid advances in mobile devices and cloud-based music service now allow consumers to enjoy music anytime and anywhere. Consequently, there has been an increasing demand in studying intelligent techniques to facilitate context-aware music recommendation. However, one important context that is generally overlooked is user’s venue, which often includes surrounding atmosphere, correlates with activities, and greatly influences the user’s music preferences. In this article, we present a novel venue-aware music recommender system called VenueMusic to effectively identify suitable songs for various types of popular venues in our daily lives. Toward this goal, a Location-aware Topic Model (LTM) is proposed to (i) mine the common features of songs that are suitable for a venue type in a latent semantic space and (ii) represent songs and venue types in the shared latent space, in which songs and venue types can be directly matched. It is worth mentioning that to discover meaningful latent topics with the LTM, a Music Concept Sequence Generation (MCSG) scheme is designed to extract effective semantic representations for songs. An extensive experimental study based on two large music test collections demonstrates the effectiveness of the proposed topic model and MCSG scheme. The comparisons with state-of-the-art music recommender systems demonstrate the superior performance of VenueMusic system on recommendation accuracy by associating venue and music contents using a latent semantic space. This work is a pioneering study on the development of a venue-aware music recommender system. The results show the importance of considering the influence of venue types in the development of context-aware music recommender systems.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {13},
numpages = {32},
keywords = {music concept, music recommendation, topic model, Venue-aware}
}

@article{10.1145/2832907,
author = {Song, Xuemeng and Ming, Zhao-Yan and Nie, Liqiang and Zhao, Yi-Liang and Chua, Tat-Seng},
title = {Volunteerism Tendency Prediction via Harvesting Multiple Social Networks},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2832907},
doi = {10.1145/2832907},
abstract = {Volunteers have always been extremely crucial and in urgent need for nonprofit organizations (NPOs) to sustain their continuing operations. However, it is expensive and time-consuming to recruit volunteers using traditional approaches. In the Web 2.0 era, abundant and ubiquitous social media data opens a door to the possibility of automatic volunteer identification. In this article, we aim to fully explore this possibility by proposing a scheme that is able to predict users’ volunteerism tendency from user-generated contents collected from multiple social networks based on a conceptual volunteering decision model. We conducted comprehensive experiments to investigate the effectiveness of our proposed scheme and further discussed its generalizibility and extendability. This novel interdisciplinary research will potentially inspire more promising and important human-centered applications.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {10},
numpages = {27},
keywords = {network-centric, user classification, multiple sources, user-centric, Volunteerism tendency prediction}
}

@article{10.1145/2816815,
author = {Petersen, Casper and Simonsen, Jakob Grue and Lioma, Christina},
title = {Power Law Distributions in Information Retrieval},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2816815},
doi = {10.1145/2816815},
abstract = {Several properties of information retrieval (IR) data, such as query frequency or document length, are widely considered to be approximately distributed as a power law. This common assumption aims to focus on specific characteristics of the empirical probability distribution of such data (e.g., its scale-free nature or its long/fat tail). This assumption, however, may not be always true. Motivated by recent work in the statistical treatment of power law claims, we investigate two research questions: (i) To what extent do power law approximations hold for term frequency, document length, query frequency, query length, citation frequency, and syntactic unigram frequency? And (ii) what is the computational cost of replacing ad hoc power law approximations with more accurate distribution fitting? We study 23 TREC and 5 non-TREC datasets and compare the fit of power laws to 15 other standard probability distributions. We find that query frequency and 5 out of 24 term frequency distributions are best approximated by a power law. All remaining properties are better approximated by the Inverse Gaussian, Generalized Extreme Value, Negative Binomial, or Yule distribution. We also find the overhead of replacing power law approximations by more informed distribution fitting to be negligible, with potential gains to IR tasks like index compression or test collection generation for IR evaluation.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {8},
numpages = {37},
keywords = {power laws, Statistical model selection}
}

@article{10.1145/2858657,
author = {Piao, Minghao and Ryu, Keun Ho},
title = {Subspace Frequency Analysis--Based Field Indices Extraction for Electricity Customer Classification},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2858657},
doi = {10.1145/2858657},
abstract = {In electricity customer classification, the most important task is to avoid the curse of dimensionality problem, as the consumption diagrams have a large number of dimensions. To avoid the curse of dimensionality problem, field indices (load shape factor) are often used instead of consumption diagrams. Field indices are directly extracted from consumption diagrams according to a predefined formula. Previous studies show that the most important thing for defining such a formula is to find meaningful time intervals from consumption diagrams. However, the inconvenient thing is that there are still a lack of details to explain how to define such time intervals.In our study, we propose a data mining--based method named SFATIE to support the extraction of field indices. The performance of the proposed method is evaluated by comparing it with other dimensionality reduction methods during the classification. For the classification, most often we have used classification methods like C5.0, SVM, Neural Net, Bayes Net, and Logistic. The experimental results show that our method is better or close to other dimensionality reduction methods. In addition, the experimental results show that our proposed method can produce the good quality of field indices and that these indices can improve the performance of electricity customer classification.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {12},
numpages = {18},
keywords = {classification, subspace analysis, dimensionality reduction, Feature extraction}
}

@article{10.1145/2838731,
author = {Li, Qing and Chen, Yuanzhu and Jiang, Li Ling and Li, Ping and Chen, Hsinchun},
title = {A Tensor-Based Information Framework for Predicting the Stock Market},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2838731},
doi = {10.1145/2838731},
abstract = {To study the influence of information on the behavior of stock markets, a common strategy in previous studies has been to concatenate the features of various information sources into one compound feature vector, a procedure that makes it more difficult to distinguish the effects of different information sources. We maintain that capturing the intrinsic relations among multiple information sources is important for predicting stock trends. The challenge lies in modeling the complex space of various sources and types of information and studying the effects of this information on stock market behavior. For this purpose, we introduce a tensor-based information framework to predict stock movements. Specifically, our framework models the complex investor information environment with tensors. A global dimensionality-reduction algorithm is used to capture the links among various information sources in a tensor, and a sequence of tensors is used to represent information gathered over time. Finally, a tensor-based predictive model to forecast stock movements, which is in essence a high-order tensor regression learning problem, is presented. Experiments performed on an entire year of data for China Securities Index stocks demonstrate that a trading system based on our framework outperforms the classic Top-N trading strategy and two state-of-the-art media-aware trading algorithms.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {11},
numpages = {30},
keywords = {stock, news, social media, trading strategy, Tensor, predictive model}
}

@article{10.1145/2824253,
author = {Gomez-Rodriguez, Manuel and Song, Le and Du, Nan and Zha, Hongyuan and Sch\"{o}lkopf, Bernhard},
title = {Influence Estimation and Maximization in Continuous-Time Diffusion Networks},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2824253},
doi = {10.1145/2824253},
abstract = {If a piece of information is released from a set of media sites, can it spread, in 1 month, to a million web pages? Can we efficiently find a small set of media sites among millions that can maximize the spread of the information, in 1 month? The two problems are called influence estimation and maximization problems respectively, which are very challenging since both the time-sensitive nature of the problems and the issue of scalability need to be addressed simultaneously. In this article, we propose two algorithms for influence estimation in continuous-time diffusion networks. The first one uses continuous-time Markov chains to estimate influence exactly on networks with exponential, or, more generally, phase-type transmission functions, but does not scale to large-scale networks, and the second one is a highly efficient randomized algorithm, which estimates the influence of every node in a network with general transmission functions, |ν| nodes and |ε| edges to an accuracy of ϵ using n = O(1/ϵ2) randomizations and up to logarithmic factors O(n|ε|+n|ν| computations. We then show that finding the set of most influential source nodes in a continuous time diffusion network is an NP-hard problem and develop an efficient greedy algorithm with provable near-optimal performance. When used as subroutines in the influence maximization algorithm, the exact influence estimation algorithm is guaranteed to find a set of C nodes with an influence of at least (1 − 1/e)OPT and the randomized algorithm is guaranteed to find a set with an influence of at least 1 − 1/e)OPT − 2Cε, where OPT is the optimal value. Experiments on both synthetic and real-world data show that the proposed algorithms significantly improve over previous state-of-the-art methods in terms of the accuracy of the estimated influence and the quality of the selected nodes to maximize the influence, and the randomized algorithm can easily scale up to networks of millions of nodes.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {9},
numpages = {33},
keywords = {influence estimation, social networks, Networks of diffusion, influence maximization}
}

@article{10.1145/2809787,
author = {Costa, Alberto and Buccio, Emanuele Di and Melucci, Massimo},
title = {A Document Retrieval Model Based on Digital Signal Filtering},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2809787},
doi = {10.1145/2809787},
abstract = {Information retrieval (IR) systems are designed, in general, to satisfy the information need of a user who expresses it by means of a query, by providing him with a subset of documents selected from a collection and ordered by decreasing relevance to the query. Such systems are based on IR models, which define how to represent the documents and the query, as well as how to determine the relevance of a document for a query. In this article, we present a new IR model based on concepts taken from both IR and digital signal processing (like Fourier analysis of signals and filtering). This allows the whole IR process to be seen as a physical phenomenon, where the query corresponds to a signal, the documents correspond to filters, and the determination of the relevant documents to the query is done by filtering that signal. Tests showed that the quality of the results provided by this IR model is comparable with the state-of-the-art.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {6},
numpages = {37},
keywords = {digital filtering, DFT, Retrieval models, discrete Fourier transform}
}

@article{10.1145/2809786,
author = {Baralis, Elena and Cagliero, Luca and Fiori, Alessandro and Garza, Paolo},
title = {MWI-Sum: A Multilingual Summarizer Based on Frequent Weighted Itemsets},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2809786},
doi = {10.1145/2809786},
abstract = {Multidocument summarization addresses the selection of a compact subset of highly informative sentences, i.e., the summary, from a collection of textual documents. To perform sentence selection, two parallel strategies have been proposed: (a) apply general-purpose techniques relying on data mining or information retrieval techniques, and/or (b) perform advanced linguistic analysis relying on semantics-based models (e.g., ontologies) to capture the actual sentence meaning. Since there is an increasing need for processing documents written in different languages, the attention of the research community has recently focused on summarizers based on strategy (a).This article presents a novel multilingual summarizer, namely MWI-Sum (Multilingual Weighted Itemset-based Summarizer), that exploits an itemset-based model to summarize collections of documents ranging over the same topic. Unlike previous approaches, it extracts frequent weighted itemsets tailored to the analyzed collection and uses them to drive the sentence selection process. Weighted itemsets represent correlations among multiple highly relevant terms that are neglected by previous approaches. The proposed approach makes minimal use of language-dependent analyses. Thus, it is easily applicable to document collections written in different languages.Experiments performed on benchmark and real-life collections, English-written and not, demonstrate that the proposed approach performs better than state-of-the-art multilingual document summarizers.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {5},
numpages = {35},
keywords = {text mining, frequent weighted itemset mining, Multilingual summarization}
}

@article{10.1145/2797137,
author = {Cui, Qing and Gao, Bin and Bian, Jiang and Qiu, Siyu and Dai, Hanjun and Liu, Tie-Yan},
title = {KNET: A General Framework for Learning Word Embedding Using Morphological Knowledge},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2797137},
doi = {10.1145/2797137},
abstract = {Neural network techniques are widely applied to obtain high-quality distributed representations of words (i.e., word embeddings) to address text mining, information retrieval, and natural language processing tasks. Most recent efforts have proposed several efficient methods to learn word embeddings from context such that they can encode both semantic and syntactic relationships between words. However, it is quite challenging to handle unseen or rare words with insufficient context. Inspired by the study on the word recognition process in cognitive psychology, in this article, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both words’ contextual information and morphological knowledge to learn word embeddings. Meanwhile, this new learning architecture is also able to benefit from noisy knowledge and balance between contextual information and morphological knowledge. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {4},
numpages = {25},
keywords = {Neural network, morphological knowledge, word embedding}
}

@article{10.1145/2749459,
author = {Quan, Xiaojun and Wang, Qifan and Zhang, Ying and Si, Luo and Wenyin, Liu},
title = {Latent Discriminative Models for Social Emotion Detection with Emotional Dependency},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2749459},
doi = {10.1145/2749459},
abstract = {Sentiment analysis of such opinionated online texts as reviews and comments has received increasingly close attention, yet most of the work is intended to deal with the detection of authors’ emotion. In contrast, this article presents our study of the social emotion detection problem, the objective of which is to identify the evoked emotions of readers by online documents such as news articles. A novel Latent Discriminative Model (LDM) is proposed for this task. LDM works by introducing intermediate hidden variables to model the latent structure of input text corpora. To achieve this, it defines a joint distribution over emotions and latent variables, conditioned on the observed text documents. Moreover, we assume that social emotions are not independent but correlated with one another, and the dependency of them is capable of providing additional guidance to LDM in the training process. The inclusion of this emotional dependency into LDM gives rise to a new Emotional Dependency-based LDM (eLDM). We evaluate the proposed models through a series of empirical evaluations on two real-world corpora of news articles. Experimental results verify the effectiveness of LDM and eLDM in social emotion detection.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {2},
numpages = {19},
keywords = {Discriminative model, opinion mining and sentiment analysis, social emotion detection}
}

@article{10.1145/2766447,
author = {Yan, Su and Wan, Xiaojun},
title = {Deep Dependency Substructure-Based Learning for Multidocument Summarization},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2766447},
doi = {10.1145/2766447},
abstract = {Most extractive style topic-focused multidocument summarization systems generate a summary by ranking textual units in multiple documents and extracting a proper subset of sentences biased to the given topic. Usually, the textual units are simply represented as sentences or n-grams, which do not carry deep syntactic and semantic information. This article presents a novel extractive topic-focused multidocument summarization framework. The framework proposes a new kind of more meaningful and informative units named frequent Deep Dependency Sub-Structure (DDSS) and a topic-sensitive Multi-Task Learning (MTL) model for frequent DDSS ranking. Given a document set, first, we parse all the sentences into deep dependency structures with a Head-driven Phrase Structure Grammar (HPSG) parser and mine the frequent DDSSs after semantic normalization. Then we employ a topic-sensitive MTL model to learn the importance of these frequent DDSSs. Finally, we exploit an Integer Linear Programming (ILP) formulation and use the frequent DDSSs as the essentials for summary extraction. Experimental results on two DUC datasets demonstrate that our proposed approach can achieve state-of-the-art performance. Both the DDSS information and the topic-sensitive MTL model are validated to be very helpful for topic-focused multidocument summarization.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {3},
numpages = {24},
keywords = {Document summarization, multi-task learning, deep dependency sub-structure}
}

@article{10.1145/2751557,
author = {Mayer, Julia M. and Jones, Quentin and Hiltz, Starr Roxanne},
title = {Identifying Opportunities for Valuable Encounters: Toward Context-Aware Social Matching Systems},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2751557},
doi = {10.1145/2751557},
abstract = {Mobile social matching systems have the potential to transform the way we make new social ties, but only if we are able to overcome the many challenges that exist as to how systems can utilize contextual data to recommend interesting and relevant people to users and facilitate valuable encounters between strangers. This article outlines how context and mobility influence people's motivations to meet new people and presents innovative design concepts for mediating mobile encounters through context-aware social matching systems. Findings from two studies are presented. The first, a survey study (n = 117) explored the concept of contextual rarity of shared user attributes as a measure to improve desirability in mobile social matches. The second, an interview study (n = 58) explored people's motivations to meet others in various contexts. From these studies we derived a set of novel context-aware social matching concepts, including contextual sociability and familiarity as an indicator of opportune social context; contextual engagement as an indicator of opportune personal context; and contextual rarity, oddity, and activity partnering as an indicator of opportune relational context. The findings of these studies establish the importance of different contextual factors and frame the design space of context-aware social matching systems.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {1},
numpages = {32},
keywords = {Context-aware social matching, social discovery, introduction systems, context-awareness, social recommender systems}
}

@article{10.1145/2747874,
author = {Yang, Hui and Guan, Dongyi and Zhang, Sicong},
title = {The Query Change Model: Modeling Session Search as a Markov Decision Process},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2747874},
doi = {10.1145/2747874},
abstract = {Modern information retrieval (IR) systems exhibit user dynamics through interactivity. These dynamic aspects of IR, including changes found in data, users, and systems, are increasingly being utilized in search engines. Session search is one such IR task—document retrieval within a session. During a session, a user constantly modifies queries to find documents that fulfill an information need. Existing IR techniques for assisting the user in this task are limited in their ability to optimize over changes, learn with a minimal computational footprint, and be responsive. This article proposes a novel query change retrieval model (QCM), which uses syntactic editing changes between consecutive queries, as well as the relationship between query changes and previously retrieved documents, to enhance session search. We propose modeling session search as a Markov decision process (MDP). We consider two agents in this MDP: the user agent and the search engine agent. The user agent’s actions are query changes that we observe, and the search engine agent’s actions are term weight adjustments as proposed in this work. We also investigate multiple query aggregation schemes and their effectiveness on session search. Experiments show that our approach is highly effective and outperforms top session search systems in TREC 2011 and TREC 2012.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {20},
numpages = {33},
keywords = {Query change model, Markov decision process, session search, QCM}
}

@article{10.1145/2744199,
author = {Dang, Edward Kai FUNG and Luk, Robert Wing Pong and Allan, James},
title = {Fast Forward Index Methods for Pseudo-Relevance Feedback Retrieval},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2744199},
doi = {10.1145/2744199},
abstract = {The inverted index is the dominant indexing method in information retrieval systems. It enables fast return of the list of all documents containing a given query term. However, for retrieval schemes involving query expansion, as in pseudo-relevance feedback (PRF), the retrieval time based on an inverted index increases linearly with the number of expansion terms. In this regard, we have examined the use of a forward index, which consists of the mapping of each document to its constituent terms. We propose a novel forward index-based reranking scheme to shorten the PRF retrieval time. In our method, a first retrieval of the original query is performed using an inverted index, and then a forward index is employed for the PRF part. We have studied several new forward indexes, including using a novel spstring data structure and the weighted variable bit-block compression (wvbc) signature. With modern hardware such as solid-state drives (SSDs) and sufficiently large main memory, forward index methods are particularly promising. We find that with the whole index stored in main memory, PRF retrieval using a spstring or wvbc forward index excels in time efficiency over an inverted index, being able to obtain the same levels of performance measures at shorter times.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {19},
numpages = {33},
keywords = {time efficiency, inverted index, forward index, Information retrieval}
}

@article{10.1145/2746231,
author = {Cummins, Ronan and Paik, Jiaul H. and Lv, Yuanhua},
title = {A P\'{o}Lya Urn Document Language Model for Improved Information Retrieval},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2746231},
doi = {10.1145/2746231},
abstract = {The multinomial language model has been one of the most effective models of retrieval for more than a decade. However, the multinomial distribution does not model one important linguistic phenomenon relating to term dependency—that is, the tendency of a term to repeat itself within a document (i.e., word burstiness). In this article, we model document generation as a random process with reinforcement (a multivariate P\'{o}lya process) and develop a Dirichlet compound multinomial language model that captures word burstiness directly.We show that the new reinforced language model can be computed as efficiently as current retrieval models, and with experiments on an extensive set of TREC collections, we show that it significantly outperforms the state-of-the-art language model for a number of standard effectiveness metrics. Experiments also show that the tuning parameter in the proposed model is more robust than that in the multinomial language model. Furthermore, we develop a constraint for the verbosity hypothesis and show that the proposed model adheres to the constraint. Finally, we show that the new language model essentially introduces a measure closely related to idf, which gives theoretical justification for combining the term and document event spaces in tf-idf type schemes.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {21},
numpages = {34},
keywords = {retrieval functions, smoothing, P\'{o}lya urn, language models}
}

@article{10.1145/2746229,
author = {White, Ryen W. and Horvitz, Eric},
title = {Belief Dynamics and Biases in Web Search},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2746229},
doi = {10.1145/2746229},
abstract = {We investigate how beliefs about the efficacy of medical interventions are influenced by searchers' exposure to information on retrieved Web pages. We present a methodology for measuring participants' beliefs and confidence about the efficacy of treatment before, during, and after search episodes. We consider interventions studied in the Cochrane collection of meta-analyses. We extract related queries from search engine logs and consider the Cochrane assessments as ground truth. We analyze the dynamics of belief over time and show the influence of prior beliefs and confidence at the end of sessions. We present evidence for confirmation bias and for anchoring-and-adjustment during search and retrieval. Then, we build predictive models to estimate postsearch beliefs using sets of features about behavior and content. The findings provide insights about the influence of Web content on the beliefs of people and have implications for the design of search systems.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {18},
numpages = {46},
keywords = {Belief dynamics, search interaction, cognitive biases}
}

@article{10.1145/2738036,
author = {Han, Shuguang and Yue, Zhen and He, Daqing},
title = {Understanding and Supporting Cross-Device Web Search for Exploratory Tasks with Mobile Touch Interactions},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2738036},
doi = {10.1145/2738036},
abstract = {Mobile devices enable people to look for information at the moment when their information needs are triggered. While experiencing complex information needs that require multiple search sessions, users may utilize desktop computers to fulfill information needs started on mobile devices. Under the context of mobile-to-desktop web search, this article analyzes users’ behavioral patterns and compares them to the patterns in desktop-to-desktop web search. Then, we examine several approaches of using Mobile Touch Interactions (MTIs) to infer relevant content so that such content can be used for supporting subsequent search queries on desktop computers. The experimental data used in this article was collected through a user study involving 24 participants and six properly designed cross-device web search tasks. Our experimental results show that (1) users’ mobile-to-desktop search behaviors do significantly differ from desktop-to-desktop search behaviors in terms of information exploration, sense-making and repeated behaviors. (2) MTIs can be employed to predict the relevance of click-through documents, but applying document-level relevant content based on the predicted relevance does not improve search performance. (3) MTIs can also be used to identify the relevant text chunks at a fine-grained subdocument level. Such relevant information can achieve better search performance than the document-level relevant content. In addition, such subdocument relevant information can be combined with document-level relevance to further improve the search performance. However, the effectiveness of these methods relies on the sufficiency of click-through documents. (4) MTIs can also be obtained from the Search Engine Results Pages (SERPs). The subdocument feedbacks inferred from this set of MTIs even outperform the MTI-based subdocument feedback from the click-through documents.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {16},
numpages = {34},
keywords = {mobile touch interactions, Cross-device web search, relevance feedback, exploratory web search}
}

@article{10.1145/2738035,
author = {Kulkarni, Anagha and Callan, Jamie},
title = {Selective Search: Efficient and Effective Search of Large Textual Collections},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2738035},
doi = {10.1145/2738035},
abstract = {The traditional search solution for large collections divides the collection into subsets (shards), and processes the query against all shards in parallel (exhaustive search). The search cost and the computational requirements of this approach are often prohibitively high for organizations with few computational resources. This article investigates and extends an alternative: selective search, an approach that partitions the dataset based on document similarity to obtain topic-based shards, and searches only a few shards that are estimated to contain relevant documents for the query. We propose shard creation techniques that are scalable, efficient, self-reliant, and create topic-based shards with low variance in size, and high density of relevant documents.The experimental results demonstrate that the effectiveness of selective search is on par with that of exhaustive search, and the corresponding search costs are substantially lower with the former. Also, the majority of the queries perform as well or better with selective search. An oracle experiment that uses optimal shard ranking for a query indicates that selective search can outperform the effectiveness of exhaustive search. Comparison with a query optimization technique shows higher improvements in efficiency with selective search. The overall best efficiency is achieved when the two techniques are combined in an optimized selective search approach.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {17},
numpages = {33},
keywords = {selective search, resource selection, distributed information retrieval, partitioned search, document collection organization, Large-scale text search}
}

@article{10.1145/2735629,
author = {Zhao, Wayne Xin and Zhang, Xudong and Lemire, Daniel and Shan, Dongdong and Nie, Jian-Yun and Yan, Hongfei and Wen, Ji-Rong},
title = {A General SIMD-Based Approach to Accelerating Compression Algorithms},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2735629},
doi = {10.1145/2735629},
abstract = {Compression algorithms are important for data-oriented tasks, especially in the era of “Big Data.” Modern processors equipped with powerful SIMD instruction sets provide us with an opportunity for achieving better compression performance. Previous research has shown that SIMD-based optimizations can multiply decoding speeds. Following these pioneering studies, we propose a general approach to accelerate compression algorithms. By instantiating the approach, we have developed several novel integer compression algorithms, called Group-Simple, Group-Scheme, Group-AFOR, and Group-PFD, and implemented their corresponding vectorized versions. We evaluate the proposed algorithms on two public TREC datasets, a Wikipedia dataset, and a Twitter dataset. With competitive compression ratios and encoding speeds, our SIMD-based algorithms outperform state-of-the-art nonvectorized algorithms with respect to decoding speeds.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {15},
numpages = {28},
keywords = {index compression, SIMD, inverted index, integer encoding}
}

@article{10.1145/2714574,
author = {Yang, Hui},
title = {Browsing Hierarchy Construction by Minimum Evolution},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2714574},
doi = {10.1145/2714574},
abstract = {Hierarchies serve as browsing tools to access information in document collections. This article explores techniques to derive browsing hierarchies that can be used as an information map for task-based search. It proposes a novel minimum-evolution hierarchy construction framework that directly learns semantic distances from training data and from users to construct hierarchies. The aim is to produce globally optimized hierarchical structures by incorporating user-generated task specifications into the general learning framework. Both an automatic version of the framework and an interactive version are presented. A comparison with state-of-the-art systems and a user study jointly demonstrate that the proposed framework is highly effective.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {13},
numpages = {33},
keywords = {minimum evolution, information organization, Browsing hierarchy construction, complex search}
}

@article{10.1145/2699660,
author = {J\"{a}rvelin, Kalervo and Vakkari, Pertti and Arvola, Paavo and Baskaya, Feza and J\"{a}rvelin, Anni and Kek\"{a}l\"{a}inen, Jaana and Keskustalo, Heikki and Kumpulainen, Sanna and Saastamoinen, Miamaria and Savolainen, Reijo and Sormunen, Eero},
title = {Task-Based Information Interaction Evaluation: The Viewpoint of Program Theory},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699660},
doi = {10.1145/2699660},
abstract = {Evaluation is central in research and development of information retrieval (IR). In addition to designing and implementing new retrieval mechanisms, one must also show through rigorous evaluation that they are effective. A major focus in IR is IR mechanisms’ capability of ranking relevant documents optimally for the users, given a query. Searching for information in practice involves searchers, however, and is highly interactive. When human searchers have been incorporated in evaluation studies, the results have often suggested that better ranking does not necessarily lead to better search task, or work task, performance. Therefore, it is not clear which system or interface features should be developed to improve the effectiveness of human task performance. In the present article, we focus on the evaluation of task-based information interaction (TBII). We give special emphasis to learning tasks to discuss TBII in more concrete terms. Information interaction is here understood as behavioral and cognitive activities related to task planning, searching information items, selecting between them, working with them, and synthesizing and reporting. These five generic activities contribute to task performance and outcome and can be supported by information systems. In an attempt toward task-based evaluation, we introduce program theory as the evaluation framework. Such evaluation can investigate whether a program consisting of TBII activities and tools works and how it works and, further, provides a causal description of program (in)effectiveness. Our goal in the present article is to structure TBII on the basis of the five generic activities and consider the evaluation of each activity using the program theory framework. Finally, we combine these activity-based program theories in an overall evaluation framework for TBII. Such an evaluation is complex due to the large number of factors affecting information interaction. Instead of presenting tested program theories, we illustrate how the evaluation of TBII should be accomplished using the program theory framework in the evaluation of systems and behaviors, and their interactions, comprehensively in context.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {3},
numpages = {30},
keywords = {evaluation, Task-based information interaction}
}

@article{10.1145/2699656,
author = {Cole, Michael J. and Hendahewa, Chathra and Belkin, Nicholas J. and Shah, Chirag},
title = {User Activity Patterns During Information Search},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699656},
doi = {10.1145/2699656},
abstract = {Personalization of support for information seeking depends crucially on the information retrieval system's knowledge of the task that led the person to engage in information seeking. Users work during information search sessions to satisfy their task goals, and their activity is not random. To what degree are there patterns in the user activity during information search sessions? Do activity patterns reflect the user's situation as the user moves through the search task under the influence of his or her task goal? Do these patterns reflect aspects of different types of information-seeking tasks? Could such activity patterns identify contexts within which information seeking takes place? To investigate these questions, we model sequences of user behaviors in two independent user studies of information search sessions (N = 32 users, 128 sessions, and N = 40 users, 160 sessions). Two representations of user activity patterns are used. One is based on the sequences of page use; the other is based on a cognitive representation of information acquisition derived from eye movement patterns in service of the reading process. One of the user studies considered journalism work tasks; the other concerned background research in genomics using search tasks taken from the TREC Genomics Track. The search tasks differed in basic dimensions of complexity, specificity, and the type of information product (intellectual or factual) needed to achieve the overall task goal. The results show that similar patterns of user activity are observed at both the cognitive and page use levels. The activity patterns at both representation layers are able to distinguish between task types in similar ways and, to some degree, between tasks of different levels of difficulty. We explore relationships between the results and task difficulty and discuss the use of activity patterns to explore events within a search session. User activity patterns can be at least partially observed in server-side search logs. A focus on patterns of user activity sequences may contribute to the development of information systems that better personalize the user's search experience.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {1},
numpages = {39},
keywords = {Task, cognitive effort, information search behavior, personalization, user study}
}

@article{10.1145/2691351,
author = {Bennett, Paul N. and Collins-Thompson, Kevyn and Kelly, Diane and White, Ryen W. and Zhang, Yi},
title = {Overview of the Special Issue on Contextual Search and Recommendation},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2691351},
doi = {10.1145/2691351},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {1e},
numpages = {7}
}

@article{10.1145/2724706,
author = {Pal, Aditya},
title = {Metrics and Algorithms for Routing Questions to User Communities},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2724706},
doi = {10.1145/2724706},
abstract = {An online community consists of a group of users who share a common interest, background, or experience, and their collective goal is to contribute toward the welfare of the community members. Several websites allow their users to create and manage niche communities, such as Yahoo! Groups, Facebook Groups, Google+ Circles, and WebMD Forums. These community services also exist within enterprises, such as IBM Connections. Question answering within these communities enables their members to exchange knowledge and information with other community members. However, the onus of finding the right community for question asking lies with an individual user. The overwhelming number of communities necessitates the need for a good question routing strategy so that new questions get routed to an appropriately focused community and thus get resolved in a reasonable time frame.In this article, we consider the novel problem of routing a question to the right community and propose a framework for selecting and ranking the relevant communities for a question. We propose several novel features for modeling the three main entities of the system: questions, users, and communities. We propose features such as language attributes, inclination to respond, user familiarity, and difficulty of a question; based on these features, we propose similarity metrics between the routed question and the system entities. We introduce a Cutoff-Aggregation (CA) algorithm that aggregates the entity similarity within a community to compute that community's relevance. We introduce two k-nearest-neighbor (knn) algorithms that are a natural instantiation of the CA algorithm, which are computationally efficient and evaluate several ranking algorithms over the aggregate similarity scores computed by the two knn algorithms. We propose clustering techniques to speed up our recommendation framework and show how pipelining can improve the model performance. We demonstrate the effectiveness of our framework on two large real-world datasets.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {14},
numpages = {29},
keywords = {group recommendation, Question answering, community question routing}
}

@article{10.1145/2699670,
author = {Yin, Hongzhi and Cui, Bin and Chen, Ling and Hu, Zhiting and Zhou, Xiaofang},
title = {Dynamic User Modeling in Social Media Systems},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699670},
doi = {10.1145/2699670},
abstract = {Social media provides valuable resources to analyze user behaviors and capture user preferences. This article focuses on analyzing user behaviors in social media systems and designing a latent class statistical mixture model, named temporal context-aware mixture model (TCAM), to account for the intentions and preferences behind user behaviors. Based on the observation that the behaviors of a user in social media systems are generally influenced by intrinsic interest as well as the temporal context (e.g., the public's attention at that time), TCAM simultaneously models the topics related to users' intrinsic interests and the topics related to temporal context and then combines the influences from the two factors to model user behaviors in a unified way. Considering that users' interests are not always stable and may change over time, we extend TCAM to a dynamic temporal context-aware mixture model (DTCAM) to capture users' changing interests. To alleviate the problem of data sparsity, we exploit the social and temporal correlation information by integrating a social-temporal regularization framework into the DTCAM model. To further improve the performance of our proposed models (TCAM and DTCAM), an item-weighting scheme is proposed to enable them to favor items that better represent topics related to user interests and topics related to temporal context, respectively. Based on our proposed models, we design a temporal context-aware recommender system (TCARS). To speed up the process of producing the top-k recommendations from large-scale social media data, we develop an efficient query-processing technique to support TCARS. Extensive experiments have been conducted to evaluate the performance of our models on four real-world datasets crawled from different social media sites. The experimental results demonstrate the superiority of our models, compared with the state-of-the-art competitor methods, by modeling user behaviors more precisely and making more effective and efficient recommendations.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {10},
numpages = {44},
keywords = {social media mining, temporal recommender system, User behavior modeling, probabilistic generative model}
}

@article{10.1145/2699671,
author = {Anagnostopoulos, Aris and Becchetti, Luca and Bordino, Ilaria and Leonardi, Stefano and Mele, Ida and Sankowski, Piotr},
title = {Stochastic Query Covering for Fast Approximate Document Retrieval},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699671},
doi = {10.1145/2699671},
abstract = {We design algorithms that, given a collection of documents and a distribution over user queries, return a small subset of the document collection in such a way that we can efficiently provide high-quality answers to user queries using only the selected subset. This approach has applications when space is a constraint or when the query-processing time increases significantly with the size of the collection. We study our algorithms through the lens of stochastic analysis and prove that even though they use only a small fraction of the entire collection, they can provide answers to most user queries, achieving a performance close to the optimal. To complement our theoretical findings, we experimentally show the versatility of our approach by considering two important cases in the context of Web search. In the first case, we favor the retrieval of documents that are relevant to the query, whereas in the second case we aim for document diversification. Both the theoretical and the experimental analysis provide strong evidence of the potential value of query covering in diverse application scenarios.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {11},
numpages = {35},
keywords = {Algorithms, query covering, stochastic analysis, caching}
}

@article{10.1145/2699669,
author = {Na, Seung-Hoon},
title = {Two-Stage Document Length Normalization for Information Retrieval},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699669},
doi = {10.1145/2699669},
abstract = {The standard approach for term frequency normalization is based only on the document length. However, it does not distinguish the verbosity from the scope, these being the two main factors determining the document length. Because the verbosity and scope have largely different effects on the increase in term frequency, the standard approach can easily suffer from insufficient or excessive penalization depending on the specific type of long document. To overcome these problems, this article proposes two-stage normalization by performing verbosity and scope normalization separately, and by employing different penalization functions. In verbosity normalization, each document is prenormalized by dividing the term frequency by the verbosity of the document. In scope normalization, an existing retrieval model is applied in a straightforward manner to the prenormalized document, finally leading us to formulate our proposed verbosity normalized (VN) retrieval model. Experimental results carried out on standard TREC collections demonstrate that the VN model leads to marginal but statistically significant improvements over standard retrieval models.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {8},
numpages = {40},
keywords = {Verbosity normalization, scope normalization, retrieval heuristics, term frequency, document length normalization}
}

@article{10.1145/2699668,
author = {Ah-Pine, Julien and Csurka, Gabriela and Clinchant, St\'{e}phane},
title = {Unsupervised Visual and Textual Information Fusion in CBMIR Using Graph-Based Methods},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699668},
doi = {10.1145/2699668},
abstract = {Multimedia collections are more than ever growing in size and diversity. Effective multimedia retrieval systems are thus critical to access these datasets from the end-user perspective and in a scalable way. We are interested in repositories of image/text multimedia objects and we study multimodal information fusion techniques in the context of content-based multimedia information retrieval. We focus on graph-based methods, which have proven to provide state-of-the-art performances. We particularly examine two such methods: cross-media similarities and random-walk-based scores. From a theoretical viewpoint, we propose a unifying graph-based framework, which encompasses the two aforementioned approaches. Our proposal allows us to highlight the core features one should consider when using a graph-based technique for the combination of visual and textual information. We compare cross-media and random-walk-based results using three different real-world datasets. From a practical standpoint, our extended empirical analyses allow us to provide insights and guidelines about the use of graph-based methods for multimodal information fusion in content-based multimedia information retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {9},
numpages = {31},
keywords = {random walk, cross-media similarity, information fusion, graph-based methods, Visual reranking, Content-based multimedia information retrieval}
}

@article{10.1145/2699667,
author = {Yuan, Quan and Cong, Gao and Zhao, Kaiqi and Ma, Zongyang and Sun, Aixin},
title = {Who, Where, When, and What: A Nonparametric Bayesian Approach to Context-Aware Recommendation and Search for Twitter Users},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699667},
doi = {10.1145/2699667},
abstract = {Micro-blogging services and location-based social networks, such as Twitter, Weibo, and Foursquare, enable users to post short messages with timestamps and geographical annotations. The rich spatial-temporal-semantic information of individuals embedded in these geo-annotated short messages provides exciting opportunity to develop many context-aware applications in ubiquitous computing environments. Example applications include contextual recommendation and contextual search. To obtain accurate recommendations and most relevant search results, it is important to capture users’ contextual information (e.g., time and location) and to understand users’ topical interests and intentions. While time and location can be readily captured by smartphones, understanding user’s interests and intentions calls for effective methods in modeling user mobility behavior. Here, user mobility refers to who visits which place at what time for what activity. That is, user mobility behavior modeling must consider user (Who), spatial (Where), temporal (When), and activity (What) aspects. Unfortunately, no previous studies on user mobility behavior modeling have considered all of the four aspects jointly, which have complex interdependencies. In our preliminary study, we propose the first solution named W4 (short for Who, Where, When, and What) to discover user mobility behavior from the four aspects. In this article, we further enhance W4 and propose a nonparametric Bayesian model named EW4 (short for Enhanced W4). EW4 requires no parameter tuning and achieves better results over W4 in our experiments. Given some of the four aspects of a user (e.g., time), our model is able to infer information of the other aspects (e.g., location and topical words). Thus, our model has a variety of context-aware applications, particularly in contextual search and recommendation. Experimental results on two real-world datasets show that the proposed model is effective in discovering users’ spatial-temporal topics. The model also significantly outperforms state-of-the-art baselines for various tasks including location prediction for tweets and requirement-aware location recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {2},
numpages = {33},
keywords = {spatiotemporal, graphical model, context-aware, requirement-aware location recommendation, geographical topic modeling, Twitter, Recommendation and search}
}

@article{10.1145/2699666,
author = {Bing, Lidong and Lam, Wai and Wong, Tak-Lam and Jameel, Shoaib},
title = {Web Query Reformulation via Joint Modeling of Latent Topic Dependency and Term Context},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699666},
doi = {10.1145/2699666},
abstract = {An important way to improve users’ satisfaction in Web search is to assist them by issuing more effective queries. One such approach is query reformulation, which generates new queries according to the current query issued by users. A common procedure for conducting reformulation is to generate some candidate queries first, then a scoring method is employed to assess these candidates. Currently, most of the existing methods are context based. They rely heavily on the context relation of terms in the history queries and cannot detect and maintain the semantic consistency of queries. In this article, we propose a graphical model to score queries. The proposed model exploits a latent topic space, which is automatically derived from the query log, to detect semantic dependency of terms in a query and dependency among topics. Meanwhile, the graphical model also captures the term context in the history query by skip-bigram and n-gram language models. In addition, our model can be easily extended to consider users’ history search interests when we conduct query reformulation for different users. In the task of candidate query generation, we investigate a social tagging data resource—Delicious bookmark—to generate addition and substitution patterns that are employed as supplements to the patterns generated from query log data.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {6},
numpages = {38},
keywords = {social tagging, Web query reformulation, query log, graphical model}
}

@article{10.1145/2699665,
author = {Nong, Ge and Chan, Wai Hong and Hu, Sheng Qing and Wu, Yi},
title = {Induced Sorting Suffixes in External Memory},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699665},
doi = {10.1145/2699665},
abstract = {We present in this article an external memory algorithm, called disk SA-IS (DSA-IS), to exactly emulate the induced sorting algorithm SA-IS previously proposed for sorting suffixes in RAM. DSA-IS is a new disk-friendly method for sequentially retrieving the preceding character of a sorted suffix to induce the order of the preceding suffix. For a sizen string of a constant or integer alphabet, given the RAM capacity Ω ((nW)0.5), where W is the size of each I/O buffer that is large enough to amortize the overhead of each access to disk, both the CPU time and peak disk use of DSA-IS are O(n). Our experimental study shows that on average, DSA-IS achieves the best time and space results of all of the existing external memory algorithms based on the induced sorting principle.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {12},
numpages = {15},
keywords = {sorting algorithm, external memory, Suffix array}
}

@article{10.1145/2699662,
author = {Tian, Yonghong and Qian, Mengren and Huang, Tiejun},
title = {TASC: A Transformation-Aware Soft Cascading Approach for Multimodal Video Copy Detection},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699662},
doi = {10.1145/2699662},
abstract = {How to precisely and efficiently detect near-duplicate copies with complicated audiovisual transformations from a large-scale video database is a challenging task. To cope with this challenge, this article proposes a transformation-aware soft cascading (TASC) approach for multimodal video copy detection. Basically, our approach divides query videos into some categories and then for each category designs a transformation-aware chain to organize several detectors in a cascade structure. In each chain, efficient but simple detectors are placed in the forepart, whereas effective but complex detectors are located in the rear. To judge whether two videos are near-duplicates, a Detection-on-Copy-Units mechanism is introduced in the TASC, which makes the decision of copy detection depending on the similarity between their most similar fractions, called copy units (CUs), rather than the video-level similarity. Following this, we propose a CU search algorithm to find a pair of CUs from two videos and a CU-based localization algorithm to find the precise locations of their copy segments that are with the asserted CUs as the center. Moreover, to address the problem that the copies and noncopies are possibly linearly inseparable in the feature space, the TASC also introduces a flexible strategy, called soft decision boundary, to replace the single threshold strategy for each detector. Its basic idea is to automatically learn two thresholds for each detector to examine the easy-to-judge copies and noncopies, respectively, and meanwhile to train a nonlinear classifier to further check those hard-to-judge ones. Extensive experiments on three benchmark datasets showed that the TASC can achieve excellent copy detection accuracy and localization precision with a very high processing efficiency.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {7},
numpages = {34},
keywords = {soft decision boundary, Video copy detection, multimodal features, copy units, transformation-aware soft cascading}
}

@article{10.1145/2699661,
author = {Alhindi, Azhar and Kruschwitz, Udo and Fox, Chris and Albakour, M-Dyaa},
title = {Profile-Based Summarisation for Web Site Navigation},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2699661},
doi = {10.1145/2699661},
abstract = {Information systems that utilise contextual information have the potential of helping a user identify relevant information more quickly and more accurately than systems that work the same for all users and contexts. Contextual information comes in a variety of types, often derived from records of past interactions between a user and the information system. It can be individual or group based. We are focusing on the latter, harnessing the search behaviour of cohorts of users, turning it into a domain model that can then be used to assist other users of the same cohort. More specifically, we aim to explore how such a domain model is best utilised for profile-biased summarisation of documents in a navigation scenario in which such summaries can be displayed as hover text as a user moves the mouse over a link. The main motivation is to help a user find relevant documents more quickly. Given the fact that the Web in general has been studied extensively already, we focus our attention on Web sites and similar document collections. Such collections can be notoriously difficult to search or explore. The process of acquiring the domain model is not a research interest here; we simply adopt a biologically inspired method that resembles the idea of ant colony optimisation. This has been shown to work well in a variety of application areas. The model can be built in a continuous learning cycle that exploits search patterns as recorded in typical query log files. Our research explores different summarisation techniques, some of which use the domain model and some that do not. We perform task-based evaluations of these different techniques—thus of the impact of the domain model and profile-biased summarisation—in the context of Web site navigation.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {4},
numpages = {39},
keywords = {navigation, browsing, single-document summarisation (SDS), group profiling, log analysis, multi-document summarisation (MDS), Term association networks}
}

@article{10.1145/2668120,
author = {Chuklin, Aleksandr and Schuth, Anne and Zhou, Ke and Rijke, Maarten De},
title = {A Comparative Analysis of Interleaving Methods for Aggregated Search},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2668120},
doi = {10.1145/2668120},
abstract = {A result page of a modern search engine often goes beyond a simple list of “10 blue links.” Many specific user needs (e.g., News, Image, Video) are addressed by so-called aggregated or vertical search solutions: specially presented documents, often retrieved from specific sources, that stand out from the regular organic Web search results. When it comes to evaluating ranking systems, such complex result layouts raise their own challenges. This is especially true for so-called interleaving methods that have arisen as an important type of online evaluation: by mixing results from two different result pages, interleaving can easily break the desired Web layout in which vertical documents are grouped together, and hence hurt the user experience.We conduct an analysis of different interleaving methods as applied to aggregated search engine result pages. Apart from conventional interleaving methods, we propose two vertical-aware methods: one derived from the widely used Team-Draft Interleaving method by adjusting it in such a way that it respects vertical document groupings, and another based on the recently introduced Optimized Interleaving framework. We show that our proposed methods are better at preserving the user experience than existing interleaving methods while still performing well as a tool for comparing ranking systems. For evaluating our proposed vertical-aware interleaving methods, we use real-world click data as well as simulated clicks and simulated ranking systems.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {5},
numpages = {38},
keywords = {Information retrieval, online evaluation, interleaving, interleaved comparison, aggregated search, clicks}
}

@article{10.1145/2661629,
author = {Wang, Jianguo and Lo, Eric and Yiu, Man Lung and Tong, Jiancong and Wang, Gang and Liu, Xiaoguang},
title = {Cache Design of SSD-Based Search Engine Architectures: An Experimental Study},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2661629},
doi = {10.1145/2661629},
abstract = {Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid-state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this article, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. Based on the results, we give insights to practitioners and researchers on how to adapt the infrastructure and caching policies for SSD-based search engines.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {21},
numpages = {26},
keywords = {cache, solid-state drive, Search engine, query processing}
}

@article{10.1145/2651363,
author = {Mahdabi, Parvaz and Crestani, Fabio},
title = {Patent Query Formulation by Synthesizing Multiple Sources of Relevance Evidence},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2651363},
doi = {10.1145/2651363},
abstract = {Patent prior art search is a task in patent retrieval with the goal of finding documents which describe prior art work related to a query patent. A query patent is a full patent application composed of hundreds of terms which does not represent a single focused information need. Fortunately, other relevance evidence sources (i.e., classification tags and bibliographical data) provide additional details about the underlying information need. In this article, we propose a unified framework that integrates multiple relevance evidence components for query formulation. We first build a query model from the textual fields of a query patent. To overcome the term mismatch, we expand this initial query model with the term distribution of documents in the citation graph, modeling old and recent domain terminology. We build an IPC lexicon and perform query expansion using this lexicon incorporating proximity information. We performed an empirical evaluation on two patent datasets. Our results show that employing the temporal features of documents has a precision enhancing effect, while query expansion using IPC lexicon improves the recall of the final rank list.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {16},
numpages = {30},
keywords = {Patent search, query expansion, proximity, citation analysis}
}

@article{10.1145/2644807,
author = {Ture, Ferhan and Lin, Jimmy},
title = {Exploiting Representations from Statistical Machine Translation for Cross-Language Information Retrieval},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2644807},
doi = {10.1145/2644807},
abstract = {This work explores how internal representations of modern statistical machine translation systems can be exploited for cross-language information retrieval. We tackle two core issues that are central to query translation: how to exploit context to generate more accurate translations and how to preserve ambiguity that may be present in the original query, thereby retaining a diverse set of translation alternatives. These two considerations are often in tension since ambiguity in natural language is typically resolved by exploiting context, but effective retrieval requires striking the right balance. We propose two novel query translation approaches: the grammar-based approach extracts translation probabilities from translation grammars, while the decoder-based approach takes advantage of n-best translation hypotheses. Both are context-sensitive, in contrast to a baseline context-insensitive approach that uses bilingual dictionaries for word-by-word translation. Experimental results show that by “opening up” modern statistical machine translation systems, we can access intermediate representations that yield high retrieval effectiveness. By combining evidence from multiple sources, we demonstrate significant improvements over competitive baselines on standard cross-language information retrieval test collections. In addition to effectiveness, the efficiency of our techniques are explored as well.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {19},
numpages = {32}
}

@article{10.1145/2641564,
author = {Forsati, Rana and Mahdavi, Mehrdad and Shamsfard, Mehrnoush and Sarwat, Mohamed},
title = {Matrix Factorization with Explicit Trust and Distrust Side Information for Improved Social Recommendation},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2641564},
doi = {10.1145/2641564},
abstract = {With the advent of online social networks, recommender systems have became crucial for the success of many online applications/services due to their significance role in tailoring these applications to user-specific needs or preferences. Despite their increasing popularity, in general, recommender systems suffer from data sparsity and cold-start problems. To alleviate these issues, in recent years, there has been an upsurge of interest in exploiting social information such as trust relations among users along with the rating data to improve the performance of recommender systems. The main motivation for exploiting trust information in the recommendation process stems from the observation that the ideas we are exposed to and the choices we make are significantly influenced by our social context. However, in large user communities, in addition to trust relations, distrust relations also exist between users. For instance, in Epinions, the concepts of personal “web of trust” and personal “block list” allow users to categorize their friends based on the quality of reviews into trusted and distrusted friends, respectively. Hence, it will be interesting to incorporate this new source of information in recommendation as well. In contrast to the incorporation of trust information in recommendation which is thriving, the potential of explicitly incorporating distrust relations is almost unexplored. In this article, we propose a matrix factorization-based model for recommendation in social rating networks that properly incorporates both trust and distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and cold-start users issues. Through experiments on the Epinions dataset, we show that our new algorithm outperforms its standard trust-enhanced or distrust-enhanced counterparts with respect to accuracy, thereby demonstrating the positive effect that incorporation of explicit distrust information can have on recommender systems.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {17},
numpages = {38},
keywords = {recommender systems, social relationships, Matrix factorization}
}

@article{10.1145/2630420,
author = {Lu, Shiyang and Mei, Tao and Wang, Jingdong and Zhang, Jian and Wang, Zhiyong and Li, Shipeng},
title = {Browse-to-Search: Interactive Exploratory Search with Visual Entities},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2630420},
doi = {10.1145/2630420},
abstract = {With the development of image search technology, users are no longer satisfied with searching for images using just metadata and textual descriptions. Instead, more search demands are focused on retrieving images based on similarities in their contents (textures, colors, shapes etc.). Nevertheless, one image may deliver rich or complex content and multiple interests. Sometimes users do not sufficiently define or describe their seeking demands for images even when general search interests appear, owing to a lack of specific knowledge to express their intents. A new form of information seeking activity, referred to as exploratory search, is emerging in the research community, which generally combines browsing and searching content together to help users gain additional knowledge and form accurate queries, thereby assisting the users with their seeking and investigation activities. However, there have been few attempts at addressing integrated exploratory search solutions when image browsing is incorporated into the exploring loop. In this work, we investigate the challenges of understanding users' search interests from the images being browsed and infer their actual search intentions. We develop a novel system to explore an effective and efficient way for allowing users to seamlessly switch between browse and search processes, and naturally complete visual-based exploratory search tasks. The system, called Browse-to-Search enables users to specify their visual search interests by circling any visual objects in the webpages being browsed, and then the system automatically forms the visual entities to represent users' underlying intent. One visual entity is not limited by the original image content, but also encapsulated by the textual-based browsing context and the associated heterogeneous attributes. We use large-scale image search technology to find the associated textual attributes from the repository. Users can then utilize the encapsulated visual entities to complete search tasks. The Browse-to-Search system is one of the first attempts to integrate browse and search activities for a visual-based exploratory search, which is characterized by four unique properties: (1) in session—searching is performed during browsing session and search results naturally accompany with browsing content; (2) in context—the pages being browsed provide text-based contextual cues for searching; (3) in focus—users can focus on the visual content of interest without worrying about the difficulties of query formulation, and visual entities will be automatically formed; and (4) intuitiveness—a touch and visual search-based user interface provides a natural user experience. We deploy the Browse-to-Search system on tablet devices and evaluate the system performance using millions of images. We demonstrate that it is effective and efficient in facilitating the user's exploratory search compared to the conventional image search methods and, more importantly, provides users with more robust results to satisfy their exploring experience.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {18},
numpages = {27},
keywords = {Multimedia information systems, gesture, multimedia browsing, exploratory search, interactive visual search, user interaction}
}

@article{10.1145/2629553,
author = {Raman, Karthik and Bennett, Paul N. and Collins-Thompson, Kevyn},
title = {Understanding Intrinsic Diversity in Web Search: Improving Whole-Session Relevance},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2629553},
doi = {10.1145/2629553},
abstract = {Current research on Web search has focused on optimizing and evaluating single queries. However, a significant fraction of user queries are part of more complex tasks [Jones and Klinkner 2008] which span multiple queries across one or more search sessions [Liu and Belkin 2010; Kotov et al. 2011]. An ideal search engine would not only retrieve relevant results for a user's particular query but also be able to identify when the user is engaged in a more complex task and aid the user in completing that task [Morris et al. 2008; Agichtein et al. 2012]. Toward optimizing whole-session or task relevance, we characterize and address the problem of intrinsic diversity (ID) in retrieval [Radlinski et al. 2009], a type of complex task that requires multiple interactions with current search engines. Unlike existing work on extrinsic diversity [Carbonell and Goldstein 1998; Zhai et al. 2003; Chen and Karger 2006] that deals with ambiguity in intent across multiple users, ID queries often have little ambiguity in intent but seek content covering a variety of aspects on a shared theme. In such scenarios, the underlying needs are typically exploratory, comparative, or breadth-oriented in nature. We identify and address three key problems for ID retrieval: identifying authentic examples of ID tasks from post-hoc analysis of behavioral signals in search logs; learning to identify initiator queries that mark the start of an ID search task; and given an initiator query, predicting which content to prefetch and rank.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {20},
numpages = {45},
keywords = {search behavior, diversity, proactive search, Search session analysis}
}

@article{10.1145/2633044,
author = {Skaggs, Bradley and Getoor, Lise},
title = {Topic Modeling for Wikipedia Link Disambiguation},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2633044},
doi = {10.1145/2633044},
abstract = {Many articles in the online encyclopedia Wikipedia have hyperlinks to ambiguous article titles; these ambiguous links should be replaced with links to unambiguous articles, a process known as disambiguation. We propose a novel statistical topic model based on link text, which we refer to as the Link Text Topic Model (LTTM), that we use to suggest new link targets for ambiguous links. To evaluate our model, we describe a method for extracting ground truth for this link disambiguation task from edits made to Wikipedia in a specific time period. We use this ground truth to demonstrate the superiority of LTTM over other existing link- and content-based approaches to disambiguating links in Wikipedia. Finally, we build a web service that uses LTTM to make suggestions to human editors wanting to fix ambiguous links in Wikipedia.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {10},
numpages = {24},
keywords = {link disambiguation, Wikipedia, Topic modeling}
}

@article{10.1145/2629685,
author = {Laere, Olivier Van and Schockaert, Steven and Tanasescu, Vlad and Dhoedt, Bart and Jones, Christopher B.},
title = {Georeferencing Wikipedia Documents Using Data from Social Media Sources},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2629685},
doi = {10.1145/2629685},
abstract = {Social media sources such as Flickr and Twitter continuously generate large amounts of textual information (tags on Flickr and short messages on Twitter). This textual information is increasingly linked to geographical coordinates, which makes it possible to learn how people refer to places by identifying correlations between the occurrence of terms and the locations of the corresponding social media objects. Recent work has focused on how this potentially rich source of geographic information can be used to estimate geographic coordinates for previously unseen Flickr photos or Twitter messages. In this article, we extend this work by analysing to what extent probabilistic language models trained on Flickr and Twitter can be used to assign coordinates to Wikipedia articles. Our results show that exploiting these language models substantially outperforms both (i) classical gazetteer-based methods (in particular, using Yahoo! Placemaker and Geonames) and (ii) language modelling approaches trained on Wikipedia alone. This supports the hypothesis that social media are important sources of geographic information, which are valuable beyond the scope of individual applications.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {12},
numpages = {32},
keywords = {Geographic information retrieval, language models, semistructured Data}
}

@article{10.1145/2629554,
author = {Brisaboa, Nieves R. and Cerdeira-Pena, Ana and Navarro, Gonzalo},
title = {XXS: Efficient XPath Evaluation on Compressed XML Documents},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2629554},
doi = {10.1145/2629554},
abstract = {The eXtensible Markup Language (XML) is acknowledged as the de facto standard for semistructured data representation and data exchange on the Web and many other scenarios. A well-known shortcoming of XML is its verbosity, which increases manipulation, transmission, and processing costs. Various structure-blind and structure-conscious compression techniques can be applied to XML, and some are even access-friendly, meaning that the documents can be efficiently accessed in compressed form. Direct access is necessary to implement the query languages XPath and XQuery, which are the standard ones to exploit the expressiveness of XML. While a good deal of theoretical and practical proposals exist to solve XPath/XQuery operations on XML, only a few ones are well integrated with a compression format that supports the required access operations on the XML data. In this work we go one step further and design a compression format for XML collections that boosts the performance of XPath queries on the data. This is done by designing compressed representations of the XML data that support some complex operations apart from just accessing the data, and those are exploited to solve key components of the XPath queries. Our system, called XXS, is aimed at XML collections containing natural language text, which are compressed to within 35%--50% of their original size while supporting a large subset of XPath operations in time competitive with, and many times outperforming, the best state-of-the-art systems that work on uncompressed representations.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {13},
numpages = {37},
keywords = {self-index, compression, Semistructured data, XML, XPath}
}

@article{10.1145/2629531,
author = {Awad, George and Over, Paul and Kraaij, Wessel},
title = {Content-Based Video Copy Detection Benchmarking at TRECVID},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2629531},
doi = {10.1145/2629531},
abstract = {This article presents an overview of the video copy detection benchmark which was run over a period of 4 years (2008--2011) as part of the TREC Video Retrieval (TRECVID) workshop series. The main contributions of the article include i) an examination of the evolving design of the evaluation framework and its components (system tasks, data, measures); ii) a high-level overview of results and best-performing approaches; and iii) a discussion of lessons learned over the four years. The content-based copy detection (CCD) benchmark worked with a large collection of synthetic queries, which is atypical for TRECVID, as was the use of a normalized detection cost framework. These particular evaluation design choices are motivated and appraised.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {14},
numpages = {40},
keywords = {Video copy detection, multimedia, TRECVID, evaluation}
}

@article{10.1145/2629530,
author = {Zhang, Richong and Mao, Yongyi},
title = {Trust Prediction via Belief Propagation},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2629530},
doi = {10.1145/2629530},
abstract = {The prediction of trust relationships in social networks plays an important role in the analytics of the networks. Although various link prediction algorithms for general networks may be adapted for this purpose, the recent notion of “trust propagation” has been shown to effectively capture the trust-formation mechanisms and resulted in an effective prediction algorithm. This article builds on the concept of trust propagation and presents a probabilistic trust propagation model. Our model exploits the modern framework of probabilistic graphical models, more specifically, factor graphs. Under this model, the trust prediction problem can be formulated as a statistical inference problem and we derive the belief propagation algorithm as a solver for trust prediction. The model and algorithm are tested using datasets from Epinions and Ciao, by which performance advantages over the previous algorithms are demonstrated.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {15},
numpages = {27},
keywords = {social network, Trust propagation, link prediction}
}

@article{10.1145/2629461,
author = {Yin, Hongzhi and Cui, Bin and Sun, Yizhou and Hu, Zhiting and Chen, Ling},
title = {LCARS: A Spatial Item Recommender System},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2629461},
doi = {10.1145/2629461},
abstract = {Newly emerging location-based and event-based social network services provide us with a new platform to understand users' preferences based on their activity history. A user can only visit a limited number of venues/events and most of them are within a limited distance range, so the user-item matrix is very sparse, which creates a big challenge to the traditional collaborative filtering-based recommender systems. The problem becomes even more challenging when people travel to a new city where they have no activity information.In this article, we propose LCARS, a location-content-aware recommender system that offers a particular user a set of venues (e.g., restaurants and shopping malls) or events (e.g., concerts and exhibitions) by giving consideration to both personal interest and local preference. This recommender system can facilitate people's travel not only near the area in which they live, but also in a city that is new to them. Specifically, LCARS consists of two components: offline modeling and online recommendation. The offline modeling part, called LCA-LDA, is designed to learn the interest of each individual user and the local preference of each individual city by capturing item cooccurrence patterns and exploiting item contents. The online recommendation part takes a querying user along with a querying city as input, and automatically combines the learned interest of the querying user and the local preference of the querying city to produce the top-k recommendations. To speed up the online process, a scalable query processing technique is developed by extending both the Threshold Algorithm (TA) and TA-approximation algorithm. We evaluate the performance of our recommender system on two real datasets, that is, DoubanEvent and Foursquare, and one large-scale synthetic dataset. The results show the superiority of LCARS in recommending spatial items for users, especially when traveling to new cities, in terms of both effectiveness and efficiency. Besides, the experimental analysis results also demonstrate the excellent interpretability of LCARS.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {11},
numpages = {37},
keywords = {location-based service, probabilistic generative model, cold start, Recommender system, TA algorithm}
}

@article{10.1145/2590988,
author = {Zhao, Jiashu and Huang, Jimmy Xiangji and Ye, Zheng},
title = {Modeling Term Associations for Probabilistic Information Retrieval},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2590988},
doi = {10.1145/2590988},
abstract = {Traditionally, in many probabilistic retrieval models, query terms are assumed to be independent. Although such models can achieve reasonably good performance, associations can exist among terms from a human being’s point of view. There are some recent studies that investigate how to model term associations/dependencies by proximity measures. However, the modeling of term associations theoretically under the probabilistic retrieval framework is still largely unexplored. In this article, we introduce a new concept cross term, to model term proximity, with the aim of boosting retrieval performance. With cross terms, the association of multiple query terms can be modeled in the same way as a simple unigram term. In particular, an occurrence of a query term is assumed to have an impact on its neighboring text. The degree of the query-term impact gradually weakens with increasing distance from the place of occurrence. We use shape functions to characterize such impacts. Based on this assumption, we first propose a bigram CRoss TErm Retrieval (CRTER2) model as the basis model, and then recursively propose a generalized n-gram CRoss TErm Retrieval (CRTERn) model for n query terms, where n &gt; 2. Specifically, a bigram cross term occurs when the corresponding query terms appear close to each other, and its impact can be modeled by the intersection of the respective shape functions of the query terms. For an n-gram cross term, we develop several distance metrics with different properties and employ them in the proposed models for ranking. We also show how to extend the language model using the newly proposed cross terms. Extensive experiments on a number of TREC collections demonstrate the effectiveness of our proposed models.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {7},
numpages = {47},
keywords = {kernel, term association, BM25, Cross term, N-gram, probabilistic information retrieval}
}

@article{10.1145/2590975,
author = {Markov, Ilya and Crestani, Fabio},
title = {Theoretical, Qualitative, and Quantitative Analyses of Small-Document Approaches to Resource Selection},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2590975},
doi = {10.1145/2590975},
abstract = {In a distributed retrieval setup, resource selection is the problem of identifying and ranking relevant sources of information for a given user’s query. For better usage of existing resource-selection techniques, it is desirable to know what the fundamental differences between them are and in what settings one is superior to others. However, little is understood still about the actual behavior of resource-selection methods. In this work, we focus on small-document approaches to resource selection that rank and select sources based on the ranking of their documents. We pose a number of research questions and approach them by three types of analyses. First, we present existing small-document techniques in a unified framework and analyze them theoretically. Second, we propose using a qualitative analysis to study the behavior of different small-document approaches. Third, we present a novel experimental methodology to evaluate small-document techniques and to validate the results of the qualitative analysis. This way, we answer the posed research questions and provide insights about small-document methods in general and about each technique in particular.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {9},
numpages = {37},
keywords = {distributed information retrieval, small-document model, Resource selection}
}

@article{10.1145/2590974,
author = {Cui, Peng and Liu, Shao-Wei and Zhu, Wen-Wu and Luan, Huan-Bo and Chua, Tat-Seng and Yang, Shi-Qiang},
title = {Social-Sensed Image Search},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2590974},
doi = {10.1145/2590974},
abstract = {Although Web search techniques have greatly facilitate users’ information seeking, there are still quite a lot of search sessions that cannot provide satisfactory results, which are more serious in Web image search scenarios. How to understand user intent from observed data is a fundamental issue and of paramount significance in improving image search performance. Previous research efforts mostly focus on discovering user intent either from clickthrough behavior in user search logs (e.g., Google), or from social data to facilitate vertical image search in a few limited social media platforms (e.g., Flickr). This article aims to combine the virtues of these two information sources to complement each other, that is, sensing and understanding users’ interests from social media platforms and transferring this knowledge to rerank the image search results in general image search engines. Toward this goal, we first propose a novel social-sensed image search framework, where both social media and search engine are jointly considered. To effectively and efficiently leverage these two kinds of platforms, we propose an example-based user interest representation and modeling method, where we construct a hybrid graph from social media and propose a hybrid random-walk algorithm to derive the user-image interest graph. Moreover, we propose a social-sensed image reranking method to integrate the user-image interest graph from social media and search results from general image search engines to rerank the images by fusing their social relevance and visual relevance. We conducted extensive experiments on real-world data from Flickr and Google image search, and the results demonstrated that the proposed methods can significantly improve the social relevance of image search results while maintaining visual relevance well.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {8},
numpages = {23},
keywords = {Social media, image ranking, hybrid random walk, image search}
}

@article{10.1145/2590972,
author = {Bast, Hannah and Celikik, Marjan},
title = {Efficient Index-Based Snippet Generation},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2590972},
doi = {10.1145/2590972},
abstract = {Ranked result lists with query-dependent snippets have become state of the art in text search. They are typically implemented by searching, at query time, for occurrences of the query words in the top-ranked documents. This document-based approach has three inherent problems: (i) when a document is indexed by terms which it does not contain literally (e.g., related words or spelling variants), localization of the corresponding snippets becomes problematic; (ii) each query operator (e.g., phrase or proximity search) has to be implemented twice, on the index side in order to compute the correct result set, and on the snippet-generation side to generate the appropriate snippets; and (iii) in a worst case, the whole document needs to be scanned for occurrences of the query words, which could be problematic for very long documents.We present a new index-based method that localizes snippets by information solely computed from the index and that overcomes all three problems. Unlike previous index-based methods, we show how to achieve this at essentially no extra cost in query processing time, by a technique we call operator inversion. We also show how our index-based method allows the caching of individual segments instead of complete documents, which enables a significantly larger cache hit-ratio as compared to the document-based approach. We have fully integrated our implementation with the CompleteSearch engine.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {6},
numpages = {24},
keywords = {efficiency, caching, Snippets, document summarization, advanced search}
}

@article{10.1145/2559170,
author = {Cummins, Ronan},
title = {Document Score Distribution Models for Query Performance Inference and Prediction},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2559170},
doi = {10.1145/2559170},
abstract = {Modelling the distribution of document scores returned from an information retrieval (IR) system in response to a query is of both theoretical and practical importance. One of the goals of modelling document scores in this manner is the inference of document relevance. There has been renewed interest of late in modelling document scores using parameterised distributions. Consequently, a number of hypotheses have been proposed to constrain the mixture distribution from which document scores could be drawn.In this article, we show how a standard performance measure (i.e., average precision) can be inferred from a document score distribution using labelled data. We use the accuracy of the inference of average precision as a measure for determining the usefulness of a particular model of document scores. We provide a comprehensive study which shows that certain mixtures of distributions are able to infer average precision more accurately than others. Furthermore, we analyse a number of mixture distributions with regard to the recall-fallout convexity hypothesis and show that the convexity hypothesis is practically useful.Consequently, based on one of the best-performing score-distribution models, we develop some techniques for query-performance prediction (QPP) by automatically estimating the parameters of the document score-distribution model when relevance information is unknown. We present experimental results that outline the benefits of this approach to query-performance prediction.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {2},
numpages = {28},
keywords = {Score distributions, query performance}
}

@article{10.1145/2559169,
author = {Ge, Yong and Xiong, Hui and Tuzhilin, Alexander and Liu, Qi},
title = {Cost-Aware Collaborative Filtering for Travel Tour Recommendations},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2559169},
doi = {10.1145/2559169},
abstract = {Advances in tourism economics have enabled us to collect massive amounts of travel tour data. If properly analyzed, this data could be a source of rich intelligence for providing real-time decision making and for the provision of travel tour recommendations. However, tour recommendation is quite different from traditional recommendations, because the tourist’s choice is affected directly by the travel costs, which includes both financial and time costs. To that end, in this article, we provide a focused study of cost-aware tour recommendation. Along this line, we first propose two ways to represent user cost preference. One way is to represent user cost preference by a two-dimensional vector. Another way is to consider the uncertainty about the cost that a user can afford and introduce a Gaussian prior to model user cost preference. With these two ways of representing user cost preference, we develop different cost-aware latent factor models by incorporating the cost information into the probabilistic matrix factorization (PMF) model, the logistic probabilistic matrix factorization (LPMF) model, and the maximum margin matrix factorization (MMMF) model, respectively. When applied to real-world travel tour data, all the cost-aware recommendation models consistently outperform existing latent factor models with a significant margin.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {4},
numpages = {31},
keywords = {tour recommendation, Cost-aware collaborative filtering}
}

@article{10.1145/2559168,
author = {Huston, Samuel and Culpepper, J. Shane and Croft, W. Bruce},
title = {Indexing Word Sequences for Ranked Retrieval},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2559168},
doi = {10.1145/2559168},
abstract = {Formulating and processing phrases and other term dependencies to improve query effectiveness is an important problem in information retrieval. However, accessing word-sequence statistics using inverted indexes requires unreasonable processing time or substantial space overhead. Establishing a balance between these competing space and time trade-offs can dramatically improve system performance.In this article, we present and analyze a new index structure designed to improve query efficiency in dependency retrieval models. By adapting a class of (ε, δ)-approximation algorithms originally proposed for sketch summarization in networking applications, we show how to accurately estimate statistics important in term-dependency models with low, probabilistically bounded error rates. The space requirements for the vocabulary of the index is only logarithmically linked to the size of the vocabulary.Empirically, we show that the sketch index can reduce the space requirements of the vocabulary component of an index of n-grams consisting of between 1 and 4 words extracted from the GOV2 collection to less than 0.01% of the space requirements of the vocabulary of a full index. We also show that larger n-gram queries can be processed considerably more efficiently than in current alternatives, such as positional and next-word indexes.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {3},
numpages = {26},
keywords = {term-dependency models, Sketching, scalability, indexing}
}

@article{10.1145/2559157,
author = {Nie, Liqiang and Zhao, Yi-Liang and Wang, Xiangyu and Shen, Jialie and Chua, Tat-Seng},
title = {Learning to Recommend Descriptive Tags for Questions in Social Forums},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2559157},
doi = {10.1145/2559157},
abstract = {Around 40% of the questions in the emerging social-oriented question answering forums have at most one manually labeled tag, which is caused by incomprehensive question understanding or informal tagging behaviors. The incompleteness of question tags severely hinders all the tag-based manipulations, such as feeds for topic-followers, ontological knowledge organization, and other basic statistics. This article presents a novel scheme that is able to comprehensively learn descriptive tags for each question. Extensive evaluations on a representative real-world dataset demonstrate that our scheme yields significant gains for question annotation, and more importantly, the whole process of our approach is unsupervised and can be extended to handle large-scale data.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {5},
numpages = {23},
keywords = {knowledge organization, social QA, Question annotation}
}

@article{10.1145/2518175,
author = {Nong, Ge and Chan, Wai Hong and Zhang, Sen and Guan, Xiao Feng},
title = {Suffix Array Construction in External Memory Using D-Critical Substrings},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2518175},
doi = {10.1145/2518175},
abstract = {We present a new suffix array construction algorithm that aims to build, in external memory, the suffix array for an input string of length n measured in the magnitude of tens of Giga characters over a constant or integer alphabet. The core of this algorithm is adapted from the framework of the original internal memory SA-DS algorithm that samples fixed-size d-critical substrings. This new external-memory algorithm, called EM-SA-DS, uses novel cache data structures to construct a suffix array in a sequential scanning manner with good data spatial locality: data is read from or written to disk sequentially. On the assumed external-memory model with RAM capacity Ω((nB)0.5), disk capacity O(n), and size of each I/O block B, all measured in log n-bit words, the I/O complexity of EM-SA-DS is O(n/B). This work provides a general cache-based solution that could be further exploited to develop external-memory solutions for other suffix-array-related problems, for example, computing the longest-common-prefix array, using a modern personal computer with a typical memory configuration of 4GB RAM and a single disk.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {1},
numpages = {15},
keywords = {external memory, sorting algorithm, Suffix array}
}

@article{10.1145/2536736.2536738,
author = {Paik, Jiaul H. and Parui, Swapan K. and Pal, Dipasree and Robertson, Stephen E.},
title = {Effective and Robust Query-Based Stemming},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2536736.2536738},
doi = {10.1145/2536736.2536738},
abstract = {Stemming is a widely used technique in information retrieval systems to address the vocabulary mismatch problem arising out of morphological phenomena. The major shortcoming of the commonly used stemmers is that they accept the morphological variants of the query words without considering their thematic coherence with the given query, which leads to poor performance. Moreover, for many queries, such approaches also produce retrieval performance that is poorer than no stemming, thereby degrading the robustness. The main goal of this article is to present corpus-based fully automatic stemming algorithms which address these issues. A set of experiments on six TREC collections and three other non-English collections containing news and web documents shows that the proposed query-based stemming algorithms consistently and significantly outperform four state of the art strong stemmers of completely varying principles. Our experiments also confirm that the robustness of the proposed query-based stemming algorithms are remarkably better than the existing strong baselines.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {18},
numpages = {29},
keywords = {stemming, Corpus, suffix}
}

@article{10.1145/2536736.2536737,
author = {Hofmann, Katja and Whiteson, Shimon and Rijke, Maarten De},
title = {Fidelity, Soundness, and Efficiency of Interleaved Comparison Methods},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2536736.2536737},
doi = {10.1145/2536736.2536737},
abstract = {Ranker evaluation is central to the research into search engines, be it to compare rankers or to provide feedback for learning to rank. Traditional evaluation approaches do not scale well because they require explicit relevance judgments of document-query pairs, which are expensive to obtain. A promising alternative is the use of interleaved comparison methods, which compare rankers using click data obtained when interleaving their rankings.In this article, we propose a framework for analyzing interleaved comparison methods. An interleaved comparison method has fidelity if the expected outcome of ranker comparisons properly corresponds to the true relevance of the ranked documents. It is sound if its estimates of that expected outcome are unbiased and consistent. It is efficient if those estimates are accurate with only little data.We analyze existing interleaved comparison methods and find that, while sound, none meet our criteria for fidelity. We propose a probabilistic interleave method, which is sound and has fidelity. We show empirically that, by marginalizing out variables that are known, it is more efficient than existing interleaved comparison methods. Using importance sampling we derive a sound extension that is able to reuse historical data collected in previous comparisons of other ranker pairs.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {17},
numpages = {43},
keywords = {Information retrieval, online evaluation, importance sampling, interleaved comparison, interleaving, clicks}
}

@article{10.1145/2516891,
author = {Chelmis, Charalampos and Prasanna, Viktor K.},
title = {Social Link Prediction in Online Social Tagging Systems},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2516891},
doi = {10.1145/2516891},
abstract = {Social networks have become a popular medium for people to communicate and distribute ideas, content, news, and advertisements. Social content annotation has naturally emerged as a method of categorization and filtering of online information. The unrestricted vocabulary users choose from to annotate content has often lead to an explosion of the size of space in which search is performed. In this article, we propose latent topic models as a principled way of reducing the dimensionality of such data and capturing the dynamics of collaborative annotation process. We propose three generative processes to model latent user tastes with respect to resources they annotate with metadata. We show that latent user interests combined with social clues from the immediate neighborhood of users can significantly improve social link prediction in the online music social media site Last.fm. Most link prediction methods suffer from the high class imbalance problem, resulting in low precision and/or recall. In contrast, our proposed classification schemes for social link recommendation achieve high precision and recall with respect to not only the dominant class (nonexistence of a link), but also with respect to sparse positive instances, which are the most vital in social tie prediction.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {20},
numpages = {27},
keywords = {social bookmarking, topic models, social media, Last.fm, machine learning, Annotation, graphical models, collaborative tagging, link recommendation, unsupervised learning, link prediction}
}

@article{10.1145/2516890,
author = {Chiu, Chih-Yi and Tsai, Tsung-Han and Han, Guei-Wun and Hsieh, Cheng-Yu and Li, Sheng-Yang},
title = {Efficient Video Stream Monitoring for Near-Duplicate Detection and Localization in a Large-Scale Repository},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2516890},
doi = {10.1145/2516890},
abstract = {In this article, we study the efficiency problem of video stream near-duplicate monitoring in a large-scale repository. Existing stream monitoring methods are mainly designed for a short video to scan over a query stream; they have difficulty being scalable for a large number of long videos. We present a simple but effective algorithm called incremental similarity update to address the problem. That is, a similarity upper bound between two videos can be calculated incrementally by leveraging the prior knowledge of the previous calculation. The similarity upper bound takes a lightweight computation to filter out unnecessary time-consuming computation for the actual similarity between two videos, making the search process more efficient. We integrate the algorithm with inverted indexing to obtain a candidate list from the repository for the given query stream. Meanwhile, the algorithm is applied to scan each candidate for locating exact near-duplicate subsequences. We implement several state-of-the-art methods for comparison in terms of accuracy, execution time, and memory consumption. Experimental results demonstrate the proposed algorithm yields comparable accuracy, compact memory size, and more efficient execution time.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {22},
numpages = {27},
keywords = {inverted indexing, content-based retrieval, Near-duplicate, video copy}
}

@article{10.1145/2516889,
author = {Esuli, Andrea and Sebastiani, Fabrizio},
title = {Improving Text Classification Accuracy by Training Label Cleaning},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2516889},
doi = {10.1145/2516889},
abstract = {In text classification (TC) and other tasks involving supervised learning, labelled data may be scarce or expensive to obtain. Semisupervised learning and active learning are two strategies whose aim is maximizing the effectiveness of the resulting classifiers for a given amount of training effort. Both strategies have been actively investigated for TC in recent years. Much less research has been devoted to a third such strategy, training label cleaning (TLC), which consists in devising ranking functions that sort the original training examples in terms of how likely it is that the human annotator has mislabelled them. This provides a convenient means for the human annotator to revise the training set so as to improve its quality. Working in the context of boosting-based learning methods for multilabel classification we present three different techniques for performing TLC and, on three widely used TC benchmarks, evaluate them by their capability of spotting training documents that, for experimental reasons only, we have purposefully mislabelled. We also evaluate the degradation in classification effectiveness that these mislabelled texts bring about, and to what extent training label cleaning can prevent this degradation.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {19},
numpages = {28},
keywords = {supervised learning, training label cleaning, synthetic noise, training label noise, Text classification}
}

@article{10.1145/2500751,
author = {Jia, Lifeng and Yu, Clement and Meng, Weiyi},
title = {The Impacts of Structural Difference and Temporality of Tweets on Retrieval Effectiveness},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2500751},
doi = {10.1145/2500751},
abstract = {To explore the information seeking behaviors in microblogosphere, the microblog track at TREC 2011 introduced a real-time ad-hoc retrieval task that aims at ranking relevant tweets in reverse-chronological order. We study this problem via a two-phase approach: 1) retrieving tweets in an ad-hoc way; 2) utilizing the temporal information of tweets to enhance the retrieval effectiveness of tweets. Tweets can be categorized into two types. One type consists of short messages not containing any URL of a Web page. The other type has at least one URL of a Web page in addition to a short message. These two types of tweets have different structures. In the first phase, to address the structural difference of tweets, we propose a method to rank tweets using the divide-and-conquer strategy. Specifically, we first rank the two types of tweets separately. This produces two rankings, one for each type. Then we merge these two rankings of tweets into one ranking. In the second phase, we first categorize queries into several types by exploring the temporal distributions of their top-retrieved tweets from the first phase; then we calculate the time-related relevance scores of tweets according to the classified types of queries; finally we combine the time scores with the IR scores from the first phase to produce a ranking of tweets. Experimental results achieved by using the TREC 2011 and TREC 2012 queries over the TREC Tweets2011 collection show that: (i) our way of ranking the two types of tweets separately and then merging them together yields better retrieval effectiveness than ranking them simultaneously; (ii) our way of incorporating temporal information into the retrieval process yields further improvements, and (iii) our method compares favorably with state-of-the-art methods in retrieval effectiveness.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {21},
numpages = {38},
keywords = {query temporal categorization, learning to rank, Ad-hoc retrieval of tweets}
}

@article{10.1145/2493175.2493181,
author = {Radinsky, Kira and Svore, Krysta M. and Dumais, Susan T. and Shokouhi, Milad and Teevan, Jaime and Bocharov, Alex and Horvitz, Eric},
title = {Behavioral Dynamics on the Web: Learning, Modeling, and Prediction},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2493175.2493181},
doi = {10.1145/2493175.2493181},
abstract = {The queries people issue to a search engine and the results clicked following a query change over time. For example, after the earthquake in Japan in March 2011, the query japan spiked in popularity and people issuing the query were more likely to click government-related results than they would prior to the earthquake. We explore the modeling and prediction of such temporal patterns in Web search behavior. We develop a temporal modeling framework adapted from physics and signal processing and harness it to predict temporal patterns in search behavior using smoothing, trends, periodicities, and surprises. Using current and past behavioral data, we develop a learning procedure that can be used to construct models of users' Web search activities. We also develop a novel methodology that learns to select the best prediction model from a family of predictive models for a given query or a class of queries. Experimental results indicate that the predictive models significantly outperform baseline models that weight historical evidence the same for all queries. We present two applications where new methods introduced for the temporal modeling of user behavior significantly improve upon the state of the art. Finally, we discuss opportunities for using models of temporal dynamics to enhance other areas of Web search and information retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {16},
numpages = {37},
keywords = {Behavioral analysis, predictive behavioral models}
}

@article{10.1145/2493175.2493180,
author = {Nong, Ge},
title = {Practical Linear-Time <i>O</i>(1)-Workspace Suffix Sorting for Constant Alphabets},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2493175.2493180},
doi = {10.1145/2493175.2493180},
abstract = {This article presents an O(n)-time algorithm called SACA-K for sorting the suffixes of an input string T[0, n-1] over an alphabet A[0, K-1]. The problem of sorting the suffixes of T is also known as constructing the suffix array (SA) for T. The theoretical memory usage of SACA-K is n log K + n log n + K log n bits. Moreover, we also have a practical implementation for SACA-K that uses n bytes + (n + 256) words and is suitable for strings over any alphabet up to full ASCII, where a word is log n bits. In our experiment, SACA-K outperforms SA-IS that was previously the most time- and space-efficient linear-time SA construction algorithm (SACA). SACA-K is around 33% faster and uses a smaller deterministic workspace of K words, where the workspace is the space needed beyond the input string and the output SA. Given K=O(1), SACA-K runs in linear time and O(1) workspace. To the best of our knowledge, such a result is the first reported in the literature with a practical source code publicly available.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {15},
numpages = {15},
keywords = {Suffix array, sorting algorithm, O(1)-workspace, linear time}
}

@article{10.1145/2493175.2493179,
author = {Lucchese, Claudio and Orlando, Salvatore and Perego, Raffaele and Silvestri, Fabrizio and Tolomei, Gabriele},
title = {Discovering Tasks from Search Engine Query Logs},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2493175.2493179},
doi = {10.1145/2493175.2493179},
abstract = {Although Web search engines still answer user queries with lists of ten blue links to webpages, people are increasingly issuing queries to accomplish their daily tasks (e.g., finding a recipe, booking a flight, reading online news, etc.). In this work, we propose a two-step methodology for discovering tasks that users try to perform through search engines. First, we identify user tasks from individual user sessions stored in search engine query logs. In our vision, a user task is a set of possibly noncontiguous queries (within a user search session), which refer to the same need. Second, we discover collective tasks by aggregating similar user tasks, possibly performed by distinct users. To discover user tasks, we propose query similarity functions based on unsupervised and supervised learning approaches. We present a set of query clustering methods that exploit these functions in order to detect user tasks. All the proposed solutions were evaluated on a manually-built ground truth, and two of them performed better than state-of-the-art approaches. To detect collective tasks, we propose four methods that cluster previously discovered user tasks, which in turn are represented by the bag-of-words extracted from their composing queries. These solutions were also evaluated on another manually-built ground truth.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {14},
numpages = {43},
keywords = {user search intent, user search session boundaries, collective task discovery, user task discovery, query clustering, user tasks, collective tasks, Query log analysis}
}

@article{10.1145/2493175.2493178,
author = {Asadi, Nima and Lin, Jimmy},
title = {Fast Candidate Generation for Real-Time Tweet Search with Bloom Filter Chains},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2493175.2493178},
doi = {10.1145/2493175.2493178},
abstract = {The rise of social media and other forms of user-generated content have created the demand for real-time search: against a high-velocity stream of incoming documents, users desire a list of relevant results at the time the query is issued. In the context of real-time search on tweets, this work explores candidate generation in a two-stage retrieval architecture where an initial list of results is processed by a second-stage rescorer to produce the final output. We introduce Bloom filter chains, a novel extension of Bloom filters that can dynamically expand to efficiently represent an arbitrarily long and growing list of monotonically-increasing integers with a constant false positive rate. Using a collection of Bloom filter chains, a novel approximate candidate generation algorithm called BWand is able to perform both conjunctive and disjunctive retrieval. Experiments show that our algorithm is many times faster than competitive baselines and that this increased performance does not require sacrificing end-to-end effectiveness. Our results empirically characterize the trade-off space defined by output quality, query evaluation speed, and memory footprint for this particular search architecture.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {13},
numpages = {36},
keywords = {bloom filters, efficiency, top-k retrieval, tweet search, Scalability}
}

@article{10.1145/2493175.2493177,
author = {Hou, Yuexian and Zhao, Xiaozhao and Song, Dawei and Li, Wenjie},
title = {Mining Pure High-Order Word Associations via Information Geometry for Information Retrieval},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2493175.2493177},
doi = {10.1145/2493175.2493177},
abstract = {The classical bag-of-word models for information retrieval (IR) fail to capture contextual associations between words. In this article, we propose to investigate pure high-order dependence among a number of words forming an unseparable semantic entity, that is, the high-order dependence that cannot be reduced to the random coincidence of lower-order dependencies. We believe that identifying these pure high-order dependence patterns would lead to a better representation of documents and novel retrieval models. Specifically, two formal definitions of pure dependence—unconditional pure dependence (UPD) and conditional pure dependence (CPD)—are defined. The exact decision on UPD and CPD, however, is NP-hard in general. We hence derive and prove the sufficient criteria that entail UPD and CPD, within the well-principled information geometry (IG) framework, leading to a more feasible UPD/CPD identification procedure. We further develop novel methods for extracting word patterns with pure high-order dependence. Our methods are applied to and extensively evaluated on three typical IR tasks: text classification and text retrieval without and with query expansion.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {12},
numpages = {32},
keywords = {Word association, pure high-order dependence, text classification, text retrieval, information geometry}
}

@article{10.1145/2493175.2493176,
author = {Macdonald, Craig and Santos, Rodrygo L.T. and Ounis, Iadh and He, Ben},
title = {About Learning Models with Multiple Query-Dependent Features},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2493175.2493176},
doi = {10.1145/2493175.2493176},
abstract = {Several questions remain unanswered by the existing literature concerning the deployment of query-dependent features within learning to rank. In this work, we investigate three research questions in order to empirically ascertain best practices for learning-to-rank deployments. (i) Previous work in data fusion that pre-dates learning to rank showed that while different retrieval systems could be effectively combined, the combination of multiple models within the same system was not as effective. In contrast, the existing learning-to-rank datasets (e.g., LETOR), often deploy multiple weighting models as query-dependent features within a single system, raising the question as to whether such a combination is needed. (ii) Next, we investigate whether the training of weighting model parameters, traditionally required for effective retrieval, is necessary within a learning-to-rank context. (iii) Finally, we note that existing learning-to-rank datasets use weighting model features calculated on different fields (e.g., title, content, or anchor text), even though such weighting models have been criticized in the literature. Experiments addressing these three questions are conducted on Web search datasets, using various weighting models as query-dependent and typical query-independent features, which are combined using three learning-to-rank techniques. In particular, we show and explain why multiple weighting models should be deployed as features. Moreover, we unexpectedly find that training the weighting model's parameters degrades learned model's effectiveness. Finally, we show that computing a weighting model separately for each field is less effective than more theoretically-sound field-based weighting models.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {11},
numpages = {39},
keywords = {field-based weighting models, Learning to rank, samples}
}

@article{10.1145/2457465.2457470,
author = {Bast, Hannah and Celikik, Marjan},
title = {Efficient Fuzzy Search in Large Text Collections},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2457465.2457470},
doi = {10.1145/2457465.2457470},
abstract = {We consider the problem of fuzzy full-text search in large text collections, that is, full-text search which is robust against errors both on the side of the query as well as on the side of the documents. Standard inverted-index techniques work extremely well for ordinary full-text search but fail to achieve interactive query times (below 100 milliseconds) for fuzzy full-text search even on moderately-sized text collections (above 10 GBs of text). We present new preprocessing techniques that achieve interactive query times on large text collections (100 GB of text, served by a single machine). We consider two similarity measures, one where the query terms match similar terms in the collection (e.g., algorithm matches algoritm or vice versa) and one where the query terms match terms with a similar prefix in the collection (e.g., alori matches algorithm). The latter is important when we want to display results instantly after each keystroke (search as you type). All algorithms have been fully integrated into the CompleteSearch engine.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {10},
numpages = {59},
keywords = {inverted index, fuzzy search, error tolerant autocomplete, HYB index, approximate text search, Approximate dictionary search}
}

@article{10.1145/2457465.2457469,
author = {Zhu, Xiaofeng and Huang, Zi and Cheng, Hong and Cui, Jiangtao and Shen, Heng Tao},
title = {Sparse Hashing for Fast Multimedia Search},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2457465.2457469},
doi = {10.1145/2457465.2457469},
abstract = {Hash-based methods achieve fast similarity search by representing high-dimensional data with compact binary codes. However, both generating binary codes and encoding unseen data effectively and efficiently remain very challenging tasks. In this article, we focus on these tasks to implement approximate similarity search by proposing a novel hash based method named sparse hashing (SH for short). To generate interpretable (or semantically meaningful) binary codes, the proposed SH first converts original data into low-dimensional data through a novel nonnegative sparse coding method. SH then converts the low-dimensional data into Hamming space (i.e., binary encoding low-dimensional data) by a new binarization rule. After this, training data are represented by generated binary codes. To efficiently and effectively encode unseen data, SH learns hash functions by taking a-priori knowledge into account, such as implicit group effect of the features in training data, and the correlations between original space and the learned Hamming space. SH is able to perform fast approximate similarity search by efficient bit XOR operations in the memory of a modern PC with short binary code representations. Experimental results show that the proposed SH significantly outperforms state-of-the-art techniques.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {9},
numpages = {24},
keywords = {Hashing, multimedia search, sparse coding, indexing}
}

@article{10.1145/2457465.2457468,
author = {Ke, Weimao and Mostafa, Javed},
title = {Studying the Clustering Paradox and Scalability of Search in Highly Distributed Environments},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2457465.2457468},
doi = {10.1145/2457465.2457468},
abstract = {With the ubiquitous production, distribution and consumption of information, today's digital environments such as the Web are increasingly large and decentralized. It is hardly possible to obtain central control over information collections and systems in these environments. Searching for information in these information spaces has brought about problems beyond traditional boundaries of information retrieval (IR) research. This article addresses one important aspect of scalability challenges facing information retrieval models and investigates a decentralized, organic view of information systems pertaining to search in large-scale networks. Drawing on observations from earlier studies, we conduct a series of experiments on decentralized searches in large-scale networked information spaces. Results show that how distributed systems interconnect is crucial to retrieval performance and scalability of searching. Particularly, in various experimental settings and retrieval tasks, we find a consistent phenomenon, namely, the Clustering Paradox, in which the level of network clustering (semantic overlay) imposes a scalability limit. Scalable searches are well supported by a specific, balanced level of network clustering emerging from local system interconnectivity. Departure from that level, either stronger or weaker clustering, leads to search performance degradation, which is dramatic in large-scale networks.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {8},
numpages = {36},
keywords = {information retrieval, efficiency, decentralized search, self-organization, scalability, Information network, loose coupling, network clustering, large-scale distributed system}
}

@article{10.1145/2457465.2457467,
author = {Pan, Sinno Jialin and Toh, Zhiqiang and Su, Jian},
title = {Transfer Joint Embedding for Cross-Domain Named Entity Recognition},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2457465.2457467},
doi = {10.1145/2457465.2457467},
abstract = {Named Entity Recognition (NER) is a fundamental task in information extraction from unstructured text. Most previous machine-learning-based NER systems are domain-specific, which implies that they may only perform well on some specific domains (e.g., Newswire) but tend to adapt poorly to other related but different domains (e.g., Weblog). Recently, transfer learning techniques have been proposed to NER. However, most transfer learning approaches to NER are developed for binary classification, while NER is a multiclass classification problem in nature. Therefore, one has to first reduce the NER task to multiple binary classification tasks and solve them independently. In this article, we propose a new transfer learning method, named Transfer Joint Embedding (TJE), for cross-domain multiclass classification, which can fully exploit the relationships between classes (labels), and reduce domain difference in data distributions for transfer learning. More specifically, we aim to embed both labels (outputs) and high-dimensional features (inputs) from different domains (e.g., a source domain and a target domain) into a unified low-dimensional latent space, where 1) each label is represented by a prototype and the intrinsic relationships between labels can be measured by Euclidean distance; 2) the distance in data distributions between the source and target domains can be reduced; 3) the source domain labeled data are closer to their corresponding label-prototypes than others. After the latent space is learned, classification on the target domain data can be done with the simple nearest neighbor rule in the latent space. Furthermore, in order to scale up TJE, we propose an efficient algorithm based on stochastic gradient descent (SGD). Finally, we apply the proposed TJE method for NER across different domains on the ACE 2005 dataset, which is a benchmark in Natural Language Processing (NLP). Experimental results demonstrate the effectiveness of TJE and show that TJE can outperform state-of-the-art transfer learning approaches to NER.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {7},
numpages = {27},
keywords = {Named entity recognition, multiclass classification, transfer learning}
}

@article{10.1145/2457465.2457466,
author = {Xue, Xiaobing and Croft, W. Bruce},
title = {Modeling Reformulation Using Query Distributions},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2457465.2457466},
doi = {10.1145/2457465.2457466},
abstract = {Query reformulation modifies the original query with the aim of better matching the vocabulary of the relevant documents, and consequently improving ranking effectiveness. Previous models typically generate words and phrases related to the original query, but do not consider how these words and phrases would fit together in actual queries. In this article, a novel framework is proposed that models reformulation as a distribution of actual queries, where each query is a variation of the original query. This approach considers an actual query as the basic unit and thus captures important query-level dependencies between words and phrases. An implementation of this framework that only uses publicly available resources is proposed, which makes fair comparisons with other methods using TREC collections possible. Specifically, this implementation consists of a query generation step that analyzes the passages containing query words to generate reformulated queries and a probability estimation step that learns a distribution for reformulated queries by optimizing the retrieval performance. Experiments on TREC collections show that the proposed model can significantly outperform previous reformulation models.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {6},
numpages = {34},
keywords = {query substitution, query segmentation, passage analysis, information retrieval, Query reformulation}
}

@article{10.1145/2414782.2414787,
author = {Wang, Quan and Xu, Jun and Li, Hang and Craswell, Nick},
title = {Regularized Latent Semantic Indexing: A New Approach to Large-Scale Topic Modeling},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2414782.2414787},
doi = {10.1145/2414782.2414787},
abstract = {Topic modeling provides a powerful way to analyze the content of a collection of documents. It has become a popular tool in many research areas, such as text mining, information retrieval, natural language processing, and other related fields. In real-world applications, however, the usefulness of topic modeling is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps, such as vastly reducing input vocabulary. In this article we introduce Regularized Latent Semantic Indexing (RLSI)---including a batch version and an online version, referred to as batch RLSI and online RLSI, respectively---to scale up topic modeling. Batch RLSI and online RLSI are as effective as existing topic modeling techniques and can scale to larger datasets without reducing input vocabulary. Moreover, online RLSI can be applied to stream data and can capture the dynamic evolution of topics. Both versions of RLSI formalize topic modeling as a problem of minimizing a quadratic loss function regularized by ℓ1 and/or ℓ2 norm. This formulation allows the learning process to be decomposed into multiple suboptimization problems which can be optimized in parallel, for example, via MapReduce. We particularly propose adopting ℓ1 norm on topics and ℓ2 norm on document representations to create a model with compact and readable topics and which is useful for retrieval. In learning, batch RLSI processes all the documents in the collection as a whole, while online RLSI processes the documents in the collection one by one. We also prove the convergence of the learning of online RLSI. Relevance ranking experiments on three TREC datasets show that batch RLSI and online RLSI perform better than LSI, PLSI, LDA, and NMF, and the improvements are sometimes statistically significant. Experiments on a Web dataset containing about 1.6 million documents and 7 million terms, demonstrate a similar boost in performance.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {5},
numpages = {44},
keywords = {sparse methods, Topic modeling, online learning, distributed learning, regularization}
}

@article{10.1145/2414782.2414786,
author = {Yom-Tov, Elad and Diaz, Fernando},
title = {The Effect of Social and Physical Detachment on Information Need},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2414782.2414786},
doi = {10.1145/2414782.2414786},
abstract = {The information need of users and the documents which answer this need are frequently contingent on the different characteristics of users. This is especially evident during natural disasters, such as earthquakes and violent weather incidents, which create a strong transient information need. In this article, we investigate how the information need of users, as expressed by their queries, is affected by their physical detachment, as estimated by their physical location in relation to that of the event, and by their social detachment, as quantified by the number of their acquaintances who may be affected by the event. Drawing on large-scale data from ten major events, we show that social and physical detachment levels of users are a major influence on their search engine queries. We demonstrate how knowing social and physical detachment levels can assist in improving retrieval for two applications: identifying search queries related to events and ranking results in response to event-related queries. We find that the average precision in identifying relevant search queries improves by approximately 18%, and that the average precision of ranking that uses detachment information improves by 10%. Using both types of detachment achieved a larger gain in performance than each of them separately.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {4},
numpages = {19},
keywords = {need, physical, distance, information, Social}
}

@article{10.1145/2414782.2414785,
author = {Costa, Gianni and Ortale, Riccardo and Ritacco, Ettore},
title = {X-Class: Associative Classification of XML Documents by Structure},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2414782.2414785},
doi = {10.1145/2414782.2414785},
abstract = {The supervised classification of XML documents by structure involves learning predictive models in which certain structural regularities discriminate the individual document classes. Hitherto, research has focused on the adoption of prespecified substructures. This is detrimental for classification effectiveness, since the a priori chosen substructures may not accord with the structural properties of the XML documents. Therein, an unexplored question is how to choose the type of structural regularity that best adapts to the structures of the available XML documents.We tackle this problem through X-Class, an approach that handles all types of tree-like substructures and allows for choosing the most discriminatory one. Algorithms are designed to learn compact rule-based classifiers in which the chosen substructures discriminate the classes of XML documents.X-Class is studied across various domains and types of substructures. Its classification performance is compared against several rule-based and SVM-based competitors. Empirical evidence reveals that the classifiers induced by X-Class are compact, scalable, and at least as effective as the established competitors. In particular, certain substructures allow the induction of very compact classifiers that generally outperform the rule-based competitors in terms of effectiveness over all chosen corpora of XML data. Furthermore, such classifiers are substantially as effective as the SVM-based competitor, with the additional advantage of a high-degree of interpretability.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {3},
numpages = {40},
keywords = {Structural XML classification, XML mining, XML transactional modeling}
}

@article{10.1145/2414782.2414784,
author = {Webber, William},
title = {Approximate Recall Confidence Intervals},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2414782.2414784},
doi = {10.1145/2414782.2414784},
abstract = {Recall, the proportion of relevant documents retrieved, is an important measure of effectiveness in information retrieval, particularly in the legal, patent, and medical domains. Where document sets are too large for exhaustive relevance assessment, recall can be estimated by assessing a random sample of documents, but an indication of the reliability of this estimate is also required. In this article, we examine several methods for estimating two-tailed recall confidence intervals. We find that the normal approximation in current use provides poor coverage in many circumstances, even when adjusted to correct its inappropriate symmetry. Analytic and Bayesian methods based on the ratio of binomials are generally more accurate but are inaccurate on small populations. The method we recommend derives beta-binomial posteriors on retrieved and unretrieved yield, with fixed hyperparameters, and a Monte Carlo estimate of the posterior distribution of recall. We demonstrate that this method gives mean coverage at or near the nominal level, across several scenarios, while being balanced and stable. We offer advice on sampling design, including the allocation of assessments to the retrieved and unretrieved segments, and compare the proposed beta-binomial with the officially reported normal intervals for recent TREC Legal Track iterations.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {2},
numpages = {33},
keywords = {probabilistic models, Posterior distributions}
}

@article{10.1145/2414782.2414783,
author = {Kim, Jinhan and Lee, Sanghoon and Hwang, Seung-Won and Kim, Sunghun},
title = {Enriching Documents with Examples: A Corpus Mining Approach},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2414782.2414783},
doi = {10.1145/2414782.2414783},
abstract = {Software developers increasingly rely on information from the Web, such as documents or code examples on application programming interfaces (APIs), to facilitate their development processes. However, API documents often do not include enough information for developers to fully understand how to use the APIs, and searching for good code examples requires considerable effort.To address this problem, we propose a novel code example recommendation system that combines the strength of browsing documents and searching for code examples and returns API documents embedded with high-quality code example summaries mined from the Web. Our evaluation results show that our approach provides code examples with high precision and boosts programmer productivity.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {1},
numpages = {27},
keywords = {ranking, Clustering, code search, API document}
}

@article{10.1145/2382438.2382446,
author = {Callan, Jamie},
title = {TOIS Reviewers: October 2009 To September 2012},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2382438.2382446},
doi = {10.1145/2382438.2382446},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {27},
numpages = {3}
}

@article{10.1145/2382438.2382445,
author = {Gerani, Shima and Carman, Mark and Crestani, Fabio},
title = {Aggregation Methods for Proximity-Based Opinion Retrieval},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2382438.2382445},
doi = {10.1145/2382438.2382445},
abstract = {The enormous amount of user-generated data available on the Web provides a great opportunity to understand, analyze, and exploit people’s opinions on different topics. Traditional Information Retrieval methods consider the relevance of documents to a topic but are unable to differentiate between subjective and objective documents. Opinion retrieval is a retrieval task in which not only the relevance of a document to the topic is important but also the amount of opinion expressed in the document about the topic. In this article, we address the blog post opinion retrieval task and propose methods that rank blog posts according to their relevance and opinionatedness toward a topic. We propose estimating the opinion density at each position in a document using a general opinion lexicon and kernel density functions. We propose and investigate different models for aggregating the opinion density at query terms positions to estimate the opinion score of every document. We then combine the opinion score with the relevance score based on a probabilistic justification. Experimental results on the BLOG06 dataset show that the proposed method provides significant improvement over the standard TREC baselines. The proposed models also achieve much higher performance compared to all state of the art methods.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {26},
numpages = {36},
keywords = {blog, proximity, retrieval, Opinion, sentiment}
}

@article{10.1145/2382438.2382444,
author = {You, Gae-Won and Hwang, Seung-Won and Song, Young-In and Jiang, Long and Nie, Zaiqing},
title = {Efficient Entity Translation Mining: A Parallelized Graph Alignment Approach},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2382438.2382444},
doi = {10.1145/2382438.2382444},
abstract = {This article studies the problem of mining entity translation, specifically, mining English and Chinese name pairs. Existing efforts can be categorized into (a) transliteration-based approaches that leverage phonetic similarity and (b) corpus-based approaches that exploit bilingual cooccurrences. These approaches suffer from inaccuracy and scarcity, respectively. In clear contrast, we use under-leveraged resources of monolingual entity cooccurrences crawled from entity search engines, which are represented as two entity-relationship graphs extracted from two language corpora, respectively. Our problem is then abstracted as finding correct mappings across two graphs. To achieve this goal, we propose a holistic approach to exploiting both transliteration similarity and monolingual cooccurrences. This approach, which builds upon monolingual corpora, complements existing corpus-based work requiring scarce resources of parallel or comparable corpus while significantly boosting the accuracy of transliteration-based work. In addition, by parallelizing the mapping process on multicore architectures, we speed up the computation by more than 10 times per unit accuracy. We validated the effectiveness and efficiency of our proposed approach using real-life datasets.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {25},
numpages = {23},
keywords = {translation, parallelization, graph alignment, Entity mining}
}

@article{10.1145/2382438.2382443,
author = {Fu, Tianjun and Abbasi, Ahmed and Zeng, Daniel and Chen, Hsinchun},
title = {Sentimental Spidering: Leveraging Opinion Information in Focused Crawlers},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2382438.2382443},
doi = {10.1145/2382438.2382443},
abstract = {Despite the increased prevalence of sentiment-related information on the Web, there has been limited work on focused crawlers capable of effectively collecting not only topic-relevant but also sentiment-relevant content. In this article, we propose a novel focused crawler that incorporates topic and sentiment information as well as a graph-based tunneling mechanism for enhanced collection of opinion-rich Web content regarding a particular topic. The graph-based sentiment (GBS) crawler uses a text classifier that employs both topic and sentiment categorization modules to assess the relevance of candidate pages. This information is also used to label nodes in web graphs that are employed by the tunneling mechanism to improve collection recall. Experimental results on two test beds revealed that GBS was able to provide better precision and recall than seven comparison crawlers. Moreover, GBS was able to collect a large proportion of the relevant content after traversing far fewer pages than comparison methods. GBS outperformed comparison methods on various categories of Web pages in the test beds, including collection of blogs, Web forums, and social networking Web site content. Further analysis revealed that both the sentiment classification module and graph-based tunneling mechanism played an integral role in the overall effectiveness of the GBS crawler.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {24},
numpages = {30},
keywords = {sentiment analysis, random walk path, focused crawlers, graph similarities, classification, opinion mining, Web crawlers}
}

@article{10.1145/2382438.2382442,
author = {Adomavicius, Gediminas and Zhang, Jingjing},
title = {Stability of Recommendation Algorithms},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2382438.2382442},
doi = {10.1145/2382438.2382442},
abstract = {The article explores stability as a new measure of recommender systems performance. Stability is defined to measure the extent to which a recommendation algorithm provides predictions that are consistent with each other. Specifically, for a stable algorithm, adding some of the algorithm’s own predictions to the algorithm’s training data (for example, if these predictions were confirmed as accurate by users) would not invalidate or change the other predictions. While stability is an interesting theoretical property that can provide additional understanding about recommendation algorithms, we believe stability to be a desired practical property for recommender systems designers as well, because unstable recommendations can potentially decrease users’ trust in recommender systems and, as a result, reduce users’ acceptance of recommendations. In this article, we also provide an extensive empirical evaluation of stability for six popular recommendation algorithms on four real-world datasets. Our results suggest that stability performance of individual recommendation algorithms is consistent across a variety of datasets and settings. In particular, we find that model-based recommendation algorithms consistently demonstrate higher stability than neighborhood-based collaborative filtering techniques. In addition, we perform a comprehensive empirical analysis of many important factors (e.g., the sparsity of original rating data, normalization of input data, the number of new incoming ratings, the distribution of incoming ratings, the distribution of evaluation data, etc.) and report the impact they have on recommendation stability.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {23},
numpages = {31},
keywords = {collaborative filtering, Recommender systems, performance measures, evaluation of recommender systems, recommendation stability, recommendation accuracy}
}

@article{10.1145/2382438.2382441,
author = {Abbasi, Ahmed and Zahedi, Fatemeh “Mariam” and Kaza, Siddharth},
title = {Detecting Fake Medical Web Sites Using Recursive Trust Labeling},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2382438.2382441},
doi = {10.1145/2382438.2382441},
abstract = {Fake medical Web sites have become increasingly prevalent. Consequently, much of the health-related information and advice available online is inaccurate and/or misleading. Scores of medical institution Web sites are for organizations that do not exist and more than 90% of online pharmacy Web sites are fraudulent. In addition to monetary losses exacted on unsuspecting users, these fake medical Web sites have severe public safety ramifications. According to a World Health Organization report, approximately half the drugs sold on the Web are counterfeit, resulting in thousands of deaths. In this study, we propose an adaptive learning algorithm called recursive trust labeling (RTL). RTL uses underlying content and graph-based classifiers, coupled with a recursive labeling mechanism, for enhanced detection of fake medical Web sites. The proposed method was evaluated on a test bed encompassing nearly 100 million links between 930,000 Web sites, including 1,000 known legitimate and fake medical sites. The experimental results revealed that RTL was able to significantly improve fake medical Web site detection performance over 19 comparison content and graph-based methods, various meta-learning techniques, and existing adaptive learning approaches, with an overall accuracy of over 94%. Moreover, RTL was able to attain high performance levels even when the training dataset composed of as little as 30 Web sites. With the increased popularity of eHealth and Health 2.0, the results have important implications for online trust, security, and public safety.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {22},
numpages = {36},
keywords = {Web mining, Fake Web sites, machine learning, Health 2.0, medical fraud, Web spam}
}

@article{10.1145/2382438.2382440,
author = {Liu, Hongyan and He, Jun and Gu, Yingqin and Xiong, Hui and Du, Xiaoyong},
title = {Detecting and Tracking Topics and Events from Web Search Logs},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2382438.2382440},
doi = {10.1145/2382438.2382440},
abstract = {Recent years have witnessed increased efforts on detecting topics and events from Web search logs, since this kind of data not only capture web content but also reflect the users’ activities. However, the majority of existing work is focused on exploiting clustering techniques for topic and event detection. Due to the huge size and the evolving nature of Web data, existing clustering approaches are limited to meet the real-time demand. To that end, in this article, we propose a method called LETD to detect evolving topics in a timely manner. Also, we design the techniques to extract events from topics and to infer the evolving relationship among the events. For topic detection, we first provide a measurement to select the important URLs, which are most likely to describe a real-life topic. Then, starting from these selected URLs, we exploit the local expansion method to find other topic-related URLs. Moreover, in the LETD framework, we design algorithms based on Random Walk and Markov Random Fields (MRF), respectively. Because the LETD method exploits a divide-and-conquer strategy to process the data, it is more efficient than existing methods based on clustering techniques. To better illustrate the LETD framework, we develop a demo system StoryTeller which can discover hot topics and events, infer the evolving relationships among events, and visualize information in a storytelling way. This demo system can provide a global view of the topic development and help users target the interesting events more conveniently. Finally, experimental results on real-world Microsoft click-through data have shown that StoryTeller can find real-life hot topics and meaningful evolving relationships among events, and has also demonstrated the efficiency and effectiveness of the LETD method.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {21},
numpages = {29},
keywords = {Random Walk, Markov Random Fields, Web search log, Topic detection and tracking}
}

@article{10.1145/2382438.2382439,
author = {Moon, Taesup and Chu, Wei and Li, Lihong and Zheng, Zhaohui and Chang, Yi},
title = {An Online Learning Framework for Refining Recency Search Results with User Click Feedback},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2382438.2382439},
doi = {10.1145/2382438.2382439},
abstract = {Traditional machine-learned ranking systems for Web search are often trained to capture stationary relevance of documents to queries, which have limited ability to track nonstationary user intention in a timely manner. In recency search, for instance, the relevance of documents to a query on breaking news often changes significantly over time, requiring effective adaptation to user intention. In this article, we focus on recency search and study a number of algorithms to improve ranking results by leveraging user click feedback. Our contributions are threefold. First, we use commercial search engine sessions collected in a random exploration bucket for reliable offline evaluation of these algorithms, which provides an unbiased comparison across algorithms without online bucket tests. Second, we propose an online learning approach that reranks and improves the search results for recency queries near real-time based on user clicks. This approach is very general and can be combined with sophisticated click models. Third, our empirical comparison of a dozen algorithms on real-world search data suggests importance of a few algorithmic choices in these applications, including generalization across different query-document pairs, specialization to popular queries, and near real-time adaptation of user clicks for reranking.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {20},
numpages = {28}
}

@article{10.1145/2328967.2328972,
author = {Comas, Pere R. and Turmo, Jordi and M\`{a}rquez, Llu\'{\i}s},
title = {Sibyl, a Factoid Question-Answering System for Spoken Documents},
year = {2012},
issue_date = {August 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2328967.2328972},
doi = {10.1145/2328967.2328972},
abstract = {In this article, we present a factoid question-answering system, Sibyl, specifically tailored for question answering (QA) on spoken-word documents. This work explores, for the first time, which techniques can be robustly adapted from the usual QA on written documents to the more difficult spoken document scenario. More specifically, we study new information retrieval (IR) techniques designed or speech, and utilize several levels of linguistic information for the speech-based QA task. These include named-entity detection with phonetic information, syntactic parsing applied to speech transcripts, and the use of coreference resolution. Sibyl is largely based on supervised machine-learning techniques, with special focus on the answer extraction step, and makes little use of handcrafted knowledge. Consequently, it should be easily adaptable to other domains and languages. Sibyl and all its modules are extensively evaluated on the European Parliament Plenary Sessions English corpus, comparing manual with automatic transcripts obtained by three different automatic speech recognition (ASR) systems that exhibit significantly different word error rates. This data belongs to the CLEF 2009 track for QA on speech transcripts. The main results confirm that syntactic information is very useful for learning to rank question candidates, improving results on both manual and automatic transcripts, unless the ASR quality is very low. At the same time, our experiments on coreference resolution reveal that the state-of-the-art technology is not mature enough to be effectively exploited for QA with spoken documents. Overall, the performance of Sibyl is comparable or better than the state-of-the-art on this corpus, confirming the validity of our approach.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {19},
numpages = {40},
keywords = {Question answering, spoken document retrieval}
}

@article{10.1145/2328967.2328971,
author = {Tejedor, Javier and Fap\v{s}o, Michal and Sz\"{o}ke, Igor and \v{C}ernock\'{y}, Jan “Honza” and Gr\'{e}zl, Franti\v{s}ek},
title = {Comparison of Methods for Language-Dependent and Language-Independent Query-by-Example Spoken Term Detection},
year = {2012},
issue_date = {August 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2328967.2328971},
doi = {10.1145/2328967.2328971},
abstract = {This article investigates query-by-example (QbE) spoken term detection (STD), in which the query is not entered as text, but selected in speech data or spoken. Two feature extractors based on neural networks (NN) are introduced: the first producing phone-state posteriors and the second making use of a compressive NN layer. They are combined with three different QbE detectors: while the Gaussian mixture model/hidden Markov model (GMM/HMM) and dynamic time warping (DTW) both work on continuous feature vectors, the third one, based on weighted finite-state transducers (WFST), processes phone lattices. QbE STD is compared to two standard STD systems with text queries: acoustic keyword spotting and WFST-based search of phone strings in phone lattices. The results are reported on four languages (Czech, English, Hungarian, and Levantine Arabic) using standard metrics: equal error rate (EER) and two versions of popular figure-of-merit (FOM). Language-dependent and language-independent cases are investigated; the latter being particularly interesting for scenarios lacking standard resources to train speech recognition systems. While the DTW and GMM/HMM approaches produce the best results for a language-dependent setup depending on the target language, the GMM/HMM approach performs the best dealing with a language-independent setup. As far as WFSTs are concerned, they are promising as they allow for indexing and fast search.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {18},
numpages = {34},
keywords = {GMM/HMM-based query-by-example, keyword spotting, WFST-based query-by-example, DTW-based query-by-example, bottleneck features, Query-by-example}
}

@article{10.1145/2328967.2328970,
author = {Luz, Saturnino},
title = {The Nonverbal Structure of Patient Case Discussions in Multidisciplinary Medical Team Meetings},
year = {2012},
issue_date = {August 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2328967.2328970},
doi = {10.1145/2328967.2328970},
abstract = {Meeting analysis has a long theoretical tradition in social psychology, with established practical ramifications in computer science, especially in computer supported cooperative work. More recently, a good deal of research has focused on the issues of indexing and browsing multimedia records of meetings. Most research in this area, however, is still based on data collected in laboratories, under somewhat artificial conditions. This article presents an analysis of the discourse structure and spontaneous interactions at real-life multidisciplinary medical team meetings held as part of the work routine in a major hospital. It is hypothesized that the conversational structure of these meetings, as indicated by sequencing and duration of vocalizations, enables segmentation into individual patient case discussions. The task of segmenting audio-visual records of multidisciplinary medical team meetings is described as a topic segmentation task, and a method for automatic segmentation is proposed. An empirical evaluation based on hand labelled data is presented, which determines the optimal length of vocalization sequences for segmentation, and establishes the competitiveness of the method with approaches based on more complex knowledge sources. The effectiveness of Bayesian classification as a segmentation method, and its applicability to meeting segmentation in other domains are discussed.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {17},
numpages = {24},
keywords = {Search of spontaneous speech, multidisciplinary medical team meetings, audio analysis, dialogue segmentation, meeting analysis}
}

@article{10.1145/2328967.2328969,
author = {Wang, Dong and King, Simon and Frankel, Joe and Vipperla, Ravichander and Evans, Nicholas and Troncy, Rapha\"{e}l},
title = {Direct Posterior Confidence for Out-of-Vocabulary Spoken Term Detection},
year = {2012},
issue_date = {August 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2328967.2328969},
doi = {10.1145/2328967.2328969},
abstract = {Spoken term detection (STD) is a key technology for spoken information retrieval. As compared to the conventional speech transcription and keyword spotting, STD is an open-vocabulary task and has to address out-of-vocabulary (OOV) terms. Approaches based on subword units, for example phones, are widely used to solve the OOV issue; however, performance on OOV terms is still substantially inferior to that of in-vocabulary (INV) terms. The performance degradation on OOV terms can be attributed to a multitude of factors. One particular factor we address in this article is the unreliable confidence estimation caused by weak acoustic and language modeling due to the absence of OOV terms in the training corpora. We propose a direct posterior confidence derived from a discriminative model, such as multilayer perceptron (MLP). The new confidence considers a wide-range acoustic context which is usually important for speech recognition and retrieval; moreover, it localizes on detected speech segments and therefore avoids the impact of long-span word context which is usually unreliable for OOV term detection.In this article, we first develop an extensive discussion about the modeling weakness problem associated with OOV terms, and then propose our approach to address this problem based on direct poster confidence. Our experiments carried out on spontaneous and conversational multiparty meeting speech, demonstrate that the proposed technique provides a significant improvement in STD performance as compared to conventional lattice-based confidence, in particular for OOV terms. Furthermore, the new confidence estimation approach is fused with other advanced techniques for OOV treatment, such as stochastic pronunciation modeling and discriminative confidence normalization. This leads to an integrated solution for OOV term detection that results in a large performance improvement.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {16},
numpages = {34},
keywords = {spontaneous speech search, Speech recognition, spoken term detection}
}

@article{10.1145/2328967.2328968,
author = {Larson, Martha and de Jong, Franciska and Kraaij, Wessel and Renals, Steve},
title = {Special Issue on Searching Speech},
year = {2012},
issue_date = {August 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2328967.2328968},
doi = {10.1145/2328967.2328968},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {15},
numpages = {2}
}

@article{10.1145/2180868.2180876,
author = {Guttenbrunner, Mark and Rauber, Andreas},
title = {A Measurement Framework for Evaluating Emulators for Digital Preservation},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180876},
doi = {10.1145/2180868.2180876},
abstract = {Accessible emulation is often the method of choice for maintaining digital objects, specifically complex ones such as applications, business processes, or electronic art. However, validating the emulator’s ability to faithfully reproduce the original behavior of digital objects is complicated.This article presents an evaluation framework and a set of tests that allow assessment of the degree to which system emulation preserves original characteristics and thus significant properties of digital artifacts. The original system, hardware, and software properties are described. Identical environment is then recreated via emulation. Automated user input is used to eliminate potential confounders. The properties of a rendered form of the object are then extracted automatically or manually either in a target state, a series of states, or as a continuous stream. The concepts described in this article enable preservation planners to evaluate how emulation affects the behavior of digital objects compared to their behavior in the original environment. We also review how these principles can and should be applied to the evaluation of migration and other preservation strategies as a general principle of evaluating the invocation and faithful rendering of digital objects and systems. The article concludes with design requirements for emulators developed for digital preservation tasks.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {14},
numpages = {28},
keywords = {system properties, preserving interactive content, automated testing, digital preservation, characteristics of rendered content, Emulation}
}

@article{10.1145/2180868.2180875,
author = {Nie, Liqiang and Wang, Meng and Zha, Zheng-Jun and Chua, Tat-Seng},
title = {Oracle in Image Search: A Content-Based Approach to Performance Prediction},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180875},
doi = {10.1145/2180868.2180875},
abstract = {This article studies a novel problem in image search. Given a text query and the image ranking list returned by an image search system, we propose an approach to automatically predict the search performance. We demonstrate that, in order to estimate the mathematical expectations of Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG), we only need to predict the relevance probability of each image. We accomplish the task with a query-adaptive graph-based learning based on the images’ ranking order and visual content. We validate our approach with a large-scale dataset that contains the image search results of 1,165 queries from 4 popular image search engines. Empirical studies demonstrate that our approach is able to generate predictions that are highly correlated with the real search performance. Based on the proposed image search performance prediction scheme, we introduce three applications: image metasearch, multilingual image search, and Boolean image search. Comprehensive experiments are conducted to validate our approach.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {13},
numpages = {23},
keywords = {search performance prediction, Image search, graph-based learning}
}

@article{10.1145/2180868.2180874,
author = {Savoy, Jacques},
title = {Authorship Attribution Based on Specific Vocabulary},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180874},
doi = {10.1145/2180868.2180874},
abstract = {In this article we propose a technique for computing a standardized Z score capable of defining the specific vocabulary found in a text (or part thereof) compared to that of an entire corpus. Assuming that the term occurrence follows a binomial distribution, this method is then applied to weight terms (words and punctuation symbols in the current study), representing the lexical specificity of the underlying text. In a final stage, to define an author profile we suggest averaging these text representations and then applying them along with a distance measure to derive a simple and efficient authorship attribution scheme. To evaluate this algorithm and demonstrate its effectiveness, we develop two experiments, the first based on 5,408 newspaper articles (Glasgow Herald) written in English by 20 distinct authors and the second on 4,326 newspaper articles (La Stampa) written in Italian by 20 distinct authors. These experiments demonstrate that the suggested classification scheme tends to perform better than the Delta rule method based on the most frequent words, better than the chi-square distance based on word profiles and punctuation marks, better than the KLD scheme based on a predefined set of words, and better than the na\"{\i}ve Bayes approach.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {12},
numpages = {30},
keywords = {text classification, Authorship attribution, lexical statistics}
}

@article{10.1145/2180868.2180873,
author = {Shtok, Anna and Kurland, Oren and Carmel, David and Raiber, Fiana and Markovits, Gad},
title = {Predicting Query Performance by Query-Drift Estimation},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180873},
doi = {10.1145/2180868.2180873},
abstract = {Predicting query performance, that is, the effectiveness of a search performed in response to a query, is a highly important and challenging problem. We present a novel approach to this task that is based on measuring the standard deviation of retrieval scores in the result list of the documents most highly ranked. We argue that for retrieval methods that are based on document-query surface-level similarities, the standard deviation can serve as a surrogate for estimating the presumed amount of query drift in the result list, that is, the presence (and dominance) of aspects or topics not related to the query in documents in the list. Empirical evaluation demonstrates the prediction effectiveness of our approach for several retrieval models. Specifically, the prediction quality often transcends that of current state-of-the-art prediction methods.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {11},
numpages = {35},
keywords = {score distribution, query drift, Query-performance prediction}
}

@article{10.1145/2180868.2180872,
author = {Pal, Aditya and Harper, F. Maxwell and Konstan, Joseph A.},
title = {Exploring Question Selection Bias to Identify Experts and Potential Experts in Community Question Answering},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180872},
doi = {10.1145/2180868.2180872},
abstract = {Community Question Answering (CQA) services enable their users to exchange knowledge in the form of questions and answers. These communities thrive as a result of a small number of highly active users, typically called experts, who provide a large number of high-quality useful answers. Expert identification techniques enable community managers to take measures to retain the experts in the community. There is further value in identifying the experts during the first few weeks of their participation as it would allow measures to nurture and retain them. In this article we address two problems: (a) How to identify current experts in CQA? and (b) How to identify users who have potential of becoming experts in future (potential experts)? In particular, we propose a probabilistic model that captures the selection preferences of users based on the questions they choose for answering. The probabilistic model allows us to run machine learning methods for identifying experts and potential experts. Our results over several popular CQA datasets indicate that experts differ considerably from ordinary users in their selection preferences; enabling us to predict experts with higher accuracy over several baseline models. We show that selection preferences can be combined with baseline measures to improve the predictive performance even further.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {10},
numpages = {28},
keywords = {Expert identification, question selection process, community question answering}
}

@article{10.1145/2180868.2180871,
author = {Tigelaar, Almer S. and Hiemstra, Djoerd and Trieschnigg, Dolf},
title = {Peer-to-Peer Information Retrieval: An Overview},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180871},
doi = {10.1145/2180868.2180871},
abstract = {Peer-to-peer technology is widely used for file sharing. In the past decade a number of prototype peer-to-peer information retrieval systems have been developed. Unfortunately, none of these has seen widespread real-world adoption and thus, in contrast with file sharing, information retrieval is still dominated by centralized solutions. In this article we provide an overview of the key challenges for peer-to-peer information retrieval and the work done so far. We want to stimulate and inspire further research to overcome these challenges. This will open the door to the development and large-scale deployment of real-world peer-to-peer information retrieval systems that rival existing centralized client-server solutions in terms of scalability, performance, user satisfaction, and freedom.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {9},
numpages = {34}
}

@article{10.1145/2180868.2180870,
author = {Miotto, Riccardo and Orio, Nicola},
title = {A Probabilistic Model to Combine Tags and Acoustic Similarity for Music Retrieval},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180870},
doi = {10.1145/2180868.2180870},
abstract = {The rise of the Internet has led the music industry to a transition from physical media to online products and services. As a consequence, current online music collections store millions of songs and are constantly being enriched with new content. This has created a need for music technologies that allow users to interact with these extensive collections efficiently and effectively. Music search and discovery may be carried out using tags, matching user interests and exploiting content-based acoustic similarity. One major issue in music information retrieval is how to combine such noisy and heterogeneous information sources in order to improve retrieval effectiveness. With this aim in mind, the article explores a novel music retrieval framework based on combining tags and acoustic similarity through a probabilistic graph-based representation of a collection of songs. The retrieval function highlights the path across the graph that most likely observes a user query and is used to improve state-of-the-art music search and discovery engines by delivering more relevant ranking lists. Indeed, by means of an empirical evaluation, we show how the proposed approach leads to better performances than retrieval strategies which rank songs according to individual information sources alone or which use a combination of them.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {8},
numpages = {29},
keywords = {graph structure, acoustic similarity, music discovery, Music information retrieval, tags, probabilistic model}
}

@article{10.1145/2180868.2180869,
author = {Cao, Xin and Cong, Gao and Cui, Bin and Jensen, Christian S. and Yuan, Quan},
title = {Approaches to Exploring Category Information for Question Retrieval in Community Question-Answer Archives},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180869},
doi = {10.1145/2180868.2180869},
abstract = {Community Question Answering (CQA) is a popular type of service where users ask questions and where answers are obtained from other users or from historical question-answer pairs. CQA archives contain large volumes of questions organized into a hierarchy of categories. As an essential function of CQA services, question retrieval in a CQA archive aims to retrieve historical question-answer pairs that are relevant to a query question. This article presents several new approaches to exploiting the category information of questions for improving the performance of question retrieval, and it applies these approaches to existing question retrieval models, including a state-of-the-art question retrieval model. Experiments conducted on real CQA data demonstrate that the proposed techniques are effective and efficient and are capable of outperforming a variety of baseline methods significantly.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {7},
numpages = {38},
keywords = {question search, cluster-based retrieval, categorization, Question-answering services}
}

@article{10.1145/2094072.2094078,
author = {Chapelle, Olivier and Joachims, Thorsten and Radlinski, Filip and Yue, Yisong},
title = {Large-Scale Validation and Analysis of Interleaved Search Evaluation},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2094072.2094078},
doi = {10.1145/2094072.2094078},
abstract = {Interleaving is an increasingly popular technique for evaluating information retrieval systems based on implicit user feedback. While a number of isolated studies have analyzed how this technique agrees with conventional offline evaluation approaches and other online techniques, a complete picture of its efficiency and effectiveness is still lacking. In this paper we extend and combine the body of empirical evidence regarding interleaving, and provide a comprehensive analysis of interleaving using data from two major commercial search engines and a retrieval system for scientific literature. In particular, we analyze the agreement of interleaving with manual relevance judgments and observational implicit feedback measures, estimate the statistical efficiency of interleaving, and explore the relative performance of different interleaving variants. We also show how to learn improved credit-assignment functions for clicks that further increase the sensitivity of interleaving.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {6},
numpages = {41},
keywords = {online evaluation, search engine, clicks, judgments, Interleaving, sensitivity}
}

@article{10.1145/2094072.2094077,
author = {Broschart, Andreas and Schenkel, Ralf},
title = {High-Performance Processing of Text Queries with Tunable Pruned Term and Term Pair Indexes},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2094072.2094077},
doi = {10.1145/2094072.2094077},
abstract = {Term proximity scoring is an established means in information retrieval for improving result quality of full-text queries. Integrating such proximity scores into efficient query processing, however, has not been equally well studied. Existing methods make use of precomputed lists of documents where tuples of terms, usually pairs, occur together, usually incurring a huge index size compared to term-only indexes. This article introduces a joint framework for trading off index size and result quality, and provides optimization techniques for tuning precomputed indexes towards either maximal result quality or maximal query processing performance under controlled result quality, given an upper bound for the index size. The framework allows to selectively materialize lists for pairs based on a query log to further reduce index size. Extensive experiments with two large text collections demonstrate runtime improvements of more than one order of magnitude over existing text-based processing techniques with reasonable index sizes.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {5},
numpages = {32},
keywords = {text retrieval, Index tuning, proximity score, performance}
}

@article{10.1145/2094072.2094076,
author = {Carterette, Benjamin A.},
title = {Multiple Testing in Statistical Analysis of Systems-Based Information Retrieval Experiments},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2094072.2094076},
doi = {10.1145/2094072.2094076},
abstract = {High-quality reusable test collections and formal statistical hypothesis testing together support a rigorous experimental environment for information retrieval research. But as Armstrong et al. [2009b] recently argued, global analysis of experiments suggests that there has actually been little real improvement in ad hoc retrieval effectiveness over time. We investigate this phenomenon in the context of simultaneous testing of many hypotheses using a fixed set of data. We argue that the most common approaches to significance testing ignore a great deal of information about the world. Taking into account even a fairly small amount of this information can lead to very different conclusions about systems than those that have appeared in published literature. We demonstrate how to model a set of IR experiments for analysis both mathematically and practically, and show that doing so can cause p-values from statistical hypothesis tests to increase by orders of magnitude. This has major consequences on the interpretation of experimental results using reusable test collections: it is very difficult to conclude that anything is significant once we have modeled many of the sources of randomness in experimental design and analysis.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {4},
numpages = {34},
keywords = {statistical analysis, test collections, effectiveness evaluation, Information retrieval, experimental design}
}

@article{10.1145/2094072.2094075,
author = {Bhatia, Sumit and Mitra, Prasenjit},
title = {Summarizing Figures, Tables, and Algorithms in Scientific Publications to Augment Search Results},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2094072.2094075},
doi = {10.1145/2094072.2094075},
abstract = {Increasingly, special-purpose search engines are being built to enable the retrieval of document-elements like tables, figures, and algorithms [Bhatia et al. 2010; Liu et al. 2007; Hearst et al. 2007]. These search engines present a thumbnail view of document-elements, some document metadata such as the title of the papers and their authors, and the caption of the document-element. While some authors in some disciplines write carefully tailored captions, generally, the author of a document assumes that the caption will be read in the context of the text in the document. When the caption is presented out of context as in a document-element-search-engine result, it may not contain enough information to help the end-user understand what the content of the document-element is. Consequently, end-users examining document-element search results would want a short “synopsis” of this information presented along with the document-element. Having access to the synopsis allows the end-user to quickly understand the content of the document-element without having to download and read the entire document as examining the synopsis takes a shorter time than finding information about a document element by downloading, opening and reading the file. Furthermore, it may allow the end-user to examine more results than they would otherwise. In this paper, we present the first set of methods to extract this useful information (synopsis) related to document-elements automatically. We use Na\"{\i}ve Bayes and support vector machine classifiers to identify relevant sentences from the document text based on the similarity and the proximity of the sentences with the caption and the sentences in the document text that refer to the document-element. We compare the two classification methods and study the effects of different features used. We also investigate the problem of choosing the optimum synopsis-size that strikes a balance between the information content and the size of the generated synopses. A user study is also performed to measure how the synopses generated by our proposed method compare with other state-of-the-art approaches.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {3},
numpages = {24},
keywords = {summarization, document-element, synopses, Classification}
}

@article{10.1145/2094072.2094074,
author = {Altingovde, Ismail S. and Ozcan, Rifat and Ulusoy, \"{O}zg\"{u}r},
title = {Static Index Pruning in Web Search Engines: Combining Term and Document Popularities with Query Views},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2094072.2094074},
doi = {10.1145/2094072.2094074},
abstract = {Static index pruning techniques permanently remove a presumably redundant part of an inverted file, to reduce the file size and query processing time. These techniques differ in deciding which parts of an index can be removed safely; that is, without changing the top-ranked query results. As defined in the literature, the query view of a document is the set of query terms that access to this particular document, that is, retrieves this document among its top results. In this paper, we first propose using query views to improve the quality of the top results compared against the original results. We incorporate query views in a number of static pruning strategies, namely term-centric, document-centric, term popularity based and document access popularity based approaches, and show that the new strategies considerably outperform their counterparts especially for the higher levels of pruning and for both disjunctive and conjunctive query processing. Additionally, we combine the notions of term and document access popularity to form new pruning strategies, and further extend these strategies with the query views. The new strategies improve the result quality especially for the conjunctive query processing, which is the default and most common search mode of a search engine.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {2},
numpages = {28},
keywords = {Query view, static inverted index pruning}
}

@article{10.1145/2094072.2094073,
author = {Fari\~{n}a, Antonio and Brisaboa, Nieves R. and Navarro, Gonzalo and Claude, Francisco and Places, \'{A}ngeles S. and Rodr\'{\i}guez, Eduardo},
title = {Word-Based Self-Indexes for Natural Language Text},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2094072.2094073},
doi = {10.1145/2094072.2094073},
abstract = {The inverted index supports efficient full-text searches on natural language text collections. It requires some extra space over the compressed text that can be traded for search speed. It is usually fast for single-word searches, yet phrase searches require more expensive intersections. In this article we introduce a different kind of index. It replaces the text using essentially the same space required by the compressed text alone (compression ratio around 35%). Within this space it supports not only decompression of arbitrary passages, but efficient word and phrase searches. Searches are orders of magnitude faster than those over inverted indexes when looking for phrases, and still faster on single-word searches when little space is available. Our new indexes are particularly fast at counting the occurrences of words or phrases. This is useful for computing relevance of words or phrases.We adapt self-indexes that succeeded in indexing arbitrary strings within compressed space to deal with large alphabets. Natural language texts are then regarded as sequences of words, not characters, to achieve word-based self-indexes. We design an architecture that separates the searchable sequence from its presentation aspects. This permits applying case folding, stemming, removing stopwords, etc. as is usual on inverted indexes.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {1},
numpages = {34},
keywords = {compressed data structures, Self-indexes, inverted indexes}
}

@article{10.1145/2037661.2037667,
author = {Balog, Krisztian and Bron, Marc and De Rijke, Maarten},
title = {Query Modeling for Entity Search Based on Terms, Categories, and Examples},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2037661.2037667},
doi = {10.1145/2037661.2037667},
abstract = {Users often search for entities instead of documents, and in this setting, are willing to provide extra input, in addition to a series of query terms, such as category information and example entities. We propose a general probabilistic framework for entity search to evaluate and provide insights in the many ways of using these types of input for query modeling. We focus on the use of category information and show the advantage of a category-based representation over a term-based representation, and also demonstrate the effectiveness of category-based expansion using example entities. Our best performing model shows very competitive performance on the INEX-XER entity ranking and list completion tasks.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {22},
numpages = {31},
keywords = {generative probabilistic model, Entity retrieval, query modeling, query expansion}
}

@article{10.1145/2037661.2037666,
author = {Liu, Jiajun and Huang, Zi and Shen, Heng Tao and Cui, Bin},
title = {Correlation-Based Retrieval for Heavily Changed near-Duplicate Videos},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2037661.2037666},
doi = {10.1145/2037661.2037666},
abstract = {The unprecedented and ever-growing number of Web videos nowadays leads to the massive existence of near-duplicate videos. Very often, some near-duplicate videos exhibit great content changes, while the user perceives little information change, for example, color features change significantly when transforming a color video with a blue filter. These feature changes contribute to low-level video similarity computations, making conventional similarity-based near-duplicate video retrieval techniques incapable of accurately capturing the implicit relationship between two near-duplicate videos with fairly large content modifications. In this paper, we introduce a new dimension for near-duplicate video retrieval. Different from existing near-duplicate video retrieval approaches which are based on video-content similarity, we explore the correlation between two videos. The intuition is that near-duplicate videos should preserve strong information correlation in spite of intensive content changes. More effective retrieval with stronger tolerance is achieved by replacing video-content similarity measures with information correlation analysis. Theoretical justification and experimental results prove the effectiveness of correlation-based near-duplicate retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {21},
numpages = {25},
keywords = {Near-duplicate video, correlation-based retrieval, similarity-based retrieval}
}

@article{10.1145/2037661.2037665,
author = {Parameswaran, Aditya and Venetis, Petros and Garcia-Molina, Hector},
title = {Recommendation Systems with Complex Constraints: A Course Recommendation Perspective},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2037661.2037665},
doi = {10.1145/2037661.2037665},
abstract = {We study the problem of making recommendations when the objects to be recommended must also satisfy constraints or requirements. In particular, we focus on course recommendations: the courses taken by a student must satisfy requirements (e.g., take two out of a set of five math courses) in order for the student to graduate. Our work is done in the context of the CourseRank system, used by students to plan their academic program at Stanford University. Our goal is to recommend to these students courses that not only help satisfy constraints, but that are also desirable (e.g., popular or taken by similar students). We develop increasingly expressive models for course requirements, and present a variety of schemes for both checking if the requirements are satisfied, and for making recommendations that take into account the requirements. We show that some types of requirements are inherently expensive to check, and we present exact, as well as heuristic techniques, for those cases. Although our work is specific to course requirements, it provides insights into the design of recommendation systems in the presence of complex constraints found in other applications.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {20},
numpages = {33},
keywords = {Complex constraints, recommender systems, package recommendations}
}

@article{10.1145/2037661.2037664,
author = {Paik, Jiaul H. and Mitra, Mandar and Parui, Swapan K. and J\"{a}rvelin, Kalervo},
title = {GRAS: An Effective and Efficient Stemming Algorithm for Information Retrieval},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2037661.2037664},
doi = {10.1145/2037661.2037664},
abstract = {A novel graph-based language-independent stemming algorithm suitable for information retrieval is proposed in this article. The main features of the algorithm are retrieval effectiveness, generality, and computational efficiency. We test our approach on seven languages (using collections from the TREC, CLEF, and FIRE evaluation platforms) of varying morphological complexity. Significant performance improvement over plain word-based retrieval, three other language-independent morphological normalizers, as well as rule-based stemmers is demonstrated.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {19},
numpages = {24},
keywords = {stemming, suffix, Corpus}
}

@article{10.1145/2037661.2037663,
author = {Chen, Keke and Bai, Jing and Zheng, Zhaohui},
title = {Ranking Function Adaptation with Boosting Trees},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2037661.2037663},
doi = {10.1145/2037661.2037663},
abstract = {Machine-learned ranking functions have shown successes in Web search engines. With the increasing demands on developing effective ranking functions for different search domains, we have seen a big bottleneck, that is, the problem of insufficient labeled training data, which has significantly slowed the development and deployment of machine-learned ranking functions for different domains. There are two possible approaches to address this problem: (1) combining labeled training data from similar domains with the small target-domain labeled data for training or (2) using pairwise preference data extracted from user clickthrough log for the target domain for training. In this article, we propose a new approach called tree-based ranking function adaptation (Trada) to effectively utilize these data sources for training cross-domain ranking functions. Tree adaptation assumes that ranking functions are trained with the Stochastic Gradient Boosting Trees method—a gradient boosting method on regression trees. It takes such a ranking function from one domain and tunes its tree-based structure with a small amount of training data from the target domain. The unique features include (1) automatic identification of the part of the model that needs adjustment for the new domain and (2) appropriate weighing of training examples considering both local and global distributions. Based on a novel pairwise loss function that we developed for pairwise learning, the basic tree adaptation algorithm is also extended (Pairwise Trada) to utilize the pairwise preference data from the target domain to further improve the effectiveness of adaptation. Experiments are performed on real datasets to show that tree adaptation can provide better-quality ranking functions for a new domain than other methods.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {18},
numpages = {31},
keywords = {user feedback, learning to rank, boosting regression trees, domain adaptation, Web search ranking}
}

@article{10.1145/2037661.2037662,
author = {Macdonald, Craig and Ounis, Iadh and Tonellotto, Nicola},
title = {Upper-Bound Approximations for Dynamic Pruning},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2037661.2037662},
doi = {10.1145/2037661.2037662},
abstract = {Dynamic pruning strategies for information retrieval systems can increase querying efficiency without decreasing effectiveness by using upper bounds to safely omit scoring documents that are unlikely to make the final retrieved set. Often, such upper bounds are pre-calculated at indexing time for a given weighting model. However, this precludes changing, adapting or training the weighting model without recalculating the upper bounds. Instead, upper bounds should be approximated at querying time from various statistics of each term to allow on-the-fly adaptation of the applied retrieval strategy. This article, by using uniform notation, formulates the problem of determining a term upper-bound given a weighting model and discusses the limitations of existing approximations. Moreover, we propose an upper-bound approximation using a constrained nonlinear maximization problem. We prove that our proposed upper-bound approximation does not impact the retrieval effectiveness of several modern weighting models from various different families. We also show the applicability of the approximation for the Markov Random Field proximity model. Finally, we empirically examine how the accuracy of the upper-bound approximation impacts the number of postings scored and the resulting efficiency in the context of several large Web test collections.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {17},
numpages = {28},
keywords = {upper bounds, Dynamic pruning}
}

@article{10.1145/1993036.1993040,
author = {Bast, Hannah and Celikik, Marjan},
title = {Fast Construction of the HYB Index},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1993036.1993040},
doi = {10.1145/1993036.1993040},
abstract = {As shown in a series of recent works, the HYB index is an alternative to the inverted index (INV) that enables very fast prefix searches, which in turn is the basis for fast processing of many other types of advanced queries, including autocompletion, faceted search, error-tolerant search, database-style select and join, and semantic search. In this work we show that HYB can be constructed at least as fast as INV, and often up to twice as fast. This is because HYB, by its nature, requires only a half-inversion of the data and allows an efficient in-place instead of the traditional merge-based index construction. We also pay particular attention to the cache efficiency of the in-memory posting accumulation, an issue that has not been addressed in previous work, and show that our simple multilevel posting accumulation scheme yields much fewer cache misses compared to related approaches. Finally, we show that HYB supports fast dynamic index updates more easily than INV.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {16},
numpages = {33},
keywords = {index construction, indexing, Autocompletion, HYB index, inverted index}
}

@article{10.1145/1993036.1993039,
author = {Yan, Xin and Lau, Raymond Y.K. and Song, Dawei and Li, Xue and Ma, Jian},
title = {Toward a Semantic Granularity Model for Domain-Specific Information Retrieval},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1993036.1993039},
doi = {10.1145/1993036.1993039},
abstract = {Both similarity-based and popularity-based document ranking functions have been successfully applied to information retrieval (IR) in general. However, the dimension of semantic granularity also should be considered for effective retrieval. In this article, we propose a semantic granularity-based IR model that takes into account the three dimensions, namely similarity, popularity, and semantic granularity, to improve domain-specific search. In particular, a concept-based computational model is developed to estimate the semantic granularity of documents with reference to a domain ontology. Semantic granularity refers to the levels of semantic detail carried by an information item. The results of our benchmark experiments confirm that the proposed semantic granularity based IR model performs significantly better than the similarity-based baseline in both a bio-medical and an agricultural domain. In addition, a series of user-oriented studies reveal that the proposed document ranking functions resemble the implicit ranking functions exercised by humans. The perceived relevance of the documents delivered by the granularity-based IR system is significantly higher than that produced by a popular search engine for a number of domain-specific search tasks. To the best of our knowledge, this is the first study regarding the application of semantic granularity to enhance domain-specific IR.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {15},
numpages = {46},
keywords = {domain ontology, information retrieval, domain-specific search, Document ranking, granular computing}
}

@article{10.1145/1993036.1993038,
author = {Schedl, Markus and Pohle, Tim and Knees, Peter and Widmer, Gerhard},
title = {Exploring the Music Similarity Space on the Web},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1993036.1993038},
doi = {10.1145/1993036.1993038},
abstract = {This article comprehensively addresses the problem of similarity measurement between music artists via text-based features extracted from Web pages. To this end, we present a thorough evaluation of different term-weighting strategies, normalization methods, aggregation functions, and similarity measurement techniques. In large-scale genre classification experiments carried out on real-world artist collections, we analyze several thousand combinations of settings/parameters that influence the similarity calculation process, and investigate in which way they impact the quality of the similarity estimates. Accurate similarity measures for music are vital for many applications, such as automated playlist generation, music recommender systems, music information systems, or intelligent user interfaces to access music collections by means beyond text-based browsing. Therefore, by exhaustively analyzing the potential of text-based features derived from artist-related Web pages, this article constitutes an important contribution to context-based music information research.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {14},
numpages = {24},
keywords = {term space, evaluation, Web content mining, Music information retrieval}
}

@article{10.1145/1993036.1993037,
author = {Pedro, Jose San and Siersdorfer, Stefan and Sanderson, Mark},
title = {Content Redundancy in YouTube and Its Application to Video Tagging},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1993036.1993037},
doi = {10.1145/1993036.1993037},
abstract = {The emergence of large-scale social Web communities has enabled users to share online vast amounts of multimedia content. An analysis of YouTube reveals a high amount of redundancy, in the form of videos with overlapping or duplicated content. We use robust content-based video analysis techniques to detect overlapping sequences between videos. Based on the output of these techniques, we present an in-depth study of duplication and content overlap in YouTube, and analyze various dependencies between content overlap and meta data such as video titles, views, video ratings, and tags. As an application, we show that content-based links provide useful information for generating new tag assignments. We propose different tag propagation methods for automatically obtaining richer video annotations. Experiments on video clustering and classification as well as a user evaluation demonstrate the viability of our approach.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {13},
numpages = {31},
keywords = {content-based links, data organization, tag propagation, neighbor-based tagging, automatic tagging, Video duplicates}
}

@article{10.1145/1961209.1961215,
author = {Sun, Bingjun and Mitra, Prasenjit and Lee Giles, C. and Mueller, Karl T.},
title = {Identifying, Indexing, and Ranking Chemical Formulae and Chemical Names in Digital Documents},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1961209.1961215},
doi = {10.1145/1961209.1961215},
abstract = {End-users utilize chemical search engines to search for chemical formulae and chemical names. Chemical search engines identify and index chemical formulae and chemical names appearing in text documents to support efficient search and retrieval in the future. Identifying chemical formulae and chemical names in text automatically has been a hard problem that has met with varying degrees of success in the past. We propose algorithms for chemical formula and chemical name tagging using Conditional Random Fields (CRFs) and Support Vector Machines (SVMs) that achieve higher accuracy than existing (published) methods. After chemical entities have been identified in text documents, they must be indexed. In order to support user-provided search queries that require a partial match between the chemical name segment used as a keyword or a partial chemical formula, all possible (or a significant number of) subformulae of formulae that appear in any document and all possible subterms (e.g., “methyl”) of chemical names (e.g., “methylethyl ketone”) must be indexed. Indexing all possible subformulae and subterms results in an exponential increase in the storage and memory requirements as well as the time taken to process the indices. We propose techniques to prune the indices significantly without reducing the quality of the returned results significantly. Finally, we propose multiple query semantics to allow users to pose different types of partial search queries for chemical entities. We demonstrate empirically that our search engines improve the relevance of the returned results for search queries involving chemical entities.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {12},
numpages = {38},
keywords = {query models, similarity search, index pruning, ranking, independent frequent subsequence, support vector machines, hierarchical text segmentation, Chemical name, entity extraction, conditional random fields, chemical formula}
}

@article{10.1145/1961209.1961214,
author = {Vallet, David and Hopfgartner, Frank and Jose, Joemon M. and Castells, Pablo},
title = {Effects of Usage-Based Feedback on Video Retrieval: A Simulation-Based Study},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1961209.1961214},
doi = {10.1145/1961209.1961214},
abstract = {We present a model for exploiting community-based usage information for video retrieval, where implicit usage information from past users is exploited in order to provide enhanced assistance in video retrieval tasks, and alleviate the effects of the semantic gap problem. We propose a graph-based model for all types of implicit and explicit feedback, in which the relevant usage information is represented. Our model is designed to capture the complex interactions of a user with an interactive video retrieval system, including the representation of sequences of user-system interaction during a search session. Building upon this model, four recommendation strategies are defined and evaluated. An evaluation strategy is proposed based on simulated user actions, which enables the evaluation of our recommendation strategies over a usage information pool obtained from 24 users performing four different TRECVid tasks. Furthermore, the proposed simulation approach is used to simulate usage information pools with different characteristics, with which the recommendation approaches are further evaluated on a larger set of tasks, and their performance is studied with respect to the scalability and quality of the available implicit information.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {11},
numpages = {32},
keywords = {Human-computer interaction, implicit feedback, evaluation model, collaborative filtering}
}

@article{10.1145/1961209.1961213,
author = {Mei, Tao and Yang, Bo and Hua, Xian-Sheng and Li, Shipeng},
title = {Contextual Video Recommendation by Multimodal Relevance and User Feedback},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1961209.1961213},
doi = {10.1145/1961209.1961213},
abstract = {With Internet delivery of video content surging to an unprecedented level, video recommendation, which suggests relevant videos to targeted users according to their historical and current viewings or preferences, has become one of most pervasive online video services. This article presents a novel contextual video recommendation system, called VideoReach, based on multimodal content relevance and user feedback. We consider an online video usually consists of different modalities (i.e., visual and audio track, as well as associated texts such as query, keywords, and surrounding text). Therefore, the recommended videos should be relevant to current viewing in terms of multimodal relevance. We also consider that different parts of videos are with different degrees of interest to a user, as well as different features and modalities have different contributions to the overall relevance. As a result, the recommended videos should also be relevant to current users in terms of user feedback (i.e., user click-through). We then design a unified framework for VideoReach which can seamlessly integrate both multimodal relevance and user feedback by relevance feedback and attention fusion. VideoReach represents one of the first attempts toward contextual recommendation driven by video content and user click-through, without assuming a sufficient collection of user profiles available. We conducted experiments over a large-scale real-world video data and reported the effectiveness of VideoReach.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {10},
numpages = {24},
keywords = {Video recommendation, relevance feedback, image retrieval}
}

@article{10.1145/1961209.1961212,
author = {Ma, Hao and Zhou, Tom Chao and Lyu, Michael R. and King, Irwin},
title = {Improving Recommender Systems by Incorporating Social Contextual Information},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1961209.1961212},
doi = {10.1145/1961209.1961212},
abstract = {Due to their potential commercial value and the associated great research challenges, recommender systems have been extensively studied by both academia and industry recently. However, the data sparsity problem of the involved user-item matrix seriously affects the recommendation quality. Many existing approaches to recommender systems cannot easily deal with users who have made very few ratings. In view of the exponential growth of information generated by online users, social contextual information analysis is becoming important for many Web applications. In this article, we propose a factor analysis approach based on probabilistic matrix factorization to alleviate the data sparsity and poor prediction accuracy problems by incorporating social contextual information, such as social networks and social tags. The complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations. Moreover, the experimental results show that our method performs much better than the state-of-the-art approaches, especially in the circumstance that users have made few ratings.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {9},
numpages = {23},
keywords = {tags, collaborative filtering, social network, Recommender systems, matrix factorization}
}

@article{10.1145/1961209.1961211,
author = {Egozi, Ofer and Markovitch, Shaul and Gabrilovich, Evgeniy},
title = {Concept-Based Information Retrieval Using Explicit Semantic Analysis},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1961209.1961211},
doi = {10.1145/1961209.1961211},
abstract = {Information retrieval systems traditionally rely on textual keywords to index and retrieve documents. Keyword-based retrieval may return inaccurate and incomplete results when different keywords are used to describe the same concept in the documents and in the queries. Furthermore, the relationship between these related keywords may be semantic rather than syntactic, and capturing it thus requires access to comprehensive human world knowledge. Concept-based retrieval methods have attempted to tackle these difficulties by using manually built thesauri, by relying on term cooccurrence data, or by extracting latent word relationships and concepts from a corpus. In this article we introduce a new concept-based retrieval approach based on Explicit Semantic Analysis (ESA), a recently proposed method that augments keyword-based text representation with concept-based features, automatically extracted from massive human knowledge repositories such as Wikipedia. Our approach generates new text features automatically, and we have found that high-quality feature selection becomes crucial in this setting to make the retrieval more focused. However, due to the lack of labeled data, traditional feature selection methods cannot be used, hence we propose new methods that use self-generated labeled training data. The resulting system is evaluated on several TREC datasets, showing superior performance over previous state-of-the-art results.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {8},
numpages = {34},
keywords = {explicit semantic analysis, Concept-based retrieval, semantic search, feature selection}
}

@article{10.1145/1961209.1961210,
author = {Fang, Hui and Tao, Tao and Zhai, Chengxiang},
title = {Diagnostic Evaluation of Information Retrieval Models},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1961209.1961210},
doi = {10.1145/1961209.1961210},
abstract = {Developing effective retrieval models is a long-standing central challenge in information retrieval research. In order to develop more effective models, it is necessary to understand the deficiencies of the current retrieval models and the relative strengths of each of them. In this article, we propose a general methodology to analytically and experimentally diagnose the weaknesses of a retrieval function, which provides guidance on how to further improve its performance. Our methodology is motivated by the empirical observation that good retrieval performance is closely related to the use of various retrieval heuristics. We connect the weaknesses and strengths of a retrieval function with its implementations of these retrieval heuristics, and propose two strategies to check how well a retrieval function implements the desired retrieval heuristics. The first strategy is to formalize heuristics as constraints, and use constraint analysis to analytically check the implementation of retrieval heuristics. The second strategy is to define a set of relevance-preserving perturbations and perform diagnostic tests to empirically evaluate how well a retrieval function implements retrieval heuristics. Experiments show that both strategies are effective to identify the potential problems in implementations of the retrieval heuristics. The performance of retrieval functions can be improved after we fix these problems.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {7},
numpages = {42},
keywords = {constraints, TF-IDF weighting, Retrieval heuristics, formal models, diagnostic evaluation}
}

@article{10.1145/1877766.1877772,
author = {Choudhury, Munmun De and Sundaram, Hari and John, Ajita and Seligmann, Doree Duncan},
title = {Extraction, Characterization and Utility of Prototypical Communication Groups in the Blogosphere},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1877766.1877772},
doi = {10.1145/1877766.1877772},
abstract = {This article analyzes communication within a set of individuals to extract the representative prototypical groups and provides a novel framework to establish the utility of such groups. Corporations may want to identify representative groups (which are indicative of the overall communication set) because it is easier to track the prototypical groups rather than the entire set. This can be useful for advertising, identifying “hot” spots of resource consumption as well as in mining representative moods or temperature of a community. Our framework has three parts: extraction, characterization, and utility of prototypical groups. First, we extract groups by developing features representing communication dynamics of the individuals. Second, to characterize the overall communication set, we identify a subset of groups within the community as the prototypical groups. Third, we justify the utility of these prototypical groups by using them as predictors of related external phenomena; specifically, stock market movement of technology companies and political polls of Presidential candidates in the 2008 U.S. elections.We have conducted extensive experiments on two popular blogs, Engadget and Huffington Post. We observe that the prototypical groups can predict stock market movement/political polls satisfactorily with mean error rate of 20.32%. Further, our method outperforms baseline methods based on alternative group extraction and prototypical group identification methods. We evaluate the quality of the extracted groups based on their conductance and coverage measures and develop metrics: predictivity and resilience to evaluate their ability to predict a related external time-series variable (stock market movement/political polls). This implies that communication dynamics of individuals are essential in extracting groups in a community, and the prototypical groups extracted by our method are meaningful in characterizing the overall communication sets.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {6},
numpages = {53},
keywords = {prototypical groups, stock market movement, Huffington Post, Blogosphere, communication dynamics, Engadget, social communication, social network analysis, political polls}
}

@article{10.1145/1877766.1877771,
author = {Dell'Amico, Matteo and Capra, Licia},
title = {Dependable Filtering: Philosophy and Realizations},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1877766.1877771},
doi = {10.1145/1877766.1877771},
abstract = {Digital content production and distribution has radically changed our business models. An unprecedented volume of supply is now on offer, whetted by the demand of millions of users from all over the world. Since users cannot be expected to browse through millions of different items to find what they might like, filtering has become a popular technique to connect supply and demand: trusted users are first identified, and their opinions are then used to create recommendations. In this domain, users' trustworthiness has been measured according to one of the following two criteria: taste similarity (i.e., “I trust those who agree with me”), or social ties (i.e., “I trust my friends, and the people that my friends trust”). The former criterion aims at identifying concordant users, but is subject to abuse by malicious behaviors. The latter aims at detecting well-intentioned users, but fails to capture the natural subjectivity of tastes. In this article, we propose a new definition of trusted recommenders, addressing those users that are both well-intentioned and concordant. Based on this characterisation, we propose a novel approach to information filtering that we call dependable filtering. We describe alternative algorithms realizing this approach, and demonstrate, by means of extensive performance evaluation on a variety of real large-scale datasets, the high degree of both accuracy and robustness they entail.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {5},
numpages = {37},
keywords = {Sybil attack, Collaborative filtering, social networks, profile injection, link analysis}
}

@article{10.1145/1877766.1877770,
author = {Minkov, Einat and Cohen, William W.},
title = {Improving Graph-Walk-Based Similarity with Reranking: Case Studies for Personal Information Management},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1877766.1877770},
doi = {10.1145/1877766.1877770},
abstract = {Relational or semistructured data is naturally represented by a graph, where nodes denote entities and directed typed edges represent the relations between them. Such graphs are heterogeneous, describing different types of objects and links. We represent personal information as a graph that includes messages, terms, persons, dates, and other object types, and relations like sent-to and has-term. Given the graph, we apply finite random graph walks to induce a measure of entity similarity, which can be viewed as a tool for performing search in the graph. Experiments conducted using personal email collections derived from the Enron corpus and other corpora show how the different tasks of alias finding, threading, and person name disambiguation can be all addressed as search queries in this framework, where the graph-walk-based similarity metric is preferable to alternative approaches, and further improvements are achieved with learning. While researchers have suggested to tune edge weight parameters to optimize the graph walk performance per task, we apply reranking to improve the graph walk results, using features that describe high-level information such as the paths traversed in the walk. High performance, together with practical runtimes, suggest that the described framework is a useful search system in the PIM domain, as well as in other semistructured domains.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {4},
numpages = {52},
keywords = {learning, Graph walk, PIM, semistructured data}
}

@article{10.1145/1877766.1877769,
author = {Krikon, Eyal and Kurland, Oren and Bendersky, Michael},
title = {Utilizing Inter-Passage and Inter-Document Similarities for Reranking Search Results},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1877766.1877769},
doi = {10.1145/1877766.1877769},
abstract = {We present a novel language-model-based approach to reranking search results; that is, reordering the documents in an initially retrieved list so as to improve precision at top ranks. Our model integrates whole-document information with that induced from passages. Specifically, inter-passage, inter-document, and query-based similarities, which constitute a rich source of information, are combined in our model. Empirical evaluation shows that the precision-at-top-ranks performance of our model is substantially better than that of the initial ranking upon which reranking is performed. Furthermore, the performance is substantially better than that of a commonly used passage-based document ranking method that does not exploit inter-item similarities. Our model also generalizes and outperforms a recently proposed reranking method that utilizes inter-document similarities, but which does not exploit passage-based information. Finally, the model's performance is superior to that of a state-of-the-art pseudo-feedback-based retrieval approach.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {3},
numpages = {28},
keywords = {inter-document similarities, passage-based retrieval, passage centrality, Ad hoc retrieval, reranking, document centrality, inter-passage similarities}
}

@article{10.1145/1877766.1877768,
author = {Transier, Frederik and Sanders, Peter},
title = {Engineering Basic Algorithms of an In-Memory Text Search Engine},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1877766.1877768},
doi = {10.1145/1877766.1877768},
abstract = {Inverted index data structures are the key to fast text search engines. We first investigate one of the predominant operation on inverted indexes, which asks for intersecting two sorted lists of document IDs of different lengths. We explore compression and performance of different inverted list data structures. In particular, we present Lookup, a new data structure that allows intersection in expected time linear in the smaller list.Based on this result, we present the algorithmic core of a full text data base that allows fast Boolean queries, phrase queries, and document reporting using less space than the input text. The system uses a carefully choreographed combination of classical data compression techniques and inverted-index-based search data structures. Our experiments show that inverted indexes are preferable over purely suffix-array-based techniques for in-memory (English) text search engines.A similar system is now running in practice in each core of the distributed data base engine TREX of SAP.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {2},
numpages = {37},
keywords = {randomization, in-memory search engine, Inverted index}
}

@article{10.1145/1877766.1877767,
author = {Culpepper, J. Shane and Moffat, Alistair},
title = {Efficient Set Intersection for Inverted Indexing},
year = {2011},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1877766.1877767},
doi = {10.1145/1877766.1877767},
abstract = {Conjunctive Boolean queries are a key component of modern information retrieval systems, especially when Web-scale repositories are being searched. A conjunctive query q is equivalent to a |q|-way intersection over ordered sets of integers, where each set represents the documents containing one of the terms, and each integer in each set is an ordinal document identifier. As is the case with many computing applications, there is tension between the way in which the data is represented, and the ways in which it is to be manipulated. In particular, the sets representing index data for typical document collections are highly compressible, but are processed using random access techniques, meaning that methods for carrying out set intersections must be alert to issues to do with access patterns and data representation. Our purpose in this article is to explore these trade-offs, by investigating intersection techniques that make use of both uncompressed “integer” representations, as well as compressed arrangements. We also propose a simple hybrid method that provides both compact storage, and also faster intersection computations for conjunctive querying than is possible even with uncompressed representations.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {1},
numpages = {25},
keywords = {byte-code, set representation, information retrieval, Compact data structures, bitvector, set intersection}
}

@article{10.1145/1852102.1852108,
author = {Huang, Zi and Hu, Bo and Cheng, Hong and Shen, Heng Tao and Liu, Hongyan and Zhou, Xiaofang},
title = {Mining Near-Duplicate Graph for Cluster-Based Reranking of Web Video Search Results},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1852102.1852108},
doi = {10.1145/1852102.1852108},
abstract = {Recently, video search reranking has been an effective mechanism to improve the initial text-based ranking list by incorporating visual consistency among the result videos. While existing methods attempt to rerank all the individual result videos, they suffer from several drawbacks. In this article, we propose a new video reranking paradigm called cluster-based video reranking (CVR). The idea is to first construct a video near-duplicate graph representing the visual similarity relationship among videos, followed by identifying the near-duplicate clusters from the video near-duplicate graph, then ranking the obtained near-duplicate clusters based on cluster properties and intercluster links, and finally for each ranked cluster, a representative video is selected and returned. Compared to existing methods, the new CVR ranks clusters and exhibits several advantages, including superior reranking by utilizing more reliable cluster properties, fast reranking on a small number of clusters, diverse and representative results. Particularly, we formulate the near-duplicate cluster identification as a novel maximally cohesive subgraph mining problem. By leveraging the designed cluster scoring properties indicating the cluster's importance and quality, random walk is applied over the near-duplicate cluster graph to rank clusters. An extensive evaluation study proves the novelty and superiority of our proposals over existing methods.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {22},
numpages = {27},
keywords = {graph mining near-duplicate, Cluster-based video reranking, Web search}
}

@article{10.1145/1852102.1852107,
author = {Clements, Maarten and De Vries, Arjen P. and Reinders, Marcel J. T.},
title = {The Task-Dependent Effect of Tags and Ratings on Social Media Access},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1852102.1852107},
doi = {10.1145/1852102.1852107},
abstract = {Recently, online social networks have emerged that allow people to share their multimedia files, retrieve interesting content, and discover like-minded people. These systems often provide the possibility to annotate the content with tags and ratings.Using a random walk through the social annotation graph, we have combined these annotations into a retrieval model that effectively balances the personal preferences and opinions of like-minded users into a single relevance ranking for either content, tags, or people. We use this model to identify the influence of different annotation methods and system design aspects on common ranking tasks in social content systems.Our results show that a combination of rating and tagging information can improve tasks like search and recommendation. The optimal influence of both sources on the ranking is highly dependent on the retrieval task and system design. Results on content search and tag suggestion indicate that the profile created by a user's annotations can be used effectively to adapt the ranking to personal preferences. The random walk reduces sparsity problems by smoothly integrating indirectly related concepts in the relevance ranking, which is especially valuable for cold-start users or individual tagging systems like YouTube and Flickr.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {21},
numpages = {42},
keywords = {social media, recommendation, movielens, librarything, rating, tagging, content retrieval, random walk, Personalization}
}

@article{10.1145/1852102.1852106,
author = {Webber, William and Moffat, Alistair and Zobel, Justin},
title = {A Similarity Measure for Indefinite Rankings},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1852102.1852106},
doi = {10.1145/1852102.1852106},
abstract = {Ranked lists are encountered in research and daily life and it is often of interest to compare these lists even when they are incomplete or have only some members in common. An example is document rankings returned for the same query by different search engines. A measure of the similarity between incomplete rankings should handle nonconjointness, weight high ranks more heavily than low, and be monotonic with increasing depth of evaluation; but no measure satisfying all these criteria currently exists. In this article, we propose a new measure having these qualities, namely rank-biased overlap (RBO). The RBO measure is based on a simple probabilistic user model. It provides monotonicity by calculating, at a given depth of evaluation, a base score that is non-decreasing with additional evaluation, and a maximum score that is nonincreasing. An extrapolated score can be calculated between these bounds if a point estimate is required. RBO has a parameter which determines the strength of the weighting to top ranks. We extend RBO to handle tied ranks and rankings of different lengths. Finally, we give examples of the use of the measure in comparing the results produced by public search engines and in assessing retrieval systems in the laboratory.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {20},
numpages = {38},
keywords = {probabilistic models, Rank correlation, ranking}
}

@article{10.1145/1852102.1852105,
author = {Magalh\~{a}es, Jo\~{a}o and R\"{u}ger, Stefan},
title = {An Information-Theoretic Framework for Semantic-Multimedia Retrieval},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1852102.1852105},
doi = {10.1145/1852102.1852105},
abstract = {This article is set in the context of searching text and image repositories by keyword. We develop a unified probabilistic framework for text, image, and combined text and image retrieval that is based on the detection of keywords (concepts) using automated image annotation technology. Our framework is deeply rooted in information theory and lends itself to use with other media types.We estimate a statistical model in a multimodal feature space for each possible query keyword. The key element of our framework is to identify feature space transformations that make them comparable in complexity and density. We select the optimal multimodal feature space with a minimum description length criterion from a set of candidate feature spaces that are computed with the average-mutual-information criterion for the text part and hierarchical expectation maximization for the visual part of the data. We evaluate our approach in three retrieval experiments (only text retrieval, only image retrieval, and text combined with image retrieval), verify the framework's low computational complexity, and compare with existing state-of-the-art ad-hoc models.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {19},
numpages = {32},
keywords = {automated keyword annotation, search, Indexing, multimedia}
}

@article{10.1145/1852102.1852104,
author = {Kurland, Oren and Lee, Lillian},
title = {PageRank without Hyperlinks: Structural Reranking Using Links Induced by Language Models},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1852102.1852104},
doi = {10.1145/1852102.1852104},
abstract = {The ad hoc retrieval task is to find documents in a corpus that are relevant to a query. Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web search, we propose a structural reranking approach to ad-hoc retrieval that applies to settings with no hyperlink information. We reorder the documents in an initially retrieved set by exploiting implicit asymmetric relationships among them. We consider generation links, which indicate that the language model induced from one document assigns high probability to the text of another. We study a number of reranking criteria based on measures of centrality in the graphs formed by generation links, and show that integrating centrality into standard language-model-based retrieval is quite effective at improving precision at top ranks; the best resultant performance is comparable, and often superior, to that of a state-of-the-art pseudo-feedback-based retrieval approach. In addition, we demonstrate the merits of our language-model-based method for inducing interdocument links by comparing it to previously suggested notions of interdocument similarities (e.g., cosines within the vector-space model).We also show that ourmethods for inducing centrality are substantially more effective than approaches based on document-specific characteristics, several of which are novel to this study.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {18},
numpages = {38},
keywords = {hubs, high-accuracy retrieval, graph-based retrieval, authorities, structural reranking, Language modeling, PageRank, social networks, HITS}
}

@article{10.1145/1852102.1852103,
author = {Tan, Qingzhao and Mitra, Prasenjit},
title = {Clustering-Based Incremental Web Crawling},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1852102.1852103},
doi = {10.1145/1852102.1852103},
abstract = {When crawling resources, for example, number of machines, crawl-time, and so on, are limited, so a crawler has to decide an optimal order in which to crawl and recrawl Web pages. Ideally, crawlers should request only those Web pages that have changed since the last crawl; in practice, a crawler may not know whether a Web page has changed before downloading it. In this article, we identify features of Web pages that are correlated to their change frequency. We design a crawling algorithm that clusters Web pages based on features that correlate to their change frequencies obtained by examining past history. The crawler downloads a sample of Web pages from each cluster, and depending upon whether a significant number of these Web pages have changed in the last crawl cycle, it decides whether to recrawl the entire cluster. To evaluate the performance of our incremental crawler, we develop an evaluation framework that measures which crawling policy results in the best search results for the end-user. We run experiments on a real Web data set of about 300,000 distinct URLs distributed among 210 Web sites. The results demonstrate that the clustering-based sampling algorithm effectively clusters the pages with similar change patterns, and our clustering-based crawling algorithm outperforms existing algorithms in that it can improve the quality of the user experience for those who query the search engine.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {17},
numpages = {27},
keywords = {refresh policy, Clustering, search engine, Web crawler, sampling}
}

@article{10.1145/1777432.1777439,
author = {Ko, Jeongwoo and Si, Luo and Nyberg, Eric and Mitamura, Teruko},
title = {Probabilistic Models for Answer-Ranking in Multilingual Question-Answering},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1777432.1777439},
doi = {10.1145/1777432.1777439},
abstract = {This article presents two probabilistic models for answering ranking in the multilingual question-answering (QA) task, which finds exact answers to a natural language question written in different languages. Although some probabilistic methods have been utilized in traditional monolingual answer-ranking, limited prior research has been conducted for answer-ranking in multilingual question-answering with formal methods. This article first describes a probabilistic model that predicts the probabilities of correctness for individual answers in an independent way. It then proposes a novel probabilistic method to jointly predict the correctness of answers by considering both the correctness of individual answers as well as their correlations. As far as we know, this is the first probabilistic framework that proposes to model the correctness and correlation of answer candidates in multilingual question-answering and provide a novel approach to design a flexible and extensible system architecture for answer selection in multilingual QA. An extensive set of experiments were conducted to show the effectiveness of the proposed probabilistic methods in English-to-Chinese and English-to-Japanese cross-lingual QA, as well as English, Chinese, and Japanese monolingual QA using TREC and NTCIR questions.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {16},
numpages = {37},
keywords = {answer-ranking, Question-answering, answer-merging, probabilistic graphical model, answer selection}
}

@article{10.1145/1777432.1777438,
author = {Lauw, Hady W. and Lim, Ee-Peng and Pang, Hweehwa and Tan, Teck-Tim},
title = {STEvent: Spatio-Temporal Event Model for Social Network Discovery},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1777432.1777438},
doi = {10.1145/1777432.1777438},
abstract = {Spatio-temporal data concerning the movement of individuals over space and time contains latent information on the associations among these individuals. Sources of spatio-temporal data include usage logs of mobile and Internet technologies. This article defines a spatio-temporal event by the co-occurrences among individuals that indicate potential associations among them. Each spatio-temporal event is assigned a weight based on the precision and uniqueness of the event. By aggregating the weights of events relating two individuals, we can determine the strength of association between them. We conduct extensive experimentation to investigate both the efficacy of the proposed model as well as the computational complexity of the proposed algorithms. Experimental results on three real-life spatio-temporal datasets cross-validate each other, lending greater confidence on the reliability of our proposed model.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {15},
numpages = {32},
keywords = {social network, Data mining, spatio-temporal databases}
}

@article{10.1145/1777432.1777437,
author = {Maslennikov, Mstislav and Chua, Tat-Seng},
title = {Combining Relations for Information Extraction from Free Text},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1777432.1777437},
doi = {10.1145/1777432.1777437},
abstract = {Relations between entities of the same semantic type tend to be sparse in free texts. Therefore, combining relations is the key to effective information extraction (IE) on free text datasets with a small set of training samples. Previous approaches to bootstrapping for IE used different types of relations, such as dependency or co-occurrence, and faced the problems of paraphrasing and misalignment of instances. To cope with these problems, we propose a framework that integrates several types of relations. After extracting candidate entities, our framework evaluates relations between them at the phrasal, dependency, semantic frame, and discourse levels. For each of these levels, we build a classifier that outputs a score for relation instances. In order to integrate these scores, we propose three strategies: (1) integrate evaluation scores from each relation classifier; (2) incorporate the elimination of negatively labeled instances in a previous strategy; and (3) add cascading of extracted relations into strategy (2). Our framework improves the state-of-art results for supervised systems by 8%, 15%, 3%, and 5% on MUC4 (terrorism); MUC6 (management succession); ACE RDC 2003 (news, general types); and ACE RDC 2003 (news, specific types) domains respectively.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {14},
numpages = {35},
keywords = {bootstrapping, dependency relations, discourse relations, semantic relations, Information extraction}
}

@article{10.1145/1777432.1777436,
author = {Harabagiu, Sanda and Lacatusu, Finley},
title = {Using Topic Themes for Multi-Document Summarization},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1777432.1777436},
doi = {10.1145/1777432.1777436},
abstract = {The problem of using topic representations for multidocument summarization (MDS) has received considerable attention recently. Several topic representations have been employed for producing informative and coherent summaries. In this article, we describe five previously known topic representations and introduce two novel representations of topics based on topic themes. We present eight different methods of generating multidocument summaries and evaluate each of these methods on a large set of topics used in past DUC workshops. Our evaluation results show a significant improvement in the quality of summaries based on topic themes over MDS methods that use other alternative topic representations.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {13},
numpages = {47},
keywords = {topic themes, Summarization, topic representations}
}

@article{10.1145/1777432.1777435,
author = {Li, Xiao and Wang, Ye-Yi and Shen, Dou and Acero, Alex},
title = {Learning with Click Graph for Query Intent Classification},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1777432.1777435},
doi = {10.1145/1777432.1777435},
abstract = {Topical query classification, as one step toward understanding users' search intent, is gaining increasing attention in information retrieval. Previous works on this subject primarily focused on enrichment of query features, for example, by augmenting queries with search engine results. In this work, we investigate a completely orthogonal approach—instead of improving feature representation, we aim at drastically increasing the amount of training data. To this end, we propose two semisupervised learning methods that exploit user click-through data. In one approach, we infer class memberships of unlabeled queries from those of labeled ones according to their proximities in a click graph; and then use these automatically labeled queries to train classifiers using query terms as features. In a second approach, click graph learning and query classifier training are conducted jointly with an integrated objective. Our methods are evaluated in two applications, product intent and job intent classification. In both cases, we expand the training data by over two orders of magnitude, leading to significant improvements in classification performance. An additional finding is that with a large amount of training data obtained in this fashion, a classifier based on simple query term features can outperform those using state-of-the-art, augmented features.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {12},
numpages = {20},
keywords = {click graph, Semisupervised learning, query classification, user intent}
}

@article{10.1145/1777432.1777434,
author = {Wu, Gang and Wei, Yimin},
title = {Arnoldi versus GMRES for Computing PageRank: A Theoretical Contribution to Google's PageRank Problem},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1777432.1777434},
doi = {10.1145/1777432.1777434},
abstract = {PageRank is one of the most important ranking techniques used in today's search engines. A recent very interesting research track focuses on exploiting efficient numerical methods to speed up the computation of PageRank, among which the Arnoldi-type algorithm and the GMRES algorithm are competitive candidates. In essence, the former deals with the PageRank problem from an eigenproblem, while the latter from a linear system, point of view. However, there is little known about the relations between the two approaches for PageRank. In this article, we focus on a theoretical and numerical comparison of the two approaches. Numerical experiments illustrate the effectiveness of our theoretical results.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {11},
numpages = {28},
keywords = {Google, PageRank, Krylov subspace, Arnoldi, Web ranking, GMRES}
}

@article{10.1145/1777432.1777433,
author = {Brisaboa, Nieves and Fari\~{n}a, Antonio and Navarro, Gonzalo and Param\'{a}, Jos\'{e}},
title = {Dynamic Lightweight Text Compression},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1777432.1777433},
doi = {10.1145/1777432.1777433},
abstract = {We address the problem of adaptive compression of natural language text, considering the case where the receiver is much less powerful than the sender, as in mobile applications. Our techniques achieve compression ratios around 32% and require very little effort from the receiver. Furthermore, the receiver is not only lighter, but it can also search the compressed text with less work than that necessary to decompress it. This is a novelty in two senses: it breaks the usual compressor/decompressor symmetry typical of adaptive schemes, and it contradicts the long-standing assumption that only semistatic codes could be searched more efficiently than the uncompressed text. Our novel compression methods are preferable in several aspects over the existing adaptive and semistatic compressors for natural language texts.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {10},
numpages = {32},
keywords = {searching compressed texts, Text compression, adaptive natural language text compression, real-time transmission, compressed pattern matching}
}

@article{10.1145/1740592.1740597,
author = {Kelly, Diane and Fu, Xin and Shah, Chirag},
title = {Effects of Position and Number of Relevant Documents Retrieved on Users' Evaluations of System Performance},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1740592.1740597},
doi = {10.1145/1740592.1740597},
abstract = {Information retrieval research has demonstrated that system performance does not always correlate positively with user performance, and that users often assign positive evaluation scores to search systems even when they are unable to complete tasks successfully. This research investigated the relationship between objective measures of system performance and users' perceptions of that performance. In this study, subjects evaluated the performance of four search systems whose search results were manipulated systematically to produce different orderings and numbers of relevant documents. Three laboratory studies were conducted with a total of eighty-one subjects. The first two studies investigated the effect of the order of five relevant and five nonrelevant documents in a search results list containing ten results on subjects' evaluations. The third study investigated the effect of varying the number of relevant documents in a search results list containing ten results on subjects' evaluations. Results demonstrate linear relationships between subjects' evaluations and the position of relevant documents in a search results list and the total number of relevant documents retrieved. Of the two, number of relevant documents retrieved was a stronger predictor of subjects' evaluation ratings and resulted in subjects using a greater range of evaluation scores.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {9},
numpages = {29},
keywords = {user evaluation of performance, presentation of search results, ranking, satisfaction, Search performance, precision}
}

@article{10.1145/1740592.1740596,
author = {Wan, Xiaojun and Xiao, Jianguo},
title = {Exploiting Neighborhood Knowledge for Single Document Summarization and Keyphrase Extraction},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1740592.1740596},
doi = {10.1145/1740592.1740596},
abstract = {Document summarization and keyphrase extraction are two related tasks in the IR and NLP fields, and both of them aim at extracting condensed representations from a single text document. Existing methods for single document summarization and keyphrase extraction usually make use of only the information contained in the specified document. This article proposes using a small number of nearest neighbor documents to improve document summarization and keyphrase extraction for the specified document, under the assumption that the neighbor documents could provide additional knowledge and more clues. The specified document is expanded to a small document set by adding a few neighbor documents close to the document, and the graph-based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor documents. Experimental results on the Document Understanding Conference (DUC) benchmark datasets demonstrate the effectiveness and robustness of our proposed approaches. The cross-document sentence relationships in the expanded document set are validated to be beneficial to single document summarization, and the word cooccurrence relationships in the neighbor documents are validated to be very helpful to single document keyphrase extraction.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {8},
numpages = {34},
keywords = {Document summarization, neighborhood knowledge, graph-based ranking, keyphrase extraction}
}

@article{10.1145/1740592.1740595,
author = {Kolbe, Dashiell and Zhu, Qiang and Pramanik, Sakti},
title = {Efficient K-Nearest Neighbor Searching in Nonordered Discrete Data Spaces},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1740592.1740595},
doi = {10.1145/1740592.1740595},
abstract = {Numerous techniques have been proposed in the past for supporting efficient k-nearest neighbor (k-NN) queries in continuous data spaces. Limited work has been reported in the literature for k-NN queries in a nonordered discrete data space (NDDS). Performing k-NN queries in an NDDS raises new challenges. The Hamming distance is usually used to measure the distance between two vectors (objects) in an NDDS. Due to the coarse granularity of the Hamming distance, a k-NN query in an NDDS may lead to a high degree of nondeterminism for the query result. We propose a new distance measure, called Granularity-Enhanced Hamming (GEH) distance, which effectively reduces the number of candidate solutions for a query. We have also implemented k-NN queries using multidimensional database indexing in NDDSs. Further, we use the properties of our multidimensional NDDS index to derive the probability of encountering valid neighbors within specific regions of the index. This probability is used to develop a new search ordering heuristic. Our experiments on synthetic and genomic data sets demonstrate that our index-based k-NN algorithm is efficient in finding k-NNs in both uniform and nonuniform data sets in NDDSs and that our heuristics are effective in improving the performance of such queries.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {7},
numpages = {33},
keywords = {nonordered discrete data space, Similarity search, nearest neighbor, spatial indexing, database, distance measurement}
}

@article{10.1145/1740592.1740594,
author = {Gao, Wei and Niu, Cheng and Nie, Jian-Yun and Zhou, Ming and Wong, Kam-Fai and Hon, Hsiao-Wuen},
title = {Exploiting Query Logs for Cross-Lingual Query Suggestions},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1740592.1740594},
doi = {10.1145/1740592.1740594},
abstract = {Query suggestion aims to suggest relevant queries for a given query, which helps users better specify their information needs. Previous work on query suggestion has been limited to the same language. In this article, we extend it to cross-lingual query suggestion (CLQS): for a query in one language, we suggest similar or relevant queries in other languages. This is very important to the scenarios of cross-language information retrieval (CLIR) and other related cross-lingual applications. Instead of relying on existing query translation technologies for CLQS, we present an effective means to map the input query of one language to queries of the other language in the query log. Important monolingual and cross-lingual information such as word translation relations and word co-occurrence statistics, and so on, are used to estimate the cross-lingual query similarity with a discriminative model. Benchmarks show that the resulting CLQS system significantly outperforms a baseline system that uses dictionary-based query translation. Besides, we evaluate CLQS with French-English and Chinese-English CLIR tasks on TREC-6 and NTCIR-4 collections, respectively. The CLIR experiments using typical retrieval models demonstrate that the CLQS-based approach has significantly higher effectiveness than several traditional query translation methods. We find that when combined with pseudo-relevance feedback, the effectiveness of CLIR using CLQS is enhanced for different pairs of languages.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {6},
numpages = {33},
keywords = {query translation, Cross-language information retrieval, query log, query expansion, query suggestion}
}

@article{10.1145/1740592.1740593,
author = {Puppin, Diego and Silvestri, Fabrizio and Perego, Raffaele and Baeza-Yates, Ricardo},
title = {Tuning the Capacity of Search Engines: Load-Driven Routing and Incremental Caching to Reduce and Balance the Load},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1740592.1740593},
doi = {10.1145/1740592.1740593},
abstract = {This article introduces an architecture for a document-partitioned search engine, based on a novel approach combining collection selection and load balancing, called load-driven routing. By exploiting the query-vector document model, and the incremental caching technique, our architecture can compute very high quality results for any query, with only a fraction of the computational load used in a typical document-partitioned architecture. By trading off a small fraction of the results, our technique allows us to strongly reduce the computing pressure to a search engine back-end; we are able to retrieve more than 2/3 of the top-5 results for a given query with only 10% the computing load needed by a configuration where the query is processed by each index partition. Alternatively, we can slightly increase the load up to 25% to improve precision and get more than 80% of the top-5 results. In fact, the flexibility of our system allows a wide range of different configurations, so as to easily respond to different needs in result quality or restrictions in computing power. More important, the system configuration can be adjusted dynamically in order to fit unexpected query peaks or unpredictable failures. This article wraps up some recent works by the authors, showing the results obtained by tests conducted on 6 million documents, 2,800,000 queries and real query cost timing as measured on an actual index.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {5},
numpages = {36},
keywords = {incremental caching, Web search engines, Distributed IR, collection selection}
}

@article{10.1145/1658377.1658381,
author = {Rosen-Zvi, Michal and Chemudugunta, Chaitanya and Griffiths, Thomas and Smyth, Padhraic and Steyvers, Mark},
title = {Learning Author-Topic Models from Text Corpora},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1658377.1658381},
doi = {10.1145/1658377.1658381},
abstract = {We propose an unsupervised learning technique for extracting information about authors and topics from large text collections. We model documents as if they were generated by a two-stage stochastic process. An author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words. The probability distribution over topics in a multi-author paper is a mixture of the distributions associated with the authors. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to three large text corpora: 150,000 abstracts from the CiteSeer digital library, 1740 papers from the Neural Information Processing Systems (NIPS) Conferences, and 121,000 emails from the Enron corporation. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, parsing of abstracts by topics and authors, and detection of unusual papers by specific authors. Experiments based on perplexity scores for test documents and precision-recall for document retrieval are used to illustrate systematic differences between the proposed author-topic model and a number of alternatives. Extensions to the model, allowing for example, generalizations of the notion of an author, are also briefly discussed.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {4},
numpages = {38},
keywords = {Gibbs sampling, Topic models, unsupervised learning, perplexity, author models}
}

@article{10.1145/1658377.1658380,
author = {Tagarelli, Andrea and Greco, Sergio},
title = {Semantic Clustering of XML Documents},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1658377.1658380},
doi = {10.1145/1658377.1658380},
abstract = {Dealing with structure and content semantics underlying semistructured documents is challenging for any task of document management and knowledge discovery conceived for such data. In this work we address the novel problem of clustering semantically related XML documents according to their structure and content features. XML features are generated by enriching syntactic with semantic information based on a lexical knowledge base. The backbone of the proposed framework for the semantic clustering of XML documents is a data representation model that exploits the notion of tree tuple to identify semantically cohesive substructures in XML documents and represent them as transactional data. This framework is equipped with two clustering algorithms based on different paradigms, namely centroid-based partitional clustering and frequent-itemset-based hierarchical clustering. An extensive experimental evaluation was conducted on real data sets from various domains, showing the significance of our approach as a solution for the semantic clustering of XML documents.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {3},
numpages = {56},
keywords = {similarity measures, XML document clustering, XML structure and content mining, XML transactional modeling, XML tree tuples}
}

@article{10.1145/1658377.1658379,
author = {Chia, Tee Kiah and Sim, Khe Chai and Li, Haizhou and Ng, Hwee Tou},
title = {Statistical Lattice-Based Spoken Document Retrieval},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1658377.1658379},
doi = {10.1145/1658377.1658379},
abstract = {Recent research efforts on spoken document retrieval have tried to overcome the low quality of 1-best automatic speech recognition transcripts, especially in the case of conversational speech, by using statistics derived from speech lattices containing multiple transcription hypotheses as output by a speech recognizer. We present a method for lattice-based spoken document retrieval based on a statistical n-gram modeling approach to information retrieval. In this statistical lattice-based retrieval (SLBR) method, a smoothed statistical model is estimated for each document from the expected counts of words given the information in a lattice, and the relevance of each document to a query is measured as a probability under such a model. We investigate the efficacy of our method under various parameter settings of the speech recognition and lattice processing engines, using the Fisher English Corpus of conversational telephone speech. Experimental results show that our method consistently achieves better retrieval performance than using only the 1-best transcripts in statistical retrieval, outperforms a recently proposed lattice-based vector space retrieval method, and also compares favorably with a lattice-based retrieval method based on the Okapi BM25 model.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {2},
numpages = {30},
keywords = {probabilistic retrieval approach, Lattice-based spoken document retrieval, retrieval of conversational speech}
}

@article{10.1145/1658377.1658378,
author = {Blanco, Roi and Barreiro, Alvaro},
title = {Probabilistic Static Pruning of Inverted Files},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1658377.1658378},
doi = {10.1145/1658377.1658378},
abstract = {Information retrieval (IR) systems typically compress their indexes in order to increase their efficiency. Static pruning is a form of lossy data compression: it removes from the index, data that is estimated to be the least important to retrieval performance, according to some criterion. Generally, pruning criteria are derived from term weighting functions, which assign weights to terms according to their contribution to a document's contents. Usually, document-term occurrences that are assigned a low weight are ruled out from the index. The main assumption is that those entries contribute little to the document content.We present a novel pruning technique that is based on a probabilistic model of IR. We employ the Probability Ranking Principle as a decision criterion over which posting list entries are to be pruned. The proposed approach requires the estimation of three probabilities, combining them in such a way that we gather all the necessary information to apply the aforementioned criterion.We evaluate our proposed pruning technique on five TREC collections and various retrieval tasks, and show that in almost every situation it outperforms the state of the art in index pruning. The main contribution of this work is proposing a pruning technique that stems directly from the same source as probabilistic retrieval models, and hence is independent of the final model used for retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {1},
numpages = {33},
keywords = {Pruning, inverted files, compression, efficiency, probabilistic models}
}

@article{10.1145/1629096.1629102,
author = {Rosaci, Domenico and Sarn\'{e}, Giuseppe M. L. and Garruzzo, Salvatore},
title = {MUADDIB: A Distributed Recommender System Supporting Device Adaptivity},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1629096.1629102},
doi = {10.1145/1629096.1629102},
abstract = {Web recommender systems are Web applications capable of generating useful suggestions for visitors of Internet sites. However, in the case of large user communities and in presence of a high number of Web sites, these tasks are computationally onerous, even more if the client software runs on devices with limited resources. Moreover, the quality of the recommendations strictly depends on how the recommendation algorithm takes into account the currently used device. Some approaches proposed in the literature provide multidimensional recommendations considering, besides items and users, also the exploited device. However, these systems do not efficiently perform, since they assign to either the client or the server the arduous cost of computing recommendations. In this article, we argue that a fully distributed organization is a suitable solution to improve the efficiency of multidimensional recommender systems. In order to address these issues, we propose a novel distributed architecture, called MUADDIB, where each user's device is provided with a device assistant that autonomously retrieves information about the user's behavior. Moreover, a single profiler, associated with the user, periodically collects information coming from the different user's device assistants to construct a global user's profile. In order to generate recommendations, a recommender precomputes data provided by the profilers. This way, the site manager has only the task of suitably presenting the content of the site, while the computation of the recommendations is assigned to the other distributed components. Some experiments conducted on real data and using some well-known metrics show that the system works more effectively and efficiently than other device-based distributed recommenders.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {24},
numpages = {41},
keywords = {personalization, adaptivity, Recommender systems}
}

@article{10.1145/1629096.1629101,
author = {White, Ryen W. and Horvitz, Eric},
title = {Cyberchondria: Studies of the Escalation of Medical Concerns in Web Search},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1629096.1629101},
doi = {10.1145/1629096.1629101},
abstract = {The World Wide Web provides an abundant source of medical information. This information can assist people who are not healthcare professionals to better understand health and illness, and to provide them with feasible explanations for symptoms. However, the Web has the potential to increase the anxieties of people who have little or no medical training, especially when Web search is employed as a diagnostic procedure. We use the term cyberchondria to refer to the unfounded escalation of concerns about common symptomatology, based on the review of search results and literature on the Web. We performed a large-scale, longitudinal, log-based study of how people search for medical information online, supported by a survey of 515 individuals' health-related search experiences. We focused on the extent to which common, likely innocuous symptoms can escalate into the review of content on serious, rare conditions that are linked to the common symptoms. Our results show that Web search engines have the potential to escalate medical concerns. We show that escalation is associated with the amount and distribution of medical content viewed by users, the presence of escalatory terminology in pages visited, and a user's predisposition to escalate versus to seek more reasonable explanations for ailments. We also demonstrate the persistence of postsession anxiety following escalations and the effect that such anxieties can have on interrupting user's activities across multiple sessions. Our findings underscore the potential costs and challenges of cyberchondria and suggest actionable design implications that hold opportunity for improving the search and navigation experience for people turning to the Web to interpret common symptoms.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {23},
numpages = {37},
keywords = {Cyberchondria}
}

@article{10.1145/1629096.1629100,
author = {Dupplaw, David and Dasmahapatra, Srinandan and Hu, Bo and Lewis, Paul and Shadbolt, Nigel},
title = {A Distributed, Service-Based Framework for Knowledge Applications with Multimedia},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1629096.1629100},
doi = {10.1145/1629096.1629100},
abstract = {The current trend in distributed systems is towards service-based integration. This article describes an ontology-driven framework implemented to provide knowledge management for data of different modalities, with multimedia processing, annotation, and reasoning provided by remote services. The framework was developed in, and is presented in the context of, the Medical Imaging and Advanced Knowledge Technologies (MIAKT) project that sought to support the Multidisciplinary Meetings (MDMs) that take place during breast cancer screening for diagnosing the patient. However, the architecture is entirely independent of the specific application domain and can be quickly prototyped into new domains. An Enterprise server provides resource access to a client-side presentation application which, in turn, provides knowledge visualization and markup of any supported media, as defined by a domain-dependent ontology-supported language.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {22},
numpages = {29},
keywords = {ontologies, services, descision support, health, breast cancer, Semantic Web}
}

@article{10.1145/1629096.1629099,
author = {Guiver, John and Mizzaro, Stefano and Robertson, Stephen},
title = {A Few Good Topics: Experiments in Topic Set Reduction for Retrieval Evaluation},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1629096.1629099},
doi = {10.1145/1629096.1629099},
abstract = {We consider the issue of evaluating information retrieval systems on the basis of a limited number of topics. In contrast to statistically-based work on sample sizes, we hypothesize that some topics or topic sets are better than others at predicting true system effectiveness, and that with the right choice of topics, accurate predictions can be obtained from small topics sets. Using a variety of effectiveness metrics and measures of goodness of prediction, a study of a set of TREC and NTCIR results confirms this hypothesis, and provides evidence that the value of a topic set for this purpose does generalize.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {21},
numpages = {26},
keywords = {evaluation experiments, Search effectiveness, topic selection, test corpora}
}

@article{10.1145/1629096.1629098,
author = {Dang, Edward Kai Fung and Wu, Ho Chung and Luk, Robert Wing Pong and Wong, Kam Fai},
title = {Building a Framework for the Probability Ranking Principle by a Family of Expected Weighted Rank},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1629096.1629098},
doi = {10.1145/1629096.1629098},
abstract = {A new principles framework is presented for retrieval evaluation of ranked outputs. It applies decision theory to model relevance decision preferences and shows that the Probability Ranking Principle (PRP) specifies optimal ranking. It has two new components, namely a probabilistic evaluation model and a general measure of retrieval effectiveness. Its probabilities may be interpreted as subjective or objective ones. Its performance measure is the expected weighted rank which is the weighted average rank of a retrieval list. Starting from this measure, the expected forward rank and some existing retrieval effectiveness measures (e.g., top n precision and discounted cumulative gain) are instantiated using suitable weighting schemes after making certain assumptions. The significance of these instantiations is that the ranking prescribed by PRP is shown to be optimal simultaneously for all these existing performance measures. In addition, the optimal expected weighted rank may be used to normalize the expected weighted rank of retrieval systems for (summary) performance comparison (across different topics) between systems. The framework also extends PRP and our evaluation model to handle graded relevance, thereby generalizing the discussed, existing measures (e.g., top n precision) and probabilistic retrieval models for graded relevance.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {20},
numpages = {37},
keywords = {Probability ranking principle, optimization}
}

@article{10.1145/1629096.1629097,
author = {Boldi, Paolo and Santini, Massimo and Vigna, Sebastiano},
title = {PageRank: Functional Dependencies},
year = {2009},
issue_date = {November 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1629096.1629097},
doi = {10.1145/1629096.1629097},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {19},
numpages = {23},
keywords = {damping factor, PageRank, power method}
}

@article{10.1145/1508850.1508856,
author = {Shen, Jialie and Shepherd, John and Cui, Bin and Tan, Kian-Lee},
title = {A Novel Framework for Efficient Automated Singer Identification in Large Music Databases},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1508850.1508856},
doi = {10.1145/1508850.1508856},
abstract = {Over the past decade, there has been explosive growth in the availability of multimedia data, particularly image, video, and music. Because of this, content-based music retrieval has attracted attention from the multimedia database and information retrieval communities. Content-based music retrieval requires us to be able to automatically identify particular characteristics of music data. One such characteristic, useful in a range of applications, is the identification of the singer in a musical piece. Unfortunately, existing approaches to this problem suffer from either low accuracy or poor scalability. In this article, we propose a novel scheme, called Hybrid Singer Identifier (HSI), for efficient automated singer recognition. HSI uses multiple low-level features extracted from both vocal and nonvocal music segments to enhance the identification process; it achieves this via a hybrid architecture that builds profiles of individual singer characteristics based on statistical mixture models. An extensive experimental study on a large music database demonstrates the superiority of our method over state-of-the-art approaches in terms of effectiveness, efficiency, scalability, and robustness.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {18},
numpages = {31},
keywords = {classification, evaluation, statistical modeling, Gaussian mixture models, Music retrieval, EM algorithm, singer identification}
}

@article{10.1145/1508850.1508855,
author = {Huang, Zi and Shen, Heng Tao and Shao, Jie and Zhou, Xiaofang and Cui, Bin},
title = {Bounded Coordinate System Indexing for Real-Time Video Clip Search},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1508850.1508855},
doi = {10.1145/1508850.1508855},
abstract = {Recently, video clips have become very popular online. The massive influx of video clips has created an urgent need for video search engines to facilitate retrieving relevant clips. Different from traditional long videos, a video clip is a short video often expressing a moment of significance. Due to the high complexity of video data, efficient video clip search from large databases turns out to be very challenging. We propose a novel video clip representation model called the Bounded Coordinate System (BCS), which is the first single representative capturing the dominating content and content—changing trends of a video clip. It summarizes a video clip by a coordinate system, where each of its coordinate axes is identified by principal component analysis (PCA) and bounded by the range of data projections along the axis. The similarity measure of BCS considers the operations of translation, rotation, and scaling for coordinate system matching. Particularly, rotation and scaling reflect the difference of content tendencies. Compared with the quadratic time complexity of existing methods, the time complexity of measuring BCS similarity is linear. The compact video representation together with its linear similarity measure makes real-time search from video clip collections feasible. To further improve the retrieval efficiency for large video databases, a two-dimensional transformation method called Bidistance Transformation (BDT) is introduced to utilize a pair of optimal reference points with respect to bidirectional axes in BCS. Our extensive performance study on a large database of more than 30,000 video clips demonstrates that BCS achieves very high search accuracy according to human judgment. This indicates that content tendencies are important in determining the meanings of video clips and confirms that BCS can capture the inherent moment of video clip to some extent that better resembles human perception. In addition, BDT outperforms existing indexing methods greatly. Integration of the BCS model and BDT indexing can achieve real-time search from large video clip databases.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {17},
numpages = {33},
keywords = {query processing, summarization, Video search, indexing}
}

@article{10.1145/1508850.1508854,
author = {Hoi, Steven C. H. and Jin, Rong and Zhu, Jianke and Lyu, Michael R.},
title = {Semisupervised SVM Batch Mode Active Learning with Applications to Image Retrieval},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1508850.1508854},
doi = {10.1145/1508850.1508854},
abstract = {Support vector machine (SVM) active learning is one popular and successful technique for relevance feedback in content-based image retrieval (CBIR). Despite the success, conventional SVM active learning has two main drawbacks. First, the performance of SVM is usually limited by the number of labeled examples. It often suffers a poor performance for the small-sized labeled examples, which is the case in relevance feedback. Second, conventional approaches do not take into account the redundancy among examples, and could select multiple examples that are similar (or even identical). In this work, we propose a novel scheme for explicitly addressing the drawbacks. It first learns a kernel function from a mixture of labeled and unlabeled data, and therefore alleviates the problem of small-sized training data. The kernel will then be used for a batch mode active learning method to identify the most informative and diverse examples via a min-max framework. Two novel algorithms are proposed to solve the related combinatorial optimization: the first approach approximates the problem into a quadratic program, and the second solves the combinatorial optimization approximately by a greedy algorithm that exploits the merits of submodular functions. Extensive experiments with image retrieval using both natural photo images and medical images show that the proposed algorithms are significantly more effective than the state-of-the-art approaches. A demo is available at http://msm.cais.ntu.edu.sg/LSCBIR/.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {16},
numpages = {29},
keywords = {batch mode active learning, human-computer interaction, active learning, support vector machines, Content-based image retrieval, semisupervised learning}
}

@article{10.1145/1508850.1508853,
author = {Candan, K. Sel\c{c}uk and D\"{o}nderler, Mehmet E. and Hedgpeth, Terri and Kim, Jong Wook and Li, Qing and Sapino, Maria Luisa},
title = {SEA: Segment-Enrich-Annotate Paradigm for Adapting Dialog-Based Content for Improved Accessibility},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1508850.1508853},
doi = {10.1145/1508850.1508853},
abstract = {While navigation within complex information spaces is a problem for all users, the problem is most evident with individuals who are blind who cannot simply locate, point, and click on a link in hypertext documents with a mouse. Users who are blind have to listen searching for the link in the document using only the keyboard and a screen reader program, which may be particularly inefficient in large documents with many links or deep hierarchies that are hard to navigate. Consequently, they are especially penalized when the information being searched is hidden under multiple layers of indirections. In this article, we introduce a segment-enrich-annotate (SEA) paradigm for adapting digital content with deep structures for improved accessibility. In particular, we instantiate and evaluate this paradigm through the iCare-Assistant, an assistive system for helping students who are blind in accessing Web and electronic course materials. Our evaluations, involving the participation of students who are blind, showed that the iCare-Assistant system, built based on the SEA paradigm, reduces the navigational overhead significantly and enables user who are blind access complex online course servers effectively.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {15},
numpages = {45},
keywords = {educational discussion boards and Web sites, segmentation, assistive technology for blind users, annotation, Web navigational aids}
}

@article{10.1145/1508850.1508852,
author = {Shokouhi, Milad and Zobel, Justin},
title = {Robust Result Merging Using Sample-Based Score Estimates},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1508850.1508852},
doi = {10.1145/1508850.1508852},
abstract = {In federated information retrieval, a query is routed to multiple collections and a single answer list is constructed by combining the results. Such metasearch provides a mechanism for locating documents on the hidden Web and, by use of sampling, can proceed even when the collections are uncooperative. However, the similarity scores for documents returned from different collections are not comparable, and, in uncooperative environments, document scores are unlikely to be reported. We introduce a new merging method for uncooperative environments, in which similarity scores for the sampled documents held for each collection are used to estimate global scores for the documents returned per query. This method requires no assumptions about properties such as the retrieval models used. Using experiments on a wide range of collections, we show that in many cases our merging methods are significantly more effective than previous techniques.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {14},
numpages = {29},
keywords = {result fusion, Result merging, uncooperative collections, distributed information retrieval}
}

@article{10.1145/1508850.1508851,
author = {Kurland, Oren and Lee, Lillian},
title = {Clusters, Language Models, and Ad Hoc Information Retrieval},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1508850.1508851},
doi = {10.1145/1508850.1508851},
abstract = {The language-modeling approach to information retrieval provides an effective statistical framework for tackling various problems and often achieves impressive empirical performance. However, most previous work on language models for information retrieval focused on document-specific characteristics, and therefore did not take into account the structure of the surrounding corpus, a potentially rich source of additional information. We propose a novel algorithmic framework in which information provided by document-based language models is enhanced by the incorporation of information drawn from clusters of similar documents. Using this framework, we develop a suite of new algorithms. Even the simplest typically outperforms the standard language-modeling approach in terms of mean average precision (MAP) and recall, and our new interpolation algorithm posts statistically significant performance improvements for both metrics over all six corpora tested. An important aspect of our work is the way we model corpus structure. In contrast to most previous work on cluster-based retrieval that partitions the corpus, we demonstrate the effectiveness of a simple strategy based on a nearest-neighbors approach that produces overlapping clusters.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {13},
numpages = {39},
keywords = {cluster-based language models, clustering, Language modeling, aspect models, interpolation model, cluster hypothesis, smoothing}
}

@article{10.1145/1462198.1462204,
author = {Schumaker, Robert P. and Chen, Hsinchun},
title = {Textual Analysis of Stock Market Prediction Using Breaking Financial News: The AZFin Text System},
year = {2009},
issue_date = {February 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1462198.1462204},
doi = {10.1145/1462198.1462204},
abstract = {Our research examines a predictive machine learning approach for financial news articles analysis using several different textual representations: bag of words, noun phrases, and named entities. Through this approach, we investigated 9,211 financial news articles and 10,259,042 stock quotes covering the S&amp;P 500 stocks during a five week period. We applied our analysis to estimate a discrete stock price twenty minutes after a news article was released. Using a support vector machine (SVM) derivative specially tailored for discrete numeric prediction and models containing different stock-specific variables, we show that the model containing both article terms and stock price at the time of article release had the best performance in closeness to the actual future stock price (MSE 0.04261), the same direction of price movement as the future price (57.1% directional accuracy) and the highest return using a simulated trading engine (2.06% return). We further investigated the different textual representations and found that a Proper Noun scheme performs better than the de facto standard of Bag of Words in all three metrics.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {12},
numpages = {19},
keywords = {SVM, stock market, prediction}
}

@article{10.1145/1462198.1462203,
author = {Xue, Gui-Rong and Han, Jie and Yu, Yong and Yang, Qiang},
title = {User Language Model for Collaborative Personalized Search},
year = {2009},
issue_date = {February 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1462198.1462203},
doi = {10.1145/1462198.1462203},
abstract = {Traditional personalized search approaches rely solely on individual profiles to construct a user model. They are often confronted by two major problems: data sparseness and cold-start for new individuals. Data sparseness refers to the fact that most users only visit a small portion of Web pages and hence a very sparse user-term relationship matrix is generated, while cold-start for new individuals means that the system cannot conduct any personalization without previous browsing history. Recently, community-based approaches were proposed to use the group's social behaviors as a supplement to personalization. However, these approaches only consider the commonality of a group of users and still cannot satisfy the diverse information needs of different users. In this article, we present a new approach, called collaborative personalized search. It considers not only the commonality factor among users for defining group user profiles and global user profiles, but also the specialties of individuals. Then, a statistical user language model is proposed to integrate the individual model, group user model and global user model together. In this way, the probability that a user will like a Web page is calculated through a two-step smoothing mechanism. First, a global user model is used to smooth the probability of unseen terms in the individual profiles and provide aggregated behavior of global users. Then, in order to precisely describe individual interests by looking at the behaviors of similar users, users are clustered into groups and group-user models are constructed. The group-user models are integrated into an overall model through a cluster-based language model. The behaviors of the group users can be utilized to enhance the performance of personalized search. This model can alleviate the two aforementioned problems and provide a more effective personalized search than previous approaches. Large-scale experimental evaluations are conducted to show that the proposed approach substantially improves the relevance of a search over several competitive methods.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {11},
numpages = {28},
keywords = {Collaborative personalized search, clustering, user language model, data Sparseness, smoothing, cold-start}
}

@article{10.1145/1462198.1462202,
author = {Tryfonopoulos, Christos and Koubarakis, Manolis and Drougas, Yannis},
title = {Information Filtering and Query Indexing for an Information Retrieval Model},
year = {2009},
issue_date = {February 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1462198.1462202},
doi = {10.1145/1462198.1462202},
abstract = {In the information filtering paradigm, clients subscribe to a server with continuous queries or profiles that express their information needs. Clients can also publish documents to servers. Whenever a document is published, the continuous queries satisfying this document are found and notifications are sent to appropriate clients. This article deals with the filtering problem that needs to be solved efficiently by each server: Given a database of continuous queries db and a document d, find all queries q ∈ db that match d. We present data structures and indexing algorithms that enable us to solve the filtering problem efficiently for large databases of queries expressed in the model AWP. AWP is based on named attributes with values of type text, and its query language includes Boolean and word proximity operators.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {10},
numpages = {47},
keywords = {query indexing algorithms, selective dissemination of information, performance evaluation, Information filtering}
}

@article{10.1145/1462198.1462201,
author = {Chen, Chien Chin and Chen, Meng Chang and Chen, Ming-Syan},
title = {An Adaptive Threshold Framework for Event Detection Using HMM-Based Life Profiles},
year = {2009},
issue_date = {February 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1462198.1462201},
doi = {10.1145/1462198.1462201},
abstract = {When an event occurs, it attracts attention of information sources to publish related documents along its lifespan. The task of event detection is to automatically identify events and their related documents from a document stream, which is a set of chronologically ordered documents collected from various information sources. Generally, each event has a distinct activeness development so that its status changes continuously during its lifespan. When an event is active, there are a lot of related documents from various information sources. In contrast when it is inactive, there are very few documents, but they are focused. Previous works on event detection did not consider the characteristics of the event's activeness, and used rigid thresholds for event detection. We propose a concept called life profile, modeled by a hidden Markov model, to model the activeness trends of events. In addition, a general event detection framework, LIPED, which utilizes the learned life profiles and the burst-and-diverse characteristic to adjust the event detection thresholds adaptively, can be incorporated into existing event detection methods. Based on the official TDT corpus and contest rules, the evaluation results show that existing detection methods that incorporate LIPED achieve better performance in the cost and F1 metrics, than without.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {9},
numpages = {35},
keywords = {hidden Markov models, life profiles, TDT, clustering, Event detection, topic detection}
}

@article{10.1145/1462198.1462200,
author = {Park, Laurence A. F. and Ramamohanarao, Kotagiri},
title = {An Analysis of Latent Semantic Term Self-Correlation},
year = {2009},
issue_date = {February 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1462198.1462200},
doi = {10.1145/1462198.1462200},
abstract = {Latent semantic analysis (LSA) is a generalized vector space method that uses dimension reduction to generate term correlations for use during the information retrieval process. We hypothesized that even though the dimension reduction establishes correlations between terms, the dimension reduction is causing a degradation in the correlation of a term to itself (self-correlation). In this article, we have proven that there is a direct relationship to the size of the LSA dimension reduction and the LSA self-correlation. We have also shown that by altering the LSA term self-correlations we gain a substantial increase in precision, while also reducing the computation required during the information retrieval process.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {8},
numpages = {35},
keywords = {Latent semantic analysis, term correlation}
}

@article{10.1145/1462198.1462199,
author = {Rodriguez, Marko A. and Bollen, Johan and Sompel, Herbert Van De},
title = {Automatic Metadata Generation Using Associative Networks},
year = {2009},
issue_date = {February 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1462198.1462199},
doi = {10.1145/1462198.1462199},
abstract = {In spite of its tremendous value, metadata is generally sparse and incomplete, thereby hampering the effectiveness of digital information services. Many of the existing mechanisms for the automated creation of metadata rely primarily on content analysis which can be costly and inefficient. The automatic metadata generation system proposed in this article leverages resource relationships generated from existing metadata as a medium for propagation from metadata-rich to metadata-poor resources. Because of its independence from content analysis, it can be applied to a wide variety of resource media types and is shown to be computationally inexpensive. The proposed method operates through two distinct phases. Occurrence and cooccurrence algorithms first generate an associative network of repository resources leveraging existing repository metadata. Second, using the associative network as a substrate, metadata associated with metadata-rich resources is propagated to metadata-poor resources by means of a discrete-form spreading activation algorithm. This article discusses the general framework for building associative networks, an algorithm for disseminating metadata through such networks, and the results of an experiment and validation of the proposed method using a standard bibliographic dataset.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {7},
numpages = {20},
keywords = {Associative networks, metadata generation, particle-swarms}
}

@article{10.1145/1416950.1416956,
author = {Lin, Jimmy and Wu, Philip and Abels, Eileen},
title = {Toward Automatic Facet Analysis and Need Negotiation: Lessons from Mediated Search},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1416950.1416956},
doi = {10.1145/1416950.1416956},
abstract = {This work explores the hypothesis that interactions between a trained human search intermediary and an information seeker can inform the design of interactive IR systems. We discuss results from a controlled Wizard-of-Oz case study, set in the context of the TREC 2005 HARD track evaluation, in which a trained intermediary executed an integrated search and interaction strategy based on conceptual facet analysis and informed by need negotiation techniques common in reference interviews. Having a human “in the loop” yielded large improvements over fully automated systems as measured by standard ranked-retrieval metrics, demonstrating the value of mediated search. We present a detailed analysis of the intermediary's actions to gain a deeper understanding of what worked and why. One contribution is a taxonomy of clarification types informed both by empirical results and existing theories in library and information science. We discuss how these findings can guide the development of future systems. Overall, this work illustrates how studying human information-seeking processes can lead to better information retrieval applications.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {6},
numpages = {42},
keywords = {interactive information retrieval, Reference interview}
}

@article{10.1145/1416950.1416955,
author = {Kerne, Andruid and Koh, Eunyee and Smith, Steven M. and Webb, Andrew and Dworaczyk, Blake},
title = {CombinFormation: Mixed-Initiative Composition of Image and Text Surrogates Promotes Information Discovery},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1416950.1416955},
doi = {10.1145/1416950.1416955},
abstract = {combinFormation is a mixed-initiative creativity support tool for searching, browsing, organizing, and integrating information. Images and text are connected to represent surrogates (enhanced bookmarks), optimizing the use of human cognitive facilities. Composition, an alternative to lists and spatial hypertext, is used to represent a collection of surrogates as a connected whole, using principles from art and design. This facilitates the creative process of information discovery, in which humans develop new ideas while finding and collecting information. To provoke the user to think about the large space of potentially relevant information resources, a generative agent proactively engages in collecting information resources, forming image and text surrogates, and composing them visually. The agent develops the collection and its visual representation over time, enabling the user to see ideas and relationships. To keep the human in control, we develop interactive mechanisms for authoring the composition and directing the agent. In a field study in an interdisciplinary course on The Design Process, over a hundred students alternated using combinFormation and Google+Word to collect prior work on information discovery invention assignments. The students that used combinFormation's mixed-initiative composition of image and text surrogates performed better.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {5},
numpages = {45},
keywords = {mixed-initiative systems, field study, exploratory search, information discovery, focused crawler, software agents, clustering, Creativity support tools, semantics, creative cognition, relevance feedback, collections}
}

@article{10.1145/1416950.1416954,
author = {Yeh, Jui-Feng and Wu, Chung-Hsien and Yu, Liang-Chih and Lai, Yu-Sheng},
title = {Extended Probabilistic HAL with Close Temporal Association for Psychiatric Query Document Retrieval},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1416950.1416954},
doi = {10.1145/1416950.1416954},
abstract = {Psychiatric query document retrieval can assist individuals to locate query documents relevant to their depression-related problems efficiently and effectively. By referring to relevant documents, individuals can understand how to alleviate their depression-related symptoms according to recommendations from health professionals. This work presents an extended probabilistic Hyperspace Analog to Language (epHAL) model to achieve this aim. The epHAL incorporates the close temporal associations between words in query documents to represent word cooccurrence relationships in a high-dimensional context space. The information flow mechanism further combines the query words in the epHAL space to infer related words for effective information retrieval. The language model perplexity is considered as the criterion for model optimization. Finally, the epHAL is adopted for psychiatric query document retrieval, and indicates its superiority in information retrieval over traditional approaches.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {4},
numpages = {30},
keywords = {information flow, Information retrieval, Hyperspace Analog to Language (HAL) model, query documents}
}

@article{10.1145/1416950.1416953,
author = {Zheleva, Elena and Kolcz, Aleksander and Getoor, Lise},
title = {Trusting Spam Reporters: A Reporter-Based Reputation System for Email Filtering},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1416950.1416953},
doi = {10.1145/1416950.1416953},
abstract = {Spam is a growing problem; it interferes with valid email and burdens both email users and service providers. In this work, we propose a reactive spam-filtering system based on reporter reputation for use in conjunction with existing spam-filtering techniques. The system has a trust-maintenance component for users, based on their spam-reporting behavior. The challenge that we consider is that of maintaining a reliable system, not vulnerable to malicious users, that will provide early spam-campaign detection to reduce the costs incurred by users and systems. We report on the utility of a reputation system for spam filtering that makes use of the feedback of trustworthy users. We evaluate our proposed framework, using actual complaint feedback from a large population of users, and validate its spam-filtering performance on a collection of real email traffic over several weeks. To test the broader implication of the system, we create a model of the behavior of malicious reporters, and we simulate the system under various assumptions using a synthetic dataset.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {3},
numpages = {27},
keywords = {Spam filtering, trust, reputation systems}
}

@article{10.1145/1416950.1416952,
author = {Moffat, Alistair and Zobel, Justin},
title = {Rank-Biased Precision for Measurement of Retrieval Effectiveness},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1416950.1416952},
doi = {10.1145/1416950.1416952},
abstract = {A range of methods for measuring the effectiveness of information retrieval systems has been proposed. These are typically intended to provide a quantitative single-value summary of a document ranking relative to a query. However, many of these measures have failings. For example, recall is not well founded as a measure of satisfaction, since the user of an actual system cannot judge recall. Average precision is derived from recall, and suffers from the same problem. In addition, average precision lacks key stability properties that are needed for robust experiments. In this article, we introduce a new effectiveness metric, rank-biased precision, that avoids these problems. Rank-biased pre-cision is derived from a simple model of user behavior, is robust if answer rankings are extended to greater depths, and allows accurate quantification of experimental uncertainty, even when only partial relevance judgments are available.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {2},
numpages = {27},
keywords = {average precision, Recall, relevance, precision, pooling}
}

@article{10.1145/1416950.1416951,
author = {Piwowarski, Benjamin and Trotman, Andrew and Lalmas, Mounia},
title = {Sound and Complete Relevance Assessment for XML Retrieval},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1416950.1416951},
doi = {10.1145/1416950.1416951},
abstract = {In information retrieval research, comparing retrieval approaches requires test collections consisting of documents, user requests and relevance assessments. Obtaining relevance assessments that are as sound and complete as possible is crucial for the comparison of retrieval approaches. In XML retrieval, the problem of obtaining sound and complete relevance assessments is further complicated by the structural relationships between retrieval results.A major difference between XML retrieval and flat document retrieval is that the relevance of elements (the retrievable units) is not independent of that of related elements. This has major consequences for the gathering of relevance assessments. This article describes investigations into the creation of sound and complete relevance assessments for the evaluation of content-oriented XML retrieval as carried out at INEX, the evaluation campaign for XML retrieval. The campaign, now in its seventh year, has had three substantially different approaches to gather assessments and has finally settled on a highlighting method for marking relevant passages within documents—even though the objective is to collect assessments at element level. The different methods of gathering assessments at INEX are discussed and contrasted. The highlighting method is shown to be the most reliable of the methods.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {1},
numpages = {37},
keywords = {evaluation, XML retrieval, relevance assessment, passage retrieval, INEX, XML}
}

@article{10.1145/1402256.1402265,
author = {Marchionini, Gary},
title = {TOIS Reviewers June 2007 through May 2008},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1402256.1402265},
doi = {10.1145/1402256.1402265},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {26},
numpages = {2}
}

@article{10.1145/1402256.1402264,
author = {Marchionini, Gary},
title = {Editorial: Reviewer Merits and Review Control in an Age of Electronic Manuscript Management Systems},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1402256.1402264},
doi = {10.1145/1402256.1402264},
abstract = {Peer review is an important resource of scholarly communities and must be managed and nurtured carefully. Electronic manuscript management systems have begun to improve some aspects of workflow for conferences and journals but also raise issues related to reviewer roles and reputations and the control of reviews over time. Professional societies should make their policies related to reviews and reviewer histories clear to authors and reviewers, develop strategies and tools to facilitate good and timely reviews, and facilitate the training of new reviewers.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {25},
numpages = {6},
keywords = {Peer review, manuscript management systems}
}

@article{10.1145/1402256.1402263,
author = {Bernstein, Michael and Van Kleek, Max and Karger, David and Schraefel, M. C.},
title = {Information Scraps: How and Why Information Eludes Our Personal Information Management Tools},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1402256.1402263},
doi = {10.1145/1402256.1402263},
abstract = {In this article we investigate information scraps—personal information where content has been scribbled on Post-it notes, scrawled on the corners of sheets of paper, stuck in our pockets, sent in email messages to ourselves, and stashed in miscellaneous digital text files. Information scraps encode information ranging from ideas and sketches to notes, reminders, shipment tracking numbers, driving directions, and even poetry. Although information scraps are ubiquitous, we have much still to learn about these loose forms of information practice. Why do we keep information scraps outside of our traditional PIM applications? What role do information scraps play in our overall information practice? How might PIM applications be better designed to accommodate and support information scraps' creation, manipulation and retrieval?We pursued these questions by studying the information scrap practices of 27 knowledge workers at five organizations. Our observations shed light on information scraps' content, form, media, and location. From this data, we elaborate on the typical information scrap lifecycle, and identify common roles that information scraps play: temporary storage, archiving, work-in-progress, reminding, and management of unusual data. These roles suggest a set of unmet design needs in current PIM tools: lightweight entry, unconstrained content, flexible use and adaptability, visibility, and mobility.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {24},
numpages = {46},
keywords = {Personal information management, information scraps, note taking}
}

@article{10.1145/1402256.1402262,
author = {Hicks, B. J. and Dong, A. and Palmer, R. and Mcalpine, H. C.},
title = {Organizing and Managing Personal Electronic Files: A Mechanical Engineer's Perspective},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1402256.1402262},
doi = {10.1145/1402256.1402262},
abstract = {This article deals with the organization and management of the computer files handled by mechanical engineers on their personal computers. In engineering organizations, a wide variety of electronic files (documents) are necessary to support both business processes and the activities of design and manufacture. Whilst a large number of files and hence information is formally archived, a significant amount of additional information and knowledge resides in electronic files on personal computers. The widespread use of these personal information stores means that all information is retained. However, its reuse is problematic for all but the individual as a result of the naming and organization of the files. To begin to address this issue, a study of the use and current practices for managing personal electronic files is described. The study considers the fundamental classes of files handled by engineers and analyses the organization of these files across the personal computers of 40 participants. The study involves a questionnaire and an electronic audit. The results of these qualitative and quantitative elements are used to elicit an understanding of the practices and requirements of engineers for managing personal electronic files. A potential scheme for naming and organizing personal electronic files is discussed as one possible way to satisfy these requirements. The aim of the scheme is to balance the personal nature of data storage with the need for personal records to be shared with others to support knowledge reuse in engineering organizations. Although this article is concerned with mechanical engineers, the issues dealt with are relevant to knowledge-based industries and, in particular, teams of knowledge workers.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {23},
numpages = {40},
keywords = {file sharing and file recognition and recall, information management, directory and file naming conventions, Engineers}
}

@article{10.1145/1402256.1402261,
author = {Siersdorfer, Stefan and Sizov, Sergej},
title = {Meta Methods for Model Sharing in Personal Information Systems},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1402256.1402261},
doi = {10.1145/1402256.1402261},
abstract = {This article introduces a methodology for automatically organizing document collections into thematic categories for Personal Information Management (PIM) through collaborative sharing of machine learning models in an efficient and privacy-preserving way. Our objective is to combine multiple independently learned models from several users to construct an advanced ensemble-based decision model by taking the knowledge of multiple users into account in a decentralized manner, for example, in a peer-to-peer overlay network. High accuracy of the corresponding supervised (classification) and unsupervised (clustering) methods is achieved by restrictively leaving out uncertain documents rather than assigning them to inappropriate topics or clusters with low confidence. We introduce a formal probabilistic model for the resulting ensemble based meta methods and explain how it can be used for constructing estimators and for goal-oriented tuning. Comprehensive evaluation results on different reference data sets illustrate the viability of our approach.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {22},
numpages = {35},
keywords = {meta methods, peer-to-peer, clustering, restrictive methods, Classification, personal information management}
}

@article{10.1145/1402256.1402260,
author = {Elsweiler, David and Baillie, Mark and Ruthven, Ian},
title = {Exploring Memory in Email Refinding},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1402256.1402260},
doi = {10.1145/1402256.1402260},
abstract = {Human memory plays an important role in personal information management (PIM). Several scholars have noted that people refind information based on what they remember and it has been shown that people adapt their management strategies to compensate for the limitations of memory. Nevertheless, little is known about what people tend to remember about their personal information and how they use their memories to refind. The aim of this article is to increase our understanding of the role that memory plays in the process of refinding personal information. Concentrating on email re-finding, we report on a user study that investigates what attributes of email messages participants remember when trying to refind. We look at how the attributes change in different scenarios and examine the factors which impact on what is remembered.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {21},
numpages = {36},
keywords = {information refinding, Email refinding, memory, user study}
}

@article{10.1145/1402256.1402259,
author = {Bergman, Ofer and Beyth-Marom, Ruth and Nachmias, Rafi and Gradovitch, Noa and Whittaker, Steve},
title = {Improved Search Engines and Navigation Preference in Personal Information Management},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1402256.1402259},
doi = {10.1145/1402256.1402259},
abstract = {Traditionally users access their personal files mainly by using
folder navigation. We evaluate whether recent improvements in
desktop search have changed this fundamental aspect of Personal
Information Management (PIM). We tested this in two studies using
the same questionnaire: (a) The Windows Studya longitudinal
comparison of Google Desktop and Windows XP Search
Companion, and (b) The Mac Studya large scale comparison of Mac
Spotlight and Sherlock. There were few effects for
improved search. First, regardless of search engine, there was a
strong navigation preference: on average, users estimated that they
used navigation for 56-68% of file retrieval events but searched
for only 4-15% of events. Second, the effect of improving the
quality of the search engine on search usage was limited and
inconsistent. Third, search was used mainly as a last resort when
users could not remember file location. Finally, there was no
evidence that using improved desktop search engines leads people to
change their filing habits to become less reliant on hierarchical
file organization. We conclude by offering theoretical explanations
for navigation preference, relating to differences between PIM and
Internet retrieval, and suggest alternative design directions for
PIM systems.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {20},
numpages = {24},
keywords = {Personal information management, search preference, navigation preference, files retrieval, user study, personal search engines}
}

@article{10.1145/1402256.1402258,
author = {Teevan, Jaime},
title = {How People Recall, Recognize, and Reuse Search Results},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1402256.1402258},
doi = {10.1145/1402256.1402258},
abstract = {When a person issues a query, that person has expectations about the search results that will be returned. These expectations can be based on the current information need, but are also influenced by how the searcher believes the search engine works, where relevant results are expected to be ranked, and any previous searches the individual has run on the topic. This paper looks in depth at how the expectations people develop about search result lists during an initial query affect their perceptions of and interactions with future repeat search result lists. Three studies are presented that give insight into how people recall, recognize, and reuse results. The first study (a study of recall) explores what people recall about previously viewed search result lists. The second study (a study of recognition) builds on the first to reveal that people often recognize a result list as one they have seen before even when it is quite different. As long as those aspects that the searcher remembers about the initial list remain the same, other aspects can change significantly. This is advantageous because, as the third study (a study of reuse) shows, when a result list appears to have changed, people have trouble re-using the previously viewed content in the list. They are less likely to find what they are looking for, less happy with the result quality, more likely to find the task hard, and more likely to take a long time searching. Although apparent consistency is important for reuse, people's inability to recognize change makes consistency without stagnation possible. New relevant results can be presented where old results have been forgotten, making both old and new content easy to find.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {19},
numpages = {27},
keywords = {recall, Refinding, personal information management, recognition, search, dynamic information, reuse}
}

@article{10.1145/1402256.1402257,
author = {Barreau, Deborah and Capra, Robert and Dumais, Susan and Jones, William and P\'{e}rez-Qui\~{n}ones, Manuel},
title = {Introduction to Keeping, Refinding and Sharing Personal Information},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1402256.1402257},
doi = {10.1145/1402256.1402257},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {18},
numpages = {3}
}

@article{10.1145/1361684.1361690,
author = {Losada, David E. and Azzopardi, Leif},
title = {Assessing Multivariate Bernoulli Models for Information Retrieval},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1361684.1361690},
doi = {10.1145/1361684.1361690},
abstract = {Although the seminal proposal to introduce language modeling in information retrieval was based on a multivariate Bernoulli model, the predominant modeling approach is now centered on multinomial models. Language modeling for retrieval based on multivariate Bernoulli distributions is seen inefficient and believed less effective than the multinomial model. In this article, we examine the multivariate Bernoulli model with respect to its successor and examine its role in future retrieval systems. In the context of Bayesian learning, these two modeling approaches are described, contrasted, and compared both theoretically and computationally. We show that the query likelihood following a multivariate Bernoulli distribution introduces interesting retrieval features which may be useful for specific retrieval tasks such as sentence retrieval. Then, we address the efficiency aspect and show that algorithms can be designed to perform retrieval efficiently for multivariate Bernoulli models, before performing an empirical comparison to study the behaviorial aspects of the models. A series of comparisons is then conducted on a number of test collections and retrieval tasks to determine the empirical and practical differences between the different models. Our results indicate that for sentence retrieval the multivariate Bernoulli model can significantly outperform the multinomial model. However, for the other tasks the multinomial model provides consistently better performance (and in most cases significantly so). An analysis of the various retrieval characteristics reveals that the multivariate Bernoulli model tends to promote long documents whose nonquery terms are informative. While this is detrimental to the task of document retrieval (documents tend to contain considerable nonquery content), it is valuable for other tasks such as sentence retrieval, where the retrieved elements are very short and focused.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {17},
numpages = {46},
keywords = {multivariate Bernoulli, multinomial, Information retrieval, language models}
}

@article{10.1145/1361684.1361689,
author = {Wang, Jun and de Vries, Arjen P. and Reinders, Marcel J. T.},
title = {Unified Relevance Models for Rating Prediction in Collaborative Filtering},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1361684.1361689},
doi = {10.1145/1361684.1361689},
abstract = {Collaborative filtering aims at predicting a user's interest for a given item based on a collection of user profiles. This article views collaborative filtering as a problem highly related to information retrieval, drawing an analogy between the concepts of users and items in recommender systems and queries and documents in text retrieval.We present a probabilistic user-to-item relevance framework that introduces the concept of relevance into the related problem of collaborative filtering. Three different models are derived, namely, a user-based, an item-based, and a unified relevance model, and we estimate their rating predictions from three sources: the user's own ratings for different items, other users' ratings for the same item, and ratings from different but similar users for other but similar items.To reduce the data sparsity encountered when estimating the probability density function of the relevance variable, we apply the nonparametric (data-driven) density estimation technique known as the Parzen-window method (or kernel-based density estimation). Using a Gaussian window function, the similarity between users and/or items would, however, be based on Euclidean distance. Because the collaborative filtering literature has reported improved prediction accuracy when using cosine similarity, we generalize the Parzen-window method by introducing a projection kernel.Existing user-based and item-based approaches correspond to two simplified instantiations of our framework. User-based and item-based collaborative filterings represent only a partial view of the prediction problem, where the unified relevance model brings these partial views together under the same umbrella. Experimental results complement the theoretical insights with improved recommendation accuracy. The unified model is more robust to data sparsity because the different types of ratings are used in concert.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {16},
numpages = {42},
keywords = {Collaborative filtering, personalization, recommendation}
}

@article{10.1145/1361684.1361688,
author = {Altingovde, Ismail Sengor and Demir, Engin and Can, Fazli and Ulusoy, \"{O}zg\"{u}r},
title = {Incremental Cluster-Based Retrieval Using Compressed Cluster-Skipping Inverted Files},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1361684.1361688},
doi = {10.1145/1361684.1361688},
abstract = {We propose a unique cluster-based retrieval (CBR) strategy using a new cluster-skipping inverted file for improving query processing efficiency. The new inverted file incorporates cluster membership and centroid information along with the usual document information into a single structure. In our incremental-CBR strategy, during query evaluation, both best(-matching) clusters and the best(-matching) documents of such clusters are computed together with a single posting-list access per query term. As we switch from term to term, the best clusters are recomputed and can dynamically change. During query-document matching, only relevant portions of the posting lists corresponding to the best clusters are considered and the rest are skipped. The proposed approach is essentially tailored for environments where inverted files are compressed, and provides substantial efficiency improvement while yielding comparable, or sometimes better, effectiveness figures. Our experiments with various collections show that the incremental-CBR strategy using a compressed cluster-skipping inverted file significantly improves CPU time efficiency, regardless of query length. The new compressed inverted file imposes an acceptable storage overhead in comparison to a typical inverted file. We also show that our approach scales well with the collection size.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {15},
numpages = {36},
keywords = {query processing, cluster-skipping inverted index structure (CS-IIS), full search (FS), inverted index structure (IIS), cluster-based retrieval (CBR), index compression, Best match}
}

@article{10.1145/1361684.1361687,
author = {Melucci, Massimo},
title = {A Basis for Information Retrieval in Context},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1361684.1361687},
doi = {10.1145/1361684.1361687},
abstract = {Information retrieval (IR) models based on vector spaces have been investigated for a long time. Nevertheless, they have recently attracted much research interest. In parallel, context has been rediscovered as a crucial issue in information retrieval. This article presents a principled approach to modeling context and its role in ranking information objects using vector spaces. First, the article outlines how a basis of a vector space naturally represents context, both its properties and factors. Second, a ranking function computes the probability of context in the objects represented in a vector space, namely, the probability that a contextual factor has affected the preparation of an object.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {14},
numpages = {41},
keywords = {vector-space model, Personalization, probability, quantum mechanics}
}

@article{10.1145/1361684.1361686,
author = {Wu, Ho Chung and Luk, Robert Wing Pong and Wong, Kam Fai and Kwok, Kui Lam},
title = {Interpreting TF-IDF Term Weights as Making Relevance Decisions},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1361684.1361686},
doi = {10.1145/1361684.1361686},
abstract = {A novel probabilistic retrieval model is presented. It forms a basis to interpret the TF-IDF term weights as making relevance decisions. It simulates the local relevance decision-making for every location of a document, and combines all of these “local” relevance decisions as the “document-wide” relevance decision for the document. The significance of interpreting TF-IDF in this way is the potential to: (1) establish a unifying perspective about information retrieval as relevance decision-making; and (2) develop advanced TF-IDF-related term weights for future elaborate retrieval models. Our novel retrieval model is simplified to a basic ranking formula that directly corresponds to the TF-IDF term weights. In general, we show that the term-frequency factor of the ranking formula can be rendered into different term-frequency factors of existing retrieval systems. In the basic ranking formula, the remaining quantity - log p(r¯|t ∈ d) is interpreted as the probability of randomly picking a nonrelevant usage (denoted by r¯) of term t. Mathematically, we show that this quantity can be approximated by the inverse document-frequency (IDF). Empirically, we show that this quantity is related to IDF, using four reference TREC ad hoc retrieval data collections.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {13},
numpages = {37},
keywords = {Information retrieval, term weight, relevance decision}
}

@article{10.1145/1361684.1361685,
author = {Abbasi, Ahmed and Chen, Hsinchun and Salem, Arab},
title = {Sentiment Analysis in Multiple Languages: Feature Selection for Opinion Classification in Web Forums},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1361684.1361685},
doi = {10.1145/1361684.1361685},
abstract = {The Internet is frequently used as a medium for exchange of information and opinions, as well as propaganda dissemination. In this study the use of sentiment analysis methodologies is proposed for classification of Web forum opinions in multiple languages. The utility of stylistic and syntactic features is evaluated for sentiment classification of English and Arabic content. Specific feature extraction components are integrated to account for the linguistic characteristics of Arabic. The entropy weighted genetic algorithm (EWGA) is also developed, which is a hybridized genetic algorithm that incorporates the information-gain heuristic for feature selection. EWGA is designed to improve performance and get a better assessment of key features. The proposed features and techniques are evaluated on a benchmark movie review dataset and U.S. and Middle Eastern Web forum postings. The experimental results using EWGA with SVM indicate high performance levels, with accuracies of over 91% on the benchmark dataset as well as the U.S. and Middle Eastern forums. Stylistic features significantly enhanced performance across all testbeds while EWGA also outperformed other feature selection methods, indicating the utility of these features and techniques for document-level classification of sentiments.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {12},
numpages = {34},
keywords = {opinion mining, text classification, Sentiment analysis, feature selection}
}

@article{10.1145/1344411.1344417,
author = {Cohen, Sara and Domshlak, Carmel and Zwerdling, Naama},
title = {On Ranking Techniques for Desktop Search},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1344411.1344417},
doi = {10.1145/1344411.1344417},
abstract = {Users tend to store huge amounts of files, of various formats, on their personal computers. As a result, finding a specific, desired file within the file system is a challenging task. This article addresses the desktop search problem by considering various techniques for ranking results of a search query over the file system. First, basic ranking techniques, which are based on various file features (e.g., file name, access date, file size, etc.), are considered and their effectiveness is empirically analyzed. Next, two learning-based ranking schemes are presented, and are shown to be significantly more effective than the basic ranking methods. Finally, a novel ranking technique, based on query selectiveness, is considered for use during the cold-start period of the system. This method is also shown to be empirically effective, even though it does not involve any learning.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {11},
numpages = {24},
keywords = {personal information management, ranking, Desktop search}
}

@article{10.1145/1344411.1344416,
author = {Wang, Xuanhui and Tao, Tao and Sun, Jian-Tao and Shakery, Azadeh and Zhai, Chengxiang},
title = {DirichletRank: Solving the Zero-One Gap Problem of PageRank},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1344411.1344416},
doi = {10.1145/1344411.1344416},
abstract = {Link-based ranking algorithms are among the most important techniques to improve web search. In particular, the PageRank algorithm has been successfully used in the Google search engine and has been attracting much attention recently. However, we find that PageRank has a “zero-one gap” problem which, to the best of our knowledge, has not been addressed in any previous work. This problem can be potentially exploited to spam PageRank results and make the state-of-the-art link-based antispamming techniques ineffective. The zero-one gap problem arises as a result of the current ad hoc way of computing transition probabilities in the random surfing model. We therefore propose a novel DirichletRank algorithm which calculates these probabilities using Bayesian estimation with a Dirichlet prior. DirichletRank is a variant of PageRank, but does not have the problem of zero-one gap and can be analytically shown substantially more resistant to some link spams than PageRank. Experiment results on TREC data show that DirichletRank can achieve better retrieval accuracy than PageRank due to its more reasonable allocation of transition probabilities. More importantly, experiments on the TREC dataset and another real web dataset from the Webgraph project show that, compared with the original PageRank, DirichletRank is more stable under link perturbation and is significantly more robust against both manually identified web spams and several simulated link spams. DirichletRank can be computed as efficiently as PageRank, and thus is scalable to large-scale web applications.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {10},
numpages = {29},
keywords = {link analysis, spamming, DirichletRank, zero-one gap, PageRank}
}

@article{10.1145/1344411.1344415,
author = {Moura, Edleno Silva de and Santos, Celia Francisca dos and Araujo, Bruno Dos santos de and Silva, Altigran Soares da and Calado, Pavel and Nascimento, Mario A.},
title = {Locality-Based Pruning Methods for Web Search},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1344411.1344415},
doi = {10.1145/1344411.1344415},
abstract = {This article discusses a novel approach developed for static index pruning that takes into account the locality of occurrences of words in the text. We use this new approach to propose and experiment on simple and effective pruning methods that allow a fast construction of the pruned index. The methods proposed here are especially useful for pruning in environments where the document database changes continuously, such as large-scale web search engines. Extensive experiments are presented showing that the proposed methods can achieve high compression rates while maintaining the quality of results for the most common query types present in modern search engines, namely, conjunctive and phrase queries. In the experiments, our locality-based pruning approach allowed reducing search engine indices to 30% of their original size, with almost no reduction in precision at the top answers. Furthermore, we conclude that even an extremely simple locality-based pruning method can be competitive when compared to complex methods that do not rely on locality information.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {9},
numpages = {28},
keywords = {Pruning, information retrieval, web search, indexing, search engines, search}
}

@article{10.1145/1344411.1344414,
author = {Lau, Raymond Y. K. and Bruza, Peter D. and Song, Dawei},
title = {Towards a Belief-Revision-Based Adaptive and Context-Sensitive Information Retrieval System},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1344411.1344414},
doi = {10.1145/1344411.1344414},
abstract = {In an adaptive information retrieval (IR) setting, the information seekers' beliefs about which terms are relevant or nonrelevant will naturally fluctuate. This article investigates how the theory of belief revision can be used to model adaptive IR. More specifically, belief revision logic provides a rich representation scheme to formalize retrieval contexts so as to disambiguate vague user queries. In addition, belief revision theory underpins the development of an effective mechanism to revise user profiles in accordance with information seekers' changing information needs. It is argued that information retrieval contexts can be extracted by means of the information-flow text mining method so as to realize a highly autonomous adaptive IR system. The extra bonus of a belief-based IR model is that its retrieval behavior is more predictable and explanatory. Our initial experiments show that the belief-based adaptive IR system is as effective as a classical adaptive IR system. To our best knowledge, this is the first successful implementation and evaluation of a logic-based adaptive IR model which can efficiently process large IR collections.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {8},
numpages = {38},
keywords = {text mining, information flow, retrieval context, Belief revision, adaptive information retrieval}
}

@article{10.1145/1344411.1344413,
author = {Abbasi, Ahmed and Chen, Hsinchun},
title = {Writeprints: A Stylometric Approach to Identity-Level Identification and Similarity Detection in Cyberspace},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1344411.1344413},
doi = {10.1145/1344411.1344413},
abstract = {One of the problems often associated with online anonymity is that it hinders social accountability, as substantiated by the high levels of cybercrime. Although identity cues are scarce in cyberspace, individuals often leave behind textual identity traces. In this study we proposed the use of stylometric analysis techniques to help identify individuals based on writing style. We incorporated a rich set of stylistic features, including lexical, syntactic, structural, content-specific, and idiosyncratic attributes. We also developed the Writeprints technique for identification and similarity detection of anonymous identities. Writeprints is a Karhunen-Loeve transforms-based technique that uses a sliding window and pattern disruption algorithm with individual author-level feature sets. The Writeprints technique and extended feature set were evaluated on a testbed encompassing four online datasets spanning different domains: email, instant messaging, feedback comments, and program code. Writeprints outperformed benchmark techniques, including SVM, Ensemble SVM, PCA, and standard Karhunen-Loeve transforms, on the identification and similarity detection tasks with accuracy as high as 94% when differentiating between 100 authors. The extended feature set also significantly outperformed a baseline set of features commonly used in previous research. Furthermore, individual-author-level feature sets generally outperformed use of a single group of attributes.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {7},
numpages = {29},
keywords = {style classification, discourse, Stylometry, online text, text mining}
}

@article{10.1145/1344411.1344412,
author = {Ipeirotis, Panagiotis G. and Gravano, Luis},
title = {Classification-Aware Hidden-Web Text Database Selection},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1344411.1344412},
doi = {10.1145/1344411.1344412},
abstract = {Many valuable text databases on the web have noncrawlable contents that are “hidden” behind search interfaces. Metasearchers are helpful tools for searching over multiple such “hidden-web” text databases at once through a unified query interface. An important step in the metasearching process is database selection, or determining which databases are the most relevant for a given user query. The state-of-the-art database selection techniques rely on statistical summaries of the database contents, generally including the database vocabulary and associated word frequencies. Unfortunately, hidden-web text databases typically do not export such summaries, so previous research has developed algorithms for constructing approximate content summaries from document samples extracted from the databases via querying. We present a novel “focused-probing” sampling algorithm that detects the topics covered in a database and adaptively extracts documents that are representative of the topic coverage of the database. Our algorithm is the first to construct content summaries that include the frequencies of the words in the database. Unfortunately, Zipf's law practically guarantees that for any relatively large database, content summaries built from moderately sized document samples will fail to cover many low-frequency words; in turn, incomplete content summaries might negatively affect the database selection process, especially for short queries with infrequent words. To enhance the sparse document samples and improve the database selection decisions, we exploit the fact that topically similar databases tend to have similar vocabularies, so samples extracted from databases with a similar topical focus can complement each other. We have developed two database selection algorithms that exploit this observation. The first algorithm proceeds hierarchically and selects the best categories for a query, and then sends the query to the appropriate databases in the chosen categories. The second algorithm uses “shrinkage,” a statistical technique for improving parameter estimation in the face of sparse data, to enhance the database content summaries with category-specific words. We describe how to modify existing database selection algorithms to adaptively decide (at runtime) whether shrinkage is beneficial for a query. A thorough evaluation over a variety of databases, including 315 real web databases as well as TREC data, suggests that the proposed sampling methods generate high-quality content summaries and that the database selection algorithms produce significantly more relevant database selection decisions and overall search results than existing algorithms.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {6},
numpages = {66},
keywords = {Distributed information retrieval, web search, database selection}
}

@article{10.1145/1292591.1292596,
author = {Darwish, Kareem and Magdy, Walid},
title = {Error Correction vs. Query Garbling for Arabic OCR Document Retrieval},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1292591.1292596},
doi = {10.1145/1292591.1292596},
abstract = {Due to the existence of large numbers of legacy documents (such as old books and newspapers), improving retrieval effectiveness for OCR'ed documents continues to be an important problem. This article compares the effect of OCR error correction with and without language modeling and the effect of query garbling with weighted structured queries on the retrieval of OCR degraded Arabic documents. The results suggest that moderate error correction does not yield statistically significant improvement in retrieval effectiveness when indexing and searching using n-grams. Also, reversing error correction models to perform query garbling in conjunction with weighted structured queries yields improved retrieval effectiveness. Lastly, using very good error correction that utilizes language modeling yields the best improvement in retrieval effectiveness.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
pages = {5–es},
numpages = {14},
keywords = {Arabic Retrieval, OCR Retrieval, OCR Correction}
}

@article{10.1145/1292591.1292595,
author = {Im, Il and Hars, Alexander},
title = {Does a One-Size Recommendation System Fit All? The Effectiveness of Collaborative Filtering Based Recommendation Systems across Different Domains and Search Modes},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1292591.1292595},
doi = {10.1145/1292591.1292595},
abstract = {Collaborative filtering (CF) is a personalization technology that generates recommendations for users based on others' evaluations. CF is used by numerous e-commerce Web sites for providing personalized recommendations. Although much research has focused on refining collaborative filtering algorithms, little is known about the effects of user and domain characteristics on the accuracy of collaborative filtering systems. In this study, the effects of two factors—product domain and users' search mode—on the accuracy of CF are investigated. The effects of those factors are tested using data collected from two experiments in two different product domains, and from two large CF datasets, EachMovie and Book-Crossing. The study shows that the search mode of the users strongly influences the accuracy of the recommendations. CF works better when users look for specific information than when they search for general information. The accuracy drops significantly when data from different modes are mixed. The study also shows that CF is more accurate for knowledge domains than for consumer product domains. The results of this study imply that for more accurate recommendations, collaborative filtering systems should be able to identify and handle users' mode of search, even within the same domain and user group.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
pages = {4–es},
numpages = {30},
keywords = {Collaborative filtering, recommendation systems}
}

@article{10.1145/1292591.1292594,
author = {Agosti, Maristella and Ferro, Nicola},
title = {A Formal Model of Annotations of Digital Content},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1292591.1292594},
doi = {10.1145/1292591.1292594},
abstract = {This article is a study of the themes and issues concerning the annotation of digital contents, such as textual documents, images, and multimedia documents in general. These digital contents are automatically managed by different kinds of digital library management systems and more generally by different kinds of information management systems.Even though this topic has already been partially studied by other researchers, the previous research work on annotations has left many open issues. These issues concern the lack of clarity about what an annotation is, what its features are, and how it is used. These issues are mainly due to the fact that models and systems for annotations have only been developed for specific purposes. As a result, there is only a fragmentary picture of the annotation and its management, and this is tied to specific contexts of use and lacks-general validity.The aim of the article is to provide a unified and integrated picture of the annotation, ranging from defining what an annotation is to providing a formal model. The key ideas of the model are: the distinction between the meaning and the sign of the annotation, which represent the semantics and the materialization of an annotation, respectively; the clear formalization of the temporal dimension involved with annotations; and the introduction of a distributed hypertext between digital contents and annotations. Therefore, the proposed formal model captures both syntactic and semantic aspects of the annotations. Furthermore, it is built on previously existing models and may be seen as an extension of them.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
pages = {3–es},
numpages = {57},
keywords = {Annotation, foundations, digital content, hypertext, digital library system}
}

@article{10.1145/1292591.1292593,
author = {Pirkola, Ari and Toivonen, Jarmo and Keskustalo, Heikki and J\"{a}rvelin, Kalervo},
title = {Frequency-Based Identification of Correct Translation Equivalents (FITE) Obtained through Transformation Rules},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1292591.1292593},
doi = {10.1145/1292591.1292593},
abstract = {We devised a novel statistical technique for the identification of the translation equivalents of source words obtained by transformation rule based translation (TRT). The effectiveness of the technique called frequency-based identification of translation equivalents (FITE) was tested using biological and medical cross-lingual spelling variants and out-of-vocabulary (OOV) words in Spanish-English and Finnish-English TRT. The results showed that, depending on the source language and frequency corpus, FITE-TRT (the identification of translation equivalents from TRT's translation set by means of the FITE technique) may achieve high translation recall. In the case of the Web as the frequency corpus, translation recall was 89.2%--91.0% for Spanish-English FITE-TRT. For both language pairs FITE-TRT achieved high translation precision: 95.0%--98.8%. The technique also reliably identified native source language words: source words that cannot be correctly translated by TRT. Dictionary-based CLIR augmented with FITE-TRT performed substantially better than basic dictionary-based CLIR where OOV keys were kept intact. FITE-TRT with Web document frequencies was the best technique among several fuzzy translation/matching approaches tested in cross-language retrieval experiments. We also discuss the application of FITE-TRT in the automatic construction of multilingual dictionaries.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
pages = {2–es},
numpages = {25},
keywords = {Cross-language information retrieval, OOV words, fuzzy matching, transformation rules, transliteration}
}

@article{10.1145/1292591.1292592,
author = {Jensen, Eric C. and Beitzel, Steven M. and Chowdhury, Abdur and Frieder, Ophir},
title = {Repeatable Evaluation of Search Services in Dynamic Environments},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1292591.1292592},
doi = {10.1145/1292591.1292592},
abstract = {In dynamic environments, such as the World Wide Web, a changing document collection, query population, and set of search services demands frequent repetition of search effectiveness (relevance) evaluations. Reconstructing static test collections, such as in TREC, requires considerable human effort, as large collection sizes demand judgments deep into retrieved pools. In practice it is common to perform shallow evaluations over small numbers of live engines (often pairwise, engine A vs. engine B) without system pooling. Although these evaluations are not intended to construct reusable test collections, their utility depends on conclusions generalizing to the query population as a whole. We leverage the bootstrap estimate of the reproducibility probability of hypothesis tests in determining the query sample sizes required to ensure this, finding they are much larger than those required for static collections. We propose a semiautomatic evaluation framework to reduce this effort. We validate this framework against a manual evaluation of the top ten results of ten Web search engines across 896 queries in navigational and informational tasks. Augmenting manual judgments with pseudo-relevance judgments mined from Web taxonomies reduces both the chances of missing a correct pairwise conclusion, and those of finding an errant conclusion, by approximately 50%.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
pages = {1–es},
numpages = {38},
keywords = {Web search, Evaluation}
}

@article{10.1145/1281485.1281491,
author = {Gerstel, Ori and Kutten, Shay and Laber, Eduardo Sany and Matichin, Rachel and Peleg, David and Pessoa, Artur Alves and Souza, Criston},
title = {Reducing Human Interactions in Web Directory Searches},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1281485.1281491},
doi = {10.1145/1281485.1281491},
abstract = {Consider a website containing a collection of webpages with data such as in Yahoo or the Open Directory project. Each page is associated with a weight representing the frequency with which that page is accessed by users. In the tree hierarchy representation, accessing each page requires the user to travel along the path leading to it from the root. By enhancing the index tree with additional edges (hotlinks) one may reduce the access cost of the system. In other words, the hotlinks reduce the expected number of steps needed to reach a leaf page from the tree root, assuming that the user knows which hotlinks to take. The hotlink enhancement problem involves finding a set of hotlinks minimizing this cost.This article proposes the first exact algorithm for the hotlink enhancement problem. This algorithm runs in polynomial time for trees with logarithmic depth. Experiments conducted with real data show that significant improvement in the expected number of accesses per search can be achieved in websites using this algorithm. These experiments also suggest that the simple and much faster heuristic proposed previously by Czyzowicz et al. [2003] creates hotlinks that are nearly optimal in the time savings they provide to the user.The version of the hotlink enhancement problem in which the weight distribution on the leaves is unknown is discussed as well. We present a polynomial-time algorithm that is optimal for any tree for any depth.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {20–es},
numpages = {28},
keywords = {hyperlink, hotlist, Hotlink, directory tree, algorithms}
}

@article{10.1145/1281485.1281490,
author = {Pinto, Alberto and Haus, Goffredo},
title = {A Novel XML Music Information Retrieval Method Using Graph Invariants},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1281485.1281490},
doi = {10.1145/1281485.1281490},
abstract = {The increasing diffusion of XML languages for the encoding of domain-specific multimedia information raises the need for new information retrieval models that can fully exploit structural information. An XML language specifically designed for music like MX allows queries to be made directly on the thematic material. The main advantage of such a system is that it can handle symbolic, notational, and audio objects at the same time through a multilayered structure. On the model side, common music information retrieval methods do not take into account the inner structure of melodic themes and the metric relationships between notes.In this article we deal with two main topics: a novel architecture based on a new XML language for music and a new model of melodic themes based on graph theory.This model takes advantage of particular graph invariants that can be linked to melodic themes as metadata in order to characterize all their possible modifications through specific transformations and that can be exploited in filtering algorithms. We provide a similarity function and show through an evaluation stage how it improves existing methods, particularly in the case of same-structured themes.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {19–es},
numpages = {44},
keywords = {XML, music information retrieval, Graphs, music, invariants, structural properties, melodic similarity, metadata}
}

@article{10.1145/1281485.1281489,
author = {Majumder, Prasenjit and Mitra, Mandar and Parui, Swapan K. and Kole, Gobinda and Mitra, Pabitra and Datta, Kalyankumar},
title = {YASS: Yet Another Suffix Stripper},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1281485.1281489},
doi = {10.1145/1281485.1281489},
abstract = {Stemmers attempt to reduce a word to its stem or root form and are used widely in information retrieval tasks to increase the recall rate. Most popular stemmers encode a large number of language-specific rules built over a length of time. Such stemmers with comprehensive rules are available only for a few languages. In the absence of extensive linguistic resources for certain languages, statistical language processing tools have been successfully used to improve the performance of IR systems. In this article, we describe a clustering-based approach to discover equivalence classes of root words and their morphological variants. A set of string distance measures are defined, and the lexicon for a given text collection is clustered using the distance measures to identify these equivalence classes. The proposed approach is compared with Porter's and Lovin's stemmers on the AP and WSJ subcollections of the Tipster dataset using 200 queries. Its performance is comparable to that of Porter's and Lovin's stemmers, both in terms of average precision and the total number of relevant documents retrieved. The proposed stemming algorithm also provides consistent improvements in retrieval performance for French and Bengali, which are currently resource-poor.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {18–es},
numpages = {20},
keywords = {corpus, stemming, Indian languages, string similarity, clustering, Bengali, French}
}

@article{10.1145/1281485.1281488,
author = {Fang, Xiao and Sheng, Olivia R. Liu and Chau, Michael},
title = {ServiceFinder: A Method towards Enhancing Service Portals},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1281485.1281488},
doi = {10.1145/1281485.1281488},
abstract = {The rapid advancement of Internet technologies enables more and more educational institutes, companies, and government agencies to provide services, namely online services, through web portals. With hundreds of online services provided through a web portal, it is critical to design web portals, namely service portals, through which online services can be easily accessed by their consumers. This article addresses this critical issue from the perspective of service selection, that is, how to select a small number of service-links (i.e., hyperlinks pointing to online services) to be featured in the homepage of a service portal such that users can be directed to find the online services they seek most effectively. We propose a mathematically formulated metric to measure the effectiveness of the selected service-links in directing users to locate their desired online services and formally define the service selection problem. A solution method, ServiceFinder, is then proposed. Using real-world data obtained from the Utah State Government service portal, we show that ServiceFinder outperforms both the current practice of service selection and previous algorithms for adaptive website design. We also show that the performance of ServiceFinder is close to that of the optimal solution resulting from exhaustive search.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {17–es},
numpages = {28},
keywords = {service selection, online service, Service portal}
}

@article{10.1145/1281485.1281487,
author = {Bailey, Christopher and Hall, Wendy and Millard, David E. and Weal, Mark J.},
title = {Adaptive Hypermedia through Contextualized Open Hypermedia Structures},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1281485.1281487},
doi = {10.1145/1281485.1281487},
abstract = {The aim of this article is to produce an alternative view of the adaptive hypermedia (AH) domain from a contextually-aware open hypermedia (OH) perspective. We believe that a wide range of AH techniques can be supported with a small number of OH structures, which can be combined together to create more complex effects, possibly simplifying the development of new AH systems.In this work we reexamine Brusilovsky's taxonomy of AH techniques from a structural OH perspective. We also show that it is possible to identify and model common structures across the taxonomy of adaptive techniques. An agent-based adaptive hypermedia system called HA3L is presented, which uses these OH structures to provide a straightforward implementation of a variety of adaptive hypermedia techniques. This enables us to reflect on the structural equivalence of many of the techniques, demonstrates the advantages of the OH approach, and can inform the design of future adaptive hypermedia systems.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {16–es},
numpages = {36},
keywords = {hypermedia structure, open hypermedia, Adaptive techniques, adaptive hypermedia, FOHM}
}

@article{10.1145/1281485.1281486,
author = {Marchionini, Gary},
title = {TOIS Reviewers January 2006 through May 2007},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1281485.1281486},
doi = {10.1145/1281485.1281486},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {15–es},
numpages = {3}
}

@article{10.1145/1247715.1247720,
author = {Jones, Rosie and Diaz, Fernando},
title = {Temporal Profiles of Queries},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1247715.1247720},
doi = {10.1145/1247715.1247720},
abstract = {Documents with timestamps, such as email and news, can be placed along a timeline. The timeline for a set of documents returned in response to a query gives an indication of how documents relevant to that query are distributed in time. Examining the timeline of a query result set allows us to characterize both how temporally dependent the topic is, as well as how relevant the results are likely to be. We outline characteristic patterns in query result set timelines, and show experimentally that we can automatically classify documents into these classes. We also show that properties of the query result set timeline can help predict the mean average precision of a query. These results show that meta-features associated with a query can be combined with text retrieval techniques to improve our understanding and treatment of text search on documents with timestamps.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {14–es},
numpages = {31},
keywords = {precision prediction, language models, event detection, query classification, ambiguity, temporal profiles, Time}
}

@article{10.1145/1247715.1247719,
author = {He, Ben and Ounis, Iadh},
title = {On Setting the Hyper-Parameters of Term Frequency Normalization for Information Retrieval},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1247715.1247719},
doi = {10.1145/1247715.1247719},
abstract = {The setting of the term frequency normalization hyper-parameter suffers from the query dependence and collection dependence problems, which remarkably hurt the robustness of the retrieval performance. Our study in this article investigates three term frequency normalization methods, namely normalization 2, BM25's normalization and the Dirichlet Priors normalization. We tackle the query dependence problem by modifying the query term weight using a Divergence From Randomness term weighting model, and tackle the collection dependence problem by measuring the correlation of the normalized term frequency with the document length. Our research hypotheses for the two problems, as well as an automatic hyper-parameter setting methodology, are extensively validated and evaluated on four Text REtrieval Conference (TREC) collections.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {13–es},
numpages = {24},
keywords = {information retrieval models, Query-dependence, relevance feedback, collection-dependence, TREC experimentation, term frequency normalization}
}

@article{10.1145/1247715.1247718,
author = {Zhou, Changqing and Frankowski, Dan and Ludford, Pamela and Shekhar, Shashi and Terveen, Loren},
title = {Discovering Personally Meaningful Places: An Interactive Clustering Approach},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1247715.1247718},
doi = {10.1145/1247715.1247718},
abstract = {The discovery of a person's meaningful places involves obtaining the physical locations and their labels for a person's places that matter to his daily life and routines. This problem is driven by the requirements from emerging location-aware applications, which allow a user to pose queries and obtain information in reference to places, for example, “home”, “work” or “Northwest Health Club”. It is a challenge to map from physical locations to personally meaningful places due to a lack of understanding of what constitutes the real users' personally meaningful places. Previous work has explored algorithms to discover personal places from location data. However, we know of no systematic empirical evaluations of these algorithms, leaving designers of location-aware applications in the dark about their choices.Our work remedies this situation. We extended a clustering algorithm to discover places. We also defined a set of essential evaluation metrics and an interactive evaluation framework. We then conducted a large-scale experiment that collected real users' location data and personally meaningful places, and illustrated the utility of our evaluation framework. Our results establish a baseline that future work can measure itself against. They also demonstrate that that our algorithm discovers places with reasonable accuracy and outperforms the well-known K-Means clustering algorithm for place discovery. Finally, we provide evidence that shapes more complex than “points” are required to represent the full range of people's everyday places.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {12–es},
numpages = {31},
keywords = {place discovery, clustering algorithms, field studies, location-aware applications, Ubiquitous computing}
}

@article{10.1145/1247715.1247717,
author = {Cormack, Gordon V. and Lynam, Thomas R.},
title = {Online Supervised Spam Filter Evaluation},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1247715.1247717},
doi = {10.1145/1247715.1247717},
abstract = {Eleven variants of six widely used open-source spam filters are tested on a chronological sequence of 49086 e-mail messages received by an individual from August 2003 through March 2004. Our approach differs from those previously reported in that the test set is large, comprises uncensored raw messages, and is presented to each filter sequentially with incremental feedback. Misclassification rates and Receiver Operating Characteristic Curve measurements are reported, with statistical confidence intervals. Quantitative results indicate that content-based filters can eliminate 98% of spam while incurring 0.1% legitimate email loss. Qualitative results indicate that the risk of loss depends on the nature of the message, and that messages likely to be lost may be those that are less critical. More generally, our methodology has been encapsulated in a free software toolkit, which may used to conduct similar experiments.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {11–es},
numpages = {31},
keywords = {Spam, text classification, email}
}

@article{10.1145/1247715.1247716,
author = {Baralis, Elena and Garza, Paolo and Quintarelli, Elisa and Tanca, Letizia},
title = {Answering XML Queries by Means of Data Summaries},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1247715.1247716},
doi = {10.1145/1247715.1247716},
abstract = {XML is a rather verbose representation of semistructured data, which may require huge amounts of storage space. We propose a summarized representation of XML data, based on the concept of instance pattern, which can both provide succinct information and be directly queried. The physical representation of instance patterns exploits itemsets or association rules to summarize the content of XML datasets. Instance patterns may be used for (possibly partially) answering queries, either when fast and approximate answers are required, or when the actual dataset is not available, for example, it is currently unreachable. Experiments on large XML documents show that instance patterns allow a significant reduction in storage space, while preserving almost entirely the completeness of the query result. Furthermore, they provide fast query answers and show good scalability on the size of the dataset, thus overcoming the document size limitation of most current XQuery engines.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {10–es},
numpages = {33},
keywords = {data summarization, intensional answers, Association rules, data mining, itemsets, semistructured data}
}

@article{10.1145/1229179.1229183,
author = {Beitzel, Steven M. and Jensen, Eric C. and Lewis, David D. and Chowdhury, Abdur and Frieder, Ophir},
title = {Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1229179.1229183},
doi = {10.1145/1229179.1229183},
abstract = {Accurate topical classification of user queries allows for increased effectiveness and efficiency in general-purpose Web search systems. Such classification becomes critical if the system must route queries to a subset of topic-specific and resource-constrained back-end databases. Successful query classification poses a challenging problem, as Web queries are short, thus providing few features. This feature sparseness, coupled with the constantly changing distribution and vocabulary of queries, hinders traditional text classification. We attack this problem by combining multiple classifiers, including exact lookup and partial matching in databases of manually classified frequent queries, linear models trained by supervised learning, and a novel approach based on mining selectional preferences from a large unlabeled query log. Our approach classifies queries without using external sources of information, such as online Web directories or the contents of retrieved pages, making it viable for use in demanding operational environments, such as large-scale Web search services. We evaluate our approach using a large sample of queries from an operational Web search engine and show that our combined method increases recall by nearly 40% over the best single method while maintaining adequate precision. Additionally, we compare our results to those from the 2005 KDD Cup and find that we perform competitively despite our operational restrictions. This suggests it is possible to topically classify a significant portion of the query stream without requiring external sources of information, allowing for deployment in operationally restricted environments.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {9–es},
numpages = {29}
}

@article{10.1145/1229179.1229182,
author = {Cui, Hang and Kan, Min-Yen and Chua, Tat-Seng},
title = {Soft Pattern Matching Models for Definitional Question Answering},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1229179.1229182},
doi = {10.1145/1229179.1229182},
abstract = {We explore probabilistic lexico-syntactic pattern matching, also known as soft pattern matching, in a definitional question answering system. Most current systems use regular expression-based hard matching patterns to identify definition sentences. Such rigid surface matching often fares poorly when faced with language variations. We propose two soft matching models to address this problem: one based on bigrams and the other on the Profile Hidden Markov Model (PHMM). Both models provide a theoretically sound method to model pattern matching as a probabilistic process that generates token sequences. We demonstrate the effectiveness of the models on definition sentence retrieval for definitional question answering. We show that both models significantly outperform the state-of-the-art manually constructed hard matching patterns on recent TREC data.A critical difference between the two models is that the PHMM has a more complex topology. We experimentally show that the PHMM can handle language variations more effectively but requires more training data to converge.While we evaluate soft pattern models only on definitional question answering, we believe that both models are generic and can be extended to other areas where lexico-syntactic pattern matching can be applied.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {8–es},
numpages = {30},
keywords = {Soft patterns, definitional question answering}
}

@article{10.1145/1229179.1229181,
author = {Joachims, Thorsten and Granka, Laura and Pan, Bing and Hembrooke, Helene and Radlinski, Filip and Gay, Geri},
title = {Evaluating the Accuracy of Implicit Feedback from Clicks and Query Reformulations in Web Search},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1229179.1229181},
doi = {10.1145/1229179.1229181},
abstract = {This article examines the reliability of implicit feedback generated from clickthrough data and query reformulations in World Wide Web (WWW) search. Analyzing the users' decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. While this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. We find that such relative preferences are accurate not only between results from an individual query, but across multiple sets of results within chains of query reformulations.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {7–es},
numpages = {27},
keywords = {implicit feedback, eye-tracking, query reformulations, user studies, Clickthrough data}
}

@article{10.1145/1229179.1229180,
author = {Lin, Jimmy},
title = {An Exploration of the Principles Underlying Redundancy-Based Factoid Question Answering},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1229179.1229180},
doi = {10.1145/1229179.1229180},
abstract = {The so-called “redundancy-based” approach to question answering represents a successful strategy for mining answers to factoid questions such as “Who shot Abraham Lincoln?” from the World Wide Web. Through contrastive and ablation experiments with Aranea, a system that has performed well in several TREC QA evaluations, this work examines the underlying assumptions and principles behind redundancy-based techniques. Specifically, we develop two theses: that stable characteristics of data redundancy allow factoid systems to rely on external “black box” components, and that despite embodying a data-driven approach, redundancy-based methods encode a substantial amount of knowledge in the form of heuristics. Overall, this work attempts to address the broader question of “what really matters” and to provide guidance for future researchers.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {6–es},
numpages = {55},
keywords = {Data redundancy, Web search}
}

@article{10.1145/1198296.1198301,
author = {Ma, Zhongming and Pant, Gautam and Sheng, Olivia R. Liu},
title = {Interest-Based Personalized Search},
year = {2007},
issue_date = {February 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1198296.1198301},
doi = {10.1145/1198296.1198301},
abstract = {Web search engines typically provide search results without considering user interests or context. We propose a personalized search approach that can easily extend a conventional search engine on the client side. Our mapping framework automatically maps a set of known user interests onto a group of categories in the Open Directory Project (ODP) and takes advantage of manually edited data available in ODP for training text classifiers that correspond to, and therefore categorize and personalize search results according to user interests. In two sets of controlled experiments, we compare our personalized categorization system (PCAT) with a list interface system (LIST) that mimics a typical search engine and with a nonpersonalized categorization system (CAT). In both experiments, we analyze system performances on the basis of the type of task and query length. We find that PCAT is preferable to LIST for information gathering types of tasks and for searches with short queries, and PCAT outperforms CAT in both information gathering and finding types of tasks, and for searches associated with free-form queries. From the subjects' answers to a questionnaire, we find that PCAT is perceived as a system that can find relevant Web pages quicker and easier than LIST and CAT.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
pages = {5–es},
numpages = {38},
keywords = {Open Directory, World Wide Web, user interest, Personalized search, information retrieval, user interface}
}

@article{10.1145/1198296.1198300,
author = {Talvensaari, Tuomas and Laurikkala, Jorma and J\"{a}rvelin, Kalervo and Juhola, Martti and Keskustalo, Heikki},
title = {Creating and Exploiting a Comparable Corpus in Cross-Language Information Retrieval},
year = {2007},
issue_date = {February 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1198296.1198300},
doi = {10.1145/1198296.1198300},
abstract = {We present a method for creating a comparable text corpus from two document collections in different languages. The collections can be very different in origin. In this study, we build a comparable corpus from articles by a Swedish news agency and a U.S. newspaper. The keys with best resolution power were extracted from the documents of one collection, the source collection, by using the relative average term frequency (RATF) value. The keys were translated into the language of the other collection, the target collection, with a dictionary-based query translation program. The translated queries were run against the target collection and an alignment pair was made if the retrieved documents matched given date and similarity score criteria. The resulting comparable collection was used as a similarity thesaurus to translate queries along with a dictionary-based translator. The combined approaches outperformed translation schemes where dictionary-based translation or corpus translation was used alone.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
pages = {4–es},
numpages = {21},
keywords = {query translation, Cross-language information retrieval, comparable corpora}
}

@article{10.1145/1198296.1198299,
author = {Chai, Joyce Y. and Zhang, Chen and Jin, Rong},
title = {An Empirical Investigation of User Term Feedback in Text-Based Targeted Image Search},
year = {2007},
issue_date = {February 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1198296.1198299},
doi = {10.1145/1198296.1198299},
abstract = {Text queries are natural and intuitive for users to describe their information needs. However, text-based image retrieval faces many challenges. Traditional text retrieval techniques on image descriptions have not been very successful. This is mainly due to the inconsistent textual descriptions and the discrepancies between user queries and terms in the descriptions. To investigate strategies to alleviate this vocabulary problem, this article examines the role of user term feedback in targeted image search that is based on text-based image retrieval. Term feedback refers to the feedback from a user on specific terms regarding their relevance to a target image. Previous studies have indicated the effectiveness of term feedback in interactive text retrieval. However, in our experiments on text-based image retrieval, the term feedback has not been shown to be effective. Our results indicate that, although term feedback has a positive effect by allowing users to identify more relevant terms, it also has a strong negative effect by providing more opportunities for users to specify irrelevant terms. To understand these different effects and their implications, this article further analyzes important factors that contribute to the utility of term feedback and discusses the outlook of term feedback in interactive text-based image retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
pages = {3–es},
numpages = {25},
keywords = {user term feedback, Text-based interactive image retrieval}
}

@article{10.1145/1198296.1198298,
author = {Lam, Wai and Chan, Shing-Kit and Huang, Ruizhang},
title = {Named Entity Translation Matching and Learning: With Application for Mining Unseen Translations},
year = {2007},
issue_date = {February 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1198296.1198298},
doi = {10.1145/1198296.1198298},
abstract = {This article introduces a named entity matching model that makes use of both semantic and phonetic evidence. The matching of semantic and phonetic information is captured by a unified framework via a bipartite graph model. By considering various technical challenges of the problem, including order insensitivity and partial matching, this approach is less rigid than existing approaches and highly robust. One major component is a phonetic matching model which exploits similarity at the phoneme level. Two learning algorithms for learning the similarity information of basic phonemic matching units based on training examples are investigated. By applying the proposed named entity matching model, a mining system is developed for discovering new named entity translations from daily Web news. The system is able to discover new name translations that cannot be found in the existing bilingual dictionary.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
pages = {2–es},
numpages = {32},
keywords = {Text mining, named entity translation, learning phonetic information}
}

@article{10.1145/1198296.1198297,
author = {Piwowarski, B. and Gallinari, P. and Dupret, G.},
title = {Precision Recall with User Modeling (PRUM): Application to Structured Information Retrieval},
year = {2007},
issue_date = {February 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1198296.1198297},
doi = {10.1145/1198296.1198297},
abstract = {Standard Information Retrieval (IR) metrics are not well suited for new paradigms like XML or Web IR in which retrievable information units are document elements and/or sets of related documents. Part of the problem stems from the classical hypotheses on the user models: They do not take into account the structural or logical context of document elements or the possibility of navigation between units. This article proposes an explicit and formal user model that encompasses a large variety of user behaviors. Based on this model, we extend the probabilistic precision-recall metric to deal with the new IR paradigms.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
pages = {1–es},
numpages = {37},
keywords = {measure, precision-recall, information retrieval, XML, Web, Evaluation}
}

@article{10.1145/1185877.1185883,
author = {Kazai, Gabriella and Lalmas, Mounia},
title = {EXtended Cumulated Gain Measures for the Evaluation of Content-Oriented XML Retrieval},
year = {2006},
issue_date = {October 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1185877.1185883},
doi = {10.1145/1185877.1185883},
abstract = {We propose and evaluate a family of measures, the eXtended Cumulated Gain (XCG) measures, for the evaluation of content-oriented XML retrieval approaches. Our aim is to provide an evaluation framework that allows the consideration of dependency among XML document components. In particular, two aspects of dependency are considered: (1) near-misses, which are document components that are structurally related to relevant components, such as a neighboring paragraph or container section, and (2) overlap, which regards the situation wherein the same text fragment is referenced multiple times, for example, when a paragraph and its container section are both retrieved. A further consideration is that the measures should be flexible enough so that different models of user behavior may be instantiated within. Both system- and user-oriented aspects are investigated and both recall and precision-like qualities are measured. We evaluate the reliability of the proposed measures based on the INEX 2004 test collection. For example, the effects of assessment variation and topic set size on evaluation stability are investigated, and the upper and lower bounds of expected error rates are established. The evaluation demonstrates that the XCG measures are stable and reliable, and in particular, that the novel measures of effort-precision and gain-recall (ep/gr) show comparable behavior to established IR measures like precision and recall.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {503–542},
numpages = {40},
keywords = {INEX, overlap, dependency, metrics, evaluation, near-miss, XML retrieval, cumulated gain}
}

@article{10.1145/1185877.1185882,
author = {Genev\`{e}s, Pierre and Laya\"{\i}da, Nabil},
title = {A System for the Static Analysis of XPath},
year = {2006},
issue_date = {October 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1185877.1185882},
doi = {10.1145/1185877.1185882},
abstract = {XPath is the standard language for navigating XML documents and returning a set of matching nodes. We present a sound and complete decision procedure for containment of XPath queries, as well as other related XPath decision problems such as satisfiability, equivalence, overlap, and coverage. The considered XPath fragment covers most of the language features used in practice. Specifically, we propose a unifying logic for XML, namely, the alternation-free modal μ-calculus with converse. We show how to translate major XML concepts such as XPath and regular XML types (including DTDs) into this logic. Based on these embeddings, we show how XPath decision problems, in the presence or absence of XML types, can be solved using a decision procedure for μ-calculus satisfiability. We provide a complexity analysis of our system together with practical experiments to illustrate the efficiency of the approach for realistic scenarios.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {475–502},
numpages = {28},
keywords = {logic, equivalence, Containment, query, XML, XPath}
}

@article{10.1145/1185877.1185881,
author = {Lehtonen, Miro},
title = {Preparing Heterogeneous XML for Full-Text Search},
year = {2006},
issue_date = {October 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1185877.1185881},
doi = {10.1145/1185877.1185881},
abstract = {XML retrieval is facing new challenges when applied to heterogeneous XML documents, where next to nothing about the document structure can be taken for granted. We have developed solutions where some of the heterogeneity issues are addressed. Our fragment selection algorithm selectively divides a heterogeneous document collection into equi-sized fragments with full-text content. If the content is considered too data-oriented, it is not accepted. The algorithm needs no information about element names. In addition, three techniques for fragment expansion are presented, all of which yield a 13--17% average improvement in average precision. These techniques and algorithms are among the first steps in developing document-type-independent indexing methods for the full text in heterogeneous XML collections.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {455–474},
numpages = {20},
keywords = {XML retrieval, heterogeneous documents, indexing}
}

@article{10.1145/1185877.1185880,
author = {Crouch, Carolyn J.},
title = {Dynamic Element Retrieval in a Structured Environment},
year = {2006},
issue_date = {October 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1185877.1185880},
doi = {10.1145/1185877.1185880},
abstract = {This research examines the feasibility of dynamic element retrieval in a structured environment. Structured documents and queries are represented in extended vector form, based on a modification of the basic vector space model suggested by Fox [1983]. A method for the dynamic retrieval of XML elements, which requires only a single indexing of the documents at the level of the basic indexing node, is presented. This method, which we refer to as flexible retrieval, produces a rank ordered list of retrieved elements that is equivalent to the result produced by the same retrieval against an all-element index of the collection. Flexible retrieval obviates the need for storing either an all-element index or multiple indices of the collection.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {437–454},
numpages = {18},
keywords = {XML, dynamic element retrieval, vector space model, flexible retrieval, structured retrieval}
}

@article{10.1145/1185877.1185879,
author = {Kamps, Jaap and Marx, Maarten and Rijke, Maarten de and Sigurbj\"{o}rnsson, B\"{o}rkur},
title = {Articulating Information Needs in XML Query Languages},
year = {2006},
issue_date = {October 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1185877.1185879},
doi = {10.1145/1185877.1185879},
abstract = {Document-centric XML is a mixture of text and structure. With the increased availability of document-centric XML documents comes a need for query facilities in which both structural constraints and constraints on the content of the documents can be expressed. How does the expressiveness of languages for querying XML documents help users to express their information needs? We address this question from both an experimental and a theoretical point of view. Our experimental analysis compares a structure-ignorant with a structure-aware retrieval approach using the test suite of the INEX XML Retrieval Evaluation Initiative. Theoretically, we create two mathematical models of users' knowledge of a set of documents and define query languages which exactly fit these models. One of these languages corresponds to an XML version of fielded search, the other to the INEX query language.Our main experimental findings are: First, while structure is used in varying degrees of complexity, two-thirds of the queries can be expressed in a fielded-search-like format which does not use the hierarchical structure of the documents. Second, three-quarters of the queries use constraints on the context of the elements to be returned; these contextual constraints cannot be captured by ordinary keyword queries. Third, structure is used as a search hint, and not as a strict requirement, when judged against the underlying information need. Fourth, the use of structure in queries functions as a precision enhancing device.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {407–436},
numpages = {30},
keywords = {Full-text XML querying, XML retrieval, XPath}
}

@article{10.1145/1185877.1185878,
author = {Baeza-Yates, Ricardo and Fuhr, Norbert and Maarek, Yoelle},
title = {Introduction to the Special Issue on XML Retrieval},
year = {2006},
issue_date = {October 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1185877.1185878},
doi = {10.1145/1185877.1185878},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {405–406},
numpages = {2}
}

@article{10.1145/1165774.1165778,
author = {Yu, Hong and Kim, Won and Hatzivassiloglou, Vasileios and Wilbur, John},
title = {A Large Scale, Corpus-Based Approach for Automatically Disambiguating Biomedical Abbreviations},
year = {2006},
issue_date = {July 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1165774.1165778},
doi = {10.1145/1165774.1165778},
abstract = {Abbreviations and acronyms are widely used in the biomedical literature and many of them represent important biomedical concepts. Because many abbreviations are ambiguous (e.g., CAT denotes both chloramphenicol acetyl transferase and computed axial tomography, depending on the context), recognizing the full form associated with each abbreviation is in most cases equivalent to identifying the meaning of the abbreviation. This, in turn, allows us to perform more accurate natural language processing, information extraction, and retrieval. In this study, we have developed supervised approaches to identifying the full forms of ambiguous abbreviations within the context they appear. We first automatically assigned multiple possible full forms for each abbreviation; we then treated the in-context full-form prediction for each specific abbreviation occurrence as a case of word-sense disambiguation. We generated automatically a dictionary of all possible full forms for each abbreviation. We applied supervised machine-learning algorithms for disambiguation. Because some of the links between abbreviations and their corresponding full forms are explicitly given in the text and can be recovered automatically, we can use these explicit links to automatically provide training data for disambiguating the abbreviations that are not linked to a full form within a text. We evaluated our methods on over 150 thousand abstracts and obtain for coverage and precision results of 82% and 92%, respectively, when performed as tenfold cross-validation, and 79% and 80%, respectively, when evaluated against an external set of abstracts in which the abbreviations are not defined.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {380–404},
numpages = {25},
keywords = {Word-sense disambiguation, machine learning, information retrieval, data mining}
}

@article{10.1145/1165774.1165777,
author = {Tsai, Chih-Fong and McGarry, Ken and Tait, John},
title = {CLAIRE: A Modular Support Vector Image Indexing and Classification System},
year = {2006},
issue_date = {July 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1165774.1165777},
doi = {10.1145/1165774.1165777},
abstract = {Many users of image retrieval systems would prefer to express initial queries using keywords. However, manual keyword indexing is very time-consuming. Therefore, a content-based image retrieval system which can automatically assign keywords to images would be very attractive. Unfortunately, it has proved very challenging to build such systems, except where either the image domain is restricted or the keywords relate only to low-level concepts such as color. This article presents a novel image indexing and classification system, called CLAIRE (CLAssifying Images for REtrieval), composed of one image processing module and three modules of support vector machines for color, texture, and high-level concept classification for keyword assignment. The experimental prototype system described here assigns up to five keywords selected from a controlled vocabulary of 60 terms to each image. The system is trained offline by 1639 examples from the Corel stock photo library. For evaluation, five judges reviewed a sample of 800 unknown images to identify which automatically assigned keywords were actually relevant to the image. The system proved to have an 80% probability to assign at least one relevant keyword to an image.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {353–379},
numpages = {27},
keywords = {image indexing, support vector machines, Content-based image retrieval, multiple classifier systems, image classification}
}

@article{10.1145/1165774.1165776,
author = {Shen, Dou and Pan, Rong and Sun, Jian-Tao and Pan, Jeffrey Junfeng and Wu, Kangheng and Yin, Jie and Yang, Qiang},
title = {Query Enrichment for Web-Query Classification},
year = {2006},
issue_date = {July 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1165774.1165776},
doi = {10.1145/1165774.1165776},
abstract = {Web-search queries are typically short and ambiguous. To classify these queries into certain target categories is a difficult but important problem. In this article, we present a new technique called query enrichment, which takes a short query and maps it to intermediate objects. Based on the collected intermediate objects, the query is then mapped to target categories. To build the necessary mapping functions, we use an ensemble of search engines to produce an enrichment of the queries. Our technique was applied to the ACM Knowledge Discovery and Data Mining competition (ACM KDDCUP) in 2005, where we won the championship on all three evaluation metrics (precision, F1 measure, which combines precision and recall, and creativity, which is judged by the organizers) among a total of 33 teams worldwide. In this article, we show that, despite the difficulty of an abundance of ambiguous queries and lack of training data, our query-enrichment technique can solve the problem satisfactorily through a two-phase classification framework. We present a detailed description of our algorithm and experimental evaluation. Our best result for F1 and precision is 42.4% and 44.4%, respectively, which is 9.6% and 24.3% higher than those from the runner-ups, respectively.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {320–352},
numpages = {33},
keywords = {KDDCUP2005, Query classification, ensemble learning, query enrichment, synonym-based classifier}
}

@article{10.1145/1165774.1165775,
author = {Jiang, Jing and Zhai, Chengxiang},
title = {Extraction of Coherent Relevant Passages Using Hidden Markov Models},
year = {2006},
issue_date = {July 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1165774.1165775},
doi = {10.1145/1165774.1165775},
abstract = {In information retrieval, retrieving relevant passages, as opposed to whole documents, not only directly benefits the end user by filtering out the irrelevant information within a long relevant document, but also improves retrieval accuracy in general. A critical problem in passage retrieval is to extract coherent relevant passages accurately from a document, which we refer to as passage extraction. While much work has been done on passage retrieval, the passage extraction problem has not been seriously studied. Most existing work tends to rely on presegmenting documents into fixed-length passages which are unlikely optimal because the length of a relevant passage is presumably highly sensitive to both the query and document.In this article, we present a new method for accurately detecting coherent relevant passages of variable lengths using hidden Markov models (HMMs). The HMM-based method naturally captures the topical boundaries between passages relevant and nonrelevant to the query. Pseudo-feedback mechanisms can be naturally incorporated into such an HMM-based framework to improve parameter estimation. We show that with appropriate parameter estimation, the HMM method outperforms a number of strong baseline methods on two datasets. We further show how the HMM method can be applied on top of any basic passage extraction method to improve passage boundaries.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {295–319},
numpages = {25},
keywords = {Hidden Markov models, passage retrieval}
}

@article{10.1145/1148020.1148024,
author = {Chen, Keke and Liu, Ling},
title = {IVIBRATE: Interactive Visualization-Based Framework for Clustering Large Datasets},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1148020.1148024},
doi = {10.1145/1148020.1148024},
abstract = {With continued advances in communication network technology and sensing technology, there is astounding growth in the amount of data produced and made available through cyberspace. Efficient and high-quality clustering of large datasets continues to be one of the most important problems in large-scale data analysis. A commonly used methodology for cluster analysis on large datasets is the three-phase framework of sampling/summarization, iterative cluster analysis, and disk-labeling. There are three known problems with this framework which demand effective solutions. The first problem is how to effectively define and validate irregularly shaped clusters, especially in large datasets. Automated algorithms and statistical methods are typically not effective in handling these particular clusters. The second problem is how to effectively label the entire data on disk (disk-labeling) without introducing additional errors, including the solutions for dealing with outliers, irregular clusters, and cluster boundary extension. The third obstacle is the lack of research about issues related to effectively integrating the three phases. In this article, we describe iVIBRATE---an interactive visualization-based three-phase framework for clustering large datasets. The two main components of iVIBRATE are its VISTA visual cluster-rendering subsystem which invites human interplay into the large-scale iterative clustering process through interactive visualization, and its adaptive ClusterMap labeling subsystem which offers visualization-guided disk-labeling solutions that are effective in dealing with outliers, irregular clusters, and cluster boundary extension. Another important contribution of iVIBRATE development is the identification of the special issues presented in integrating the two components and the sampling approach into a coherent framework, as well as the solutions for improving the reliability of the framework and for minimizing the amount of errors generated within the cluster analysis process. We study the effectiveness of the iVIBRATE framework through a walkthrough example dataset of a million records and we experimentally evaluate the iVIBRATE approach using both real-life and synthetic datasets. Our results show that iVIBRATE can efficiently involve the user in the clustering process and generate high-quality clustering results for large datasets.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {245–294},
numpages = {50},
keywords = {performance, large datasets, interactive visualization, labeling, Clustering}
}

@article{10.1145/1148020.1148023,
author = {Zhou, Zhi-Hua and Chen, Ke-Jia and Dai, Hong-Bin},
title = {Enhancing Relevance Feedback in Image Retrieval Using Unlabeled Data},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1148020.1148023},
doi = {10.1145/1148020.1148023},
abstract = {Relevance feedback is an effective scheme bridging the gap between high-level semantics and low-level features in content-based image retrieval (CBIR). In contrast to previous methods which rely on labeled images provided by the user, this article attempts to enhance the performance of relevance feedback by exploiting unlabeled images existing in the database. Concretely, this article integrates the merits of semisupervised learning and active learning into the relevance feedback process. In detail, in each round of relevance feedback two simple learners are trained from the labeled data, that is, images from user query and user feedback. Each learner then labels some unlabeled images in the database for the other learner. After retraining with the additional labeled data, the learners reclassify the images in the database and then their classifications are merged. Images judged to be positive with high confidence are returned as the retrieval result, while those judged with low confidence are put into the pool which is used in the next round of relevance feedback. Experiments show that using semisupervised learning and active learning simultaneously in CBIR is beneficial, and the proposed method achieves better performance than some existing methods.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {219–244},
numpages = {26},
keywords = {Relevance feedback, content-based image retrieval machine learning, semisupervised learning, active learning, learning with unlabeled data}
}

@article{10.1145/1148020.1148022,
author = {Gao, Sheng and Wu, Wen and Lee, Chin-Hui and Chua, Tat-Seng},
title = {A Maximal Figure-of-Merit (MFoM)-Learning Approach to Robust Classifier Design for Text Categorization},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1148020.1148022},
doi = {10.1145/1148020.1148022},
abstract = {We propose a maximal figure-of-merit (MFoM)-learning approach for robust classifier design, which directly optimizes performance metrics of interest for different target classifiers. The proposed approach, embedding the decision functions of classifiers and performance metrics into an overall training objective, learns the parameters of classifiers in a decision-feedback manner to effectively take into account both positive and negative training samples, thereby reducing the required size of positive training data. It has three desirable properties: (a) it is a performance metric, oriented learning; (b) the optimized metric is consistent in both training and evaluation sets; and (c) it is more robust and less sensitive to data variation, and can handle insufficient training data scenarios. We evaluate it on a text categorization task using the Reuters-21578 dataset. Training an F1-based binary tree classifier using MFoM, we observed significantly improved performance and enhanced robustness compared to the baseline and SVM, especially for categories with insufficient training samples. The generality for designing other metrics-based classifiers is also demonstrated by comparing precision, recall, and F1-based classifiers. The results clearly show consistency of performance between the training and evaluation stages for each classifier, and MFoM optimizes the chosen metric.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {190–218},
numpages = {29},
keywords = {information retrieval, maximal figure-of-merit, latent semantic indexing, decision tree, generalized probabilistic descent method, Text categorization}
}

@article{10.1145/1148020.1148021,
author = {Lee, Hyowon and Smeaton, Alan F. and O'connor, Noel E. and Smyth, Barry},
title = {User Evaluation of F\'{\i}Schl\'{a}r-News: An Automatic Broadcast News Delivery System},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1148020.1148021},
doi = {10.1145/1148020.1148021},
abstract = {Technological developments in content-based analysis of digital video information are undergoing much progress, with ideas for fully automatic systems now being proposed and demonstrated. Yet because we do not yet have robust operational video retrieval systems that can be deployed and used, the usual HCI practise of conducting a usage study and an informed iterative system design is thus not possible. F\'{\i}schl\'{a}r-News is one of the first automatic, content-based broadcast news analysis and archival systems that process broadcast news video so that users can search, browse, and play it in an easy-to-use manner with a conventional web browser. The system incorporates a number of state-of-the-art research components, some of which are not yet considered mature technology, yet it has been built to be robust enough to be deployed to users who are interested in access to daily news throughout a university campus. In this article we report and discuss a user-evaluation study conducted with 16 users, each of whom utilized the system freely for a one month period. Results from a detailed qualitative analysis are presented, looking at collected questionnaires, incident diaries, and interaction-log data. The findings suggest that our users employed the system in conjunction with their other news update methods, such as watching TV news at home and browsing online news websites at their workplace, their major concerns being up-to-dateness and coverage of the news content. They tried to accommodate the system to fit their established web browsing habits, and they found local news content and the ability to play self-contained news stories on their desktop as major values of the system. Our study also resulted in a detailed wishlist of new features which will help in the further development of both our and others' systems.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {145–189},
numpages = {45},
keywords = {usage analysis, content-based video retrieval, User-evaluation}
}

@article{10.1145/1125857.1125862,
author = {Marchionini, Gary},
title = {TOIS Reviewers 2003--2005},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1125857.1125862},
doi = {10.1145/1125857.1125862},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {142–143},
numpages = {2}
}

@article{10.1145/1125857.1125861,
author = {McDonald, Daniel M. and Chen, Hsinchun},
title = {Summary in Context: Searching versus Browsing},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1125857.1125861},
doi = {10.1145/1125857.1125861},
abstract = {The use of text summaries in information-seeking research has focused on query-based summaries. Extracting content that resembles the query alone, however, ignores the greater context of the document. Such context may be central to the purpose and meaning of the document. We developed a generic, a query-based, and a hybrid summarizer, each with differing amounts of document context. The generic summarizer used a blend of discourse information and information obtained through traditional surface-level analysis. The query-based summarizer used only query-term information, and the hybrid summarizer used some discourse information along with query-term information. The validity of the generic summarizer was shown through an intrinsic evaluation using a well-established corpus of human-generated summaries. All three summarizers were then compared in an information-seeking experiment involving 297 subjects. Results from the information-seeking experiment showed that the generic summaries outperformed all others in the browse tasks, while the query-based and hybrid summaries outperformed the generic summary in the search tasks. Thus, the document context of generic summaries helped users browse, while such context was not helpful in search tasks. Such results are interesting given that generic summaries have not been studied in search tasks and the that majority of Internet search engines rely solely on query-based summaries.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {111–141},
numpages = {31},
keywords = {generic summaries, search, Summarization, information seeking, browse, natural language processing, text processing, indicative summaries}
}

@article{10.1145/1125857.1125860,
author = {Qian, Gang and Zhu, Qiang and Xue, Qiang and Pramanik, Sakti},
title = {A Space-Partitioning-Based Indexing Method for Multidimensional Non-Ordered Discrete Data Spaces},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1125857.1125860},
doi = {10.1145/1125857.1125860},
abstract = {There is an increasing demand for similarity searches in a multidimensional non-ordered discrete data space (NDDS) from application areas such as bioinformatics and data mining. The non-ordered and discrete nature of an NDDS raises new challenges for developing efficient indexing methods for similarity searches. In this article, we propose a new indexing technique, called the NSP-tree, to support efficient similarity searches in an NDDS. As we know, overlap causes a performance degradation for indexing methods (e.g., the R-tree) for a continuous data space. In an NDDS, this problem is even worse due to the limited number of elements available on each dimension of an NDDS. The key idea of the NSP-tree is to use a novel discrete space-partitioning (SP) scheme to ensure no overlap at each level in the tree. A number of heuristics and strategies are incorporated into the tree construction algorithms to deal with the challenges for developing an SP-based index tree for an NDDS. Our experiments demonstrate that the NSP-tree is quite promising in supporting efficient similarity searches in NDDSs. We have compared the NSP-tree with the ND-tree, a data-partitioning-based indexing technique for NDDSs that was proposed recently, and the linear scan using different NDDSs. It was found that the search performance of the NSP-tree was better than those of both methods.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {79–110},
numpages = {32},
keywords = {Hamming distance, similarity search, multidimensional index tree, Non-ordered discrete data spaces}
}

@article{10.1145/1125857.1125859,
author = {Fagni, Tiziano and Perego, Raffaele and Silvestri, Fabrizio and Orlando, Salvatore},
title = {Boosting the Performance of Web Search Engines: Caching and Prefetching Query Results by Exploiting Historical Usage Data},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1125857.1125859},
doi = {10.1145/1125857.1125859},
abstract = {This article discusses efficiency and effectiveness issues in caching the results of queries submitted to a Web search engine (WSE). We propose SDC (Static Dynamic Cache), a new caching strategy aimed to efficiently exploit the temporal and spatial locality present in the stream of processed queries. SDC extracts from historical usage data the results of the most frequently submitted queries and stores them in a static, read-only portion of the cache. The remaining entries of the cache are dynamically managed according to a given replacement policy and are used for those queries that cannot be satisfied by the static portion. Moreover, we improve the hit ratio of SDC by using an adaptive prefetching strategy, which anticipates future requests by introducing a limited overhead over the back-end WSE. We experimentally demonstrate the superiority of SDC over purely static and dynamic policies by measuring the hit ratio achieved on three large query logs by varying the cache parameters and the replacement policy used for managing the dynamic part of the cache. Finally, we deploy and measure the throughput achieved by a concurrent version of our caching system. Our tests show how the SDC cache can be efficiently exploited by many threads that concurrently serve the queries of different users.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {51–78},
numpages = {28},
keywords = {multithreading, Caching, Web search engines}
}

@article{10.1145/1125857.1125858,
author = {Zobel, Justin and Hoad, Timothy C.},
title = {Detection of Video Sequences Using Compact Signatures},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1125857.1125858},
doi = {10.1145/1125857.1125858},
abstract = {Digital representations are widely used for audiovisual content, enabling the creation of large online repositories of video, allowing access such as video on demand. However, the ease of copying and distribution of digital video makes piracy a growing concern for content owners. We investigate methods for identifying coderivative video content---that is, video clips that are derived from the same original source. By using dynamic programming to identify regions of similarity in video signatures, it is possible to efficiently and accurately identify coderivatives, even when these regions constitute only a small section of the clip being searched. We propose four new methods for producing compact video signatures, based on the way in which the video changes over time. The intuition is that such properties are likely to be preserved even when the video is badly degraded. We demonstrate that these signatures are insensitive to dramatic changes in video bitrate and resolution, two parameters that are often altered when reencoding. In the presence of mild degradations, our methods can accurately identify copies of clips that are as short as 5 s within a dataset 140 min long. These methods are much faster than previously proposed techniques; using a more compact signature, this query can be completed in a few milliseconds.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–50},
numpages = {50},
keywords = {dynamic programming, local alignment, Video similarity detection}
}

@article{10.1145/1095872.1095876,
author = {Ivory, Melody Y. and Megraw, Rodrick},
title = {Evolution of Web Site Design Patterns},
year = {2005},
issue_date = {October 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1095872.1095876},
doi = {10.1145/1095872.1095876},
abstract = {The Web enables broad dissemination of information and services; however, the ways in which sites are designed can either facilitate or impede users' benefit from these resources. We present a longitudinal study of web site design from 2000 to 2003. We analyze over 150 quantitative measures of interface aspects (e.g., amount of text on pages, numbers and types of links, consistency, accessibility, etc.) for 22,000 pages and over 1,500 sites that received ratings from Internet professionals. We examine characteristics of highly rated sites and provide three perspectives on the evolution of web site design patterns: (1) descriptions of design patterns during each time period; (2) changes in design patterns across the three time periods; and (3) comparisons of design patterns to those that are recommended in the relevant literature (i.e., texts by recognized experts and user studies). We illustrate how design practices conform to or deviate from recommended practices and the consequent implications. We show that the most glaring deficiency of web sites, even for sites that are highly rated, is their inadequate accessibility, in particular for browser scripts, tables, and form elements.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {463–497},
numpages = {35},
keywords = {design guidelines, web site design, World Wide Web, empirical studies, usability, accessibility, automated usability evaluation}
}

@article{10.1145/1095872.1095875,
author = {Pant, Gautam and Srinivasan, Padmini},
title = {Learning to Crawl: Comparing Classification Schemes},
year = {2005},
issue_date = {October 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1095872.1095875},
doi = {10.1145/1095872.1095875},
abstract = {Topical crawling is a young and creative area of research that holds the promise of benefiting from several sophisticated data mining techniques. The use of classification algorithms to guide topical crawlers has been sporadically suggested in the literature. No systematic study, however, has been done on their relative merits. Using the lessons learned from our previous crawler evaluation studies, we experiment with multiple versions of different classification schemes. The crawling process is modeled as a parallel best-first search over a graph defined by the Web. The classifiers provide heuristics to the crawler thus biasing it towards certain portions of the Web graph. Our results show that Naive Bayes is a weak choice for guiding a topical crawler when compared with Support Vector Machine or Neural Network. Further, the weak performance of Naive Bayes can be partly explained by extreme skewness of posterior probabilities generated by it. We also observe that despite similar performances, different topical crawlers cover subspaces on the Web with low overlap.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {430–462},
numpages = {33},
keywords = {Topical crawlers, machine learning, focused crawlers, classifiers}
}

@article{10.1145/1095872.1095874,
author = {P\^{o}ssas, Bruno and Ziviani, Nivio and Meira, Wagner and Ribeiro-Neto, Berthier},
title = {Set-Based Vector Model: An Efficient Approach for Correlation-Based Ranking},
year = {2005},
issue_date = {October 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1095872.1095874},
doi = {10.1145/1095872.1095874},
abstract = {This work presents a new approach for ranking documents in the vector space model. The novelty lies in two fronts. First, patterns of term co-occurrence are taken into account and are processed efficiently. Second, term weights are generated using a data mining technique called association rules. This leads to a new ranking mechanism called the set-based vector model. The components of our model are no longer index terms but index termsets, where a termset is a set of index terms. Termsets capture the intuition that semantically related terms appear close to each other in a document. They can be efficiently obtained by limiting the computation to small passages of text. Once termsets have been computed, the ranking is calculated as a function of the termset frequency in the document and its scarcity in the document collection. Experimental results show that the set-based vector model improves average precision for all collections and query types evaluated, while keeping computational costs small. For the 2-gigabyte TREC-8 collection, the set-based vector model leads to a gain in average precision figures of 14.7% and 16.4% for disjunctive and conjunctive queries, respectively, with respect to the standard vector space model. These gains increase to 24.9% and 30.0%, respectively, when proximity information is taken into account. Query processing times are larger but, on average, still comparable to those obtained with the standard vector model (increases in processing time varied from 30% to 300%). Our results suggest that the set-based vector model provides a correlation-based ranking formula that is effective with general collections and computationally practical.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {397–429},
numpages = {33},
keywords = {weighting index term co-occurrences, association rule mining, data mining, Information retrieval models, correlation-based ranking}
}

@article{10.1145/1095872.1095873,
author = {Chuang, Shui-Lung and Chien, Lee-Feng},
title = {Taxonomy Generation for Text Segments: A Practical Web-Based Approach},
year = {2005},
issue_date = {October 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1095872.1095873},
doi = {10.1145/1095872.1095873},
abstract = {It is crucial in many information systems to organize short text segments, such as keywords in documents and queries from users, into a well-formed taxonomy. In this article, we address the problem of taxonomy generation for diverse text segments with a general and practical approach that uses the Web as an additional knowledge source. Unlike long documents, short text segments typically do not contain enough information to extract reliable features. This work investigates the possibilities of using highly ranked search-result snippets to enrich the representation of text segments. A hierarchical clustering algorithm is then designed for creating the hierarchical topic structure of text segments. Text segments with close concepts can be grouped together in a cluster, and relevant clusters linked at the same or near levels. Different from traditional clustering algorithms, which tend to produce cluster hierarchies with a very unnatural shape, the algorithm tries to produce a more natural and comprehensive tree hierarchy. Extensive experiments were conducted on different domains of text segments, including subject terms, people names, paper titles, and natural language questions. The obtained experimental results have shown the potential of the proposed approach, which provides a basis for the in-depth analysis of text segments on a larger scale and is believed able to benefit many information systems.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {363–396},
numpages = {34},
keywords = {search-result snippet, Taxonomy generation, text data mining, hierarchical clustering, text segment, partitioning}
}

@article{10.1145/1080343.1080347,
author = {White, Ryen W. and Ruthven, Ian and Jose, Joemon M. and Rijsbergen, C. J. Van},
title = {Evaluating Implicit Feedback Models Using Searcher Simulations},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1080343.1080347},
doi = {10.1145/1080343.1080347},
abstract = {In this article we describe an evaluation of relevance feedback (RF) algorithms using searcher simulations. Since these algorithms select additional terms for query modification based on inferences made from searcher interaction, not on relevance information searchers explicitly provide (as in traditional RF), we refer to them as implicit feedback models. We introduce six different models that base their decisions on the interactions of searchers and use different approaches to rank query modification terms. The aim of this article is to determine which of these models should be used to assist searchers in the systems we develop. To evaluate these models we used searcher simulations that afforded us more control over the experimental conditions than experiments with human subjects and allowed complex interaction to be modeled without the need for costly human experimentation. The simulation-based evaluation methodology measures how well the models learn the distribution of terms across relevant documents (i.e., learn what information is relevant) and how well they improve search effectiveness (i.e., create effective search queries). Our findings show that an implicit feedback model based on Jeffrey's rule of conditioning outperformed other models under investigation.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {325–361},
numpages = {37},
keywords = {relevance feedback, implicit feedback, evaluation, User simulations}
}

@article{10.1145/1080343.1080346,
author = {Gladney, H. M. and Lorie, R. A.},
title = {Trustworthy 100-Year Digital Objects: Durable Encoding for When It's Too Late to Ask},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1080343.1080346},
doi = {10.1145/1080343.1080346},
abstract = {How can an author store digital information so that it will be reliably intelligible, even years later when he or she is no longer available to answer questions? Methods that might work are not good enough; what is preserved today should be reliably intelligible whenever someone wants it. Prior proposals fail because they generally confound saved data with irrelevant details of today's information technology---details that are difficult to define, extract, and save completely and accurately.We use a virtual machine to represent and eventually to render any data whatsoever. We focus on a case of intermediate difficulty---an executable procedure---and identify a variant for every other data type.This solution might be more elaborate than needed to render some text, image, audio, or video data. Simple data can be preserved as representations using well-known standards. We sketch practical methods for files ranging from simple structures to those containing computer programs, treating simple cases here and deferring complex cases for future work. Enough of the complete solution is known to enable practical aggressive preservation programs today.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {299–324},
numpages = {26},
keywords = {encoding, Long-term digital preservation}
}

@article{10.1145/1080343.1080345,
author = {Park, Laurence A. F. and Ramamohanarao, Kotagiri and Palaniswami, Marimuthu},
title = {A Novel Document Retrieval Method Using the Discrete Wavelet Transform},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1080343.1080345},
doi = {10.1145/1080343.1080345},
abstract = {Current information retrieval methods either ignore the term positions or deal with exact term positions; the former can be seen as coarse document resolution, the latter as fine document resolution. We propose a new spectral-based information retrieval method that is able to utilize many different levels of document resolution by examining the term patterns that occur in the documents. To do this, we take advantage of the multiresolution analysis properties of the wavelet transform. We show that we are able to achieve higher precision when compared to vector space and proximity retrieval methods, while producing fast query times and using a compact index.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {267–298},
numpages = {32},
keywords = {document retrieval, multiresolution analysis, proximity search, vector space methods, Daubechies, wavelet transform, Haar}
}

@article{10.1145/1080343.1080344,
author = {Wei, Yan Zheng and Moreau, Luc and Jennings, Nicholas R.},
title = {A Market-Based Approach to Recommender Systems},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1080343.1080344},
doi = {10.1145/1080343.1080344},
abstract = {Recommender systems have been widely advocated as a way of coping with the problem of information overload for knowledge workers. Given this, multiple recommendation methods have been developed. However, it has been shown that no one technique is best for all users in all situations. Thus we believe that effective recommender systems should incorporate a wide variety of such techniques and that some form of overarching framework should be put in place to coordinate the various recommendations so that only the best of them (from whatever source) are presented to the user. To this end, we show that a marketplace, in which the various recommendation methods compete to offer their recommendations to the user, can be used in this role. Specifically, this article presents the principled design of such a marketplace (including the auction protocol, the reward mechanism, and the bidding strategies of the individual recommendation agents) and evaluates the market's capability to effectively coordinate multiple methods. Through analysis and simulation, we show that our market is capable of shortlisting recommendations in decreasing order of user perceived quality and of correlating the individual agent's internal quality rating to the user's perceived quality.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {227–266},
numpages = {40},
keywords = {Recommender systems, auctions, marketplace}
}

@article{10.1145/1059981.1059984,
author = {Xu, Jennifer J. and Chen, Hsinchun},
title = {CrimeNet Explorer: A Framework for Criminal Network Knowledge Discovery},
year = {2005},
issue_date = {April 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1059981.1059984},
doi = {10.1145/1059981.1059984},
abstract = {Knowledge about the structure and organization of criminal networks is important for both crime investigation and the development of effective strategies to prevent crimes. However, except for network visualization, criminal network analysis remains primarily a manual process. Existing tools do not provide advanced structural analysis techniques that allow extraction of network knowledge from large volumes of criminal-justice data. To help law enforcement and intelligence agencies discover criminal network knowledge efficiently and effectively, in this research we proposed a framework for automated network analysis and visualization. The framework included four stages: network creation, network partition, structural analysis, and network visualization. Based upon it, we have developed a system called CrimeNet Explorer that incorporates several advanced techniques: a concept space approach, hierarchical clustering, social network analysis methods, and multidimensional scaling. Results from controlled experiments involving student subjects demonstrated that our system could achieve higher clustering recall and precision than did untrained subjects when detecting subgroups from criminal networks. Moreover, subjects identified central members and interaction patterns between groups significantly faster with the help of structural analysis functionality than with only visualization functionality. No significant gain in effectiveness was present, however. Our domain experts also reported that they believed CrimeNet Explorer could be very useful in crime investigation.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {201–226},
numpages = {26},
keywords = {visualization, concept space, complete-link algorithm, Law enforcement, precision and recall, crime investigation, clustering, knowledge discovery, social network analysis, multidimensional scaling}
}

@article{10.1145/1059981.1059983,
author = {Cooper, Brian F. and Garcia-Molina, Hector},
title = {Ad Hoc, Self-Supervising Peer-to-Peer Search Networks},
year = {2005},
issue_date = {April 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1059981.1059983},
doi = {10.1145/1059981.1059983},
abstract = {Peer-to-peer search networks are a popular and widely deployed means of searching massively distributed digital information repositories. Unfortunately, as such networks grow, peers may become overloaded processing messages from other peers. This article examines how to reduce the load on nodes in P2P networks by allowing them to self-organize into a relatively efficient network, and then self-tune to make the network even more efficient. Two local operations used by a peer are introduced: connect(), in which the peer forms an ad hoc search or index link to another peer, and break(), in which the peer breaks a link that is producing too much load. By replacing fixed rules with dynamic local decision-making, such “self-supervising” networks can better adjust to network conditions. Different ways to implement connect() and break() are described, and the network structures that form under different configurations are examined. Simulation results indicate that the ad hoc networks formed using the described techniques are more efficient than popular supernode topologies for several important scenarios. Results for the fault tolerance and search latency of such ad hoc networks are also presented.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {169–200},
numpages = {32},
keywords = {information search and discovery, Peer-to-peer systems}
}

@article{10.1145/1059981.1059982,
author = {Fox, Steve and Karnawat, Kuldeep and Mydland, Mark and Dumais, Susan and White, Thomas},
title = {Evaluating Implicit Measures to Improve Web Search},
year = {2005},
issue_date = {April 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1059981.1059982},
doi = {10.1145/1059981.1059982},
abstract = {Of growing interest in the area of improving the search experience is the collection of implicit user behavior measures (implicit measures) as indications of user interest and user satisfaction. Rather than having to submit explicit user feedback, which can be costly in time and resources and alter the pattern of use within the search experience, some research has explored the collection of implicit measures as an efficient and useful alternative to collecting explicit measure of interest from users.This research article describes a recent study with two main objectives. The first was to test whether there is an association between explicit ratings of user satisfaction and implicit measures of user interest. The second was to understand what implicit measures were most strongly associated with user satisfaction. The domain of interest was Web search. We developed an instrumented browser to collect a variety of measures of user activity and also to ask for explicit judgments of the relevance of individual pages visited and entire search sessions. The data was collected in a workplace setting to improve the generalizability of the results.Results were analyzed using traditional methods (e.g., Bayesian modeling and decision trees) as well as a new usage behavior pattern analysis (“gene analysis”). We found that there was an association between implicit measures of user activity and the user's explicit satisfaction ratings. The best models for individual pages combined clickthrough, time spent on the search result page, and how a user exited a result or ended a search session (exit type/end action). Behavioral patterns (through the gene analysis) can also be used to predict user satisfaction for search sessions.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {147–168},
numpages = {22},
keywords = {search sessions, prediction model, user satisfaction, user interest, explicit ratings, Implicit measures, explicit feedback}
}

@article{10.1145/1055709.1055714,
author = {Adomavicius, Gediminas and Sankaranarayanan, Ramesh and Sen, Shahana and Tuzhilin, Alexander},
title = {Incorporating Contextual Information in Recommender Systems Using a Multidimensional Approach},
year = {2005},
issue_date = {January 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1055709.1055714},
doi = {10.1145/1055709.1055714},
abstract = {The article presents a multidimensional (MD) approach to recommender systems that can provide recommendations based on additional contextual information besides the typical information on users and items used in most of the current recommender systems. This approach supports multiple dimensions, profiling information, and hierarchical aggregation of recommendations. The article also presents a multidimensional rating estimation method capable of selecting two-dimensional segments of ratings pertinent to the recommendation context and applying standard collaborative filtering or other traditional two-dimensional rating estimation techniques to these segments. A comparison of the multidimensional and two-dimensional rating estimation approaches is made, and the tradeoffs between the two are studied. Moreover, the article introduces a combined rating estimation method, which identifies the situations where the MD approach outperforms the standard two-dimensional approach and uses the MD approach in those situations and the standard two-dimensional approach elsewhere. Finally, the article presents a pilot empirical study of the combined approach, using a multidimensional movie recommender system that was developed for implementing this approach and testing its performance.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {103–145},
numpages = {43},
keywords = {multidimensional recommender systems, multidimensional data models, rating estimation, context-aware recommender systems, personalization, collaborative filtering, Recommender systems}
}

@article{10.1145/1055709.1055713,
author = {Tao, Yufei and Papadias, Dimitris},
title = {Historical Spatio-Temporal Aggregation},
year = {2005},
issue_date = {January 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1055709.1055713},
doi = {10.1145/1055709.1055713},
abstract = {Spatio-temporal databases store information about the positions of individual objects over time. However, in many applications such as traffic supervision or mobile communication systems, only summarized data, like the number of cars in an area for a specific period, or phone-calls serviced by a cell each day, is required. Although this information can be obtained from operational databases, its computation is expensive, rendering online processing inapplicable. In this paper, we present specialized methods, which integrate spatio-temporal indexing with pre-aggregation. The methods support dynamic spatio-temporal dimensions for the efficient processing of historical aggregate queries without a priori knowledge of grouping hierarchies. The superiority of the proposed techniques over existing methods is demonstrated through a comprehensive probabilistic analysis and an extensive experimental evaluation.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {61–102},
numpages = {42},
keywords = {Aggregation, cost models, access methods}
}

@article{10.1145/1055709.1055712,
author = {Sander, J\"{o}rg and Ng, Raymond T. and Sleumer, Monica C. and Yuen, Man Saint and Jones, Steven J.},
title = {A Methodology for Analyzing SAGE Libraries for Cancer Profiling},
year = {2005},
issue_date = {January 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1055709.1055712},
doi = {10.1145/1055709.1055712},
abstract = {Serial Analysis of Gene Expression (SAGE) has proven to be an important alternative to microarray techniques for global profiling of mRNA populations. We have developed preprocessing methodologies to address problems in analyzing SAGE data due to noise caused by sequencing error, normalization methodologies to account for libraries sampled at different depths, and missing tag imputation methodologies to aid in the analysis of poorly sampled SAGE libraries. We have also used subspace selection using the Wilcoxon rank sum test to exclude tags that have similar expression levels regardless of source. Using these methodologies we have clustered, using the OPTICS algorithm, 88 SAGE libraries derived from cancerous and normal tissues as well as cell line material. Our results produced eight dense clusters representing ovarian cancer cell line, brain cancer cell line, brain cancer bulk tissue, prostate tissue, pancreatic cancer, breast cancer cell line, normal brain, and normal breast bulk tissue. The ovarian cancer and brain cancer cell lines clustered closely together, leading to a further investigation on possible associations between these two cancer types. We also investigated the utility of gene expression data in the classification between normal and cancerous tissues. Our results indicate that brain and breast cancer libraries have strong identities allowing robust discrimination from their normal counterparts. However, the SAGE expression data provide poor predictive accuracy in discriminating between prostate and ovarian cancers and their respective normal tissues.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {35–60},
numpages = {26},
keywords = {clustering, classification, cancer profiling, Gene expression}
}

@article{10.1145/1055709.1055711,
author = {Korodi, Gergely and Tabus, Ioan},
title = {An Efficient Normalized Maximum Likelihood Algorithm for DNA Sequence Compression},
year = {2005},
issue_date = {January 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1055709.1055711},
doi = {10.1145/1055709.1055711},
abstract = {This article presents an efficient algorithm for DNA sequence compression, which achieves the best compression ratios reported over a test set commonly used for evaluating DNA compression programs. The algorithm introduces many refinements to a compression method that combines: (1) encoding by a simple normalized maximum likelihood (NML) model for discrete regression, through reference to preceding approximate matching blocks, (2) encoding by a first order context coding and (3) representing strings in clear, to make efficient use of the redundancy sources in DNA data, under fast execution times. One of the main algorithmic features is the constraint on the matching blocks to include reasonably long contiguous matches, which not only reduces significantly the search time, but also can be used to modify the NML model to exploit the constraint for getting smaller code lengths. The algorithm handles the changing statistics of DNA data in an adaptive way and by predictively encoding the matching pointers it is successful in compressing long approximate matches. Apart from comparison with previous DNA encoding methods, we present compression results for the recently published human genome data.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {3–34},
numpages = {32},
keywords = {normalized maximum likelihood model, DNA compression, Approximate sequence matching}
}

@article{10.1145/1055709.1055710,
author = {Williams, Hugh E.},
title = {Introduction to Genomic Information Retrieval},
year = {2005},
issue_date = {January 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1055709.1055710},
doi = {10.1145/1055709.1055710},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–2},
numpages = {2}
}

@article{10.1145/1028099.1028103,
author = {Park, Jinsoo and Ram, Sudha},
title = {Information Systems Interoperability: What Lies Beneath?},
year = {2004},
issue_date = {October 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1028099.1028103},
doi = {10.1145/1028099.1028103},
abstract = {Interoperability is the most critical issue facing businesses that need to access information from multiple information systems. Our objective in this research is to develop a comprehensive framework and methodology to facilitate semantic interoperability among distributed and heterogeneous information systems. A comprehensive framework for managing various semantic conflicts is proposed. Our proposed framework provides a unified view of the underlying representational and reasoning formalism for the semantic mediation process. This framework is then used as a basis for automating the detection and resolution of semantic conflicts among heterogeneous information sources. We define several types of semantic mediators to achieve semantic interoperability. A domain-independent ontology is used to capture various semantic conflicts. A mediation-based query processing technique is developed to provide uniform and integrated access to the multiple heterogeneous databases. A usable prototype is implemented as a proof-of-concept for this work. Finally, the usefulness of our approach is evaluated using three cases in different application domains. Various heterogeneous datasets are used during the evaluation phase. The results of the evaluation suggest that correct identification and construction of both schema and ontology-schema mapping knowledge play very important roles in achieving interoperability at both the data and schema levels.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {595–632},
numpages = {38},
keywords = {Information integration, ontology, semantic heterogeneity, mediators, semantic conflict resolution}
}

@article{10.1145/1028099.1028102,
author = {Williams, Hugh E. and Zobel, Justin and Bahle, Dirk},
title = {Fast Phrase Querying with Combined Indexes},
year = {2004},
issue_date = {October 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1028099.1028102},
doi = {10.1145/1028099.1028102},
abstract = {Search engines need to evaluate queries extremely fast, a challenging task given the quantities of data being indexed. A significant proportion of the queries posed to search engines involve phrases. In this article we consider how phrase queries can be efficiently supported with low disk overheads. Our previous research has shown that phrase queries can be rapidly evaluated using nextword indexes, but these indexes are twice as large as conventional inverted files. Alternatively, special-purpose phrase indexes can be used, but it is not feasible to index all phrases. We propose combinations of nextword indexes and phrase indexes with inverted files as a solution to this problem. Our experiments show that combined use of a partial nextword, partial phrase, and conventional inverted index allows evaluation of phrase queries in a quarter the time required to evaluate such queries with an inverted file alone; the additional space overhead is only 26% of the size of the inverted file.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {573–594},
numpages = {22},
keywords = {phrase queries, Nextword indexes, query evaluation, Web search}
}

@article{10.1145/1028099.1028101,
author = {Park, Seung-Taek and Pennock, David M. and Giles, C. Lee and Krovetz, Robert},
title = {Analysis of Lexical Signatures for Improving Information Persistence on the World Wide Web},
year = {2004},
issue_date = {October 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1028099.1028101},
doi = {10.1145/1028099.1028101},
abstract = {A <i>lexical signature</i> (LS) consisting of several key words from a Web document is often sufficient information for finding the document later, even if its URL has changed. We conduct a large-scale empirical study of nine methods for generating lexical signatures, including Phelps and Wilensky's original proposal (PW), seven of our own static variations, and one new dynamic method. We examine their performance on the Web over a 10-month period, and on a TREC data set, evaluating their ability to both (1) uniquely identify the original (possibly modified) document, and (2) locate other relevant documents if the original is lost. Lexical signatures chosen to minimize document frequency (DF) are good at unique identification but poor at finding relevant documents. PW works well on the relatively small TREC data set, but acts almost identically to DF on the Web, which contains billions of documents. Term-frequency-based lexical signatures (TF) are very easy to compute and often perform well, but are highly dependent on the ranking system of the search engine used. The term-frequency inverse-document-frequency- (TFIDF-) based method and hybrid methods (which combine DF with TF or TFIDF) seem to be the most promising candidates among static methods for generating effective lexical signatures. We propose a dynamic LS generator called <i>Test &amp; Select</i> (TS) to mitigate LS conflict. TS outperforms all eight static methods in terms of both extracting the desired document and finding relevant information, over three different search engines. All LS methods show significant performance degradation as documents in the corpus are edited.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {540–572},
numpages = {33},
keywords = {robust hyperlinks, dead links, TREC, information retrieval, indexing, lexical signatures, inverse document frequency, term frequency, search engines, digital libraries, Broken URLs, World Wide Web}
}

@article{10.1145/1028099.1028100,
author = {Brafman, Ronen I. and Domshlak, Carmel and Shimony, Solomon E.},
title = {Qualitative Decision Making in Adaptive Presentation of Structured Information},
year = {2004},
issue_date = {October 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/1028099.1028100},
doi = {10.1145/1028099.1028100},
abstract = {We present a new approach for adaptive presentation of structured information, based on preference-based constrained optimization techniques rooted in qualitative decision-theory. In this approach, document presentation is viewed as a configuration problem whose goal is to determine the optimal presentation of a document, while taking into account the preferences of the content provider, viewer interaction with the browser, and, possibly, some layout constraints. The preferences of the content provider are represented by a CP-net, a graphical, qualitative preference model developed in Boutilier et al. [1999]. The layout constraints are represented as geometric constraints, integrated within the optimization process. We discuss the theoretical basis of our approach, as well as implemented prototype systems for Web pages and for general media-rich document presentation.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {503–539},
numpages = {37},
keywords = {Adaptive information presentation, preference representation, qualitative decision theory}
}

@article{10.1145/1010614.1010619,
author = {King, Irwin and Ng, Cheuk Hang and Sia, Ka Cheung},
title = {Distributed Content-Based Visual Information Retrieval System on Peer-to-Peer Networks},
year = {2004},
issue_date = {July 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1010614.1010619},
doi = {10.1145/1010614.1010619},
abstract = {With the recent advances of distributed computing, the limitation of information retrieval from a centralized image collection can be removed by allowing distributed image data sources to interact with each other for data storage sharing and information retrieval. In this article, we present our design and implementation of DISCOVIR: DIStributed COntent-based Visual Information Retrieval system using the Peer-to-Peer (P2P) Network. We describe the system architecture and detail the interactions among various system modules. Specifically, we propose a Firework Query Model for distributed information retrieval, which aims to reduce the network traffic of query passing in the network. We carry out experiments to show the distributed image retrieval system and the Firework information retrieval algorithm. The results show that the algorithm reduces network traffic while increases searching performance.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {477–501},
numpages = {25},
keywords = {information retrieval, content-based image retrieval (CBIR), peer clustering, Peer-to-peer (P2P) network, intelligent query routing}
}

@article{10.1145/1010614.1010618,
author = {Miller, Bradley N. and Konstan, Joseph A. and Riedl, John},
title = {PocketLens: Toward a Personal Recommender System},
year = {2004},
issue_date = {July 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1010614.1010618},
doi = {10.1145/1010614.1010618},
abstract = {Recommender systems using collaborative filtering are a popular technique for reducing information overload and finding products to purchase. One limitation of current recommenders is that they are not portable. They can only run on large computers connected to the Internet. A second limitation is that they require the user to trust the owner of the recommender with personal preference data. Personal recommenders hold the promise of delivering high quality recommendations on palmtop computers, even when disconnected from the Internet. Further, they can protect the user's privacy by storing personal information locally, or by sharing it in encrypted form. In this article we present the new PocketLens collaborative filtering algorithm along with five peer-to-peer architectures for finding neighbors. We evaluate the architectures and algorithms in a series of offline experiments. These experiments show that Pocketlens can run on connected servers, on usually connected workstations, or on occasionally connected portable devices, and produce recommendations that are as good as the best published algorithms to date.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {437–476},
numpages = {40},
keywords = {Peer-to-Peer Networking, Collaborative Filtering, Recommender Systems, Privacy}
}

@article{10.1145/1010614.1010617,
author = {Gladney, Henry M.},
title = {Trustworthy 100-Year Digital Objects: Evidence after Every Witness is Dead},
year = {2004},
issue_date = {July 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1010614.1010617},
doi = {10.1145/1010614.1010617},
abstract = {In ancient times, wax seals impressed with signet rings were affixed to documents as evidence of their authenticity. A digital counterpart is a message authentication code fixed firmly to each important document. If a digital object is sealed together with its own audit trail, each user can examine this evidence to decide whether to trust the content---no matter how distant this user is in time, space, and social affiliation from the document's source.We propose an architecture and design that accomplish this: encapsulation of digital object content with metadata describing its origins, cryptographic sealing, webs of trust for public keys rooted in a forest of respected institutions, and a certain way of managing information identifiers. These means will satisfy emerging needs in civilian and military record management, including medical patient records, regulatory records for aircraft and pharmaceuticals, business records for financial audit, legislative and legal briefs, and scholarly works.This is true for any kind of digital object, independent of its purposes and of most data type and representation details, and provides every kind of user---information authors and editors, librarians and collection managers, and information consumers---with autonomy for implied tasks. Our prototype will conform to applicable standards, will be interoperable over most computing bases, and will be compatible with existing digital library software.The proposed architecture integrates software that is mostly available and widely accepted.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {406–436},
numpages = {31}
}

@article{10.1145/1010614.1010616,
author = {Wu, Xindong and Zhang, Chengqi and Zhang, Shichao},
title = {Efficient Mining of Both Positive and Negative Association Rules},
year = {2004},
issue_date = {July 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1010614.1010616},
doi = {10.1145/1010614.1010616},
abstract = {This paper presents an efficient method for mining both positive and negative association rules in databases. The method extends traditional associations to include association rules of forms A ⇒ ¬ B, ¬ A ⇒ B, and ¬ A ⇒ ¬ B, which indicate negative associations between itemsets. With a pruning strategy and an interestingness measure, our method scales to large databases. The method has been evaluated using both synthetic and real-world databases, and our experimental results demonstrate its effectiveness and efficiency.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {381–405},
numpages = {25},
keywords = {negative associations, Association rules}
}

@article{10.1145/1010614.1010615,
author = {Bodoff, David},
title = {Relevance Models to Help Estimate Document and Query Parameters},
year = {2004},
issue_date = {July 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1010614.1010615},
doi = {10.1145/1010614.1010615},
abstract = {A central idea of Language Models is that documents (and perhaps queries) are random variables, generated by data-generating functions that are characterized by document (query) parameters. The key new idea of this paper is to model that a relevance judgment is also generated stochastically, and that its data generating function is also governed by those same document and query parameters. The result of this addition is that any available relevance judgments are easily incorporated as additional evidence about the true document and query model parameters. An additional aspect of this approach is that it also resolves the long-standing problem of document-oriented versus query-oriented probabilities. The general approach can be used with a wide variety of hypothesized distributions for documents, queries, and relevance. We test the approach on Reuters Corpus Volume 1, using one set of possible distributions. Experimental results show that the approach does succeed in incorporating relevance data to improve estimates of both document and query parameters, but on this data and for the specific distributions we hypothesized, performance was no better than two separate one-sided models. We conclude that the model's theoretical contribution is its integration of relevance models, document models, and query models, and that the potential for additional performance improvement over one-sided methods requires refinements.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {357–380},
numpages = {24},
keywords = {Probabilistic models, language models}
}

@article{10.1145/984321.984326,
author = {Fuhr, Norbert and Groβjohann, Kai},
title = {XIRQL: An XML Query Language Based on Information Retrieval Concepts},
year = {2004},
issue_date = {April 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/984321.984326},
doi = {10.1145/984321.984326},
abstract = {XIRQL ("circle") is an XML query language that incorporates imprecision and vagueness for both structural and content-oriented query conditions. The corresponding uncertainty is handled by a consistent probabilistic model. The core features of XIRQL are (1) document ranking based on index term weighting, (2) specificity-oriented search for retrieving the most relevant parts of documents, (3) datatypes with vague predicates for dealing with specific types of content and (4) structural vagueness for vague interpretation of structural query conditions. A XIRQL database may contain several classes of documents, where all documents in a class conform to the same DTD; links between documents also are supported. XIRQL queries are translated into a path algebra, which can be processed by our HyREX retrieval engine.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {313–356},
numpages = {44},
keywords = {XQuery, ranked retrieval, XML, vague predicates, probabilistic retrieval, Path algebra}
}

@article{10.1145/984321.984325,
author = {Gon\c{c}alves, Marcos Andr\'{e} and Fox, Edward A. and Watson, Layne T. and Kipp, Neill A.},
title = {Streams, Structures, Spaces, Scenarios, Societies (5s): A Formal Model for Digital Libraries},
year = {2004},
issue_date = {April 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/984321.984325},
doi = {10.1145/984321.984325},
abstract = {Digital libraries (DLs) are complex information systems and therefore demand formal foundations lest development efforts diverge and interoperability suffers. In this article, we propose the fundamental abstractions of Streams, Structures, Spaces, Scenarios, and Societies (5S), which allow us to define digital libraries rigorously and usefully. Streams are sequences of arbitrary items used to describe both static and dynamic (e.g., video) content. Structures can be viewed as labeled directed graphs, which impose organization. Spaces are sets with operations on those sets that obey certain constraints. Scenarios consist of sequences of events or actions that modify states of a computation in order to accomplish a functional requirement. Societies are sets of entities and activities and the relationships among them. Together these abstractions provide a formal foundation to define, relate, and unify concepts---among others, of digital objects, metadata, collections, and services---required to formalize and elucidate "digital libraries". The applicability, versatility, and unifying power of the 5S model are demonstrated through its use in three distinct applications: building and interpretation of a DL taxonomy, informal and formal analysis of case studies of digital libraries (NDLTD and OAI), and utilization as a formal basis for a DL description language.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {270–312},
numpages = {43},
keywords = {definitions, applications., taxonomy, foundations}
}

@article{10.1145/984321.984324,
author = {Lu, Wen-Hsiang and Chien, Lee-Feng and Lee, Hsi-Jian},
title = {Anchor Text Mining for Translation of Web Queries: A Transitive Translation Approach},
year = {2004},
issue_date = {April 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/984321.984324},
doi = {10.1145/984321.984324},
abstract = {To discover translation knowledge in diverse data resources on the Web, this article proposes an effective approach to finding translation equivalents of query terms and constructing multilingual lexicons through the mining of Web anchor texts and link structures. Although Web anchor texts are wide-scoped hypertext resources, not every particular pair of languages contains sufficient anchor texts for effective extraction of translations for Web queries. For more generalized applications, the approach is designed based on a transitive translation model. The translation equivalents of a query term can be extracted via its translation in an intermediate language. To reduce interference from translation errors, the approach further integrates a competitive linking algorithm into the process of determining the most probable translation. A series of experiments has been conducted, including performance tests on term translation extraction, cross-language information retrieval, and translation suggestions for practical Web search services, respectively. The obtained experimental results have shown that the proposed approach is effective in extracting translations of unknown queries, is easy to combine with the probabilistic retrieval model to improve the cross-language retrieval performance, and is very useful when the considered language pairs lack a sufficient number of anchor texts. Based on the approach, an experimental system called LiveTrans has been developed for English--Chinese cross-language Web search.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {242–269},
numpages = {28},
keywords = {competitive linking algorithm, anchor text mining, cross-language Web search, Multilingual translation, cross-language information retrieval}
}

@article{10.1145/984321.984323,
author = {Ma\~{n}a-L\'{o}pez, Manuel J. and De Buenaga, Manuel and G\'{o}mez-Hidalgo, Jos\'{e} M.},
title = {Multidocument Summarization: An Added Value to Clustering in Interactive Retrieval},
year = {2004},
issue_date = {April 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/984321.984323},
doi = {10.1145/984321.984323},
abstract = {A more and more generalized problem in effective information access is the presence in the same corpus of multiple documents that contain similar information. Generally, users may be interested in locating, for a topic addressed by a group of similar documents, one or several particular aspects. This kind of task, called instance or aspectual retrieval, has been explored in several TREC Interactive Tracks. In this article, we propose in addition to the classification capacity of clustering techniques, the possibility of offering a indicative extract about the contents of several sources by means of multidocument summarization techniques. Two kinds of summaries are provided. The first one covers the similarities of each cluster of documents retrieved. The second one shows the particularities of each document with respect to the common topic in the cluster. The document multitopic structure has been used in order to determine similarities and differences of topics in the cluster of documents. The system is independent of document domain and genre. An evaluation of the proposed system with users proves significant improvements in effectiveness. The results of previous experiments that have compared clustering algorithms are also reported.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {215–241},
numpages = {27},
keywords = {topic segmentation, Multidocument summarization}
}

@article{10.1145/984321.984322,
author = {Zhai, Chengxiang and Lafferty, John},
title = {A Study of Smoothing Methods for Language Models Applied to Information Retrieval},
year = {2004},
issue_date = {April 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/984321.984322},
doi = {10.1145/984321.984322},
abstract = {Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role---to make the estimated document language model more accurate and to "explain" the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to---or better than---the best results achieved using a single smoothing method and exhaustive parameter search on the test data.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {179–214},
numpages = {36},
keywords = {risk minimization, backoff smoothing, leave-one-out, absolute discounting smoothing, interpolation smoothing, TF-IDF weighting, Jelinek--Mercer smoothing, Dirichlet prior smoothing, term weighting, two-stage smoothing, Statistical language models, EM algorithm}
}

@article{10.1145/963770.963776,
author = {Deshpande, Mukund and Karypis, George},
title = {Item-Based Top-<i>N</i> Recommendation Algorithms},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/963770.963776},
doi = {10.1145/963770.963776},
abstract = {The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems---a personalized information filtering technology used to identify a set of items that will be of interest to a certain user. User-based collaborative filtering is the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers, which in typical commercial applications can be several millions. To address these scalability concerns model-based recommendation techniques have been developed. These techniques analyze the user--item matrix to discover relations between the different items and use these relations to compute the list of recommendations.In this article, we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {143–177},
numpages = {35},
keywords = {e-commerce, world wide web, predicting user behavior}
}

@article{10.1145/963770.963775,
author = {Huang, Zan and Chen, Hsinchun and Zeng, Daniel},
title = {Applying Associative Retrieval Techniques to Alleviate the Sparsity Problem in Collaborative Filtering},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/963770.963775},
doi = {10.1145/963770.963775},
abstract = {Recommender systems are being widely applied in many application settings to suggest products, services, and information items to potential consumers. Collaborative filtering, the most successful recommendation approach, makes recommendations based on past transactions and feedback from consumers sharing similar interests. A major problem limiting the usefulness of collaborative filtering is the sparsity problem, which refers to a situation in which transactional or feedback data is sparse and insufficient to identify similarities in consumer interests. In this article, we propose to deal with this sparsity problem by applying an associative retrieval framework and related spreading activation algorithms to explore transitive associations among consumers through their past transactions and feedback. Such transitive associations are a valuable source of information to help infer consumer interests and can be explored to deal with the sparsity problem. To evaluate the effectiveness of our approach, we have conducted an experimental study using a data set from an online bookstore. We experimented with three spreading activation algorithms including a constrained Leaky Capacitor algorithm, a branch-and-bound serial symbolic search algorithm, and a Hopfield net parallel relaxation search algorithm. These algorithms were compared with several collaborative filtering approaches that do not consider the transitive associations: a simple graph search approach, two variations of the user-based approach, and an item-based approach. Our experimental results indicate that spreading activation-based approaches significantly outperformed the other collaborative filtering methods as measured by recommendation precision, recall, the F-measure, and the rank score. We also observed the over-activation effect of the spreading activation approach, that is, incorporating transitive associations with past transactional data that is not sparse may "dilute" the data used to infer user preferences and lead to degradation in recommendation performance.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {116–142},
numpages = {27},
keywords = {Recommender system, spreading activation, collaborative filtering, associative retrieval, sparsity problem}
}

@article{10.1145/963770.963774,
author = {Hofmann, Thomas},
title = {Latent Semantic Models for Collaborative Filtering},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/963770.963774},
doi = {10.1145/963770.963774},
abstract = {Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {89–115},
numpages = {27}
}

@article{10.1145/963770.963773,
author = {Middleton, Stuart E. and Shadbolt, Nigel R. and De Roure, David C.},
title = {Ontological User Profiling in Recommender Systems},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/963770.963773},
doi = {10.1145/963770.963773},
abstract = {We explore a novel ontological approach to user profiling within recommender systems, working on the problem of recommending on-line academic research papers. Our two experimental systems, Quickstep and Foxtrot, create user profiles from unobtrusively monitored behaviour and relevance feedback, representing the profiles in terms of a research paper topic ontology. A novel profile visualization approach is taken to acquire profile feedback. Research papers are classified using ontological classes and collaborative recommendation algorithms used to recommend papers seen by similar people on their current topics of interest. Two small-scale experiments, with 24 subjects over 3 months, and a large-scale experiment, with 260 subjects over an academic year, are conducted to evaluate different aspects of our approach. Ontological inference is shown to improve user profiling, external ontological knowledge used to successfully bootstrap a recommender system and profile visualization employed to improve profiling accuracy. The overall performance of our ontological recommender systems are also presented and favourably compared to other systems in the literature.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {54–88},
numpages = {35},
keywords = {ontology, personalization, user profiling, Agent, machine learning, recommender systems, user modelling}
}

@article{10.1145/963770.963772,
author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Terveen, Loren G. and Riedl, John T.},
title = {Evaluating Collaborative Filtering Recommender Systems},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/963770.963772},
doi = {10.1145/963770.963772},
abstract = {Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {5–53},
numpages = {49},
keywords = {evaluation, Collaborative filtering, metrics, recommender systems}
}

@article{10.1145/963770.963771,
author = {Konstan, Joseph A.},
title = {Introduction to Recommender Systems: Algorithms and Evaluation},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/963770.963771},
doi = {10.1145/963770.963771},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–4},
numpages = {4}
}

@article{10.1145/944012.944018,
author = {ACM Transactions on Information Systems staff},
title = {TOIS Reviewers},
year = {2003},
issue_date = {October 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/944012.944018},
doi = {10.1145/944012.944018},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {492–493},
numpages = {2}
}

@article{10.1145/944012.944017,
author = {Si, Luo and Callan, Jamie},
title = {A Semisupervised Learning Method to Merge Search Engine Results},
year = {2003},
issue_date = {October 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/944012.944017},
doi = {10.1145/944012.944017},
abstract = {The proliferation of searchable text databases on local area networks and the Internet causes the problem of finding information that may be distributed among many disjoint text databases (distributed information retrieval). How to merge the results returned by selected databases is an important subproblem of the distributed information retrieval task. Previous research assumed that either resource providers cooperate to provide normalizing statistics or search clients download all retrieved documents and compute normalized scores without cooperation from resource providers.This article presents a semisupervised learning solution to the result merging problem. The key contribution is the observation that information used to create resource descriptions for resource selection can also be used to create a centralized sample database to guide the normalization of document scores returned by different databases. At retrieval time, the query is sent to the selected databases, which return database-specific document scores, and to a centralized sample database, which returns database-independent document scores. Documents that have both a database-specific score and a database-independent score serve as training data for learning to normalize the scores of other documents. An extensive set of experiments demonstrates that this method is more effective than the well-known CORI result-merging algorithm under a variety of conditions.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {457–491},
numpages = {35},
keywords = {Distributed information retrieval, results merging, resource selection, resource ranking, server selection, semisupervised learning method}
}

@article{10.1145/944012.944016,
author = {Powell, Allison L. and French, James C.},
title = {Comparing the Performance of Collection Selection Algorithms},
year = {2003},
issue_date = {October 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/944012.944016},
doi = {10.1145/944012.944016},
abstract = {The proliferation of online information resources increases the importance of effective and efficient information retrieval in a multicollection environment. Multicollection searching is cast in three parts: collection selection (also referred to as database selection), query processing and results merging. In this work, we focus our attention on the evaluation of the first step, collection selection.In this article, we present a detailed discussion of the methodology that we used to evaluate and compare collection selection approaches, covering both test environments and evaluation measures. We compare the CORI, CVV and gGLOSS collection selection approaches using six test environments utilizing three document testbeds. We note similar trends in performance among the collection selection approaches, but the CORI approach consistently outperforms the other approaches, suggesting that effective collection selection can be achieved using limited information about each collection.The contributions of this work are both the assembled evaluation methodology as well as the application of that methodology to compare collection selection approaches in a standardized environment.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {412–456},
numpages = {45},
keywords = {resource discovery, server ranking, text retrieval, resource selection, server selection, Collection selection, distributed information retrieval, metasearch engine, database selection, distributed text retrieval, resource ranking}
}

@article{10.1145/944012.944015,
author = {Eastman, Caroline M. and Jansen, Bernard J.},
title = {Coverage, Relevance, and Ranking: The Impact of Query Operators on Web Search Engine Results},
year = {2003},
issue_date = {October 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/944012.944015},
doi = {10.1145/944012.944015},
abstract = {Research has reported that about 10% of Web searchers utilize advanced query operators, with the other 90% using extremely simple queries. It is often assumed that the use of query operators, such as Boolean operators and phrase searching, improves the effectiveness of Web searching. We test this assumption by examining the effects of query operators on the performance of three major Web search engines. We selected one hundred queries from the transaction log of a Web search service. Each of these original queries contained query operators such as AND, OR, MUST APPEAR (+), or PHRASE (" "). We then removed the operators from these one hundred advanced queries. We submitted both the original and modified queries to three major Web search engines; a total of 600 queries were submitted and 5,748 documents evaluated. We compared the results from the original queries with the operators to the results from the modified queries without the operators. We examined the results for changes in coverage, relative precision, and ranking of relevant documents. The use of most query operators had no significant effect on coverage, relative precision, or ranking, although the effect varied depending on the search engine. We discuss implications for the effectiveness of searching techniques as currently taught, for future information retrieval system design, and for future research.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {383–411},
numpages = {29},
keywords = {Web results, Boolean operators, Relative precision, coverage, search engines, query operators, ranking}
}

@article{10.1145/944012.944014,
author = {Chang, Edward and Li, Beitao},
title = {MEGA---the Maximizing Expected Generalization Algorithm for Learning Complex Query Concepts},
year = {2003},
issue_date = {October 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/944012.944014},
doi = {10.1145/944012.944014},
abstract = {Specifying exact query concepts has become increasingly challenging to end-users. This is because many query concepts (e.g., those for looking up a multimedia object) can be hard to articulate, and articulation can be subjective. In this study, we propose a query-concept learner that learns query criteria through an intelligent sampling process. Our concept learner aims to fulfill two primary design objectives: (1) it has to be expressive in order to model most practical query concepts and (2) it must learn a concept quickly and with a small number of labeled data since online users tend to be too impatient to provide much feedback. To fulfill the first goal, we model query concepts in k-CNF, which can express almost all practical query concepts. To fulfill the second design goal, we propose our maximizing expected generalization algorithm (MEGA), which converges to target concepts quickly by its two complementary steps: sample selection and concept refinement. We also propose a divide-and-conquer method that divides the concept-learning task into G subtasks to achieve speedup. We notice that a task must be divided carefully, or search accuracy may suffer. Through analysis and mining results, we observe that organizing image features in a multiresolution manner, and minimizing intragroup feature correlation, can speed up query-concept learning substantially while maintaining high search accuracy. Through examples, analysis, experiments, and a prototype implementation, we show that MEGA converges to query concepts significantly faster than traditional methods.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {347–382},
numpages = {36},
keywords = {Active learning, data mining, query concept, relevance feedback}
}

@article{10.1145/944012.944013,
author = {Turney, Peter D. and Littman, Michael L.},
title = {Measuring Praise and Criticism: Inference of Semantic Orientation from Association},
year = {2003},
issue_date = {October 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/944012.944013},
doi = {10.1145/944012.944013},
abstract = {The evaluative character of a word is called its semantic orientation. Positive semantic orientation indicates praise (e.g., "honest", "intrepid") and negative semantic orientation indicates criticism (e.g., "disturbing", "superfluous"). Semantic orientation varies in both direction (positive or negative) and degree (mild to strong). An automated system for measuring semantic orientation would have application in text classification, text filtering, tracking opinions in online discussions, analysis of survey responses, and automated chat systems (chatbots). This article introduces a method for inferring the semantic orientation of a word from its statistical association with a set of positive and negative paradigm words. Two instances of this approach are evaluated, based on two different statistical measures of word association: pointwise mutual information (PMI) and latent semantic analysis (LSA). The method is experimentally tested with 3,596 words (including adjectives, adverbs, nouns, and verbs) that have been manually labeled positive (1,614 words) and negative (1,982 words). The method attains an accuracy of 82.8% on the full test set, but the accuracy rises above 95% when the algorithm is allowed to abstain from classifying mild words.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {315–346},
numpages = {32},
keywords = {web mining, text classification, mutual information, semantic association, latent semantic analysis, text mining, unsupervised learning, semantic orientation}
}

@article{10.1145/858476.858479,
author = {Upstill, Trystan and Craswell, Nick and Hawking, David},
title = {Query-Independent Evidence in Home Page Finding},
year = {2003},
issue_date = {July 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/858476.858479},
doi = {10.1145/858476.858479},
abstract = {Hyperlink recommendation evidence, that is, evidence based on the structure of a web's link graph, is widely exploited by commercial Web search systems. However there is little published work to support its popularity. Another form of query-independent evidence, URL-type, has been shown to be beneficial on a home page finding task. We compared the usefulness of these types of evidence on the home page finding task, combined with both content and anchor text baselines. Our experiments made use of five query sets spanning three corpora---one enterprise crawl, and the WT10g and VLC2 Web test collections.We found that, in optimal conditions, all of the query-independent methods studied (in-degree, URL-type, and two variants of PageRank) offered a better than random improvement on a content-only baseline. However, only URL-type offered a better than random improvement on an anchor text baseline. In realistic settings, for either baseline, only URL-type offered consistent gains. In combination with URL-type the anchor text baseline was more useful for finding popular home pages, but URL-type with content was more useful for finding randomly selected home pages. We conclude that a general home page finding system should combine evidence from document content, anchor text, and URL-type classification.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {286–313},
numpages = {28},
keywords = {connectivity, citation and link analysis, Web information retrieval}
}

@article{10.1145/858476.858478,
author = {Bolchini, Cristiana and Salice, Fabio and Schreiber, Fabio A. and Tanca, Letizia},
title = {Logical and Physical Design Issues for Smart Card Databases},
year = {2003},
issue_date = {July 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/858476.858478},
doi = {10.1145/858476.858478},
abstract = {The design of very small databases for smart cards and for portable embedded systems is deeply constrained by the peculiar features of the physical medium. We propose a joint approach to the logical and physical database design phases and evaluate several data structures with respect to the performance, power consumption, and endurance parameters of read/program operations on the Flash-EEPROM storage medium.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {254–285},
numpages = {32},
keywords = {Design methodology, personal information systems, flash memory, access methods, smart card, data structures}
}

@article{10.1145/858476.858477,
author = {Leroy, Gondy and Lally, Ann M. and Chen, Hsinchun},
title = {The Use of Dynamic Contexts to Improve Casual Internet Searching},
year = {2003},
issue_date = {July 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/858476.858477},
doi = {10.1145/858476.858477},
abstract = {Research has shown that most users' online information searches are suboptimal. Query optimization based on a relevance feedback or genetic algorithm using dynamic query contexts can help casual users search the Internet. These algorithms can draw on implicit user feedback based on the surrounding links and text in a search engine result set to expand user queries with a variable number of keywords in two manners. Positive expansion adds terms to a user's keywords with a Boolean "and," negative expansion adds terms to the user's keywords with a Boolean "not." Each algorithm was examined for three user groups, high, middle, and low achievers, who were classified according to their overall performance. The interactions of users with different levels of expertise with different expansion types or algorithms were evaluated. The genetic algorithm with negative expansion tripled recall and doubled precision for low achievers, but high achievers displayed an opposed trend and seemed to be hindered in this condition. The effect of other conditions was less substantial.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {229–253},
numpages = {25},
keywords = {Internet, automatic query expansion, Information retrieval, personalization, relevance feedback, genetic algorithm, implicit user feedback}
}

@article{10.1145/763693.763696,
author = {Amato, Giuseppe and Rabitti, Fausto and Savino, Pasquale and Zezula, Pavel},
title = {Region Proximity in Metric Spaces and Its Use for Approximate Similarity Search},
year = {2003},
issue_date = {April 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/763693.763696},
doi = {10.1145/763693.763696},
abstract = {Similarity search structures for metric data typically bound object partitions by ball regions. Since regions can overlap, a relevant issue is to estimate the proximity of regions in order to predict the number of objects in the regions' intersection. This paper analyzes the problem using a probabilistic approach and provides a solution that effectively computes the proximity through realistic heuristics that only require small amounts of auxiliary data. An extensive simulation to validate the technique is provided. An application is developed to demonstrate how the proximity measure can be successfully applied to the approximate similarity search. Search speedup is achieved by ignoring data regions whose proximity to the query region is smaller than a user-defined threshold. This idea is implemented in a metric tree environment for the similarity range and "nearest neighbors" queries. Several measures of efficiency and effectiveness are applied to evaluate proposed approximate search algorithms on real-life data sets. An analytical model is developed to relate proximity parameters and the quality of search. Improvements of two orders of magnitude are achieved for moderately approximated search results. We demonstrate that the precision of proximity measures can significantly influence the quality of approximated algorithms.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {192–227},
numpages = {36},
keywords = {performance evaluation, metric data, Approximation algorithms, metric trees, approximate similarity search}
}

@article{10.1145/763693.763695,
author = {Bertino, Elisa and Fan, Jianping and Ferrari, Elena and Hacid, Mohand-Said and Elmagarmid, Ahmed K. and Zhu, Xingquan},
title = {A Hierarchical Access Control Model for Video Database Systems},
year = {2003},
issue_date = {April 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/763693.763695},
doi = {10.1145/763693.763695},
abstract = {Content-based video database access control is becoming very important, but it depends on the progresses of the following related research issues: (a) efficient video analysis for supporting semantic visual concept representation; (b) effective video database indexing structure; (c) the development of suitable video database models; and (d) the development of access control models tailored to the characteristics of video data. In this paper, we propose a novel approach to support multilevel access control in video databases. Our access control technique combines a video database indexing mechanism with a hierarchical organization of visual concepts (i.e., video database indexing units), so that different classes of users can access different video elements or even the same video element with different quality levels according to their permissions. These video elements, which, in our access control mechanism, are used for specifying the authorization objects, can be a semantic cluster, a subcluster, a video scene, a video shot, a video frame, or even a salient object (i.e., region of interest). In the paper, we first introduce our techniques for obtaining these multilevel video access units. We also propose a hierarchical video database indexing technique to support our multilevel video access control mechanism. Then, we present an innovative access control model which is able to support flexible multilevel access control to video elements. Moreover, the application of our multilevel video database modeling, representation, and indexing for MPEG-7 is discussed.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {155–191},
numpages = {37},
keywords = {access control, Video database models, indexing schemes}
}

@article{10.1145/763693.763694,
author = {Moldovan, Dan and Pa\c{s}ca, Marius and Harabagiu, Sanda and Surdeanu, Mihai},
title = {Performance Issues and Error Analysis in an Open-Domain Question Answering System},
year = {2003},
issue_date = {April 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/763693.763694},
doi = {10.1145/763693.763694},
abstract = {This paper presents an in-depth analysis of a state-of-the-art Question Answering system. Several scenarios are examined: (1) the performance of each module in a serial baseline system, (2) the impact of feedbacks and the insertion of a logic prover, and (3) the impact of various retrieval strategies and lexical resources. The main conclusion is that the overall performance depends on the depth of natural language processing resources and the tools used for answer finding.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {133–154},
numpages = {22},
keywords = {natural language applications, performance analysis, Question answering, text retrieval}
}

@article{10.1145/635484.635488,
author = {Conrad, Jack G. and Claussen, Joanne R. S.},
title = {Early User---System Interaction for Database Selection in Massive Domain-Specific Online Environments},
year = {2003},
issue_date = {January 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/635484.635488},
doi = {10.1145/635484.635488},
abstract = {The continued growth of very large data environments such as Westlaw and Dialog, in addition to the World Wide Web, increases the importance of effective and efficient database selection and searching. Current research focuses largely on completely autonomous and automatic selection, searching, and results merging in distributed environments. This fully automatic approach has significant deficiencies, including reliance upon thresholds below which databases with relevant documents are not searched (compromised recall). It also merges documents, often from disparate data sources that users may have discarded before their source selection task proceeded (diluted precision). We examine the impact that early user interaction can have on the process of database selection. After analyzing thousands of real user queries, we show that precision can be significantly increased when queries are categorized by the users themselves, then handled effectively by the system. Such query categorization strategies may eliminate limitations of fully automated query processing approaches. Our system harnesses the WIN search engine, a sibling to INQUERY, run against one or more authority sources when search is required. We compare our approach to one that does not recognize or utilize distinct features associated with user queries. We show that by avoiding a one-size-fits-all approach that restricts the role users can play in information discovery, database selection effectiveness can be appreciably improved.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {94–131},
numpages = {38},
keywords = {metadata for retrieval, user interaction, structuring information to aid search and navigation, Database selection}
}

@article{10.1145/635484.635487,
author = {Ganesan, Prasanna and Garcia-Molina, Hector and Widom, Jennifer},
title = {Exploiting Hierarchical Domain Structure to Compute Similarity},
year = {2003},
issue_date = {January 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/635484.635487},
doi = {10.1145/635484.635487},
abstract = {The notion of similarity between objects finds use in many contexts, for example, in search engines, collaborative filtering, and clustering. Objects being compared often are modeled as sets, with their similarity traditionally determined based on set intersection. Intersection-based measures do not accurately capture similarity in certain domains, such as when the data is sparse or when there are known relationships between items within sets. We propose new measures that exploit a hierarchical domain structure in order to produce more intuitive similarity scores. We extend our similarity measures to provide appropriate results in the presence of multisets (also handled unsatisfactorily by traditional measures), for example, to correctly compute the similarity between customers who buy several instances of the same product (say milk), or who buy several products in the same category (say dairy products). We also provide an experimental comparison of our measures against traditional similarity measures, and report on a user study that evaluated how well our measures match human intuition.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {64–93},
numpages = {30},
keywords = {data mining, hierarchy, collaborative filtering, Similarity measures}
}

@article{10.1145/635484.635486,
author = {Calado, P\'{a}vel and Ribeiro-Neto, Berthier and Ziviani, Nivio and Moura, Edleno and Silva, Ilm\'{e}rio},
title = {Local versus Global Link Information in the Web},
year = {2003},
issue_date = {January 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/635484.635486},
doi = {10.1145/635484.635486},
abstract = {Information derived from the cross-references among the documents
in a hyperlinked environment, usually referred to as link
information, is considered important since it can be used to
effectively improve document retrieval. Depending on the retrieval
strategy, link information can be local or global. Local link
information is derived from the set of documents returned as
answers to the current user query. Global link information is
derived from all the documents in the collection. In this work, we
investigate how the use of local link information compares to the
use of global link information. For the comparison, we run a series
of experiments using a large document collection extracted from the
Web. For our reference collection, the results indicate that the
use of local link information improves precision by 74%.
When global link information is used, precision improves by
35%. However, when only the first 10 documents in the
ranking are considered, the average gain in precision obtained with
the use of global link information is higher than the gain obtained
with the use of local link information. This is an interesting
result since it provides insight and justification for the use of
global link information in major Web search engines, where users
are mostly interested in the first 10 answers. Furthermore, global
information can be computed in the background, which allows
speeding up query processing.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {42–63},
numpages = {22},
keywords = {link analysis, Belief networks, local and global information, World Wide Web}
}

@article{10.1145/635484.635485,
author = {Gravano, Luis and Ipeirotis, Panagiotis G. and Sahami, Mehran},
title = {QProber: A System for Automatic Classification of Hidden-Web Databases},
year = {2003},
issue_date = {January 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/635484.635485},
doi = {10.1145/635484.635485},
abstract = {The contents of many valuable Web-accessible databases are only available through search interfaces and are hence invisible to traditional Web "crawlers." Recently, commercial Web sites have started to manually organize Web-accessible databases into Yahoo!-like hierarchical classification schemes. Here we introduce QProber, a modular system that automates this classification process by using a small number of query probes, generated by document classifiers. QProber can use a variety of types of classifiers to generate the probes. To classify a database, QProber does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of QProber over collections of real documents, experimenting with different types of document classifiers and retrieval models. We have also tested our system with over one hundred Web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–41},
numpages = {41},
keywords = {hidden Web, Database classification, Web databases}
}

@article{10.1145/582415.582418,
author = {J\"{a}rvelin, Kalervo and Kek\"{a}l\"{a}inen, Jaana},
title = {Cumulated Gain-Based Evaluation of IR Techniques},
year = {2002},
issue_date = {October 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/582415.582418},
doi = {10.1145/582415.582418},
abstract = {Modern large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop IR techniques in this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of IR techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using TREC data: sample system run results for 20 queries in TREC-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, for example, from the user point of view.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {422–446},
numpages = {25},
keywords = {cumulated gain, Graded relevance judgments}
}

@article{10.1145/582415.582417,
author = {Feng, Ling and Chang, Elizabeth and Dillon, Tharam},
title = {A Semantic Network-Based Design Methodology for XML Documents},
year = {2002},
issue_date = {October 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/582415.582417},
doi = {10.1145/582415.582417},
abstract = {The eXtensible Markup Language (XML) is fast emerging as the dominant standard for describing and interchanging data among various systems and databases on the Internet. It offers the Document Type Definition (DTD) as a formalism for defining the syntax and structure of XML documents. The XML Schema definition language, as a replacement for the DTD, provides more rich facilities for defining and constraining the content of XML documents. However, it does not concentrate on the semantics that underlies these documents, representing a logical data model rather than a conceptual model. To enable efficient business application development in large-scale electronic commerce environments, it is necessary to describe and model real-world data semantics and their complex interrelationships. In this article, we describe a design methodology for XML documents. The aim is to enforce XML conceptual modeling power and bridge the gap between software development and XML document structures. The proposed methodology is comprised of two design levels: the semantic level and the schema level. The first level is based on a semantic network, which provides semantic modeling of XML through four major components: a set of atomic and complex nodes, representing real-world objects; a set of directed edges, representing semantic relationships between the objects; a set of labels denoting different types of semantic relationships, including aggregation, generalization, association, and of-property relationships; and finally a set of constraints defined over nodes and edges to constrain semantic relationships and object domains. The other level of the proposed methodology is concerned with detailed XML schema design, including element/attribute declarations and simple/complex type definitions. The mapping between the two design levels is proposed to transform the XML semantic model into the XML Schema, based on which XML documents can be systematically created, managed, and validated.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {390–421},
numpages = {32},
keywords = {XML Schema, conceptual modeling, semantic network, design methodology, XML}
}

@article{10.1145/582415.582416,
author = {Amati, Gianni and Van Rijsbergen, Cornelis Joost},
title = {Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness},
year = {2002},
issue_date = {October 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/582415.582416},
doi = {10.1145/582415.582416},
abstract = {We introduce and create a framework for deriving probabilistic models of Information Retrieval. The models are nonparametric models of IR obtained in the language model approach. We derive term-weighting models by measuring the divergence of the actual term distribution from that obtained under a random process. Among the random processes we study the binomial distribution and Bose--Einstein statistics. We define two types of term frequency normalization for tuning term weights in the document--query matching process. The first normalization assumes that documents have the same length and measures the information gain with the observed term once it has been accepted as a good descriptor of the observed document. The second normalization is related to the document length and to other statistics. These two normalization methods are applied to the basic models in succession to obtain weighting formulae. Results show that our framework produces different nonparametric models forming baseline alternatives to the standard tf-idf model.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {357–389},
numpages = {33},
keywords = {Bose--Einstein statistics, idf, Poisson, probabilistic models, document length normalization, randomness, binomial law, term frequency normalization, BM25, Laplace, eliteness, succession law, Aftereffect model, term weighting, information retrieval}
}

@article{10.1145/568727.568730,
author = {Cannane, Adam and Williams, Hugh E.},
title = {A General-Purpose Compression Scheme for Large Collections},
year = {2002},
issue_date = {July 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/568727.568730},
doi = {10.1145/568727.568730},
abstract = {Compression of large collections can lead to improvements in retrieval times by offsetting the CPU decompression costs with the cost of seeking and retrieving data from disk. We propose a semistatic phrase-based approach called xray that builds a model offline using sample training data extracted from a collection, and then compresses the entire collection online in a single pass. The particular benefits of xray are that it can be used in applications where individual records or documents must be decompressed, and that decompression is fast. The xray scheme also allows new data to be added to a collection without modifying the semistatic model. Moreover, xray can be used to compress general-purpose data such as genomic, scientific, image, and geographic collections without prior knowledge of the structure of the data. We show that xray is effective on both text and general-purpose collections. In general, xray is more effective than the popular gzip and compress schemes, while being marginally less effective than bzip2. We also show that xray is efficient: of the popular schemes we tested, it is typically only slower than gzip in decompression. Moreover, the query evaluation costs of retrieval of documents from a large collection with our search engine is improved by more than 30% when xray is incorporated compared to an uncompressed approach. We use simple techniques for obtaining the training data from the collection to be compressed and show that with just over 4% of data the entire collection can be effectively compressed. We also propose four schemes for phrase-match selection during the single pass compression of the collection. We conclude that with these novel approaches xray is a fast and effective scheme for compression and decompression of large general-purpose collections.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {329–355},
numpages = {27},
keywords = {phrase-based compression, random access, sampling}
}

@article{10.1145/568727.568729,
author = {Owei, Vesper},
title = {An Intelligent Approach to Handling Imperfect Information in Concept-Based Natural Language Queries},
year = {2002},
issue_date = {July 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/568727.568729},
doi = {10.1145/568727.568729},
abstract = {Missing information, imprecision, inconsistency, vagueness, uncertainty, and ignorance abound in information systems. Such imperfection is a fact of life in database systems. Although these problems are widely studied in relational database systems, this is not the case in conceptual query systems. And yet, concept-based query languages have been proposed and some are already commercial products. It is therefore imperative to study these problems in concept-based query languages, with a view to prescribing formal approaches to dealing with the problems. In this article, we have done just that for a concept-based natural language query system that we developed. A methodology for handling and resolving each type of imperfection is developed. The proposed approaches are automated as much as possible, with the user mainly serving an assistive function.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {291–328},
numpages = {38},
keywords = {conceptual query language, natural language interface, semantically mismatched query, missing information, inexplicit query, natural language query, inconsistency, incomplete information, imperfect queries, concept-based query, elliptical query, anaphoric query, ambiguous query}
}

@article{10.1145/568727.568728,
author = {Carpineto, Claudio and Romano, Giovanni and Giannini, Vittorio},
title = {Improving Retrieval Feedback with Multiple Term-Ranking Function Combination},
year = {2002},
issue_date = {July 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/568727.568728},
doi = {10.1145/568727.568728},
abstract = {In this article we consider methods for automatic query expansion from top retrieved documents (i.e., retrieval feedback) that make use of various functions for scoring expansion terms within Rocchio's classical reweighting scheme. An analytical comparison shows that the retrieval performance of methods based on distinct term-scoring functions is comparable on the whole query set but differs considerably on single queries, consistent with the fact that the ordered sets of expansion terms suggested for each query by the different functions are largely uncorrelated. Motivated by these findings, we argue that the results of multiple functions can be merged, by analogy with ensembling classifiers, and present a simple combination technique based on the rank values of the suggested terms. The combined retrieval feedback method is effective not only with respect to unexpanded queries but also to any individual method, with notable improvements on the system's precision. Furthermore, the combined method is robust with respect to variation of experimental parameters and it is beneficial even when the same information needs are expressed with shorter queries.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {259–290},
numpages = {32},
keywords = {information retrieval, short queries, retrieval feedback, automatic query expansion, method combination}
}

@article{10.1145/506309.506313,
author = {Zhu, Lei and Rao, Al Bing and Zhang, Aldong},
title = {Theory of Keyblock-Based Image Retrieval},
year = {2002},
issue_date = {April 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/506309.506313},
doi = {10.1145/506309.506313},
abstract = {The success of text-based retrieval motivates us to investigate analogous techniques which can support the querying and browsing of image data. However, images differ significantly from text both syntactically and semantically in their mode of representing and expressing information. Thus, the generalization of information retrieval from the text domain to the image domain is non-trivial. This paper presents a framework for information retrieval in the image domain which supports content-based querying and browsing of images. A critical first step to establishing such a framework is to construct a codebook of "keywords" for images which is analogous to the dictionary for text documents. We refer to such "keywords" in the image domain as "keyblocks." In this paper, we first present various approaches to generating a codebook containing keyblocks at different resolutions. Then we present a keyblock-based approach to content-based image retrieval. In this approach, each image is encoded as a set of one-dimensional index codes linked to the keyblocks in the codebook, analogous to considering a text document as a linear list of keywords. Generalizing upon text-based information retrieval methods, we then offer various techniques for image-based information retrieval. By comparing the performance of this approach with conventional techniques using color and texture features, we demonstrate the effectiveness of the keyblock-based approach to content-based image retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {224–257},
numpages = {34},
keywords = {clustering, content-based image retrieval, keyblock, codebook}
}

@article{10.1145/506309.506312,
author = {Heinz, Steffen and Zobel, Justin and Williams, Hugh E.},
title = {Burst Tries: A Fast, Efficient Data Structure for String Keys},
year = {2002},
issue_date = {April 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/506309.506312},
doi = {10.1145/506309.506312},
abstract = {Many applications depend on efficient management of large sets of distinct strings in memory. For example, during index construction for text databases a record is held for each distinct word in the text, containing the word itself and information such as counters. We propose a new data structure, the burst trie, that has significant advantages over existing options for such applications: it uses about the same memory as a binary search tree; it is as fast as a trie; and, while not as fast as a hash table, a burst trie maintains the strings in sorted or near-sorted order. In this paper we describe burst tries and explore the parameters that govern their performance. We experimentally determine good choices of parameters, and compare burst tries to other structures used for the same task, with a variety of data sets. These experiments show that the burst trie is particularly effective for the skewed frequency distributions common in text collections, and dramatically outperforms all other data structures for the task of managing strings while maintaining sort order.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {192–223},
numpages = {32},
keywords = {vocabulary accumulation, Binary trees, splay trees, string data structures, text databases, tries}
}

@article{10.1145/506309.506311,
author = {Chowdhury, Abdur and Frieder, Ophir and Grossman, David and McCabe, Mary Catherine},
title = {Collection Statistics for Fast Duplicate Document Detection},
year = {2002},
issue_date = {April 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/506309.506311},
doi = {10.1145/506309.506311},
abstract = {We present a new algorithm for duplicate document detection that
uses collection statistics. We compare our approach with the
state-of-the-art approach using multiple collections. These
collections include a 30 MB 18,577 web document collection
developed by Excite@Home and three NIST collections. The first NIST
collection consists of 100 MB 18,232 LA-Times documents, which is
roughly similar in the number of documents to the
Excite&amp;at;Home collection. The other two collections are both 2
GB and are the 247,491-web document collection and the TREC disks 4
and 5---528,023 document collection. We show that our approach
called I-Match, scales in terms of the number of documents and
works well for documents of all sizes. We compared our solution to
the state of the art and found that in addition to improved
accuracy of detection, our approach executed in roughly one-fifth
the time.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {171–191},
numpages = {21}
}

@article{10.1145/506309.506310,
author = {Cooper, Brian F. and Garcia-Molina, Hector},
title = {Peer-to-Peer Data Trading to Preserve Information},
year = {2002},
issue_date = {April 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/506309.506310},
doi = {10.1145/506309.506310},
abstract = {Data archiving systems rely on replication to preserve information. This paper discusses how a network of autonomous archiving sites can trade data to achieve the most reliable replication. A series of binary trades among sites produces a peer-to-peer archiving network. Two trading algorithms are examined, one based on trading collections (even if they are different sizes) and another based on trading equal sized blocks of space (which can then store collections). The concept of deeds is introduced; deeds track the blocks of space owned by one site at another. Policies for tuning these algorithms to provide the highest reliability, for example by changing the order in which sites are contacted and offered trades, are discussed. Finally, simulation results are presented that reveal which policies are best. The experiments indicate that a digital archive can achieve the best reliability by trading blocks of space (deeds), and that following certain policies will allow that site to maximize its reliability.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {133–170},
numpages = {38},
keywords = {resource negotiation, digital archiving, fault tolerance, Data replication, digital library}
}

@article{10.1145/503104.503110,
title = {Placing Search in Context: The Concept Revisited},
year = {2002},
issue_date = {January 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/503104.503110},
doi = {10.1145/503104.503110},
abstract = {Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document ("the context"). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {116–131},
numpages = {16},
keywords = {invisible web, Search, context, statistical natural language processing, semantic processing}
}

@article{10.1145/503104.503109,
title = {Efficient Web Browsing on Handheld Devices Using Page and Form Summarization},
year = {2002},
issue_date = {January 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/503104.503109},
doi = {10.1145/503104.503109},
abstract = {We present a design and implementation for displaying and manipulating HTML pages on small handheld devices such as personal digital assistants (PDAs), or cellular phones. We introduce methods for summarizing parts of Web pages and HTML forms. Each Web page is broken into text units that can each be hidden, partially displayed, made fully visible, or summarized. A variety of methods are introduced that summarize the text units. In addition, HTML forms are also summarized by displaying just the text labels that prompt the use for input. We tested the relative performance of the summarization methods by asking human subjects to accomplish single-page information search tasks. We found that the combination of keywords and single-sentence summaries provides significant improvements in access times and number of required pen actions, as compared to other schemes. Our experiments also show that our algorithms can identify the appropriate labels for forms in 95% of the cases, allowing effective form support for small screens.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {82–115},
numpages = {34},
keywords = {handheld computers, WAP, ubiquitous computing, forms, summarization, mobile computing, PDA, wireless computing, Personal digital assistant, WML}
}

@article{10.1145/503104.503108,
title = {Query Clustering Using User Logs},
year = {2002},
issue_date = {January 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/503104.503108},
doi = {10.1145/503104.503108},
abstract = {Query clustering is a process used to discover frequently asked questions or most popular topics on a search engine. This process is crucial for search engines based on question-answering. Because of the short lengths of queries, approaches based on keywords are not suitable for query clustering. This paper describes a new query clustering method that makes use of user logs which allow us to identify the documents the users have selected for a query. The similarity between two queries may be deduced from the common documents the users selected for them. Our experiments show that a combination of both keywords and user logs is better than using either method alone.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {59–81},
numpages = {23},
keywords = {Query clustering, web data mining, search engine, user log}
}

@article{10.1145/503104.503107,
author = {Bharat, Krishna and Mihaila, George A.},
title = {When Experts Agree: Using Non-Affiliated Experts to Rank Popular Topics},
year = {2002},
issue_date = {January 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/503104.503107},
doi = {10.1145/503104.503107},
abstract = {In response to a query, a search engine returns a ranked list of documents. If the query is about a popular topic (i.e., it matches many documents), then the returned list is usually too long to view fully. Studies show that users usually look at only the top 10 to 20 results. However, we can exploit the fact that the best targets for popular topics are usually linked to by enthusiasts in the same domain. In this paper, we propose a novel ranking scheme for popular topics that places the most authoritative pages on the query topic at the top of the ranking. Our algorithm operates on a special index of "expert documents." These are a subset of the pages on the WWW identified as directories of links to non-affiliated sources on specific topics. Results are ranked based on the match between the query and relevant descriptive text for hyperlinks on expert pages pointing to a given result page. We present a prototype search engine that implements our ranking scheme and discuss its performance. With a relatively small (2.5 million page) expert index, our algorithm was able to perform comparably on popular queries with the best of the mainstream search engines.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {47–58},
numpages = {12},
keywords = {WWW search, authorities, topic experts, link analysis, connectivity, ranking, host affiliation}
}

@article{10.1145/503104.503106,
author = {Aridor, Yariv and Carmel, David and Maarek, Yoelle S. and Soffer, Aya and Lempel, Ronny},
title = {Knowledge Encapsulation for Focused Search from Pervasive Devices},
year = {2002},
issue_date = {January 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/503104.503106},
doi = {10.1145/503104.503106},
abstract = {Mobile knowledge seekers often need access to information on the Web during a meeting or on the road, while away from their desktop. A common practice today is to use pervasive devices such as Personal Digital Assistants or mobile phones. However, these devices have inherent constraints (e.g., slow communication, form factor) which often make information discovery tasks impractical.In this paper, we present a new focused-search approach specifically oriented for the mode of work and the constraints dictated by pervasive devices. It combines focused search within specific topics with encapsulation of topic-specific information in a persistent repository. One key characteristic of these persistent repositories is that their footprint is small enough to fit on local devices, and yet they are rich enough to support many information discovery tasks in disconnected mode. More specifically, we suggest a representation for topic-specific information based on "knowledge-agent bases" that comprise all the information necessary to access information about a topic (under the form of key concepts and key Web pages) and assist in the full search process from query formulation assistance to result scanning on the device itself. The key contribution of our work is the coupling of focused search with encapsulated knowledge representation making information discovery from pervasive devices practical as well as efficient. We describe our model in detail and demonstrate its aspects through sample scenarios.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {25–46},
numpages = {22},
keywords = {Focused searches, knowledge agents, pervasive devices, disconnected search}
}

@article{10.1145/503104.503105,
title = {PicASHOW: Pictorial Authority Search by Hyperlinks on the Web},
year = {2002},
issue_date = {January 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/503104.503105},
doi = {10.1145/503104.503105},
abstract = {We describe PicASHOW, a fully automated WWW image retrieval system that is based on several link-structure analyzing algorithms. Our basic premise is that a page p displays (or links to) an image when the author of p considers the image to be of value to the viewers of the page. We thus extend some well known link-based WWW page retrieval schemes to the context of image retrieval.PicASHOW's analysis of the link structure enables it to retrieve relevant images even when those are stored in files with meaningless names. The same analysis also allows it to identify image containers and image hubs. We define these as Web pages that are rich in relevant images, or from which many images are readily accessible.PicASHOW requires no image analysis whatsoever and no creation of taxonomies for preclassification of the Web's images. It can be implemented by standard WWW search engines with reasonable overhead, in terms of both computations and storage, and with no change to user query formats. It can thus be used to easily add image retrieving capabilities to standard search engines.Our results demonstrate that PicASHOW, while relying almost exclusively on link analysis, compares well with dedicated WWW image retrieval systems. We conclude that link analysis, a proven effective technique for Web page search, can improve the performance of Web image retrieval, as well as extend its definition to include the retrieval of image hubs and containers.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–24},
numpages = {24},
keywords = {Image retrieval, image hubs, link structure analysis, hubs and authorities}
}

@article{10.1145/502795.502798,
author = {Yoshioka, Takeshi and Herman, George and Yates, JoAnne and Orlikowski, Wanda},
title = {Genre Taxonomy: A Knowledge Repository of Communicative Actions},
year = {2001},
issue_date = {October 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/502795.502798},
doi = {10.1145/502795.502798},
abstract = {We propose a genre taxonomy as a knowledge repository of communicative structures or "typified actions" enacted by organizational members. The genre taxonomy is intended to help people make sense of diverse types of communicative actions and provide ideas for improving work processes that coordinate the communication of information. It engages several features to achieve this objective. First, the genre taxonomy represents the elements of both genres and genre systems as embedded in a social context reflecting the communicative questions why, what, who, when, where, and how (5W1H). In other words, the genre taxonomy represents the purpose, content, participants, timing, location, and form of communicative action. Second, the genre taxonomy distinguishes between widely recognized genres such as a report and specific genres such as a particular company's technical report, because the difference sheds light on the context of genre use. Third, the genre taxonomy represents use and evolution of a genre over time to help people understand how a genre is used and changed by a community over time. Fourth, the genre taxonomy represents aspects of information coordination via genres, thus providing ideas for improving work processes using genres. We have constructed a prototype of such a genre taxonomy using the Process Handbook, a process knowledge repository developed at MIT. We have included both widely recognized genres such as the memo and specific genres such as those used in the Process Handbook itself. We suggest that this genre taxonomy may be useful in the innovation of new document templates or methods for communication because it helps to clarify different possible uses of similar genres and explicates how genres play a coordination role among people and between people and their tasks.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {431–456},
numpages = {26},
keywords = {taxonomy, information search and retrieval, grammar, genre systems, Genre}
}

@article{10.1145/502795.502797,
author = {Comai, Sara and Damiani, Ernesto and Fraternali, Piero},
title = {Computing Graphical Queries over XML Data},
year = {2001},
issue_date = {October 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/502795.502797},
doi = {10.1145/502795.502797},
abstract = {The rapid evolution of XML from a mere data exchange format to a universal syntax for encoding domain-specific information raises the need for new query languages specifically conceived to address the characteristics of XML. Such languages should be able not only to extract information from XML documents, but also to apply powerful transformation and restructuring operators, based on a well-defined semantics. Moreover, XML queries should be natural to write and understand, as nontechnical persons also are expected to access the large XML information bases supporting their businesses. This article describes XML-GL, a graphical query language for XML data. XML-GL's uniqueness is in the definition of a graph-based syntax to express a wide variety of XML queries, ranging from simple selections to expressive data transformations involving grouping, aggregation, and arithmetic calculations. XML-GL has an operational semantics based on the notion of graph matching, which serves as a guideline both for the implementation of native processors, and for the adoption of XML-GL as a front-end to any of the XML query languages that are presently under discussion as the standard paradigm for querying XML data.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {371–430},
numpages = {60},
keywords = {semantics, graphical query languages, Document restructuring}
}

@article{10.1145/502795.502796,
author = {Wong, Kam-Fai and Song, Dawei and Bruza, Peter and Cheng, Chun-Hung},
title = {Application of Aboutness to Functional Benchmarking in Information Retrieval},
year = {2001},
issue_date = {October 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/502795.502796},
doi = {10.1145/502795.502796},
abstract = {Experimental approaches are widely employed to benchmark the performance of an information retrieval (IR) system. Measurements in terms of recall and precision are computed as performance indicators. Although they are good at assessing the retrieval effectiveness of an IR system, they fail to explore deeper aspects such as its underlying functionality and explain why the system shows such performance. Recently, inductive (i.e., theoretical) evaluation of IR systems has been proposed to circumvent the controversies of the experimental methods. Several studies have adopted the inductive approach, but they mostly focus on theoretical modeling of IR properties by using some metalogic. In this article, we propose to use inductive evaluation for functional benchmarking of IR models as a complement of the traditional experiment-based performance benchmarking. We define a functional benchmark suite in two stages: the evaluation criteria based on the notion of "aboutness," and the formal evaluation methodology using the criteria. The proposed benchmark has been successfully applied to evaluate various well-known classical and logic-based IR models. The functional benchmarking results allow us to compare and analyze the functionality of the different IR models.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {337–370},
numpages = {34},
keywords = {inductive evaluation, logic-based information retrieval, Aboutness, functional benchmarking}
}

@article{10.1145/502115.502120,
author = {Meng, Weiyi and Wu, Zonghuan and Yu, Clement and Li, Zhuogang},
title = {A Highly Scalable and Effective Method for Metasearch},
year = {2001},
issue_date = {July 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/502115.502120},
doi = {10.1145/502115.502120},
abstract = {A metasearch engine is a system that supports unified access to multiple local search engines. Database selection is one of the main challenges in building a large-scale metasearch engine. The problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query. In order to enable accurate selection, metadata that reflect the contents of each search engine need to be collected and used. This article proposes a highly scalable and accurate database selection method. This method has several novel features. First, the metadata for representing the contents of all search engines are organized into a single integrated representative. Such a representative yields both computational efficiency and storage efficiency. Second, the new selection method is based on a theory for ranking search engines optimally. Experimental results indicate that this new method is very effective. An operational prototype system has been built based on the proposed approach.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {310–335},
numpages = {26},
keywords = {metasearch engine, resource discovery, Database selection, distributed text retrieval}
}

@article{10.1145/502115.502119,
author = {Aggarwal, Charu C. and Al-Garawi, Fatima and Yu, Philip S.},
title = {On the Design of a Learning Crawler for Topical Resource Discovery},
year = {2001},
issue_date = {July 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/502115.502119},
doi = {10.1145/502115.502119},
abstract = {In recent years, the World Wide Web has shown enormous growth in size. Vast repositories of information are available on practically every possible topic. In such cases, it is valuable to perform topical resource discovery effectively. Consequently, several new ideas have been proposed in recent years; among them a key technique is focused crawling which is able to crawl particular topical portions of the World Wide Web quickly, without having to explore all web pages. In this paper, we propose the novel concept of intelligent crawling which actually learns characteristics of the linkage structure of the World Wide Web while performing the crawling. Specifically, the intelligent crawler uses the inlinking web page content, candidate URL structure, or other behaviors of the inlinking web pages or siblings in order to estimate the probability that a candidate is useful for a given crawl. This is a much more general framework than the focused crawling technique which is based on a pre-defined understanding of the topical structure of the web. The techniques discussed in this paper are applicable for crawling web pages which satisfy arbitrary user-defined predicates such as topical queries, keyword queries, or any combinations of the above. Unlike focused crawling, it is not necessary to provide representative topical examples, since the crawler can learn its way into the appropriate topic. We refer to this technique as intelligent crawling because of its adaptive nature in adjusting to the web page linkage structure. We discuss how to intelligently select features which are most useful for a given crawl. The learning crawler is capable of reusing the knowledge gained in a given crawl in order to provide more efficient crawling for closely related predicates.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {286–309},
numpages = {24},
keywords = {Crawling, World Wide Web}
}

@article{10.1145/502115.502118,
title = {WebQuilt: A Proxy-Based Approach to Remote Web Usability Testing},
year = {2001},
issue_date = {July 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/502115.502118},
doi = {10.1145/502115.502118},
abstract = {WebQuilt is a web logging and visualization system that helps web design teams run usability tests (both local and remote) and analyze the collected data. Logging is done through a proxy, overcoming many of the problems with server-side and client-side logging. Captured usage traces can be aggregated and visualized in a zooming interface that shows the web pages people viewed. The visualization also shows the most common paths taken through the web site for a given task, as well as the optimal path for that task, as designated by the designer. This paper discusses the architecture of WebQuilt and describes how it can be extended for new kinds of analyses and visualizations.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {263–285},
numpages = {23},
keywords = {web proxy, Usability evalution, log file analysis, WebQuilt, web visualization}
}

@article{10.1145/502115.502117,
author = {Kwok, Cody and Etzioni, Oren and Weld, Daniel S.},
title = {Scaling Question Answering to the Web},
year = {2001},
issue_date = {July 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/502115.502117},
doi = {10.1145/502115.502117},
abstract = {The wealth of information on the web makes it an attractive resource for seeking quick answers to simple, factual questions such as “who was the first American in space?” or “what is the second tallest mountain in the world?” Yet today's most advanced web search services (e.g., Google and AskJeeves) make it surprisingly tedious to locate answers to such questions. In this paper, we extend question-answering techniques, first studied in the information retrieval literature, to the web and experimentally evaluate their performance.First we introduce Mulder, which we believe to be the first general-purpose, fully-automated question-answering system available on the web. Second, we describe Mulder's architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare Mulder's performance to that of Google and AskJeeves on questions drawn from the TREC-8 question answering track. We find that Mulder's recall is more than a factor of three higher than that of AskJeeves. In addition, we find that Google requires 6.6 times as much user effort to achieve the same level of recall as Mulder.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {242–262},
numpages = {21},
keywords = {search engines, answer extraction, query formulation, answer selection, natural language processing}
}

@article{10.1145/502115.502116,
author = {Melink, Sergey and Raghavan, Sriram and Yang, Beverly and Garcia-Molina, Hector},
title = {Building a Distributed Full-Text Index for the Web},
year = {2001},
issue_date = {July 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/502115.502116},
doi = {10.1145/502115.502116},
abstract = {We identify crucial design issues in building a distributed inverted index for a large collection of Web pages. We introduce a novel pipelining technique for structuring the core index-building system that substantially reduces the index construction time. We also propose a storage scheme for creating and managing inverted files using an embedded database system. We suggest and compare different strategies for collecting global statistics from distributed inverted indexes. Finally, we present performance results from experiments on a testbed distributed Web indexing system that we have implemented.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {217–241},
numpages = {25},
keywords = {Embedded databases, Distributed indexing, Text retrieval, Inverted files, Pipelining}
}

@article{10.1145/382979.383042,
author = {Meuss, Holger and Schulz, Klaus U.},
title = {Complete Answer Aggregates for Treelike Databases: A Novel Approach to Combine Querying and Navigation},
year = {2001},
issue_date = {April 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/382979.383042},
doi = {10.1145/382979.383042},
abstract = {The use of markup languages like SGML, HTML or XML for encoding the strucutre of documents or linguistic data has lead to many databases where entries are adequately described as trees. In this context querying formalisms are interesting that offer the possiblity to refer both to textual content and logical structure. We consider models where the strucutre specified in a query is not only used as a filter, but also for selecting and presenting different parts of the data. If answers are formalized as mapping from query nodes to the database, a simple enumeration of all mappings in the answer set will often suffer from the effect that many answers have common subparts. From a theoretical point of view this may lead to an exponential time complexity of the computation and presentation of all answers. Concentration on the language of so called tree queries—a variant and extension of Kilpel\"{a}inen's Tree Matching formalism—we introduce the notion of a “complete answer aggregate” for a given query. This new data strucutre offers a compact view of the set of all answer and supports active exploration of the ansewer space. Since complete answer aggregates use a powerful structure-sharing mechanism their maximal size is of order 𝒪(d•h•q) where d and q respectively denote the size of the database and the query, and h is the maximal depth of a path of the database. An algorithm is given that computes a complete answer aggregate for a given treee query in time 𝒪(d•log(d)•h•). For the sublanguage of so-called rigid tree queries, as well as for so-called “nonrecursive” databases, an improved bound of 𝒪(d•log(d)•q) is obtained. The algorithm is based on a specific index structure that supports practical efficiency.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {161–215},
numpages = {55},
keywords = {information retrieval, query languages, tree matching, structured documents, SGML, logic, answer presentation, tree databases, semistructured data, XML}
}

@article{10.1145/382979.383041,
author = {Lempel, R. and Moran, S.},
title = {SALSA: The Stochastic Approach for Link-Structure Analysis},
year = {2001},
issue_date = {April 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/382979.383041},
doi = {10.1145/382979.383041},
abstract = {Today, when searching for information on the WWW, one usually performs a query through a term-based search engine. These engines return, as the query's result, a list of Web pages whose contents matches the query. For broad-topic queries, such searches often result in a huge set of retrieved documents, many of which are irrelevant to the user. However, much information is contained in the link-structure of the WWW. Information such as which pages are linked to others can be used to augment search algorithms. In this context, Jon Kleinberg introduced the notion of two distinct types of Web pages: hubs and authorities. Kleinberg argued that hubs and authorities exhibit a mutually reinforcing relationship: a good hub will point to many  authorities, and a good authority will be pointed at by many hubs. In light of this, he dervised an algoirthm aimed at finding authoritative pages. We present SALSA, a new stochastic approach for link-structure analysis, which examines random walks on graphs derived from the link-structure. We show that both SALSA and Kleinberg's Mutual Reinforcement approach employ the same metaalgorithm.  We then prove that SALSA is quivalent to a weighted in degree analysis of the link-sturcutre of WWW subgraphs, making it computationally more efficient than the Mutual reinforcement approach. We compare that results of applying SALSA to the results derived through Kleinberg's approach. These comparisions reveal a topological Phenomenon called the TKC effectwhich, in certain cases,  prevents the Mutual reinforcement approach from identifying meaningful authorities.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {131–160},
numpages = {30},
keywords = {hubs and authorities, Link-structure analysis, TKC effect, random walks, SALSA}
}

@article{10.1145/382979.383040,
author = {Callan, Jamie and Connell, Margaret},
title = {Query-Based Sampling of Text Databases},
year = {2001},
issue_date = {April 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/382979.383040},
doi = {10.1145/382979.383040},
abstract = {The proliferation of searchable text databases on corporate networks and the Internet causes a database selection problem for many people. Algorithms such as gGLOSS and CORI can automatically select which text databases to search for a given information need, but only if given a set of resource descriptions that accurately represent the contents of each database. The existing techniques for a acquiring resource descriptions have significant limitations when used in wide-area networks controlled by many parties. This paper presents query-based sampling, a new technicque for acquiring accurate resource descriptions. Query-based sampling does not require the cooperation of resource providers, nor does it require that resource providers use a particular search engine or  representation technique. An extensive set of experimental results demonstrates that accurate resource descriptions are crated, that computation and communication costs are reasonable, and that the resource descriptions do in fact enable accurate automatic dtabase selection.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {97–130},
numpages = {34},
keywords = {server selection, resource selection, query-based sampling, distributed information retrieval, resource ranking}
}

@article{10.1145/366836.366874,
author = {Papadias, Dimitris and Mamoulis, Nikos and Delis, Vasilis},
title = {Approximate Spatio-Temporal Retrieval},
year = {2001},
issue_date = {Jan. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/366836.366874},
doi = {10.1145/366836.366874},
abstract = {This paper proposes a framework for the handling of spatio-temporal queries with inexact matches, using the concept of relation similarity. We initially describe a binary string encoding for 1D relations that permits the automatic derivation of similarity measures. We then extend this model to various granularity levels and many dimensions, and show that reasoning on spatio-temporal structure is significantly facilitated in the new framework. Finally, we provide algorithms and optimization methods for four types of queries: (i) object retrieval based on some spatio-temporal relations with respect to a reference object, (ii) spatial joins, i.e., retrieval of object pairs that satisfy some input relation, (iii) structural queries, which retrieve configurations matching a particular spatio-temporal structure, and (iv) special cases of motion queries. Considering the current large availability of multidimensional data and the increasing need for flexible query-answering mechanisms, our techniques can be used as the core of spatio-temporal query processors.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {53–96},
numpages = {44}
}

@article{10.1145/366836.366869,
author = {de Oliveira, Maria Cristina Ferreira and Turine, Marcelo Augusto Santos and Masiero, Paulo Cesar},
title = {A Statechart-Based Model for Hypermedia Applications},
year = {2001},
issue_date = {Jan. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/366836.366869},
doi = {10.1145/366836.366869},
abstract = {This paper presents a formal definition for HMBS (Hypermedia Model Based on Statecharts). HMBS uses the structure and execution semantics of statecharts to specify both the structural organization and the browsing semantics of hypermedia applications. Statecharts are an extension of finite-state machines and the model is thus a generalization of hypergraph-based hypertext models. Some of the most important features of HMBS are its ability to model hierarchy and synchronization of information; provision of mechanisms for specifying access structures, navigational contexts, access control, multiple tailored versions,and hierarchical views. Analysis of the underlying statechart machine allows verification of page reachability, valid paths, and other properties, thus providing mechanisms to support authors in the development of structured applications.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {28–52},
numpages = {25},
keywords = {statecharts, navigational model, hypermedia specification, browsing semantics, HMBS}
}

@article{10.1145/366836.366860,
author = {Carpineto, Claudio and de Mori, Renato and Romano, Giovanni and Bigi, Brigitte},
title = {An Information-Theoretic Approach to Automatic Query Expansion},
year = {2001},
issue_date = {Jan. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/366836.366860},
doi = {10.1145/366836.366860},
abstract = {Techniques for automatic query expansion from top retrieved documents have shown promise for improving retrieval effectiveness on large collections; however, they often rely on an empirical ground, and there is a shortage of cross-system comparisons. Using ideas from Information Theory, we present a computationally simple and theoretically justified method for assigning scores to candidate expansion terms. Such scores are used to select and weight expansion terms within Rocchio's framework for query reweigthing. We compare ranking with information-theoretic query expansion versus ranking with other query expansion techniques, showing that the former achieves better retrieval effectiveness on several performance measures. We also discuss the effect on retrieval effectiveness of the main parameters involved in automatic query expansion, such as data sparseness, query difficulty, number of selected documents, and number of selected terms, pointing out interesting relationships.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–27},
numpages = {27},
keywords = {pseudorelevance feedback, automatic query expansion, information retrieval, information theory}
}

@article{10.1145/358108.358114,
author = {Lu, Hongjun and Feng, Ling and Han, Jiawei},
title = {Beyond Intratransaction Association Analysis: Mining Multidimensional Intertransaction Association Rules},
year = {2000},
issue_date = {Oct. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/358108.358114},
doi = {10.1145/358108.358114},
abstract = {In this paper, we extend the scope of mining association rules from traditional single-dimensional intratransaction associations, to multidimensional intertransaction associations. Intratransaction associations are the associations among items with the same transaction, where the notion of the transaction could be the items bought by the same customer, the events happened on the same day, and so on. However, an intertransaction association describes the association relationships among different transactions, such as “if(company) A's stock goes up on day 1, B's stock will go down on day 2, but go up on day 4.” In this case, whether we treat company or day as the unit of transaction, the associated items belong to different transactions. Moreover, such an intertransaction association can be extended to associate multiple contextual properties in the same rule, so that multidimensional intertransaction associations can be defined and discovered. A two-dimensional intertransaction association rule example is “After McDonald and Burger King open branches, KFC will open a branch two months later and one mile away,” which involves two dimensions: time and space. Mining intertransaction associations poses more challenges on efficient processing than mining intratransaction associations. Interestingly, intratransaction association can be treated as a special case of intertransaction association from both a conceptual and algorithmic point of view. In this study, we introduce the notion of multidimensional intertransaction association rules, study their measurements—support and confidence—and develop algorithms for mining intertransaction associations by extension of Apriori. We overview our experience using the algorithms on both real-life and synthetic data sets. Further extensions of multidimensional intertransaction association rules and potential applications are also discussed.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {423–454},
numpages = {32},
keywords = {multidimensional context, data mining, association rules, intra/intertransaction}
}

@article{10.1145/358108.358111,
author = {Katzenstein, Gary and Lerch, F. Javier},
title = {Beneath the Surface of Organizational Processes: A Social Representation Framework for Business Process Redesign},
year = {2000},
issue_date = {Oct. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/358108.358111},
doi = {10.1145/358108.358111},
abstract = {This paper raises the question, “What is an effective representation framework for organizational process design?” By combining our knowledge of existing process models with data from a field study, the paper develops criteria for an effective process representation. Using these criteria and the case study, the paper integrates the process redesign and information system literatures to develop a representation framework that captures a process' social context. The paper argues that this social context framework, which represents people's motivations, social relationships, and social constraints, gives redesigners a richer sense of the process and allows process redesigners to simultaneously change social and logistic systems. The paper demonstrates the framework and some of its benefits and limitations.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {383–422},
numpages = {40},
keywords = {process representation, organizational change, business process redesign}
}

@article{10.1145/358108.358110,
author = {Fraternali, Piero and Paolini, Paolo},
title = {Model-Driven Development of Web Applications: The AutoWeb System},
year = {2000},
issue_date = {Oct. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/358108.358110},
doi = {10.1145/358108.358110},
abstract = {This paper describes a methodology for the development of WWW applications and a tool environment specifically tailored for the methodology. The methodology and the development environment are based upon models and techniques already used in the hypermedia, information systems, and software engineering fields, adapted and blended in an original mix. The foundation of the proposal is the conceptual design of WWW applications, using HDM-lite, a notation for the specification of structure, navigation, and presentation semantics. The conceptual schema is then translated into a “traditional” database schema, which describes both the organization of the content and the desired navigation and presentation features. The WWW pages can therefore be dynamically generated from the  database content, following the navigation requests of the user. A CASE environment, called Autoweb System, offers a set of software tools, which assist the design and the execution of a WWW application, in all its different aspects, Real-life experiences of the use of the methodology and of the AutoWeb System in both the industrial and academic context are reported.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {323–382},
numpages = {60},
keywords = {intranet, HTML, application, modeling, WWW, development}
}

@article{10.1145/352595.352598,
author = {Cohen, William W.},
title = {Data Integration Using Similarity Joins and a Word-Based Information Representation Language},
year = {2000},
issue_date = {July 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/352595.352598},
doi = {10.1145/352595.352598},
abstract = {The integration of distributed, heterogeneous databases, such as those available on the World Wide Web, poses many problems. Herer we consider the problem of integrating data from sources that lack common object identifiers. A solution to this problem is proposed for databases that contain informal, natural-language “names” for objects; most Web-based databases satisfy this requirement, since they usually present their information to the end-user through a veneer of text. We describe WHIRL, a “soft” database management system which supports “similarity joins,” based on certain robust, general-purpose similarity metrics for text. This enables fragments of text (e.g., informal names of objects) to be used as keys. WHIRL includes   textual objects as a built-in type, similarity reasoning as a built-in predicate, and answers every query with a list of answer substitutions that are ranked according to an overall score. Experiments show that WHIRL is much faster than naive inference methods, even for short queries, and efficient on typical queries to real-world databases with tens of thousands of tuples. Inferences made by WHIRL are also surprisingly accurate, equaling the accuracy of hand-coded normalization routines on one benchmark problem, and outerperforming exact matching with a plausible global domain on a second.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {288–321},
numpages = {34}
}

@article{10.1145/352595.352597,
author = {Greiff, Warren R. and Ponte, Jay M.},
title = {The Maximum Entropy Approach and Probabilistic IR Models},
year = {2000},
issue_date = {July 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/352595.352597},
doi = {10.1145/352595.352597},
abstract = {This paper takes a fresh look at modeling approaches to information retrieval that have been the basis of much of the probabilistically motivated IR research over the last 20 years. We shall adopt a subjectivist Bayesian view of probabilities and argue that classical work on probabilistic retrieval is best understood from this perspective. The main focus of the paper will be the ranking formulas corresponding to the Binary Independence Model (BIM), presented originally by Roberston and Sparck Jones [1977] and the Combination Match Model (CMM), developed shortly thereafter by Croft and Harper [1979]. We will show how these same ranking formulas can result from a probabilistic methodology commonly known as Maximum Entropy (MAXENT).},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {246–287},
numpages = {42},
keywords = {idf weighting, linked dependence, binary independence model, probability ranking principle, combination match}
}

@article{10.1145/352595.352596,
author = {Anderson, Kenneth M. and Taylor, Richard N. and Whitehead, E. James},
title = {Chimera: Hypermedia for Heterogeneous Software Development Enviroments},
year = {2000},
issue_date = {July 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/352595.352596},
doi = {10.1145/352595.352596},
abstract = {Emerging software development environments are characterized by heterogeneity: they are composed of diverse object stores, user interfaces, and tools. This paper presents an approach for providing hypermedia services in this heterogeneous setting. Central notions of the approach include the following: anchors are established with respect to interactive views of objects, rather than the objects themselves; composable, n-ary links can be established between anchors on different views of objects which may be stored in distinct object bases; viewers may be implemented in different programming languages; and, hypermedia services are provided to multiple, concurrently active, viewers. The paper describes the approach, supporting architecture, and lessons  learned. Related work in the areas of supporing heterogeneity and hypermedia data modeling is discussed. The system has been employed in a variety of contexts including research, development, and education.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {211–245},
numpages = {35},
keywords = {link servers, software development environments, hypermedia system architectures, open hypermedia systems, heterogeneous hypermedia}
}

@article{10.1145/348751.348762,
author = {El-Kwae, Essam A. and Kabuka, Mansur R.},
title = {Efficient Content-Based Indexing of Large Image Databases},
year = {2000},
issue_date = {April 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/348751.348762},
doi = {10.1145/348751.348762},
abstract = {Large image databases have emerged in various applications in recent years. A prime requisite of these databases is the means by which their contents can be indexed and retrieved. A multilevel signature file called the Two Signature Multi-level Signature File (2SMLSF) is introduced as an efficient access structure for large image databases. The 2SMLSF encodes image information into binary signatures and creates a tree structures can be efficiently searched to satisfy a user's query. Two types of signatures are generated. Type I signatures are used at all tree levels except the leaf level and are based only on the domain objects included in the image. Type II signatures, on the other hand, are stored at the leaf level and are based on the included domain objects and their spatial relationships. The   2SMLSF was compared analytically to existing signature file techniques. The 2SMLSF significantly reduces the storage requirements; the index structure can answer more queries; and the 2SMLSF performance significantly improves over current techniques. Both storage reduction and performance improvement increase with the number of objects per image and the number of images in the database. For an example large image database, a storage reduction of 78% may be archieved  while the performance improvement may reach 98%.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {171–210},
numpages = {40},
keywords = {multimedia databases, content analysis and indexing, document managing, image databases, index generation}
}

@article{10.1145/348751.348758,
author = {Dourish, Paul and Edwards, W. Keith and LaMarca, Anthony and Lamping, John and Petersen, Karin and Salisbury, Michael and Terry, Douglas B. and Thornton, James},
title = {Extending Document Management Systems with User-Specific Active Properties},
year = {2000},
issue_date = {April 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/348751.348758},
doi = {10.1145/348751.348758},
abstract = {Document properties are a compelling infrastructure on which to develop document management applications. A property-based approach avoids many of the problems of traditional heierarchical storage mechanisms, reflects document organizations meaningful to user tasks, provides a means to integrate the perspectives of multiple individuals and groups, and does this all within a uniform interaction framework. Document properties can reflect not only categorizations of documents and document use, but also expressions of desired system activity, such as sharing criteria, replication management, and versioning. Augmenting property-based document management systems with active properties that carry executable code enables the provision of document-based services on a property infrastructure.   The combination of document properties as a uniform mechanism for document management, and active properties as a way of delivering document services, represents a new paradigm for document management infrastructures. The Placeless Documents system is an experimental prototype developed to explore this new paradigm. It is based on the seamless integration of user-specific, active properties. We present the fundamental design approach, explore the challenges and opportunities it presents, and show our architectures deals with them.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {140–170},
numpages = {31},
keywords = {document services, document management systems, component software, active properties, user experience}
}

@article{10.1145/348751.348754,
author = {Silva de Moura, Edleno and Navarro, Gonzalo and Ziviani, Nivio and Baeza-Yates, Ricardo},
title = {Fast and Flexible Word Searching on Compressed Text},
year = {2000},
issue_date = {April 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/348751.348754},
doi = {10.1145/348751.348754},
abstract = {We present a fast compression technique for natural language texts. The novelties are that (1) decompression of arbitrary portions of the text can be done very efficiently, (2) exact search for words and phrases can be done on the compressed text directly, using any known sequential pattern-matching algorithm, and (3) word-based approximate and extended search can also be done efficiently without any decoding. The compression scheme uses a semistatic word-based model and a Huffman code where the coding alphabet is byte-oriented rather than bit-oriented. We compress typical English texts to about 30% of their original size, against 40% and 35% for Compress and Gzip, respectively. Compression time is close to that of Compress and   approximately half of the time of Gzip, and decompression time is lower than that of Gzip and one third of that of Compress. We present three algorithms to search the compressed text. They allow a large number of variations over the basic word and phrase search capability, such as sets of characters, arbitrary regular expressions, and approximate matching. Separators and stopwords can be discarded at search time without significantly increasing the cost. When searching for simple words, the experiments show that running our algorithms on a compressed text is twice as fast as running the best existing software on the uncompressed version of the same text. When searching complex or approximate patterns, our algorithms are up to 8 times   faster than the search on uncompressed text. We also discuss the impact of our technique in inverted files pointing to logical blocks and argue for the possibility of keeping the text compressed all the time, decompressing only for displaying purposes.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {113–139},
numpages = {27},
keywords = {natural language text compression, compressed pattern matching, word-based Huffman coding, word searching}
}

@article{10.1145/333135.333138,
author = {Xu, Jinxi and Croft, W. Bruce},
title = {Improving the Effectiveness of Information Retrieval with Local Context Analysis},
year = {2000},
issue_date = {Jan. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/333135.333138},
doi = {10.1145/333135.333138},
abstract = {Techniques for automatic query expansion have been extensively studied in information research as a means of addressing the word mismatch between queries and documents. These techniques can be categorized as either global or local. While global techniques rely on analysis of a whole collection to discover word relationships, local techniques emphasize analysis of the top-ranked documents retrieved for a query. While local techniques have shown to be more effective that global techniques in general, existing local techniques are not robust and can seriously hurt retrieved when few of the retrieval documents are relevant. We propose a new technique, called local context analysis, which selects expansion terms based on cooccurrence with the query terms within the  top-ranked documents. Experiments on a number of collections, both English and non-English, show that local context analysis offers more effective and consistent retrieval results. },
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {79–112},
numpages = {34},
keywords = {local context analysis, feedback, cooccurrence, global techniques, information retrieval, document analysis, local techniques}
}

@article{10.1145/333135.333137,
author = {Clarke, Charles L. A. and Cormack, Gordon V.},
title = {Shortest-Substring Retrieval and Ranking},
year = {2000},
issue_date = {Jan. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/333135.333137},
doi = {10.1145/333135.333137},
abstract = {We present a model for arbitrary passage retrieval using Boolean queries. The model is applied to the task of ranking documents, or other structural elements, in the order of their expected relevance. Features such as phrase matching, truncation, and stemming integrate naturally into the model. Properties of Boolean algebra are obeyed, and the exact-match semantics of Boolean retrieval are preserved. Simple inverted-list file structures provide an efficient implementation. Retrieval effectiveness is comparable to that of standard ranking techniques. Since global statistics are not used, the method is of particular value in distributed environments. Since ranking is based on arbitrary passages, the structural elements to be ranked may be specified at query time and do not need to be  restricted to predefined elements.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {44–78},
numpages = {35},
keywords = {Boolean retrieval model, relevance ranking, passage retrieval}
}

@article{10.1145/333135.333136,
author = {Cahoon, Brendon and McKinley, Kathryn S. and Lu, Zhihong},
title = {Evaluating the Performance of Distributed Architectures for Information Retrieval Using a Variety of Workloads},
year = {2000},
issue_date = {Jan. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/333135.333136},
doi = {10.1145/333135.333136},
abstract = {The information explosion across the Internet and elswhere offers access to an increasing number of document collections. In order for users to effectively access these collections, information retrieval (IR) systems must provide coordinated, concurrent, and distributed access. In this article, we explore how to achieve scalable performance in a distributed system for collection sizes ranging from 1GB to 128GB. We implement a fully functional distributed IR system based on a multithreaded version of the Inquery simulation model. We measure performance as a function of system parameters such as client command rate, number of document collections, ter ms per query, query term frequency, number of answers returned, and command mixture. Our results show that it is important to model both  query and document commands because the heterogeneity of commands significantly impacts performance. Based on our results, we recommend simple changes to the prototype and  evaluate the changes using the simulator. Because of the significant resource demands of information retrieval, it is not difficult to generate workloads that overwhelm system resources regardless of the architecture. However under some realistic workloads, we demonstrate system organizations for which response time gracefully degrades as the workload increases and performance scales with the number of processors. This scalable architecture includes a surprisingly small number of brokers through which a large number of clients and servers communicate.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–43},
numpages = {43},
keywords = {distributed information retrieval architectures}
}

@article{10.1145/326440.326447,
author = {Sanderson, Mark and Van Rijsbergen, C. J.},
title = {The Impact on Retrieval Effectiveness of Skewed Frequency Distributions},
year = {1999},
issue_date = {Oct. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/326440.326447},
doi = {10.1145/326440.326447},
abstract = {We present an analysis of word senses that provides a fresh insight 
into the impact of word ambiguity on retrieval effectiveness with potential broader implications for other processes of information retrieval. Using a methodology of forming artifically ambiguous words, known as pseudowords, and through reference to other researchers' work, the analysis illustrates that the distribution of the frequency of occurrance of the senses of a word plays a strong role in ambiguity's impact of effectiveness. Further investigation shows that this analysis may also be applicable to other processes of retrieval, such as Cross Language Information Retrieval, query expansion, retrieval of OCR'ed texts, and stemming. The analysis appears to provide a means of explaining, at least in part, reasons  for the processes' impact (or lack of it) on effectiveness.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {440–465},
numpages = {26},
keywords = {pseudowords, word sense ambiguity, word sense disambiguation}
}

@article{10.1145/326440.326445,
author = {Kaszkiel, Marcin and Zobel, Justin and Sacks-Davis, Ron},
title = {Efficient Passage Ranking for Document Databases},
year = {1999},
issue_date = {Oct. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/326440.326445},
doi = {10.1145/326440.326445},
abstract = {Queries to text collections are resolved by ranking the documents in the collection and returning the highest-scoring documents to the user. An alternative retrieval method is to rank passages, that is, short fragments of documents, a strategy that can improve effectiveness and identify relevant material in documents that are too large for users to consider as a whole. However, ranking of passages can considerably increase retrieval costs. In this article we explore alternative query evaluation techniques, and develop new tecnhiques for evaluating queries on passages. We show experimentally that, appropriately implemented, effective passage retrieval is practical in limited memory on a desktop machine. Compared to passage ranking with adaptations of current document ranking algorithms,  our new “DO-TOS” passage-ranking algorithm requires only a fraction of the resources, at the cost of a small loss of effectiveness.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {406–439},
numpages = {34},
keywords = {inverted files, text databases, passage retrieval, query evaluation, text retrieval}
}

@article{10.1145/326440.326444,
author = {Greiff, Warren R. and Croft, W. Bruce and Turtle, Howard},
title = {PIC Matrices: A Computationally Tractable Class of Probabilistic Query Operators},
year = {1999},
issue_date = {Oct. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/326440.326444},
doi = {10.1145/326440.326444},
abstract = {The inference network model of information retrieval allows a 
probabilistic interpretation of query operators. In particular, Boolean query operators are conveniently modeled as link matrices of the Bayesian Network. Prior work has shown, however, that these operators do not perform as well as the pnorm operators used for modeling query operators in the context of the vector space model. This motivates the search for alternative probabilistic formulations for these operators. The design of such alternatives must contend with the issue of computational tractability, since the evaluation of an arbitrary operator requires exponential time. We define a flexible class of link matrices that are natural candidates for the implementation of query operators and an   O(n2) algorithm (n = the number of parent nodes) for the computation of probabilities involving link matrices of this class. We present experimental results indicating that Boolean operators implemented in terms of link matrices from this class perform as well as pnorm operators in the context of the INQUERY inference network.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {367–405},
numpages = {39},
keywords = {inference networks, query operators, probabilistic information retrieval, computational complexity, pnorm, Boolean queries, Bayesian networks, piecewise linear functions, link matrices}
}

@article{10.1145/326440.326442,
author = {Chen, Hao and Hu, Jianying and Sproat, Richard W.},
title = {Integrating Geometrical and Linguistic Analysis for Email Signature Block Parsing},
year = {1999},
issue_date = {Oct. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/326440.326442},
doi = {10.1145/326440.326442},
abstract = {The signature block is a common structured component found in email messages. Accurate identification and analysis of signature blocks is important in many multimedia messaging and information retrieval applications such as email text-to-speech rendering, automatic construction of personal address databases, and interactive message retrieval. It is also a very challenging task, because signature blocks often appear in complex two-dimensional layouts which are guided only by loose conventions. Traditional text analysis methods designed to deal with sequential text cannot handle two-dimensional structures, while the highly unconstrained nature of signature blocks makes the application of two-dimensional grammars very difficult. In this article, we describe an algorithm for signature  block analysis which combines two-dimensional structural segmentation with one-dimensional grammatical constraints. The information obtained from both layout and linguistic analysis is integrated in the form of weighted finite-state transducers. The algorithm is currently implemented as a component in a preprocessing system for email text-to-speech rendering.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {343–366},
numpages = {24},
keywords = {finite-state transducer, text-to-speech rendering, linguistic analysis, geometrical analysis, email signature block}
}

@article{10.1145/314516.314522,
author = {Plaisant, Catherine and Shneiderman, Ben and Doan, Khoa and Bruns, Tom},
title = {Interface and Data Architecture for Query Preview in Networked Information Systems},
year = {1999},
issue_date = {July 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/314516.314522},
doi = {10.1145/314516.314522},
abstract = {There are numerous problems associated with formulating queries on 
networked information systems. These include increased data volume and complexity, accompanied by slow network access. This article proposes a new approach to a network query user interfaces that consists of two phases: query preview and query refinement. This new approach is based on the concepts of dynamic queries and query previews, which guides users in rapidly and dynamically eliminating undesired records, reducing the data volume to a manageable size, and refining queries locally before submission over a network. Examples of two applications are given: a Restaurant Finder and a prototype for NASA's Earth Observing Systems Data Information Systems (EOSDIS). Data architecture is discussed, and user feedback is  presented.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {320–341},
numpages = {22},
keywords = {graphical user interface, science data, EOSDIS, dynamic query, query refinement, query preview, direct manipulation}
}

@article{10.1145/314516.314521,
author = {Lim, Ee-Peng and Lu, Ying},
title = {Harp: A Distributed Query System for Legacy Public Libraries and Structured Databases},
year = {1999},
issue_date = {July 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/314516.314521},
doi = {10.1145/314516.314521},
abstract = {The main purpose of a digital library is to facilitate users easy 
access to enormous amount of globally networked information. Typically, this information includes preexisting public library catalog data, digitized document collections, and other databases. In this article, we describe the distributed query system of a digital library prototype system known as HARP. In the HARP project, we have designed and implemented a distributed query processor and its query front-end to support integrated queries to preexisting public library catalogs and structured databases. This article describes our experiences in the design of an extended Sequel (SQL) query language known as HarpSQL. It also presents the design and implementation of the distributed query system. Our experience in  distributed query processor and user interface design and development will be highlighted. We believe that our prototyping effort will provide useful lessons to the development of a complete digital library infrastructure.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {294–319},
numpages = {26},
keywords = {digital libraries, Internet databases, interoperable databases}
}

@article{10.1145/314516.314520,
author = {Goh, Cheng Hian and Bressan, St\'{e}phane and Madnick, Stuart and Siegel, Michael},
title = {Context Interchange: New Features and Formalisms for the Intelligent Integration of Information},
year = {1999},
issue_date = {July 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/314516.314520},
doi = {10.1145/314516.314520},
abstract = {The Context Interchange strategy presents a novel 
perspective for mediated data access in which semantic conflicts among heterogeneous systems are not identified a priori, but are detected and reconciled by a context mediator through comparison of contexts axioms corresponding to the systems engaged in data exchange. In this article, we show that queries formulated on shared views, export schema, and shared “ontologies” can be mediated in the same way using the Context Interchange framework. The proposed framework provides a logic-based object-oriented formalsim for representing and reasoning about data semantics in disparate systems, and has been validated in a prototype implementation providing  mediated data access to both traditional and web-based information sources.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {270–293},
numpages = {24},
keywords = {abductive reasoning, semantic heterogeneity, information integration, semantic interoperability, mediators}
}

@article{10.1145/314516.314519,
author = {Gauch, Susan and Wang, Jianying and Rachakonda, Satya Mahesh},
title = {A Corpus Analysis Approach for Automatic Query Expansion and Its Extension to Multiple Databases},
year = {1999},
issue_date = {July 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/314516.314519},
doi = {10.1145/314516.314519},
abstract = {Searching online text collections can be both rewarding and 
frustrating. While valuable information can be found, typically many irrelevant documents are also retrieved, while many relevant ones are missed. Terminology mismatches between the user's query and document contents are a main cause of retrieval failures. Expanding a user's query with related words can improve search performances, but finding and using related words is an open problem. This research uses corpus analysis techniques to automatically discover similar words directly from the contents of the databases which are not tagged with part-of-speech labels. Using these similarities, user queries are automatically expanded, resulting in conceptual retrieval rather than requiring exact word matches between queries and  documents. We are able to achieve a 7.6% improvement for TREC 5 queries and up to a 28.5% improvement on the narrow-domain Cystic Fibrosis collection. This work has been extended to multidatabase collections where each subdatabase has a collection-specific similarity matrix associated with it. If the best matrix is selected, substantial search improvements are possible. Various techniques to select the appropriate matrix for a particular query are analyzed, and a 4.8% improvement in the results is validated.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {250–269},
numpages = {20},
keywords = {query expansion}
}

@article{10.1145/314516.314517,
author = {Fuhr, Norbert},
title = {A Decision-Theoretic Approach to Database Selection in Networked IR},
year = {1999},
issue_date = {July 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/314516.314517},
doi = {10.1145/314516.314517},
abstract = {In networked IR, a client submits a query to a broker, which is in 
contact with a large number of databases. In order to yield a maximum number of documents at minimum cost, the broker has to make estimates about the retrieval cost of each database, and then decide for each database whether or not to use it for the current query, and if, how many documents to retrieve from it. For this purpose, we develop a general decision-theoretic model and discuss different cost structures. Besides cost for retrieving relevant versus nonrelevant documents, we consider the following parameters for each database: expected retrieval quality, expected number of relevant documents in the database and cost factors for query processing and document delivery. For computing the overall optimum, a  divide-and-conquer algorithm is given. If there are several brokers knowing different databases, a preselection of brokers can only be performed heuristically, but the computation of the optimum can be done similarily to the single-broker case. In addition, we derive a formula which estimates the number of relevant documents in a database based on dictionary information.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {229–249},
numpages = {21},
keywords = {probability ranking principle, probabilistic retrieval, networked retrieval, resource discovery}
}

@article{10.1145/306686.306690,
author = {Shipman, Frank M. and McCall, Raymond J.},
title = {Incremental Formalization with the Hyper-Object Substrate},
year = {1999},
issue_date = {April 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/306686.306690},
doi = {10.1145/306686.306690},
abstract = {Computers require formally represented information to perform computations that support users; yet users who have needed such support have often proved to be unable or unwilling to formalize it. To address this problem, this article introduces an approach called incremental formalization, in which, first, users express information informally and then the system aids them in formalizing it. Incremental formalization requires a system architecture the (1) integrates formal and informal representations and (2) supports progressive formalization of information. The system should have both tools to capture naturally available informal information and techniques to suggest possible formalizations of this information. The hyper-object substrate (HOS) was developed to satisfy these  requirements. HOS has been applied to a number of problem domains, including network design, archeological site analysis, and neuroscience education. Users have been successful in adding informal information and then later formalizing it incrementally with the aid of the system. Our experience with HOS has reaffirmed the need for information spaces to evolve during use and has identified additional considerations in the design and instantiation of systems enabling and supporting incremental formalization},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {199–227},
numpages = {29}
}

@article{10.1145/306686.306689,
author = {El-Kwae, Essam A. and Kabuka, Mansur R.},
title = {A Robust Framework for Content-Based Retrieval by Spatial Similarity in Image Databases},
year = {1999},
issue_date = {April 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/306686.306689},
doi = {10.1145/306686.306689},
abstract = {A framework for retrieving images by spatial similarity (FRISS) in ima ge databases is presented. In this framework, a robust retrieval by spatial similarity (RSS) algorithm is defined as one that incorporates both directional and topological spatial constraints, retrieves similar images, and recognized images even after they undergo translation, scaling, rotation (both perfect and multiple), or any arbitrary combination of transformatioins. The FRISS framework is discussed and used as a base for comparing various existing RSS algorithms. Analysis shows that none of them satisfies all the FRISS specifications. An algorithm, SIMdtc, is then presented. SIMdtc introduces the concept of a rotation correction angle(RCA) to align objects in one image spatially closer to  matching objects in another image for more accurate similarity assessment. Similarity between two images is a function of the number of common objects between them and the closeness of directional and topological spatial relationships between object pairs in both images. The SIMdtc retrieval is invariant under translation, scaling, and perfect rotation, and the algorithm is able to rank multiple rotation variants. The algorithm was tested using synthetic images and the TESSA image database. Analysis shows the robustness of the SIMdtc algorithm over current algorithms.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {174–198},
numpages = {25},
keywords = {spatial similarity, image databases, content-based retrieval, similarity retrieval, query formulation, multimedia databases, retrieval models}
}

@article{10.1145/306686.306688,
author = {Cohen, William W. and Singer, Yoram},
title = {Context-Sensitive Learning Methods for Text Categorization},
year = {1999},
issue_date = {April 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/306686.306688},
doi = {10.1145/306686.306688},
abstract = {Two recently implemented machine-learning algorithms, RIPPERand sleeping-experts for phrases, are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the “context” of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination. In spite of these differences,  both RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods. We view this result as a confirmation of the usefulness of classifiers that represent contextual information.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {141–173},
numpages = {33},
keywords = {context-sensitive models, rule learning, on-line learning, text categorization, mistake-driven algorithms}
}

@article{10.1145/306686.306687,
author = {Bertino, Elisa and Jajodia, Sushil and Samarati, Pierangela},
title = {A Flexible Authorization Mechanism for Relational Data Management Systems},
year = {1999},
issue_date = {April 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/306686.306687},
doi = {10.1145/306686.306687},
abstract = {In this article, we present an authorization model that can be used to express a number of discretionary access control policies for relational data management systems. The model permits both positive and negative authorizations and supports exceptions at the same time. The model is flexible in that the users can specify, for each authorization they grant, whether the authorization can allow for exceptions or whether it must be strongly obeyed. It provides authorization management for groups with exceptions at any level of the group hierarchy, and temporary suspension of authorizations. The model supports ownership together with decentralized administration of authorizations. Administrative privileges can also be restricted so that owners retain control over their tables.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {101–140},
numpages = {40},
keywords = {data management system, access control policy, access control mechanism, group management support, authorization, relational database}
}

@article{10.1145/297117.297126,
author = {Tan, Bernard C. Y. and Wei, Kwok-kee and Watson, Richard T.},
title = {The Equalizing Impact of a Group Support System on Status Differentials},
year = {1999},
issue_date = {Jan. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/297117.297126},
doi = {10.1145/297117.297126},
abstract = {This study investigates the impact of the electronic communication capability of a group support system (GSS) on status differentials in small groups. A laboratory experiment was used to answer the research questions. Three support levels were studied: manual, face-to-face GSS, and dispersed GSS. Two task types were examined: intellective and preference. Five dependent variables reflecting different aspects of status differentials were measured: status influence, sustained influence, residual disagreement, perceived influence, and decision confidence. The results show that manual groups had higher status influence, sustained influence, and decision confidence, but lower residual disagreement than face-to-face GSS and dispersed GSS groups. Preference task groups also produced higher  status influence and sustained influence, but lower residual disagreement compared to intellective task groups. In addition, manual groups working on the preference task reported higher perceived influence than face-to-face GSS and dispersed GSS groups working on the same task. These findings suggest that when groups are engaged in activities for which status differentials are undesirable, a GSS can be used in both face-to-face and dispersed settings to dampen status differentials. Moreover, when a task amplifies status differentials, the use of a GSS tends to produce corresponding stronger dampening effects.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {77–100},
numpages = {24},
keywords = {status differentials, group support systems, electronic communication, task type}
}

@article{10.1145/297117.297123,
author = {Hawking, David and Thistlewaite, Paul},
title = {Methods for Information Server Selection},
year = {1999},
issue_date = {Jan. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/297117.297123},
doi = {10.1145/297117.297123},
abstract = {The problem of using a broker to select a subset of available information servers in order to achieve a good trade-off between document retrieval effectiveness and cost is addressed. Server selection methods which are capable of operating in the absence of global information, and where servers have no knowledge of brokers, are investigated. A novel method using Lightweight Probe queries (LWP method) is compared with several methods based on data from past query processing, while Random and Optimal server rankings serve as controls. Methods are evaluated, using TREC data and relevance judgments, by computing ratios, both empirical and ideal, of recall and early precision for the subset versus the complete set of available servers. Estimates are also made of the best-possible  performance of each of the methods. LWP and Topic Similarity methods achieved best results, each being capable of retrieving about 60% of the relevant documents for only one-third of the cost of querying all servers. Subject to the applicable cost model, the LWP method is likely to be preferred because it is suited to dynamic environments. The good results obtained with a simple automatic LWP implementation were replicated using different data and a larger set of query topics.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {40–76},
numpages = {37},
keywords = {network servers, text retrieval, server selection, information servers, Lightweight Probe queries, server ranking}
}

@article{10.1145/297117.297120,
author = {Chang, Chen-Chuan K. and Garcia-Molina, H\'{e}ctor and Paepcke, Andreas},
title = {Predicate Rewriting for Translating Boolean Queries in a Heterogeneous Information System},
year = {1999},
issue_date = {Jan. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/297117.297120},
doi = {10.1145/297117.297120},
abstract = {Searching over heterogeneous information sources is difficult in part because of the nonuniform query languages. Our approach is to allow users to compose Boolean queries in one rich front-end language. For each user query and target source, we transform the user query into a subsuming query that can be supported by the source but that may return extra documents. The results are then processed by a filter query to yield the correct final results. In this article we introduce the architecture and associated mechanism for query translation. In particular, we discuss techniques for rewriting predicates in Boolean queries into native subsuming forms, which is a basis of translating complex queries. In addition, we present experimental results for evaluating the cost of postfiltering. We  also discuss the drawbacks of this approach and cases when it may not be effective. We have implemented prototype versions of these mechanisms and demonstrated them on heterogeneous Boolean systems.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–39},
numpages = {39},
keywords = {predicate rewriting, query subsumption, content-based retrieval, filtering, Boolean queries, query translation}
}

@article{10.1145/291128.291133,
author = {Croft, W. Bruce},
title = {Author Index},
year = {1998},
issue_date = {Oct. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/291128.291133},
doi = {10.1145/291128.291133},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {413–414},
numpages = {2}
}

@article{10.1145/291128.291132,
author = {Wang, Weigang and Rada, Roy},
title = {Structured Hypertext with Domain Semantics},
year = {1998},
issue_date = {Oct. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/291128.291132},
doi = {10.1145/291128.291132},
abstract = {One important facet of current hypertext research involves using knowledge-based techniques to develop and maintain document structures. A semantic net is one such technique. However, most semantic-net-based hypertext systems leave the linking consistency of the net to individual users. Users without guidance may accidentally introduce structural and relational inconsistencies in the semantic nets. The relational inconsistency hinders the creation of domain information models. The structural inconsistency leads to unstable documents, especially when a document is composed by computation with traversal algorithms. This work tackles to above problems by integrating logical structure and domain semantics into a semantic net. A semantic-net-based structured-hypertext model has been  formalized. The model preserves structural and relational consistency after changes to the semantic net. The hypertext system (RICH) based on this model has been implemented and tested. The RICH system can define and enforce a set of rules to maintain to integrity of the semantic net and provide particular support for creating multihierarchies with the reuse of existing contents and structures. Users have found such flexible but enforceable semantics to be helpful.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {372–412},
numpages = {41},
keywords = {hypertext structures, hypertext models, graph theory}
}

@article{10.1145/291128.291131,
author = {Kolda, Tamara G. and O'Leary, Dianne P.},
title = {A Semidiscrete Matrix Decomposition for Latent Semantic Indexing Information Retrieval},
year = {1998},
issue_date = {Oct. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/291128.291131},
doi = {10.1145/291128.291131},
abstract = {The vast amount of textual information available today is useless unless it can be effectively and efficiently searched. The goal in information retrieval is to find documents that are relevant to a given user query. We can represent and document collection by a matrix whose (i, j) entry is nonzero only if the ith term appears in the jth document; thus each document corresponds to a columm vector. The query is also represented as a column vector whose ith term is nonzero only if the ith term appears in the query. We score each document for relevancy by taking its inner product with the query. The highest-scoring documents are considered the most relevant. Unfortunately, this method does not necessarily retrieve all relevant documents because it is based on literal term matching.  Latent semantic indexing (LSI) replaces the document matrix with an approximation generated by the truncated singular-value decomposition (SVD). This method has been shown to overcome many difficulties associated with literal term matching. In this article we propose replacing the SVD with the semidiscrete decomposition (SDD). We will describe the SDD approximation, show how to compute it, and compare the SDD-based LSI method to the SVD-based LSI methods. We will show that SDD-based LSI does as well as SVD-based LSI in terms of document retrieval while requiring only one-twentieth the storage and one-half the time to compute each query. We will also show how to update the SDD approximation when documents are added or deleted from the document collection.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {322–346},
numpages = {25},
keywords = {latent semantic indexing, semidiscrete decomposition, singular-value decomposition, data mining, text retrieval}
}

@article{10.1145/291128.291130,
author = {Ram, Sudha and Ramesh, V.},
title = {Collaborative Conceptual Schema Design: A Process Model and Prototype System},
year = {1998},
issue_date = {Oct. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/291128.291130},
doi = {10.1145/291128.291130},
abstract = {Recent years have seen an increased interest in providing support for collaborative activities among groups of users participating in various information systems design tasks such as, requirements determination and process modeling. However, little attention has been paid to the collaborative conceptual database design process.  In this article, we develop a model of the collaborative conceptual schema development process and describe the design and implementation of a graphical multiuser conceptual schema design tool that is based on the model.  The system we describe allows a group of users to work collaboratively on the creation of database schemas in synchronous (same-time) mode (either in a face-to-face or distributed setting). Extensive modeling support is provided to assist  users in creating semantically correct conceptual schemas. The system also provides users with several graphical facilities such as, a large drawing workspace with the ability to scroll or “jump” to any portion of this workspace, zooming capabilities, and the ability to move object(s) to any portion of the workspace. The unique component of the system, however, is its built-in support for collaborative schema design.  The system supports a relaxed WYSIWIS environment, i.e., each user can control the graphical layout of the same set of schema objects.  The system ensures that changes/additions made by any user are consistent.  Any conflicts that may compromise to the integrity of the shared schema are flagged and resolved by the system.  The results from a  preliminary experiment suggest that the use of our system in a collaborative mode improved information sharing among users, minimized conflicts, and led to a more comprehensive schema definition.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {347–371},
numpages = {25},
keywords = {database design, groupware, graphical CASE tools, collaboration, semantic modeling, conceptual modeling}
}

@article{10.1145/291128.291129,
author = {Egenhofer, Max J. and Shariff, A. Rashid B. M.},
title = {Metric Details for Natural-Language Spatial Relations},
year = {1998},
issue_date = {Oct. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/291128.291129},
doi = {10.1145/291128.291129},
abstract = {Spatial relations often are desired answers that a geographic information system (GIS) should generate in response to a user's query.  Current GIS's provide only rudimentary support for processing and interpreting natural-language-like spatial relations, because their models and representations are primarily quantitative, while natural-language spatial relations are usually dominated by qualitative properties. Studies of the use of spatial relations in natural language showed that topology accounts for a significant portion of the geometric properties. This article develops a formal model that captures metric details for the description of natural-language spatial relations.  The metric details are expressed as refinements of the categories identified by the   9-intersection, a model for topological spatial relations, and provide a more precise measure than does topology alone as to whether a geometric configuration matches with a spatial term or not. Similarly, these measures help in identifying the spatial term that describes a particular configuration. Two groups of metric details are derived: splitting ratios as the normalized values of lengths and areas of intersections; and closeness measures as the normalized distances between disjoint object parts. The resulting model of topological and metric properties was calibrated for 64 spatial terms in English, providing values for the best fit as well as value ranges for the significant parameters of each term.  Three examples demonstrate how the framework and   its calibrated values are used to determine the best spatial term for a relationship between two geometric objects.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {295–321},
numpages = {27},
keywords = {spatial relations, geographic information systems, topological relations, GIS, Metric refinements}
}

@article{10.1145/290159.290162,
author = {Moffat, Alistair and Neal, Radford M. and Witten, Ian H.},
title = {Arithmetic Coding Revisited},
year = {1998},
issue_date = {July 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/290159.290162},
doi = {10.1145/290159.290162},
abstract = {Over the last decade, arithmetic coding has emerged as an important compression tool. It is now the method of choice for adaptive coding on myltisymbol alphabets because of its speed, low storage requirements, and effectiveness of compression. This article describes a new implementation of arithmetic coding that incorporates several improvements over a widely used earlier version by Witten, Neal, and Cleary, which has become a de facto standard. These improvements include fewer multiplicative operations, greatly extended range of alphabet sizes and symbol probabilities, and the use of low-precision arithmetic, permitting implementation by fast shift/add operations. We also describe a modular structure that separates the coding, modeling, and probability estimation  components of a compression system. To motivate the improved coder, we consider the needs of a word-based text compression program. We report a range of experimental results using this and other models. Complete source code is available.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {256–294},
numpages = {39},
keywords = {text compression, arithmetic coding, word-based model, approximate coding}
}

@article{10.1145/290159.290161,
author = {Crestani, F. and van Rijsbergen, C. J.},
title = {A Study of Probability Kinematics in Information Retrieval},
year = {1998},
issue_date = {July 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/290159.290161},
doi = {10.1145/290159.290161},
abstract = {We analyze the kinematics of probabilistic term weights at retrieval time for different Information Retrieval models. We present four models based on different notions of probabilistic retrieval. Two of these models are based on classical probability theory and can be considered as prototypes of models long in use in Information Retrieval, like the Vector Space Model and the Probabilistic Model. The two other models are based on a logical technique of evaluating the probability of a conditional called imaging; one is a generalization of the other. We analyze the transfer of probabilities occurring in the term space at retrieval time for these four models, compare their retrieval performance using classical test collections, and discuss the results. We believe that our results provide  useful suggestions on how to improve existing probabilistic models of Information Retrieval by taking into consideration term-term similarity.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {225–255},
numpages = {31},
keywords = {probabilistic retrieval, logical imaging, probabilistic modeling}
}

@article{10.1145/290159.290160,
author = {Ackerman, Mark S.},
title = {Augmenting Organizational Memory: A Field Study of Answer Garden},
year = {1998},
issue_date = {July 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/290159.290160},
doi = {10.1145/290159.290160},
abstract = {A growing concern for organizations and groups has been to augment their knowledge and expertise. One such augmentation is to provide an organizational memory, some record of the organization's knowledge. However, relatively little is known about how computer systems might enhance organizational, group, or community memory. This article presents Answer Garden, a system for growing organizational memory. The article describes the system and its underlying implementation. It then presents findings from a field study of Answer Garden. The article discusses the usage data and qualitative evaluations from the field study, and then draws a set of lessons for next-generation organizational memory systems.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {203–224},
numpages = {22},
keywords = {group memory, organizational memory, CSCW, collective memory, computer-supported cooperative work, community memory, field studies}
}

@article{10.1145/279339.279342,
author = {Belussi, Alberto and Faloutsos, Christos},
title = {Self-Spacial Join Selectivity Estimation Using Fractal Concepts},
year = {1998},
issue_date = {April 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/279339.279342},
doi = {10.1145/279339.279342},
abstract = {The problem of selectivity estimation for queries of nontraditional databases is still an open issue. In this article, we examine the problem of selectivity estimation for some types of spatial queries in databases containing real data. We have shown earlier [Faloutsos and Kamel 1994] that real point sets typically have a nonuniform distribution, violating consistently the uniformity and independence assumptions. Moreover, we demonstrated that the theory of fractals can help to describe real point sets. In this article we show how the concept of fractal dimension, i.e., (noninteger) dimension, can lead to the solution for the selectivity estimation problem in spatial databases. Among the infinite family of fractal dimensions, we consider here the  Hausdorff fractal dimension D0 and the “Correlation” fractal dimension D2. Specifically, we show that (a) the average number of neighbors for a given point set follows a power law, with D2 as exponent, and (b) the average number of nonempty range queries follows a power law with E − D0 as exponent (E is the dimension of the embedding space). We present the formulas to estimate the selectivity for “biased” range queries, for self-spatial joins, and for the average number of nonempty range queries. The result of some experiments on real and synthetic point sets are shown. Our formulas achieve very low  relative errors, typically about 10%, versus 40%–100% of the formulas that are based on the uniformity and independence assumptions.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {161–201},
numpages = {41},
keywords = {spatial join, range query, selectivity estimation, fractal dimension}
}

@article{10.1145/279339.279341,
author = {Hicks, David L. and Leggett, John J. and N\"{u}rnberg, Peter J. and Schnase, John L.},
title = {A Hypermedia Version Control Framework},
year = {1998},
issue_date = {April 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/279339.279341},
doi = {10.1145/279339.279341},
abstract = {The areas of application of hypermedia technology, combined with the capabilities that hypermedia provides for manipulating structure, create an environment in which version control is very important. A hypermedia version control framework has been designed to specifically address the version control problem in open hypermedia environments. One of the primary distinctions of the framework is the partitioning of hypermedia version control functionality into intrinsic and application-specific categories. The version control has been used as a model for the design of version control services for a hyperbase management system that provides complete version support for both data and structural entities. In addition to serving as a version control  model for open hypermedia environments, the framework offers a clarifying and unifying context in which to examine the issues of version control in hypermedia.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {127–160},
numpages = {34},
keywords = {hypermedia, hyperbase management system}
}

@article{10.1145/279339.279340,
author = {Wilber, W. John},
title = {The Knowledge in Multiple Human Relevance Judgments},
year = {1998},
issue_date = {April 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/279339.279340},
doi = {10.1145/279339.279340},
abstract = {We show first that the pooling of multiple human judgments of relevance provides predictor of relevance that is superior to that obtained from a single human's relevance judgemts. A learning algorithm applied to a set of relevance judgments obtained from a single human would be expected to perform on new material at a level somewhat below that human. However, we examine two learning methods which when trained on the superior source of pooled human relevance judgments are able to perform at the level of a single human on new material. All performance comparisons are based on an independent human judge. Both algorithms function by producing term weights—one by a log odds calculation and the other by producing a least-squares fit to human relevance ratings. Some characteristics of  the algorithms are examined.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {101–126},
numpages = {26},
keywords = {vector model, minimal-length least-square fit, human relevance judgments, document retrieval test set construction, log odds of relevance, inverse document frequency weights, word weights}
}

@article{10.1145/267954.267958,
author = {Romm, Celia T. and Pliskin, Nava},
title = {Electronic Mail as a Coalition-Building Information Technology},
year = {1998},
issue_date = {Jan. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/267954.267958},
doi = {10.1145/267954.267958},
abstract = {One of the most intriguing lines of research within the literature on diffusion of information technologies (IT) is the study of the power and politics of this process. The major objective of this article is to build on the work of Kling and Markus on power and IT, by extending their perspective to email. To demonstrate how email can be used for political purposes within an organizational context, a case study is presented. The case study describes a series of events which took place in a university. In the case, email was used by a group of employees to stage a rebellion against the university president. The discussion demonstrates that email features make it amenable to a range of political uses. The article is concluded with a discussion of the implications from this case to email research and practice.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {82–100},
numpages = {19},
keywords = {abuse, politics, MIS, email, coalition building}
}

@article{10.1145/267954.267957,
author = {Xu, Jinxi and Croft, W. Bruce},
title = {Corpus-Based Stemming Using Cooccurrence of Word Variants},
year = {1998},
issue_date = {Jan. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/267954.267957},
doi = {10.1145/267954.267957},
abstract = {Stemming is used in many information retrieval (IR) systems to reduce variant word forms to common roots. It is one of the simplest applications of natural-language processing to IR and is one of the most effective in terms of user acceptance and consistency, though small retrieval improvements. Current stemming techniques do not, however, reflect the language use in specific corpora, and this can lead to occasional serious retrieval failures. We propose a technique for using corpus-based word variant cooccurrence statistics to modify or create a stemmer. The experimental results generated using English newspaper and legal text and Spanish text demonstrate the viability of this technique and its advantages relative to conventional approaches that only employ morphological rules.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {61–81},
numpages = {21},
keywords = {class refinement, n-gram, information retrieval, corpus analysis, cooccurence, stemming}
}

@article{10.1145/267954.267956,
author = {Vujovic, N. and Brzakovic, D.},
title = {Evaluation of an Algorithm for Finding a Match of a Distorted Texture Pattern in a Large Image Database},
year = {1998},
issue_date = {Jan. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/267954.267956},
doi = {10.1145/267954.267956},
abstract = {Evaluation of an algorithm for finding a match for a random texture pattern in a large image database is presented. The algorithm was designed assuming that the random pattern may be subject to misregistration relative to its representation in the database and assuming that it may have missing parts. The potential applications involve authentication of legal documents, bank notes, or credit cards, where thin fibers are embedded randomly into the document medium during medium fabrication. The algorithm achieves image matching by a three-step hierarchical procedure, which starts by matching parts of fiber patterns while solving the misregistration problem and ends up by matching complete fiber patterns. Performance of the algorithm is studied both theoretically and experimentally. Theoretical analysis includes the study of the probability that two documents have the same pattern, and the probability of the algorithm establishing a wrong match, as well as the algorithm's performance in terms of processing  time. Experiments involving over 250,000 trials using databases of synthetic documents, containing up to 100,000 documents, were used to confirm theoretical predictions. In addition, experiments involving a database containing real images were conducted in order to confirm that the algorithm has potential in real applications.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {31–60},
numpages = {30},
keywords = {random pattern, image database, presentation of information, image matching, misregistration}
}

@article{10.1145/267954.267955,
author = {Stotts, P. David and Furuta, Richard and Cabarrus, Cyrano Ruiz},
title = {Hyperdocuments as Automata: Verification of Trace-Based Browsing Properties by Model Checking},
year = {1998},
issue_date = {Jan. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/267954.267955},
doi = {10.1145/267954.267955},
abstract = {We present a view of hyperdocuments in which each document encodes its own browsing semantics in its links. This requires a mental shift in how a hyperdocument is thought of abstractly. Instead of treating the links of a document as defining a static directed graph, they are thought of as defining an abstract program, termed the links-automaton of the document. A branching temporal logic notation, termed HTL*, is introduced for specifying properties a document should exhibit during browsing. An automated program verification technique called model checking is used to verify that browsing specifications in a subset of HTL* are met by the behavior defined in the links-automation. We illustrate the generality of these techniques by applying them first to several Trellis documents and then to a Hyperties document.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–30},
numpages = {30},
keywords = {Petri nets, browsing semantics, model checking, temporal logic, hypermedia, hypertext}
}

@article{10.1145/263479.263482,
author = {Navarro, Gonzalo and Baeza-Yates, Ricardo},
title = {Proximal Nodes: A Model to Query Document Databases by Content and Structure},
year = {1997},
issue_date = {Oct. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/263479.263482},
doi = {10.1145/263479.263482},
abstract = {A model to query document databases by both their content and structure is presented. The goal is to obtain a query language that is expressive in practice while being efficiently implementable, features not present at the same time in previous work. The key ideas of the model are a set-oriented query language based on operations on nearby structure elements of one or more hierarchies, together with content and structural indexing and bottom-up evaluation. The model is evaluated in regard to expressiveness and efficiency, showing that it provides a good trade-off between both goals. Finally, it is shown how to include in the model other media different from text.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {400–435},
numpages = {36},
keywords = {structured text, hierarchical documents, text algebras, expressivity and efficiency of query languages}
}

@article{10.1145/263479.263481,
author = {Mostafa, J. and Mukhopadhyay, S. and Palakal, M. and Lam, W.},
title = {A Multilevel Approach to Intelligent Information Filtering: Model, System, and Evaluation},
year = {1997},
issue_date = {Oct. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/263479.263481},
doi = {10.1145/263479.263481},
abstract = {In information-filtering environments, uncertainties associated with changing interests of the user and the dynamic document stream must be handled efficiently. In this article, a filtering model is proposed that decomposes the overall task into subsystem functionalities and highlights the need for multiple adaptation techniques to cope with uncertainties. A filtering system, SIFTER, has been implemented based on the model, using established techniques in information retrieval and artificial intelligence. These techniques include document representation by a vector-space model, document classification by unsupervised learning, and user modeling by reinforcement learning. The system can filter information based on content and a user's specific interests. The user's interests are automatically learned with only limited user intervention in the form of optional relevance feedback for documents. We also describe experimental studies conducted with SIFTER to filter computer and information science documents collected from the Internet and commercial database services. The experimental results demonstrate that the system performs very well in filtering documents in a realistic problem setting.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {368–399},
numpages = {32},
keywords = {information filtering, user modeling, automated document representation}
}

@article{10.1145/263479.263480,
author = {Kimbrough, Steven O. and Moore, Scott A.},
title = {On Automated Message Processing in Electronic Commerce and Work Support Systems: Speech Act Theory and Expressive Felicity},
year = {1997},
issue_date = {Oct. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/263479.263480},
doi = {10.1145/263479.263480},
abstract = {Electronic messaging, whether in an office environment or for electronic commerce, is normally carried out in natural language, even when supported by information systems. For a variety of reasons, it would be useful if electronic messaging systems could have semantic access to, that is, access to the meanings and contents of, the messages they process. Given that natural language understanding is not a practicable alternative, there remain three approaches to delivering systems with semantic access: electronic data interchange (EDI), tagged messages, and the development of a formal language for business communication (FLBC). We favor the latter approach. In this article we compare and contrast these three approaches, present a theoretical basis for an FLBC (using speech act theory), and describe a prototype implementation.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {321–367},
numpages = {47},
keywords = {formal language for business communication, electronic commerce, speech act theory}
}

@article{10.1145/256163.256168,
author = {Cohen, Jonathan D.},
title = {Recursive Hashing Functions for <i>n</i>-Grams},
year = {1997},
issue_date = {July 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/256163.256168},
doi = {10.1145/256163.256168},
abstract = {Many indexing, retrieval, and comparison methods are based on counting or cataloguing n-grams in streams of symbols. The fastest method of implementing such operations is through the use of hash tables. Rapid hashing of consecutive n-grams is best done using a recursive hash function, in which the hash value of the current n-gram is drived from the hash value of its predecessor. This article generalizes recursive hash functions found in the literature and proposes new methods offering superior performance. Experimental results demonstrate substantial speed improvement over conventional approaches, while retaining near-ideal hash value distribution.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {291–320},
numpages = {30},
keywords = {hashing, recursive hashing, n-grams, hashing functions}
}

@article{10.1145/256163.256166,
author = {Bookstein, A. and Klein, S. T. and Raita, T.},
title = {Modeling Word Occurrences for the Compression of Concordances},
year = {1997},
issue_date = {July 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/256163.256166},
doi = {10.1145/256163.256166},
abstract = {An earlier paper developed a procedure for compressing concordances, assuming that all alements occurred independently. The models introduced in that paper are extended here to take the possiblity of clustering into account. The concordance is conceptualized as a set of bitmaps, in which the bit locations reporesent documents, and the one-bits represent the occurrence of given terms. Hidden Markov Models (HMM's) are used to describe the clustering of the one-bits. However, for computational reasons, the HMM is approximated by traditional Markov models. A set of criteria is developed to constrain the allowable set of n-state models, and a full inventory is given for n ≤ 4. Graph-theoretic reduction and complementation  operations are defined among the various models and are used to provide a structure relating the models studied. Finally, the new methods were tested on the concordances of the English Bible and of two of the world's largest full-text retrieval systems: the Tre´sor de la Langue Franc¸aise and the Responsa Project.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {254–290},
numpages = {37},
keywords = {concordance storage, concordance organization, graph structure, classification of graph nodes}
}

@article{10.1145/256163.256165,
author = {Tomasic, Anthony and Gravano, Luis and Lue, Calvin and Schwarz, Peter and Haas, Laura},
title = {Data Structures for Efficient Broker Implementation},
year = {1997},
issue_date = {July 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/256163.256165},
doi = {10.1145/256163.256165},
abstract = {With the profusion of text databases on the Internet, it is becoming increasingly hard to find the most useful databases for a given query. To attack this problem, several existing and proposed systems employ brokers to direct user queries, using a local database of summary information about the available databases. This summary information must effectively distinguish relevant databases and must be compact while allowing efficient access. We offer evidence that one broker, GlOSS, can be effective at locating databases of interest even in a system of hundreds of databased and can examine the performance of accessing the
GlOSS summeries for two promising storage methods: the grid file and partitioned hashing. We show that both methods can be tuned to provide good performance for a  particular workload (within a broad range of workloads), and we discuss the tradeoffs between the two data structures. As a side effect of our work, we show that grid files are more broadly applicable than previously thought; inparticular, we show that by varying the policies used to construct the grid file we can provide good performance for a wide range of workloads even when storing highly skewed data.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {223–253},
numpages = {31},
keywords = {grid files, broker performance, partitioned hashing, distributed information, GlOSS, broker architecture}
}

@article{10.1145/256163.256164,
author = {Dreilinger, Daniel and Howe, Adele E.},
title = {Experiences with Selecting Search Engines Using Metasearch},
year = {1997},
issue_date = {July 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/256163.256164},
doi = {10.1145/256163.256164},
abstract = {Search engines are among the most useful and high-profile resources on the Internet. The problem of finding information on the Internet has been replaced with the problem of knowing where search engines are, what they are designed to retrieve, and how to use them. This article describes and evaluates SavvySearch, a metasearch engine designed to intelligently select and interface with multiple remote search engines. The primary metasearch issue examined is the importance of carefully selecting and ranking remote search engines for user queries. We studied the efficacy of SavvySearch's incrementally acquired metaindex approach to selecting search engines by analyzing the effect of time and experience on performance. We also compared the metaindex approach to the 
 simpler categorical approach and showed how much experience is required to surpass the simple scheme.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {195–222},
numpages = {28},
keywords = {machine learning, WWW, search engine, information retrieval}
}

@article{10.1145/248625.248652,
author = {Gladney, H. M.},
title = {Access Control for Large Collections},
year = {1997},
issue_date = {April 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/248625.248652},
doi = {10.1145/248625.248652},
abstract = {Efforts to place vast information resources at the fingertips of each individual in large user populations must be balanced by commensurate attention to information protection. For distributed systems with less-structured tasks, more-diversified information, and a heterogeneous user set, the computing system must administer enterprise-chosen access control policies. One kind of resource is a digital library that emulates massive collections of paper and other physical media for clerical, engineering, and cultural applications. This article considers the security requirements for such libraries and proposes an access control method that mimics organizational practice by combining a subject tree with ad hoc role granting that controls privileges for many operations independently, that  treats (all but one) privileged roles (e.g., auditor, security officer) like every other individual authorization, and that binds access control information to objects indirectly for scaling, flexibility, and reflexive protection. We sketch a realization and show that it will perform well, generalizes many deployed proposed access control policies, and permits individual data centers to implement other models economically and without disruption.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {154–194},
numpages = {41},
keywords = {digital library, information security, electronic library, document, access control}
}

@article{10.1145/248625.248650,
author = {Dunlop, Mark D.},
title = {The Effect of Accessing Nonmatching Documents on Relevance Feedback},
year = {1997},
issue_date = {April 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/248625.248650},
doi = {10.1145/248625.248650},
abstract = {Traditional information retrieval (IR) systems only allow users access to documents that match their current query, and therefore, users can only give relevance feedback on matching documents (or those with a matching strength greater than a set threshold. This article shows that, in systems that allow access to nonmatching documents (e.g., hybrid hypertext and information retrieval systems), the strength of the effect of giving relevance feedback varies between matching and nonmatching documents. For positive feedback the results shown here are encouraging, as they can be justified by an intuitive view of the process. However, for negative feedback the results show behavior that cannot easily be justified and that varies greatly depending on the model of feedback used.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {137–153},
numpages = {17},
keywords = {negative feedback, relevance feedback, hypertext, probabilistic model, vector space model, free-text information retrieval}
}

@article{10.1145/248625.248639,
author = {Manber, Udi},
title = {A Text Compression Scheme That Allows Fast Searching Directly in the Compressed File},
year = {1997},
issue_date = {April 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/248625.248639},
doi = {10.1145/248625.248639},
abstract = {A new text compression scheme is presented in this article. The main purpose of this scheme is to speed up string matching by searching the compressed file directly. The scheme requires no modification of the string-matching algorithm, which is used as a black box; any string-matching procedure can be used. Instead, the pattern is modified; only the outcome of the matching of the modified pattern against the compressed file is decompressed. Since the compressed file is smaller than the original file, the search is faster both in terms of I/O time and precessing time than a search in the original file. For typical text files, we achieve about 30% reduction of space and slightly less of search time. A 30% space saving is not competitive with good text compression schemes, and thus should not be used where space is the predominant concern. The intended applications  of this scheme are files that are searched often, such as catalogs, bibliographic files, and address books. Such files are typically not compressed, but with this scheme they can remain compressed indefinitely, saving space while allowing faster search at the same time. A particular application to an information retrieval system that we developed is also discussed.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {124–136},
numpages = {13},
keywords = {data compression search}
}

@article{10.1145/248625.248627,
author = {Entlich, Richard and Olsen, Jan and Garson, Lorrin and Lesk, Michael and Normore, Lorraine and Weibel, Stuart},
title = {Making a Digital Library: The Contents of the CORE Project},
year = {1997},
issue_date = {April 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/248625.248627},
doi = {10.1145/248625.248627},
abstract = {The CORE (Chemical Online Retrieval Experiment) project is a library of primary journal articles in chemistry. Any library has an inside and an outside; in this article we describe the inside of the library and the methods for building the system and accumulating the database. A later article will describe the outside (user experiences). Among electronic-library projects, the CORE project is unusual in that it has both ASCII derived from typesetting and image data for all its pages, and among experimental electronic-library projects, it is unusually large. We describe here (a) the processes of scanning and analyzing about 400,000 pages of primary journal material, (b) the conversion of a similar amount of textual database material, (c) the linking of these two data sources, and (d)  the indexing of the text material.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {103–123},
numpages = {21},
keywords = {image segmentation}
}

@article{10.1145/239041.239048,
author = {Rus, Daniela and Subramanian, Devika},
title = {Customizing Information Capture and Access},
year = {1997},
issue_date = {Jan. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/239041.239048},
doi = {10.1145/239041.239048},
abstract = {This article presents a customizable architecture for software agents that capture and access information in large, heterogeneous, distributed electronic repositories. The key idea is to exploit underlying structure at various levels of granularity to build high-level indices with task-specific interpretations. Information agents construct such indices and are configured as a network of reusable modules called structure detectors and segmenters. We illustrate our architecture with the design and implementation of smart information filters in two contexts: retrieving stock market data from Internet newsgroups and retrieving technical reports from Internet FTP sites.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {67–101},
numpages = {35},
keywords = {software agents, table recognition, information gathering}
}

@article{10.1145/239041.239045,
author = {Fuhr, Norbert and R\"{o}lleke, Thomas},
title = {A Probabilistic Relational Algebra for the Integration of Information Retrieval and Database Systems},
year = {1997},
issue_date = {Jan. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/239041.239045},
doi = {10.1145/239041.239045},
abstract = {We present a probabilistic relational algebra (PRA) which is a generalization of standard relational algebra. In PRA, tuples are assigned probabilistic weights giving the probability that a tuple belongs to a relation. Based on intensional semantics, the tuple weights of the result of a PRA expression always conform to the underlying probabilistic model. We also show for which expressions extensional semantics yields the same results. Furthermore, we discuss complexity issues and indicate possibilities for optimization. With regard to databases, the approach allows for representing imprecise attribute values, whereas for information retrieval, probabilistic document indexing and probabilistic search term weighting can be modeled. We introduce the concept of vague predicates which  yield probabilistic weights instead of Boolean values, thus allowing for queries with vague selection conditions. With these features, PRA implements uncertainty and vagueness in combination with the relational model.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {32–66},
numpages = {35},
keywords = {logical retrieval model, relational data model, uncortain data, vague predicates, probabilistic retrieval, hypertext retrieval, imprecise data}
}

@article{10.1145/239041.239043,
author = {Wiil, Uffe K. and Leggett, John J.},
title = {Hyperform: A Hypermedia System Development Environment},
year = {1997},
issue_date = {Jan. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/239041.239043},
doi = {10.1145/239041.239043},
abstract = {Development of hypermedia systems is a complex matter. The current trend toward open, extensible, and distributed multiuser hypermedia systems adds additional complexity to the development process. As a means of reducing this complexity, there has been an increasing interest in hyperbase management systems that allow hypermedia system developers to abstract from the intricacies and complexity of the hyperbase layer and fully attend to application and user interface issues. Design, development, and deployment experiences of a dynamic, open, and distributed multiuser hypermedia system development environment called Hyperform is presented. Hyperform is based on the concepts of extensibility, tailorability, and rapid prototyping of hypermedia system services. Open, extensible hyperbase management systems permit hypermedia system developers to tailor hypermedia functionality for specific applications and to serve as a platform for research. The Hyperform development environment is comprised of multiple instances of four component types: (1) a hyperbase management system server, (2) a tool integrator, (3) editors, and (4) participating tools. Hyperform has been deployed in Unix environments, and experiments have shown that Hyperform greatly reduces the effort required to provide customized hyperbase management system support for distributed multiuser hypermedia systems.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–31},
numpages = {31},
keywords = {advanced hypermedia system architecture, object-oriented extension language, extensible hyperbase management system}
}

@article{10.1145/237496.237500,
author = {Author Index},
title = {Author Index},
year = {1996},
issue_date = {Oct. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/237496.237500},
doi = {10.1145/237496.237500},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {471–472},
numpages = {2}
}

@article{10.1145/237496.237499,
author = {Cheung, Waiman and Hsu, Cheng},
title = {The Model-Assisted Global Query System for Multiple Databases in Distributed Enterprises},
year = {1996},
issue_date = {Oct. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/237496.237499},
doi = {10.1145/237496.237499},
abstract = {Today's enterprises typically employ multiple information systems, which are independently developed, locally administered, and different in logical or physical designs. Therefore, a fundamental challenge in enterprise information management is the sharing of information for enterprise users across organizational boundaries; this requires a global query system capable of providing on-line intelligent assistance to users. Conventional technologies, such as schema-based query languages and hard-coded schema integration, are not sufficient to solve this problem. This article develops a new approach, a “model-assisted global query system,” that utilizes an on-line repository of enterprise metadata—the Metadatabase—to facilitate global query formulation and processing with  certain desirable properties such as adaptiveness and open-systems architecture. A definitional model characterizing the various classes and roles of the required metadata as knowledge for the system is presented. The significance of possessing this knowledge (via a Metadatabase) toward improving the global query capabilities available previously is analyzed. On this basis, a direct method using model traversal and a query language using global model constructs are developed along with other new methods required for this approach. It is then tested through a prototype system in a computer-integrated manufacturing (CIM) setting.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {421–470},
numpages = {50}
}

@article{10.1145/237496.237498,
author = {Oberweis, Andreas and Sander, Peter},
title = {Information System Behavior Specification by High Level Petri Nets},
year = {1996},
issue_date = {Oct. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/237496.237498},
doi = {10.1145/237496.237498},
abstract = {The specification of an information system should include a description of structural system aspects as well as a description of the system behavior. In this article, we show how this can be achieved by high-level Petri nets—namely, the so-called NR/T-nets (Nested-Relation/Transition Nets). In NR/T-nets, the structural part is modeled by nested relations, and the behavioral part is modeled by a novel Petri net formalism. Each place of a net represents a nested relation scheme, and the marking of each place is given as a nested relation of the respective type. Insert and delete operations in a nested relational database (NF2-database) are expressed by transitions in a net. These operations may operate not only on whole tuples of a given relation, but also on “subtuples”  of existing tuples. The arcs of a net are inscribed with so-called Filter Tables, which allow (together with an optional logical expression as transition inscription) conditions to be formulated on the specified (sub-) tuples. The occurrence rule for NR/T-net transitions is defined by the operations union, intersection, and “negative” in lattices of nested relations. The structure of an NR/T-net, together with the occurrence rule, defines classes of possible information system procedures, i.e., sequences of (possibly concurrent) operations in an information system.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {380–420},
numpages = {41}
}

@article{10.1145/237496.237497,
author = {Moffat, Alistair and Zobel, Justin},
title = {Self-Indexing Inverted Files for Fast Text Retrieval},
year = {1996},
issue_date = {Oct. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/237496.237497},
doi = {10.1145/237496.237497},
abstract = {Query-processing costs on large text databases are dominated by the need to retrieve and scan the inverted list of each query term. Retrieval time for inverted lists can be greatly reduced by the use of compression, but this adds to the CPU time required. Here we show that the CPU component of query response time for conjunctive Boolean queries and for informal ranked queries can be similarly reduced, at little cost in terms of storage, by the inclusion of an internal index in each compressed inverted list. This method has been applied in a retrieval system for a collection of nearly two million short documents. Our experimental results show that the self-indexing strategy adds less than 20% to the size of the compressed inverted file, which itself occupies less than 10% of the  indexed text, yet can reduce processing time for Boolean queries of 5-10 terms to under one fifth of the previous cost. Similarly, ranked queries of 40-50 terms can be evaluated in as little as 25% of the previous time, with little or no loss of retrieval effectiveness.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {349–379},
numpages = {31}
}

@article{10.1145/230538.230561,
author = {Friedman, Batya and Nissenbaum, Helen},
title = {Bias in Computer Systems},
year = {1996},
issue_date = {July 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/230538.230561},
doi = {10.1145/230538.230561},
abstract = {From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria—including reliability, accuracy, and efficiency—according to which the quality of systems in use in society should be judged.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {330–347},
numpages = {18},
keywords = {system design, computers and society, ethics, social impact, values, computer ethics, standards, social computing, universal design, bias, design methods, human values}
}

@article{10.1145/230538.230560,
author = {Gulla, Jon Atle},
title = {A General Explanation Component for Conceptual Modeling in CASE Environments},
year = {1996},
issue_date = {July 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/230538.230560},
doi = {10.1145/230538.230560},
abstract = {In information systems engineering, conceptual models are constructed to assess existing information systems and work out requirements for new ones. As these models serve as a means for communication between customers and developers, it is paramount that both parties understand the models, as well as that the models form a proper basis for the subsequent design and implementation of the systems. New CASE environments are now experimenting with formal modeling languages and various techniques for validating conceptual models, though it seems difficult to come up with a technique that handles the linguistic barriers between the parties involved in a satisfactory manner. In this article, we discuss the theoretical basis of an explanation component implemented for the PPP CASE environment. This component integrates other validation techniques and provides a very flexible natural-language interface to complex model information. It describes properties of the modeling language and the conceptual models in terms familiar to users, and the explanations can be combined with graphical model views. When models are executed, it can justify requested inputs and explain computed outputs by relating trace information to properties of the models.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {297–329},
numpages = {33},
keywords = {paraphrasing, explanation generation, conceptual modeling, linguistics, requirements engineering, help systems}
}

@article{10.1145/230538.230540,
author = {Gottlob, Georg and Schrefl, Michael and R\"{o}ck, Brigitte},
title = {Extending Object-Oriented Systems with Roles},
year = {1996},
issue_date = {July 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/230538.230540},
doi = {10.1145/230538.230540},
abstract = {In many class-based object-oriented systems the association between as instance and a class is exclusive and permanent. Therefore these systems have serious difficulties in representing objects taking on different roles over time. Such objects must be reclassified any time they evolve (e.g., if a person becomes a student and later an employee). Class hierarchies must be planned carefully and may grow exponentially if entities may take on serveral independent roles. The problem is even more servere for object-oriented databases than for common object-oriented programming. Databases store objects over longer periods, during which the represented entities evolve. This article shows how class-based object-oriented systems can be extended to handle evolving objects well. Class hierarchies are complemented by role hierarchies, whose nodes represent role types an object classified in the root may take on. At any point in time, an entity is represented by an instance of the root and an instance of every role type whose role it currently plays. In a natural way, the approach extends traditional object-oriented concepts, such as classification, object identity, specialization, inheritance, and polymorphism in a natural way. The practicability of the approach is demonstrated by an implementation in Smalltalk. Smalltalk was chosen because it is widely known, which is not true for any particular class-based object-oriented database programming language. Roles can be provided in Smalltalk by adding a few classes. There is no need to modify the semantics of Smalltalk itself. Role hierarchies are mapped transparently onto ordinary classes. The presented implementation can easily be ported to object-oriented database programming languages based on Smalltalk, such as Gemstone's OPAL hierarchies are complemented by role hierarchies, whose nodes represent role types an object classified in the root may take on. At any point in time, an entity is represented by an instance of the root and an instance of every role type whose role in currently plays.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {268–296},
numpages = {29}
}

@article{10.1145/230538.230539,
author = {Guglielmo, Eugene J. and Rowe, Neil C.},
title = {Natural-Language Retrieval of Images Based on Descriptive Captions},
year = {1996},
issue_date = {July 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/230538.230539},
doi = {10.1145/230538.230539},
abstract = {We describe a prototype intelligent information retrieval system that uses natural-language understanding to efficiently locate captioned data. Multimedia data generally require captions to explain their features and significance. Such descriptive captions often rely on long nominal compounds (strings of consecutive nouns) which create problems of disambiguating word sence. In our system, captions and user queries are parsed and interpreted to produce a logical form using a detailed theory of the meaning of nominal compounds. A fine-grain match can then compare the logical form of the query to the logical forms for each caption. To improve system efficiency, we first perform a coarse-grain match with index files, using nouns and verbs extracted from the query. Our experiments with randomly selected queries and captions from an existing image library show an increase of 30% in precision and 50% in recall over the keyphrase approach currently used. Our processing times have a median of seven seconds as compared to eight minutes for the existing system, and our system is much easier to use.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {237–267},
numpages = {31},
keywords = {type hierarchy, captions, multimedia database}
}

@article{10.1145/226163.226167,
author = {Grant, Rebecca A. and Higgins, Chris A.},
title = {Computerized Performance Monitors as Multidimensional Systems: Derivation and Application},
year = {1996},
issue_date = {April 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/226163.226167},
doi = {10.1145/226163.226167},
abstract = {An increasing number of companies are introducing computer technology into more aspects of work. Effective use of information systems to support office and service work can improve staff productivity, broaden a company's market, or dramatically change its business. It can also increase the extent to which work is computer mediated and thus within the reach of software known as Computerized Performance Monitoring and Control Systems (CPMCSs). Virtually all research has studied CPMCSs as unidimensional systems. Employees are described as “monitored” or “unmonitored” or as subject to “high,” “moderate,” or “low” levels of monitoring. Research that does not clearly distinguish among possible monitor design cannot explain how  designs may differ in effect. Nor can it suggest how to design better monitors. A multidimensional view of CPMCSs describes monitor designs in terms of object of measurements, tasks measured, recipient of data, reporting period, and message content. This view is derived from literature in control systems, organizational behavior, and management information systems. The multidimensional view can then be incorporated into causal models to explain contradictory results of earlier CPMCS research.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {212–235},
numpages = {24},
keywords = {computerized performance evaluation, work monitoring system design, computerized work monitoring}
}

@article{10.1145/226163.226166,
author = {Jungclaus, Ralf and Saake, Gunter and Hartmann, Thorsten and Sernadas, Cristina},
title = {TROLL: A Language for Object-Oriented Specification of Information Systems},
year = {1996},
issue_date = {April 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/226163.226166},
doi = {10.1145/226163.226166},
abstract = {TROLL is a language particularly suited for the early stages of information system development, when the universe of discourse must be described. In TROLL the descriptions of the static and dynamic aspects of entities are integrated into object descriptions. Sublanguages for data terms, for first-order and temporal assertions, and for processes, are used to describe respectively the static properties, the behavior, and the evolution over time of objects. TROLL organizes system design through object-orientation and the support of abstractions such as classification, specialization, roles, and aggregation. Language features for state interactions and dependencies among components support the composition of the system from smaller modules, as does the facility of defining interfaces on top of object descriptions.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {175–211},
numpages = {37}
}

@article{10.1145/226163.226165,
author = {Rowe, Neil C.},
title = {Using Local Optimality Criteria for Efficient Information Retrieval with Redundant Information Filters},
year = {1996},
issue_date = {April 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/226163.226165},
doi = {10.1145/226163.226165},
abstract = {We consider information retrieval when the data—for instance, multimedia—is computationally expensive to fetch. Our approach uses “information filters” to considerably narrow the universe of possibilities before retrieval. We are especially interested in redundant information filters that save time over more general but more costly filters. Efficient retrieval requires that decisions must be made about the necessity, order, and concurrent processing of proposed filters (an “execution plan”). We develop simple polynomial-time local criteria for optimal execution plans and show that most forms of concurrency are suboptimal with information filters. Although the general problem of finding an optimal execution plan is likely to be exponential in the number of filters, we show experimentally that our local optimality criteria, used in a polynomial-time algorithm, nearly always find the global optimum with 15 filters or less, a sufficient number of filters for most applications. Our methods require no special hardware and avoid the high processor idleness that is characteristic of massive-parallelism solutions to this problem. We apply our ideas to an important application, information retrieval of captioned data using natural-language understanding, a problem for which the natural-language processing can be the bottleneck if not implemented well.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {138–174},
numpages = {37},
keywords = {conjunction, Boolean algebra, queries, filters, natural language, optimization}
}

@article{10.1145/226163.226164,
author = {Lee, Dik Kun and Ren, Liming},
title = {Document Ranking on Weight-Partitioned Signature Files},
year = {1996},
issue_date = {April 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/226163.226164},
doi = {10.1145/226163.226164},
abstract = {A signature file organization, called the weight-partitioned signature file, for supporting document ranking is proposed. It employs multiple signature files, each of which corresponds to one term frequency, to represent terms with different term frequencies. Words with the same term frequency in a document are grouped together and hashed into the signature file corresponding to that term frequency. This eliminates the need to record the term frequency explicitly for each word. We investigate the effect of false drops on retrieval effectiveness if they are not eliminated in the search process. We have shown that false drops introduce insignificant degradation on precision and recall when the false-drop probability is below a certain threshold. This is an important result since  false-drop elimination could become the bottleneck in systems using fast signature file search techniques. We perform an analytical study on the performance of the weight-partitioned signature file under different search strategies and configurations. An optimal formula is obtained to determine for a fixed total storage overhead the storage to be allocated to each partition in order to minimize the effect of false drops on document ranks. Experiments were performed using a document collection to support the analytical results.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {109–137},
numpages = {29},
keywords = {document retrieval, superimposed coding, signature file, access method, text retrieval, information retrieval}
}

@article{10.1145/214174.214183,
author = {Berghel, Hal and Roach, David},
title = {An Extension of Ukkonen's Enhanced Dynamic Programming ASM Algorithm},
year = {1996},
issue_date = {Jan. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/214174.214183},
doi = {10.1145/214174.214183},
abstract = {We describe an improvement on Ukkonen's Enhanced Dynamic Programming (EHD) approximate string-matching algorithm for unit-penalty four-edit comparisons. The new algorithm has an asymptotic complexity similar to that of Ukkonen's but is significantly faster due to a decrease in the number of array cell calculations. A 42% speedup was achieved in an application involving name comparisons. Even greater improvements are possible when comparing longer and more dissimilar strings. Although the speed of the algorithm under consideration is comparable to other fast ASM algorithms, it has greater effectiveness in text-processing applications because it supports all four basic Damerau-type editing operations.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {94–106},
numpages = {13},
keywords = {enhanced dynamic programming, approximate string matching, similarity relations, dynamic programming}
}

@article{10.1145/214174.214180,
author = {Taghva, Kazem and Borsack, Julie and Condit, Allen},
title = {Evaluation of Model-Based Retrieval Effectiveness with OCR Text},
year = {1996},
issue_date = {Jan. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/214174.214180},
doi = {10.1145/214174.214180},
abstract = {We give a comprehensive report on our experiments with retrieval from OCR-generated text using systems based on standard models of retrieval. More specifically, we show that average precision and recall is not affected by OCR errors across systems for several collections. The collections used in these experiments include both actual OCR-generated text and standard information retrieval collections corrupted through the simulation of OCR errors. Both the actual and simulation experiments include full-text and abstract-length documents. We also demonstrate that the ranking and feedback methods associated with these models are generally not robust enough to deal with OCR errors. It is further shown that the OCR errors and garbage strings generated from the mistranslation of graphic  objects increase the size of the index by a wide margin. We not only point out problems that can arise from applying OCR text within an information retrieval environment, we also suggest solutions to overcome some of these problems.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {64–93},
numpages = {30},
keywords = {error correction, optical character recognition, feedback, ranking algorithms}
}

@article{10.1145/214174.214178,
author = {Robey, Daniel and Newman, Michael},
title = {Sequential Patterns in Information Systems Development: An Application of a Social Process Model},
year = {1996},
issue_date = {Jan. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/214174.214178},
doi = {10.1145/214174.214178},
abstract = {We trace the process of developing and implementing a materials management system in one company over a 15-year period. Using a process research model developed by Newman and Robey, we identify 44 events in the process and define them as either encounters or episodes. Encounters are concentrated events, such as meetings and announcements, that separate episodes, which are events of longer duration. By examining the sequence of events over the 15 years of the case, we identify a pattern of repeated failure, followed by success. Our discussion centers on the value of detecting and displaying such patterns and the need for theoretical interpretation of recurring sequences of events. Five alternative theoretical perspectives, originally proposed by Kling, are used to interpret the sequential patterns identified by the model. We conclude that the form of the process model allows researchers who operate from different perspectives to enrich their understanding of the process of system development.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {30–63},
numpages = {34},
keywords = {social processes, system implementation}
}

@article{10.1145/214174.214175,
author = {Lucarella, Dario and Zanzi, Antonella},
title = {A Visual Retrieval Environment for Hypermedia Information Systems},
year = {1996},
issue_date = {Jan. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/214174.214175},
doi = {10.1145/214174.214175},
abstract = {We present a graph-based object model that may be used as a uniform framework for direct manipulation of multimedia information. After an introduction motivating the need for abstraction and structuring mechanisms in hypermedia systems, we introduce the data model and the notion of perspective, a form of data abstraction that acts as a user interface to the system, providing control over the visibility of the objects and their properties. A perspective is defined to include an intension and an extension. The intension is defined in terms of a pattern, a subgraph of the schema graph, and the extension is the set of pattern-matching instances. Perspectives, as well as database schema and instances, are graph structures that can be manipulated in various ways. The resulting uniform approach is well suited to a visual interface. A visual interface for complex information systems provides high semantic power, thus exploiting the semantic expressibility of the underlying data model, while maintaining ease of interaction with the system. In this way, we reach the goal of decreasing cognitive load on the user, with the additional advantage of always maintaining the same interaction style. We present a visual retrieval environment that effectively combines filtering, browsing, and navigation to provide an integrated view of the retrieval problem. Design and implementation issues are outlined for MORE (Multimedia Object Retrieval Environment), a prototype system relying on the proposed model. The focus is on the main user interface functionalities, and actual interaction sessions are presented including schema creation, information loading, and information retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {3–29},
numpages = {27},
keywords = {visual interface, information filtering, direct object manipulation, graph-oriented models, complex objects, hypermedia applications, browsing}
}

@article{10.1145/211430.215258,
author = {Stevens, Scott and Little, Thomas},
title = {Introduction to the Special Issue on Video Information Retrieval},
year = {1995},
issue_date = {Oct. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/211430.215258},
doi = {10.1145/211430.215258},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {371–372},
numpages = {2}
}

@article{10.1145/211430.211440,
author = {Keller, Ralf and Effelsberg, Wolfgang and Lamparter, Bernd},
title = {XMovie: Architecture and Implementation of a Distributed Movie System},
year = {1995},
issue_date = {Oct. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/211430.211440},
doi = {10.1145/211430.211440},
abstract = {We describe a system for storing, transmitting, and presenting digital movies in a computer network. The hardware used in the system is standard hardware, as found in typical workstations today; no special hardware is required, but if available it can be used to provide better performance. The XMovie system has several innovative features. First, it contains a new algorithm for the gradual adaptation of the color lookup table during the presentation of the movie to ensure optimal color quality on low-end workstations. Second, it is a multistandard system supporting the compression techniques MPEG, Motion JPEG, and a newly developed extension to the well-known Color Cell Compression method. Third, it contains AdFEC, a new adaptable forward error correction method for our movie  transmission protocol.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {471–499},
numpages = {29},
keywords = {distributed multimedia system, software motion picture, transmission protocol, digital video}
}

@article{10.1145/211430.211439,
author = {Bulterman, Dick C. A.},
title = {Embedded Video in Hypermedia Documents: Supporting Integration and Adaptive Control},
year = {1995},
issue_date = {Oct. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/211430.211439},
doi = {10.1145/211430.211439},
abstract = {As the availability of digital video becomes commonplace, a shift in application focus will occur from merely accessing video as an independent data stream to embedding video with other multimedia data types into coordinated hypermedia presentations. The migration to embedded video will present new demands on application and support environments: processing of any one piece of video data will depend on how that data relates to other data streams active within the same presentation. This article describes presentation, synchronization, and interaction control issues for manipulating embedded video. First we describe the requirements for embedded video, contrasted against other forms of video use. Next we consider mechanisms for describing and implementing the behavior of embedded-video segments relative to other data items in a document; these relationships form the basis of implementing cooperative control among the events in a presentation. Finally we consider extending the possibilities for tailoring embedded video to the characteristics of the local runtime environment; this forms the basis for adaptive, application-level quality-of-service control of a presentation. In all cases, we describe a mechanism to externalize the behavior of hypermedia presentations containing resource-intensive data requirements so that effective control can be implemented by low-level system facilities based on application-specific requirements. We present our results in terms of the CMIFed authoring/presentation system.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {440–470},
numpages = {31},
keywords = {hypermedia documents, adaptive control, video presentation, embedded video, multimedia, synchronization}
}

@article{10.1145/211430.211433,
author = {Dimitrova, Nevenka and Golshani, Forouzan},
title = {Motion Recovery for Video Content Classification},
year = {1995},
issue_date = {Oct. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/211430.211433},
doi = {10.1145/211430.211433},
abstract = {Like other types of digital information, video sequences must be classified based on the semantics of their contents. A more-precise and completer extraction of semantic information will result in a more-effective classification. The most-discernible difference between still images and moving pictures stems from movements and variations. Thus, to go from the realm of still-image repositories to video databases, we must be able to deal with motion. Particularly, we need the ability to classify objects appearing in a video sequence based on their characteristics and features such as shape or color, as well as their movements. By describing the movements that we derive from the process of motion analysis, we introduce a dual hierarchy consisting of spatial and  temporal parts for video sequence representation. This gives us the flexibility to examine arbitrary sequences of frames at various levels of abstraction and to retrieve the associated temporal information (say, object trajectories) in addition to the spatial representation. Our algorithm for motion detection uses the motion compensation component of the MPEG video-encoding scheme and then computes trajectories for objects of interest. The specification of a language for retrieval of video based on the spatial as well as motion characteristics is presented.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {408–439},
numpages = {32},
keywords = {content-based retrieval of video, motion recovery, video databases, video retrieval, MPEG compressed video analysis}
}

@article{10.1145/211430.211431,
author = {Chua, Tat-Seng and Ruan, Li-Qun},
title = {A Video Retrieval and Sequencing System},
year = {1995},
issue_date = {Oct. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/211430.211431},
doi = {10.1145/211430.211431},
abstract = {Video is an effective medium for capturing the events in the real world around us, and a vast amount of video materials exists, covering a wide range of applications. However, widespread use of video in computer applications is often impeded by the lack of effective tools to manage video information systematically. This article discusses the design and implementation of a frame-based video retrieval and sequencing system (VRSS). The system is designed to support the entire process of video information management: segmenting, indexing, retrieving, and sequencing of video data. A semiautomatic tool is developed to divide video sequences into meaningful shots. Each video shot is logged using text descriptions, audio dialogue, and cinematic attributes. A two-layered, concept-based model is used as the basis for accurately retrieving relevant video shots based on users' free-text queries. A cinematic, rule-based, virtual editing tool is also developed to sequence the video shots retrieved for presentation within a specified time constraint. The system has been tested on a video documentary on the NUS (National University of Singapore) engineering faculty. The results of video retrieval experiments are encouraging.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {373–407},
numpages = {35},
keywords = {cinematic rules, frame-based modeling, multimedia, video retrieval, virtual editing}
}

@article{10.1145/203052.203074,
author = {Kong, Q. and Chen, G.},
title = {On Deductive Databases with Incomplete Information},
year = {1995},
issue_date = {July 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/203052.203074},
doi = {10.1145/203052.203074},
abstract = {In order to extend the ability to handle incomplete information in a definite deductive database, a Horn clause-based system representing incomplete information as incomplete constants is proposed. By using the notion of incomplete constants the deductive database system handles incomplete information in the form of sets of possible values, thereby giving more information than null values. The resulting system extends Horn logic to express a restricted form of indefiniteness. Although a deductive database with this kind of incomplete information is, in fact, a subset of an indefinite deductive database system, it represents indefiniteness in terms of value incompleteness, and therefore it can make use of the existing Horn logic computation rules. The inference rules for such a system are presented, its model theory discussed, and a model theory of indefiniteness proposed. The theory is consistent with minimal model theory and extends its expressive power.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {354–370},
numpages = {17},
keywords = {query evaluation, Horn clause, deductive databases, Prolog, incomplete information}
}

@article{10.1145/203052.203067,
author = {Kwok, K. L.},
title = {A Network Approach to Probabilistic Information Retrieval},
year = {1995},
issue_date = {July 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/203052.203067},
doi = {10.1145/203052.203067},
abstract = {In this article we show how probabilistic information retrieval based on document components may be implemented as a feedforward (feedbackward) artificial neural network. The network supports adaptation of connection weights as well as the growing of new edges between queries and terms based on user relevance feedback data for training, and it reflects query modification and expansion in information retrieval. A learning rule is applied that can also be viewed as supporting sequential learning using a harmonic sequence learning rate. Experimental results with four standard small collections and a large Wall Street Journal collection (173,219 documents) show that performance of feedback improves substantially over no feedback, and further gains are obtained when queries are expanded  with terms from the feedback documents. The effect is much more pronounced in small collections than in the large collection. Query expansion may be considered as a tool for both precision and recall enhancement. In particular, small query expansion levels of about 30 terms can achieve most of the gains at the low-recall high-precision region, while larger expansion levels continue to provide gains at the high-recall low-precision region of a precision recall curve.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {324–353},
numpages = {30},
keywords = {learning, probabilistic retrieval, training, probabilistic indexing, indexing and retrieval, document-focused and query-focused relevance feedback, query expansion, artificial neural networks, item self-learning}
}

@article{10.1145/203052.203065,
author = {Koike, Hideki},
title = {Fractal Views: A Fractal-Based Method for Controlling Information Display},
year = {1995},
issue_date = {July 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/203052.203065},
doi = {10.1145/203052.203065},
abstract = {Computer users often must view large amounts of information through video displays which are physically limited in size. Although some methods, which automatically display/erase information units based on their degrees of importance, have been proposed, they lack an ability to keep the total amount of displayed information nearly constant. We propose a new method for information display based on fractal theory. By regarding the information structures used in computers as complex objects, we can abstract these objects as well as control their amount. Using our method, (1) the total amount of information is kept nearly constant even when users change their focuses of attention and (2) this amount can be set flexibly. Through mathematical analysis, we show our method's ability to control the amount. An application to program display is also shown. When this method is applied to the display of structured programs, it provides fisheye-like views which integrate local details around the focal point and major landmarks further away.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {305–323},
numpages = {19},
keywords = {information visualization, program display, fractals, abstracting methods, UI theory}
}

@article{10.1145/203052.203061,
author = {Tuzhilin, Alexander},
title = {Templar: A Knowledge-Based Language for Software Specifications Using Temporal Logic},
year = {1995},
issue_date = {July 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/203052.203061},
doi = {10.1145/203052.203061},
abstract = {A software specification language Templar is defined in this article. The development of the language was guided by the following objectives: requirements specifications written in Templar should have a clear syntax and formal semantics, should be easy for a systems analyst to develop and for an end-user to understand, and it should be easy to map them into a broad range of design specifications. Templar is based on temporal logic and on the Activity-Event-Condition-Activity model of a rule which is an extension of the Event-Condition-Activity model in active databases. The language supports a rich set of modeling primitives, including rules, procedures, temporal logic operators, events, activities, hierarchical decomposition of activities, parallelism, and decisions combined together into a cohesive system.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {269–304},
numpages = {36},
keywords = {events, temporal logic, rule-based systems, time, activities, specification languages}
}

@article{10.1145/203052.203056,
author = {Celentano, Augusto and Fugini, Maria Grazia and Pozzi, Silvano},
title = {Knowledge-Based Document Retrieval in Office Environments: The Kabiria System},
year = {1995},
issue_date = {July 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/203052.203056},
doi = {10.1145/203052.203056},
abstract = {In the office environment, the retrieval of documents is performed using the concepts contained in the documents, information about the procedural context where the documents are used, and information about the regulations and laws that discipline the life of documents within a given application domain. To fulfill the requirements of such a sophisticated retrieval, we propose a document retrieval model and system based on the representation of knowledge describing the semantic contents of documents, the way in which the documents are managed by procedures and by people in the office, and the application domain where the office operates. The article describes the knowledge representation issues needed for the document retrieval system and presents a document retrieval model that  captures these issues. The effectiveness of the approach is illustrated by describing a system, named Kabiria, built on top of such model. The article describes the querying and browsing environments, and the architecture of the system.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {237–268},
numpages = {32},
keywords = {knowledge base, class, object orientation, instance, link, browser, user interface, hypertext}
}

@article{10.1145/201040.201049,
author = {Strong, Diane M. and Miller, Steven M.},
title = {Exceptions and Exception Handling in Computerized Information Processes},
year = {1995},
issue_date = {April 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/201040.201049},
doi = {10.1145/201040.201049},
abstract = {Exceptions, situations that cannot be correctly processed by computer systems, occur frequently in computer-based information processes. Five perspectives on exceptions provide insights into why exceptions occur and how they might be eliminated or more efficiently handled. We investigate these perspectives using an in-depth study of an operating information process that has frequent exceptions. Our results support the use of a total quality management (TQM) approach of eliminating exceptions for some exceptions, in particular, those caused by computer systems that are poor matches to organizational processes. However, some exceptions are explained better by a political system perspective of conflicting goals between subunits. For these exceptions and several other types, designing an integrated human-computer process will provide better performance than will eliminating exceptions and moving toward an entirely automated process.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {206–233},
numpages = {28},
keywords = {process design, exceptions, Total Quality Management, exception handling}
}

@article{10.1145/201040.201047,
author = {Malone, Thomas W. and Lai, Kum-Yew and Fry, Christopher},
title = {Experiments with Oval: A Radically Tailorable Tool for Cooperative Work},
year = {1995},
issue_date = {April 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/201040.201047},
doi = {10.1145/201040.201047},
abstract = {This article describes a series of tests of the generality of a “radically tailorable” tool for cooperative work. Users of this system can create applications by combining and modifying four kinds of building blocks: objects, views, agents, and links. We found that user-level tailoring of these primitives can provide most of the functionality found in well-known cooperative work systems such as gIBIS, Coordinator, Lotus Notes, and Information Lens. These primitives, therefore, appear to provide an elementary “tailoring language” out of which a wide variety of integrated information management and collaboration applications can be constructed by end users.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {177–205},
numpages = {29},
keywords = {groupware, radical tailorability, computer-supported cooperative work, end-user programming}
}

@article{10.1145/201040.201044,
author = {Rangan, P. Venkat and Ramanathan, Srinivas and Sampathkumar, Srihari},
title = {Feedback Techniques for Continuity and Synchronization in Multimedia Information Retrieval},
year = {1995},
issue_date = {April 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/201040.201044},
doi = {10.1145/201040.201044},
abstract = {Future advances in storage and networking technologies will make it feasible to build multimedia on-demand information servers capable of providing services similar to those of a neighborhood videotape rental store over metropolitan area networks. Such multimedia information servers must not only support retrieval of continuous media units (such as video frames and audio samples), but also preserve synchrony among playback of the different media components constituting a multimedia object. We develop techniques for supporting continuous and synchronous retrieval from multimedia servers. We present feedback techniques by which, during retrieval of multimedia objects from a multimedia server to mediaphones, the multimedia server uses lightweight messages called feedback units transmitted periodically back to it (by mediaphones) to detect impending discontinuities as well as asynchronies at mediaphones. The multimedia server then preventively readjusts media transmission so as to avoid either anomaly, and steers the mediaphones back to synchrony. Given the available buffer sizes at mediaphones and the maximum tolerable asynchrony, we present methods to determine the minimum rate at which feedback units must be transmitted so as to maintain both continuity and synchronization. These feedback techniques remain robust even in the presence of playback rate mismatches and network delay jitter, and their initial simulation for video-audio playback yields a feedback rate of one per 1,000 media units to keep the asynchrony within 250ms, showing that the overhead due to feedback transmission is very small. The constant rate feedback techniques developed in this article form the basis of a prototype on-demand information server being developed at the UCSD Multimedia Laboratory.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {145–176},
numpages = {32},
keywords = {intermedia synchronization, multimedia, synchronization, intramedia continuity, multimedia on-demand information services}
}

@article{10.1145/201040.201041,
author = {Gudivada, Venkat N. and Raghavan, Vijay V.},
title = {Design and Evaluation of Algorithms for Image Retrieval by Spatial Similarity},
year = {1995},
issue_date = {April 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/201040.201041},
doi = {10.1145/201040.201041},
abstract = {Similarity-based retrieval of images is an important task in many image database applications. A major class of users' requests requires retrieving those images in the database that are spatially similar to the query image. We propose an algorithm for computing the spatial similarity between two symbolic images. A symbolic image is a logical representation of the original image where the image objects are uniquely labeled with symbolic names. Spatial relationships in a symbolic image are represented as edges in a weighted graph referred to as spatial-orientation graph. Spatial similarity is then quantified in terms of the number of, as well as the extent to which, the edges of the spatial-orientation graph of the database image conform to the corresponding edges of the spatial-orientation graph of the query image.The proposed algorithm is robust in the sense that it can deal with translation, scale, and rotational variances in images. The algorithm has quadratic time complexity in terms of the total number of objects in both the database and query images. We also introduce the idea of quantifying a system's retrieval quality by having an expert specify the expected rank ordering with respect to each query for a set of test queries. This enables us to assess the quality of algorithms comprehensively for retrieval in image databases. The characteristics of the proposed algorithm are compared with those of the previously available algorithms using a testbed of images. The comparison demonstrated that our algorithm is not only more efficient but also provides a rank ordering of images that consistently matches with the expert's expected rank ordering.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {115–144},
numpages = {30},
keywords = {image databases, image retrieval systems, image retrieval, spatial similarity, rotational invariance}
}

@article{10.1145/195705.195735,
author = {Cooper, William S.},
title = {Some Inconsistencies and Misidentified Modeling Assumptions in Probabilistic Information Retrieval},
year = {1995},
issue_date = {Jan. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/195705.195735},
doi = {10.1145/195705.195735},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {100–111},
numpages = {12},
keywords = {independence, document retrieval, modeling, assumptions, logic, consistency, bibliographic searching}
}

@article{10.1145/195705.195717,
author = {Salminen, Airi and Tague-Sutcliffe, Jean and McClellan, Charles},
title = {From Text to Hypertext by Indexing},
year = {1995},
issue_date = {Jan. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/195705.195717},
doi = {10.1145/195705.195717},
abstract = {A model is presented for converting a collection of documents to hypertext by means of indexing. The documents are assumed to be semistructured, i.e., their text is a hierarchy of parts, and some of the parts consist of natural language. The model is intended as a framework for specifying hypertextual reading capabilities for specific application areas and for developing new automated tools for the conversion of semistructured text to hypertext. In the model, two well-known paradigms—formal grammars and document indexing—are combined.The structure of the source text is defined by a schema that is a constrained context-free grammar. The hierarchic structure of the source may thus be modeled by a parse tree for the grammar. The effect of indexing is described by  grammar transformations. The new grammar, called an indexing schema, is associated with a new parse tree where some text parts are index elements. The indexing schema may hide some parts of the original documents or the structure of some parts. For information retrieval, parts of the indexed text are considered to be nodes of a hypergraph. In the hypergraph-based information access, the navigation capabilities of the hypertext systems are combined with the querying capabilities of information retrieval systems.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {69–99},
numpages = {31},
keywords = {text entities, hypertext, test types, grammars, structured text, constrained grammars, transient hypergraphs, properties}
}

@article{10.1145/195705.195713,
author = {Wong, S. K. M. and Yao, Y. Y.},
title = {On Modeling Information Retrieval with Probabilistic Inference},
year = {1995},
issue_date = {Jan. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/195705.195713},
doi = {10.1145/195705.195713},
abstract = {This article examines and extends the logical models of information retrieval in the context of probability theory. The fundamental notions of term weights and relevance are given probabilistic interpretations. A unified framework is developed for modeling the retrieval process with probabilistic inference. This new approach provides a common conceptual and mathematical basis for many retrieval models, such as the Boolean, fuzzy set, vector space, and conventional probabilistic models. Within this framework, the underlying assumptions employed by each model are identified, and the inherent relationships between these models are analyzed. Although this article is mainly a theoretical analysis of probabilistic inference for information retrieval, practical methods for estimating the required probabilities are provided by simple examples.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {38–68},
numpages = {31}
}

@article{10.1145/195705.195708,
author = {Isakowitz, Tom\'{a}s and Schocken, Shimon and Lucas, Henry C.},
title = {Toward a Logical/Physical Theory of Spreadsheet Modeling},
year = {1995},
issue_date = {Jan. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/195705.195708},
doi = {10.1145/195705.195708},
abstract = {In spite of the increasing sophistication and power of commercial spreadsheet packages, we still lack a formal theory or a methodology to support the construction and maintenance of spreadsheet models. Using a dual logical/physical perspective, we identify four principal components that characterize any spread sheet model: schema, data, editorial, and binding. We present a factoring algorithm for identifying and extracting these components from conventional spreadsheets with minimal user intervention, and a synthesis algorithm that assists users in the construction of executable spreadsheets from reusable model components. This approach opens new possibilities for applying object-oriented and model management  techniques to support the construction, sharing, and reuse of spreadsheet models in organizations. Importantly, our approach to model management and the Windows-based prototype that we have developed are designed to coexist with, rather than replace, traditional spreadsheet programs. In other words, the users are not required to learn a new modeling language; instead, their logical models and data sets are extracted from their spreadsheets transparently, as a side-effect of using standard spreadsheet programs.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–37},
numpages = {37},
keywords = {model management}
}

@article{10.1145/185462.185484,
author = {Wong, Stephen T. C.},
title = {Preference-Based Decision Making for Cooperative Knowledge-Based Systems},
year = {1994},
issue_date = {Oct. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/185462.185484},
doi = {10.1145/185462.185484},
abstract = {Recent advances in cooperative knowledge-based systems (CKBS) offer significant promise for intelligent interaction between multiple AI systems for solving larger, more complex problems. In this paper, we propose a logical, qualitative problem-solving scheme for CKBS that uses social choice theory as a formal basis for making joint decisions and promoting conflict resolution. This scheme consists of three steps: (1) the selection of decision criteria and competing alternatives, (2) the formation of preference profiles and collective choices, and (3) the negotiation among agents as conflicts arise in group decision making. In this paper, we focus on the computational mechanisms developed to support steps (2) and (3) of the scheme. In addition, the practicality of the scheme is illustrated with examples taken from a working prototype dealing with collaborative structural design of buildings.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {407–435},
numpages = {29},
keywords = {social choice theory, decision making, cooperative problem solving, cooperative knowledge-based systems}
}

@article{10.1145/185462.185483,
author = {Chimera, Richard and Shneiderman, Ben},
title = {An Exploratory Evaluation of Three Interfaces for Browsing Large Hierarchical Tables of Contents},
year = {1994},
issue_date = {Oct. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/185462.185483},
doi = {10.1145/185462.185483},
abstract = {Three different interfaces were used to browse a large (1296 items) table of contents. A fully expanded stable interface, expand/contract interface, and multipane interface were studied in a between-groups experiment with 41 novice participants. Nine timed fact retrieval tasks were performed; each task is analyzed and discussed separately. We found that both the expand/contract and multipane interfaces produced significantly faster times than the stable interface for many tasks using this large hierarchy; other advantages of the expand/contract and multipane interfaces over the stable interface are discussed. The animation characteristics of the expand/contract interface appear to play a major role. Refinements to the multipane and expand/contract interfaces are suggested. A predictive model for measuring navigation effort of each interface is presented.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {383–406},
numpages = {24},
keywords = {table of contents, hierarchies, browsing, user interfaces}
}

@article{10.1145/185462.185477,
author = {Chang, Man Kit and Woo, Carson C.},
title = {A Speech-Act-Based Negotiation Protocol: Design, Implementation, and Test Use},
year = {1994},
issue_date = {Oct. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/185462.185477},
doi = {10.1145/185462.185477},
abstract = {Existing negotiation protocols used in Distributed Artificial Intelligence (DAI) systems rarely take into account the results from negotiation research. We propose a negotiation protocol, SANP (Speech-Act-based Negotiation Protocol), which is based on Ballmer and Brennenstuhl's speech act classification and on negotiation analysis literature. The protocol is implemented as a domain-independent system using Strudel, which is an electronic mail toolkit. A small study tested the potential use of the protocol. Although a number of limitations were found in the study, the protocol appears to have potential in domains without these limitations, and it can serve as a building block to design more general negotiation protocols.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {360–382},
numpages = {23},
keywords = {speech act theory, negotiation, organizational computing systems}
}

@article{10.1145/185462.185472,
author = {Merz, Ulla and King, Roger},
title = {DIRECT: A Query Facility for Multiple Databases},
year = {1994},
issue_date = {Oct. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/185462.185472},
doi = {10.1145/185462.185472},
abstract = {The subject of this research project is the architecture and design of a multidatabase query facility. These databases contain structured data, typical for business applications. Problems addressed are: presenting a uniform interface for retrieving data from multiple databases, providing autonomy for the component databases, and defining an architecture for semantic services.DIRECT is a query facility for heterogeneous databases. The databases and their definitions can differ in their data models, names, types, and encoded values. Instead of creating a global schema, descriptions of different databases are allowed to coexist. A multidatabase query language provides a uniform interface for retrieving data from different databases. DIRECT has been exercised with operational  databases that are part of an automated business system.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {339–359},
numpages = {21},
keywords = {query languages, heterogeneous databases, data models}
}

@article{10.1145/183422.183428,
author = {Riloff, Ellen and Lehnert, Wendy},
title = {Information Extraction as a Basis for High-Precision Text Classification},
year = {1994},
issue_date = {July 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/183422.183428},
doi = {10.1145/183422.183428},
abstract = {We describe an approach to text classification that represents a compromise between traditional word-based techniques and in-depth natural language processing. Our approach uses a natural language processing task called “information extraction” as a basis for high-precision text classification. We present three algorithms that use varying amounts of extracted information to classify texts. The relevancy signatures algorithm uses linguistic phrases; the augmented relevancy signatures algorithm uses phrases and local context; and the case-based text classification algorithm uses larger pieces of context. Relevant phrases and contexts are acquired automatically using a training corpus. We evaluate the algorithms on the basis  of two test sets from the MUC-4 corpus. All three algorithms achieved high precision on both test sets, with the augmented relevancy signatures algorithm and the case-based algorithm reaching 100% precision with over 60% recall on one set. Additionally, we compare the algorithms on a larger collection of 1700 texts and describe an automated method for empirically deriving appropriate threshold values. The results suggest that information extraction techniques can support high-precision text classification and, in general, that using more extracted information improves performance. As a practical matter, we also explain how the text classification system can be easily ported across  domains.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {296–333},
numpages = {38},
keywords = {information extraction, text classification}
}

@article{10.1145/183422.183425,
author = {Liddy, Elizabeth D. and Paik, Woojin and Yu, Edmund S.},
title = {Text Categorization for Multiple Users Based on Semantic Features from a Machine-Readable Dictionary},
year = {1994},
issue_date = {July 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/183422.183425},
doi = {10.1145/183422.183425},
abstract = {The text categorization module described here provides a front-end filtering function for the larger DR-LINK text retrieval system [Liddy and Myaeing 1993]. The model evaluates a large incoming stream of documents to determine which documents are sufficiently similar to a profile at the broad subject level to warrant more refined representation and matching. To accomplish this task, each substantive word in a text is first categorized using a feature set based on the semantic Subject Field Codes (SFCs) assigned to individual word senses in a machine-readable dictionary. When tested on 50 user profiles and 550 megabytes of documents, results indicate that the feature set that is the basis of the text categorization module and the algorithm that establishes the boundary of categories  of potentially relevant documents accomplish their tasks with a high level of performance.This means that the category of potentially relevant documents for most profiles would contain at least 80% of all documents later determined to be relevant to the profile. The number of documents in this set would be uniquely determined by the system's category-boundary predictor, and this set is likely to contain less than 5% of the incoming stream of documents.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {278–295},
numpages = {18},
keywords = {subject field coding, semantic vectors}
}

@article{10.1145/183422.183424,
author = {Yang, Yiming and Chute, Christopher G.},
title = {An Example-Based Mapping Method for Text Categorization and Retrieval},
year = {1994},
issue_date = {July 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/183422.183424},
doi = {10.1145/183422.183424},
abstract = {A unified model for text categorization and text retrieval is introduced. We use a training set of manually categorized documents to learn word-category associations, and use these associations to predict the categories of arbitrary documents. Similarly, we use a training set of queries and their related documents to obtain empirical associations between query words and indexing terms of documents, and use these associations to predict the related documents of arbitrary queries. A Linear Least Squares Fit (LLSF) technique is employed to estimate the likelihood of these associations. Document collections from the MEDLINE database and Mayo patient records are used for studies on the effectiveness of our approach, and on how much the effectiveness depends on the choices of training  data, indexing language, word-weighting scheme, and morphological canonicalization. Alternative methods are also tested on these data collections for comparison. It is evident that the LLSF approach uses the relevance information effectively within human decisions of categorization and retrieval, and achieves a semantic mapping of free texts to their representations in an indexing language. Such a semantic mapping lead to a significant improvement in categorization and retrieval, compared to alternative approaches.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {252–277},
numpages = {26},
keywords = {document categorization, statistical learning of human decisions, query categorization}
}

@article{10.1145/183422.183423,
author = {Apt\'{e}, Chidanand and Damerau, Fred and Weiss, Sholom M.},
title = {Automated Learning of Decision Rules for Text Categorization},
year = {1994},
issue_date = {July 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/183422.183423},
doi = {10.1145/183422.183423},
abstract = {We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to “read” documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67% recall/precision breakeven point to 80.5%. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {233–251},
numpages = {19}
}

@article{10.1145/196734.196746,
author = {Ruhleder, Karen},
title = {Rich and Lean Representations of Information for Knowledge Work: The Role of Computing Packages in the Work of Classical Scholars},
year = {1994},
issue_date = {April 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/196734.196746},
doi = {10.1145/196734.196746},
abstract = {Applying information systems to complex intellectual tasks requires the representation and codification of ambiguous and fragmentary forms of data. This application effects changes not only in representation of this data, but in the relationships between users and tools, techniques, or systems for data interpretation. It also affects the complex infrastructures that support this process. This article uses a package metaphor to examine the impact on one domain of knowledge work, classical scholarship, of the “computerization” of a key data source, the textual edition. The construction of one on-line textual databank, the Thesaurus Linguae Graecae (TLG), has altered the traditional relationships between text “owners” and “users” has changed the role of the text as a conduit for social and historical information, and has disrupted traditional patterns of transmitting domain expertise. A rich information resource has become lean in its electronic form.The TLG has standardized the corpus of Greek literature and eased access to a broad range of works, including rare and out-of-print materials. At the same time, its construction has decoupled often-contested textual sources from their accompanying critical notes and supplemental materials. The use of the TLG has also shifted notions of objectivity, accuracy, and requisite expertise within the community. The transmission of domain knowledge must now be coupled with the transmission of technical knowledge, a process for which no infrastructure is currently in place. These experiences parallel those of other knowledge workers. “Mechanistic” paradigms of information and knowledge cannot accommodate important components of computing packages, including the transmission of expertise and infrastructures for tool development and evaluation. Recent developments in information storage and dissemination, including gophers and ftp sites may indicate that despite technical advances that could be used to support rich representations (such as hypermedia and multimedia), leaner forms of data may prevail.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {208–230},
numpages = {23},
keywords = {locus of expertise, computing packages, decontextualization of information, computerization of knowledge work, information representations}
}

@article{10.1145/196734.196745,
author = {Orlikowski, Wanda J. and Gash, Debra C.},
title = {Technological Frames: Making Sense of Information Technology in Organizations},
year = {1994},
issue_date = {April 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/196734.196745},
doi = {10.1145/196734.196745},
abstract = {In this article, we build on and extend research into the cognitions and values of users and designers by proposing a systematic approach for examining the underlying assumptions, expectations, and knowledge that people have about technology. Such interpretations of technology (which we call technological frames) are central to understanding technological development, use, and change in organizations. We suggest that where the technological frames of key groups in organizations—such as managers, technologists, and users— are significantly different, difficulties and conflict around the development, use, and change of technology may result. We use the findings of an empirical study to illustrate how the nature, value, and use of a groupware technology were interpreted by  various organizational stakeholders, resulting in outcomes that deviated from those expected. We argue that technological frames offer an interesting and useful analytic perspective for explaining an anticipating actions and meanings that are not easily obtained with other theoretical lenses.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {174–207},
numpages = {34},
keywords = {technological implementation, social cognitions, technological frames, managing expectations, technology use}
}

@article{10.1145/196734.196744,
author = {Walsham, G. and Waema, T.},
title = {Information Systems Strategy and Implementation: A Case Study of a Building Society},
year = {1994},
issue_date = {April 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/196734.196744},
doi = {10.1145/196734.196744},
abstract = {The formation and implementation of strategy with respect to computer-based information systems (IS) are important issues in many contemporary organizations, including those in the financial services sector. This paper describes and analyzes an in-depth case study of the strategy formation and implementation process in one such organization, a medium-sized UK building society, and relates the process to its organizational and broader contexts; the organization is examined over a period of several years and under the contrasting leadership of two different chief executives. The case study is used to develop some general implications on IS strategy and implementation, which can be taken as themes for debate in any new situation. The paper provides an example of a more detailed  perspective on processes in IS strategy and implementation than typically available in the literature. In addition, a new framework for further research in this area is developed, which directs the researcher toward exploring the dynamic interplay of strategic content, multilevel contexts, and cultural and political perspectives on the process of change.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {150–173},
numpages = {24},
keywords = {culture, strategy, politics, change process, multilevel context, implementation}
}

@article{10.1145/196734.196738,
author = {Markus, M. L.},
title = {Finding a Happy Medium: Explaining the Negative Effects of Electronic Communication on Social Life at Work},
year = {1994},
issue_date = {April 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/196734.196738},
doi = {10.1145/196734.196738},
abstract = {The sometimes observed negative social effects of electronic communication technology are often attributed to the characteristics of the technology itself. Electronic mail, for instance, filters out personal and social cues and provides new capabilities not found in traditional media, and it has been argued that these factors have consequences such as “flaming” and depersonalization. Alternative theoretical perspectives on the impacts of information technology suggest that our ability to explain these outcomes might be enhanced by attending to users' intentional choices about how to use technology and to unpredictable technology usage patterns that emerge when users interact with the technology and each other. These alternative perspectives are examined in the context of  an exploratory case study of a complex organization in which electronic mail was heavily used.Users were found to select email deliberately when they wished to avoid unwanted social interactions. At the same time, they actively took steps to avoid negative outcomes, such as depersonalization of their relationships with subordinates. However, despite their well-intentioned efforts, some negative social effects did occur that cannot entirely be attributed to the technological characteristics of electronic communication. Instead, they appear to be ironic side effects of users' thoughtful efforts to use email effectively. These results suggest the value of according a prominent role in explanations of technology impacts to users' intended and unintended technology uses. The  results also imply that negative social effects from using electronic communication technology may not prove easy to eradicate, despite technological developments such as multimedia integration, and despite efforts to train users in the best email “etiquette.”},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {119–149},
numpages = {31},
keywords = {politics, depersonalization, electronic mail, connectedness, social distance, etiquette}
}

@article{10.1145/174608.174612,
author = {Fuhr, Norbert and Pfeifer, Ulrich},
title = {Probabilistic Information Retrieval as a Combination of Abstraction, Inductive Learning, and Probabilistic Assumptions},
year = {1994},
issue_date = {Jan. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/174608.174612},
doi = {10.1145/174608.174612},
abstract = {We show that former approaches in probabilistic information retrieval are based on one or two of the three concepts abstraction, inductive learning, and probabilistic assumptions, and we propose a new approach which combines all three concepts. This approach is illustrated for the case of indexing with a controlled vocabulary. For this purpose, we describe a new probabilistic model first, which is then combined with logistic regression, thus yielding a generalization of the original model. Experimental results for the pure theoretical model as well as for heuristic variants are given. Furthermore, linear and logistic regression are compared.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {92–115},
numpages = {24},
keywords = {probabilistic indexing, probabilistic retrieval, controlled vocabulary, logistic regression}
}

@article{10.1145/174608.174611,
author = {Sch\"{a}uble, Peter and W\"{u}thrich, Beat},
title = {On the Expressive Power of Query Languages},
year = {1994},
issue_date = {Jan. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/174608.174611},
doi = {10.1145/174608.174611},
abstract = {Two main topics are addressed. First, an algebraic approach is presented to define a general notion of expressive power. Heterogeneous algebras represent information systems and morphisms represent the correspondences between the instances of databases, the correspondences between answers, and the correspondences between queries. An important feature of this new notion of expressive power is that query languages of different types can be compared with respect to their expressive power. In the case of relational query languages, the new notion of expressive power is shown to be equivalent to the notion used by Chandra and Harel. In the case of nonrelational query languages, the versatility of the new notion of expressive power is demonstrated by comparing the fixpoint query languages  with an object-oriented query language called FQL. The expressive power of the Functional Query Language FQL is the second main topic of this paper. The specifications of FQL functions can be recursive or even mutually recursive, FQL has a fixpoint semantics based on a complete lattice consisting of bag functions. The query language FQL is shown to be more expressive than the fixpoint query languages. This result implies that FQL is also more expressive than Datalog with stratified negation. Examples of recursive FQL functions are given that determine the ancestors of persons and the bill of materials.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {69–91},
numpages = {23},
keywords = {functional query languages, fixpoint query languages, expressive power of query languages, datalog, relational query languages}
}

@article{10.1145/174608.174610,
author = {Poulovassilis, Alexandra and Levene, Mark},
title = {A Nested-Graph Model for the Representation and Manipulation of Complex Objects},
year = {1994},
issue_date = {Jan. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/174608.174610},
doi = {10.1145/174608.174610},
abstract = {Three recent trends in database research are object-oriented and deductive databases and graph-based user interfaces. We draw these trends together in a data model we call the Hypernode Model. The single data structure of this model is the hypernode, a graph whose nodes can themselves be graphs. Hypernodes are typed, and types, too, are nested graphs. We give the theoretical foundations of hypernodes and types, and we show that type checking is tractable. We show also how conventional type-forming operators can be simulated by our graph types, including cyclic types. The Hypernode Model comes equipped with a rule-based query language called Hyperlog, which is complete with respect to computation and update. We define the operational semantics of Hyperlog and show  that the evaluation can be performed efficiently. We discuss also the use of Hyperlog for supporting database browsing, an essential feature of Hypertext databases. We compare our work with other graph-based data models—unlike previous graph-based models, the Hypernode Model provides inherent support for data abstraction via its nesting of graphs. Finally, we briefly discuss the implementation of a DBMS based on the Hypernode Model.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {35–68},
numpages = {34},
keywords = {nested graph, types, rule-based query and update language, complex object, object store}
}

@article{10.1145/174608.174609,
author = {Marchionini, Gary and Crane, Gregory},
title = {Evaluating Hypermedia and Learning: Methods and Results from the Perseus Project},
year = {1994},
issue_date = {Jan. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/174608.174609},
doi = {10.1145/174608.174609},
abstract = {The Perseus Project has developed a hypermedia corpus of materials related to the ancient Greek world. The materials include a variety of texts and images, and tools for using these materials and navigating the sytem. Results from a three-year evaluation of Perseus use in a variety of college settings are described. The evaluation assessed both this particular system and the application of the technological genre to information management and to learning. The evaluation used a variety of methods to address questions about learning and teaching with hypermedia and to guide the development of early versions of the system. Results illustrate that such environments offer potential for accelerating learning and for supporting new types of learning and teaching; that students and instructors must develop new strategies for learning and teaching with such technology; and that institutions must develop infrastructural support for such technology. The results also illustrate the importance of well-designed interfaces and different types of assignments on user performance.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {5–34},
numpages = {30},
keywords = {hypermedia, teaching, human-computer interaction, learning}
}

@article{10.1145/159764.159763,
author = {Olson, Judith S. and Olson, Gary M. and Storr\o{}sten, Marianne and Carter, Mark},
title = {Groupwork Close up: A Comparison of the Group Design Process with and without a Simple Group Editor},
year = {1993},
issue_date = {Oct. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/159764.159763},
doi = {10.1145/159764.159763},
abstract = {A simple collaborative tool, a shared text editor called ShrEdit, changed the way groups of designers performed their work, and changed it for the better. First, the designs produced by the 19 groups of three designers were of higher quality than those of the 19 groups who worked with conventional whiteboard, paper and pencil. The groups with the new tool reported liking their work process a little less, probably because they had to adapt their work style to a new tool. We expected, from the brainstorming literature and recent work on Group Support Systems, that the reason the designs were of better quality was that the supported groups generated more ideas. To our surprise, the groups working with ShrEdit generated fewer design ideas, but apparently   better ones. It appears that the tool helped the supported groups keep more focused on the core issued in the emerging design, to waste less time on less important topics, and to capture what was said as they went. This suggests that small workgroups can capitalize on the free access they have to a shared workspace, without requiring a facilitator or a work process embedded in the software.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {321–348},
numpages = {28},
keywords = {concurrent editing, small group behavior, groupware, group support system, face-to-face work, collaboration}
}

@article{10.1145/159764.159762,
author = {Ishii, Hiroshi and Kobayashi, Minoru and Grudin, Jonathan},
title = {Integration of Interpersonal Space and Shared Workspace: ClearBoard Design and Experiments},
year = {1993},
issue_date = {Oct. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/159764.159762},
doi = {10.1145/159764.159762},
abstract = {We describe the evolution of the novel shared drawing medium ClearBoard which was designed to seamlessly integrate an interpersonal space and a shared workspace. ClearBoard permits coworkers in two locations to draw with color markers or with electronic pens and software tools while maintaining direct eye contact and the ability to employ natural gestures. The ClearBoard design is based on the key metaphor of “talking through and drawing on a transparent glass window.” We describe the evolution from ClearBoard-1 (which enables shared video drawing) to ClearBoard-2 (which incorporates TeamPaint, a multiuser paint editor). Initial observations and findings gained through the experimental use of the prototype, including the feature of “gaze awareness,” are  discussed. Further experiments are conducted with ClearBoard-0 (a simple mockup), ClearBoard-1, and an actual desktop as a control. In the settings we examined, the ClearBoard environment led to more eye contact and potential awareness of collaborator's gaze direction over the traditional desktop environment.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {349–375},
numpages = {27},
keywords = {video conference, eye contact, groupware, gaze direction, shared drawing, seamless design, gaze awareness}
}

@article{10.1145/159764.159761,
author = {Hindus, Debby and Schmandt, Chris and Horner, Chris},
title = {Capturing, Structuring, and Representing Ubiquitous Audio},
year = {1993},
issue_date = {Oct. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/159764.159761},
doi = {10.1145/159764.159761},
abstract = {Although talking is an integral part of collaboration, there has been little computer support for acquiring and accessing the contents of conversations. Our approach has focused on ubiquitous audio, or the unobtrusive capture of speech interactions in everyday work environments. Speech recognition technology cannot yet transcribe fluent conversational speech, so the words themselves are not available for organizing the captured interactions. Instead, the structure of an interaction is derived from acoustical information inherent in the stored speech and augmented by user interaction during or after capture. This article describes applications for capturing and structuring audio from office discussions and telephone calls, and mechanisms for later retrieval of these   stored interactions. An important aspect of retrieval is choosing an appropriate visual representation, and this article describes the evolution of a family of representations across a range of applications. Finally, this work is placed within the broader context of desktop audio, mobile audio applications, and social implications.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {376–400},
numpages = {25},
keywords = {collaborative work, ubiquitous computing, multimedia workstation software, stored speech, audio interactions, semi-structured data, software telephony}
}

@article{10.1145/159764.159760,
author = {Resnick, Paul},
title = {Phone-Based CSCW: Tools and Trials},
year = {1993},
issue_date = {Oct. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/159764.159760},
doi = {10.1145/159764.159760},
abstract = {Telephones are the most ubiquitous, best-networked, and simplest computer terminals available today. They have been used for voice mail but largely overlooked as a platform for asynchronous cooperative-work applications such as event calendars, issue discussions, and question-and-answer gathering. HyperVoice is a software toolkit for constructing such applications. Its building blocks are high-level presentation formats for collections of structured voice messages. The presentation formats can themselves be presented and manipulated, enabling significant customization of applications by phone. Results of two field trials suggest social-context factors that will influence the success or failure of phone-based cooperative work applications in particular settings.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {401–424},
numpages = {24},
keywords = {telephone bulletin board, application generator, semi-structured messages, interactive voice response, phone-based interface, cooperative work, voice mail, groupware}
}

@article{10.1145/159161.173948,
author = {Shaw, Chris and Green, Mark and Liang, Jiandong and Sun, Yunqi},
title = {Decoupled Simulation in Virtual Reality with the MR Toolkit},
year = {1993},
issue_date = {July 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/159161.173948},
doi = {10.1145/159161.173948},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {287–317},
numpages = {31},
keywords = {interaactive 3D graphics, user interface software}
}

@article{10.1145/159161.159160,
author = {Fitzmaurice, George W. and Zhai, Shumin and Chignell, Mark H.},
title = {Virtual Reality for Palmtop Computers},
year = {1993},
issue_date = {July 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/159161.159160},
doi = {10.1145/159161.159160},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {197–218},
numpages = {22},
keywords = {3D control and display, palmtop computers, virtual reality}
}

@article{10.1145/159161.159159,
author = {Sturman, David J. and Zeltzer, David},
title = {A Design Method for “Whole-Hand” Human-Computer Interaction},
year = {1993},
issue_date = {July 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/159161.159159},
doi = {10.1145/159161.159159},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {219–238},
numpages = {20},
keywords = {input devices, interaction techniques, interface design, user interface, virtual environments}
}

@article{10.1145/159161.155370,
author = {Koike, Hideki},
title = {The Role of Another Spatial Dimension in Software Visualization},
year = {1993},
issue_date = {July 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/159161.155370},
doi = {10.1145/159161.155370},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {266–286},
numpages = {21},
keywords = {information visualization, parallel manipulator, electric power control system}
}

@article{10.1145/159161.155359,
author = {Arthur, Kevin W. and Booth, Kellogg S. and Ware, Colin},
title = {Evaluating 3D Task Performance for Fish Tank Virtual Worlds},
year = {1993},
issue_date = {July 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/159161.155359},
doi = {10.1145/159161.155359},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {239–265},
numpages = {27},
keywords = {virtual worlds, stereopsis, head-coupled display, virtual reality}
}

@article{10.1145/130226.148055,
author = {Bansler, Jorgen P. and B\o{}dker, Keld},
title = {A Reappraisal of Structured Analysis: Design in an Organizational Context},
year = {1993},
issue_date = {April 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/130226.148055},
doi = {10.1145/130226.148055},
abstract = {We review Structured Analysis as presented by Yourdon and DeMarco. First, we examine the implicit assumptions embodied in the method about the nature of organizations, work processes, and design. Following this we present the results of an exploratory study, conducted to find out how the method is applied in practice. This study reveals that while some of the tools of Structured Analysis—notably the data flow diagrams—are used and combined with other tools, the designers do not follow the analysis and design procedures prescribed by the method. Our findings suggest that there is a gap between the way systems development is portrayed in the normative technical literature and the way in which it is carried out.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {165–193},
numpages = {29},
keywords = {design process, structured analysis, qualitative empirical studies}
}

@article{10.1145/130226.145014,
author = {Ciaccia, Paulo and Zezula, Pavel},
title = {Estimating Accesses in Partitioned Signature File Organizations},
year = {1993},
issue_date = {April 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/130226.145014},
doi = {10.1145/130226.145014},
abstract = {We show that performance of some basic methods for the partitioning of signature files, namely Quick Filter and Fixed Prefix, can be easily evaluated by means of a closed formula. The approximation is based on well-known results from probability theory, and, as shown by simulations, introduces no appreciable errors when compared with the exact, cumbersome formulas used so far. Furthermore, we prove that the exact formulas for the two methods coincide. Although this does not imply that the two methods behave in the same way, it sheds light on the way they could be compared.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {133–142},
numpages = {10},
keywords = {access method, performance evaluation, superimposed coding, information retrieval}
}

@article{10.1145/130226.134481,
author = {King, Roger and Novak, Michael},
title = {Designing Database Interfaces with DBface},
year = {1993},
issue_date = {April 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/130226.134481},
doi = {10.1145/130226.134481},
abstract = {DBface is a toolkit for designing interfaces to object-oriented databases. It provides users with a set of tools for building custom interfaces with minimal programming. This is accomplished by combining techniques from User Interface Management Systems (UIMS) with a built-in knowledge about the specific kinds of techniques used by object-oriented databases. DBface allows users to create graphical constructs and interactive techniques by taking advantage of an object-oriented database environment and tools. Not only can database tools be used for creating an interface, but information about the interface being built is stored within a database schema and is syntactically consistent with all other schema information. Thus, an interface can deal with data and schema information, including information about another interface. This allows for easy reusability of graphical constructs such as data representations.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {105–132},
numpages = {28},
keywords = {object-oriented databases, graphical interfaces, user interface management systems}
}

@article{10.1145/130226.134466,
author = {Can, Fazli},
title = {Incremental Clustering for Dynamic Information Processing},
year = {1993},
issue_date = {April 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/130226.134466},
doi = {10.1145/130226.134466},
abstract = {Clustering of very large document databases is useful for both searching and browsing. The periodic updating of clusters is required due to the dynamic nature of databases. An algorithm for incremental clustering is introduced. The complexity and cost analysis of the algorithm together with an investigation of its expected behavior are presented. Through empirical testing it is shown that the algorithm achieves cost effectiveness and generates statistically valid clusters that are compatible with those of reclustering. The experimental evidence shows that the algorithm creates an effective and efficient retrieval environment.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {143–164},
numpages = {22},
keywords = {information retrieval, best-match cluster search, dynamic information retrieval environment, cover coefficient, information retrieval effectiveness, information retrieval efficiency, cluster validity}
}

@article{10.1145/151480.151521,
author = {Schnase, John L. and Leggett, John J. and Hicks, David L. and Szabo, Ron L.},
title = {Semantic Data Modeling of Hypermedia Associations},
year = {1993},
issue_date = {Jan. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/151480.151521},
doi = {10.1145/151480.151521},
abstract = {Many important issues in the design and implementation of hypermedia system functionality focus on the way interobject connections are represented, manipulated, and stored. A prototypic system called HB1 is being designed to meet the storage needs of next-generation hypermedia system architectures. HB1 is referred to as a hyperbase management system (HBMS) because it supports, not only the storage and manipulation of information, but the storage and manipulation of the connectivity data that link information together to form hypermedia. Among HB1's distinctions is its use of a semantic network database system to manage physical storage. Here, basic semantic modeling concepts as they apply to hypermedia systems are reviewed, and experiences using a semantic database system in HB1 are discussed.Semantic data models attempt to provide more powerful mechanisms for structuring objects than are provided by traditional approaches. In HB1, it was necessary to abstract interobject connectivity, behaviors, and information for hypermedia. Building on top of a semantic database system facilitated such a separation and made the structural aspects of hypermedia conveniently accessible to manipulation. This becomes particularly important in the implementation of structure-related operations such as structural queries. Our experience suggests that an integrated semantic object-oriented database paradigm appears to be superior to purely relational, semantic, or object-oriented methodologies for representing the structurally complex interrelationships that arise in hypermedia.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {27–50},
numpages = {24}
}

@article{10.1145/151480.151509,
author = {Szczur, Martha R. and Sheppard, Sylvia B.},
title = {TAE Plus: Transportable Applications Environment Plus: A User Interface Development Environment},
year = {1993},
issue_date = {Jan. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/151480.151509},
doi = {10.1145/151480.151509},
abstract = {The Transportable Applications Environment Plus (TAE Plus) is a NASA-developed user interface development environment (UIDE) for the rapid prototyping, evaluation, implementation, and management of user interfaces. TAE Plus provides an intuitive What You See Is What You Get (WYSIWYG) WorkBench for designing an application's user interface. The WorkBench supports the creation and sequencing of displays, including real-time, data-driven display objects. Users can define context-sensitive help for a target application. They can rehearse the user interface and also generate code automatically. In addition TAE Plus contains application services for the runtime manipulation and management of the user interface. Based on Motif and the MIT X Window System, TAE Plus runs on a variety  of Unix- or VMS-based workstations. TAE Plus is an evolving system. User-defined requirements and new technology guide the development of each new version. Advances in virtual operating systems, human factors, computer graphics, command language design, standardization, and software portability are monitored and incorporated as they become available.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {76–101},
numpages = {26},
keywords = {prototyping, graphical user interfaces, user interface development tools}
}

@article{10.1145/151480.151490,
author = {Rama, D. V. and Srinivasan, Padmini},
title = {An Investigation of Content Representation Using Text Grammars},
year = {1993},
issue_date = {Jan. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/151480.151490},
doi = {10.1145/151480.151490},
abstract = {We extend prior work on a model for natural language text representation and retrieval using a linguistic device called text grammar. We demonstrate the value of this approach in accessing relevant items from a collection of empirical abstracts in a medical domain. The advantage, when compared to traditional keyword retrieval, is that this approach is a significant move towards knowledge representation and retrieval. Text representation in this model includes keywords and their conceptual roles in the text. In particular, it involves extracting TOPIC predicates representing the research issue addressed and DESIGN predicates representing important methodological features of the empirical study. Preliminary experimentation shows that keywords exhibit a variety of text-grammar roles in a text database. Second, as intuitively expected, retrieval using TOPIC predicates identifies a smaller subset of texts than Boolean retrieval does. These empirical results along with the theoretical work indicate that the representation and retrieval strategies proposed have a significant potential. Finally, EMPIRICIST, a prototype system is described. In it the text representation predicates are implemented as a network while retrieval is through constrained-spreading activation strategies.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {51–75},
numpages = {25},
keywords = {text representation for medical abstracts, text representation for retrieval, text grammar}
}

@article{10.1145/151480.151483,
author = {Garzotto, Franca and Paolini, Paolo and Schwabe, Daniel},
title = {HDM—a Model-Based Approach to Hypertext Application Design},
year = {1993},
issue_date = {Jan. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/151480.151483},
doi = {10.1145/151480.151483},
abstract = {Hypertext development should benefit from a systematic, structured development, especially in the case of large and complex applications. A structured approach to hypertext development suggests the notion of authoring-in-the-large. Authoring-in-the-large allows the description of overall classes of information elements and navigational structures of complex applications without much concern with implementation details, and in a system-independent manner. The paper presents HDM (Hypertext Design Model), a first step towards defining a general purpose model for authoring-in-the-large. Some of the most innovative features of HDM are: the notion of perspective; the identification of different categories of links (structural links, application links, and perspective links) with different representational roles; the distinction between hyperbase and access structures; and the possibility of easily integrating the structure of a hypertext application with its browsing semantics. HDM can be used in different manners: as a modeling device or as an implementation device. As a modeling device, it supports producing high level specifications of existing or to-be-developed applications. As an implementation device, it is the basis for designing tools that directly support application development. One of the central advantages of HDM in the design and practical construction of hypertext applications is that the definition of a significant number of links can be derived automatically from a conceptual-design level description. Examples of usage of HDM are also included.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–26},
numpages = {26},
keywords = {hypertext design models, derived links, hypertext structures, hypertext applications, HDM}
}

@article{10.1145/146486.146558,
author = {Kataoka, Yutaka and Morisaki, Masato and Kuribayashi, Hiroshi and Ohara, Hiroyoshi},
title = {A Model for Input and Output of Multilingual Text in a Windowing Environment},
year = {1992},
issue_date = {Oct. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/146486.146558},
doi = {10.1145/146486.146558},
abstract = {The layered multilingual input/output(I/O) sytems we designed, based on typological studies of major-language writing conventions, unifies common features of such conventions to enable international and local utilization. The internationalization layer input module converts keystroke sequences to phonograms and ideograms. The corresponding output module displays position-independent and dependent characters. The localization layer positions language-specific functions outside the structure, integrating them as tables used by finite automaton interpreters and servers to add new languages and code sets without recompilation.The I/O system generates and displays stateful and stateless code sets, enabling interactive language switching. Going beyond POSIX locale model bounds,  the system generates ISO 2022, ISO/DIS 10646 (1990), and Compound Text, defined for the interchange encoding format in X11 protocols, for basic polyglot text communication and processing. Able to generate multilingual code sets, the I/O system clearly demonstrates that code sets should be selected by applications which have purposes beyond selecting one element from a localization set. Functionality and functions related to text manipulation in an operating sytem (OS) must also be determined by such applications.A subset of this I/O system was implemented in the X window system as a basic use of X11R5 I/O by supplying basic code set generation and string manipulation to eliminate OS interference. To ensure polyglot string manipulation, the I/O system must clearly be  implemented separately from an OS and its limitations.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {438–451},
numpages = {14},
keywords = {linguistics, input method, X window systems, input/output, localization, multilingual, multiwindow, output method, internationalization}
}

@article{10.1145/146486.146557,
author = {Matsuoka, Satoshi and Takahashi, Shin and Kamada, Tomihisa and Yonezawa, Akinori},
title = {A General Framework for Bidirectional Translation between Abstract and Pictorial Data},
year = {1992},
issue_date = {Oct. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/146486.146557},
doi = {10.1145/146486.146557},
abstract = {The merits of direct manipulation are now widely recognized. However, direct manipulation interfaces incur high cost in their creation. To cope with this problem, we present a model of bidirectional translation between pictures and abstract application data, and a prototype system, TRIP2, based on this model. Using this model, general mapping from abstract data to pictures and from pictures to abstract data is realized merely by giving declarative mapping rules, allowing fast and easy creation of direct manipulation interfaces. We apply the prototype system to the generation of the interfaces for kinship diagrams, Graph Editors, E-R diagrams, and an Othello game.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {408–437},
numpages = {30},
keywords = {user interface management systems, direct manipulation, user interface, bidirectional translation, visualization}
}

@article{10.1145/146486.146547,
author = {Bier, Eric A.},
title = {EmbeddedButtons: Supporting Buttons in Documents},
year = {1992},
issue_date = {Oct. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/146486.146547},
doi = {10.1145/146486.146547},
abstract = {EmbeddedButtons is a library of routines and a runtime kernel that support the integration of buttons into document media, including text and graphics. Existing document editors can be modified to participate in this open architecture with the addition of a few simple routines. Unlike many button systems that insert special button objects into document media, this system supports turning existing document objects into buttons. As a consequence, buttons inherit all of the attributes of normal document objects, and the appearance of buttons can be edited using operations already familiar to users. Facilities are provided for linking buttons to application windows so that documents can serve as application control panels. Hence, user interface designers can lay out  control panels using familiar document editors rather than special-purpose tools. Three classes of buttons have been implemented, including buttons that pop up a menu and buttons that store and display the value of a variable. New button classes, editors, and applications can be added at run time. Two editors, one for text and one for graphics, currently participate in the architecture.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {381–407},
numpages = {27},
keywords = {user interface layout, buttons, active documents}
}

@article{10.1145/146486.146495,
author = {Dewan, Prasun and Choudhary, Rajiv},
title = {A High-Level and Flexible Framework for Implementing Multiuser User Interfaces},
year = {1992},
issue_date = {Oct. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/146486.146495},
doi = {10.1145/146486.146495},
abstract = {We have developed a high-level and flexible framework for supporting the construction of multiuser interfaces. The framework is based on a generalized editing interaction model, which allows users to view programs as active data that can be concurrently edited by multiple users. It consists of several novel components including a refinement of both the Seeheim UIMS architecture and the distributed graphics architecture that explicitly addresses multiuser interaction; the abstractions of shared active variables and interaction variables, which allow users and applications to exchange information; a set of default collaboration rules designed to keep the collaboration-awareness low in multiuser programs; and a small but powerful set of primitives for overriding these rules. The  framework allows users to be dynamically added and removed from a multiuser sesssion, different users to use different user interfaces to interact with an application, the modules interacting with a particular user to execute on the local workstation, and programmers to incrementally trade automation for flexibility. We have implemented the framework as part of a system called Suite. This paper motivates, describes, and illustrates the framework using the concrete example of Suite, discusses how it can be implemented in other kinds of systems, compares it with related work, discusses its shortcomings, and suggests directions for future work.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {345–380},
numpages = {36},
keywords = {user interface management systems, groupware, computer-supported cooperative work, editing}
}

@article{10.1145/146486.146489,
author = {Pausch, Randy and Conway, Matthew and Deline, Robert},
title = {Lessons Learned from SUIT, the Simple User Interface Toolkit},
year = {1992},
issue_date = {Oct. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/146486.146489},
doi = {10.1145/146486.146489},
abstract = {In recent years, the computer science community has realized the advantages of GUIs (Graphical User Interfaces). Because high-quality GUIs are difficult to build, support tools such as UIMSs, UI Toolkits, and Interface Builders have been developed. Although these tools are powerful, they typically make two assumptions: first, that the programmer has some familiarity with the GUI model, and second, that he is willing to invest several weeks becoming proficient with the tool. These tools typically operate only on specific platforms, such as DOS, the Macintosh, or UNIX/X-windows.The existing tools are beyond the reach of most undergraduate computer science majors, or professional programmers who wish to quickly build GUIs without investing the time to become specialists in  GUI design. For this class of users, we developed SUIT, the Simple User Iinterface Toolkit. SUIT is an attempt to distill the fundamental components of an interface builder and GUI toolkit, and to explain those concepts with the tool itself, all in a short period of time. We have measured that college juniors with no previous GUI programming experience can use SUIT productively after less than three hours. SUIT is a C subroutine library which provides an external control UIMS, an interactive layout editor, and a set of standard “widgets,” such as sliders, buttons, and check boxes. SUIT-based applications run transparently across the Macintosh, DOS, and UNIX/X platforms. SUIT has been exported  to hundreds of external sites on the internet. This paper describes SUIT's architecture, the design decisions we made during its development, and the lessons we learned from extensive observations of over 120 users.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {320–344},
numpages = {25},
keywords = {GUI, pedagogy, user interface toolkit, learnability, graphical user interface, software tools, export, rapid prototyping, UIMS, portability}
}

@article{10.1145/146760.146791,
author = {Rada, Roy},
title = {Converting a Textbook to Hypertext},
year = {1992},
issue_date = {July 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/146760.146791},
doi = {10.1145/146760.146791},
abstract = {Traditional documents may be transformed into hypertext by first reflecting the document's logical markup in the hypertext (producing first-order hypertext) and then by adding links not evident in the document markup (producing second-order hypertext). In our transformation of a textbook to hypertext, the textbook is placed in an intermediate form based on a semantic net and is then placed into the four hypertext systems: Emacs-Info, Guide, HyperTies, and Super-Book. The first-order Guide and SuperBook hypertexts reflect a depth-first traversal of the semantic net, and the Emacs-Info and HyperTies hypertexts reflect a breadth-first traversal. The semantic net is augmented manually, and then new traversal programs automatically generate alternate outlines. An index based on work patterns in the textbook is also automatically generated for the second-order hypertext. Our suite of programs has been applied to a published textbook, and the resulting hypertexts are publicly available.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {294–315},
numpages = {22},
keywords = {electronic publishing, hypermedia models, document markup, human-computer interaction}
}

@article{10.1145/146760.146779,
author = {Ionnidis, Yannis E. and Saulys, Tomas and Whitsitt, Andrew J.},
title = {Conceptual Learning in Database Design},
year = {1992},
issue_date = {July 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/146760.146779},
doi = {10.1145/146760.146779},
abstract = {This paper examines the idea of incorporating machine learning algorithms into a database system for monitoring its stream of incoming queries and generating hierarchies with the most important concepts expressed in those queries. The goal is for these hierarchies to provide
valuable input to the database administrator for dynamically modifying the physical and external schemas of a database for improved system performance and user productivity. The criteria for choosing the appropriate learning algorithms are analyzed, and based on them, two such algorithms, UNIMEM and COBWEB, are selected as the most suitable ones for the task. Standard UNIMEM and COBWEB implementations have been modified to support queries as input. Based on the results of experiments with these modified implementations, the whole approach appears to be quite promising, expecially if the concept hierarchy from which the learning algorithms start their processing is initialized with some of the most obvious concepts captured in the database.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {265–293},
numpages = {29},
keywords = {/UNIMEM, learning from examples, COBWEB, adaptive database systems}
}

@article{10.1145/146760.146770,
author = {Palaniappan, Murugappan and Yankelovich, Nicole and Fitzmaurice, George and Loomis, Anne and Haan, Bernard and Coombs, James and Meyrowitz, Norman},
title = {The Envoy Framework: An Open Architecture for Agents},
year = {1992},
issue_date = {July 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/146760.146770},
doi = {10.1145/146760.146770},
abstract = {The Envoy Framework addresses a need for computer-based assistants or agents that operate in conjunction with users' existing applications, helping them perform tedious, repetitive, or time-consuming tasks more easily and efficiently. Envoys carry out missions for users by invoking envoy-aware applications called operatives and inform users of mission results via envoy-aware applications called informers. The distributed, open architecture developed for Envoys is derived from an analysis of the best characteristics of existing agent systems. This architecture has been designed as a model for how agent technology can be seamlessly integrated into the electronic desktop. It defines a set of application programmer's interfaces so that developers may convert their software to envoy-aware applications. A subset of the architecture described in this paper has been implemented in an Envoy Framework prototype.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {233–264},
numpages = {32},
keywords = {user agent, application programmer interface}
}

@article{10.1145/146760.146764,
author = {Blake, G. Elizabeth and Bray, Tim and Tompa, Frank Wm.},
title = {Shortening the <i>OED</i>: Experience with a Grammar-Defined Database},
year = {1992},
issue_date = {July 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/146760.146764},
doi = {10.1145/146760.146764},
abstract = {Textual databases with highly variable structure can be usefully described by a grammar-defined model. One example of such a text is the Oxford English Dictionary. This paper describes a first attempt to apply technology based on this model to a real problem. A language called GOEDEL, which is a partial implementation of a set of grammar-defined database operators, was used to extract and alter a subset of the OED in order to assist the editors in their production of The Shorter Oxford English Dictionary. The implementation of the pstring data structure to describe a piece of text and the functions that operate on this pstring are illustrated with some detailed examples. The project was judged a success and the resulting program used in production by the Oxford University Press.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {213–232},
numpages = {20},
keywords = {grammar defined model, parsed string, text database}
}

@article{10.1145/146802.146834,
author = {Carroll, John M. and Rosson, Mary Beth},
title = {Getting around the Task-Artifact Cycle: How to Make Claims and Design by Scenario},
year = {1992},
issue_date = {April 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/146802.146834},
doi = {10.1145/146802.146834},
abstract = {We are developing an “action science” approach to human-computer interaction (HCI), seeking to better integrate activities directed at understanding with those directed at design. The approach leverages development practices of current HCI with methods and concepts to support a shift toward using broad and explicit design rationale to reify where we are in a design process, why we are there, and to guide reasoning about where we might go from there. We represent a designed artifact as the set of user scenarios supported by that artifact and more finely by causal schemas detailing the underlying psychological rationale. These schemas, called claims, unpack wherefores and whys of the scenarios. In this paper, we stand back from several empirical projects to clarify our commitments and practices.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {181–212},
numpages = {32},
keywords = {planning, user interfaces, design rationale}
}

@article{10.1145/146802.146826,
author = {Botafogo, Rodrigo A. and Rivlin, Ehud and Shneiderman, Ben},
title = {Structural Analysis of Hypertexts: Identifying Hierarchies and Useful Metrics},
year = {1992},
issue_date = {April 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/146802.146826},
doi = {10.1145/146802.146826},
abstract = {Hypertext users often suffer from the “lost in hyperspace” problem: disorientation from too many jumps while traversing a complex network. One solution to this problem is improved authoring to create more comprehensible structures. This paper proposes several authoring tools, based on hypertext structure analysis.In many hypertext systems authors are encouraged to create hierarchical structures, but when writing, the hierarchy is lost because of the inclusion of cross-reference links. The first part of this paper looks at ways of recovering lost hierarchies and finding new ones, offering authors different views of the same hypertext. The second part helps authors by identifying properties of the hypertext document. Multiple metrics are developed including compactness and stratum. Compactness indicates the intrinsic connectedness of the hypertext, and stratum reveals to what degree the hypertext is organized so that some nodes must be read before others.Several existing hypertexts are used to illustrate the benefits of each technique. The collection of techniques provides a multifaceted view of the hypertext, which should allow authors to reduce undesired structural complexity and create documents that readers can traverse more easily.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {142–180},
numpages = {39},
keywords = {hierarchies, structural analysis, graph theory, metrics, hypertext}
}

@article{10.1145/146802.146810,
author = {Krovetz, Robert and Croft, W. Bruce},
title = {Lexical Ambiguity and Information Retrieval},
year = {1992},
issue_date = {April 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/146802.146810},
doi = {10.1145/146802.146810},
abstract = {Lexical ambiguity is a pervasive problem in natural language processing. However, little quantitative information is available about the extent of the problem or about the impact that it has on information retrieval systems. We report on an analysis of lexical ambiguity in information retrieval test collections and on experiments to determine the utility of word meanings for separating relevant from nonrelevant documents. The experiments show that there is considerable ambiguity even in a specialized database. Word senses provide a significant separation between relevant and nonrelevant documents, but several factors contribute to determining whether disambiguation will make an improvement in performance. For example, resolving lexical ambiguity was found to have little impact on retrieval effectiveness for documents that have many words in common with the query. Other uses of word sense disambiguation in an information retrieval context are discussed.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {115–141},
numpages = {27},
keywords = {word senses, document retrieval, semantically based search, disambiguation}
}

@article{10.1145/128756.128761,
author = {Wiecha, Charles},
title = {ITS and User Interface Consistency: A Response to Grudin},
year = {1992},
issue_date = {Jan. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/128756.128761},
doi = {10.1145/128756.128761},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {112–114},
numpages = {3}
}

@article{10.1145/128756.128760,
author = {Grudin, Jonathan},
title = {Consistency, Standards, and Formal Approaches to Interface Development and Evaluation: A Note on Wiecha, Bennett, Boies, Gould, and Greene},
year = {1992},
issue_date = {Jan. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/128756.128760},
doi = {10.1145/128756.128760},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {103–111},
numpages = {9}
}

@article{10.1145/128756.128759,
author = {Want, Roy and Hopper, Andy and Falc\~{a}o, Veronica and Gibbons, Jonathan},
title = {The Active Badge Location System},
year = {1992},
issue_date = {Jan. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/128756.128759},
doi = {10.1145/128756.128759},
abstract = {A novel system for the location of people in an office environment is described. Members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. The paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. Location systems raise concerns about the privacy of an individual and these issues are also addressed.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {91–102},
numpages = {12},
keywords = {active badges, tagging systems, location systems, privacy issues, PBX}
}

@article{10.1145/128756.128758,
author = {Gemmell, Jim and Christodoulakis, Stavros},
title = {Principles of Delay-Sensitive Multimedia Data Storage Retrieval},
year = {1992},
issue_date = {Jan. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/128756.128758},
doi = {10.1145/128756.128758},
abstract = {This paper establishes some fundamental principles for the retrieval and storage of delay-sensitive multimedia data. Delay-sensitive data include digital audio, animations, and video. Retrieval of these data types from secondary storage has to satisfy certain time constraints in order to be acceptable to the user. The presentation is based on digital audio in order to provide intuition to the reader, although the results are applicable to all delay-sensitive data. A theoretical framework is developed for the real-time requirements of digital audio playback. We show how to describe these requirements in terms of the consumption rate of the audio data and the nature of the data-retrieval rate from secondary storage. Making use of this framework, bounds are derived for buffer space requirements for certain common retrieval scenarios. Storage placement strategies for multichannel synchronized data are then categorized and examined. The results presented in this paper are basic to any playback of delay-sensitive data and should assist the multimedia system designer in estimating hardware requirements and in evaluating possible design choices.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {51–90},
numpages = {40},
keywords = {real-time, delay-sensitive, continuous media}
}

@article{10.1145/128756.128757,
author = {Jarke, M. and Mylopoulos, J. and Schmidt, J. W. and Vassiliou, Y.},
title = {DAIDA: An Environment for Evolving Information Systems},
year = {1992},
issue_date = {Jan. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/128756.128757},
doi = {10.1145/128756.128757},
abstract = {We present a framework for the development of information systems based on the premise that the knowledge that influences the development process needs to somehow be captured, represented, and managed if the development process is to be rationalized. Experiences with a prototype environment developed in ESPRIT project DAIDA demonstrate the approach. The project has implemented an environment based on state-of-the-art languages for requirements modeling, design and implementation of information systems. In addition, the environment offers tools for aiding the mapping process from requirements to design and then to implementation, also for representing decisions reached during the development process. The development process itself is represented explicitly within the system, thus making the DAIDA development framework easier to comprehend, use, and modify.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–50},
numpages = {50},
keywords = {knowledge engineering, software information system, mapping assistant, software process model, multi-level specification, repository}
}

@article{10.1145/119311.119315,
author = {Kacmar, Charles J. and Leggett, John J.},
title = {PROXHY: A Process-Oriented Extensible Hypertext Architecture},
year = {1991},
issue_date = {Oct. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/119311.119315},
doi = {10.1145/119311.119315},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {399–419},
numpages = {21}
}

@article{10.1145/119311.119314,
author = {Hart, Paul and Estrin, Deborah},
title = {Inter-Organization Networks, Computer Integration, and Shifts in Interdependence: The Case of the Semiconductor Industry},
year = {1991},
issue_date = {Oct. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/119311.119314},
doi = {10.1145/119311.119314},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {370–398},
numpages = {29}
}

@article{10.1145/119311.119313,
author = {Zezula, P. and Rabitti, F. and Tiberio, P.},
title = {Dynamic Partitioning of Signature Files},
year = {1991},
issue_date = {Oct. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/119311.119313},
doi = {10.1145/119311.119313},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {336–367},
numpages = {32}
}

@article{10.1145/119311.119312,
author = {Siochi, Antonio C. and Ehrich, Roger W.},
title = {Computer Analysis of User Interfaces Based on Repetition in Transcripts of User Sessions},
year = {1991},
issue_date = {Oct. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/119311.119312},
doi = {10.1145/119311.119312},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {309–335},
numpages = {27}
}

@article{10.1145/125187.125200,
author = {Fox, Edward A. and Chen, Qi Fan and Daoud, Amjad M. and Heath, Lenwood S.},
title = {Order-Preserving Minimal Perfect Hash Functions and Information Retrieval},
year = {1991},
issue_date = {July 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/125187.125200},
doi = {10.1145/125187.125200},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {281–308},
numpages = {28},
keywords = {Dictionary structure, inverted file structures, perfect hashing, minimal perfect hashing, indexing, random graph}
}

@article{10.1145/125187.125193,
author = {Gauch, Susan and Smith, John B.},
title = {Search Improvement via Automatic Query Reformulation},
year = {1991},
issue_date = {July 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/125187.125193},
doi = {10.1145/125187.125193},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {249–280},
numpages = {32},
keywords = {Expert Systems, textbases, query reformulation, online search assistance, full-text information retrieval}
}

@article{10.1145/125187.125189,
author = {Fuhr, Norbert and Buckley, Chris},
title = {A Probabilistic Learning Approach for Document Indexing},
year = {1991},
issue_date = {July 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/125187.125189},
doi = {10.1145/125187.125189},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {223–248},
numpages = {26},
keywords = {relevance descriptions, linear retrieval functions, linear indexing functions, probabilistic retrieval, probabilistic indexing, complex document representation}
}

@article{10.1145/125187.125188,
author = {Turtle, Howard and Croft, W. Bruce},
title = {Evaluation of an Inference Network-Based Retrieval Model},
year = {1991},
issue_date = {July 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/125187.125188},
doi = {10.1145/125187.125188},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {187–222},
numpages = {36},
keywords = {document retrieval, network retrieval models, inference networks}
}

@article{10.1145/123078.128729,
author = {Tang, John C. and Minneman, Scott L.},
title = {Videodraw: A Video Interface for Collaborative Drawing},
year = {1991},
issue_date = {April 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/123078.128729},
doi = {10.1145/123078.128729},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {170–184},
numpages = {15},
keywords = {gestural interfaces, shared drawing, user interface, video technology, work practice analysis, colloborative systems}
}

@article{10.1145/123078.128728,
author = {Jacob, Robert J. K.},
title = {The Use of Eye Movements in Human-Computer Interaction Techniques: What You Look at is What You Get},
year = {1991},
issue_date = {April 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/123078.128728},
doi = {10.1145/123078.128728},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {152–169},
numpages = {18},
keywords = {human-computer interaction, input, eye movements, UIMS, eye tracking, state transition diagram}
}

@article{10.1145/123078.128727,
author = {Fischer, Gerhard and Lemke, Andreas C. and Mastaglio, Thomas and Morch, Andres I.},
title = {The Role of Critiquing in Cooperative Problem Solving},
year = {1991},
issue_date = {April 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/123078.128727},
doi = {10.1145/123078.128727},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {123–151},
numpages = {29},
keywords = {high-functionality computer systems, intelligent support systems, design environments, critics, cooperative problem-solving systems, critiquing}
}

@article{10.1145/123078.128726,
author = {Card, Stuart K. and Mackinlay, Jock D. and Robertson, George G.},
title = {A Morphological Analysis of the Design Space of Input Devices},
year = {1991},
issue_date = {April 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/123078.128726},
doi = {10.1145/123078.128726},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {99–122},
numpages = {24},
keywords = {design rationale, design space, input devices, morphological analysis, design knowledge sytematization, semantics}
}

@article{10.1145/103731.103735,
author = {Aiken, Milam W. and Liu Sheng, Olivia R. and Vogel, Douglas R.},
title = {Integrating Expert Systems with Group Decision Support Systems},
year = {1991},
issue_date = {Jan. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/103731.103735},
doi = {10.1145/103731.103735},
abstract = {Expert systems are powerful tools that serve as adjuncts to decision making and have found wide applicability in a wide variety of areas. Integrating expert systems with group decision support systems has the potential to enhance the quality and effeciency of group communication, negotiation, and collaborative work. This paper examines possible synergies between the two technologies and provides a survey of current partially-integrated systems. Finally, a prototype design of a highly-integrated system is described with directions for further research.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {75–95},
numpages = {21},
keywords = {expert systems, group decision support systems, artificial intelligence, knowledge-based systems}
}

@article{10.1145/103731.103734,
author = {Mak, Victor Wing-Kit and Lee, Kuo Chu and Frieder, Ophir},
title = {Exploiting Parallelism in Pattern Matching: An Information Retrieval Application},
year = {1991},
issue_date = {Jan. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/103731.103734},
doi = {10.1145/103731.103734},
abstract = {We propose a document-searching architecture based on high-speed hardware pattern matching to increase the throughput of an information retrieval system. We also propose a new parallel VLSI pattern-matching algorithm called the Data Parallel Pattern Matching (DPPM) algorithm, which serially broadcasts and compares the pattern to a block of data in parallel. The DPPM algorithm utilizes the high degree of integration of VLSI technology to attain very high-speed processing through parallelism. Performance of the DPPM has been evaluated both analytically and by simulation. Based on the simulation statistics and timing analysis on the hardware design, a search rate of multiple gigabytes per second is achievable using 2-μm CMOS technology. The potential performance of the proposed  document-searching architecture is also analyzed using the simulation statistics of the DPPM algorithm.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {52–74},
numpages = {23},
keywords = {pattern matcher, DPPM}
}

@article{10.1145/103731.103733,
author = {Kim, Won and Ballou, Nat and Garza, Jorge F. and Woelk, Darrell},
title = {A Distributed Object-Oriented Database System Supporting Shared and Private Databases},
year = {1991},
issue_date = {Jan. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/103731.103733},
doi = {10.1145/103731.103733},
abstract = {ORION-2 is a commercially available, federated, object-oriented database management system designed and implemented at MCC. One major architectural innovation in ORION-2 is the coexistence of a shared databese and a number of private databases. The shared database is accessible to all authorized users of the system, while each private database is accessible to only the user who owns it. A distributed database system with a shared database and private databases for individual users is a natural architecture for data-intensive application environments on a network of workstations, notably computer-aided design and engineering systems. This paper discusses the benefits and limitations of such a system and explores the impact of such an architecture on the semantics and implementation of some of the key functions of a database system, notably queries, database schema, and versions. Although the issues are discussed in the context of an object-oriented data model, the results (at least significant portions thereof) are applicable to database systems supporting other data models.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {31–51},
numpages = {21},
keywords = {federated databases, client-server architecture, object-oriented databases}
}

@article{10.1145/103731.103732,
author = {Ford, Daniel Alexander and Christodoulakis, Stavros},
title = {Optimal Placement of High-Probability Randomly Retrieved Blocks on CLV Optical Discs},
year = {1991},
issue_date = {Jan. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/103731.103732},
doi = {10.1145/103731.103732},
abstract = {Optimal data placement on a CLV (Constant Linear Velocity) format optical discs has an objective the minimization of the expected access cost of data retrievals from the disc when the probabilities of access of data items may be different. The problem of optimal data placement for optical discs is both important and more difficult than the corresponding problem on magnetic discs. A good data placement on optical discs is more important because data sets on optical discs such as WORM and CD ROM cannot be modified or moved once they are placed on disc. Currently, even rewritable optical discs are best suited for applications that are archival in nature. The problem of optimal data placement on CLV format optical discs is more difficult, mainly because the useful storage space is not  uniformly distributed across the disc surface (along the radius). This leads to a complicated positional performance trade-off not present for magnetic disks.We present a model that encompasses all the important aspects of the placement problem on CLV format optical discs. The model takes into account the nonuniform distribution of useful storage, the dependency of the rotational delay on disc position, a  parameterized seek cost function for optical discs, and the varying access probabilities of data items. We show that the optimal placement of high-probability blocks satisfies a unimodality property. Based on this observation, we solve the optimal placement problem. We then study the impact of the relative weights of the problem parameters and show that the optimal data  placement may be very different from the optimal data placement on magnetic disks. We also validate our model and analysis and give an algorithm for computing the placement of disc sectors.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–30},
numpages = {30},
keywords = {performance, management}
}

@article{10.1145/102675.102678,
author = {Straube, David D. and \"{O}zsu, M. Tamer},
title = {Queries and Query Processing in Object-Oriented Database Systems},
year = {1990},
issue_date = {Oct. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/102675.102678},
doi = {10.1145/102675.102678},
abstract = {Object-oriented database mangement systems (OODBMS) combine the data abstraction and computational models of object-oriented programming languages with the query and performance capabilities of database management systems. A concise, formal data model for OODBMS has not been universally accepted, preventing detailed investigation of various system issues such as query processing. We define a data model that captures the essence of classification-based object-oriented systems and formalize concepts such as object identity, inheritence, and methods. The main topic of the paper is the presentation of a query processing methodology complete with an object calculus to object algebra translation are discussed in detail. The paper concludes with a discussion of equivalence-preserving     transformation rules for object algebra expressions.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {387–430},
numpages = {44},
keywords = {object-oriented databases, query transformation rules, object algebra, object calculus}
}

@article{10.1145/102675.102677,
author = {Kwok, K. L.},
title = {Experiments with a Component Theory of Probabilistic Information Retrieval Based on Single Terms as Document Components},
year = {1990},
issue_date = {Oct. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/102675.102677},
doi = {10.1145/102675.102677},
abstract = {A component theory of information retrieval using single content terms as component for queries and documents was reviewed and experimented with. The theory has the advantages of being able to (1) bootstrap itself, that is, define initial term weights naturally based on the fact that items are self relevent; (2) make use of within-item term frequencies; (3) account for query-focused and document-focused indexing and retrieval strategies cooperatively; and (4) allow for component-specific feedback if such information is available. Retrieval results with four collections support the effectiveness of all the first three aspects, except for predictive retrieval. At the initial indexing stage, the retrieval theory performed much more consistantly across collections than croft's model and   provided results comparable to Salton's tf*idf approach. An inverse collection term frequency (ICTF) formula was also tested that performed much better than the inverse document frequency (IDF). With full feedback retrospective retrieval, the component theory performed substantially better than Croft's, because of the highly specific nature of document-focused feedback. Repetitive retireval results with partial relevance feedback mirrored those for the retrospective. However, for the important case of predictive retrieval using residual ranking, results were not unequivocal.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {363–386},
numpages = {24},
keywords = {document-focused and query-focused relevance feedback, probabilistic retrieval, inverse collection term frequency weighting, probabilistic indexing, ranking and weighting of composite objects, indexing and retrieval, inverse document frequency weighting}
}

@article{10.1145/102675.102676,
author = {Mylopoulos, John and Borgida, Alex and Jarke, Matthias and Koubarakis, Manolis},
title = {Telos: Representing Knowledge about Information Systems},
year = {1990},
issue_date = {Oct. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/102675.102676},
doi = {10.1145/102675.102676},
abstract = {We describe Telos, a language intended to support the development of information systems. The design principles for the language are based on the premise that information system development is knowledge intensive and that the primary responsibility of any language intended for the task is to be able to formally represent the relevent knowledge. Accordingly, the proposed language is founded on concepts from knowledge representations. Indeed, the language is appropriate for representing knowledge about a variety of worlds related to a particular information system, such as the subject world (application domain), the usage world (user models, environments), the system world (software requirements, design), and the development world (teams, metodologies).We introduce the   features of the language through examples, focusing on those provided for desribing metaconcepts that can then be used to describe knowledge relevant to a particular information system. Telos' fetures include an object-centered framework which supports aggregation, generalization, and classification; a novel treatment of attributes; an explicit representation of time; and facilities for specifying integrity constraints and deductive rules. We review actual applications of the language through further examples, and we sketch a formalization of the language.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {325–362},
numpages = {38},
keywords = {proposition, deductive rules, instance, belief time, history time, integrity constraints, class, knowledge base, temporal knowledge, metaclass}
}

@article{10.1145/98188.98204,
author = {Myers, Brad A.},
title = {A New Model for Handling Input},
year = {1990},
issue_date = {July 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/98188.98204},
doi = {10.1145/98188.98204},
abstract = {Although there has been important progress in models and packages for the output of graphics to computer screens, there has been little change in the way that input from the mouse, keyboard, and other input devices is handled. New graphics standards are still using a fifteen-year-old model even though it is widely accepted as inadequate, and most modern window managers simply return a stream of low-level, device-dependent input events. This paper presents a new model that handles input devices for highly interactive, direct manipulation, graphical user interfaces, which could be used in future toolkits, window managers, and graphics standards. This model encapsulates interactive behaviors into a few “Interactor” object types. Application  programs can then create instances of these Interactor objects which hide the details of the underlying window manager events. In addition, Interactors allow a clean separation between the input handling, the graphics, and the application programs. This model has been extensively used as part of the Garnet system and has proven to be convenient, efficient, and easy to learn.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {289–320},
numpages = {32}
}

@article{10.1145/98188.98201,
author = {Hudson, Scott E. and Mohamed, Shamim P.},
title = {Interactive Specification of Flexible User Interface Displays},
year = {1990},
issue_date = {July 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/98188.98201},
doi = {10.1145/98188.98201},
abstract = {One of the problems with conventional UIMSs is that very often there is no graphical way to specify interfaces. This paper describes OPUS, the user interface editor of the Penguims UIMS. This system allows the presentation component of graphical user interfaces to be specified interactively in a graphical notation without explicit programming. The Penguims UIMS supports an underlying model of computation based loosely on spreadsheets. In particular, it supports incremental computations based on a system of equations (one-way constraints) over a set of named values (spreadsheet cells). These equations are used to provide immediate feedback at all levels of the interface. They are used to incrementally determine the position and dynamic appearance of    the individual interactor objects that make up the interface. They are also used to connect the presentation directly to underlying application data thereby supporting semantic feedback. The OPUS user interface editor employs a special graphical notation for specifying the presentation component of a user interface. This notation allows the power of the underlying computational model to be expressed simply and quickly. The resulting presentations are very flexible in nature. They can automatically respond to changes in the size and position of display objects and can directly support derivation of their appearance from application data objects.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {269–288},
numpages = {20}
}

@article{10.1145/98188.98197,
author = {Vlissides, John M. and Linton, Mark A.},
title = {Unidraw: A Framework for Building Domain-Specific Graphical Editors},
year = {1990},
issue_date = {July 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/98188.98197},
doi = {10.1145/98188.98197},
abstract = {Unidraw is a framework for creating graphical editors in domains such as technical and artistic drawing, music composition, and circuit design. The Unidraw architecture simplifies the construction of these editors by proving programming abstractions that are common across domains. Unidraw defines four basic abstractions: components define operations on components, and external representations define the mapping between components and the file format generated by the editor. Unidraw also supports multiple views, graphical connectivity, and dataflow between components. This paper describes the Unidraw design, implementation issues, and three experimental domain specific editors we have developed with Unidraw: a drawing editor, a user interface builder,  and a schematic capture system. Our results indicate a substantial reduction in implementation time and effort compared with existing tools.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {237–268},
numpages = {32}
}

@article{10.1145/98188.98194,
author = {Wiecha, Charles and Bennett, William and Boies, Stephen and Gould, John and Greene, Sharon},
title = {ITS: A Tool for Rapidly Developing Interactive Applications},
year = {1990},
issue_date = {July 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/98188.98194},
doi = {10.1145/98188.98194},
abstract = {The ITS architecture separates applications into four layers. The action layer implements back-end application functions. The dialog layer defines the content of the user interface, independent of its style. Content specifies the objects included in each frame of the interface, the flow of control among frames, and what actions are associated with each object. The style rule layer defines the presentation and behavior of a family of interaction techniques. Finally, the style program layer implements primitive toolkit objects that are composed by the rule layer into complete interaction techniques. This paper describes the architecture in detail, compares it with previous User Interface Management systems and toolkits, and describes how ITS is being used to implement the visitor  information system for EXPO '92.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {204–236},
numpages = {33}
}

@article{10.1145/98188.98191,
author = {Hartson, H. Rex and Siochi, Antonio C. and Hix, D.},
title = {The UAN: A User-Oriented Representation for Direct Manipulation Interface Designs},
year = {1990},
issue_date = {July 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/98188.98191},
doi = {10.1145/98188.98191},
abstract = {Many existing interface representation techniques, especially those associated with UIMS, are constructional and focused on interface implementation, and therefore do not adequately support a user-centered focus. But it is in the behavioral domain of the user that interface designers and evaluators do their work. We are seeking to complement constructional methods by providing a tool-supported technique capable of specifying the behavioral aspects of an interactive system–the tasks and the actions a user performs to accomplish those tasks. In particular, this paper is a practical introduction to use of the User Action Notation (UAN), a task- and user-oriented notation for behavioral representation of asynchronous, direct manipulation interface    designs. Interfaces are specified in UAN as a quasihierarchy of asynchronous tasks. At the lower levels, user actions are associated with feedback and system state changes. The notation makes use of visually onomatopoeic symbols and is simple enough to read with little instruction. UAN is being used by growing numbers of interface developers and researchers. In addition to its design role, current research is investigating how UAN can support production and maintenance of code and documentation.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {181–203},
numpages = {23}
}

@article{10.1145/96105.96113,
author = {Morrissey, J. M.},
title = {Imprecise Information and Uncertainty in Information Systems},
year = {1990},
issue_date = {Apr. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/96105.96113},
doi = {10.1145/96105.96113},
abstract = {Information systems exist to model, store, and retrieve all types of data. Problems arise when some of the data are missing or imprecisely known or when an attribute is not applicable to a particular object. A consistent and useful treatment of such exceptions is necessary. The approach taken here is to allow any attribute value to be a regular precise value, a string denoting that the value is missing, a string denoting that the attribute is not applicable, or an imprecise value. The imprecise values introduce uncertainty into query evaluation, since it is no longer obvious which objects should be retrieved. To handle the uncertainty, two set of objects are retrieved in response to every query: the set of objects that are known to satisfy with complete certainty and the set that possibly satisfies the query with various degrees of uncertainty. Two methods of estimating this uncertainty, based on information theory, are proposed. The measure of uncertainty is used to rank objects for presentation to a user.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {159–180},
numpages = {22}
}

@article{10.1145/96105.96111,
author = {Shasha, Dennis and Wang, Tsong-Li},
title = {New Techniques for Best-Match Retrieval},
year = {1990},
issue_date = {Apr. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/96105.96111},
doi = {10.1145/96105.96111},
abstract = {A scheme to answer best-match queries from a file containing a collection of objects is described. A best-match query is to find the objects in the file that are closest (according to some (dis)similarity measure) to a given target.Previous work [5, 331] suggests that one can reduce the number of comparisons required to achieve the desired results using the triangle inequality, starting with a data structure for the file that reflects some precomputed intrafile distances. We generalize the technique to allow the optimum use of any given set of precomputed intrafile distances. Some empirical results are presented which illustrate the effectiveness of our scheme, and its performance relative to previous algorithms.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {140–158},
numpages = {19}
}

@article{10.1145/96105.96109,
author = {Moss, J. Eliot B.},
title = {Design of the Mneme Persistent Object Store},
year = {1990},
issue_date = {Apr. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/96105.96109},
doi = {10.1145/96105.96109},
abstract = {The Mneme project is an investigation of techniques for integrating programming language and database features to provide better support for cooperative, information-intensive tasks such as computer-aided software engineering. The project strategy is to implement efficient, distributed, persistent programming languages. We report here on the Mneme persistent object store, a fundamental component of the project, discussing its design and initial prototype. Mneme stores objects in a simple and general format, preserving object identity and object interrelationships. Specific goals for the store include portability, extensibility (especially with respect to object management policies), and performance. The model of memory that the store aims at is a single, cooperatively-shared heap, distributed across a collection of networked computers. The initial prototype is intended mainly to explore performance issues and to support object-oriented persistent programming languages. We include performance measurements from the prototype as well as more qualitative results.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {103–139},
numpages = {37}
}

@article{10.1145/96105.96107,
author = {Watters, Carolyn and Shepherd, Michael A.},
title = {A Transient Hypergraph-Based Model for Data Access},
year = {1990},
issue_date = {Apr. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/96105.96107},
doi = {10.1145/96105.96107},
abstract = {Two major methods of accessing data in current database systems are querying and browsing. The more traditional query method returns an answer set that may consist of data values (DBMS), items containing the answer (full text), or items referring the user to items containing the answer (bibliographic). Browsing within a database, as best exemplified by hypertext systems, consists of viewing a database item and linking to related items on the basis of some attribute or attribute value.A model of data access has been developed that supports both query and browse access methods. The model is based on hypergraph representation of data instances. The hyperedges and nodes are manipulated through a set of operators to compose new nodes and to instantiate new links dynamically, resulting in transient hypergraphs. These transient hypergraphs are virtual structures created in response to user queries, and lasting only as long as the query session. The model provides a framework for general data access that accommodates user-directed browsing and querying, as well as traditional models of information and data retrieval, such as the Boolean, vector space, and probabilistic models. Finally, the relational database model is shown to provide a reasonable platform for the implementation of this transient hypergraph-based model of data access.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {77–102},
numpages = {26}
}

@article{10.1145/78915.78918,
author = {H\"{a}mm\"{a}inen, Heikki and Eloranta, Eero and Alasuvanto, Jari},
title = {Distributed Form Management},
year = {1990},
issue_date = {Jan. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/78915.78918},
doi = {10.1145/78915.78918},
abstract = {An open architecture for distributed form management is described. The model employs object-orientation in describing organizational units as well as individual users as entities with uniform external interfaces. Each entity is represented by an autonomous user agent which operates on local and migrating forms. The form concept encapsulates data, layout, and rules into a unified object which is the basic unit of presentation, processing, storage, and communication. All functionality of the system appears in rules of form classes and all data in instances of these form classes. This approach applies the techniques of computer supported cooperative work to provide a flexible mechanism for interpersonal, intraoffice, and interoffice procedures. The main challenge is to organize the collaboration without affecting the autonomy of individual user agents. In this respect, the contribution of the model is the mechanism for form migration. The dynamic integration of forms into different agents is solved with the coordinated interchange of form classes. A specific inheritance scheme provides the desired flexibility by separating the interrelated private and public form operations within each agent. The paper first describes the architecture by starting from a single agent and moving progressively towards a set of cooperating agents. Then an agent implementation called PAGES is described, experiences reported, and the open issues discussed. A typical distributed ordering procedure is used as an example throughout the text.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {50–76},
numpages = {27}
}

@article{10.1145/78915.78917,
author = {Bookstein, Abraham and Klein, Shmuel T.},
title = {Compression, Information Theory, and Grammars: A Unified Approach},
year = {1990},
issue_date = {Jan. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/78915.78917},
doi = {10.1145/78915.78917},
abstract = {Text compression is of considerable theoretical and practical interest. It is, for example, becoming increasingly important for satisfying the requirements of fitting a large database onto a single CD-ROM. Many of the compression techniques discussed in the literature are model based. We here propose the notion of a formal grammar as a flexible model of text generation that encompasses most of the models offered before as well as, in principle, extending the possibility of compression to a much more general class of languages. Assuming a general model of text generation, a derivation is given of the well known Shannon entropy formula, making possible a theory of information based upon text representation rather than on communication. The ideas are shown to apply to a number of commonly used text models. Finally, we focus on a Markov model of text generation, suggest an information theoretic measure of similarity between two probability distributions, and develop a clustering algorithm based on this measure. This algorithm allows us to cluster Markov states, and thereby base our compression algorithm on a smaller number of probability distributions than would otherwise have been required. A number of theoretical consequences of this approach to compression are explored, and a detailed example is given.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {27–49},
numpages = {23}
}

@article{10.1145/78915.78916,
author = {Lee, Jintae and Malone, Thomas W.},
title = {Partially Shared Views: A Scheme for Communicating among Groups That Use Different Type Hierarchies},
year = {1990},
issue_date = {Jan. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/78915.78916},
doi = {10.1145/78915.78916},
abstract = {Many computer systems are based on various types of messages, forms, or other objects. When users of such systems need to communicate with people who use different object types, some kind of translation is necessary. In this paper, we explore the space of general solutions to this translation problem and propose a scheme that synthesizes these solutions. After first illustrating the problem in the Object Lens system, we identify two partly conflicting objectives that any translation scheme should satisfy: preservation of meaning and autonomous evolution of group languages. Then we partition the space of possible solutions to this problem in terms of the set theoretic relations between group languages and a common language. This leads to five primary solution classes and we illustrate and evaluate each one. Finally, we describe a composite scheme, called Partially Shared Views, that combines many of the best features of the other schemes. A key insight of the analysis is that partially shared type hierarchies allow “foreign” object types to be automatically translated into their nearest common “ancestor” types. The partial interoperability attained in this way makes possible flexible standards from which people can benefit from whatever agreements they do have without having to agree on everything. Even though our examples deal primarily with extensions to the Object Lens system, the analysis also suggests how other kinds of systems, such as EDI applications, might exploit specialization hierarchies of object types to simplify the translation problem.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–26},
numpages = {26}
}

@article{10.1145/76158.76893,
author = {Pernici, B. and Barbic, F. and Maiocchi, R. and Fugini, M. G. and Rames, J. R. and Rolland, C.},
title = {C-TODOS: An Automatic Tool for Office System Conceptual Design},
year = {1989},
issue_date = {Oct. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/76158.76893},
doi = {10.1145/76158.76893},
abstract = {Designers of office information systems, which share various features with information systems and software development, need to carefully consider special issues such as document and communication flows, user roles, user interfaces, and available technology.The ESPRIT Project, Automatic TOols for Designing Office Information Systems (TODOS), proposes an integrated environment for office design with tools for requirements collection and analysis, conceptual design, rapid prototyping, and architecture selection.Conceptual design is a central phase of office system design: It provides correct and complete functional requirements from which the office prototype will be developed and the final architecture chosen. C-TODOS, the conceptual design support tool developed within TODOS, is presented in this paper. The purpose of C-TODOS is to give the designer tools for supporting conceptual modeling activities with the goal of obtaining correct, consistent, and good quality office-functional specifications.This paper presents C-TODOS within the TODOS development environment and describes the basic features of the tool: the TODOS Conceptual Model, the Specification Database, and the Modeling, Query and Consistency Checking Modules. The use of C-TODOS, through illustration of the development of a test case, and possible future research are discussed.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {378–419},
numpages = {42}
}

@article{10.1145/76158.76892,
author = {Afsarmanesh, Hamideh and McLeod, Dennis},
title = {The 3DIS: An Extensible Object-Oriented Information Management Environment},
year = {1989},
issue_date = {Oct. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/76158.76892},
doi = {10.1145/76158.76892},
abstract = {The 3-Dimensional Information Space (3DIS) is an extensible object-oriented framework for information management. It is specifically oriented toward supporting the database requirements for data-intensive information system applications in which (1) information objects of various levels of abstraction and modalities must be accommodated, (2) descriptive and structural information (metadata) is rich and dynamic, and (3) users who are not database experts must be able to design, manipulate, and evolve databases. In response to these needs, the 3DIS provides an approach in which data and the descriptive information about data are handled uniformly in an extensible framework. The 3DIS provides a simple, geometric, and formal representation of data which forms a basis for understanding, defining, and manipulating databases. Several prototype implementations based upon the 3DIS have been designed and implemented and are in experimental use.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {339–377},
numpages = {39}
}

@article{10.1145/76158.76891,
author = {Olson, Margrethe H.},
title = {Work at Home for Computer Professionals: Current Attitudes and Future Prospects},
year = {1989},
issue_date = {Oct. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/76158.76891},
doi = {10.1145/76158.76891},
abstract = {The subject of this paper is work performed in the home with computer and communications technology, also known as telecommuting. The article reports on two studies of work at home: a quasi-experimental field study of organizational telecommuting pilot programs, and an attitude survey comparing computer professionals who work at home to employees doing similar jobs in traditional office settings. The results of the field study demonstrated that working in the home had little impact on employee performance; however, supervisors were not comfortable with remote workers and preferred their employees to be on site. In the survey, work in the home was related to lower job satisfaction, lower organizational commitment, and higher role conflict. The survey also included computer professionals who worked at home in addition to the regular work day. The author suggests that performing additional unpaid work in the home after regular work hours may be an important trend that merits further investigation. The studies demonstrate that while computer and communications technology have the potential to relax constraints on information work in terms of space and time, in today's traditional work environments, corporate culture and management style limit acceptance of telecommuting as a substitute for office work.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {317–338},
numpages = {22}
}

@article{10.1145/65943.65949,
author = {Metzler, Douglas P. and Haas, Stephanie W.},
title = {The Constituent Object Parser: Syntactic Structure Matching for Information Retrieval},
year = {1989},
issue_date = {July 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/65943.65949},
doi = {10.1145/65943.65949},
abstract = {The Constituent Object Parser is a shallow syntactic parser designed to produce dependency tree representations of syntactic structure that can be used to specify the intended meanings of a sentence more precisely than can the key terms of the sentence alone. It is intended to improve the precision/recall performance of information retrieval and similar text processing applications by providing more powerful matching procedures. The dependency tree representation and the relationship between the intended use of this parser and its design is described, and several problems concerning the processing of ambiguous structures are discussed.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {292–316},
numpages = {25}
}

@article{10.1145/65943.65948,
author = {Campagnoni, F. R. and Ehrlich, Kate},
title = {Information Retrieval Using a Hypertext-Based Help System},
year = {1989},
issue_date = {July 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/65943.65948},
doi = {10.1145/65943.65948},
abstract = {Hypertext offers users a simple, flexible way to navigate through electronic information systems but at the potential risk of becoming lost in the network of interconnected pieces of information. A study was conducted on information retrieval using a commercial hypertext-based help system. It was found that the predominant search strategy was “browsing” (characterized by scanning tables of contents and paging through topics), rather than employing the indexes ("analytical search"). Although subjects did not get lost, individuals with better spatial visualization skills, as measured by a standardized test, were faster at retrieving information and returned to the top of the information hierarchy less often than those with poorer spatial visualization skills. These results support previous studies that have found a strong preference by users for browsing in hypertext systems and extend those findings to a new domain (help), a different type of user interface, and a different information architecture. In addition, the results demonstrate the importance of spatial visualization ability for efficient navigation and information retrieval in a hierarchical hypertext system.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {271–291},
numpages = {21}
}

@article{10.1145/65943.65947,
author = {Smith, Philip J. and Shute, Steven J. and Galdes, Beb and Chignell, Mark H.},
title = {Knowledge-Based Search Tactics for an Intelligent Intermediary System},
year = {1989},
issue_date = {July 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/65943.65947},
doi = {10.1145/65943.65947},
abstract = {Research on the nature of knowledge-based systems for bibliographic information retrieval is summarized. Knowledge-based search tactics are then considered in terms of their role in the functioning of a semantically based search system for bibliographic information retrieval, EP-X. This system uses such tactics to actively assist users in defining or refining their topics of interest. It does so by applying these tactics to a knowledge base describing topics in a particular domain and to a database describing the contents of individual documents in terms of these topics. This paper, then, focuses on the two central concepts behind EP-X: semantically based search and knowledge-based search tactics.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {246–270},
numpages = {25}
}

@article{10.1145/65943.65946,
author = {Klein, Shmuel T. and Bookstein, Abraham and Deerwester, Scott},
title = {Storing Text Retrieval Systems on CD-ROM: Compression and Encryption Considerations},
year = {1989},
issue_date = {July 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/65943.65946},
doi = {10.1145/65943.65946},
abstract = {The emergence of the CD-ROM as a storage medium for full-text databases raises the question of the maximum size database that can be contained by this medium. As an example, the problem of storing the Tr\'{e}sor de la Langue Fran\c{c}aise on a CD-ROM is examined in this paper. The text alone of this database is 700 megabytes long, more than a CD-ROM can hold. In addition, the dictionary and concordance needed to access these data must be stored. A further constraint is that some of the material is copyrighted, and it is desirable that such material be difficult to decode except through software provided by the system. Pertinent approaches to compression of the various files are reviewed, and the compression of the text is related to the problem of data encryption: Specifically, it is shown that, under simple models of text generation, Huffman encoding produces a bit-string indistinguishable from a representation of coin flips.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {230–245},
numpages = {16}
}

@article{10.1145/65943.65945,
author = {Raghavan, Vijay and Bollmann, Peter and Jung, Gwang S.},
title = {A Critical Investigation of Recall and Precision as Measures of Retrieval System Performance},
year = {1989},
issue_date = {July 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/65943.65945},
doi = {10.1145/65943.65945},
abstract = {Recall and precision are often used to evaluate the effectiveness of information retrieval systems. They are easy to define if there is a single query and if the retrieval result generated for the query is a linear ordering. However, when the retrieval results are weakly ordered, in the sense that several documents have an identical retrieval status value with respect to a query, some probabilistic notion of precision has to be introduced. Relevance probability, expected precision, and so forth, are some alternatives mentioned in the literature for this purpose. Furthermore, when many queries are to be evaluated and the retrieval results averaged over these queries, some method of interpolation of precision values at certain preselected recall levels is needed. The currently popular approaches for handling both a weak ordering and interpolation are found to be inconsistent, and the results obtained are not easy to interpret. Moreover, in cases where some alternatives are available, no comparative analysis that would facilitate the selection of a particular strategy has been provided. In this paper, we systematically investigate the various problems and issues associated with the use of recall and precision as measures of retrieval system performance. Our motivation is to provide a comparative analysis of methods available for defining precision in a probabilistic sense and to promote a better understanding of the various issues involved in retrieval performance evaluation.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {205–229},
numpages = {25}
}

@article{10.1145/65943.65944,
author = {Fuhr, Norbert},
title = {Optimum Polynomial Retrieval Functions Based on the Probability Ranking Principle},
year = {1989},
issue_date = {July 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/65943.65944},
doi = {10.1145/65943.65944},
abstract = {We show that any approach to developing optimum retrieval functions is based on two kinds of assumptions: first, a certain form of representation for documents and requests, and second, additional simplifying assumptions that predefine the type of the retrieval function. Then we describe an approach for the development of optimum polynomial retrieval functions: request-document pairs (fl, dm) are mapped onto description vectors x(fl, dm), and a polynomial function e(x) is developed such that it yields estimates of the probability of relevance P(R | x (fl, dm) with minimum square errors. We give experimental results for the application of this approach to documents with weighted indexing as well as to documents with complex representations. In contrast to other probabilistic models, our approach yields estimates of the actual probabilities, it can handle very complex representations of documents and requests, and it can be easily applied to multivalued relevance scales. On the other hand, this approach is not suited to log-linear probabilistic models and it needs large samples of relevance feedback data for its application.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {183–204},
numpages = {22}
}

@article{10.1145/65935.65939,
author = {G\"{u}ting, Ralf Hartmut and Zicari, Roberto and Choy, David M.},
title = {An Algebra for Structured Office Documents},
year = {1989},
issue_date = {April 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/65935.65939},
doi = {10.1145/65935.65939},
abstract = {We describe a data model for structured office information objects, which we generically call “documents,” and a practically useful algebraic language for the retrieval and manipulation of such objects. Documents are viewed as hierarchical structures; their layout (presentation) aspect is to be treated separately. The syntax and semantics of the language are defined precisely in terms of the formal model, an extended relational algebra.The proposed approach has several new features, some of which are particularly useful for the management of office information. The data model is based on nested sequences of tuples rather than nested relations. Therefore, sorting and sequence operations and the explicit handling of duplicates can be described by the model. Furthermore, this is the first model based on a many-sorted instead of a one-sorted algebra, which means that atomic data values as well as nested structures are objects of the algebra. As a consequence, arithmetic operations, aggregate functions, and so forth can be treated inside the model and need not be introduced as query language extensions to the model. Many-sorted algebra also allows arbitrary algebra expressions (with Boolean result) to be admitted as selection or join conditions and the results of arbitrary expressions to be embedded into tuples. In contrast to other formal models, this algebra can be used directly as a rich query language for office documents with precisely defined semantics.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {123–157},
numpages = {35}
}

@article{10.1145/65935.65937,
author = {Lee, Dik Lun and Leng, Chun-Wu},
title = {Partitioned Signature Files: Design Issues and Performance Evaluation},
year = {1989},
issue_date = {April 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/65935.65937},
doi = {10.1145/65935.65937},
abstract = {A signature file acts as a filtering mechanism to reduce the amount of text that needs to be searched for a query. Unfortunately, the signature file itself must be exhaustively searched, resulting in degraded performance for a large file size. We propose to use a deterministic algorithm to divide a signature file into partitions, each of which contains signatures with the same “key.” The signature keys in a partition can be extracted and represented as the partition's key. The search can then be confined to the subset of partitions whose keys match the query key. Our main concern here is to study methods for obtaining the keys and their performance in terms of their ability to reduce the search space.Owing to the reduction of search space, partitioning a signature file has a direct benefit in a sequential search (single-processor) environment. In a parallel environment, search can be conducted in parallel effectively by allocating one or more partitions to a processor. Partitioning the signature tile with a deterministic method (as opposed to a random partitioning scheme) provides intraquery parallelism as well as interquery parallelism.In this paper, we outline the criteria for evaluating partitioning schemes. Three algorithms are described and studied. An analytical study of the performance of the algorithms is provided and the results are verified with simulation.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {158–180},
numpages = {23}
}

@article{10.1145/65935.65936,
author = {Sciore, Edward},
title = {Object Specialization},
year = {1989},
issue_date = {April 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/65935.65936},
doi = {10.1145/65935.65936},
abstract = {Specialization hierarchies typically are treated as type-level constructs and are used to define various inheritance mechanisms. In this paper we consider specialization at the level of objects. We show that doing so creates a more flexible and powerful notion of inheritance by allowing objects to define their own inheritance path. Object specialization can also be used to model certain forms of versioning, implement data abstraction, and provide a “classless” prototype-based language interface to the user.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {103–122},
numpages = {20}
}

@article{10.1145/64789.64993,
author = {Tompa, Frank WM.},
title = {A Data Model for Flexible Hypertext Database Systems},
year = {1989},
issue_date = {Jan. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/64789.64993},
doi = {10.1145/64789.64993},
abstract = {Hypertext and other page-oriented databases cannot be schematized in the same manner as record-oriented databases. As a result, most hypertext databases implicitly employ a data model based on a simple, unrestricted graph. This paper presents a hypergraph model for maintaining page-oriented databases in such a way that some of the functionality traditionally provided by database schemes can be available to hypertext databases. In particular, the model formalizes identification of commonality in the structure, set-at-a-time database access, and definition of user-specific views. An efficient implementation of the model is also discussed.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {85–100},
numpages = {16}
}

@article{10.1145/64789.64992,
author = {Utting, Kenneth and Yankelovich, Nicole},
title = {Context and Orientation in Hypermedia Networks},
year = {1989},
issue_date = {Jan. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/64789.64992},
doi = {10.1145/64789.64992},
abstract = {The core of hypermedia's power lies in the complex networks of links that can be created within and between documents. However, these networks frequently overwhelm the user and become a source of confusion. Within Intermedia, we have developed the Web View-a tool for viewing and navigating such networks with a minimum of user confusion and disorientation. The key factors in the Web View's success are a display that combines a record of the user's path through the network with a map of the currently available links; a scope line that summarizes the number of documents and links in the network; and a set of commands that permit the user to open documents directly from the Web View.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {58–84},
numpages = {27}
}

@article{10.1145/64789.64791,
author = {Stotts, P. David and Furuta, Richard},
title = {Petri-Net-Based Hypertext: Document Structure with Browsing Semantics},
year = {1989},
issue_date = {Jan. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/64789.64791},
doi = {10.1145/64789.64791},
abstract = {We present a formal definition of the Trellis model of hypertext and describe an authoring and browsing prototype called αTrellis that is based on the model. The Trellis model not only represents the relationships that tie individual pieces of information together into a document (i.e., the adjacencies), but specifies the browsing semantics to be associated with the hypertext as well (i.e., the manner in which the information is to be visited and presented). The model is based on Petri nets, and is a generalization of existing directed graph-based forms of hypertext. The Petri net basis permits more powerful specification of what is to be displayed when a hypertext is browsed and permits application of previously developed Petri net analysis techniques to verify properties of the hypertext. A number of useful hypertext constructs, easily described in the Trellis model, are presented. These include the synchronization of simultaneous traversals of separate paths through a hypertext, the incorporation of access controls into a hypertext (i.e., specifying nodes that can be proven to be accessible only to certain classes of browsers), and construction of multiple specialized (tailored) versions from a single hypertext.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {3–29},
numpages = {27}
}

@article{10.1145/64789.64790,
author = {Egan, Dennis E. and Remde, Joel R. and Gomez, Louis M. and Landauer, Thomas K. and Eberhardt, Jennifer and Lochbaum, Carol C.},
title = {Formative Design Evaluation of Superbook},
year = {1989},
issue_date = {Jan. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/64789.64790},
doi = {10.1145/64789.64790},
abstract = {SuperBook is a hypertext browsing system designed to improve the usability of conventional documents. Successive versions of SuperBook were evaluated in a series of behavioral studies. Students searched for information in a statistics text. presented either in conventional printed form or in SuperBook form. The best version of SuperBook enabled students to answer search questions more quickly and accurately than they could with the conventional text. Students wrote higher quality “open-book” essays using SuperBook than they did with the conventional text, and their subjective ratings of the documentation strongly favored SuperBook.This work is a case study of formative design-evaluation. Behavioral evaluation of the first version of SuperBook showed how design factors and user strategies affected search and established baseline performance measures with printed text. The second version of SuperBook was implemented with the goal of improving search accuracy and speed. User strategies that had proved effective in the first study were made very easy and attractive to use. System response time for common operations was greatly improved. Behavioral evaluation of the new SuperBook demonstrated its superiority to printed text and suggested additional improvements that were incorporated into “MiteyBook,” a SuperBook implementation for PC-size screens. Search with MiteyBook proved to be approximately 25 percent faster and 25 percent more accurate than that obtained with a conventional printed book.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {30–57},
numpages = {28}
}

@article{10.1145/58566.59299,
author = {Trigg, Randall H.},
title = {Guided Tours and Tabletops: Tools for Communicating in a Hypertext Environment},
year = {1988},
issue_date = {Oct. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/58566.59299},
doi = {10.1145/58566.59299},
abstract = {The author of a complex hypertext document is often faced with the problem of conveying the document's meaning to future readers through a shared computer environment. Two tools implemented in the NoteCards hypertext environment, guided tours and tabletops, allow authors to employ annotation, graphic layout, and ordered presentation when communicating to readers. This paper describes these tools and gives examples of their use. Issues of remote pointing arising from an application in legal argumentation are discussed as well as early work on the use of these tools to support sharing of hypertext strategies among NoteCards users.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {398–414},
numpages = {17}
}

@article{10.1145/58566.59298,
author = {Lai, Kum-Yew and Malone, Thomas W. and Yu, Keh-Chiang},
title = {Object Lens: A “Spreadsheet” for Cooperative Work},
year = {1988},
issue_date = {Oct. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/58566.59298},
doi = {10.1145/58566.59298},
abstract = {Object Lens allows unsophisticated computer users to create their own cooperative work applications using a set of simple, but powerful, building blocks. By defining and modifying templates for various semistructured objects, users can represent information about people, tasks, products, messages, and many other kinds of information in a form that can be processed intelligently by both people and their computers. By collecting these objects in customizable folders, users can create their own displays which summarize selected information from the objects in table or tree formats. Finally, by creating semiautonomous agents, users can specify rules for automatically processing this information in different ways at different times.The combination of these primitives provides a single consistent interface that integrates facilities for object-oriented databases, hypertext, electronic messaging, and rule-based intelligent agents. To illustrate the power of this combined approach, we describe several simple examples of applications (such as task tracking, intelligent message routing, and database retrieval) that we have developed in this framework.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {332–353},
numpages = {22}
}

@article{10.1145/58566.59297,
author = {Conklin, Jeff and Begeman, Michael L.},
title = {GIBIS: A Hypertext Tool for Exploratory Policy Discussion},
year = {1988},
issue_date = {Oct. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/58566.59297},
doi = {10.1145/58566.59297},
abstract = {This paper describes an application-specific hypertext system designed to facilitate the capture of early design deliberations. It implements a specific method, called Issue Based Information Systems (IBIS), which has been developed for use on large, complex design problems. The hypertext system described here, gIBIS (for graphical IBIS), makes use of color and a high-speed relational database server to facilitate building and browsing typed IBIS networks. Further, gIBIS is designed to support the collaborative construction of these networks by any number of cooperating team members spread across a local area network. Early experiments suggest that the IBIS method is still incomplete, but there is a good match between the tool and method even in this experimental version.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {303–331},
numpages = {29}
}

@article{10.1145/58566.58568,
author = {Eveland, J. D. and Bikson, T. K.},
title = {Work Group Structures and Computer Support: A Field Experiment},
year = {1988},
issue_date = {Oct. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/58566.58568},
doi = {10.1145/58566.58568},
abstract = {It is frequently suggested that work groups that have computer technology to support activities such as text editing, data manipulation, and communication develop systematically different structures and working processes from groups that rely on more conventional technologies such as memos, phone calls, and meetings. However, cross-sectional or retrospective research designs do not allow this hypothesis to be tested with much power. This field experiment created two task forces, each composed equally of recently retired employees and employees still at work but eligible to retire. They were given the identical tasks of preparing reports for their company on retirement planning issues, but they were randomly assigned to different technology conditions. One group had full conventional office support; the other had, in addition, networked microcomputers with electronic mail and routine office software. Structured interviews were conducted four times during the year-long project; in addition, electronic mail activity was logged in the on-line group. Although both groups produced effective reports, the two differed significantly in the kind of work they produced, the group structures that emerged, and evaluations of their own performance. Although the standard group was largely dominated by the employees through the extensive reliance on informal meetings, the electronic technology used by the other task force allowed the retirees to exercise primary leverage. We conclude that use of computer support for cooperative work results in both quantitative and qualitative changes but that effective participation in such electronically supported groups requires significant investments of time and energy on the part of its members to master the technology and a relatively high level of assistance during the learning process.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {354–379},
numpages = {26}
}

@article{10.1145/58566.58567,
author = {Mackay, Wendy E.},
title = {Diversity in the Use of Electronic Mail: A Preliminary Inquiry},
year = {1988},
issue_date = {Oct. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/58566.58567},
doi = {10.1145/58566.58567},
abstract = {This paper describes a series of interviews that examine the ways that professional office workers use electronic mail to manage their daily work. The purpose is to generate hypotheses for future research. A number of implications for the design of flexible mail systems are discussed.Two principal claims are made. First, the use of electronic mail is strikingly diverse, although not infinitely so. Individuals vary both in objective measures of mail use and in preferred strategies for managing work electronically. Feelings of control are similarly diverse and are related to the size of the user's inbox, numbers of folders, and subscriptions to distribution lists. This diversity implies that one's own experiences with electronic mail are unlikely to provide sufficient understanding of other's uses of mail. Mail designers should thus seek flexible primitives that capture the important dimensions of use and provide flexibility for a wide range of users.The second claim is that electronic mail is more than just a communication system. Users archive messages for subject retrieval, prioritize messages to sequence work activities, and delegate tasks via mail. A taxonomy of work management is proposed in which mail is used for information management, time management, and task management activities. Directions for future research are suggested.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {380–397},
numpages = {18}
}

@article{10.1145/45945.48028,
author = {Neches, Robert},
title = {Knowledge-Based Tools to Promote Shared Goals and Terminology between Interface Designers},
year = {1988},
issue_date = {July 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/45945.48028},
doi = {10.1145/45945.48028},
abstract = {Two tools that support cooperation are described: one for the construction of consistent and principled human-computer interfaces and the other for the construction of AI knowledge bases. These tools provide a central repository for design knowledge that otherwise would not be easily shared among users. The AI knowledge representation technology upon which the tools are founded is first described. A knowledge-based approach to interface construction is discussed, and how that approach applies to detecting design conflicts and inconsistencies stemming from two different kinds of team communication failure is illustrated. Next, a knowledge acquisition aid that is utilized within the interface construction paradigm and that also illustrates the same approach to supporting cooperative work is described. Finally, four sources of difficulty in team design efforts, which this approach seeks to address, are reviewed.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {215–231},
numpages = {17}
}

@article{10.1145/45945.48027,
author = {Motro, Amihai},
title = {VAGUE: A User Interface to Relational Databases That Permits Vague Queries},
year = {1988},
issue_date = {July 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/45945.48027},
doi = {10.1145/45945.48027},
abstract = {A specific query establishes a rigid qualification and is concerned only with data that match it precisely. A vague query establishes a target qualification and is concerned also with data that are close to this target. Most conventional database systems cannot handle vague queries directly, forcing their users to retry specific queries repeatedly with minor modifications until they match data that are satisfactory. This article describes a system called VAGUE that can handle vague queries directly. The principal concept behind VAGUE is its extension to the relational data model with data metrics, which are definitions of distances between values of the same domain. A problem with implementing data distances is that different users may have different interpretations for the notion of distance. VAGUE incorporates several features that enable it to adapt itself to the individual views and priorities of its users.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {187–214},
numpages = {28}
}

@article{10.1145/45945.45946,
author = {Pahlavan, K.},
title = {Wireless Intraoffice Networks},
year = {1988},
issue_date = {July 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/45945.45946},
doi = {10.1145/45945.45946},
abstract = {An overview of the existing and growing demands for wireless office information networks is provided, and the existing research activities are assessed in some detail. The radio frequency (RF) and infrared (IR) communication technologies are examined as candidates for wireless intraoffice communications. The available bandwidths, according to federal regulations and characteristics of the channel for RF communications, are given. Digital narrow-band and wideband spread-spectrum RF communications are assessed in terms of supportable data rate or number of simultaneous users in one cell of a cellular architecture in an office environment. Various limitations of IR communications are discussed and existing systems and architectures are reviewed.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {277–302},
numpages = {26}
}

@article{10.1145/45945.214327,
author = {Pollock, Stephen},
title = {A Rule-Based Message Filtering System},
year = {1988},
issue_date = {July 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/45945.214327},
doi = {10.1145/45945.214327},
abstract = {Much computerized support for knowledge workers has consisted of tools to handle low-level functions such as distribution, storage, and retrieval of information. However, the higher level processes of making decisions and taking actions with respect to this information have not been supported to the same degree. This paper describes the ISCREEN prototype system for screening text messages. ISCREEN includes a high-level interface for users to define rules, a component that screens text messages, and a conflict detection component that examines rules for inconsistencies. An explanation component uses text generation to answer user queries about past or potential system actions based on Grice's conversational maxims.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {232–254},
numpages = {23}
}

@article{10.1145/45945.214325,
author = {Rice, Ronald E. and Shook, Douglas E.},
title = {Access to, Usage of, and Outcomes from an Electronic Messaging System},
year = {1988},
issue_date = {July 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/45945.214325},
doi = {10.1145/45945.214325},
abstract = {This study examines relationships among perceived accessibility to an electronic messaging system (EMS), computer-monitored and reported usage of the system by approximately 100 employees of one division of an aerospace firm, user's job type, perceived appropriateness of the EMS, and reported outcomes such as changes in effectiveness and use of paper-based media. Greater accessibility resulted in more usage and reported increases in effectiveness. Physical distance to a terminal affects the associations of other aspects of accessibility with usage and has a greater influence on these associations earlier in one's adoption process. Differences in job type showed statistically significant associations with usage, independent of the influence of accessibility. Computer-monitored and reported usage measures were only moderately correlated and were differentially associated with the access measures and with the two outcomes. The article ends by discussing implications for implementation and evaluation of computer-based communication systems, theories of media characteristics and information value, and methodological issues in using computer-monitored usage data.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {255–276},
numpages = {22}
}

@article{10.1145/45941.45944,
author = {Lee, Ronald M.},
title = {Bureaucracies as Deontic Systems},
year = {1988},
issue_date = {April 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/45941.45944},
doi = {10.1145/45941.45944},
abstract = {Bureaucratic offices are not only for clerical work, but more important, they are for officiating in the sense of issuing directives, granting permissions, enforcing prohibitions, waiving obligations, and so forth. Bureaucracies are thus deontic systems for organizational and social control. Conventional information processing approaches are inadequate for capturing these aspects of bureaucratic modeling. A logic-based representation that emphasizes deontic and performative aspects is proposed.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {87–108},
numpages = {22}
}

@article{10.1145/45941.45943,
author = {Flores, Fernando and Graves, Michael and Hartfield, Brad and Winograd, Terry},
title = {Computer Systems and the Design of Organizational Interaction},
year = {1988},
issue_date = {April 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/45941.45943},
doi = {10.1145/45941.45943},
abstract = {The goal of this paper is to relate theory to invention and application in the design of systems for organizational communication and management. We propose and illustrate a theory of design, technology, and action that we believe has been missing in the mainstream of work on office systems. At the center of our thinking is a theory of language as social action, which differs from the generally taken-for-granted understandings of what goes on in an organization. This approach has been presented elsewhere, and our aim here is to examine its practical implications and assess its effectiveness in the design of The Coordinator, a workgroup productivity system that is in widespread commercial use on personal computers.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {153–172},
numpages = {20}
}

@article{10.1145/45941.45942,
author = {Holt, Anatol W.},
title = {Diplans: A New Language for the Study and Implementation of Coordination},
year = {1988},
issue_date = {April 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/45941.45942},
doi = {10.1145/45941.45942},
abstract = {In this paper the reader is introduced to coordination in the workplace as an object of scientific study and computer automation. Diplans are the expressions of a new graphical language used to describe plans of operation in human organizations. With diplans, systems of constraint, which may or may not take the form of procedure definitions, can be specified. Among the special strengths of diplans is their ability to render explicit the interactive aspects of complex work distributed over many people and places—in other words, coordination. Diplans are central to coordination technology, a new approach to developing support for cooperative work on heterogeneous computer networks.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {109–125},
numpages = {17}
}

@article{10.1145/45941.383895,
author = {Suchman, Lucy},
title = {Designing with the User: Book Review of <i>Computers and Democracy: A Scandinavian Challenge,</i> G. Bjerknes, P. Ehn, and M. Kyng, Eds. Gower Press, Brookfield, VT, 1987},
year = {1988},
issue_date = {April 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/45941.383895},
doi = {10.1145/45941.383895},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {173–183},
numpages = {11}
}

@article{10.1145/45941.214328,
author = {Auram\"{a}ki, Esa and Lehtinen, Erkki and Lyytinen, Kalle},
title = {A Speech-Act-Based Office Modeling Approach},
year = {1988},
issue_date = {April 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/45941.214328},
doi = {10.1145/45941.214328},
abstract = {In this paper methods and principles that help to analyze offices as systems of communicative action are explored. In communicative action, office agents create commitments through symbolic means. A SAMPO (Speech-Act-based office Modeling aPprOach), which studies office activities as a series of speech acts creating, maintaining, modifying, reporting, and terminating commitments, is presented. The main steps and methods in the office system specification are outlined and their application illustrated through a simple example. In the final section advantages and disadvantages in the SAMPO are noted and some research directions for the future are suggested.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {126–152},
numpages = {27}
}

@article{10.1145/42279.45947,
author = {Croft, W. Bruce and Savino, Pasquale},
title = {Implementing Ranking Strategies Using Text Signatures},
year = {1988},
issue_date = {Jan. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/42279.45947},
doi = {10.1145/42279.45947},
abstract = {Signature files provide an efficient access method for text in documents, but retrieval is usually limited to finding documents that contain a specified Boolean pattern of words. Effective retrieval requires that documents with similar meanings be found through a process of plausible inference. The simplest way of implementing this retrieval process is to rank documents in order of their probability of relevance. In this paper techniques are described for implementing probabilistic ranking strategies with sequential and bit-sliced signature tiles and the limitations of these implementations with regard to their effectiveness are pointed out. A detailed comparison is made between signature-based ranking techniques and ranking using term-based document representatives and inverted files. The comparison shows that term-based representations are at least competitive (in terms of efficiency) with signature files and, in some situations, superior.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {42–62},
numpages = {21}
}

@article{10.1145/42279.42281,
author = {Bertino, Elisa and Rabbiti, Fausto and Gibbs, Simon},
title = {Query Processing in a Multimedia Document System},
year = {1988},
issue_date = {Jan. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/42279.42281},
doi = {10.1145/42279.42281},
abstract = {Query processing in a multimedia document system is described. Multimedia documents are information objects containing formatted data, text, image, graphics, and voice. The query language is based on a conceptual document model that allows the users to formulate queries on both document content and structure. The architecture of the system is outlined, with focus on the storage organization in which both optical and magnetic devices can coexist. Query processing and the different strategies evaluated by our optimization algorithm are discussed.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–41},
numpages = {41}
}

@article{10.1145/42279.42280,
author = {Postel, Jonathan B. and Finn, Gregory G. and Katz, Alan R. and Reynolds, Joyce K.},
title = {An Experimental Multimedia Mail System},
year = {1988},
issue_date = {Jan. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/42279.42280},
doi = {10.1145/42279.42280},
abstract = {A computer-based experimental multimedia mail system that allows the user to read, create, edit, send, and receive messages containing text, images, and voice is discussed.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {63–81},
numpages = {19}
}

@article{10.1145/42196.42266,
author = {Clement, Andrew and Gotlieb, C. C.},
title = {Evolution of an Organizational Interface: The New Business Department at a Largeinsurance Firm},
year = {1987},
issue_date = {Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/42196.42266},
doi = {10.1145/42196.42266},
abstract = {This paper describes how the work organization and computer system of the New Business Department at a large life insurance firm have interacted and evolved over time. The dynamics of interaction are explained largely in terms of the economic incentive to reduce the length of transaction-processing chains and the more political goal of extending managerial control. It is argued that examining the interaction of organizations and computer systems can contribute to a better theoretical understanding of the development of large computer systems and offer guidance to designers of user-computer interfaces. A graphical technique for depicting organizational interfaces is presented.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {328–339},
numpages = {12}
}

@article{10.1145/42196.42246,
author = {Rada, Roy and Martin, Brian K.},
title = {Augmenting Thesauri for Information Systems},
year = {1987},
issue_date = {Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/42196.42246},
doi = {10.1145/42196.42246},
abstract = {A thesaurus can be a critical component of an office information system. Access to various sets of documents can be facilitated by thesauri and by the connections that are made among thesauri. In the projects described in this paper, the thesauri are stored and manipulated through a relational database management system. The system detects inheritance properties in a thesaurus and uses them to guide a human expert in decisions about how to augment the thesaurus. New strategies will extend our ability to augment existing thesauri.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {378–392},
numpages = {15}
}

@article{10.1145/42196.42200,
author = {Whang, Kyu-Young and Ammann, Art and Bolmarcich, Anthony and Hanrahan, Maria and Hochgesang, Guy and Huang, Kuan-Tsae and Khorasani, Al and Krishnamurthy, Ravi and Sockut, Gary and Sweeney, Paula and Waddle, Vance and Zloof, Mosh\'{e}},
title = {Office-by-Example: An Integrated Office System and Database Manager},
year = {1987},
issue_date = {Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/42196.42200},
doi = {10.1145/42196.42200},
abstract = {Office-by-Example (OBE) is an integrated office information system that has been under development at IBM Research. OBE, an extension of Query-by-Example, supports various office features such as database tables, word processing, electronic mail, graphics, images, and so forth. These seemingly heterogeneous features are integrated through a language feature called example elements. Applications involving example elements are processed by the database manager, an integrated part of the OBE system. In this paper we describe the facilities and architecture of the OBE system and discuss the techniques for integrating heterogeneous objects.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {393–427},
numpages = {35}
}

@article{10.1145/42196.42199,
author = {Gould, John D. and Salaun, Josiane},
title = {Behavioral Experiments on Handmarkings},
year = {1987},
issue_date = {Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/42196.42199},
doi = {10.1145/42196.42199},
abstract = {Handmarkings or handwritten editing marks can be used as direct editing commands to an interactive computer system. Five exploratory experiments studied the potential value of handmarkings for editing text and pictures, as well as for some specific results. Circles are the most frequently used scoping mark, and arrows are the most frequently used operator and target indicators. Experimental comparisons showed that handmarkings have the potential to be faster than keyboards and mice for editing tasks. Their ultimate value will, however, depend on the style and details of their user-interface implementation.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {358–377},
numpages = {20}
}

@article{10.1145/42196.42198,
author = {Ehrlich, Susan F.},
title = {Strategies for Encouraging Successful Adoption of Office Communication Systems},
year = {1987},
issue_date = {Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/42196.42198},
doi = {10.1145/42196.42198},
abstract = {The adoption of new computer communication systems into organizations requires behavioral change. Planning for successful adoption requires knowledge of individual organizational communication patterns and the relationship between those patterns and particular communication system solutions. This paper documents a sequence of studies of organizational communication. Needs for office communication systems were identified, as were social and psychological factors temporarily inhibiting their use. Strategies for assuring smooth adoption of such systems are highlighted.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {340–357},
numpages = {18}
}

@article{10.1145/42196.42197,
author = {Kaye, A. Roger and Karam, Gerald M.},
title = {Cooperating Knowledge-Based Assistants for the Office},
year = {1987},
issue_date = {Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/42196.42197},
doi = {10.1145/42196.42197},
abstract = {This paper presents an approach to high-level support of office workers by embedding office knowledge in a network of distributed cooperating knowledge-based or expert “assistants” and servers. These knowledge-based systems incorporate both factual and procedural knowledge and are capable of making use of existing conventional office technology. They constitute a form of computer-supported cooperative work. We describe a common architecture for our assistants and servers that incorporates several key features. Our systems are capable of supporting concurrent multiple consultations or tasks and have facilities for the interruption and resumption of consultations as appropriate. The various assistants and servers, which may reside on different machines, cooperate in solving problems or completing tasks by passing messages. We propose a taxonomy of the general office knowledge normally used by office workers, together with a frame and rule-based knowledge representation scheme. We also describe an experimental system, written in PROLOG, that incorporates the above design principles.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {297–326},
numpages = {30}
}

@article{10.1145/27641.28059,
author = {Sassone, Peter G.},
title = {Cost-Benefit Methodology for Office Systems},
year = {1987},
issue_date = {July 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/27641.28059},
doi = {10.1145/27641.28059},
abstract = {The time savings times salary (TSTS) approach is a widely used methodology for the financial justification of office information systems, yet its theoretical basis is largely unexplored. In this paper, we identify its underlying economic model, including five critical assumptions. We find that the model, though somewhat restrictive, is not unreasonable. However, we find that the time-saving-times-salary calculation, per se, is implicitly based on a very particular assumption about how saved time will be used. This assumption has neither a behavioral nor normative basis, and we conclude that the TSTS calculation is not meaningful in most cases. An alternate approach, the hedonic wage model, is proposed. This model overcomes most of the deficiencies of the TSTS approach, although it has somewhat greater data requirements and computational complexity. A case study illustrating the use of the hedonic wage model is presented.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {273–289},
numpages = {17}
}

@article{10.1145/27641.28058,
author = {Brown, Polly S. and Gould, John D.},
title = {An Experimental Study of People Creating Spreadsheets},
year = {1987},
issue_date = {July 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/27641.28058},
doi = {10.1145/27641.28058},
abstract = {Nine experienced users of electronic spreadsheets each created three spreadsheets. Although participants were quite confident that their spreadsheets were accurate, 44 percent of the spreadsheets contained user-generated programming errors. With regard to the spreadsheet creation process, we found that experienced spreadsheet users spend a large percentage of their time using the cursor keys, primarily for the purpose of moving the cursor around the spreadsheet. Users did not spend a lot of time planning before launching into spreadsheet creation, nor did they spend much time in a separate, systematic debugging stage. Participants spent 21 percent of their time pausing, presumably reading and/or thinking, prior to the initial keystrokes of spreadsheet creation episodes.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {258–272},
numpages = {15}
}

@article{10.1145/27641.28057,
author = {Faloutsos, Christos and Christodoulakis, Stavros},
title = {Description and Performance Analysis of Signature File Methods for Office Filing},
year = {1987},
issue_date = {July 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/27641.28057},
doi = {10.1145/27641.28057},
abstract = {Signature files have attracted a lot of interest as an access method for text and specifically for messages in the office environment. Messages are stored sequentially in the message file, whereas their hash-coded abstractions (signatures) are stored sequentially in the signature file. To answer a query, the signature file is examined first, and many nonqualifying messages are immediately rejected. In this paper we examine the problem of designing signature extraction methods and studying their performance. We describe two old methods, generalize another one, and propose a new method and its variation. We provide exact and approximate formulas for the dependency between the false drop probability and the signature size for all the methods, and we show that the proposed method (VBC) achieves approximately ten times smaller false drop probability than the old methods, whereas it is well suited for collections of documents with variable document sizes.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {237–257},
numpages = {21}
}

@article{10.1145/27641.27642,
author = {Chang, Shi-Huo and Leung, L.},
title = {A Knowledge-Based Message Management System},
year = {1987},
issue_date = {July 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/27641.27642},
doi = {10.1145/27641.27642},
abstract = {The design approach of a knowledge-based message management system is described. A linguistic message filter is used to filter out junk messages. Relevant messages are then processed by an expert system, driven by user-defined alerter rules. An alerter rule base for a secretarial office is illustrated. Further research topics in knowledge-base design, evaluation, and learning are also discussed.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {213–236},
numpages = {24}
}

@article{10.1145/27636.28056,
author = {Stefik, M. and Bobrow, D. G. and Foster, G. and Lanning, S. and Tatar, D.},
title = {WYSIWIS Revised: Early Experiences with Multiuser Interfaces},
year = {1987},
issue_date = {April 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/27636.28056},
doi = {10.1145/27636.28056},
abstract = {WYSIWIS (What You See Is What I See) is a foundational abstraction for multiuser interfaces that expresses many of the characteristics of a chalkboard in face-to-face meetings. In its strictest interpretation, it means that everyone can also see the same written information and also see where anyone else is pointing. In our attempts to build software support for collaboration in meetings, we have discovered that WYSIWIS is crucial, yet too inflexible when strictly enforced. This paper is about the design issues and choices that arose in our first generation of meeting tools based on WYSIWIS. Several examples of multiuser interfaces that start from this abstraction are presented. These tools illustrate that there are inherent conflicts between the needs of a group and the needs of individuals, since user interfaces compete for the same display space and meeting time. To help minimize the effect of these conflicts, constraints were relaxed along four key dimensions of WYSIWIS: display space, time of display, subgroup population, and congruence of view. Meeting tools must be designed to support the changing needs of information sharing during process transitions, as subgroups are formed and dissolved, as individuals shift their focus of activity, and as the group shifts from multiple parallel activities to a single focused activity and back again.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {147–167},
numpages = {21}
}

@article{10.1145/27636.27640,
author = {Greif, Irene and Sarin, Sunil},
title = {Data Sharing in Group Work},
year = {1987},
issue_date = {April 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/27636.27640},
doi = {10.1145/27636.27640},
abstract = {Data sharing is fundamental to computer-supported cooperative work: People share information through explicit communication channels and through their coordinated use of shared databases. This paper examines the data management requirements of group work applications on the basis of experience with three prototype systems and on observations from the literature. Database and object management technologies that support these requirements are briefly surveyed, and unresolved issues in the particular areas of access control and concurrency control are identified for future research.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {187–211},
numpages = {25}
}

@article{10.1145/27636.27639,
author = {Delisle, Norman M. and Schwartz, Mayer D.},
title = {Contexts—a Partitioning Concept for Hypertext},
year = {1987},
issue_date = {April 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/27636.27639},
doi = {10.1145/27636.27639},
abstract = {Hypertext systems provide good information management support for a wide variety of documentation efforts. These efforts range from developing software to writing a book. However, existing hypertext systems provide poor support for collaboration among teams of authors. This paper starts by briefly describing properties of several existing hypertext systems. Then several models for forming partitions in a hypertext database are examined and contexts, a partitioning scheme that supports multiperson cooperative efforts, are introduced. The semantic issues involved in defining contexts are explored in detail.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {168–186},
numpages = {19}
}

@article{10.1145/27636.27638,
author = {Cook, Peter and Ellis, Clarence and Graf, Mike and Rein, Gail and Smith, Tom},
title = {Project Nick: Meetings Augmentation and Analysis},
year = {1987},
issue_date = {April 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/27636.27638},
doi = {10.1145/27636.27638},
abstract = {The Software Technology Program of MCC is investigating the early part of the design process, before requirements are established, for large-scale distributed systems. Face-to-face meetings are an important activity during this phase of a project since they provide a medium for direction, exploration, and consensus building. Project Nick is attempting to apply automated facilities to the process, conduct, and semantic capture of design meetings. Primary topics covered in this paper are meeting analysis, meeting augmentation, and a model of meeting progression that serves as the framework for our work.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {132–146},
numpages = {15}
}

@article{10.1145/27636.27637,
author = {Malone, Thomas W. and Grant, Kenneth R. and Lai, Kum-Yew and Rao, Ramana and Rosenblitt, David},
title = {Semistructured Messages Are Surprisingly Useful for Computer-Supported Coordination},
year = {1987},
issue_date = {April 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/27636.27637},
doi = {10.1145/27636.27637},
abstract = {This paper argues that using a set of semistructured message templates is surprisingly helpful in designing a variety of computer-based communication and coordination systems. Semistructured messages can help provide automatic aids for (1) composing messages to be sent, (2) selecting, sorting, and prioritizing messages that are received, (3) responding automatically to some messages, and (4) suggesting likely responses to other messages. The use of these capabilities is illustrated in a range of applications including electronic mail, computer conferencing, calendar management, and task tracking. The applications show how ideas from artificial intelligence (such as inheritance and production rules) and ideas from user interface design (such as interactive graphical editors) can be combined in novel ways for dealing with semistructured messages. The final part of the paper discusses how communities can evolve a useful set of message type definitions.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {115–131},
numpages = {17}
}

@article{10.1145/22890.23001,
author = {Tsichritzis, D. and Fiume, E. and Gibbs, S. and Nierstrasz, O.},
title = {KNOs: KNowledge Acquisition, Dissemination, and Manipulation Objects},
year = {1987},
issue_date = {Jan. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/22890.23001},
doi = {10.1145/22890.23001},
abstract = {Most object-oriented systems lack two useful facilities: the ability of objects to migrate to new environments and the ability of objects to acquire new operations dynamically. This paper proposes Knos, an object-oriented environment that supports these actions. Knos' operations, data structures, and communication mechanisms are discussed. Knos objects “learn” by exporting and importing new or modified operations. The use of such objects as intellectual support tools is outlined. In particular, various applications involving cooperation, negotiation, and apprenticeship among objects are described.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {96–112},
numpages = {17}
}

@article{10.1145/22890.23000,
author = {Purdy, Alan and Schuchardt, Bruce and Maier, David},
title = {Integrating an Object Server with Other Worlds},
year = {1987},
issue_date = {Jan. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/22890.23000},
doi = {10.1145/22890.23000},
abstract = {Object-oriented database servers are beginning to appear on the commercial market in response to a demand by application developers for increased modeling power in database systems. Before these new servers can enhance the productivity of application designers, systems designers must provide simple interfaces to them from both procedural and object-oriented languages. This paper first describes a successful interface between an object server and two procedural languages (C and Pascal). Because C and Pascal do not support the object-oriented paradigm application, designers using these languages must deal with database objects in less than natural ways. Fortunately, workstations supporting object-oriented languages have the potential for interacting with database objects in a much more integrated manner. To integrate these object-oriented workstations with an object server, we provide a design framework based on the notion of workstation agent objects representing principal objects in the database. We distinguish two types of agents: proxies, which forward most messages to the principal objects, and deputies, which can cache state for their principal and act with more autonomy. The interaction of cache, transaction, and message management strategies makes the implementation of deputies a nontrivial problem. The agent metaphor is being used currently to integrate an object server with a Smalltalk-8O™ workstation.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {27–47},
numpages = {21}
}

@article{10.1145/22890.22945,
author = {Banerjee, Jay and Chou, Hong-Tai and Garza, Jorge F. and Kim, Won and Woelk, Darrell and Ballou, Nat and Kim, Hyoung-Joo},
title = {Data Model Issues for Object-Oriented Applications},
year = {1987},
issue_date = {Jan. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/22890.22945},
doi = {10.1145/22890.22945},
abstract = {Presented in this paper is the data model for ORION, a prototype database system that adds persistence and sharability to objects created and manipulated in object-oriented applications. The ORION data model consolidates and modifies a number of major concepts found in many object-oriented systems, such as objects, classes, class lattice, methods, and inheritance. These concepts are reviewed and three major enhancements to the conventional object-oriented data model, namely, schema evolution, composite objects, and versions, are elaborated upon. Schema evolution is the ability to dynamically make changes to the class definitions and the structure of the class lattice. Composite objects are recursive collections of exclusive components that are treated as units of storage, retrieval, and integrity enforcement. Versions are variations of the same object that are related by the history of their derivation. These enhancements are strongly motivated by the data management requirements of the ORION applications from the domains of artificial intelligence, computer-aided design and manufacturing, and office information systems with multimedia documents.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {3–26},
numpages = {24}
}

@article{10.1145/22890.22891,
author = {Hornick, Mark F. and Zdonik, Stanley B.},
title = {A Shared, Segmented Memory System for an Object-Oriented Database},
year = {1987},
issue_date = {Jan. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/22890.22891},
doi = {10.1145/22890.22891},
abstract = {This paper describes the basic data model of an object-oriented database and the basic architecture of the system implementing it. In particular, a secondary storage segmentation scheme and a transaction-processing scheme are discussed. The segmentation scheme allows for arbitrary clustering of objects, including duplicates. The transaction scheme allows for many different sharing protocols ranging from those that enforce serializability to those that are nonserializable and require communication with the server only on demand. The interaction of these two features is described such that segment-level transfer and object-level locking is achieved.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {70–95},
numpages = {26}
}

@article{10.1145/9760.9764,
author = {Christodoulakis, S. and Theodoridou, M. and Ho, F. and Papa, M. and Pathria, A.},
title = {Multimedia Document Presentation, Information Extraction, and Document Formation in MINOS: A Model and a System},
year = {1986},
issue_date = {Oct. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/9760.9764},
doi = {10.1145/9760.9764},
abstract = {MINOS is an object-oriented multimedia information system that provides integrated facilities for creating and managing complex multimedia objects. In this paper the model for multimedia documents supported by MINOS and its implementation is described. Described in particular are functions provided in MINOS that exploit the capabilities of a modern workstation equipped with image and voice input-output devices to accomplish an active multimedia document presentation and browsing within documents. These functions are powerful enough to support a variety of office applications. Also described are functions provided for the extraction of information from multimedia documents that exist in a large repository of information (multimedia document archiver) and functions that select and transform this information. Facilities for information sharing among objects of the archiver are described; an interactive multimedia editor that is used for the extraction and interactive creation of new information is outlined; finally, a multimedia document formatter that is used to synthesize a new multimedia document from extracted and interactively generated information is presented.This prototype system runs on a SUN-3 workstation running UNIX'". An Instavox, directly addressable, analog device is used to store voice segments.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
pages = {345–383},
numpages = {39}
}

@article{10.1145/9760.9763,
author = {Hirschheim, R. A.},
title = {Understanding the Office: A Social-Analytic Perspective},
year = {1986},
issue_date = {Oct. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/9760.9763},
doi = {10.1145/9760.9763},
abstract = {In order to apply office automation in a meaningful fashion, it is apparent that some understanding of the office is necessary. Most descriptive studies of the office have placed great emphasis on manifest office actions, suggesting that offices are the embodiment of these actions. The meanings of these actions or tasks, however, have been given scant attention. There exist a number of office activity or task taxonomies, but they do little more than provide a simple and limited structure through which to conceive of an office. From a social-analytic perspective this appears to be overly simplistic and misses the richness of social action in an office. Focusing on the overt and manifest aspects of the office may very well lead to its misrepresentation. This paper takes a critical look at the way offices are conceived in the office automation literature and suggests alternatives that may provide a better understanding of the real functions of an office.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
pages = {331–344},
numpages = {14}
}

@article{10.1145/9760.9762,
author = {Motro, Amihai},
title = {SEAVE: A Mechanism for Verifying User Presuppositions in Query Systems},
year = {1986},
issue_date = {Oct. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/9760.9762},
doi = {10.1145/9760.9762},
abstract = {Every information system incorporates a database component, and a frequent activity of users of information systems is to present it with queries. These queries reflect the presuppositions of their authors about the system and the information it contains. With most query processors, queries that are based on erroneous presuppositions often result in null answers. These fake nulls are misleading, since they do not point out the user's erroneous presuppositions (and can even be interpreted as their affirmation). This article describes the SEAVE mechanism for extracting presuppositions from queries and verifying their correctness. The verification is done against three repositories of information: the actual data, their integrity constraints, and their completeness assertions. Consequently, queries that reflect erroneous presuppositions are answered with informative messages instead of null answers, and user-system communication is thus improved (an aspect that is particularly important in systems that often are accessed by naive users). First, the principles of SEAVE are described abstractly. Then, specific algorithms for implementing it with relational databases are presented, including a new method for storing knowledge and an efficient algorithm for processing queries against the knowledge.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
pages = {312–330},
numpages = {19}
}

@article{10.1145/9760.9761,
author = {Hauzeur, Bernard M.},
title = {A Model for Naming, Addressing and Routing},
year = {1986},
issue_date = {Oct. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/9760.9761},
doi = {10.1145/9760.9761},
abstract = {Naming and addressing are areas in which there is still a need for clarification. Many definitions for names, addresses, and routes have been proposed, but the exact relations among these concepts are obscure. A taxonomy of names, addresses, and routes is presented. First, we identify names and routes as the essential concepts of communication. Then, addresses are introduced as an intermediate form that eases the process of mapping between names and routes; an original definition of an address is thus proposed. Relations among names, addresses, and routes are explained with the concept of mapping. On this basis, a general model relating names, addresses, and routes is built and then applied recursively throughout a layered architecture, leading to a layered naming and addressing model which may play the same role for naming and addressing features that the OS1 reference model plays for the definition of services and protocols. Finally, the model is particularized to a typical network architecture. The model may also be applied to non-OSI layered systems; naming, addressing, and routing issues in any network architecture could be a particular instance of this layered model.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
pages = {293–311},
numpages = {19}
}

@article{10.1145/214427.214432,
author = {Hewitt, Carl},
title = {Offices Are Open Systems},
year = {1986},
issue_date = {July 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/214427.214432},
doi = {10.1145/214427.214432},
abstract = {This paper is intended as a contribution to analysis of the implications of viewing offices as open systems. It takes a prescriptive stance on how to establish the information-processing foundations for taking action and making decisions in office work from an open systems perspective. We propose due process as a central activity in organizational information processing. Computer systems are beginning to play important roles in mediating the ongoing activities of organizations. We expect that these roles will gradually increase in importance as computer systems take on more of the authority and responsibility for ongoing activities. At the same time we expect computer systems to acquire more of the characteristics and structure of human organizations.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {271–287},
numpages = {17},
keywords = {debate, open systems, logic, microtheories, decision making, negotiation, due process, offices}
}

@article{10.1145/214427.214431,
author = {Gerson, Elihu M. and Star, Susan Leigh},
title = {Analyzing Due Process in the Workplace},
year = {1986},
issue_date = {July 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/214427.214431},
doi = {10.1145/214427.214431},
abstract = {Every office is an open system, and the products of office work are the result of decentralized negotiations. Changing patterns of task organization and alliance inevitably give rise to inconsistent knowledge bases and procedures. This implies that there are no globally correct answers to problems addressed by OISs. Rather, systems must deal with multiple competing, possibly irreconcilable, solutions. Articulating alternative solutions is the problem of due process. This problem and its consequences are illustrated by a case study of a rate-setting group in a large health insurance firm.There is no formal solution to the problem of due process. But it must be solved in practice if distributed intelligent OISs are to be developed. We propose an alternative approach based on the work of social scientists concerned with analyzing analogous problems in human organization. Solution of the due process problem hinges on developing local closures to the problem faced by an organization. This means analyzing (a) local, tacit knowledge and its transfer ability; (b) articulation work, that is, reconciling incommensurate assumptions and procedures.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {257–270},
numpages = {14}
}

@article{10.1145/214427.214430,
author = {Davison, Jay W. and Zdonik, Stanley B.},
title = {A Visual Interface for a Database with Version Management},
year = {1986},
issue_date = {July 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/214427.214430},
doi = {10.1145/214427.214430},
abstract = {This paper describes a graphical interface to an experimental database system which incorporates a built-in version control mechanism that maintains a history of the database development and changes. The system is an extension of ISIS [6], Interface for a Semantic Information System, a workstation-based, graphical database programming tool developed at Brown University. ISIS supports a graphical interface to a modified subset of the Semantic Data Model (SDM) [7]. The ISIS extension introduces a transaction mechanism that interacts with the version control facilities.A series of version control support tools have been added to ISIS to provide a notion of history to user-created databases. The user can form new versions of three types of ISIS objects: a class definition object (a type), the set of instances of a class (the content), and an entity. A version-viewing mechanism is provided to allow for the comparison of various object versions. Database operations are grouped together in atomic units to form transactions, which are stored as entities in the database. A sample session demonstrates the capabilities of version and transaction control during the creation and manipulation of database objects.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {226–256},
numpages = {31},
keywords = {transaction processing, version control, semantic data model, visual interfaces, historical database}
}

@article{10.1145/214427.214429,
author = {Gasser, Les},
title = {The Integration of Computing and Routine Work},
year = {1986},
issue_date = {July 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/214427.214429},
doi = {10.1145/214427.214429},
abstract = {Most computing serves as a resource or tool to support other work: performing complex analyses for engineering projects, preparing documents, or sending electronic mail using office automation equipment, etc. To improve the character, quality, and ease of computing work, we must understand how automated systems actually are integrated into the work they support. How do people actually adapt to computing as a resource? How do they deal with the unreliability in hardware, software, or operations; data inaccuracy; system changes; poor documentation; inappropriate designs; etc.; which are present in almost every computing milieu, even where computing is widely used and considered highly successful? This paper presents some results of a detailed empirical study of routine computer use in several organizations. We present a theoretical account of computing work and use it to explain a number of observed phenomena, such as:How people knowingly use “false” data to obtain desired analytical results by tricking their systems.  How organizations come to rely upon complex, critical computer systems despite significant, recurrent, known errors and inaccurate data.  How people work around inadequate computing systems by using manual or duplicate systems, rather than changing their systems via maintenance or enhancement.  In addition, the framework for analyzing computing and routine work presented here proves useful for representing and reasoning about activity in multiactor systems in general, and in understanding how better to integrate organizations of people and computers in which work is coordinated.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {205–225},
numpages = {21},
keywords = {computing and work, articulation work, workarounds, social analysis of computing, multiagent systems, integration of computing, computing in organizations}
}

@article{10.1145/214427.214428,
author = {Woo, Carson C. and Lochovsky, Frederick H.},
title = {Supporting Distributed Office Problem Solving in Organizations},
year = {1986},
issue_date = {July 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/214427.214428},
doi = {10.1145/214427.214428},
abstract = {To improve the effectiveness of office workers in their decision making, office systems have been built to support (rather than replace) their judgment. However, these systems model office work in a centralized environment, and/or they can only support a single office worker. Office work that is divided into specialized domains handled by different office workers (where cooperation is needed in order to accomplish the work) is not supported. In this paper, we will present a model that supports office problem solving in a logically distributed environment. (In some systems, information is geographically distributed for performance purposes rather than for conceptual need. The term, logically, is therefore used to indicate the logical need of organizing information without having to worry about the physical location of the information.) In particular, cooperative tools that can be used to support office workers during the process of their problem solving is discussed.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {185–204},
numpages = {20},
keywords = {cooperative tools, office communication, managerial office work, object-oriented environment}
}

@article{10.1145/6168.6172,
author = {Motro, Amihai},
title = {BAROQUE: A Browser for Relational Databases},
year = {1986},
issue_date = {April 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/6168.6172},
doi = {10.1145/6168.6172},
abstract = {The standard, most efficient method to retrieve information from databases can be described as systematic retrieval: The needs of the user are described in a formal query, and the database management system retrieves the data promptly. There are several situations, however, in which systematic retrieval is difficult or even impossible. In such situations exploratory search (browsing) is a helpful alternative. This paper describes a new user interface, called BAROQUE, that implements exploratory searches in relational databases. BAROQUE requires few formal skills from its users. It does not assume knowledge of the principles of the relational data model or familiarity with the organization of the particular database being accessed. It is especially helpful when retrieval targets are vague or cannot be specified satisfactorily. BAROQUE establishes a view of the relational database that resembles a semantic network, and provides several intuitive functions for scanning it. The network integrates both schema and data, and supports access by value. BAROQUE can be implemented on top of any basic relational database management system but can be modified to take advantage of additional capabilities and enhancements often present in relational systems.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {164–181},
numpages = {18}
}

@article{10.1145/6168.6171,
author = {Hudson, Scott E. and King, Roger},
title = {A Generator of Direct Manipulation Office Systems},
year = {1986},
issue_date = {April 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/6168.6171},
doi = {10.1145/6168.6171},
abstract = {A system for generating direct manipulation office systems is described. In these systems, the user directly manipulates graphical representations of office entities instead of dealing with these entities abstractly through a command language or menu system. These systems employ a new semantic data model to describe office entities. New techniques based on attribute grammars and incremental attribute evaluation are used to implement this data model in an efficient manner. In addition, the system provides a means of generating sophisticated graphics-based user interfaces that are integrated with the underlying semantic model. Finally, the generated systems contain a general user reversal and recovery (or undo) mechanism that allows them to be much more tolerant of human errors.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {132–163},
numpages = {32}
}

@article{10.1145/6168.6170,
author = {Ho, Cheng-Seen and Hong, Yang-Chang and Kuo, Te-Son},
title = {A Society Model for Office Information Systems},
year = {1986},
issue_date = {April 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/6168.6170},
doi = {10.1145/6168.6170},
abstract = {A society model, which characterizes the behavior and procedure of offices, is proposed. It is our belief that an office system capable of dealing with all real office problems only through the modeling of the internal behavior of an office can be developed. In this society model, office entities are viewed as agents. An agent is modeled as a microsociety of interacting knowledge sources. Within the microsociety, there exists a microknowledge exchange system, which provides a set of microknowledge exchange protocols as a coordination system among those knowledge sources during their cooperative reasoning process. An office is then modeled as a society of various interacting agents using their knowledge to complete the office goals cooperatively. It is this unified view that allows offices to be modeled in a flexible and general way.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {104–131},
numpages = {28}
}

@article{10.1145/6168.6169,
author = {Bui, Tung X. and Jarke, Matthias},
title = {Communications Design for Co-OP: A Group Decision Support System},
year = {1986},
issue_date = {April 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/6168.6169},
doi = {10.1145/6168.6169},
abstract = {Decision Support Systems (DSSs), computer-based systems intended to assist managers in preparing and analyzing decisions, have been single-user systems for most of the past decade. Only recently has DSS research begun to study the implications of the fact that most complex managerial decisions involve multiple decision makers and analysts. A number of tools for facilitating group decisions have been proposed under the label Group Decision Support Systems (GDSSs).One of the most important functions of a GDSS is to provide problem-oriented services for communication among decision makers. On the basis of an analysis of the communication requirements in various group decision settings, this paper presents an architecture for defining and enforcing dynamic application-level protocols that organize decision group interaction. The architecture has been implemented on a network of personal computers in Co-oP, a GDSS for cooperative group decision making based on interactive, multiple-criteria decision methods.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {81–103},
numpages = {23}
}

@article{10.1145/5401.5405,
author = {Jones, William P. and Dumais, Susan T.},
title = {The Spatial Metaphor for User Interfaces: Experimental Tests of Reference by Location versus Name},
year = {1986},
issue_date = {Jan. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/5401.5405},
doi = {10.1145/5401.5405},
abstract = {The enduring dichotomy between spatial and symbolic modes of representation and retrieval acquires an added pragmatic dimension through recent developments in computer-based information retrieval. The standard name-based approach to object reference is now supplemented on some systems by a spatial alternative-often driven by an office or desktop metaphor. Little rigorous evidence is available, however, to support the supposition that spatial memory in itself is more effective than symbolic memory.The accuracy of spatial versus symbolic reference was assessed in three experiments. In Experiment 1 accuracy of location reference in a location-only filing condition was initially comparable to that in a name-only condition, but deteriorated much more rapidly with increases in the number of objects filed. In Experiment 2 subjects placed objects in a two-dimensional space containing landmarks (drawings of a desk, table, filing cabinets, etc.) designed to evoke an office metaphor, and in Experiment 3 subjects placed objects in an actual, three-dimensional mock office. Neither of these enhancements served to improve significantly the accuracy of location reference, and performance remained below that of a name-only condition in Experiment 1. The results raise questions about the utility of spatial metaphor over symbolic filing and highlight the need for continuing research in which considerations of technological and economic feasibility are balanced by considerations of psychological utility.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {42–63},
numpages = {22}
}

@article{10.1145/5401.5404,
author = {Martin, P. and Tsichritzis, D.},
title = {Complete Logical Routings in Computer Mail Systems},
year = {1986},
issue_date = {Jan. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/5401.5404},
doi = {10.1145/5401.5404},
abstract = {The logical routing of a message in a computer mail system involves the identification and location of the set of intended recipients for that message. This function is carried out by the naming and addressing mechanism of the mail system. An important property of that mechanism is that it should be able to identify and locate all the intended recipients of a message, so that, once submitted, a message will not become lost or stuck in the system. We first discuss message addressing schemes, which are a framework for dealing with the naming and addressing problem. Message addressing schemes can also serve as a basis for the analysis of some of the properties of logical message routing within a system. We examine the conditions necessary for a complete message addressing scheme, that is, one that guarantees to deliver all possible messages.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {64–80},
numpages = {17}
}

@article{10.1145/5401.5403,
author = {Donahue, James and Widom, Jennifer},
title = {Whiteboards: A Graphical Database Tool},
year = {1986},
issue_date = {Jan. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/5401.5403},
doi = {10.1145/5401.5403},
abstract = {The “Whiteboards” system is intended to be an electronic equivalent of the whiteboards and corkboards that we have in our offices. A Whiteboard database has similar qualities of storing disparate collections of data and saving their spatial location in a window to help with organization. A Whiteboard database can contain references to arbitrary entities: text files, notes, programs, tools, pictures, etc. Whiteboards runs as an application in the Cedar programming environment developed at the Xerox Palo Alto Research Center.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {24–41},
numpages = {18}
}

@article{10.1145/5401.5402,
author = {Trigg, Randall H. and Weiser, Mark},
title = {TEXTNET: A Network-Based Approach to Text Handling},
year = {1986},
issue_date = {Jan. 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/5401.5402},
doi = {10.1145/5401.5402},
abstract = {Textnet is a new system for structuring text. The Textnet approach uses one uniform data structure to capture graphlike pools of text, as well as embedded hierarchical structures. By using a semantic network formalism of nodes connected by typed links, the relationships between neighboring pieces of text are made explicit. Also described is our partial implementation of the Textnet approach, which makes use of an object-oriented window/menu-driven user interface. Users peruse the network by moving among object menus or by reading text along a path through the network. In addition, critiquing, reader linking, searching, and jumping are easily accessible operations. Finally, the results of a short trial with users are presented.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–23},
numpages = {23}
}

@article{10.1145/4656.4660,
author = {Biermann, Alan W. and Fineman, Linda and Gilbert, Kermit C.},
title = {An Imperative Sentence Processor for Voice Interactive Office Applications},
year = {1985},
issue_date = {Oct. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/4656.4660},
doi = {10.1145/4656.4660},
abstract = {An imperative sentence processor that enables a user to manipulate text with connected speech and touch-graphics input is described. The processor includes capabilities to follow dialogue focus, execute a variety of imperative commands, and handle nested noun groups, pronouns, and other phenomena. A micromodel of the system, giving enough of the structure to enable the reader to observe internal mechanisms in considerable detail, is included. This processor is designed to be transportable to a number of other office automation domains such as calendar management, message-passing, and desk calculation. Various examples and statistics related to its behavior in the text manipulation application are given. The system has been implemented in PASCAL and can run on any machine that supports this language.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {321–346},
numpages = {26}
}

@article{10.1145/4656.4659,
author = {King, Roger and Stanley, Carolyn},
title = {Ensuring Court Admissibility of Computer-Generated Records},
year = {1985},
issue_date = {Oct. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/4656.4659},
doi = {10.1145/4656.4659},
abstract = {An informal methodology is described for optimizing the likelihood of computer-generated records being admissible in a U.S. court of law. This methodology is intended for individuals who are converting to automated office procedures, as well as for those whose businesses are already highly computerized. However, this paper does not purport to be a formal legal guide; rather, it is intended as an overview of this issue.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {398–412},
numpages = {15}
}

@article{10.1145/4656.4658,
author = {Buchman, Cary and Berry, Daniel M. and Gonczarowski, Jakob},
title = {DITROFF/FFORTID, an Adaptation of the UNIX/DITROFF for Formatting Bidirectional Text},
year = {1985},
issue_date = {Oct. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/4656.4658},
doi = {10.1145/4656.4658},
abstract = {DITROFF/FFORTID, a collection of pre- and postprocessors for the UNIX DITROFF (Device Independent Typesetter RunOFF) is described. DITROFF/FFORTID permits formatting of text involving a mixture of languages written from left to right and from right to left, such as English and Hebrew. The programs are table driven or macro-generated to permit them to be used for any languages written from left to right and from right to left so long as fonts with the proper character sets can be mounted on a typesetting device supported by DITROFF. The preprocessors are set up to permit phonetic, unidirectional input of all of the alphabets needed using only the two alphabets (each case counts as an alphabet) available on the input device. These macro-generated preprocessors can be adjusted to the user's pronunciation, the language's rules about a letter's form, depending on its position in the word, and the language of the user's input keyboard. The postprocessor is set up to properly change direction of formatting when the text switches to a language written in a different direction. The collection of programs is also designed to allow use of any of DITROFF's preprocessors, such as PIC, EQN, TBL, and the various device drivers.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {380–397},
numpages = {18}
}

@article{10.1145/4656.4657,
author = {Peels, Arno J. H. and Janssen, Norbert J. M. and Nawijn, Wop},
title = {Document Architecture and Text Formatting},
year = {1985},
issue_date = {Oct. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/4656.4657},
doi = {10.1145/4656.4657},
abstract = {The formalization of the architecture of documents and text formatting are the central issues of this paper. Besides a fundamental and theoretical approach toward these topics, an overview is presented of the COBATEF system. The COBATEF system is a context-based text formatting system, for which a software, as well as a hardware, implementation is available.A unique feature of the system is its automatic text-element recognition mechanism, which is context based and consequently takes advantage of the implicit structure of text. A predefined layout for each type of text element then opens the way for a fully automatic text-processing system in which user control information can be reduced to an absolute minimum.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {347–369},
numpages = {23}
}

@article{10.1145/4656.214329,
author = {Panko, Raymond R.},
title = {Productivity Trends in Certain Office-Intensive Sectors of the U.S. Federal Government},
year = {1985},
issue_date = {Oct. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/4656.214329},
doi = {10.1145/4656.214329},
abstract = {It is often said that office productivity is virtually stagnant, increasing only about 4 percent every 10 years. The methodology used to estimate this 4 percent figure is examined and found to be inaccurate! There is no known way to estimate overall national office productivity trends. Productivity trends in a single part of the economy, however, can be examined, namely, office-intensive sectors of the U.S. federal government. Productivity in these sectors is found to be anything but stagnant, having increased 1.7 percent annually from 1967 to 1981 and 3.0 percent annually from 1977 through 1981.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {370–379},
numpages = {10}
}

@article{10.1145/4229.4234,
author = {Dannenberg, Roger B. and Hibbard, Peter G.},
title = {A Butler Process for Resource Sharing on Spice Machines},
year = {1985},
issue_date = {July 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/4229.4234},
doi = {10.1145/4229.4234},
abstract = {A network of personal computers may contain a large amount of distributed computing resources. For a number of reasons it is desirable to share these resources, but sharing is complicated by issues of security and autonomy. A process known as the Butler addresses these problems and provides support for resource sharing. The Butler relies upon a capability-based accounting system called the Banker to monitor the use of local resources.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {234–252},
numpages = {19}
}

@article{10.1145/4229.4233,
author = {Heimbigner, Dennis and McLeod, Dennis},
title = {A Federated Architecture for Information Management},
year = {1985},
issue_date = {July 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/4229.4233},
doi = {10.1145/4229.4233},
abstract = {An approach to the coordinated sharing and interchange of computerized information is described emphasizing partial, controlled sharing among autonomous databases. Office information systems provide a particularly appropriate context for this type of information sharing and exchange. A federated database architecture is described in which a collection of independent database systems are united into a loosely coupled federation in order to share and exchange information. A federation consists of components (of which there may be any number) and a single federal dictionary. The components represent individual users, applications, workstations, or other components in an office information system. The federal dictionary is a specialized component that maintains the topology of the federation and oversees the entry of new components. Each component in the federation controls its interactions with other components by means of an export schema and an import schema. The export schema specifies the information that a component will share with other components, while the import schema specifies the nonlocal information that a component wishes to manipulate. The federated architecture provides mechanisms for sharing data, for sharing transactions (via message types) for combining information from several components, and for coordinating activities among autonomous components (via negotiation). A prototype implementation of the federated database mechanism is currently operational on an experimental basis.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {253–278},
numpages = {26}
}

@article{10.1145/4229.4232,
author = {Thoma, G. R. and Suthasinekul, S. and Walker, F. L. and Cookson, J. and Rashidian, M.},
title = {A Prototype System for the Electronic Storage and Retrieval of Document Images},
year = {1985},
issue_date = {July 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/4229.4232},
doi = {10.1145/4229.4232},
abstract = {A prototype system has been implemented for electronic scanning, digitization, storage, retrieval, and display of images of biomedical documents. Paper documents are scanned and digitized at a scan density of 200 picture elements (pels) per inch by either a high-speed loose-leaf scanner with an automatic document transport or a book scanner with a manual book holder. Each scanner employs a high-resolution charge-coupled device (CCD) linear array operating at a sampling rate close to 10 MHz. The analog output signal of the CCD array is digitized into 1 bit per pixel two-tone images by means of dynamic thresholding. The digitized images are stored on magnetic disks to be processed and will eventually be transferred onto optical disks for archival storage. Existing on-line bibliographic databases developed by the National Library of Medicine are used as directories for the retrieval of document images. These images are displayed at a resolution of 200 pels/inch in both soft-copy (raster-refreshed CRT) and hard-copy forms.This prototype system, developed as part of a research and development program, offers the opportunity to investigate the areas of document image enhancement, image compression, and omnifont text recognition and to conduct experiments designed to answer key questions on the role of electronic document storage and retrieval technology in library information processing and the preservation of library documents.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {279–291},
numpages = {13}
}

@article{10.1145/4229.4231,
author = {Nicholson, Robert T.},
title = {Usage Patterns in an Integrated Voice and Data Communications System},
year = {1985},
issue_date = {July 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/4229.4231},
doi = {10.1145/4229.4231},
abstract = {Recently, office communication systems have begun to integrate voice recordings into their mail and data communications facilities. The study of usage patterns on one such system shows that voice is used for informal, person-to-person communications, as opposed to the formal content of typed messages. Voice messages are generally sent to fewer recipients (often only one), and sometimes replace face-to-face meetings.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {307–314},
numpages = {8}
}

@article{10.1145/4229.4230,
author = {Harris, Sidney E. and Brightman, Harvey J.},
title = {Design Implications of a Task-Driven Approach to Unstructured Cognitive Tasks in Office Work},
year = {1985},
issue_date = {July 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/4229.4230},
doi = {10.1145/4229.4230},
abstract = {Previous research in modeling office activities has been primarily oriented toward office work that is structured and organized. In this paper we report on efforts to develop a new methodology for needs assessment evaluation. We use the Critical Task Method to identify the “bottleneck cognitive tasks” of principals with an unstructured work profile. Data were collected on the computer-support needs of faculty researchers, and the findings indicate that a “knowledge-based” design offers the most promise for delivering effective support. In addition, the systems design suggests the integration of text, data, voice, and images.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {292–306},
numpages = {15}
}

@article{10.1145/3914.3985,
author = {Epstein, Samuel S.},
title = {Transportable Natural Language Processing through Simplicity—the PRE System},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3914.3985},
doi = {10.1145/3914.3985},
abstract = {PRE (Purposefully Restricted English) is a restricted English database query language whose implementation has addressed engineering goals, namely, habitability, interapplication transportability, performance, and use with a reliable database management system that supports large numbers of concurrent users and large databases. Habitability has not been demonstrated, but initial indications are encouraging. The other goals have clearly been achieved. The existence of the PRE system demonstrates that an explicitly “minimalist” approach to natural language processing can facilitate achievement of transportability.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {107–120},
numpages = {14}
}

@article{10.1145/3914.3984,
author = {Marsh, Elaine and Friedman, Carol},
title = {Transporting the Linguistic String Project System from a Medical to a Navy Domain},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3914.3984},
doi = {10.1145/3914.3984},
abstract = {The Linguistic String Project (LSP) natural language processing system has been developed as a domain-independent natural language processing system. Initially utilized for processing sets of medical messages and other texts in the medical domain, it has been used at the Naval Research Laboratory for processing Navy messages about shipboard equipment failures. This paper describes the structure of the LSP system and the features that make it transportable from one domain to another. The processing procedures encourage the isolation of domain-specific information, yet take advantage of the syntactic and semantic similarities between the medical and Navy domains. From our experience in transporting the LSP system, we identify the features that are required for transportable natural language systems.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {121–140},
numpages = {20}
}

@article{10.1145/3914.3983,
author = {Thompson, Bozena H. and Thompson, Frederick B.},
title = {ASK is Transportable in Half a Dozen Ways},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3914.3983},
doi = {10.1145/3914.3983},
abstract = {This paper is a discussion of the technical issues and solutions encountered in making the ASK System transportable. A natural language system can be “transportable” in a number of ways. Although transportability to a new domain is most prominent, other ways are also important if the system is to have viability in the commercial marketplace.On the one hand, transporting a system to a new domain may start with the system prior to adding any domain of knowledge and extend it to incorporate the new domain. On the other hand, one may wish to add to a system that already has knowledge of one domain the knowledge concerning a second domain, that is, to extend the system to cover this second domain. In the context of ASK, it has been natural to implement extending and then achieve transportability as a special case.In this paper, we consider six ways in which the ASK System can be extended to include new capabilities:Special-purpose applications, such as those to accommodate standard office tasks, would make use of these various means of extension.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {185–203},
numpages = {19}
}

@article{10.1145/3914.3982,
author = {Hafner, Carole D. and Godden, Kurt},
title = {Portability of Syntax and Semantics in DATALOG},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3914.3982},
doi = {10.1145/3914.3982},
abstract = {This paper presents a discussion of the techniques developed and problems encountered during the design, implementation, and experimental use of a portable natural language processor. Datalog (for “database dialogue") is an experimental natural language query system, which was designed to achieve a maximum degree of portability and extendibility. Datalog uses a three-level architecture to provide both portability of syntax to new and extended tasks and portability of semantics to new database applications. The implementation of each of the three levels, the structures and conventions that control the interactions among them, and the way in which different aspects of the design contribute to portability are described. Finally, two specific, implemented examples are presented, showing how it was possible to transport or extend Datalog by changing only one “layer” of the system's knowledge and achieve correct processing of the extended input by the entire system.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {141–164},
numpages = {24}
}

@article{10.1145/3914.3981,
author = {Slocum, Jonathan and Justus, Carol F.},
title = {Transportability to Other Languages: The Natural Language Processing Project in the AI Program at MCC},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3914.3981},
doi = {10.1145/3914.3981},
abstract = {We discuss a recently launched, long-term project in natural language processing, the primary concern of which is that natural language applications be transportable among human languages. In particular, we seek to develop system tools and linguistic processing techniques that are themselves language-independent to the maximum extent practical. In this paper we discuss our project goals and outline our intended approach, address some cross-linguistic requirements, and then present some new linguistic data that we feel support our approach.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {204–230},
numpages = {27}
}

@article{10.1145/3914.3915,
author = {Damerau, Fred J.},
title = {Problems and Some Solutions in Customization of Natural Language Database Front Ends},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3914.3915},
doi = {10.1145/3914.3915},
abstract = {This paper is concerned with some of the issues arising in the development of a domain-independent English interface to IBM SQL-based program products. The TQA system falls into the class of multilayered natural language processing systems. As a result, there is a large number of potential points at which customization to a particular database can be done. Of these, we discuss procedures that affect the reader, the lexicon, the lowest level of grammar rules, the semantic interpreter, and the output formatter. Our tests lead us to believe that the approach we are taking will make it possible for database administrators to generate robust English interfaces to particular databases without help from linguistic experts.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {165–184},
numpages = {20}
}

@article{10.1145/3864.3869,
author = {King, Roger and McLeod, Dennis},
title = {A Database Design Methodology and Tool for Information Systems},
year = {1985},
issue_date = {Jan. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3864.3869},
doi = {10.1145/3864.3869},
abstract = {A model and methodology for describing the information objects in an office information system and how such objects flow among the components of such a system are presented. The model and methodology support the specification of information objects at multiple levels of abstraction. An interactive prototype design tool based on the methodology and model has been designed and experimentally implemented.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {2–21},
numpages = {20}
}

@article{10.1145/3864.3868,
author = {Kincaid, Christine M. and Dupont, Pierre B. and Kaye, A. R.},
title = {Electronic Calendars in the Office: An Assessment of User Needs and Current Technology},
year = {1985},
issue_date = {Jan. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3864.3868},
doi = {10.1145/3864.3868},
abstract = {Manufacturers of integrated electronic office systems have included electronic versions of the calendar in almost every system they offer. This paper describes a survey of office workers, carried out to examine their use both of paper calendars and of electronic calendars that are commercially available as part of integrated office systems. It assesses the degree to which electronic calendars meet the needs of users. Our survey shows that the simple paper calendar is a tool whose power and flexibility is matched by few, if any, of the current commercially available electronic calendars. Recommendations for features that should be included in electronic calendars and automatic schedulers are included.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {89–102},
numpages = {14}
}

@article{10.1145/3864.3867,
author = {Gould, John D. and Lewis, Clayton and Barnes, Vincent},
title = {Cursor Movement during Text Editing},
year = {1985},
issue_date = {Jan. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3864.3867},
doi = {10.1145/3864.3867},
abstract = {Nine participants used a full-screen computer text editor (XEDIT) with an IBM 3277 terminal to edit marked-up documents at each of three cursor speeds (3.3, 4.7, and 11.0 cm/s). These speeds occur when a user continuously holds down an arrow key to move the cursor more than one character position (i.e., in  repeat or typamatic mode). Results show that cursor speed did not seem to act as a pacing device for the entire editing task. Since cursor speed is a form of system response, this finding is in contrast with the generally found positive relation between system-response time and user-response time. Participants preferred the Fast cursor speed, however. Overall, more than one-third of all keystrokes were used to move the cursor. We estimate that 9-14 percent of editing time was spent controlling and moving the cursor, regardless of cursor speed.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {22–34},
numpages = {13}
}

@article{10.1145/3864.3866,
author = {Hevner, Alan R. and Wu, O. Q. and Yao, S. B.},
title = {Query Optimization on Local Area Networks},
year = {1985},
issue_date = {Jan. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3864.3866},
doi = {10.1145/3864.3866},
abstract = {Local area networks are becoming widely used as the database communication framework for sophisticated information systems. Databases can be distributed among stations on a network to achieve the advantages of performance, reliability, availability, and modularity. Efficient distributed query optimization algorithms are presented here for two types of local area networks: address ring networks and broadcast networks. Optimal algorithms are designed for simple queries. Optimization principles from these algorithms guide the development of effective heuristic algorithms for general queries on both types of networks. Several examples illustrate distributed query processing on local area networks.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {35–62},
numpages = {28}
}

@article{10.1145/3864.3865,
author = {Weyer, Stephen A. and Borning, Alan H.},
title = {A Prototype Electronic Encyclopedia},
year = {1985},
issue_date = {Jan. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3864.3865},
doi = {10.1145/3864.3865},
abstract = {We describe a prototype electronic encyclopedia implemented on a powerful personal computer, in which user interface, media presentation, and knowledge representation techniques are applied to improving access to a knowledge resource. In itself, an electronic encyclopedia is an important information resource, but this work also illustrates the issues and approaches for many types of electronic information retrieval environments. In the prototype we make dynamic use of the structure and semantics of the text articles and index of an existing encyclopedia, while experimenting with other forms of representation, such as simulation and videodisc images. We present a long-term vision of an intelligent user-interface agent; summarize previous work related to futuristic encyclopedias, electronic books, decision support systems, and knowledge libraries; and outline current and potential research directions.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {63–88},
numpages = {26}
}

@article{10.1145/2275.357411,
author = {Faloutsos, Chris and Christodoulakis, Stavros},
title = {Signature Files: An Access Method for Documents and Its Analytical Performance Evaluation},
year = {1984},
issue_date = {Oct. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2275.357411},
doi = {10.1145/2275.357411},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {267–288},
numpages = {22}
}

@article{10.1145/2275.2279,
author = {Higgins, Christopher A. and Safayeni, Frank R.},
title = {A Critical Appraisal of Task Taxonomies as a Tool for Studying Office Activities},
year = {1984},
issue_date = {Oct. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2275.2279},
doi = {10.1145/2275.2279},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {331–339},
numpages = {9}
}

@article{10.1145/2275.2278,
author = {Mazer, Murray S. and Lochovsky, Frederick H.},
title = {Logical Routing Specification in Office Information Systems},
year = {1984},
issue_date = {Oct. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2275.2278},
doi = {10.1145/2275.2278},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {303–330},
numpages = {28}
}

@article{10.1145/2275.2277,
author = {Paddock, Charles E. and Scamell, Richard W.},
title = {Office Automation Projects and Their Impact on Organization, Planning, and Control},
year = {1984},
issue_date = {Oct. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/2275.2277},
doi = {10.1145/2275.2277},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {289–302},
numpages = {14}
}

@article{10.1145/1206.357413,
author = {Yao, S. Bing and Hevner, Alan R. and Shi, Zhongzhi and Luo, Dawei},
title = {FORMANAGER: An Office Forms Management System},
year = {1984},
issue_date = {July 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1206.357413},
doi = {10.1145/1206.357413},
journal = {ACM Trans. Inf. Syst.},
month = aug,
pages = {235–262},
numpages = {28}
}

@article{10.1145/1206.357412,
author = {Ellis, Clarence A.},
title = {Editor's Introduction},
year = {1984},
issue_date = {July 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1206.357412},
doi = {10.1145/1206.357412},
journal = {ACM Trans. Inf. Syst.},
month = aug,
pages = {171–172},
numpages = {2}
}

@article{10.1145/1206.1485,
author = {Panko, Raymond R.},
title = {38 Offices:  Analyzing Needs in Individual Offices},
year = {1984},
issue_date = {July 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1206.1485},
doi = {10.1145/1206.1485},
journal = {ACM Trans. Inf. Syst.},
month = aug,
pages = {226–234},
numpages = {9}
}

@article{10.1145/1206.1484,
author = {Radicati, Siranush},
title = {Managing Transient Internetwork Links in the Xerox Internet},
year = {1984},
issue_date = {July 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1206.1484},
doi = {10.1145/1206.1484},
journal = {ACM Trans. Inf. Syst.},
month = aug,
pages = {213–225},
numpages = {13}
}

@article{10.1145/1206.1483,
author = {Croft, W Bruce and Lefkowitz, Lawrence S.},
title = {Task Support in an Office System},
year = {1984},
issue_date = {July 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1206.1483},
doi = {10.1145/1206.1483},
journal = {ACM Trans. Inf. Syst.},
month = aug,
pages = {197–212},
numpages = {16}
}

@article{10.1145/1206.1207,
author = {Ahlsen, Matts and Bjornerstedt, Anders and Britts, Stefan and Hulten, Christer and Soderlund, Lars},
title = {An Architecture for Object Management in OIS},
year = {1984},
issue_date = {July 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/1206.1207},
doi = {10.1145/1206.1207},
journal = {ACM Trans. Inf. Syst.},
month = aug,
pages = {173–196},
numpages = {24}
}

@article{10.1145/521.523,
author = {Culnan, Mary J.},
title = {The Dimensions of Accessibility to Online Information:  Implications for Implementing Office Information Systems},
year = {1984},
issue_date = {April 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/521.523},
doi = {10.1145/521.523},
journal = {ACM Trans. Inf. Syst.},
month = may,
pages = {141–150},
numpages = {10}
}

@article{10.1145/521.522,
author = {Trauth, Eileen M. and Kwan, Stephen K and Barber, Susanna},
title = {Channel Selection and Effective Communication for Managerial Decision Making},
year = {1984},
issue_date = {April 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/521.522},
doi = {10.1145/521.522},
journal = {ACM Trans. Inf. Syst.},
month = may,
pages = {123–140},
numpages = {18}
}

@article{10.1145/521.357416,
author = {Bracchi, Giampio and Pernici, Barbara},
title = {The Design Requirements of Office Systems},
year = {1984},
issue_date = {April 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/521.357416},
doi = {10.1145/521.357416},
journal = {ACM Trans. Inf. Syst.},
month = may,
pages = {151–170},
numpages = {20}
}

@article{10.1145/521.357415,
author = {Lyngbaek, Peter and McLeod, Dennis},
title = {Object Management in Distributed Information Systems},
year = {1984},
issue_date = {April 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/521.357415},
doi = {10.1145/521.357415},
journal = {ACM Trans. Inf. Syst.},
month = may,
pages = {96–122},
numpages = {27}
}

@article{10.1145/521.357414,
author = {Terry, Douglas B. and Andler, Sten},
title = {The COSIE Communications Subsystem: Support for Distributed Office Applications},
year = {1984},
issue_date = {April 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/521.357414},
doi = {10.1145/521.357414},
journal = {ACM Trans. Inf. Syst.},
month = may,
pages = {79–95},
numpages = {17}
}

@article{10.1145/357417.357422,
author = {Tsichritzis, D.},
title = {Message Addressing Schemes},
year = {1984},
issue_date = {Jan. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357417.357422},
doi = {10.1145/357417.357422},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {58–77},
numpages = {20}
}

@article{10.1145/357417.357421,
author = {Hanson, Stephen Jos\'{e} and Kraut, Robert E. and Farber, James M.},
title = {Interface Design and Multivariate Analysis of UNIX Command Use},
year = {1984},
issue_date = {Jan. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357417.357421},
doi = {10.1145/357417.357421},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {42–57},
numpages = {16}
}

@article{10.1145/357417.357420,
author = {Kelley, J. F.},
title = {An Iterative Design Methodology for User-Friendly Natural Language Office Information Applications},
year = {1984},
issue_date = {Jan. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357417.357420},
doi = {10.1145/357417.357420},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {26–41},
numpages = {16}
}

@article{10.1145/357417.357418,
author = {Ballard, Bruce W. and Lusth, John C. and Tinkham, Nancy L.},
title = {LDC-1: A Transportable, Knowledge-Based Natural Language Processor for Office Environments},
year = {1984},
issue_date = {Jan. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357417.357418},
doi = {10.1145/357417.357418},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–25},
numpages = {25}
}

@article{10.1145/357442.357445,
author = {Suchman, Lucy A.},
title = {Office Procedure as Practical Action: Models of Work and System Design},
year = {1983},
issue_date = {Oct. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/357442.357445},
doi = {10.1145/357442.357445},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {320–328},
numpages = {9}
}

@article{10.1145/357442.357444,
author = {Gibbs, Simon and Tsichritzis, Dionysis},
title = {A Data Modeling Approach for Office Information Systems},
year = {1983},
issue_date = {Oct. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/357442.357444},
doi = {10.1145/357442.357444},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {299–319},
numpages = {21}
}

@article{10.1145/357442.357443,
author = {Gould, John D. and Boies, Stephen J.},
title = {Human Factors Challenges in Creating a Principal Support Office System—the Speech Filing System Approach},
year = {1983},
issue_date = {Oct. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/357442.357443},
doi = {10.1145/357442.357443},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {273–298},
numpages = {26}
}

@article{10.1145/357436.357440,
author = {Mack, Robert L. and Lewis, Clayton H. and Carroll, John M.},
title = {Learning to Use Word Processors: Problems and Prospects},
year = {1983},
issue_date = {July 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/357436.357440},
doi = {10.1145/357436.357440},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {254–271},
numpages = {18}
}

@article{10.1145/357436.357439,
author = {Oppen, Derek C. and Dalal, Yogen K.},
title = {The Clearinghouse: A Decentralized Agent for Locating Named Objects in a Distributed Environment},
year = {1983},
issue_date = {July 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/357436.357439},
doi = {10.1145/357436.357439},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {230–253},
numpages = {24}
}

@article{10.1145/357436.357438,
author = {Smith, Stephen A. and Benjamin, Robert I.},
title = {Projecting Demand for Electronic Communications in Automated Offices},
year = {1983},
issue_date = {July 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/357436.357438},
doi = {10.1145/357436.357438},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {211–229},
numpages = {19}
}

@article{10.1145/357436.357437,
author = {Israel, Jay E. and Linden, Theodore A.},
title = {Authentication in Office System Internetworks},
year = {1983},
issue_date = {July 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/357436.357437},
doi = {10.1145/357436.357437},
journal = {ACM Trans. Inf. Syst.},
month = jul,
pages = {193–210},
numpages = {18}
}

@article{10.1145/357431.357435,
author = {Brotz, Douglas K.},
title = {Message System Mores: Etiquette in Laurel},
year = {1983},
issue_date = {April 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/357431.357435},
doi = {10.1145/357431.357435},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {179–192},
numpages = {14}
}

@article{10.1145/357431.357434,
author = {Allen, Robert B. and Scerbo, M. W.},
title = {Details of Command-Language Keystrokes},
year = {1983},
issue_date = {April 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/357431.357434},
doi = {10.1145/357431.357434},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {159–178},
numpages = {20}
}

@article{10.1145/357431.357433,
author = {Stonebraker, Michael and Stettner, Heidi and Lynn, Nadene and Kalash, Joseph and Guttman, Antonin},
title = {Document Processing in a Relational Database System},
year = {1983},
issue_date = {April 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/357431.357433},
doi = {10.1145/357431.357433},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {143–158},
numpages = {16}
}

@article{10.1145/357431.357432,
author = {Nutt, Gary J.},
title = {An Experimental Distributed Modeling System},
year = {1983},
issue_date = {April 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/357431.357432},
doi = {10.1145/357431.357432},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {117–142},
numpages = {26}
}

@article{10.1145/357423.357430,
author = {Malone, Thomas W.},
title = {How Do People Organize Their Desks? Implications for the Design of Office Information Systems},
year = {1983},
issue_date = {Jan. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357423.357430},
doi = {10.1145/357423.357430},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {99–112},
numpages = {14}
}

@article{10.1145/357423.357429,
author = {Tsichritzis, Dennis and Christodoulakis, Stavros},
title = {Message Files},
year = {1983},
issue_date = {Jan. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357423.357429},
doi = {10.1145/357423.357429},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {88–98},
numpages = {11}
}

@article{10.1145/357423.357428,
author = {Srihari, Sargur N. and Hull, Jonathan J. and Choudhari, Ramesh},
title = {Integrating Diverse Knowledge Sources in Text Recognition},
year = {1983},
issue_date = {Jan. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357423.357428},
doi = {10.1145/357423.357428},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {68–87},
numpages = {20}
}

@article{10.1145/357423.357427,
author = {Barber, Gerald},
title = {Supporting Organizational Problem Solving with a Work Station},
year = {1983},
issue_date = {Jan. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357423.357427},
doi = {10.1145/357423.357427},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {45–67},
numpages = {23}
}

@article{10.1145/357423.357426,
author = {Bailey, Andrew D. and Gerlach, James H. and McAfee, R. Preston and Whinston, Andrew B.},
title = {An OIS Model for Internal Accounting Control Evaluation},
year = {1983},
issue_date = {Jan. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357423.357426},
doi = {10.1145/357423.357426},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {25–44},
numpages = {20}
}

@article{10.1145/357423.357425,
author = {Purvy, Robert and Farrell, Jerry and Klose, Paul},
title = {The Design of Star's Records Processing: Data Processing for the Noncomputer Professional},
year = {1983},
issue_date = {Jan. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357423.357425},
doi = {10.1145/357423.357425},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {3–24},
numpages = {22}
}

@article{10.1145/357423.357424,
author = {Limb, John O.},
title = {Editor's Introduction},
year = {1983},
issue_date = {Jan. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/357423.357424},
doi = {10.1145/357423.357424},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {1–2},
numpages = {2}
}
