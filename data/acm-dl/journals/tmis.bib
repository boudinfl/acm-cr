@article{10.1145/2019618.2019625,
author = {Lin, Ming-Chih and Lee, Anthony J. T. and Kao, Rung-Tai and Chen, Kuo-Tay},
title = {Stock Price Movement Prediction Using Representative Prototypes of Financial Reports},
year = {2008},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2019618.2019625},
doi = {10.1145/2019618.2019625},
abstract = {Stock price movement prediction is an appealing topic not only for research but also for commercial applications. Most of prior research separately analyzes the meanings of the qualitative or quantitative features, and does not consider the categorical information when clustering financial reports. Since quantitative or qualitative features contain only partial information, there may be no synergy by considering them individually. It is more appropriate to predict stock price movements by simultaneously taking both quantitative and qualitative features into account. Therefore, in this study, we utilize a weighting scheme to combine both qualitative and quantitative features of financial reports together, and propose a method to predict short-term stock price movements. The proposed method employs the categorical information to localize the clusters and improve the purity of each resultant cluster. We gathered 26,255 reports of companies listed in the S&amp;P 500 index from the EDGAR database and conducted the GICS (Global Industrial Classification System) experiments based on the industry sectors. The empirical evaluation results show that the proposed method outperforms the SVM, na\"{\i}ve Bayes, and PFHC methods in terms of accuracy and average profit.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {19},
numpages = {18},
keywords = {hybrid clustering, document clustering, financial report, Stock price movement prediction}
}

@article{10.1145/2019618.2019624,
author = {Huang, Ke-Wei and Li, Zhuolun},
title = {A Multilabel Text Classification Algorithm for Labeling Risk Factors in SEC Form 10-K},
year = {2008},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2019618.2019624},
doi = {10.1145/2019618.2019624},
abstract = {This study develops, implements, and evaluates a multilabel text classification algorithm called the multilabel categorical K-nearest neighbor (ML-CKNN). The proposed algorithm is designed to automatically identify 25 types of risk factors with specific meanings reported in Section 1A of SEC form 10-K. The idea of ML-CKNN is to compute a categorical similarity score for each label by the K-nearest neighbors in that category. ML-CKNN is tailored to achieve the goal of extracting risk factors from 10Ks. The proposed algorithm can perfectly classify 74.94% of risk factors and 98.75% of labels. Moreover, ML-CKNN is empirically shown to outperform ML-KNN and other multilabel algorithms. The extracted risk factors could be valuable to empirical studies in accounting or finance.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {18},
numpages = {19},
keywords = {multilabel classification, Text classification, annual reports, text mining, risk factors}
}

@article{10.1145/2019618.2019623,
author = {Schmidt-Rauch, Susanne and Schwabe, Gerhard},
title = {From Telesales to Tele-Advisory in Travel Agencies: Business Problems, Generic Design Goals and Requirements},
year = {2008},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2019618.2019623},
doi = {10.1145/2019618.2019623},
abstract = {This article describes a multiloop design approach, demonstrating two concatenated build-and-evaluate loops of a project that aims at enhancing and improving tele-counseling at travel agency call-centers. The socio-technical system design intends to implement and support a collaborative travel counseling concept which equalizes travel agents and customers within a value co-created service encounter to resolve the specific problems in travel agency call-centers. The design is finally guided by four instrumental goals: (1) increase transparency, (2) improve information quality, (3) support joint problem solving, and (4) create advisory experience. While the first build-and-evaluate loop mainly enhanced the workplace picture, the second build-and-evaluate loop primarily revealed organizational demands. This indicates that the proposed design approach enables sequentially broadening the problem space for design and supports iteratively moving forward the utility of the emerging artifact. Design is informed by concatenating insights from the previous loops which enrich understanding of the instrumental goals. The presented goals as well as the design approach are encouraging candidates for additional testing as a base for general design principles.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {17},
numpages = {13},
keywords = {value cocreation, concatenation, Design science, tele-advisory service, remote travel counseling}
}

@article{10.1145/2019618.2019622,
author = {Masud, Mohammad M. and Al-Khateeb, Tahseen M. and Hamlen, Kevin W. and Gao, Jing and Khan, Latifur and Han, Jiawei and Thuraisingham, Bhavani},
title = {Cloud-Based Malware Detection for Evolving Data Streams},
year = {2008},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2019618.2019622},
doi = {10.1145/2019618.2019622},
abstract = {Data stream classification for intrusion detection poses at least three major challenges. First, these data streams are typically infinite-length, making traditional multipass learning algorithms inapplicable. Second, they exhibit significant concept-drift as attackers react and adapt to defenses. Third, for data streams that do not have any fixed feature set, such as text streams, an additional feature extraction and selection task must be performed. If the number of candidate features is too large, then traditional feature extraction techniques fail.In order to address the first two challenges, this article proposes a multipartition, multichunk ensemble classifier in which a collection of v classifiers is trained from r consecutive data chunks using v-fold partitioning of the data, yielding an ensemble of such classifiers. This multipartition, multichunk ensemble technique significantly reduces classification error compared to existing single-partition, single-chunk ensemble approaches, wherein a single data chunk is used to train each classifier. To address the third challenge, a feature extraction and selection technique is proposed for data streams that do not have any fixed feature set. The technique's scalability is demonstrated through an implementation for the Hadoop MapReduce cloud computing architecture. Both theoretical and empirical evidence demonstrate its effectiveness over other state-of-the-art stream classification techniques on synthetic data, real botnet traffic, and malicious executables.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {16},
numpages = {27},
keywords = {n-gram analysis, Data mining, data streams, malicious executable, malware detection}
}

@article{10.1145/2019618.2019621,
author = {Peng, Jing and Zeng, Daniel D. and Huang, Zan},
title = {Latent Subject-Centered Modeling of Collaborative Tagging: An Application in Social Search},
year = {2008},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2019618.2019621},
doi = {10.1145/2019618.2019621},
abstract = {Collaborative tagging or social bookmarking is a main component of Web 2.0 systems and has been widely recognized as one of the key technologies underpinning next-generation knowledge management platforms. In this article, we propose a subject-centered model of collaborative tagging to account for the ternary cooccurrences involving users, items, and tags in such systems. Extending the well-established probabilistic latent semantic analysis theory for knowledge representation, our model maps the user, item, and tag entities into a common latent subject space that captures the “wisdom of the crowd” resulted from the collaborative tagging process. To put this model into action, we have developed a novel way to estimate the probabilistic subject-centered model approximately in a highly efficient manner taking advantage of a matrix factorization method. Our empirical evaluation shows that our proposed approach delivers substantial performance improvement on the knowledge resource recommendation task over the state-of-the-art standard and tag-aware resource recommendation algorithms.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {15},
numpages = {23},
keywords = {tag-based recommendation, social search, subject-centered modeling, item recommendation, Collaborative tagging}
}

@article{10.1145/2019618.2019620,
author = {Sutanto, Juliana and Kankanhalli, Atreyi and Tan, Bernard Cheng Yian},
title = {Eliciting a Sense of Virtual Community among Knowledge Contributors},
year = {2008},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2019618.2019620},
doi = {10.1145/2019618.2019620},
abstract = {Member-initiated virtual communities for product knowledge sharing and commerce purposes are proliferating as useful alternatives to company information and commerce Web sites. Although such communities are easy to create with the availability of numerous tools, the challenge lies in keeping the community alive and thriving. Key to sustainability is members' Sense Of Virtual Community (SOVC) so that they feel responsible for contributing their knowledge and creating value for others. However, it is unclear what leads to the SOVC among knowledge contributors. Building on appraisal theory, we hypothesize that the fulfillment of contributors' informational, instrumental, entertainment, self-discovery, and social enhancement needs will increase their SOVC. To test the hypotheses, we surveyed knowledge contributors in a beauty-product-related community to examine the relationship between their needs' fulfillment and SOVC levels. Other than the social enhancement need, all other needs' fulfillment were found to be positively related to SOVC levels. To further understand how the SOVC of knowledge contributors changes over time, we conducted a longitudinal analysis of a panel of these members. We discovered that over time, changes in the perceived fulfillment of their instrumental, entertainment, and self-discovery needs determined the change of their SOVC. The results have implications for future research as well as for the sustainability and value generation from such virtual communities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {14},
numpages = {17},
keywords = {knowledge contributors, longitudinal analysis, needs fulfillment, Sense of virtual community}
}

@article{10.1145/1877725.1877732,
author = {Fu, Yu and Chen, Zhiyuan and Koru, Gunes and Gangopadhyay, Aryya},
title = {A Privacy Protection Technique for Publishing Data Mining Models and Research Data},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1877725.1877732},
doi = {10.1145/1877725.1877732},
abstract = {Data mining techniques have been widely used in many research disciplines such as medicine, life sciences, and social sciences to extract useful knowledge (such as mining models) from research data. Research data often needs to be published along with the data mining model for verification or reanalysis. However, the privacy of the published data needs to be protected because otherwise the published data is subject to misuse such as linking attacks. Therefore, employing various privacy protection methods becomes necessary. However, these methods only consider privacy protection and do not guarantee that the same mining models can be built from sanitized data. Thus the published models cannot be verified using the sanitized data. This article proposes a technique that not only protects privacy, but also guarantees that the same model, in the form of decision trees or regression trees, can be built from the sanitized data. We have also experimentally shown that other mining techniques can be used to reanalyze the sanitized data. This technique can be used to promote sharing of research data.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {7},
numpages = {20},
keywords = {Preserving data mining}
}

@article{10.1145/1877725.1877731,
author = {Arazy, Ofer and Croitoru, Arie},
title = {The Sustainability of Corporate Wikis: A Time-Series Analysis of Activity Patterns},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1877725.1877731},
doi = {10.1145/1877725.1877731},
abstract = {While existing theoretical frameworks describe collective technology adoption patterns, they provide little insight regarding the expected patterns of wiki activity within projects. Another impediment to the study of wiki sustainability is the absence of time-series analysis methods that are suitable for the unique patterns of wiki activity logs. The primary goals of this study are to: (i) develop a novel method for analyzing wiki edit activity logs, (ii) reveal the temporal patterns of corporate wiki edit activity, and (iii) study the factors impacting wikis' sustainability. A validation of our proposed method demonstrates that it is superior to the baseline algorithm in the face of noisy data. Our empirical study combines wiki system edit activity logs with a survey of users' perceptions, and explores 33277 distinct wiki applications within one global organization over the first 5 years of wiki operation. Our results reveal six different prototypical wiki activity patterns, and show that most corporate wikis become inactive after a relatively short period. Findings from the user survey show that users of sustainable wikis are more satisfied with the wiki system and its contents, and feel that the wiki provides them with a sense of community and productivity enhancements.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {6},
numpages = {24},
keywords = {edit activity patterns, corporate, time series, sustainability, clustering, lifecycle, Wiki}
}

@article{10.1145/1877725.1877730,
author = {Cao, Lan and Ramesh, Balasubramaniam and Abdel-Hamid, Tarek},
title = {Modeling Dynamics in Agile Software Development},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1877725.1877730},
doi = {10.1145/1877725.1877730},
abstract = {Changes in the business environment such as turbulent market forces, rapidly evolving system requirements, and advances in technology demand agility in the development of software systems. Though agile approaches have received wide attention, empirical research that evaluates their effectiveness and appropriateness is scarce. Further, research to-date has investigated individual practices in isolation rather than as an integrated system. Addressing these concerns, we develop a system dynamics simulation model that considers the complex interdependencies among the variety of practices used in agile development. The model is developed on the basis of an extensive review of the literature as well as quantitative and qualitative data collected from real projects in nine organizations. We present the structure of the model focusing on essential agile practices. The validity of the model is established based on extensive structural and behavioral validation tests. Insights gained from experimentation with the model answer important questions faced by development teams in implementing two unique practices used in agile development. The results suggest that due to refactoring, the cost of implementing changes to a system varies cyclically and increases during later phases of development. Delays in refactoring also increase costs and decrease development productivity. Also, the simulation shows that pair programming helps complete more tasks and at a lower cost. The systems dynamics model developed in this research can be used as a tool by IS organizations to understand and analyze the impacts of various agile development practices and project management strategies.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {5},
numpages = {26},
keywords = {process modeling, system dynamics, Agile software development, simulation}
}

@article{10.1145/1877725.1877729,
author = {Ba, Sulin and Ke, Dan and Stallaert, Jan and Zhang, Zhongju},
title = {Why Give Away Something for Nothing? Investigating Virtual Goods Pricing and Permission Strategies},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1877725.1877729},
doi = {10.1145/1877725.1877729},
abstract = {With the rapid increase of virtual goods created for virtual world exchanges and the record growth of user-to-user transactions in these in-world economies, an important question is how a creator sets prices for a virtual good so as to maximize her profit from her creation. Virtual goods share similar economic properties (such as substantial production cost and negligible marginal cost) with other types of digital goods. However, one aspect that distinguishes a virtual good is that consumers in a virtual world may want to use multiple copies of the identical good at the same time, and such simultaneous use of multiple copies of the identical good increases a consumer's utility. In this research, we focus on the COPY permission of virtual goods. We develop an economic model to examine under what conditions the COPY permission setting leads to the highest profit for the creator of a virtual good, and what the pricing strategies are in a dynamic setting when such permission choices are present. Theoretical and practical implications of the research are discussed.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {4},
numpages = {22},
keywords = {digital goods, permissions, pricing strategy, virtual goods, Virtual worlds}
}

@article{10.1145/1877725.1877728,
author = {Wang, Jingguo and Xiao, Nan and Rao, H. Raghav},
title = {Drivers of Information Security Search Behavior: An Investigation of Network Attacks and Vulnerability Disclosures},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1877725.1877728},
doi = {10.1145/1877725.1877728},
abstract = {More and more people use search engines to seek for various information. This study investigates the search behavior that drives the search for information security knowledge via a search engine. Based on theories in information search and information security behavior we examine the effects of network attacks and vulnerability disclosures on search for information security knowledge by ordinary users. We construct a unique dataset from publicly available sources, and use a dynamic regression model to test the hypotheses empirically. We find that network attacks of current day and one day prior significantly impact the search, while vulnerability disclosure does not significantly affect the search. Implications of the study are discussed.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {3},
numpages = {23},
keywords = {search engine, vulnerability disclosures, information security, Information systems, network attacks, dynamic regression, information search behavior}
}

@article{10.1145/1877725.1877727,
author = {Davis, Gordon B. and Gray, Paul and Madnick, Stuart and Nunamaker, Jay F. and Sprague, Ralph and Whinston, Andrew},
title = {Ideas for the Future of the IS Field},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1877725.1877727},
doi = {10.1145/1877725.1877727},
abstract = {Information systems as a field of intellectual inquiry is now approximately 50 years old. It has many achievements and extensive research to its credit and has established a large group of researchers and experts worldwide. The field has changed and changed and changed again over the last half century. The question addressed in this inaugural issue article is: Where does IS go from here? This article presents the views of six of the “fathers of the field” about its directions in the years ahead. Each coauthor presents two ideas about the future. The topics covered includes continuing support of the work of organizations, emerging technologies, new ways of communicating, expanding the ways IS performs research, expanding its vision both of what IS is and of its impact, its role as a resource, its model of the IS professional and its graduates, and its staying on top of new technologies and new areas of inquiry.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {2},
numpages = {15},
keywords = {information systems impacts, university-wide resource, publications, research methods, Emerging technologies, new technologies, model of graduate, information systems vision, professional communications, model of professional, health care, systems requirements, innovations}
}

@article{10.1145/1877725.1877726,
author = {Chen, Hsinchun},
title = {Editorial: Welcome to the First Issue of <i>ACM TMIS</i>},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1877725.1877726},
doi = {10.1145/1877725.1877726},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {1},
numpages = {5}
}

@article{10.1145/1929916.1929922,
author = {Dey, Debabrata and Fan, Ming and Peng, Gang},
title = {Computer Use and Wage Returns: The Complementary Roles of IT-Related Human Capital and Nonroutine Tasks},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1929916.1929922},
doi = {10.1145/1929916.1929922},
abstract = {The effect of computer use on individual workers is relatively underresearched in the IS literature. Prior studies on computer use usually treat technologies as a “black box” and rarely look into how computers are used in workplaces and why IT-related skills are important there. In this study, we use the data from the Current Population Survey (CPS) and the data on job requirements for over 12,000 occupations from the Dictionary of Occupational Titles (DOT), to examine the complementary roles of skill sets and nonroutine tasks in providing computer-use wage returns for individual workers. We find that computer use is associated with increased levels of interactive and numerical skills required for the general workforce. In addition, workers who use computers at work and possess higher levels of interactive skills receive higher wages. We also find that computer use complements performing nonroutine tasks, particularly nonroutine abstract tasks in contributing to the wage premium. As the tasks become increasingly routine, the impact of computer use on wage returns diminishes.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {6},
numpages = {21},
keywords = {nonroutine tasks, human capital, Computer use, wage returns}
}

@article{10.1145/1929916.1929921,
author = {Dawson, Gregory S. and Watson, Richard T.},
title = {Uncovering and Testing Archetypes of Effective Public Sector CIOs},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1929916.1929921},
doi = {10.1145/1929916.1929921},
abstract = {Given the importance of public sector CIOs to government performance and citizens' faith in democracy as an efficient provider of services, it is important to understand what makes some government CIOs more effective than others. Q Method is used to uncover five archetypes of public sector CIOs which are shown to be reliable across two Q sorts. These archetypes include politically-oriented CIO, savvy negotiator, technology optimizer, and skillful communicator. Further analysis using a tournament scoring approach indicates that business-oriented CIOs are the most effective. Applying a stakeholder perspective to interpret the results, it is proposed that business-oriented CIOs understand the value in tracking closely to an organization's business leaders and strategically ignoring other stakeholders in their environment, even politically powerful ones. The development and comparison of archetypes provide a new focus of CIO research by extending from the individual level of the attribute to a combination of attributes (archetypes).},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {5},
numpages = {19},
keywords = {leadership effectiveness, public sector, stakeholder theory, strategic alignment, Chief information officer, information asymmetry}
}

@article{10.1145/1929916.1929920,
author = {Kane, Gerald C.},
title = {A Multimethod Study of Information Quality in Wiki Collaboration},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1929916.1929920},
doi = {10.1145/1929916.1929920},
abstract = {In this article, the author presents the results of a two-phase, multimethod study of wiki-based collaboration in an attempt to better understand how peer-produced collaboration is done well in wiki environments. Phase 1 involves an in-depth case study of the collaborative processes surrounding the development of the Wikipedia article on the 2007 Virginia Tech massacre. The rich data collected are used to develop an initial set of testable hypotheses of factors that enhance the quality of peer-produced information in wiki environments. Phase 2 tests these theories through a quantitative analysis of the collaborative features associated with 188 similar articles that Wikipedia considered for recognition as their best (i.e., the top 0.1%). Four collaborative features are examined for their effects on quality: volume of contributor activity, type of contributor activity, number of anonymous contributors, and top contributor experience. Volume of contributor activity is the only feature that is unsupported, a particularly interesting result because previous literature connects that factor most clearly to success in wiki-based collaboration. Implications are discussed.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {4},
numpages = {16},
keywords = {information quality, electronic communities, Wikipedia, Wiki, electronic collaboration, shaping, collaboration, peer-production, anonymity, wiki, virtual teams, Web 2.0, multimethod studies}
}

@article{10.1145/1929916.1929919,
author = {Kuo, Feng-Yang and Yin, Chun-Po},
title = {A Linguistic Analysis of Group Support Systems Interactions for Uncovering Social Realities of Organizations},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1929916.1929919},
doi = {10.1145/1929916.1929919},
abstract = {Talks are actions, and language represents the medium through which we encounter reality, carry out practical reasoning, and construct social actions. This study applies the speech act theory to analyze the data collected in a study by Trauth and Jessup [2000] and confirms previous research findings that both the topic and the group size influence the pattern of discussion, especially when issues are threatening. It also shows that the abundance of speech acts like assertives, directives, and expressives can be accounted for by a few simple recurring patterns, indicating participants are rather close-minded. More important, linguistic analysis helps uncovering defensive speech routines that inhibit the generation of valid information and create self-sealing patterns of escalating error. Linguistic analysis may therefore complement positivist and interpretive analysis by examining if participants' engagement is superficial or profound, if consensus is reached or blocked, and if certain speech acts lead to dysfunctional organizational learning. Hence, in the era of participatory Web in which language is the primary medium for interactive sharing and dynamic collaboration, linguistic analysis can be applied to study the promises and declarations that people rely on to initiate, coordinate, and complete social actions.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {3},
numpages = {21},
keywords = {speech act theory, linguistic analysis, group dynamic, Group support systems}
}

@article{10.1145/1929916.1929918,
author = {Chau, Michael},
title = {Visualizing Web Search Results Using Glyphs: Design and Evaluation of a Flower Metaphor},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1929916.1929918},
doi = {10.1145/1929916.1929918},
abstract = {While the Web provides a lot of useful information to managers and decision makers in organizations for decision support, it requires a lot of time and cognitive effort for users to sift through a search result list returned by search engines to find useful information. Previous research in information visualization has shown that visualization techniques can help users comprehend information and accomplish information tasks more efficiently and effectively. However, only a limited number of such techniques have been applied to Web search result visualization with mixed evaluation results. Using a design science approach, this research designed and implemented a glyph (a graphical object that represents the values of multiple dimensions using multiple visual parameters) and a system for visualizing Web search results. A flower metaphor was adopted in the glyph design to represent the characteristics and metadata of Web documents. Following the cognitive fit theory, an experimental study was conducted to evaluate three displays: a numeric display, a glyph display, and a combined display which showed numbers only, glyphs only, and both, respectively. Experimental results showed that the glyph display and the combined display performed better when task complexity was high, and the numeric display and the combined display performed better when task complexity was low. The combined display also received the best perceived usability from the subjects. Based on the findings, the implications of the study to research and practice are discussed and some future research directions are suggested.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {2},
numpages = {27},
keywords = {Web searching, glyphs, Information visualization, design science}
}

@article{10.1145/1929916.1929917,
author = {Chen, Hsinchun},
title = {Editorial: Design Science, Grand Challenges, and Societal Impacts},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/1929916.1929917},
doi = {10.1145/1929916.1929917},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {1},
numpages = {10}
}

@article{10.1145/1985347.1985353,
author = {Mcknight, D. Harrison and Carter, Michelle and Thatcher, Jason Bennett and Clay, Paul F.},
title = {Trust in a Specific Technology: An Investigation of Its Components and Measures},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/1985347.1985353},
doi = {10.1145/1985347.1985353},
abstract = {Trust plays an important role in many Information Systems (IS)-enabled situations. Most IS research employs trust as a measure of interpersonal or person-to-firm relations, such as trust in a Web vendor or a virtual team member. Although trust in other people is important, this article suggests that trust in the Information Technology (IT) itself also plays a role in shaping IT-related beliefs and behavior. To advance trust and technology research, this article presents a set of trust in technology construct definitions and measures. We also empirically examine these construct measures using tests of convergent, discriminant, and nomological validity. This study contributes to the literature by providing: (a) a framework that differentiates trust in technology from trust in people, (b) a theory-based set of definitions necessary for investigating different kinds of trust in technology, and (c) validated trust in technology measures useful to research and practice.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {12},
numpages = {25},
keywords = {construct development, Trust, trust in technology}
}

@article{10.1145/1985347.1985352,
author = {Liu, Jun and Ram, Sudha},
title = {Who Does What: Collaboration Patterns in the Wikipedia and Their Impact on Article Quality},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/1985347.1985352},
doi = {10.1145/1985347.1985352},
abstract = {The quality of Wikipedia articles is debatable. On the one hand, existing research indicates that not only are people willing to contribute articles but the quality of these articles is close to that found in conventional encyclopedias. On the other hand, the public has never stopped criticizing the quality of Wikipedia articles, and critics never have trouble finding low-quality Wikipedia articles. Why do Wikipedia articles vary widely in quality? We investigate the relationship between collaboration and Wikipedia article quality. We show that the quality of Wikipedia articles is not only dependent on the different types of contributors but also on how they collaborate. Based on an empirical study, we classify contributors based on their roles in editing individual Wikipedia articles. We identify various patterns of collaboration based on the provenance or, more specifically, who does what to Wikipedia articles. Our research helps identify collaboration patterns that are preferable or detrimental for article quality, thus providing insights for designing tools and mechanisms to improve the quality of Wikipedia articles.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {11},
numpages = {23},
keywords = {collaboration pattern, article quality, Wikipedia}
}

@article{10.1145/1985347.1985351,
author = {Du, Anna Ye and Das, Sanjukta and Gopal, Ram D. and Ramesh, R.},
title = {Risk Hedging in Storage Grid Markets: Do Options Add Value to Forwards?},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/1985347.1985351},
doi = {10.1145/1985347.1985351},
abstract = {Internet storage services allow businesses to move away from maintaining their own internal storage networks. Service providers currently follow a utility pricing model which translates to them absorbing all the risk that arises from the fluctuating storage needs of their customers. The risk borne by the Internet storage service providers has large revenue implications as Internet startups and smaller companies, which face significant demand stochasticity, constitute an important segment of their clientele. We develop an option pricing mechanism to hedge against this risk and evaluate its effectiveness vis-\`{a}-vis forward contracts. We obtain the conditions under which options dominate forward contracts and the trade-offs involved when the provider has to decide on appropriate pricing mechanisms. Our empirical study uses publicly obtainable traffic data of Amazon S3 clients to validate the analytical results. We show that providers can significantly benefit from including options in their risk-hedging portfolio, especially when there is less variation in the costs faced by the buyers in building their own data networks as opposed to using cloud services.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {10},
numpages = {23},
keywords = {cloud computing, grid computing, forward contracts, online storage, Options}
}

@article{10.1145/1985347.1985350,
author = {Uhl, Matthias W.},
title = {Explaining U.S. Consumer Behavior with News Sentiment},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/1985347.1985350},
doi = {10.1145/1985347.1985350},
abstract = {We introduce a novel dataset with a news sentiment index that was constructed from a selection of over 300,000 newspaper articles from five of the top ten U.S. newspapers by circulation. By constructing ARMA models, we show that news and consumer sentiment, when combined with other macroeconomic variables, achieve statistically significant results to explain changes in private consumption. We make three distinct findings with respect to sentiment in consumption behavior models: first, both consumer and news sentiment add explanatory power and statistical significance to conventional consumer behavior models. Second, consumer sentiment, measured by the University of Michigan Index of Consumer Sentiment, adds more explanatory power and statistical significance than news sentiment when tested individually. Third, news sentiment is able to determine the signs of all coefficients in the model correctly, whereas consumer sentiment does not. In general, we conclude that news sentiment is a useful variable to add in consumer behavior models, especially when coupled with consumer sentiment and other macroeconomic variables. Tested individually, news sentiment is as good a proxy as personal income for explaining private consumption growth when tested individually.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {9},
numpages = {18},
keywords = {News sentiment, private consumption, consumer sentiment}
}

@article{10.1145/1985347.1985349,
author = {Bhattacharjee, Sudip and Gopal, Ram D. and Marsden, James R. and Sankaranarayanan, Ramesh},
title = {Digital Goods and Markets: Emerging Issues and Challenges},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/1985347.1985349},
doi = {10.1145/1985347.1985349},
abstract = {This research commentary examines the changing landscape of digital goods, and discusses important emerging issues for IS researchers to explore. We begin with a discussion of the major technological milestones that have shaped digital goods industries such as music, movies, software, books, video games, and recently emerging digital goods. Our emphasis is on economic and legal issues, rather than on design science or sociological issues. We explore how research has been influenced by the major technological milestones and discuss the major findings of prior research. Based on this, we offer a roadmap for future researchers to explore the emergent changes in the digital goods arena, covering different aspects of digital goods industries such as risk management, value chain, legal aspects, transnational and cross-cultural issues.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {8},
numpages = {14},
keywords = {emerging technologies, distribution channels, distribution, disembodiment, digital goods markets, Digital goods, cross-cultural issues, value chain, risk management, piracy, pricing}
}

@article{10.1145/1985347.1985348,
author = {Hu, Paul Jen-Hwa and Chen, Hsinchun},
title = {Analyzing Information Systems Researchers' Productivity and Impacts: A Perspective on the H Index},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/1985347.1985348},
doi = {10.1145/1985347.1985348},
abstract = {Quantitative assessments of researchers' productivity and impacts are crucial for the information systems (IS) discipline. Motivated by its growing popularity and expanding use, we offer a perspective on the h index, which refers to the number of papers a researcher has coauthored with at least h citations each. We studied a partial list of 232 top IS researchers who received doctoral degrees between 1957 and 2003 and chose Google Scholar as the source for our analyses. At the individual level, we attempted to identify some of the most productive, high-impact researchers, as well as those who exhibited impressive paces of productivity. At the institution level, we revealed some institutions with relatively more productive researchers, as well as institutions that had produced more productive researchers. We also analyzed the overall IS community by examining the primary research areas of productive scholars identified by our analyses. We then compared their h index scores with those of top scholars in several related disciplines},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {7},
numpages = {8},
keywords = {H index, Information systems, analysis of scholars' productivity and impacts}
}

@article{10.1145/2019618.2019619,
author = {Tuzhilin, Alexander},
title = {Knowledge Management Revisited: Old Dogs, New Tricks},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2019618.2019619},
doi = {10.1145/2019618.2019619},
abstract = {The article considers the (old) field of Knowledge Management (KM) and examines the factors that led to its decline over the last several years. It also argues that the demise of knowledge management is premature and that it is time to revisit the field and take it to the “next level” of academic research and business practice. Then the article presents a proposal of how to do it by deploying certain “new tricks,” that is, various modern developments in the fields related to KM that can make a difference and solve the old problems of knowledge management.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {13},
numpages = {11},
keywords = {content management, personalization, user-generated content, Knowledge management, content processing and analysis}
}

@article{10.1145/2070710.2070717,
author = {Marx, Frederik and Mayer, J\"{o}rg H. and Winter, Robert},
title = {Six Principles for Redesigning Executive Information Systems—Findings of a Survey and Evaluation of a Prototype},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2070710.2070717},
doi = {10.1145/2070710.2070717},
abstract = {Information Systems (IS) meant to help senior managers are known as Executive Information Systems (EIS). Despite a five-decade tradition of such IS, many executives still complain that they bear little relevance to managing a company and, even more, fail to accommodate their working style. The increasing acceptance of IS among today's executives and technological advances of the Internet era make the present moment favorable for redesigning EIS. Following the design science paradigm in IS research, this article provides six principles for such a redesign. To do so, we survey executives regarding their requirements and the IS they currently use. We then derive principles for a redesign to fill the gaps. They address diverse areas: a comprehensive information model, functions to better analyze and process information, easy-to-use IS handling, a more flexible IS architecture and data model, a proper information management, and fast prototype implementation. Finally a field test demonstrates and evaluates the utility of our proposal by means of a prototype.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {26},
numpages = {19},
keywords = {Corporate business intelligence, design principles, design requirements, executive information system (EIS), working style}
}

@article{10.1145/2070710.2070716,
author = {Lau, Raymond Y. K. and Liao, S. Y. and Kwok, Ron Chi-Wai and Xu, Kaiquan and Xia, Yunqing and Li, Yuefeng},
title = {Text Mining and Probabilistic Language Modeling for Online Review Spam Detection},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2070710.2070716},
doi = {10.1145/2070710.2070716},
abstract = {In the era of Web 2.0, huge volumes of consumer reviews are posted to the Internet every day. Manual approaches to detecting and analyzing fake reviews (i.e., spam) are not practical due to the problem of information overload. However, the design and development of automated methods of detecting fake reviews is a challenging research problem. The main reason is that fake reviews are specifically composed to mislead readers, so they may appear the same as legitimate reviews (i.e., ham). As a result, discriminatory features that would enable individual reviews to be classified as spam or ham may not be available. Guided by the design science research methodology, the main contribution of this study is the design and instantiation of novel computational models for detecting fake reviews. In particular, a novel text mining model is developed and integrated into a semantic language model for the detection of untruthful reviews. The models are then evaluated based on a real-world dataset collected from amazon.com. The results of our experiments confirm that the proposed models outperform other well-known baseline models in detecting fake reviews. To the best of our knowledge, the work discussed in this article represents the first successful attempt to apply text mining methods and semantic language models to the detection of fake consumer reviews. A managerial implication of our research is that firms can apply our design artifacts to monitor online consumer reviews to develop effective marketing or product design strategies based on genuine consumer feedback posted to the Internet.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {25},
numpages = {30},
keywords = {spam detection, text mining, design science, review spam, Language models}
}

@article{10.1145/2070710.2070715,
author = {Goes, Paulo and Ilk, Noyan and Yue, Wei T. and Zhao, J. Leon},
title = {Live-Chat Agent Assignments to Heterogeneous e-Customers under Imperfect Classification},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2070710.2070715},
doi = {10.1145/2070710.2070715},
abstract = {Many e-commerce firms provide live-chat capability on their Web sites to promote product sales and to offer customer support. With increasing traffic on e-commerce Web sites, providing such live-chat services requires a good allocation of service resources to serve the customers. When resources are limited, firms may consider employing priority-processing and reserving resources for high-value customers. In this article, we model a reserve-based priority-processing policy for e-commerce systems that have imperfect customer classification. Two policy decisions considered in the model are: (1) the number of agents exclusively reserved for high-value customers, and (2) the configuration of the classification system. We derive explicit expressions for average waiting times of high-value and low-value customer classes and define a total waiting cost function. Through numerical analysis, we study the impact of these two policy decisions on average waiting times and total waiting costs. Our analysis finds that reserving agents for high-value customers may have negative consequences for such customers under imperfect classification. Further, we study the interaction between the two policy decisions and discuss how one decision should be modified with respect to a change in the other one in order to keep the waiting costs minimized.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {24},
numpages = {15},
keywords = {live-chat system, priority policy, Resource allocation, customer classification}
}

@article{10.1145/2070710.2070714,
author = {Arora, Hina and Raghu, T. S. and Vinze, Ajay},
title = {Decision Support for Containing Pandemic Propagation},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2070710.2070714},
doi = {10.1145/2070710.2070714},
abstract = {This research addresses complexities inherent in dynamic decision making settings represented by global disasters such as influenza pandemics. By coupling a theoretically grounded Equation-Based Modeling (EBM) approach with more practically nuanced Agent-Based Modeling (ABM) approach we address the inherent heterogeneity of the “influenza pandemic” decision space more effectively. In addition to modeling contributions, results and findings of this study have three important policy implications for pandemic containment; first, an effective way of checking the progression of a pandemic is a multipronged approach that includes a combination of pharmaceutical and non-pharmaceutical interventions. Second, mutual aid is effective only when regions that have been affected by the pandemic are sufficiently isolated from other regions through non-pharmaceutical interventions. When regions are not sufficiently isolated, mutual aid can in fact be detrimental. Finally, intraregion non-pharmaceutical interventions such as school closures are more effective than interregion nonpharmaceutical interventions such as border closures.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {23},
numpages = {25},
keywords = {pandemics, public health, resource allocation, Dynamic decision making, multiagent simulation}
}

@article{10.1145/2070710.2070713,
author = {Rui, Huaxia and Whinston, Andrew},
title = {Designing a Social-Broadcasting-Based Business Intelligence System},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2070710.2070713},
doi = {10.1145/2070710.2070713},
abstract = {The rise of social media has fundamentally changed the way information is produced, disseminated, and consumed in the digital age, which has profound economic and business effects. Among many different types of social media, social broadcasting networks such as Twitter in the U.S. and “Weibo” in China are particularly interesting from a business perspective. In the case of Twitter, the huge amounts of real-time data with extremely rich text, along with valuable structural information, makes Twitter a great platform to build Business Intelligence (BI) systems. We propose a framework of social-broadcasting-based BI systems that utilizes real-time information extracted from these data with text mining techniques. To demonstrate this framework, we designed and implemented a Twitter-based BI system that forecasts movie box office revenues during the opening weekend and forecasts daily revenue after 4 weeks. We found that incorporating information from Twitter could reduce the Mean Absolute Percentage Error (MAPE) by 44% for the opening weekend and by 36% for total revenue. For daily revenue forecasting, including Twitter information into a baseline model could reduce forecasting errors by 17.5% on average. On the basis of these results, we conclude that social-broadcasting-based BI systems have great potential and should be explored by both researchers and practitioners.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {22},
numpages = {19},
keywords = {Business intelligence, social broadcasting, forecasting, Twitter}
}

@article{10.1145/2070710.2070712,
author = {Padmanabhan, Balaji and Hevner, Alan and Cuenco, Michael and Shi, Crystal},
title = {From Information to Operations: Service Quality and Customer Retention},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2070710.2070712},
doi = {10.1145/2070710.2070712},
abstract = {In business, information is abundant. Yet, effective use of that information to inform and drive business operations is a challenge. Our industry-university collaborative project draws from a rich dataset of commercial demographics, transaction history, product features, and Service Quality Index (SQI) factors on shipping transactions at FedEx. We apply inductive methods to understand and predict customer churn in a noncontractual setting. Results identify several SQI variables as important determinants of churn across a variety of analytic approaches. Building on this we propose the design of a Business Intelligence (BI) dashboard as an innovative approach for increasing customer retention by identifying potential churners based on combinations of predictor variables such as demographics and SQI factors. This empirical study contributes to BI research and practice by demonstrating the application of data analytics to the fundamental business operations problem of customer churn.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {21},
numpages = {21},
keywords = {pattern discovery, service quality index, data mining, Business intelligence, customer churn, data analytics}
}

@article{10.1145/2070710.2070711,
author = {Nunamaker, Jr., Jay F. and Briggs, Robert O.},
title = {Toward a Broader Vision for Information Systems},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2070710.2070711},
doi = {10.1145/2070710.2070711},
abstract = {In December of 2009, several founders of the Information Systems (IS) academic discipline gathered for a panel discussion at the International Conference on Information Systems to present their visions for the future of the field, and their comments were summarized in the inaugural issue of TMIS [Davis et al., 2010; J. F. J. Nunamaker et al., 1991]. To assure a robust future, they argued, IS journals, conferences, reviewers, promotion committees, teachers, researchers, and curriculum developers must broaden the scope of IS. This article explores the need for a broader vision to drive future development of the IS discipline.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {20},
numpages = {12},
keywords = {emerging technologies, information systems vision, innovations, Design science, research methods, health care, model of professional, model of graduate, professional communications, systems requirements, new technologies, information systems impacts, university-wide resource, publications}
}

@article{10.1145/2151163.2151169,
author = {Malhotra, Arvind and Majchrzak, Ann},
title = {How Virtual Teams Use Their Virtual Workspace to Coordinate Knowledge},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2151163.2151169},
doi = {10.1145/2151163.2151169},
abstract = {Virtual team members increasingly rely on virtual workspace tools to coordinate knowledge that each individual brings to the team. How the use of these tools affects knowledge coordination within virtual teams is not well understood. We distinguish between tools as features and the use of the virtual workspace as providing affordances for behaviors. Using situational awareness theory, we hypothesized two affordances of virtual workspaces that facilitate knowledge coordination. Using trading zone theory, we hypothesized two forms of trading zones created by features of virtual workspaces and the impact of these trading zones on the creation of affordances for team members. Members of 54 teams were asked about the affordances of the virtual workspace, and team leaders were asked about specific tools provided to the team. Our hypothesized model was supported: the different forms of trading zones were differentially related to the different affordances and on affordances were related to knowledge coordination satisfaction. Theoretical implications focus on the distinction between features and affordances and on the identification of specific features that affect specific affordances. Practical implications for managers and engineers supporting virtual teams include the utility of becoming knowledgeable about different forms of trading zones that virtual workspaces can provide and understanding the relationship between trading zones and different affordances.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {6},
numpages = {14},
keywords = {tool categories, task awareness, presence awareness, situational awareness, Virtual teams}
}

@article{10.1145/2151163.2151168,
author = {Zhang, Zhu and Li, Xin and Chen, Yubo},
title = {Deciphering Word-of-Mouth in Social Media: Text-Based Metrics of Consumer Reviews},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2151163.2151168},
doi = {10.1145/2151163.2151168},
abstract = {Enabled by Web 2.0 technologies, social media provide an unparalleled platform for consumers to share their product experiences and opinions through word-of-mouth (WOM) or consumer reviews. It has become increasingly important to understand how WOM content and metrics influence consumer purchases and product sales. By integrating marketing theories with text mining techniques, we propose a set of novel measures that focus on sentiment divergence in consumer product reviews. To test the validity of these metrics, we conduct an empirical study based on data from Amazon.com and BN.com (Barnes &amp; Noble). The results demonstrate significant effects of our proposed measures on product sales. This effect is not fully captured by nontextual review measures such as numerical ratings. Furthermore, in capturing the sales effect of review content, our divergence metrics are shown to be superior to and more appropriate than some commonly used textual measures the literature. The findings provide important insights into the business impact of social media and user-generated content, an emerging problem in business intelligence research. From a managerial perspective, our results suggest that firms should pay special attention to textual content information when managing social media and, more importantly, focus on the right measures.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {5},
numpages = {23},
keywords = {text mining, social media, sentiment analysis, consumer reviews, Word-of-mouth}
}

@article{10.1145/2151163.2151167,
author = {Robinson, William N. and Akhlaghi, Arash and Deng, Tianjie and Syed, Ali Raza},
title = {Discovery and Diagnosis of Behavioral Transitions in Patient Event Streams},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2151163.2151167},
doi = {10.1145/2151163.2151167},
abstract = {Users with cognitive impairments use assistive technology (AT) as part of a clinical treatment plan. As the AT interface is manipulated, data stream mining techniques are used to monitor user goals. In this context, real-time data mining aids clinicians in tracking user behaviors as they attempt to achieve their goals. Quality metrics over stream-mined models identify potential changes in user goal attainment, as the user learns his or her personalized emailing system. When the quality of some data-mined models varies significantly from nearby models—as defined by quality metrics—the user's behavior is then flagged as a significant behavioral change. The specific changes in user behavior are then characterized by differencing the data-mined decision tree models. This article describes how model quality monitoring and decision tree differencing can aid in recognition and diagnoses of behavioral changes in a case study of cognitive rehabilitation via emailing. The technique may be more widely applicable to other real-time data-intensive analysis problems.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {4},
numpages = {28},
keywords = {stream mining, decision trees, behavioral rehabilitation, real-time data mining, clinical treatment plans, Patient monitoring}
}

@article{10.1145/2151163.2151166,
author = {Adomavicius, Gediminas and Zhang, Jingjing},
title = {Impact of Data Characteristics on Recommender Systems Performance},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2151163.2151166},
doi = {10.1145/2151163.2151166},
abstract = {This article investigates the impact of rating data characteristics on the performance of several popular recommendation algorithms, including user-based and item-based collaborative filtering, as well as matrix factorization. We focus on three groups of data characteristics: rating space, rating frequency distribution, and rating value distribution. A sampling procedure was employed to obtain different rating data subsamples with varying characteristics; recommendation algorithms were used to estimate the predictive accuracy for each sample; and linear regression-based models were used to uncover the relationships between data characteristics and recommendation accuracy. Experimental results on multiple rating datasets show the consistent and significant effects of several data characteristics on recommendation accuracy.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {3},
numpages = {17},
keywords = {data characteristics, Performance of recommender systems, collaborative filtering, accuracy of recommendation algorithms}
}

@article{10.1145/2151163.2151165,
author = {Basoglu, K. Asli and Fuller, Mark A. and Valacich, Joseph S.},
title = {Enhancement of Recall within Technology-Mediated Teams through the Use of Online Visual Artifacts},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2151163.2151165},
doi = {10.1145/2151163.2151165},
abstract = {Given the distributed nature of modern organizations, the use of technology-mediated teams is a critical aspect of their success. These teams use various media that are arguably less personal than face-to-face communication. One factor influencing the success of these teams is their ability to develop an understanding of who knows what during the initial team development stage. However, this development of understanding within dispersed teams may be impeded because of the limitations of technology-enabled communication environments. Past research has found that a limited understanding of team member capabilities hinders team performance. As such, this article investigates mechanisms for improving the recall of individuals within dispersed teams. Utilizing the input-process-output model to conceptualize the group interaction process, three input factors—visual artifacts (i.e., a computer-generated image of each team member), team size, and work interruptions—are manipulated to assess their influence on a person's ability to recall important characteristics of their virtual team members. Results show that visual artifacts significantly increase the recall of individuals' information. However, high-urgency interruptions significantly deteriorate the recall of individuals, regardless of the visual artifact or team size. These findings provide theoretical and practical implications on knowledge acquisition and project success within technology-mediated teams.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {2},
numpages = {22},
keywords = {visual artifact, team size, recall, team complexity, Technology-mediated teams, interruptions}
}

@article{10.1145/2151163.2151164,
author = {Niederman, Fred and March, Salvatore T.},
title = {Design Science and the Accumulation of Knowledge in the Information Systems Discipline},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2151163.2151164},
doi = {10.1145/2151163.2151164},
abstract = {Design science has emerged as an important research paradigm in the information systems (IS) discipline, and much has been written on how it should be conducted and evaluated (e.g., Hevner et al. [2004]; Walls et al. [1992]; Vaishnavi and Kuechler [2007]; Kuechler and Vaishnavi [2008]; Peffers et al. [2007]; Iivari [2010]; Pigneur [2011]). We contend that, as a socio-technical discipline, IS research must address the interaction between design and behavior. We begin with a background discussion of what we mean by IS research and the nature of the relationship between design and behavioral approaches to IS research. We discuss the nature of design, design science, and IT artifacts within information systems research and describe the importance of linking design and behavioral perspectives. We illustrate several key points using selected articles recently published in ACM Transactions on Management Information Systems [Schmidt-Rauch and Schwabe 2011; Lau et al. 2011]. We conclude with a vision of IS research in which the capabilities and affordances of IT artifacts are incorporated into behavioral studies; the results of behavioral studies are utilized in the development and evaluation of IT artifacts; and both behavioral and design perspectives are used to address the important problems of our constituent community.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {1},
numpages = {15},
keywords = {design science, IS behavioral research performance, IS artifacts, philosophy of IS}
}

@article{10.1145/2229156.2229161,
author = {Nussbaumer, Philipp and Matter, Inu and Schwabe, Gerhard},
title = {“Enforced” vs. “Casual” Transparency -- Findings from IT-Supported Financial Advisory Encounters},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2229156.2229161},
doi = {10.1145/2229156.2229161},
abstract = {In sales-oriented service encounters like financial advice, the client may perceive information and interest asymmetries as a lack of transparency regarding the advisor’s activities. In this article, we will discuss two design iterations of a supportive tabletop application that we built to increase process and information transparency as compared to the traditional pen and paper encounters. While the first iteration’s design was “enforcing” transparency and therefore proved to be a failure [Nussbaumer et al. 2011], we built the second iteration on design rationales enabling more “casual” transparency. Experimental evaluations show that the redesigned system significantly increases the client’s perceived transparency, her perceived control of the encounter and improves her perceived trustworthiness of and satisfaction with the encounter. With these findings, we contribute to (1) insight into the role of transparency advisory encounter design; (2) design solutions for establishing particular facets of transparency and their potential instantiations in tabletop systems; and (3) insight into the process of designing for transparency with socio-technical artifacts that are emergent as a result of design activities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {11},
numpages = {19},
keywords = {collaboration, tabletop, Design science, investment advice, concatenation}
}

@article{10.1145/2229156.2229160,
author = {Shan, Zhe and Kumar, Akhil},
title = {Optimal Adapter Creation for Process Composition in Synchronous vs. Asynchronous Communication},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2229156.2229160},
doi = {10.1145/2229156.2229160},
abstract = {A key issue in process-aware e-commerce collaboration is to orchestrate business processes of multiple business partners throughout a supply chain network in an automated and seamless way. Since each partner has its own internal processes with different control flow structures and message interfaces, the real challenge lies in verifying the correctness of process collaboration, and reconciling conflicts in an automated manner to make collaboration successful. The purpose of business process adaptation is to mediate the communication between independent processes to overcome their mismatches and incompatibilities. The goal of this article is to develop and compare efficient approaches of optimal adapter (i.e. one that minimizes the number of messages to be adapted) creation for multiple interacting processes under both synchronous and asynchronous communication. We start with an analysis of interactions of each message pair, and show how to identify incompatible cases and their adaptation elements for both types of communication. Then, we show how to extend this analysis into more general cases involving M messages and N processes (M, N &gt; 2). Further, we present optimal adapter creation algorithms for both scenarios based on our analysis technique. The algorithms were implemented in a Java-based prototype system, and results of two experiments are reported. We compare and discuss the insights gained about adapter creation in these two scenarios.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {8},
numpages = {33},
keywords = {process metrics, Process compatibility, optimal adapter, integer programming, synchronous and asynchronous communication, performance evaluation, adapter creation}
}

@article{10.1145/2229156.2229159,
author = {Huang, Zan and Zhao, Huimin and Zhu, Dan},
title = {Two New Prediction-Driven Approaches to Discrete Choice Prediction},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2229156.2229159},
doi = {10.1145/2229156.2229159},
abstract = {The ability to predict consumer choices is essential in understanding the demand structure of products and services. Typical discrete choice models that are targeted at providing an understanding of the behavioral process leading to choice outcomes are developed around two main assumptions: the existence of a utility function that represents the preferences over a choice set and the relatively simple and interpretable functional form for the utility function with respect to attributes of alternatives and decision makers. These assumptions lead to models that can be easily interpreted to provide insights into the effects of individual variables, such as price and promotion, on consumer choices. However, these restrictive assumptions might impede the ability of such theory-driven models to deliver accurate predictions and forecasts. In this article, we develop novel approaches targeted at providing more accurate choice predictions. Specifically, we propose two prediction-driven approaches: pairwise preference learning using classification techniques and ranking function learning using evolutionary computation. We compare our proposed approaches with a multiclass classification approach, as well as a standard discrete choice model. Our empirical results show that the proposed approaches achieved significantly higher choice prediction accuracy.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {9},
numpages = {32},
keywords = {genetic programming, Choice prediction, utility theory, discrete choice modeling, data mining}
}

@article{10.1145/2229156.2229158,
author = {Ngo-Ye, Thomas L. and Sinha, Atish P.},
title = {Analyzing Online Review Helpfulness Using a Regressional ReliefF-Enhanced Text Mining Method},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2229156.2229158},
doi = {10.1145/2229156.2229158},
abstract = {Within the emerging context of Web 2.0 social media, online customer reviews are playing an increasingly important role in disseminating information, facilitating trust, and promoting commerce in the e-marketplace. The sheer volume of customer reviews on the web produces information overload for readers. Developing a system that can automatically identify the most helpful reviews would be valuable to businesses that are interested in gathering informative and meaningful customer feedback. Because the target variable---review helpfulness---is continuous, common feature selection techniques from text classification cannot be applied. In this article, we propose and investigate a text mining model, enhanced using the Regressional ReliefF (RReliefF) feature selection method, for predicting the helpfulness of online reviews from Amazon.com. We find that RReliefF significantly outperforms two popular dimension reduction methods. This study is the first to investigate and compare different dimension reduction techniques in the context of applying text regression for predicting online review helpfulness. Another contribution is that our analysis of the keywords selected by RReliefF reveals meaningful feature groupings.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {10},
numpages = {20},
keywords = {text mining, dimension reduction, regressional reliefF, Business intelligence, online reviews}
}

@article{10.1145/2229156.2229157,
author = {van der Aalst, Wil},
title = {Process Mining: Overview and Opportunities},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2229156.2229157},
doi = {10.1145/2229156.2229157},
abstract = {Over the last decade, process mining emerged as a new research field that focuses on the analysis of processes using event data. Classical data mining techniques such as classification, clustering, regression, association rule learning, and sequence/episode mining do not focus on business process models and are often only used to analyze a specific step in the overall process. Process mining focuses on end-to-end processes and is possible because of the growing availability of event data and new process discovery and conformance checking techniques.Process models are used for analysis (e.g., simulation and verification) and enactment by BPM/WFM systems. Previously, process models were typically made by hand without using event data. However, activities executed by people, machines, and software leave trails in so-called event logs. Process mining techniques use such logs to discover, analyze, and improve business processes.Recently, the Task Force on Process Mining released the Process Mining Manifesto. This manifesto is supported by 53 organizations and 77 process mining experts contributed to it. The active involvement of end-users, tool vendors, consultants, analysts, and researchers illustrates the growing significance of process mining as a bridge between data mining and business process modeling. The practical relevance of process mining and the interesting scientific challenges make process mining one of the “hot” topics in Business Process Management (BPM). This article introduces process mining as a new research field and summarizes the guiding principles and challenges described in the manifesto.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {7},
numpages = {17},
keywords = {business intelligence, data mining, business process management, Process mining}
}

@article{10.1145/2361256.2361261,
author = {Hu, Nan and Cavusoglu, Hasan and Liu, Ling and Ni, Chenkai},
title = {Do Vendors’ Pricing Decisions Fully Reflect Information in Online Reviews?},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2361256.2361261},
doi = {10.1145/2361256.2361261},
abstract = {By using online retail data collected from Amazon, Barnes &amp; Nobel, and Pricegrabber, this paper investigates whether online vendors’ pricing decisions fully reflect the information contained in various components of customers’ online reviews. The findings suggest that there is inefficiency in vendors’ pricing decisions. Specifically, vendors do not appear to fully understand the incremental predictive power of online reviews in forecasting future sales when they adjust their prices. However, they do understand demand persistence. Interestingly, vendors reduce price if the actual demand is higher than the expected demand (positive demand shock). This phenomenon is attributed to the advertising effect suggested in previous literature and the intense competitiveness of e-Commerce. Finally, we document that vendors do not change their prices directly in response to online reviews; their response to online reviews is through forecasting consumer’s future demand.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {16},
numpages = {26},
keywords = {Online reviews, market efficiency, online pricing, Mishkin test, rational expectation, vendor decisions}
}

@article{10.1145/2361256.2361260,
author = {Wei, Wei and Ram, Sudha},
title = {Using a Network Analysis Approach for Organizing Social Bookmarking Tags and Enabling Web Content Discovery},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2361256.2361260},
doi = {10.1145/2361256.2361260},
abstract = {This article describes an innovative approach to reorganizing the tag space generated by social bookmarking services. The objective of this work is to enable effective search and discovery of Web content using social bookmarking tags. Tags are metadata generated by users for Web content annotation. Their potential as effective Web search and discovery tool is hindered by challenges such as, the tag space being untidy due to ambiguity, and hidden or implicit semantics. Using a novel analytics approach, we conducted network analyses on tags and discovered that tags are generated for different purposes and that there are inherent relationships among tags. Our approach can be used to extract the purposes of tags and relationships among the tags and this information can be used as facets to add structure and hierarchy to reorganize the flat tag space. The semantics of relationships and hierarchy in our proposed faceted model of tags enable searches on annotated Web content in an effective manner. We describe the implementation of a prototype system called FASTS to demonstrate feasibility and effectiveness of our approach.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {15},
numpages = {16},
keywords = {content discovery, tags, Social bookmarking, facets}
}

@article{10.1145/2361256.2361259,
author = {Lu, Hsin-Min and Tsai, Feng-Tse and Chen, Hsinchun and Hung, Mao-Wei and Li, Shu-Hsing},
title = {Credit Rating Change Modeling Using News and Financial Ratios},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2361256.2361259},
doi = {10.1145/2361256.2361259},
abstract = {Credit ratings convey credit risk information to participants in financial markets, including investors, issuers, intermediaries, and regulators. Accurate credit rating information plays a crucial role in supporting sound financial decision-making processes. Most previous studies on credit rating modeling are based on accounting and market information. Text data are largely ignored despite the potential benefit of conveying timely information regarding a firm’s outlook. To leverage the additional information in news full-text for credit rating prediction, we designed and implemented a news full-text analysis system that provides firm-level coverage, topic, and sentiment variables. The novel topic-specific sentiment variables contain a large fraction of missing values because of uneven news coverage. The missing value problem creates a new challenge for credit rating prediction approaches. We address this issue by developing a missing-tolerant multinomial probit (MT-MNP) model, which imputes missing values based on the Bayesian theoretical framework. Our experiments using seven and a half years of real-world credit ratings and news full-text data show that (1) the overall news coverage can explain future credit rating changes while the aggregated news sentiment cannot; (2) topic-specific news coverage and sentiment have statistically significant impact on future credit rating changes; (3) topic-specific negative sentiment has a more salient impact on future credit rating changes compared to topic-specific positive sentiment; (4) MT-MNP performs better in predicting future credit rating changes compared to support vector machines (SVM). The performance gap as measured by macroaveraging F-measure is small but consistent.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {14},
numpages = {30},
keywords = {SVM, topic-specific news coverage, topic-specific news sentiment, latent Dirichlet allocation, missing-tolerant multinomial probit, news coverage, news sentiment, Credit rating changes}
}

@article{10.1145/2361256.2361258,
author = {Achananuparp, Palakorn and Lim, Ee-Peng and Jiang, Jing and Hoang, Tuan-Anh},
title = {Who is Retweeting the Tweeters? Modeling, Originating, and Promoting Behaviors in the Twitter Network},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2361256.2361258},
doi = {10.1145/2361256.2361258},
abstract = {Real-time microblogging systems such as Twitter offer users an easy and lightweight means to exchange information. Instead of writing formal and lengthy messages, microbloggers prefer to frequently broadcast several short messages to be read by other users. Only when messages are interesting, are they propagated further by the readers. In this article, we examine user behavior relevant to information propagation through microblogging. We specifically use retweeting activities among Twitter users to define and model originating and promoting behavior. We propose a basic model for measuring the two behaviors, a mutual dependency model, which considers the mutual relationships between the two behaviors, and a range-based model, which considers the depth and reach of users’ original tweets. Next, we compare the three behavior models and contrast them with the existing work on modeling influential Twitter users. Last, to demonstrate their applicability, we further employ the behavior models to detect interesting events from sudden changes in aggregated information propagation behavior of Twitter users. The results will show that the proposed behavior models can be effectively applied to detect interesting events in the Twitter stream, compared to the baseline tweet-based approaches.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {13},
numpages = {30},
keywords = {Twitter, promoting behavior, behavior modeling, retweet, event detection, weak retweet, originating behavior, information propagation}
}

@article{10.1145/2361256.2361257,
author = {Chiang, Roger H. L. and Goes, Paulo and Stohr, Edward A.},
title = {Business Intelligence and Analytics Education, and Program Development: A Unique Opportunity for the Information Systems Discipline},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2361256.2361257},
doi = {10.1145/2361256.2361257},
abstract = {“Big Data,” huge volumes of data in both structured and unstructured forms generated by the Internet, social media, and computerized transactions, is straining our technical capacity to manage it. More importantly, the new challenge is to develop the capability to understand and interpret the burgeoning volume of data to take advantage of the opportunities it provides in many human endeavors, ranging from science to business. Data Science, and in business schools, Business Intelligence and Analytics (BI&amp;A) are emerging disciplines that seek to address the demands of this new era. Big Data and BI&amp;A present unique challenges and opportunities not only for the research community, but also for Information Systems (IS) programs at business schools. In this essay, we provide a brief overview of BI&amp;A, speculate on the role of BI&amp;A education in business schools, present the challenges facing IS departments, and discuss the role of IS curricula and program development, in delivering BI&amp;A education. We contend that a new vision for the IS discipline should address these challenges.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {12},
numpages = {13},
keywords = {data mining, data warehousing, Big data, business intelligence &amp; analytics, text analytics}
}

@article{10.1145/2407740.2407744,
author = {Zhang, Zhu and Guo, Chenhui and Goes, Paulo},
title = {Product Comparison Networks for Competitive Analysis of Online Word-of-Mouth},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2407740.2407744},
doi = {10.1145/2407740.2407744},
abstract = {Enabled by Web 2.0 technologies social media provide an unparalleled platform for consumers to share their product experiences and opinions---through word-of-mouth (WOM) or consumer reviews. It has become increasingly important to understand how WOM content and metrics thereof are related to consumer purchases and product sales. By integrating network analysis with text sentiment mining techniques, we propose product comparison networks as a novel construct, computed from consumer product reviews. To test the validity of these product ranking measures, we conduct an empirical study based on a digital camera dataset from Amazon.com. The results demonstrate significant linkage between network-based measures and product sales, which is not fully captured by existing review measures such as numerical ratings. The findings provide important insights into the business impact of social media and user-generated content, an emerging problem in business intelligence research. From a managerial perspective, our results suggest that WOM in social media also constitutes a competitive landscape for firms to understand and manipulate.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {20},
numpages = {22},
keywords = {sentiment analysis, text mining, Word-of-mouth, network analysis, consumer reviews, social media}
}

@article{10.1145/2407740.2407743,
author = {Pervin, Nargis and Fang, Fang and Datta, Anindya and Dutta, Kaushik and Vandermeer, Debra},
title = {Fast, Scalable, and Context-Sensitive Detection of Trending Topics in Microblog Post Streams},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2407740.2407743},
doi = {10.1145/2407740.2407743},
abstract = {Social networks, such as Twitter, can quickly and broadly disseminate news and memes across both real-world events and cultural trends. Such networks are often the best sources of up-to-the-minute information, and are therefore of considerable commercial and consumer interest. The trending topics that appear first on these networks represent an answer to the age-old query “what are people talking about?” Given the incredible volume of posts (on the order of 45,000 or more per minute), and the vast number of stories about which users are posting at any given time, it is a formidable problem to extract trending stories in real time. In this article, we describe a method and implementation for extracting trending topics from a high-velocity real-time stream of microblog posts. We describe our approach and implementation, and a set of experimental results that show that our system can accurately find “hot” stories from high-rate Twitter-scale text streams.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {19},
numpages = {24},
keywords = {microblogs, Trending topics, scalability, Twitter}
}

@article{10.1145/2407740.2407742,
author = {Zhang, Bin and Thomas, Andrew C. and Doreian, Patrick and Krackhardt, David and Krishnan, Ramayya},
title = {Contrasting Multiple Social Network Autocorrelations for Binary Outcomes, With Applications To Technology Adoption},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2407740.2407742},
doi = {10.1145/2407740.2407742},
abstract = {The rise of socially targeted marketing suggests that decisions made by consumers can be predicted not only from their personal tastes and characteristics, but also from the decisions of people who are close to them in their networks. One obstacle to consider is that there may be several different measures for closeness that are appropriate, either through different types of friendships, or different functions of distance on one kind of friendship, where only a subset of these networks may actually be relevant. Another is that these decisions are often binary and more difficult to model with conventional approaches, both conceptually and computationally. To address these issues, we present a hierarchical auto-probit model for individual binary outcomes that uses and extends the machinery of the auto-probit method for binary data. We demonstrate the behavior of the parameters estimated by the multiple network-regime auto-probit model (m-NAP) under various sensitivity conditions, such as the impact of the prior distribution and the nature of the structure of the network. We also demonstrate several examples of correlated binary data outcomes in networks of interest to information systems, including the adoption of caller ring-back tones, whose use is governed by direct connection but explained by additional network topologies.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {18},
numpages = {21},
keywords = {autocorrelation model, Bayesian method, Social network, diffusion}
}

@article{10.1145/2407740.2407741,
author = {Lim, Ee-Peng and Chen, Hsinchun and Chen, Guoqing},
title = {Business Intelligence and Analytics: Research Directions},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2407740.2407741},
doi = {10.1145/2407740.2407741},
abstract = {Business intelligence and analytics (BIA) is about the development of technologies, systems, practices, and applications to analyze critical business data so as to gain new insights about business and markets. The new insights can be used for improving products and services, achieving better operational efficiency, and fostering customer relationships. In this article, we will categorize BIA research activities into three broad research directions: (a) big data analytics, (b) text analytics, and (c) network analytics. The article aims to review the state-of-the-art techniques and models and to summarize their use in BIA applications. For each research direction, we will also determine a few important questions to be addressed in future research.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {17},
numpages = {10},
keywords = {Business intelligence, business analytics}
}

@article{10.1145/2445560.2445564,
author = {Ullah, Azmat and Lai, Richard},
title = {A Systematic Review of Business and Information Technology Alignment},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2445560.2445564},
doi = {10.1145/2445560.2445564},
abstract = {Business organizations have become heavily dependent on information technology (IT) services. The process of alignment is defined as the mutual synchronization of business goals and IT services. However, achieving mature alignment between business and IT is difficult due to the rapid changes in the business and IT environments. This article provides a systematic review of studies on the alignment of business and IT. The research articles reviewed are based on topics of alignment, the definition of alignment, history, alignment challenges, phases of alignment, alignment measurement approaches, the importance of alignment in business industries, how software engineering helps in better alignment, and the role of the business environment in aligning business with IT. It aims to present a thorough understanding of business-IT alignment and to provide a list of future research directions regarding alignment. To perform the systematic review, we used the guidelines developed by Kitchenham for reviewing the available research papers relevant to our topic.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {4},
numpages = {30},
keywords = {business issues, business environment modeling, IT support, alignment measurement, literature, Systematic review, business, IT issues, alignment phases, IT alignment}
}

@article{10.1145/2445560.2445563,
author = {Choi, Jae and Nazareth, Derek L. and Jain, Hemant K.},
title = {The Impact of SOA Implementation on IT-Business Alignment: A System Dynamics Approach},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2445560.2445563},
doi = {10.1145/2445560.2445563},
abstract = {With firms facing intense rivalry, globalization, and time-to-market pressures, the need for organizational agility assumes greater importance. One of the primary vehicles for achieving organizational agility is the use of agile information systems [IS] and the close alignment of information technologies [IT] with business. However, IS is often viewed as an impediment to organization agility. Recently, service-oriented architecture [SOA] has emerged as a prominent IS agility-enhancing technology. The fundamental question of how SOA can enhance organization agility and foster closer alignment between IT and business has not been adequately addressed. The dynamic interaction among external business environmental factors, organizational agility, and IS architecture makes the process of keeping IT and business aligned more complex. This study uses a design science approach to build a system dynamics model to examine the effect of employing alternative SOA implementation strategies in various organizational and external business environments on the IT business alignment and IS cost. The results provide insights into the shaping of IT-business alignment. Additionally, the system dynamics model serves as a tool for supporting managerial decisions related to SOA implementation.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {3},
numpages = {22},
keywords = {IT-business alignment, system dynamics modeling, information systems agility, Service-oriented architecture}
}

@article{10.1145/2445560.2445562,
author = {Valecha, Rohit and Sharman, Raj and Rao, H. Raghav and Upadhyaya, Shambhu},
title = {A Dispatch-Mediated Communication Model for Emergency Response Systems},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2445560.2445562},
doi = {10.1145/2445560.2445562},
abstract = {The current state of emergency communication is dispatch-mediated (the messages from the scene are directed towards the responders and agencies through the dispatch agency). These messages are logged in electronic documents called incident reports, which are useful in monitoring the incident, off-site supervision, resource allocation, and post-incident analysis. However, these messages do not adhere to any particular structure, and there is no set format. The lack of standards creates a problem for sharing information among systems and responders and has a detrimental impact on systems interoperability. In this article, we develop a National Information Exchange Model (NIEM) and Universal Core (UCORE) compliant messaging model, considering message structures and formats, to foster message standardization.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {2},
numpages = {25},
keywords = {message structuring, message queuing, design science, emergency response systems, dispatch-mediated communication, Message classification}
}

@article{10.1145/2445560.2445561,
author = {Yaraghi, Niam and Du, Anna Ye and Sharman, Raj and Gopal, Ram D. and Ramesh, R.},
title = {Network Effects in Health Information Exchange Growth},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2445560.2445561},
doi = {10.1145/2445560.2445561},
abstract = {The importance of the Healthcare Information Exchange (HIE) in increasing healthcare quality and reducing risks and costs has led to greater interest in identifying factors that enhance adoption and meaningful use of HIE by healthcare providers. In this research we study the interlinked network effects between two different groups of physicians -- primary care physicians and specialists -- as significant factors in increasing the growth of each group in an exchange. An analytical model of interlinked and intragroup influences on adoption is developed using the Bass diffusion model as a basis. Adoption data on 1,060 different primary and secondary care physicians over 32 consecutive months was used to test the model. The results indicate not only the presence of interlinked effects, but also that their influence is stronger than that of the intragroup. Further, the influence of primary care physicians on specialists is stronger than that of specialists on primary care physicians. We also provide statistical evidence that the new model performs better than the conventional Bass model, and the assumptions of diffusion symmetry in the market are statistically valid. Together, the findings provide important guidelines on triggers that enhance the overall growth of HIE and potential marketing strategies for HIE services.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {1},
numpages = {26},
keywords = {Health Information Exchange, diffusion of innovation, Bass model}
}

@article{10.1145/2500750,
author = {Wu, Jiming and Holsapple, Clyde W.},
title = {Does Knowledge Management Matter? The Empirical Evidence from Market-Based Valuation},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2500750},
doi = {10.1145/2500750},
abstract = {Information technology is inseparable from contemporary knowledge management (KM). Although anecdotal evidence and individual case studies suggest that effective knowledge management initiatives contribute to superior firm performance, other kinds of empirical investigations are scarce, and more to the point, most of them are based on perceptions of survey participants embedded in the firms being studied. Moreover, studies analyzing the question of whether superior KM performance can predict superior market-based valuation appear to be virtually nonexistent. Findings of such studies would be of value to those who champion and direct a firm’s KM efforts, and to the firm’s strategists, planners, and operational managers. Here, we empirically examine the relationship between KM performance and firm valuation; the former is assessed by international panels of independent KM experts and the latter is evaluated in terms of market-based measures. Based on data spanning eight years, the results show that superior KM performance has a statistically significant positive association with firm valuation in terms of Tobin’s q, price-to-book ratio, and price-to-sales ratio. This study contributes to the management literature by using independent expert judges and archival data to substantiate the notion that KM competencies are an important ingredient in a firm’s performance as indicated by market-based valuation.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {6},
numpages = {23},
keywords = {market performance, resource-based view, market valuation, Tobin’s q, knowledge management performance, knowledge chain theory, organizational learning theory, Knowledge management}
}

@article{10.1145/2499962.2499967,
author = {Derrick, Douglas C. and Meservy, Thomas O. and Jenkins, Jeffrey L. and Burgoon, Judee K. and Nunamaker, Jay F.},
title = {Detecting Deceptive Chat-Based Communication Using Typing Behavior and Message Cues},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2499962.2499967},
doi = {10.1145/2499962.2499967},
abstract = {Computer-mediated deception is prevalent and may have serious consequences for individuals, organizations, and society. This article investigates several metrics as predictors of deception in synchronous chat-based environments, where participants must often spontaneously formulate deceptive responses. Based on cognitive load theory, we hypothesize that deception influences response time, word count, lexical diversity, and the number of times a chat message is edited. Using a custom chatbot to conduct interviews in an experiment, we collected 1,572 deceitful and 1,590 truthful chat-based responses. The results of the experiment confirm that deception is positively correlated with response time and the number of edits and negatively correlated to word count. Contrary to our prediction, we found that deception is not significantly correlated with lexical diversity. Furthermore, the age of the participant moderates the influence of deception on response time. Our results have implications for understanding deceit in chat-based communication and building deception-detection decision aids in chat-based systems.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {9},
numpages = {21},
keywords = {Decision support system, typing bahavior, deception detection, chat}
}

@article{10.1145/2499962.2499963,
author = {Gill, T. Grandon and Hevner, Alan R.},
title = {A Fitness-Utility Model for Design Science Research},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2499962.2499963},
doi = {10.1145/2499962.2499963},
abstract = {Current thinking in design science research (DSR) defines the usefulness of the design artifact in a relevant problem environment as the primary research goal. Here we propose a complementary evaluation model for DSR. Drawing from evolutionary economics, we define a fitness-utility model that better captures the evolutionary nature of design improvements and the essential DSR nature of searching for a satisfactory design across a fitness landscape. Our goal is to move DSR to more meaningful evaluations of design artifacts for sustainable impacts. A key premise of this new thinking is that the evolutionary fitness of a design artifact is more valuable than its immediate usefulness. We conclude with a discussion of the strengths and challenges of the fitness-utility model for the performance of rigorous and relevant DSR.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {5},
numpages = {24},
keywords = {sustainability, evolutionary economics, research paradigms, utility, Design science research, fitness}
}

@article{10.1145/2490860,
author = {Zhang, Zhu and Zeng, Daniel D. and Abbasi, Ahmed and Peng, Jing and Zheng, Xiaolong},
title = {A Random Walk Model for Item Recommendation in Social Tagging Systems},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2490860},
doi = {10.1145/2490860},
abstract = {Social tagging, as a novel approach to information organization and discovery, has been widely adopted in many Web 2.0 applications. Tags contributed by users to annotate a variety of Web resources or items provide a new type of information that can be exploited by recommender systems. Nevertheless, the sparsity of the ternary interaction data among users, items, and tags limits the performance of tag-based recommendation algorithms. In this article, we propose to deal with the sparsity problem in social tagging by applying random walks on ternary interaction graphs to explore transitive associations between users and items. The transitive associations in this article refer to the path of the link between any two nodes whose length is greater than one. Taking advantage of these transitive associations can allow more accurate measurement of the relevance between two entities (e.g., user-item, user-user, and item-item). A PageRank-like algorithm has been developed to explore these transitive associations by spreading users’ preferences on an item similarity graph and spreading items’ influences on a user similarity graph. Empirical evaluation on three real-world datasets demonstrates that our approach can effectively alleviate the sparsity problem and improve the quality of item recommendation.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {8},
numpages = {24},
keywords = {social tagging, sparsity, Recommender systems, random walk}
}

@article{10.1145/2489790,
author = {Orman, Levent V.},
title = {Bayesian Inference in Trust Networks},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2489790},
doi = {10.1145/2489790},
abstract = {Trust has emerged as a major impediment to the success of electronic markets and communities where interaction with the strangers is the norm. Social Networks and Online Communities enable interaction with complete strangers, and open up new commercial, political, and social possibilities. But those promises are rarely achieved because it is difficult to trust the online contacts. A common approach to remedy this problem is to compute trust values for the new contacts from the existing trust values in the network. There are two main methods: aggregation and transitivity. Yet, neither method provides satisfactory results because trust networks are sparse and transitivity may not hold. This article develops a Bayesian formulation of the problem, where trust is defined as a conditional probability, and a Bayesian Network analysis is employed to compute the unknown trust values in terms of the known trust values. The algorithms used to propagate conditional probabilities through the network are theoretically sound and based on a long-standing literature on probability propagation in Bayesian networks. Moreover, the context information that is typically ignored in trust literature is included here as a major factor in computing new trust values. These changes have led to significant improvements over existing approaches in the accuracy of computed trust, and with some modifications to the algorithm, in its reach. Real data acquired from Advogato network is used to do extensive testing, and the results confirm the practical value of a theoretically sound Bayesian approach.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {7},
numpages = {21},
keywords = {trust inference, trust networks, Social networks, trust propagation, Bayesian networks}
}

@article{10.1145/2524263,
author = {Sarker, Suprateek and Chakraborty, Suranjan and Tansuhaj, Patriya Silpakit and Mulder, Mark and Dogerlioglu-Demir, Kivilcim},
title = {The “Mail-Order-Bride” (MOB) Phenomenon in the Cyberworld: An Interpretive Investigation},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2524263},
doi = {10.1145/2524263},
abstract = {Information technology (IT) is often an enabler in bringing people together. In the context of this study, IT helps connect matchmaking service providers with those looking for love, particularly when a male seeks to meet and possibly marry a female from another country: a process which results in over 16,500 such ‘mail-order-bride’ (MOB) marriages a year in the United States alone. Past research in business disciplines has been largely silent about the way in which this process unfolds, the perspectives of the participants at different points of time, and the role of IT underlying the MOB matchmaking service. Adopting an interpretivist stance, and utilizing some of the methodological guidelines associated with the Grounded Theory Methodology (GTM), we develop a process model which highlights: a) the key states of the process through which the relationship between the MOB seeker (the man) and the MOB (the woman) unfolds, b) the transitions between states, and c) the triggering conditions for the transitions from one state to another. This study also highlights key motivations of the individuals participating in the MOB process, the effect of power and the role it plays in the dynamics of the relationships, the status of women and how their status evolves during the MOB process, and the unique affordance provided by IT as the relationships evolve.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {10},
numpages = {36},
keywords = {information technology, e-commerce, process model, interpretive research, grounded theory, On-line relationships, mail order brides (“MOB”)}
}

@article{10.1145/2523025.2523026,
author = {Edgcomb, Alex and Vahid, Frank},
title = {Accurate and Efficient Algorithms That Adapt to Privacy-Enhanced Video for Improved Assistive Monitoring},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2523025.2523026},
doi = {10.1145/2523025.2523026},
abstract = {Automated monitoring algorithms operating on live video streamed from a home can effectively aid in several assistive monitoring goals, such as detecting falls or estimating daily energy expenditure. Use of video raises obvious privacy concerns. Several privacy enhancements have been proposed such as modifying a person in video by introducing blur, silhouette, or bounding-box. Person extraction is fundamental in video-based assistive monitoring and degraded in the presence of privacy enhancements; however, privacy enhancements have characteristics that can opportunistically be adapted to. We propose two adaptive algorithms for improving assistive monitoring goal performance with privacy-enhanced video: specific-color hunter and edge-void filler. A nonadaptive algorithm, foregrounding, is used as the default algorithm for the adaptive algorithms. We compare nonadaptive and adaptive algorithms with 5 common privacy enhancements on the effectiveness of 8 automated monitoring goals. The nonadaptive algorithm performance on privacy-enhanced video is degraded from raw video. However, adaptive algorithms can compensate for the degradation. Energy estimation accuracy in our tests degraded from 90.9% to 83.9%, but the adaptive algorithms significantly compensated by bringing the accuracy up to 87.1%. Similarly, fall detection accuracy degraded from 1.0 sensitivity to 0.86 and from 1.0 specificity to 0.79, but the adaptive algorithms compensated accuracy back to 0.92 sensitivity and 0.90 specificity. Additionally, the adaptive algorithms were computationally more efficient than the nonadaptive algorithm, averaging 1.7% more frames processed per second.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {14},
numpages = {16},
keywords = {smart homes, Assistive monitoring, embedded systems, privacy-enhanced video, telehealth, ubiquitous systems}
}

@article{10.1145/2517310,
author = {Mathew, George and Obradovic, Zoran},
title = {Distributed Privacy-Preserving Decision Support System for Highly Imbalanced Clinical Data},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2517310},
doi = {10.1145/2517310},
abstract = {When a medical practitioner encounters a patient with rare symptoms that translates to rare occurrences in the local database, it is quite valuable to draw conclusions collectively from such occurrences in other hospitals. However, for such rare conditions, there will be a huge imbalance in classes among the relevant base population. Due to regulations and privacy concerns, collecting data from other hospitals will be problematic. Consequently, distributed decision support systems that can use just the statistics of data from multiple hospitals are valuable. We present a system that can collectively build a distributed classification model dynamically without the need of patient data from each site in the case of imbalanced data. The system uses a voting ensemble of experts for the decision model. The imbalance condition and number of experts can be determined by the system. Since only statistics of the data and no raw data are required by the system, patient privacy issues are addressed. We demonstrate the outlined principles using the Nationwide Inpatient Sample (NIS) database. Results of experiments conducted on 7,810,762 patients from 1050 hospitals show improvement of 13.68% to 24.46% in balanced prediction accuracy using our model over the baseline model, illustrating the effectiveness of the proposed methodology.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {12},
numpages = {15},
keywords = {Distributed decision support, privacy, clinical decision support system, privacy preserving frameworks}
}

@article{10.1145/2517309,
author = {Kasiri, Narges and Sharda, Ramesh},
title = {Real Options and System Dynamics for Information Technology Investment Decisions: Application to RFID Adoption in Retail},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2517309},
doi = {10.1145/2517309},
abstract = {We propose a unique combination of system dynamics and real options into a robust and innovative model for analyzing return on investments in IT. Real options modeling allows a cost benefit analysis to take into account managerial flexibilities when there is uncertainty in the investment, while system dynamics can build a predictive model, in which one can simulate different real-life and hypothetical scenarios in order to provide measurements that can be used in the real options model. Our return on the investment model combines these long-established quantitative techniques in a novel manner. This study applies this robust hybrid model to a challenging IT investment problem: adoption of RFID in retail. Item-level RFID is the next generation of identification technology in the retail sector. Our method can help managers to overcome the complexity and uncertainties in the investment timing of this technology. We analyze the RFID considerations in retail decision-making using real data compiled from a Delphi study. Our model demonstrates how the cost and benefits of such an investment change over time. The results highlight the variable cost of RFID tags as the key factor in the decision process concerning whether to immediately adopt or postpone the use of RFID in retail. Our exploratory work suggests that it is possible to combine merchandising and pricing issues in addition to the traditional supply chain management issues in studying any multifaceted problem in retail.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {11},
numpages = {25},
keywords = {retail industry, RFID investment, real options, system dynamics, simulation}
}

@article{10.1145/2517084,
author = {Sakata, Masato and Y\"{u}cel, Zeynep and Shinozawa, Kazuhiko and Hagita, Norihiro and Imai, Michita and Furutani, Michiko and Matsuoka, Rumiko},
title = {An Inference Engine for Estimating Outside States of Clinical Test Items},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2517084},
doi = {10.1145/2517084},
abstract = {Common periodical health check-ups include several clinical test items with affordable cost. However, these standard tests do not directly indicate signs of most lifestyle diseases. In order to detect such diseases, a number of additional specific clinical tests are required, which increase the cost of the health check-up. This study aims to enrich our understanding of the common health check-ups and proposes a way to estimate the signs of several lifestyle diseases based on the standard tests in common examinations without performing any additional specific tests. In this manner, we enable a diagnostic process, where the physician may prefer to perform or avoid a costly test according to the estimation carried out through a set of common affordable tests. To that end, the relation between standard and specific test results is modeled with a multivariate kernel density estimate. The condition of the patient regarding a specific test is assessed following a Bayesian framework. Our results indicate that the proposed method achieves an overall estimation accuracy of 84%. In addition, an outstanding estimation accuracy is achieved for a subset of high-cost tests. Moreover, comparison with standard artificial intelligence methods suggests that our algorithm outperforms the conventional methods.Our contributions are as follows: (i) promotion of affordable health check-ups, (ii) high estimation accuracy in certain tests, (iii) generalization capability due to ease of implementation on different platforms and institutions, (iv) flexibility to apply to various tests and potential to improve early detection rates.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {13},
numpages = {21},
keywords = {Bayesian inference, Data mining, health care, computational intelligence methodologies for health care, medical informatics, data analytics for clinical care, clinical decision support, database}
}

@article{10.1145/2555810.2555811,
author = {Yang, Christopher C. and Leroy, Gondy and Ananiadou, Sophia},
title = {Smart Health and Wellbeing},
year = {2013},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2555810.2555811},
doi = {10.1145/2555810.2555811},
abstract = {Healthcare informatics has drawn substantial attention in recent years. Current work on healthcare informatics is highly interdisciplinary involving methodologies from computing, engineering, information science, behavior science, management science, social science, as well as many different areas in medicine and public health. Three major tracks, (i) systems, (ii) analytics, and (iii) human factors, can be identified. The systems track focuses on healthcare system architecture, framework, design, engineering, and application; the analytics track emphasizes data/information processing, retrieval, mining, analytics, as well as knowledge discovery; the human factors track targets the understanding of users or context, interface design, and user studies of healthcare applications. In this article, we discuss some of the latest development and introduce several articles selected for this special issue. We envision that the development of computing-oriented healthcare informatics research will continue to grow rapidly. The integration of different disciplines to advance the healthcare and wellbeing of our society will also be accelerated.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {15},
numpages = {8}
}

@article{10.1145/2544105,
author = {Wang, Zidong and Eatock, Julie and McClean, Sally and Liu, Dongmei and Liu, Xiaohui and Young, Terry},
title = {Modeling Throughput of Emergency Departments via Time Series: An Expectation Maximization Algorithm},
year = {2013},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2544105},
doi = {10.1145/2544105},
abstract = {In this article, the expectation maximization (EM) algorithm is applied for modeling the throughput of emergency departments via available time-series data. The dynamics of emergency department throughput is developed and evaluated, for the first time, as a stochastic dynamic model that consists of the noisy measurement and first-order autoregressive (AR) stochastic dynamic process. By using the EM algorithm, the model parameters, the actual throughput, as well as the noise intensity, can be identified simultaneously. Four real-world time series collected from an emergency department in West London are employed to demonstrate the effectiveness of the introduced algorithm. Several quantitative indices are proposed to evaluate the inferred models. The simulation shows that the identified model fits the data very well.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {16},
keywords = {modeling, Healthcare, emergency department, EM algorithm, time series data}
}

@article{10.1145/2544104,
author = {Khosla, Rajiv and Chu, Mei-Tai},
title = {Embodying Care in Matilda: An Affective Communication Robot for Emotional Wellbeing of Older People in Australian Residential Care Facilities},
year = {2013},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2544104},
doi = {10.1145/2544104},
abstract = {Ageing population is at the center of the looming healthcare crisis in most parts of the developed and developing world. Australia, like most of the western world, is bracing up for the looming ageing population crisis, spiraling healthcare costs, and expected serious shortage of healthcare workers. Assistive service and companion (social) robots are being seen as one of the ways for supporting aged care facilities to meet this challenge and improve the quality of care of older people including mental and physical health outcomes, as well as to support healthcare workers in personalizing care. In this article, the authors report on the design and implementation of first-ever field trials of Matilda, a human-like assistive communication (service and companion) robot for improving the emotional well-being of older people in three residential care facilities in Australia involving 70 participants. The research makes several unique contributions including Matilda’s ability to break technology barriers, positively engage older people in group and one-to-one activities, making these older people productive and useful, helping them become resilient and cope better through personalization of care, and finally providing them sensory enrichment through Matilda’s multimodal communication capabilities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {18},
numpages = {33},
keywords = {Affective communication robot, nursing home personhood, aged care, well-being, personalization of care}
}

@article{10.1145/2544103,
author = {Lisetti, Christine and Amini, Reza and Yasavur, Ugan and Rishe, Naphtali},
title = {I Can Help You Change! An Empathic Virtual Agent Delivers Behavior Change Health Interventions},
year = {2013},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2544103},
doi = {10.1145/2544103},
abstract = {We discuss our approach to developing a novel modality for the computer-delivery of Brief Motivational Interventions (BMIs) for behavior change in the form of a personalized On-Demand VIrtual Counselor (ODVIC), accessed over the internet. ODVIC is a multimodal Embodied Conversational Agent (ECA) that empathically delivers an evidence-based behavior change intervention by adapting, in real-time, its verbal and nonverbal communication messages to those of the user’s during their interaction. We currently focus our work on excessive alcohol consumption as a target behavior, and our approach is adaptable to other target behaviors (e.g., overeating, lack of exercise, narcotic drug use, non-adherence to treatment). We based our current approach on a successful existing patient-centered brief motivational intervention for behavior change---the Drinker’s Check-Up (DCU)---whose computer-delivery with a text-only interface has been found effective in reducing alcohol consumption in problem drinkers. We discuss the results of users’ evaluation of the computer-based DCU intervention delivered with a text-only interface compared to the same intervention delivered with two different ECAs (a neutral one and one with some empathic abilities). Users rate the three systems in terms of acceptance, perceived enjoyment, and intention to use the system, among other dimensions. We conclude with a discussion of how our positive results encourage our long-term goals of on-demand conversations, anytime, anywhere, with virtual agents as personal health and well-being helpers.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {19},
numpages = {28},
keywords = {information systems, empathy modeling, affective computing, embodied conversational agent, alcohol interventions, brief motivational interviewing intervention, Intelligent virtual agent, behavior change, multimodal communication, computer-based interventions, health informatics, healthy lifestyles}
}

@article{10.1145/2544102,
author = {Zhang, He and Mehotra, Sanjay and Liebovitz, David and Gunter, Carl A. and Malin, Bradley},
title = {Mining Deviations from Patient Care Pathways via Electronic Medical Record System Audits},
year = {2013},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2544102},
doi = {10.1145/2544102},
abstract = {In electronic medical record (EMR) systems, administrators often provide EMR users with broad access privileges, which may leave the system vulnerable to misuse and abuse. Given that patient care is based on a coordinated workflow, we hypothesize that care pathways can be represented as the progression of a patient through a system and introduce a strategy to model the patient’s flow as a sequence of accesses defined over a graph. Elements in the sequence correspond to features associated with the access transaction (e.g., reason for access). Based on this motivation, we model patterns of patient record usage, which may indicate deviations from care workflows. We evaluate our approach using several months of data from a large academic medical center. Empirical results show that this framework finds a small portion of accesses constitute outliers from such flows. We also observe that the violation patterns deviate for different types of medical services. Analysis of our results suggests greater deviation from normal access patterns by nonclinical users. We simulate anomalies in the context of real accesses to illustrate the efficiency of the proposed method for different medical services. As an illustration of the capabilities of our method, it was observed that the area under the receiver operating characteristic (ROC) curve for the Pediatrics service was found to be 0.9166. The results suggest that our approach is competitive with, and often better than, the existing state-of-the-art in its outlier detection performance. At the same time, our method is more efficient, by orders of magnitude, than previous approaches, allowing for detection of thousands of accesses in seconds.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {17},
numpages = {20},
keywords = {Anomaly detection, temporal systems, graph-based analysis, data mining, electronic medical record systems, audit}
}

@article{10.1145/2543900,
author = {Mirani, Rajesh and Harpalani, Anju},
title = {Business Benefits or Incentive Maximization? Impacts of the Medicare EHR Incentive Program at Acute Care Hospitals},
year = {2013},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2543900},
doi = {10.1145/2543900},
abstract = {This study investigates the influence of the Medicare EHR Incentive Program on EHR adoption at acute care hospitals and the impact of EHR adoption on operational and financial efficiency/effectiveness. It finds that even before joining the incentive program, adopter hospitals had more efficient and effective Medicare operations than those of non-adopters. Adopters were also financially more efficient. After joining the program, adopter hospitals treated significantly more Medicare patients by shortening their stay durations, relative to their own non-Medicare patients and also to patients at non-adopter hospitals, even as their overall capacity utilization remained relatively unchanged. The study concludes that many of these hospitals had implemented EHR even before the initiation of the incentive program. It further infers that they joined this program with opportunistic intentions of tapping into incentive payouts which they maximized by taking on more Medicare patients. These findings give credence to critics of the program who have questioned its utility and alleged that it serves only to reward existing users of EHR technologies.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {20},
numpages = {19},
keywords = {Medicare EHR Incentive Program, perverse incentives, information technology adoption, Electronic health records, organizational change, electronic medical records, opportunism, information technology implementation}
}

@article{10.1145/2591676,
author = {Ho, Joyce C. and Lee, Cheng H. and Ghosh, Joydeep},
title = {Septic Shock Prediction for Patients with Missing Data},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2591676},
doi = {10.1145/2591676},
abstract = {Sepsis and septic shock are common and potentially fatal conditions that often occur in intensive care unit (ICU) patients. Early prediction of patients at risk for septic shock is therefore crucial to minimizing the effects of these complications. Potential indications for septic shock risk span a wide range of measurements, including physiological data gathered at different temporal resolutions and gene expression levels, leading to a nontrivial prediction problem. Previous works on septic shock prediction have used small, carefully curated datasets or clinical measurements that may not be available for many ICU patients. The recent availability of a large, rich ICU dataset called MIMIC-II has provided the opportunity for more extensive modeling of this problem. However, such a large clinical dataset inevitably contains a substantial amount of missing data. We investigate how different imputation selection criteria and methods can overcome the missing data problem. Our results show that imputation methods in conjunction with predictive modeling can lead to accurate septic shock prediction, even if the features are restricted primarily to noninvasive measurements. Our models provide a generalized approach for predicting septic shock in any ICU patient.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {1},
numpages = {15},
keywords = {imputation methods, data mining, Missing data, sepsis}
}

@article{10.1145/2576757,
author = {Yeo, M. Lisa and Rolland, Erik and Ulmer, Jackie Rees and Patterson, Raymond A.},
title = {Risk Mitigation Decisions for IT Security},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2576757},
doi = {10.1145/2576757},
abstract = {Enterprises must manage their information risk as part of their larger operational risk management program. Managers must choose how to control for such information risk. This article defines the flow risk reduction problem and presents a formal model using a workflow framework. Three different control placement methods are introduced to solve the problem, and a comparative analysis is presented using a robust test set of 162 simulations. One year of simulated attacks is used to validate the quality of the solutions. We find that the math programming control placement method yields substantial improvements in terms of risk reduction and risk reduction on investment when compared to heuristics that would typically be used by managers to solve the problem. The contribution of this research is to provide managers with methods to substantially reduce information and security risks, while obtaining significantly better returns on their security investments. By using a workflow approach to control placement, which guides the manager to examine the entire infrastructure in a holistic manner, this research is unique in that it enables information risk to be examined strategically.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {5},
numpages = {21},
keywords = {Controls, information risk management, workflows}
}

@article{10.1145/2576756,
author = {Huang, Lihua and Ba, Sulin and Lu, Xianghua},
title = {Building Online Trust in a Culture of Confucianism: The Impact of Process Flexibility and Perceived Control},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2576756},
doi = {10.1145/2576756},
abstract = {The success of e-commerce companies in a Confucian cultural context takes more than advanced IT and process design that have proven successful in Western countries. The example of eBay’s failure in China indicates that earning the trust of Chinese consumers is essential to success, yet the process of building that trust requires something different from that in the Western culture. This article attempts to build a theoretical model to explore the relationship between the Confucian culture and online trust. We introduce two new constructs, namely process flexibility and perceived control, as particularly important factors in online trust formation in the Chinese cultural context. A survey was conducted to test the proposed theoretical model. This study offers a new explanation for online trust formation in the Confucian context. The findings of this article can provide guidance for companies hoping to successfully navigate the Chinese online market in the future.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {4},
numpages = {23},
keywords = {Confucianism, e-commerce, culture, process flexibility, trust, online market, perceived control}
}

@article{10.1145/2576233,
author = {Yang, Christopher C. and Yang, Haodong and Jiang, Ling},
title = {Postmarketing Drug Safety Surveillance Using Publicly Available Health-Consumer-Contributed Content in Social Media},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2576233},
doi = {10.1145/2576233},
abstract = {Postmarketing drug safety surveillance is important because many potential adverse drug reactions cannot be identified in the premarketing review process. It is reported that about 5% of hospital admissions are attributed to adverse drug reactions and many deaths are eventually caused, which is a serious concern in public health. Currently, drug safety detection relies heavily on voluntarily reporting system, electronic health records, or relevant databases. There is often a time delay before the reports are filed and only a small portion of adverse drug reactions experienced by health consumers are reported. Given the popularity of social media, many health social media sites are now available for health consumers to discuss any health-related issues, including adverse drug reactions they encounter. There is a large volume of health-consumer-contributed content available, but little effort has been made to harness this information for postmarketing drug safety surveillance to supplement the traditional approach. In this work, we propose the association rule mining approach to identify the association between a drug and an adverse drug reaction. We use the alerts posted by Food and Drug Administration as the gold standard to evaluate the effectiveness of our approach. The result shows that the performance of harnessing health-related social media content to detect adverse drug reaction is good and promising.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {2},
numpages = {21},
keywords = {postmarketing surveillance, adverse drug reactions, social media, health-consumer-contributed content, Drug safety signal detection}
}

@article{10.1145/2555596,
author = {Bouktif, Salah and Sahraoui, Houari and Ahmed, Faheem},
title = {Predicting Stability of Open-Source Software Systems Using Combination of Bayesian Classifiers},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2555596},
doi = {10.1145/2555596},
abstract = {The use of free and Open-Source Software (OSS) systems is gaining momentum. Organizations are also now adopting OSS, despite some reservations, particularly about the quality issues. Stability of software is one of the main features in software quality management that needs to be understood and accurately predicted. It deals with the impact resulting from software changes and argues that stable components lead to a cost-effective software evolution. Changes are most common phenomena present in OSS in comparison to proprietary software. This makes OSS system evolution a rich context to study and predict stability. Our objective in this work is to build stability prediction models that are not only accurate but also interpretable, that is, able to explain the link between the architectural aspects of a software component and its stability behavior in the context of OSS. Therefore, we propose a new approach based on classifiers combination capable of preserving prediction interpretability. Our approach is classifier-structure dependent. Therefore, we propose a particular solution for combining Bayesian classifiers in order to derive a more accurate composite classifier that preserves interpretability. This solution is implemented using a genetic algorithm and applied in the context of an OSS large-scale system, namely the standard Java API. The empirical results show that our approach outperforms state-of-the-art approaches from both machine learning and software engineering.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {3},
numpages = {26},
keywords = {Bayesian classifiers, Software stability prediction, genetic algorithm}
}

@article{10.1145/2629692,
author = {Deodhar, Suruchi and Bisset, Keith R. and Chen, Jiangzhuo and Ma, Yifei and Marathe, Madhav V.},
title = {An Interactive, Web-Based High Performance Modeling Environment for Computational Epidemiology},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629692},
doi = {10.1145/2629692},
abstract = {We present an integrated interactive modeling environment to support public health epidemiology. The environment combines a high resolution individual-based model with a user-friendly Web-based interface that allows analysts to access the models and the analytics backend remotely from a desktop or a mobile device. The environment is based on a loosely coupled service-oriented-architecture that allows analysts to explore various counterfactual scenarios. As the modeling tools for public health epidemiology are getting more sophisticated, it is becoming increasingly difficult for noncomputational scientists to effectively use the systems that incorporate such models. Thus an important design consideration for an integrated modeling environment is to improve ease of use such that experimental simulations can be driven by the users. This is achieved by designing intuitive and user-friendly interfaces that allow users to design and analyze a computational experiment and steer the experiment based on the state of the system.A key feature of a system that supports this design goal is the ability to start, stop, pause, and roll back the disease propagation and intervention application process interactively. An analyst can access the state of the system at any point in time and formulate dynamic interventions based on additional information obtained through state assessment. In addition, the environment provides automated services for experiment set-up and management, thus reducing the overall time for conducting end-to-end experimental studies.We illustrate the applicability of the system by describing computational experiments based on realistic pandemic planning scenarios. The experiments are designed to demonstrate the system’s capability and enhanced user productivity.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {7},
numpages = {27},
keywords = {user productivity, Computational epidemiology, network-based epidemiological modeling, usability, interactive computations, service oriented architectures, computational steering}
}

@article{10.1145/2629636,
author = {Goodman, S. E.},
title = {Building the Nation’s Cyber Security Workforce: Contributions from the CAE Colleges and Universities},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629636},
doi = {10.1145/2629636},
abstract = {This article presents a view of the necessary size and composition of the US national cyber security workforce, and considers some of the contributions that the government-designated Centers of Academic Excellence (CAE) might make to it. Over the last dozen years about 200 million taxpayer dollars have gone into funding many of these CAEs, with millions explicitly targeted to help them build capacity. The most visible intended output has been in the form of around 125 Scholarship for Service (SFS) students per year going mostly into the workforce of the federal government. Surely the output capacity of these 181 colleges and universities is greater than that, and should be helping to protect the rest of US citizens and taxpayers. We take a need-based look at what the nation’s workforce should look like, and then consider some possibilities of what the CAE schools could be doing to help to close the gaps between that perceived need and the supply and demand.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {6},
numpages = {9}
}

@article{10.1145/2597892,
author = {Kostkova, Patty and Szomszor, Martin and St. Louis, Connie},
title = {#swineflu: The Use of Twitter as an Early Warning and Risk Communication Tool in the 2009 Swine Flu Pandemic},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2597892},
doi = {10.1145/2597892},
abstract = {The need to improve population monitoring and enhance surveillance of infectious diseases has never been more pressing. Factors such as air travel act as a catalyst in the spread of new and existing viruses. The unprecedented user-generated activity on social networks over the last few years has created real-time streams of personal data that provide an invaluable tool for monitoring and sampling large populations. Epidemic intelligence relies on constant monitoring of online media sources for early warning, detection, and rapid response; however, the real-time information available in social networks provides a new paradigm for the early warning function.The communication of risk in any public health emergency is a complex task for governments and healthcare agencies. This task is made more challenging in the current situation when the public has access to a wide range of online resources, ranging from traditional news channels to information posted on blogs and social networks. Twitter’s strength is its two-way communication nature --- both as an information source but also as a central hub for publishing, disseminating and discovering online media.This study addresses these two challenges by investigating the role of Twitter during the 2009 swine flu pandemic by analysing data collected from the SN, and by Twitter using the opposite way for dissemination information through the network. First, we demonstrate the role of the social network for early warning by detecting an upcoming spike in an epidemic before the official surveillance systems by up to two weeks in the U.K. and up to two to three weeks in the U.S. Second, we illustrate how online resources are propagated through Twitter at the time of the WHO’s declaration of the swine flu “pandemic”. Our findings indicate that Twitter does favour reputable t bogus information can still leak into the network.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {8},
numpages = {25},
keywords = {social media, monitoring spread of disease, Epidemic intelligence, real-time data management and public health response, Twitter, swine flu 2009 analysis, data mining, global health and well-being}
}

@article{10.1145/2591672,
author = {Tsai, Chih-Fong and Quan, Zen-Yu},
title = {Stock Prediction by Searching for Similarities in Candlestick Charts},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2591672},
doi = {10.1145/2591672},
abstract = {The aim of stock prediction is to effectively predict future stock market trends (or stock prices), which can lead to increased profit. One major stock analysis method is the use of candlestick charts. However, candlestick chart analysis has usually been based on the utilization of numerical formulas. There has been no work taking advantage of an image processing technique to directly analyze the visual content of the candlestick charts for stock prediction. Therefore, in this study we apply the concept of image retrieval to extract seven different wavelet-based texture features from candlestick charts. Then, similar historical candlestick charts are retrieved based on different texture features related to the query chart, and the “future” stock movements of the retrieved charts are used for stock prediction. To assess the applicability of this approach to stock prediction, two datasets are used, containing 5-year and 10-year training and testing sets, collected from the Dow Jones Industrial Average Index (INDU) for the period between 1990 and 2009. Moreover, two datasets (2010 and 2011) are used to further validate the proposed approach. The experimental results show that visual content extraction and similarity matching of candlestick charts is a new and useful analytical method for stock prediction. More specifically, we found that the extracted feature vectors of 30, 90, and 120, the number of textual features extracted from the candlestick charts in the BMP format, are more suitable for predicting stock movements, while the 90 feature vector offers the best performance for predicting short- and medium-term stock movements. That is, using the 90 feature vector provides the lowest MAPE (3.031%) and Theil’s U (1.988%) rates in the twenty-year dataset, and the best MAPE (2.625%, 2.945%) and Theil’s U (1.622%, 1.972%) rates in the two validation datasets (2010 and 2011).},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {9},
numpages = {21},
keywords = {candlestick chart, Stock prediction, data mining}
}

@article{10.1145/2638544,
author = {Fridgen, Gilbert and Klier, Julia and Beer, Martina and Wolf, Thomas},
title = {Improving Business Value Assurance in Large-Scale IT Projects—A Quantitative Method Based on Founded Requirements Assessment},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2638544},
doi = {10.1145/2638544},
abstract = {The probability of IT project failures can be mitigated more successfully when discovered early. To support a more insightful management of IT projects, which may also facilitate an early detection of IT project failures, transparency regarding a project's cash flows shall be increased. Therefore, an appropriate analysis of a project's benefits, costs, requirements, their respective risks and interdependencies is inevitable. However, to date, in requirements engineering only few methods exist that appropriately consider these factors when estimating the ex ante project business case. Furthermore, empirical studies reveal that a lot of risk factors emerge during the runtime of projects why the ex ante valuation of IT projects even with respect to requirements seems insufficient. Therefore, using the Action Design Research approach, we design, apply, and evaluate a practicable method for value-based continuous IT project steering especially for large-scale IT projects.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {12},
numpages = {17},
keywords = {business value of IT, value assurance, IT project controlling, Requirements engineering, Action Design Research}
}

@article{10.1145/2629376,
author = {Jureta, Ivan J. and Borgida, Alexander and Ernst, Neil A. and Mylopoulos, John},
title = {The Requirements Problem for Adaptive Systems},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629376},
doi = {10.1145/2629376},
abstract = {Requirements Engineering (RE) focuses on eliciting, modeling, and analyzing the requirements and environment of a system-to-be in order to design its specification. The design of the specification, known as the Requirements Problem (RP), is a complex problem-solving task because it involves, for each new system, the discovery and exploration of, and decision making in a new problem space. A system is adaptive if it can detect deviations between its runtime behavior and its requirements, specifically situations where its behavior violates one or more of its requirements. Given such a deviation, an Adaptive System uses feedback mechanisms to analyze these changes and decide, with or without human intervention, how to adjust its behavior as a result. We are interested in defining the Requirements Problem for Adaptive Systems (RPAS). In our case, we are looking for a configurable specification such that whenever requirements fail to be fulfilled, the system can go through a series of adaptations that change its configuration and eventually restore fulfilment of the requirements. From a theoretical perspective, this article formally shows the fundamental differences between standard RE (notably Zave and Jackson [1997]) and RE for Adaptive Systems (see the seminal work by Fickas and Feather [1995], to Letier and van Lamsweerde [2004], and up to Whittle et al. [2010]). The main contribution of this article is to introduce the RPAS as a new RP class that is specific to Adaptive Systems. We relate the RPAS to RE research on the relaxation of requirements, the evaluation of their partial satisfaction, and the monitoring and control of requirements, all topics of particular interest in research on adaptive systems [de Lemos et al. 2013]. From an engineering perspective, we define a proto-framework for solving RPAS, which illustrates features needed in future frameworks for adaptive software systems.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {17},
numpages = {33},
keywords = {requirements problem, requirements modelling language, requirements problem for adaptive systems, requirements engineering, Adaptive systems}
}

@article{10.1145/2629450,
author = {Guo, Xitong and Sun, Sherry X. and Vogel, Doug},
title = {A Dataflow Perspective for Business Process Integration},
year = {2014},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629450},
doi = {10.1145/2629450},
abstract = {Business process integration has become prevalent as business increasingly crosses organizational boundaries. To address the issue of protecting organizations’ competitive knowledge and private information while also enabling business-to-business (B2B) collaboration, past research has focused mainly on customized public and private process design, as well as structural correctness of the integrated workflow. However, a dataflow perspective is important for business process integration. This article presents a data-flow perspective using workflow management and mathematical techniques to address data exchange problems in independent multistakeholder business process integration in dynamic circumstances. The research is conducted following a design science paradigm. We build artifacts that include interorganizational workflow concepts, a workflow model, and a public dataset calculation method. The use of the proposed artifacts is illustrated by applying them to a real-world case in the Shenzhen (Chaiwan) port. The utility of the artifacts is evaluated through interviews with practitioners in industry. We conclude that this research complements the control-flow perspective in the interorganizational workflow management area and also contributes to B2B information-sharing literature; further, the dataflow formalism can help practitioners to formally provide the right data at the right time in dynamic circumstances.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {22},
numpages = {33},
keywords = {dataflow, Process integration}
}

@article{10.1145/2629449,
author = {Keller, Thorben and Thiesse, Fr\'{e}d\'{e}ric and Fleisch, Elgar},
title = {Classification Models for RFID-Based Real-Time Detection of Process Events in the Supply Chain: An Empirical Study},
year = {2014},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629449},
doi = {10.1145/2629449},
abstract = {RFID technology allows the collecting of fine-grained real-time information on physical processes in the supply chain that often cannot be monitored using conventional approaches. However, because of the phenomenon of false-positive reads, RFID data streams resemble noisy analog measurements rather than the desired recordings of activities within a business process. The present study investigates the use of data mining techniques for filtering and aggregating raw RFID data. We consider classifiers based on logistic regression, decision trees, and artificial neural networks using attributes derived from low-level reader data. In addition, we present a custom-made algorithm for generating decision rules using artificial attributes and an iterative training procedure. We evaluate the classifiers using a massive set of data on pallet movements collected under real-world conditions at one of the largest retailers worldwide. The results clearly indicate high classification performance of the classification models, with the rule-based classifier outperforming all others. Moreover, we show that utilizing the full spectrum of data generated by the reader hardware leads to superior performance compared with the approaches based on timestamp and antenna information proposed in prior research.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {25},
numpages = {30},
keywords = {decision trees, false-positive reads, RFID, logistic regression, machine learning, artificial neural networks, time series analysis}
}

@article{10.1145/2629436,
author = {Silva, Thushari and Jian, Ma and Chen, Yang},
title = {Process Analytics Approach for R&amp;D Project Selection},
year = {2014},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629436},
doi = {10.1145/2629436},
abstract = {R&amp;D project selection plays an important role in government funding agencies, as allocation of billions of dollars among the proposals deemed highly influential and contributive solely depend on it. Efficacious assignment of reviewers is one of the most critical processes that controls the quality of the entire project selection and also has a serious implication on business profit. Current methods that focus on workflow automation are more efficient than manual assignment; however, they are not effective, as they fail to consider the real insight of core tasks. Other decision models that analyze core tasks are effective but inefficient when handling large amounts of submissions, and they suffer from irrelevant assignment. Furthermore, they largely ignore real deep insight of back-end data such as quality of the reviewers (e.g., quality and citation impact of their produced research) and the effect of social relationships in project selection processes that are essential for identifying reviewers for interdisciplinary proposal evaluation. In light of these deficiencies, this research proposes a novel hybrid process analytics approach to decompose the complex reviewer assignment process into manageable subprocesses and applies data-driven decision models cum process analytics systematically from a triangular perspective via the research analytics framework to achieve high operational efficiencies and high-quality assignment. It also analyzes big data from scientific databases and generates visualized decision-ready information to support effective decision making. The proposed approach has been implemented to aid the project selection process of the largest funding agency in China and has been tested. The test results show that the proposed approach has the potential to add great benefits, including cost saving, improved effectiveness, and increased business value.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {21},
numpages = {34},
keywords = {research analytics, reviewer assignment, big data, Process analytics}
}

@article{10.1145/2629445,
author = {Wang, G. Alan and Wang, Harry Jiannan and Li, Jiexun and Abrahams, Alan S. and Fan, Weiguo},
title = {An Analytical Framework for Understanding Knowledge-Sharing Processes in Online Q&amp;A Communities},
year = {2014},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629445},
doi = {10.1145/2629445},
abstract = {Online communities have become popular knowledge sources for both individuals and organizations. Computer-mediated communication research shows that communication patterns play an important role in the collaborative efforts of online knowledge-sharing activities. Existing research is mainly focused on either user egocentric positions in communication networks or communication patterns at the community level. Very few studies examine thread-level communication and process patterns and their impacts on the effectiveness of knowledge sharing. In this study, we fill this research gap by proposing an innovative analytical framework for understanding thread-level knowledge sharing in online Q&amp;A communities based on dialogue act theory, network analysis, and process mining. More specifically, we assign a dialogue act tag for each post in a discussion thread to capture its conversation purpose and then apply graph and process mining algorithms to examine knowledge-sharing processes. Our results, which are based on a real support forum dataset, show that the proposed analytical framework is effective in identifying important communication, conversation, and process patterns that lead to helpful knowledge sharing in online Q&amp;A communities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {18},
numpages = {31},
keywords = {knowledge sharing, online community, communication network, process mining, Computer-mediated communication, dialogue act}
}

@article{10.1145/2629395,
author = {Jain, Radhika and Cao, Lan and Mohan, Kannan and Ramesh, Balasubramaniam},
title = {Situated Boundary Spanning: An Empirical Investigation of Requirements Engineering Practices in Product Family Development},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629395},
doi = {10.1145/2629395},
abstract = {Requirements Engineering (RE) faces considerable challenges that are often related to boundaries between various stakeholders involved in the software development process. These challenges may be addressed by boundary spanning practices. We examine how boundary spanning can be adapted to address RE challenges in Product Family Development (PFD), a context that involves complex RE. We study two different development approaches, namely, conventional and agile PFD, because these present considerably different challenges. Our findings from a multisite case study present boundary spanning as a solution to improve the quality of RE processes and highlight interesting differences in how boundary spanner roles and boundary objects are adapted in conventional and agile PFD.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {29},
keywords = {Boundary spanning, agile development, product family development, requirements engineering}
}

@article{10.1145/2629432,
author = {Nekvi, Md Rashed I. and Madhavji, Nazim H.},
title = {Impediments to Regulatory Compliance of Requirements in Contractual Systems Engineering Projects: A Case Study},
year = {2014},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629432},
doi = {10.1145/2629432},
abstract = {Large-scale contractual systems engineering projects often need to comply with myriad government regulations and standards as part of contractual obligations. A key activity in the requirements engineering (RE) process for such a project is to demonstrate that all relevant requirements have been elicited from the regulatory documents and have been traced to the contract as well as to the target system components. That is, the requirements have met regulatory compliance. However, there are impediments to achieving this level of compliance due to such complexity factors as voluminous contract, large number of regulatory documents, and multiple domains of the system. Little empirical research has been conducted in the scientific community on identifying these impediments. Knowing these impediments is a driver for change in the solutions domain (i.e., creating improved or new methods, tools, processes, etc.) to deal with such impediments. Through a case study of an industrial RE project, we have identified a number of key impediments to achieving regulatory compliance in a large-scale, complex, systems engineering project. This project is an upgrade of a rail infrastructure system. The key contribution of the article is a number of hitherto uncovered impediments described in qualitative and quantitative terms. The article also describes an artefact model, depicting key artefacts and relationships involved in such a compliance project. This model was created from data gathered and observations made in this compliance project. In addition, the article describes emergent metrics on regulatory compliance of requirements that can possibly be used for estimating the effort needed to achieve regulatory compliance of system requirements.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {15},
numpages = {35},
keywords = {rail infrastructure system, effort estimation, Legal requirements elicitation, compliance of requirements}
}

@article{10.1145/2629630,
author = {Jiang, Jie and Aldewereld, Huib and Dignum, Virginia and Tan, Yao-Hua},
title = {Compliance Checking of Organizational Interactions},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629630},
doi = {10.1145/2629630},
abstract = {In business environments, different sorts of regulations are imposed to restrict the behavior of both public and private organizations, ranging from legal regulations to internal policies. Regulatory compliance is important for the safety of individual actors as well as the overall business environment. However, complexity derives from not only the contents of the regulations but also their interdependencies. As such, the verification of whether actors are able to comply with the combined regulations cannot be done by checking each regulation separately.To these ends, we introduce a normative structure Norm Nets (NNs) for modeling sets of interrelated regulations and setting a basis for compliance checking of organizational interactions against interrelated regulations. NNs support a modular design by providing the constructs to represent regulations and the relationships between them. Additionally, we propose a computational mechanism to reason about regulatory compliance by mapping NNs to Colored Petri Nets (CPNs). We show that compliance checking of both individual actors’ behavior and the collective behavior of the business environment can be achieved automatically using state space analysis techniques of CPNs. The approach is illustrated with a case study from the domain of international trade.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {23},
numpages = {24},
keywords = {regulated business environments, Norm compliance, Colored Petri Nets}
}

@article{10.1145/2629448,
author = {Yan, Jiaqi and Hu, Daning and Liao, Stephen S. and Wang, Huaiqing},
title = {Mining Agents’ Goals in Agent-Oriented Business Processes},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629448},
doi = {10.1145/2629448},
abstract = {When designing a business process, individual agents are assigned to perform tasks based on certain goals (i.e., designed process goals). However, based on their own interests, real-world agents often have different goals (i.e., agents’ goals) and thus may behave differently than designed, often resulting in reduced effectiveness or efficiencies of the executed process. Moreover, existing business process research lacks effective methods for discovering agents’ goals in the actual execution of the designed business processes. To address this problem, we propose an agent-oriented goal mining approach to modeling, discovering, and analyzing agents’ goals in executed business processes using historical event logs and domain data. To the best of our knowledge, our research is the first to adopt the agents’ goal perspective to study inconsistencies between the design and execution of business processes. Moreover, it also provides a useful tool for stakeholders to discover real-world agents’ actual goals and thus provides insights for improving the task assignment mechanism or business process design in general.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {20},
numpages = {22},
keywords = {Goal mining, agent-oriented business process, belief-desire-intention model}
}

@article{10.1145/2629597,
author = {Jarke, Matthias and Lyytinen, Kalle},
title = {Editorial: “Complexity of Systems Evolution: Requirements Engineering Perspective”},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629597},
doi = {10.1145/2629597},
abstract = {Walking on water, and programming according to specifications is easy—as long as both of them are frozen. --Robert GlassThis introduction discusses the changing nature of complexity associated with requirements engineering (RE) tasks and how it has shifted from managing internal complexity to adapting and leveraging upon external and dynamic complexity. We note several significant drivers in the requirements knowledge that have resulted in this change and discuss in light of complexity theory how the RE research community can respond to this. We observe several research challenges associated with “new complexity” and highlight how the articles included in the special issue advance the field by defining complexity more accurately, observing more vigilantly new sources of complexity, and suggesting new ways to manage complexity in terms of economic assessments, knowledge flows, and modeling for adaptability.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {11},
numpages = {7},
keywords = {edge computing, complexity, requirements knowledge, agility, requirements engineering, System evolution}
}

@article{10.1145/2629447,
author = {Ciccio, Claudio Di and Mecella, Massimo},
title = {On the Discovery of Declarative Control Flows for Artful Processes},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629447},
doi = {10.1145/2629447},
abstract = {Artful processes are those processes in which the experience, intuition, and knowledge of the actors are the key factors in determining the decision making. They are typically carried out by the “knowledge workers,” such as professors, managers, and researchers. They are often scarcely formalized or completely unknown a priori. Throughout this article, we discuss how we addressed the challenge of discovering declarative control flows in the context of artful processes. To this extent, we devised and implemented a two-phase algorithm, named MINERful. The first phase builds a knowledge base, where statistical information extracted from logs is represented. During the second phase, queries are evaluated on that knowledge base, in order to infer the constraints that constitute the discovered process. After outlining the overall approach and offering insight on the adopted process modeling language, we describe in detail our discovery technique. Thereupon, we analyze its performances, both from a theoretical and an experimental perspective. A user-driven evaluation of the quality of results is also reported on the basis of a real case study. Finally, a study on the fitness of discovered models with respect to synthetic and real logs is presented.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {24},
numpages = {37},
keywords = {artful processes, MailOfMine, control-flow discovery, Process mining, declarative process model}
}

@article{10.1145/2629446,
author = {Partington, Andrew and Wynn, Moe and Suriadi, Suriadi and Ouyang, Chun and Karnon, Jonathan},
title = {Process Mining for Clinical Processes: A Comparative Analysis of Four Australian Hospitals},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629446},
doi = {10.1145/2629446},
abstract = {Business process analysis and process mining, particularly within the health care domain, remain under-utilized. Applied research that employs such techniques to routinely collected health care data enables stakeholders to empirically investigate care as it is delivered by different health providers. However, cross-organizational mining and the comparative analysis of processes present a set of unique challenges in terms of ensuring population and activity comparability, visualizing the mined models, and interpreting the results. Without addressing these issues, health providers will find it difficult to use process mining insights, and the potential benefits of evidence-based process improvement within health will remain unrealized. In this article, we present a brief introduction on the nature of health care processes, a review of process mining in health literature, and a case study conducted to explore and learn how health care data and cross-organizational comparisons with process-mining techniques may be approached. The case study applies process-mining techniques to administrative and clinical data for patients who present with chest pain symptoms at one of four public hospitals in South Australia. We demonstrate an approach that provides detailed insights into clinical (quality of patient health) and fiscal (hospital budget) pressures in the delivery of health care. We conclude by discussing the key lessons learned from our experience in conducting business process analysis and process mining based on the data from four different hospitals.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {19},
numpages = {18},
keywords = {comparative analysis, patient pathways, Process mining, data preparation, health care delivery}
}

@article{10.1145/2629375,
author = {King, John Leslie and Simon, Carl P.},
title = {Complications with Complexity in Requirements},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629375},
doi = {10.1145/2629375},
abstract = {Requirements engineering must recognize the difference between complicated and complex problems. The former can lead to successful solutions. The latter should be avoided because they often lead to failure. As a starting point for distinguishing between complicated and complex, this article offers six characteristics of complex problems, with examples from economics, logistics, forecasting, among others. These characteristics make it easier and more systematic to recognize complexity during requirements elicitation and formulation.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {13},
numpages = {12},
keywords = {complex, requirements, Complicated, information systems, software}
}

@article{10.1145/2629351,
author = {Chakraborty, Suranjan and Rosenkranz, Christoph and Dehlinger, Josh},
title = {Getting to the Shalls: Facilitating Sensemaking in Requirements Engineering},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629351},
doi = {10.1145/2629351},
abstract = {Sensemaking in Requirements Engineering (RE) relies on knowledge transfer, communication, and negotiation of project stakeholders. It is a critical and challenging aspect of Information Systems (IS) development. One of the most fundamental aspects of RE is the specification of traceable, unambiguous, and operationalizable functional and nonfunctional requirements. This remains a nontrivial task in the face of the complexity inherent in RE due to the lack of well-documented, systematic procedures that facilitate a structured analysis of the qualitative data from stakeholder interviews, observations, and documents that are typically the input to this activity. This research develops a systematic and traceable procedure, for non-functional requirements the Grounded and Linguistic-Based Requirements Analysis Procedure (GLAP), which can fill this gap by incorporating perspectives from Grounded Theory Method, linguistic analysis of language quality, Volere typology, and the Nonfunctional Requirements Framework without significantly deviating from existing practice. The application of GLAP is described along with empirical illustrations using RE data from a redesign initiative of a library website of a public university in the United States. An outlook is given on further work and necessary evaluation steps.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {14},
numpages = {30},
keywords = {grounded theory methodology, Qualitative analysis, nonfunctional requirements framework, linguistic analysis}
}

@article{10.1145/2688488,
author = {Lee, Yen-Hsien and Hu, Paul Jen-Hwa and Tu, Ching-Yi},
title = {Ontology-Based Mapping for Automated Document Management: A Concept-Based Technique for Word Mismatch and Ambiguity Problems in Document Clustering},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2688488},
doi = {10.1145/2688488},
abstract = {Document clustering is crucial to automated document management, especially for the fast-growing volume of textual documents available digitally. Traditional lexicon-based approaches depend on document content analysis and measure overlap of the feature vectors representing different documents, which cannot effectively address word mismatch or ambiguity problems. Alternative query expansion and local context discovery approaches are developed but suffer from limited efficiency and effectiveness, because the large number of expanded terms create noise and increase the dimensionality and complexity of the overall feature space. Several techniques extend lexicon-based analysis by incorporating latent semantic indexing but produce less comprehensible clustering results and questionable performance. We instead propose a concept-based document representation and clustering (CDRC) technique and empirically examine its effectiveness using 433 articles concerning information systems and technology, randomly selected from a popular digital library. Our evaluation includes two widely used benchmark techniques and shows that CDRC outperforms them. Overall, our results reveal that clustering documents at an ontology-based, concept-based level is more effective than techniques using lexicon-based document features and can generate more comprehensible clustering results.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {4},
numpages = {22},
keywords = {ontology-supported document clustering, knowledge management, document clustering, Document-category management}
}

@article{10.1145/2676869,
author = {Li, Shing-Han and Kao, Yu-Cheng and Zhang, Zong-Cyuan and Chuang, Ying-Ping and Yen, David C.},
title = {A Network Behavior-Based Botnet Detection Mechanism Using PSO and K-Means},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2676869},
doi = {10.1145/2676869},
abstract = {In today's world, Botnet has become one of the greatest threats to network security. Network attackers, or Botmasters, use Botnet to launch the Distributed Denial of Service (DDoS) to paralyze large-scale websites or steal confidential data from infected computers. They also employ “phishing” attacks to steal sensitive information (such as users’ accounts and passwords), send bulk email advertising, and/or conduct click fraud. Even though detection technology has been much improved and some solutions to Internet security have been proposed and improved, the threat of Botnet still exists. Most of the past studies dealing with this issue used either packet contents or traffic flow characteristics to identify the invasion of Botnet. However, there still exist many problems in the areas of packet encryption and data privacy, simply because Botnet can easily change the packet contents and flow characteristics to circumvent the Intrusion Detection System (IDS). This study combines Particle Swarm Optimization (PSO) and K-means algorithms to provide a solution to remedy those problems and develop, step by step, a mechanism for Botnet detection. First, three important network behaviors are identified: long active communication behavior (ActBehavior), connection failure behavior (FailBehavior), and network scanning behavior (ScanBehavior). These behaviors are defined according to the relevant prior studies and used to analyze the communication activities among the infected computers. Second, the features of network behaviors are extracted from the flow traces in the network layer and transport layer of the network equipment. Third, PSO and K-means techniques are used to uncover the host members of Botnet in the organizational network. This study mainly utilizes the flow traces of a campus network as an experiment. The experimental findings show that this proposed approach can be employed to detect the suspicious Botnet members earlier than the detection application systems. In addition, this proposed approach is easy to implement and can be further used and extended in the campus dormitory network, home networks, and the mobile 3G network.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {3},
numpages = {30},
keywords = {K-means clustering, particle swarm optimization, network traffic analysis, Botnet}
}

@article{10.1145/2675693,
author = {Zimbra, David and Chen, Hsinchun and Lusch, Robert F.},
title = {Stakeholder Analyses of Firm-Related Web Forums: Applications in Stock Return Prediction},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2675693},
doi = {10.1145/2675693},
abstract = {In this study, we present stakeholder analyses of firm-related web forums. Prior analyses of firm-related forums have considered all participants in the aggregate, failing to recognize the potential for diversity within the populations. However, distinctive groups of forum participants may represent various interests and stakes in a firm worthy of consideration. To perform the stakeholder analyses, the Stakeholder Analyzer system for firm-related web forums is developed following the design science paradigm of information systems research. The design of the system and its approach to stakeholder analysis is guided by two kernel theories, the stakeholder theory of the firm and the systemic functional linguistic theory. A stakeholder analysis identifies distinctive groups of forum participants with shared characteristics expressed in discussion and evaluates their specific opinions and interests in the firm. Stakeholder analyses are performed in six major firm-related forums hosted on Yahoo Finance over a 3-month period. The relationships between measures extracted from the forums and subsequent daily firm stock returns are examined using multiple linear regression models, revealing statistically significant indicators of firm stock returns in the discussions of the stakeholder groups of each firm with stakeholder-model-adjusted R2 values reaching 0.83. Daily stock return prediction is also performed for 31 trading days, and stakeholder models correctly predicted the direction of return on 67% of trading days and generated an impressive 17% return in simulated trading of the six firm stocks. These evaluations demonstrate that the stakeholder analyses provided more refined assessments of the firm-related forums, yielding measures at the stakeholder group level that better explain and predict daily firm stock returns than aggregate forum-level information.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {2},
numpages = {38},
keywords = {sentiment analysis, Social media analytics, stock prediction, stakeholder analysis, web forums}
}

@article{10.1145/2669368,
author = {Berndt, Donald J. and McCart, James A. and Finch, Dezon K. and Luther, Stephen L.},
title = {A Case Study of Data Quality in Text Mining Clinical Progress Notes},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2669368},
doi = {10.1145/2669368},
abstract = {Text analytic methods are often aimed at extracting useful information from the vast array of unstructured, free format text documents that are created by almost all organizational processes. The success of any text mining application rests on the quality of the underlying data being analyzed, including both predictive features and outcome labels. In this case study, some focused experiments regarding data quality are used to assess the robustness of Statistical Text Mining (STM) algorithms when applied to clinical progress notes. In particular, the experiments consider the impacts of task complexity (by removing signals), training set size, and target outcome quality. While this research is conducted using a dataset drawn from the medical domain, the data quality issues explored are of more general interest.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {1},
numpages = {21},
keywords = {feature selection, predictive model quality, health informatics, electronic health records, text mining, noisy text analysis, clinical progress notes, data quality, Machine learning}
}

@article{10.1145/2685352,
author = {Aalst, Wil Van Der and Zhao, J. Leon and Wang, Harry Jiannan},
title = {Editorial: “Business Process Intelligence: Connecting Data and Processes”},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2685352},
doi = {10.1145/2685352},
abstract = {This introduction to the special issue on Business Process Intelligence (BPI) discusses the relation between data and processes. The recent attention for Big Data illustrates that organizations are aware of the potential of the torrents of data generated by today's information systems. However, at the same time, organizations are struggling to extract value from this overload of data. Clearly, there is a need for data scientists able to transform event data into actionable information. To do this, it is crucial to take a process perspective. The ultimate goal of BPI is not to improve information systems or the recording of data; instead the focus should be in improving the process. For example, we may want to aim at reducing costs, minimizing response times, and ensuring compliance. This requires a “confrontation” between process models and event data. Recent advances in process mining allow us to automatically learn process models showing the bottlenecks from “raw” event data. Moreover, given a normative model, we can use conformance checking to quantify and understand deviations. Automatically learned models may also be used for prediction and recommendation. BPI is rapidly developing as a field linking data science to business process management. This article aims to provide an overview thereby paving the way for the other contributions in this special issue.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {18e},
numpages = {7},
keywords = {process modeling, business process intelligence, Process mining, performance analysis, compliance checking}
}

@article{10.1145/2688489,
author = {Sutanto, Juliana and Kankanhalli, Atreyi and Yian Tan, Bernard Cheng},
title = {Investigating Task Coordination in Globally Dispersed Teams: A Structural Contingency Perspective},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2688489},
doi = {10.1145/2688489},
abstract = {Task coordination poses significant challenges for globally dispersed teams (GDTs). Although various task coordination mechanisms have been proposed for such teams, there is a lack of systematic examination of the appropriate coordination mechanisms for different teams based on the nature of their task and the context under which they operate. Prior studies on collocated teams suggest matching their levels of task dependence to specific task coordination mechanisms for effective coordination. This research goes beyond the earlier work by also considering additional contextual factors of GDT (i.e., temporal dispersion and time constraints) in deriving their optimal IT-mediated task coordination mechanisms. Adopting the structural contingency theory, we propose optimal IT-mediated task coordination portfolios to fit the different levels of task dependence, temporal dispersion, and perceived time constraint of GDTs. The proposed fit is tested through a survey and profile analysis of 95 globally dispersed software development teams in a large financial organization. We find, as hypothesized, that the extent of fit between the actual IT-mediated task coordination portfolios used by the surveyed teams and their optimal portfolios proposed here is positively related to their task coordination effectiveness that in turn impacts the team's efficiency and effectiveness. The implications for theory and practice are discussed.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {5},
numpages = {31},
keywords = {fit analysis, perceived time constraint, IT-mediated task coordination portfolio, globally dispersed team, temporal dispersion, task dependence}
}

@article{10.1145/2744207,
author = {Zhao, Xiaohui and Liu, Chengfei and Yongchareon, Sira and Kowalkiewicz, Marek and Sadiq, Wasim},
title = {Role-Based Process View Derivation and Composition},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2744207},
doi = {10.1145/2744207},
abstract = {The process view concept deploys a partial and temporal representation to adjust the visible view of a business process according to various perception constraints of users. Process view technology is of practical use for privacy protection and authorization control in process-oriented business management. Owing to complex organizational structure, it is challenging for large companies to accurately specify the diverse perception of different users over business processes. Aiming to tackle this issue, this article presents a role-based process view model to incorporate role dependencies into process view derivation. Compared to existing process view approaches, ours particularly supports runtime updates to the process view perceivable to a user with specific view merging operations, thereby enabling the dynamic tracing of process perception. A series of rules and theorems are established to guarantee the structural consistency and validity of process view transformation. A hypothetical case is conducted to illustrate the feasibility of our approach, and a prototype is developed for the proof-of-concept purpose.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {7},
numpages = {24},
keywords = {Business process view, collaborative business process, process perception}
}

@article{10.1145/2724730,
author = {Basole, Rahul C. and Russell, Martha G. and Huhtam\"{a}ki, Jukka and Rubens, Neil and Still, Kaisa and Park, Hyunwoo},
title = {Understanding Business Ecosystem Dynamics: A Data-Driven Approach},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2724730},
doi = {10.1145/2724730},
abstract = {Business ecosystems consist of a heterogeneous and continuously evolving set of entities that are interconnected through a complex, global network of relationships. However, there is no well-established methodology to study the dynamics of this network. Traditional approaches have primarily utilized a single source of data of relatively established firms; however, these approaches ignore the vast number of relevant activities that often occur at the individual and entrepreneurial levels. We argue that a data-driven visualization approach, using both institutionally and socially curated datasets, can provide important complementary, triangulated explanatory insights into the dynamics of interorganizational networks in general and business ecosystems in particular. We develop novel visualization layouts to help decision makers systemically identify and compare ecosystems. Using traditionally disconnected data sources on deals and alliance relationships (DARs), executive and funding relationships (EFRs), and public opinion and discourse (POD), we empirically illustrate our data-driven method of data triangulation and visualization techniques through three cases in the mobile industry Google’s acquisition of Motorola Mobility, the coopetitive relation between Apple and Samsung, and the strategic partnership between Nokia and Microsoft. The article concludes with implications and future research opportunities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {6},
numpages = {32},
keywords = {business ecosystem, interorganizational networks, Data triangulation, information visualization}
}

@article{10.1145/2764920,
author = {Liu, Dengpan and Sarkar, Sumit and Sriskandarajah, Chelliah},
title = {Who's Next? Scheduling Personalization Services with Variable Service Times},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2764920},
doi = {10.1145/2764920},
abstract = {Online personalization has become quite prevalent in recent years, with firms able to derive additional profits from such services. As the adoption of such services grows, firms implementing such practices face some operational challenges. One important challenge lies in the complexity associated with the personalization process and how to deploy available resources to handle such complexity. The complexity is exacerbated when a site faces a large volume of requests in a short amount of time, as is often the case for e-commerce and content delivery sites. In such situations, it is generally not possible for a site to provide perfectly personalized service to all requests. Instead, a firm can provide differentiated service to requests based on the amount of profiling information available about the visitor. We consider a scenario where the revenue function is concave, capturing the diminishing returns from personalization effort. Using a batching approach, we determine the optimal scheduling policy (i.e., time allocation and sequence of service) for a batch that accounts for the externality cost incurred when a request is provided service before other waiting requests. The batching approach leads to sunk costs incurred when visitors wait for the next batch to begin. An optimal admission control policy is developed to prescreen new request arrivals. We show how the policy can be implemented efficiently when the revenue function is complex and there are a large number of requests that can be served in a batch. Numerical experiments show that the proposed approach leads to substantial improvements over a linear approximation of the concave revenue function. Interestingly, we find that the improvements in firm profits are not only (or primarily) due to the different service times that are obtained when using the nonlinear personalization function—there is a ripple effect on the admission control policy that incorporates these optimized service times, which contributes even more to the additional profits than the service time optimization by itself.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {8},
numpages = {22},
keywords = {admission control policy, resource allocation, Online personalization, hierarchical decision tables, batch processing}
}

@article{10.1145/2811269,
author = {Roy, Arindam and Sural, Shamik and Majumdar, Arun Kumar and Vaidya, Jaideep and Atluri, Vijayalakshmi},
title = {Minimizing Organizational User Requirement While Meeting Security Constraints},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2811269},
doi = {10.1145/2811269},
abstract = {Large systems are complex and typically need automatic configuration to be managed effectively. In any organization, numerous tasks have to be carried out by employees. However, due to security needs, it is not feasible to directly assign any existing task to the first available employee. In order to meet many additional security requirements, constraints such as separation of duty, cardinality and binding have to be taken into consideration. Meeting these requirements imposes extra burden on organizations, which, however, is unavoidable in order to ensure security. While a trivial way of ensuring security is to assign each user to a single task, business organizations would typically like to minimize their costs and keep staffing requirements to a minimum. To meet these contradictory goals, we define the problem of Cardinality Constrained-Mutually Exclusive Task Minimum User Problem (CMUP), which aims to find the minimum users that can carry out a set of tasks while satisfying the given security constraints. We show that the CMUP problem is equivalent to a constrained version of the weak chromatic number problem in hypergraphs, which is NP-hard. We, therefore, propose a greedy solution. Our experimental evaluation shows that the proposed algorithm is both efficient and effective.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {12},
numpages = {25},
keywords = {constraints, hypergraph, weak chromatic number problem, User task assignments}
}

@article{10.1145/2795235,
author = {Bhowmik, Tanmay and Niu, Nan and Singhania, Prachi and Wang, Wentao},
title = {On the Role of Structural Holes in Requirements Identification: An Exploratory Study on Open-Source Software Development},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2795235},
doi = {10.1145/2795235},
abstract = {Requirements identification is a human-centric activity that involves interaction among multiple stakeholders. Traditional requirements engineering (RE) techniques addressing stakeholders’ social interaction are mainly part of a centralized process intertwined with a specific phase of software development. However, in open-source software (OSS) development, stakeholders’ social interactions are often decentralized, iterative, and dynamic. Little is known about new requirements identification in OSS and the stakeholders’ organizational arrangements supporting such an activity. In this article, we investigate the theory of structural hole from the context of contributing new requirements in OSS projects. Structural hole theory suggests that stakeholders positioned in the structural holes in their social network are able to produce new ideas. In this study, we find that structural hole positions emerge in stakeholders’ social network and these positions are positively related to contributing a higher number of new requirements. We find that along with structural hole positions, stakeholders’ role is also an important part in identifying new requirements. We further observe that structural hole positions evolve over time, thereby identifying requirements to realize enriched features. Our work advances the fundamental understanding of the RE process in a decentralized environment and opens avenues for improved techniques supporting this process.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {10},
numpages = {30},
keywords = {stakeholders’ social network, brokerage, social information foraging theory, structural hole, Requirements identification, social capital, open-source requirements engineering}
}

@article{10.1145/2764919,
author = {Bai, Xue and Marsden, James R. and Ross, William T. and Wang, Gang},
title = {Relationships Among Minimum Requirements, Facebook Likes, and Groupon Deal Outcomes},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2764919},
doi = {10.1145/2764919},
abstract = {Daily deal coupons have gained a prominent foothold on the web. The earliest and largest player is Groupon. Originally, Groupon deals were a mix of deals with a minimum requirement (MR) of coupon sales before a deal became effective and of deals without a minimum requirement (NMR). Eventually, Groupon stopped using MR deals. For Groupon and its retailer customers, might this decision have actually resulted in negative impacts for both parties (fewer coupons sold and lower revenue)? The structure of Groupon deals (including a “Facebook like” option) together with electronic access to the necessary data offered the opportunity to empirically investigate these questions. We analyzed relationships among MR, Facebook likes (FL), quantity of coupons sold, and total revenue, performing the analysis across the four largest retail categories. Using timestamped empirical data, we completed a propensity score analysis of causal effects. We find that the presence of MR increases Facebook likes, quantity of coupons sold, and total revenue at the time point when the MR is met and at subsequent 2-hour intervals over the horizon of deals. A key finding is that the initial differences observed when MR is met not only continue but also actually increase over the life of the deals.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {9},
numpages = {28},
keywords = {daily deal sites, Groupon, Social media, e-WOM, propensity score analysis (PSA)}
}

@article{10.1145/2811270,
author = {Bhattacharya, Devipsita and Ram, Sudha},
title = {RT @News: An Analysis of News Agency Ego Networks in a Microblogging Environment},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2811270},
doi = {10.1145/2811270},
abstract = {News agencies regularly use Twitter to publicize and increase readership of their articles. Although substantial research on the spread of news on Twitter exists, there hasn't been much focus on the study of the spread of news articles. In this study, we present an innovative methodology involving weighted ego networks to understand how news agencies propagate news articles using their Twitter handle. We propose a set of measures to compare the propagation process of different news agencies by studying important aspects such as volume, extent of spread, conversion rate, multiplier effect, lifespan, hourly response, and audience participation. Using a dataset of tweets collected over a period of 6 months, we apply our methodology and suggest a framework to help news agencies gauge their performance on social media and also provide critical insights into the phenomenon of news article propagation on Twitter.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {11},
numpages = {25},
keywords = {Twitter, news propagation, microblogging, article propagation}
}

@article{10.1145/2843948,
author = {Gomez-Uribe, Carlos A. and Hunt, Neil},
title = {The Netflix Recommender System: Algorithms, Business Value, and Innovation},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2843948},
doi = {10.1145/2843948},
abstract = {This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a recommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention and medium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {13},
numpages = {19},
keywords = {Recommender systems}
}

@article{10.1145/2850423,
author = {Liaskos, Christos and Tsioliaridou, Ageliki},
title = {Service Ratio-Optimal, Content Coherence-Aware Data Push Systems},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2850423},
doi = {10.1145/2850423},
abstract = {Advertising new information to users via push is the trigger of operation for many contemporary information systems. Furthermore, passive optical networks are expected to extend the reachability of high-quality push services to thousands of clients. The efficiency of a push service is the ratio of successfully informed users. However, pushing only data of high popularity can degrade the thematic coherency of the content. The present work offers a novel, analysis-derived, tunable way for selecting data for push services. The proposed scheme can maximize the service ratio of a push system with regard to data coherence constraints. Extensive simulations demonstrate the efficiency of the scheme compared to alternative solutions. The proposed scheme is the first to tackle the problem of data coherence-aware, service ratio optimization of push services.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {15},
numpages = {23},
keywords = {Data push, data selection, coherence, service ratio}
}

@article{10.1145/2820618,
author = {Krishnamurthy, Rajiv and Jacob, Varghese and Radhakrishnan, Suresh and Dogan, Kutsal},
title = {Peripheral Developer Participation in Open Source Projects: An Empirical Analysis},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2820618},
doi = {10.1145/2820618},
abstract = {The success of the Open Source model of software development depends on the voluntary participation of external developers (the peripheral developers), a group that can have distinct motivations from that of project founders (the core developers). In this study, we examine peripheral developer participation by empirically examining approximately 2,600 open source projects. In particular, we hypothesize that peripheral developer participation is higher when the potential for building reputation by gaining recognition from project stakeholders is higher. We consider recognition by internal stakeholders (such as core developers) and external stakeholders (such as end-users and peers). We find a positive association between peripheral developer participation and the potential of stakeholder recognition after controlling for bug reports, feature requests, and other key factors. Our findings provide important insights for OSS founders and corporate managers for open sourcing or OSS adoption decisions.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {14},
numpages = {31},
keywords = {open source software, software metrics, project management, Code ownership}
}

@article{10.1145/2883816,
author = {Lu, Xianghua and Zhao, Xia and Xue, Ling},
title = {Is Combining Contextual and Behavioral Targeting Strategies Effective in Online Advertising?},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2883816},
doi = {10.1145/2883816},
abstract = {Online targeting has been increasingly used to deliver ads to consumers. But discovering how to target the most valuable web visitors and generate a high response rate is still a challenge for advertising intermediaries and advertisers. The purpose of this study is to examine how behavioral targeting (BT) impacts users’ responses to online ads and particularly whether BT works better in combination with contextual targeting (CT). Using a large, individual-level clickstream data set of an automobile advertising campaign from an Internet advertising intermediary, this study examines the impact of BT and CT strategies on users’ click behavior. The results show that (1) targeting a user with behavioral characteristics that are closely related to ads does not necessarily increase the click through rates (CTRs); whereas, targeting a user with behavioral characteristics that are loosely related to ads leads to a higher CTR, and (2) BT and CT work better in combination. Our study contributes to online advertising design literature and provides important managerial implications for advertising intermediaries and advertisers on targeting individual users.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {1},
numpages = {20},
keywords = {online advertising, Targeted advertising, contextual targeting, behavioral targeting}
}

@article{10.1145/2875444,
author = {Lui, Tsz-Wai and Piccoli, Gabriele},
title = {The Effect of a Multichannel Customer Service System on Customer Service and Financial Performance},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2875444},
doi = {10.1145/2875444},
abstract = {Customer service is an important competitive lever for the modern firm. At the same time, the continuous evolution and performance improvements in information technology (IT) capabilities have enabled the utilization of multichannel service delivery strategies. Our research focuses on IT-enabled customer service systems (CSS) and their effect on firm performance. Previous studies have failed to find a consensus on the effect of a new self-service channel on the firm's performance. We argue that the embedded assumptions underpinning the previous research are responsible for these mixed findings. Consequently, using archival data from 169 hotels affiliated with a hotel chain, we designed a longitudinal multichannel study to resolve some of these inconsistencies. Our results illustrate that when firms implement an IT-enabled self-service channel to complement their existing customer service infrastructure, they experience an early negative effect on financial performance due to the disruption of the service processes. Thus, the multichannel CSS generates a positive effect only when the new process becomes a stable part of the organizational procedures. Our findings suggest that researchers evaluate the effect of a technological initiative after the new business process has been stabilized and consider that an additional IT-enabled self-service channel rarely operates in isolation.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {2},
numpages = {15},
keywords = {process disruption, multichannel, Customer service systems}
}

@article{10.1145/2629569,
author = {Li, Shing-Han and Yen, David C. and Chuang, Ying-Ping},
title = {A Real-Time Audit Mechanism Based on the Compression Technique},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629569},
doi = {10.1145/2629569},
abstract = {Log management and log auditing have become increasingly crucial for enterprises in this era of information and technology explosion. The log analysis technique is useful for discovering possible problems in business processes and preventing illegal-intrusion attempts and data-tampering attacks. Because of the complexity of the dynamically changing environment, auditing a tremendous number of data is a challenging issue. We provide a real-time audit mechanism to improve the aforementioned problems in log auditing. This mechanism was developed based on the Lempel-Ziv-Welch (LZW) compression technique to facilitate effective compression and provide reliable auditing log entries. The mechanism can be used to predict unusual activities when compressing the log data according to pre-defined auditing rules. Auditors using real-time and continuous monitoring can perceive instantly the most likely anomalies or exceptions that could cause problems. We also designed a user interface that allows auditors to define the various compression and audit parameters, using real log cases in the experiment to verify the feasibility and effectiveness of this proposed audit mechanism. In summary, this mechanism changes the log access method and improves the efficiency of log analysis. This mechanism greatly simplifies auditing so that auditors must only trace the sources and causes of the problems related to the detected anomalies. This greatly reduces the processing time of analytical audit procedures and the manual checking time, and improves the log audit efficiency.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {4},
numpages = {25},
keywords = {LZW algorithm, dictionary-based compression, continuous auditing, Real time log audit}
}

@article{10.1145/2886104,
author = {Sun, Yutian and Su, Jianwen and Yang, Jian},
title = {Universal Artifacts: A New Approach to Business Process Management (BPM) Systems},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2886104},
doi = {10.1145/2886104},
abstract = {In most BPM systems (a.k.a. workflow systems), the data for process execution is scattered across databases for enterprise, auxiliary local data stores within the BPM systems, and even file systems (e.g., specification of process models). The interleaving nature of data management and BP execution and the lack of a coherent conceptual data model for all data needed for execution make it hard for (1) providing Business-Process-as-a-Service (BPaaS) and (2) effective support for collaboration between business processes. The primary reason is that an enormous effort is required for maintaining both the engines and the data for the client applications. In particular, different modeling languages and different BPM systems make process interoperation one of the toughest challenges. In this article, we formulate a concept of a “universal artifact,” which extends artifact-centric models by capturing all needed data for a process instance throughout its execution. A framework called SeGA based on universal artifacts is developed to support separation of data and BP execution, a key principle for BPM systems. We demonstrate in this article that SeGA is versatile enough to fully facilitate not only executions of individual processes (to support BPaaS) but also various collaboration models. Moreover, SeGA reduces the complexity in runtime management including runtime querying, constraints enforcement, and dynamic modification upon collaboration across possibly different BPM systems.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {3},
numpages = {26},
keywords = {workflow systems, BPaaS, process collaboration}
}

@article{10.1145/2893187,
author = {Hashmi, Khayyam and Malik, Zaki and Najmi, Erfan and Alhosban, Amal and Medjahed, Brahim},
title = {A Web Service Negotiation Management and QoS Dependency Modeling Framework},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2893187},
doi = {10.1145/2893187},
abstract = {Information Management Systems that outsource part of the functionality to other (likely unknown) services need an effective way to communicate with these services, so that a mutually beneficial solution can be generated. This includes bargaining for their optimal customizations and the discovery of overlooked potential solutions. In this article, we present an automated negotiation framework for information systems (denoted as WebNeg) that can be used by both the parties for conducting negotiations. WebNeg uses a Genetic Algorithm (GA)-based approach for finding acceptable solutions in multiparty and multiobjective scenarios. The GA is enhanced using a new operator called Norm, which represents the cumulative knowledge of all the parties involved in the negotiation process. Norm incorporates the dependencies of different quality attributes of independently developed component services for the system composition. This enables WebNeg to find a better solution in the context of the current requirements. Experiment results indicate the applicability and improved performance of WebNeg (in comparison with existing similar works) in facilitating the negotiation management involved in a web service-based information composition process.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {5},
numpages = {33},
keywords = {Negotiation management, web service, genetic algorithm}
}

@article{10.1145/2934695,
author = {Gupta, Agam and Saha, Biswatosh and Sarkar, Uttam K.},
title = {Systemic Concentration in Sponsored Search Markets: The Role of Time Window in Click-Through-Rate Computation},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2934695},
doi = {10.1145/2934695},
abstract = {Keyword-based search engine advertising markets on the Internet, referred to as Sponsored Search Markets (SSMs), have reduced entry barriers to advertising for niche players. Known empirical research, though scant and emerging, suggests that while these markets provided niche firms with greater access, they do exhibit high levels of concentration—a phenomenon that warrants further study. This research, using agent-based simulation of SSM, investigates the role of “market rules” and “advertiser practices” in generating emergent click share heterogeneity among advertisers in an industry. SSMs often rank ads based on the click-through rate (CTR) that gives rise to reinforcing dynamics at an individual keyword level. In the presence of spillovers arising from advertisers’ practice of managing keyword bids with a cost cap operating on the keyword portfolio, these reinforcing dynamics can endogenously generate industry-level concentration. Analysis of counterfactual markets with different window sizes used to compute CTR reveals that industry-level concentration bears an inverted-“U” relationship with window size.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {6},
numpages = {26},
keywords = {auctions, agent-based models, user response, Sponsored search, electronic markets, concentration}
}

@article{10.1145/2948072,
author = {Tsai, Ming-Feng and Wang, Chuan-Ju and Chien, Po-Chuan},
title = {Discovering Finance Keywords via Continuous-Space Language Models},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2948072},
doi = {10.1145/2948072},
abstract = {The growing amount of public financial data makes it increasingly important to learn how to discover valuable information for financial decision making. This article proposes an approach to discovering financial keywords from a large number of financial reports. In particular, we apply the continuous bag-of-words (CBOW) model, a well-known continuous-space language model, to the textual information in 10-K financial reports to discover new finance keywords. In order to capture word meanings to better locate financial terms, we also present a novel technique to incorporate syntactic information into the CBOW model. Experimental results on four prediction tasks using the discovered keywords demonstrate that our approach is effective for discovering predictability keywords for post-event volatility, stock volatility, abnormal trading volume, and excess return predictions. We also analyze the discovered keywords that attest to the ability of the proposed method to capture both syntactic and contextual information between words. This shows the success of this method when applied to the field of finance.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {7},
numpages = {17},
keywords = {finance, Continuous-space language model, text mining}
}

@article{10.1145/2990197,
author = {Xu, Jiajie and Liu, Chengfei and Zhao, Xiaohui and Yongchareon, Sira and Ding, Zhiming},
title = {Resource Management for Business Process Scheduling in the Presence of Availability Constraints},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2990197},
doi = {10.1145/2990197},
abstract = {In the context of business process management, the resources required by business processes, such as workshop staff, manufacturing machines, etc., tend to follow certain availability patterns, due to maintenance cycles, work shifts and other factors. Such availability patterns heavily influence the efficiency and effectiveness of enterprise resource management. Most existing process scheduling and resource management approaches tend to tune the process structure to seek better resource utilisation, yet neglect the constraints on resource availability. In this article, we investigate the scheduling of business process instances in accordance with resource availability patterns, to find out how enterprise resources can be rationally and sufficiently used. Three heuristic-based planning strategies are proposed to maximise the process instance throughput together with another strategy based on a genetic algorithm. The performance of these strategies has been evaluated by conducting experiments of different settings and analysing the strategy characteristics.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {9},
numpages = {26},
keywords = {business process management, business process scheduling, Resource management, resource planning}
}

@article{10.1145/2965085,
author = {Burnay, Corentin},
title = {Are Stakeholders the Only Source of Information for Requirements Engineers? Toward a Taxonomy of Elicitation Information Sources},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2965085},
doi = {10.1145/2965085},
abstract = {Requirements elicitation consists in collecting and documenting information about the requirements from a system-to-be and about the environment of that system. Elicitation forms a critical step in the design of any information system, subject to many challenges like information incompleteness, variability, or ambiguity. To deal with these challenges, requirements engineers heavily rely on stakeholders, who turn out to be one of the most significant provider of information during elicitation. Sometimes, this comes at the cost of less attention being paid by engineers to other sources of information accessible in a business. In this article, we try to deal with this issue by studying the different sources of information that can be used by engineers when designing a system. We propose TELIS (a Taxonomy of Elicitation Sources), which can be used during elicitation to review more systematically the sources of information about a system-to-be. TELIS was produced through a series of empirical studies and was partially validated through a real-world case study. Our objective in this article is to increase the awareness of engineers about the other information providers within a business. Ultimately, we believe our taxonomy may help in better dealing with classical elicitation challenges and increase the chances of successful information systems design.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {8},
numpages = {29},
keywords = {elicitation, exploratory research, taxonomy, empirical study, information sources, Requirements engineering}
}

@article{10.1145/2996470,
author = {Roy, Arindam and Sural, Shamik and Majumdar, Arun Kumar and Vaidya, Jaideep and Atluri, Vijayalakshmi},
title = {On Optimal Employee Assignment in Constrained Role-Based Access Control Systems},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2996470},
doi = {10.1145/2996470},
abstract = {Since any organizational environment is typically resource constrained, especially in terms of human capital, organization managers would like to maximize the utilization of available human resources. However, tasks cannot simply be assigned to arbitrary employees since the employee needs to have the necessary capabilities for executing a task. Furthermore, security policies constrain the assignment of tasks to employees, especially given the other tasks assigned to the same employee. Since role-based access control (RBAC) is the most commonly used access control model for commercial information systems, we limit our attention to consider constraints in RBAC. In this article, we define the Employee Assignment Problem (EAP), which aims to identify an employee to role assignment such that it permits the maximal flexibility in assigning tasks to employees while ensuring that the required security constraints are met. We prove that finding an optimal solution is NP-complete and therefore provide a greedy solution. Experimental evaluation of the proposed approach shows that it is both efficient and effective.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {10},
numpages = {24},
keywords = {graph coloring, statically mutually exclusive roles (SMER) constraint, Role-based access control (RBAC), greedy algorithm}
}

@article{10.1145/3021380,
author = {Chen, Hao and Xiao, Keli and Sun, Jinwen and Wu, Song},
title = {A Double-Layer Neural Network Framework for High-Frequency Forecasting},
year = {2017},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3021380},
doi = {10.1145/3021380},
abstract = {Nowadays, machine trading contributes significantly to activities in the equity market, and forecasting market movement under high-frequency scenario has become an important topic in finance. A key challenge in high-frequency market forecasting is modeling the dependency structure among stocks and business sectors, with their high dimensionality and the requirement of computational efficiency. As a group of powerful models, neural networks (NNs) have been used to capture the complex structure in many studies. However, most existing applications of NNs only focus on forecasting with daily or monthly data, not with minute-level data that usually contains more noises. In this article, we propose a novel double-layer neural (DNN) network for high-frequency forecasting, with links specially designed to capture dependence structures among stock returns within different business sectors. Various important technical indicators are also included at different layers of the DNN framework. Our model framework allows update over time to achieve the best goodness-of-fit with the most recent data. The model performance is tested based on 100 stocks with the largest capitals from the S8P 500. The results show that the proposed framework outperforms benchmark methods in terms of the prediction accuracy and returns. Our method will help in financial analysis and trading strategy designs.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {11},
numpages = {17},
keywords = {S8P 500, neural networks, High-frequency forecasting}
}

@article{10.1145/3023365,
author = {Lu, Yan and Chau, Michael and Chau, Patrick Y. K.},
title = {Are Sponsored Links Effective? Investigating the Impact of Trust in Search Engine Advertising},
year = {2017},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3023365},
doi = {10.1145/3023365},
abstract = {As information on the Internet grows exponentially, online users primarily rely on search engines (SEs) to locate e-commerce sites for online shopping. To generate revenue while providing free service to users, SE companies offer sponsored link (SL) placements to e-commerce sites that want to appear in the first SE results page. However, the lack of users’ trust in SE advertising indicates that SEs should utilize strategies to project trustworthiness on this mechanism. Despite these insights, the role of users’ trust in the operation of SE advertising is still an unexplored territory. To address this issue, a theoretical model was synthesized from the social psychology literature, the marketing literature, and the trust literature to investigate the factors that may pose impacts on the effectiveness of SE advertising by influencing users’ perception of both cognitive and emotional trust. A laboratory experiment was conducted. The findings document the importance of incorporating emotional components of trust in the study of online communication by showing that emotional dimension of trust is different from and complementary to cognitive trust in facilitating online communication. The findings also provide valuable implications for practitioners to design and provide more effective SLs that can benefit all parties involved.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {12},
numpages = {33},
keywords = {Online advertising effectiveness, cognitive trust, emotional trust, trust in online advertising}
}

@article{10.1145/3041218,
author = {Pika, Anastasiia and Leyer, Michael and Wynn, Moe T. and Fidge, Colin J. and Hofstede, Arthur H. M. Ter and Aalst, Wil M. P. Van Der},
title = {Mining Resource Profiles from Event Logs},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3041218},
doi = {10.1145/3041218},
abstract = {In most business processes, several activities need to be executed by human resources and cannot be fully automated. To evaluate resource performance and identify best practices as well as opportunities for improvement, managers need objective information about resource behaviors. Companies often use information systems to support their processes, and these systems record information about process execution in event logs. We present a framework for analyzing and evaluating resource behavior through mining such event logs. The framework provides (1) a method for extracting descriptive information about resource skills, utilization, preferences, productivity, and collaboration patterns; (2) a method for analyzing relationships between different resource behaviors and outcomes; and (3) a method for evaluating the overall resource productivity, tracking its changes over time, and comparing it to the productivity of other resources. To demonstrate the applicability of our framework, we apply it to analyze employee behavior in an Australian company and evaluate its usefulness by a survey among industry managers.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {1},
numpages = {30},
keywords = {mining resource behavior, evidence-based performance evaluation, event log, Resource profile}
}

@article{10.1145/3057272,
author = {Eftekhari, Saeede and Yaraghi, Niam and Singh, Ranjit and Gopal, Ram D. and Ramesh, R.},
title = {Do Health Information Exchanges Deter Repetition of Medical Services?},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3057272},
doi = {10.1145/3057272},
abstract = {Repetition of medical services by providers is one of the major sources of healthcare costs. The lack of access to previous medical information on a patient at the point of care often leads a physician to perform medical procedures that have already been done. Multiple healthcare initiatives and legislation at both the federal and state levels have mandated Health Information Exchange (HIE) systems to address this problem. This study aims to assess the extent to which HIE could reduce these repetitions, using data from Centers for Medicare 8 Medicaid Services and a regional HIE organization. A 2-Stage Least Square model is developed to predict the impact of HIE on repetitions of two classes of procedures: diagnostic and therapeutic. The first stage is a predictive analytic model that estimates the duration of tenure of each HIE member-practice. Based on these estimates, the second stage predicts the effect of providers’ HIE tenure on their repetition of medical services. The model incorporates moderating effects of a federal quality assurance program and the complexity of medical procedures with a set of control variables. Our analyses show that a practice's tenure with HIE significantly lowers the repetition of therapeutic medical procedures, while diagnostic procedures are not impacted. The medical reasons for the effects observed in each class of procedures are discussed. The results will inform healthcare policymakers and provide insights on the business models of HIE platforms.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {2},
numpages = {27},
keywords = {healthcare cost, 2sls model, therapeutic procedures, Health information exchanges, repetition of medical procedures, healthcare quality, diagnostic procedures}
}

@article{10.1145/3057271,
author = {Kakar, Adarsh Kumar},
title = {Investigating the Relationships Between the Use Contexts, User Perceived Values, and Loyalty to a Software Product},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3057271},
doi = {10.1145/3057271},
abstract = {In this study, we propose that software products provide three types of value—utilitarian, hedonic, and social—that impact user loyalty. Although the Technology Acceptance Model (TAM) has focused on the user impacts of utilitarian and hedonic values provided by utilitarian and hedonic software products on system use, the impact of social value provided by the software products in general have been largely ignored. The results of a longitudinal study with actual users of three types of software products show that all three types of software products—utilitarian (Producteev), hedonic (Kerbal), and social (Facebook)—provide significant but varying degrees of all three types of values. Further, the value derived by the users’ primary use context moderated the impact of the secondary values provided by the software product to the users on their loyalty for the product.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {3},
numpages = {23},
keywords = {Hedonic value, user loyalty, social value, utilitarian value}
}

@article{10.1145/3083726,
author = {Taghavi, Atefeh and Woo, Carson},
title = {The Role Clarity Framework to Improve Requirements Gathering},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2–3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3083726},
doi = {10.1145/3083726},
abstract = {Incorrect and incomplete requirements have been reported as two of the top reasons for information systems (IS) project failures. In order to address these concerns, several IS analysis and design studies have focused on understanding the business needs and organizational factors prior to specifying the requirements. In this research, we add to the existing incremental solutions, such as the work system method and goal-oriented requirements engineering, by proposing the Role Clarity Framework drawn from the theories of “role dynamics” and “goal setting and task performance” in organization studies. The Role Clarity Framework consists of three main concepts related to any organizational role: expectations, activities, and consequences. Based on the interactions among different roles, this framework demonstrates how the business goals and activities of each role, as played out by IS users, are formed and/or changed in the organization. Finally, the Role Clarity Framework helps IS analysts to improve their communication with users and anticipate changes in their requirements, thus improving the gathering of requirements for IS design.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {9},
numpages = {16},
keywords = {role clarity, communication between analysts and users, Organizational role, business activities, Role Clarity Framework, principles, changes in requirements}
}

@article{10.1145/3070684,
author = {Mannino, Michael and Fredrickson, Joel and Banaei-Kashani, Farnoush and Linck, Iris and Raghda, Raghda Alqurashi},
title = {Development and Evaluation of a Similarity Measure for Medical Event Sequences},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2–3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3070684},
doi = {10.1145/3070684},
abstract = {We develop a similarity measure for medical event sequences (MESs) and empirically evaluate it using U.S. Medicare claims data. Existing similarity measures do not use unique characteristics of MESs and have never been evaluated on real MESs. Our similarity measure, the Optimal Temporal Common Subsequence for Medical Event Sequences (OTCS-MES), provides a matching component that integrates event prevalence, event duplication, and hierarchical coding, important elements of MESs. The OTCS-MES also uses normalization to mitigate the impact of heavy positive skew of matching events and compact distribution of event prevalence. We empirically evaluate the OTCS-MES measure against two other measures specifically designed for MESs, the original OTCS and Artemis, a measure incorporating event alignment. Our evaluation uses two substantial data sets of Medicare claims data containing inpatient and outpatient sequences with different medical event coding. We find a small overlap in nearest neighbors among the three similarity measures, demonstrating the superior design of the OTCS-MES with its emphasis on unique aspects of MESs. The evaluation also provides evidence about the impact of component weights, neighborhood size, and sequence length.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {8},
numpages = {26},
keywords = {similarity measure, OTCS-MES, Medical event sequence, nearest neighbor search}
}

@article{10.1145/3052931,
author = {Bauman, Konstantin and Tuzhilin, Alexander and Zaczynski, Ryan},
title = {Using Social Sensors for Detecting Emergency Events: A Case of Power Outages in the Electrical Utility Industry},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2–3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3052931},
doi = {10.1145/3052931},
abstract = {This article presents a novel approach to detecting emergency events, such as power outages, that utilizes social media users as “social sensors” for virtual detection of such events. The proposed new method is based on the analysis of the Twitter data that leads to the detection of Twitter discussions about these emergency events. The method described in the article was implemented and deployed by one of the vendors in the context of detecting power outages as a part of their comprehensive social engagement platform. It was also field tested on Twitter users in an industrial setting and performed well during these tests.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {7},
numpages = {20},
keywords = {event detection, power outages, tweets, social sensors, Social media}
}

@article{10.1145/3046684,
author = {Deng, Shuyuan and Sinha, Atish P. and Zhao, Huimin},
title = {Resolving Ambiguity in Sentiment Classification: The Role of Dependency Features},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2–3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3046684},
doi = {10.1145/3046684},
abstract = {Sentiment analysis has become popular in business intelligence and analytics applications due to the great need for learning insights from the vast amounts of user generated content on the Internet. One major challenge of sentiment analysis, like most text classification tasks, is finding structures from unstructured texts. Existing sentiment analysis techniques employ the supervised learning approach and the lexicon scoring approach, both of which largely rely on the representation of a document as a collection of words and phrases. The semantic ambiguity (i.e., polysemy) of single words and the sparsity of phrases negatively affect the robustness of sentiment analysis, especially in the context of short social media texts. In this study, we propose to represent texts using dependency features. We test the effectiveness of dependency features in supervised sentiment classification. We compare our method with the current standard practice using a labeled data set containing 170,874 microblogging messages. The combination of unigram features and dependency features significantly outperformed other popular types of features.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {4},
numpages = {13},
keywords = {Sentiment analysis, feature extraction, supervised learning, text mining, dependency}
}

@article{10.1145/3055534,
author = {Al-Ramahi, Mohammad A. and Liu, Jun and El-Gayar, Omar F.},
title = {Discovering Design Principles for Health Behavioral Change Support Systems: A Text Mining Approach},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2–3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3055534},
doi = {10.1145/3055534},
abstract = {Behavioral Change Support Systems (BCSSs) aim to change users’ behavior and lifestyle. These systems have been gaining popularity with the proliferation of wearable devices and recent advances in mobile technologies. In this article, we extend the existing literature by discovering design principles for health BCSSs based on a systematic analysis of users’ feedback. Using mobile diabetes applications as an example of Health BCSSs, we use topic modeling to discover design principles from online user reviews. We demonstrate the importance of the design principles through analyzing their existence in users’ complaints. Overall, the results highlight the necessity of going beyond the techno-centric approach used in current practice and incorporating the social and organizational features into persuasive systems design, as well as integrating with medical devices and other systems in their usage context.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {5},
numpages = {24},
keywords = {topic mining, Mobile diabetes apps, online user reviews, Latent Dirichlet Allocation (LDA)}
}

@article{10.1145/3086188,
author = {Mukherjee, Anik and Sundarraj, R. P. and Dutta, Kaushik},
title = {Apriori Rule--Based In-App Ad Selection Online Algorithm for Improving Supply-Side Platform Revenues},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2–3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3086188},
doi = {10.1145/3086188},
abstract = {Today, smartphone-based in-app advertisement forms a substantial portion of the online advertising market. In-app publishers go through ad-space aggregators known as Supply-Side Platforms (SSPs), who, in turn, act as intermediaries for ad-agency aggregators known as demand-side platforms. The SSPs face the twin issue of making ad placement decisions within an order of milliseconds, even though their revenue streams can be optimized only by a careful selection of ads that elicit appropriate user responses regarding impressions, clicks, and conversions. This article considers the SSP's perspective and presents an online algorithm that balances these two issues. Our experimental results indicate that the decision-making time generally ranges between 20 ms and 50 ms and accuracy from 1% to 10%. Further, we conduct statistical analysis comparing the theoretical complexity of the online algorithm with its empirical performance. Empirically, we observe that the time is directly proportional to the number of incoming ads and the number of online rules.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {10},
numpages = {28},
keywords = {Supply-side platform, demand-side platform, optimization, rule generation, Apriori algorithm, integer linear programming, online algorithm}
}

@article{10.1145/3057273,
author = {Sun, Can and Ji, Yonghua and Kolfal, Bora and Patterson, Ray},
title = {Business-to-Consumer Platform Strategy: How Vendor Certification Changes Platform and Seller Incentives},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2–3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3057273},
doi = {10.1145/3057273},
abstract = {We build an economic model to study the problem of offering a new, high-certainty channel on an existing business-to-consumer platform such as Taobao and eBay. On this new channel, the platform owner exerts effort to reduce the uncertainty of service quality. Sellers can either sell through the existing low-certainty channel or go through additional screening to sell on this new channel. We model the problem as a Bertrand competition game where sellers compete on price and exert effort to provide better service to consumers. In this game, we consider a reputation spillover effect that refers to the impact of the high-certainty channel on the perceived service quality in the low-certainty channel. Counter-intuitively, we find that low-certainty channel demand will decrease as the reputation spillover effect increases, in the case of low inter-channel competition. Also, low-certainty channel demand increases as the quality uncertainty increases, in the case of intense inter-channel competition. Furthermore, the platform owner should offer a new high-certainty channel when (i) the perceived quality for this channel is sufficiently high, (ii) sellers in this channel are able to efficiently provide quality service, (iii) consumers in this channel are not so sensitive to the quality uncertainty, or (iv) the reputation spillover effect is high. In the one-channel case, the incentives of the platform owner and sellers are aligned for all model parameters. However, this is not the case for the two-channel solution, and our model reveals where tensions will arise between parties.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {6},
numpages = {42},
keywords = {online shopping, vetted sellers, E-commerce infrastructure, reputation spillover, incentive alignment, B2C channel competition}
}

@article{10.1145/3108899,
author = {Bhattacharjee, Sudip and Jacob, Varghese and Jiang, Zhengrui (Jeffrey) and Kumar, Subodha},
title = {Introduction to WITS 2015 Special Issue in TMIS},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2–3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3108899},
doi = {10.1145/3108899},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {4e},
numpages = {4}
}

@article{10.1145/3110939,
author = {Cazier, Joseph and Shao, Benjamin and Louis, Robert St.},
title = {Value Congruence, Trust, and Their Effects on Purchase Intention and Reservation Price},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3110939},
doi = {10.1145/3110939},
abstract = {We study the roles of value congruence and trust in increasing online shoppers’ intention to purchase goods and their reservation prices for these goods. Hypotheses are developed and a controlled experiment is conducted to measure subjects’ value congruence with and their trust in online sellers with disparate values, along with their purchase intention and willingness to pay price premiums. Using social exchange theory, we find that, for business-to-consumer (B2C) e-commerce, value congruence increases consumer online trust, and both value congruence and online trust have direct effects on purchase intention and reservation prices. In particular, in the positive value congruence vs. value neutral case, trust has a greater effect than value congruence on purchase intention, but value congruence has a greater effect than trust on reservation price. These findings suggest that trust is essential to a consumer's intention to purchase online but value congruence can induce price premiums from potential buyers for online sellers. This implies that trust is essential to B2C e-commerce, but value congruence can be a more effective instrument for online sellers to achieve competitive advantage through value-based differentiation in the virtual marketplace.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {13},
numpages = {28},
keywords = {online market, price premium, value congruence, trust, purchase intention, E-commerce, reservation price}
}

@article{10.1145/3086449,
author = {Ha, Tuan Minh and Samejima, Masaki and Komoda, Norihisa},
title = {Power and Performance Estimation for Fine-Grained Server Power Capping via Controlling Heterogeneous Applications},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3086449},
doi = {10.1145/3086449},
abstract = {Power capping is a method to save power consumption of servers by limiting performance of the servers. Although users frequently run applications on different virtual machines (VMs) for keeping their performance and having them isolated from the other applications, power capping may degrade performance of all the applications running on the server. We present fine-grained power capping by limiting performance of each application individually. For keeping performance defined in Quality of Service (QoS) requirements, it is important to estimate applications’ performance and power consumption after the fine-grained power capping is applied. We propose the estimation method of physical CPU usage when limiting virtual CPU usage of applications on VMs. On servers where multiple VMs run, VM’s usage of physical CPU is interrupted by the other VMs, and a hypervisor uses physical CPU to control VMs. These VMs’ and hypervisor’s behaviors make it difficult to estimate performance and power consumption by straightforward methods, such as linear regression and polynomial regression. The proposed method uses Piecewise Linear Regression to estimate physical CPU usage by assuming that VM’s access to physical CPU is not interrupted by the other VMs. Then we estimate how much physical CPU usage is reduced by the interruption. Because physical CPU usage is not stable soon after limiting CPU usage, the proposed method estimates a convergence value of CPU usage after many interruptions are repeated.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {11},
numpages = {19},
keywords = {power consumption, response time, Server power capping, piecewise linear regression}
}

@article{10.1145/3086308,
author = {Basole, Rahul C. and Major, Timothy and Srinivasan, Arjun},
title = {Understanding Alliance Portfolios Using Visual Analytics},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3086308},
doi = {10.1145/3086308},
abstract = {In an increasingly global and competitive business landscape, firms must collaborate and partner with others to ensure survival, growth, and innovation. Understanding the evolutionary composition of a firm’s relationship portfolio and the underlying formation strategy is a difficult task given the multidimensional, temporal, and geospatial nature of the data. In collaboration with senior executives, we iteratively determine core design requirements and then design and implement an interactive visualization system that enables decision makers to gain both systemic (macro) and detailed (micro) insights into a firm’s alliance activities and discover patterns of multidimensional relationship formation. Our system provides both sequential and temporal representation modes, a rich set of additive cross-linked filters, the ability to stack multiple alliance portfolios, and a dynamically updated activity state model visualization to inform decision makers of past and likely future relationship moves. We illustrate our tool with examples of alliance activities of firms listed on the S8P 500. A controlled experiment and real-world evaluation with practitioners and researchers reveals significant evidence of the value of our visual analytic tool. Our design study contributes to design science by addressing a known problem (i.e., alliance portfolio analysis) with a novel solution (interactive, pixel-based multivariate visualization) and to the rapidly emerging area of data-driven visual decision support in corporate strategy contexts. We conclude with implications and future research opportunities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {12},
numpages = {21},
keywords = {visual analytics, strategy decision support, Alliances}
}

@article{10.1145/3131780,
author = {Lukyanenko, Roman and Samuel, Binny M.},
title = {Are All Classes Created Equal? Increasing Precision of Conceptual Modeling Grammars},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3131780},
doi = {10.1145/3131780},
abstract = {Recent decade has seen a dramatic change in the information systems landscape that alters the ways we design and interact with information technologies, including such developments as the rise of business analytics, user-generated content, and NoSQL databases, to name just a few. These changes challenge conceptual modeling research to offer innovative solutions tailored to these environments. Conceptual models typically represent classes (categories, kinds) of objects rather than concrete specific objects, making the class construct a critical medium for capturing domain semantics. While representation of classes may differ between grammars, a common design assumption is what we term different semantics same syntax (D3S). Under D3S, all classes are depicted using the same syntactic symbols. Following recent findings in psychology, we introduce a novel assumption semantics-contingent syntax (SCS) whereby syntactic representations of classes in conceptual models may differ based on their semantic meaning. We propose a core SCS design principle and five guidelines pertinent for conceptual modeling. We believe SCS carries profound implications for theory and practice of conceptual modeling as it seeks to better support modern information environments.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {14},
numpages = {15},
keywords = {Conceptual modeling, database design}
}

@article{10.1145/3131781,
author = {Xiao, Keli and Liu, Qi and Liu, Chuanren and Xiong, Hui},
title = {Price Shock Detection With an Influence-Based Model of Social Attention},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3131781},
doi = {10.1145/3131781},
abstract = {There has been increasing interest in exploring the impact of human behavior on financial market dynamics. One of the important related questions is whether attention from society can lead to significant stock price movements or even abnormal returns. To answer the question, we develop a new measurement of social attention, named periodic cumulative degree of social attention, by simultaneously considering the individual influence and the information propagation in social networks. Based on the vast social network data, we evaluate the new attention measurement by testing its significance in explaining future abnormal returns. In addition, we test the forecasting ability of social attention for stock price shocks, defined by the cumulative abnormal returns. Our results provide significant evidence to support the intercorrelated relationship between the social attention and future abnormal returns. The outperformance of the new approach in predicting price shocks is also confirmed by comparison with several benchmark methods.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {2},
numpages = {21},
keywords = {social attention, influence propagation, Social network, price shock, abnormal return, the Chinese stock market}
}

@article{10.1145/3131782,
author = {Li, Zhepeng (Lionel) and Fang, Xiao and Sheng, Olivia R. Liu},
title = {A Survey of Link Recommendation for Social Networks: Methods, Theoretical Foundations, and Future Research Directions},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3131782},
doi = {10.1145/3131782},
abstract = {Link recommendation has attracted significant attention from both industry practitioners and academic researchers. In industry, link recommendation has become a standard and most important feature in online social networks, prominent examples of which include “People You May Know” on LinkedIn and “You May Know” on Google+. In academia, link recommendation has been and remains a highly active research area. This article surveys state-of-the-art link recommendation methods, which can be broadly categorized into learning-based methods and proximity-based methods. We further identify social and economic theories, such as social interaction theory, that underlie these methods and explain from a theoretical perspective why a link recommendation method works. Finally, we propose to extend link recommendation research in several directions that include utility-based link recommendation, diversity of link recommendation, link recommendation from incomplete data, and experimental study of link recommendation.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {1},
numpages = {26},
keywords = {Link Recommendation, Network Formation, Social Network}
}

@article{10.1145/3159445,
author = {Tuarob, Suppawong and Strong, Ray and Chandra, Anca and Tucker, Conrad S.},
title = {Discovering Discontinuity in Big Financial Transaction Data},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3159445},
doi = {10.1145/3159445},
abstract = {Business transactions are typically recorded in the company ledger. The primary purpose of such financial information is to accompany a monthly or quarterly report for executives to make sound business decisions and strategies for the next business period. These business strategies often result in transitions that cause underlying infrastructures and components to change, including alteration in the nomenclature system of the business components. As a result, a transaction stream of an affected component would be replaced by another stream with a different component name, resulting in discontinuity of a financial stream of the same component. Recently, advancement in large-scale data mining technologies has enabled a set of critical applications to utilize knowledge extracted from a vast amount of existing data that would otherwise have been unused or underutilized. In financial and services computing domains, recent studies have illustrated that historical financial data could be used to predict future revenues and profits, optimizing costs, among other potential applications. These prediction models rely on long-term availability of the historical data that traces back for multiple years. However, the discontinuity of the financial transaction stream associated with a business component has limited the learning capability of the prediction models. In this article, we propose a set of machine learning–based algorithms to automatically discover component name replacements, using information available in general ledger databases. The algorithms are designed to be scalable for handling massive data points, especially in large companies. Furthermore, the proposed algorithms are generalizable to other domains whose data is time series and shares the same nature as the financial data available in business ledgers. A case study of real-world IBM service delivery retrieved from four different geographical regions is used to validate the efficacy of the proposed methodology.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {3},
numpages = {26},
keywords = {Services delivery, machine learning, classification, name replacement discovery}
}

@article{10.1145/3183367,
author = {Mendling, Jan and Weber, Ingo and Aalst, Wil Van Der and Brocke, Jan Vom and Cabanillas, Cristina and Daniel, Florian and Debois, S\o{}ren and Ciccio, Claudio Di and Dumas, Marlon and Dustdar, Schahram and Gal, Avigdor and Garc\'{\i}a-Ba\~{n}uelos, Luciano and Governatori, Guido and Hull, Richard and Rosa, Marcello La and Leopold, Henrik and Leymann, Frank and Recker, Jan and Reichert, Manfred and Reijers, Hajo A. and Rinderle-Ma, Stefanie and Solti, Andreas and Rosemann, Michael and Schulte, Stefan and Singh, Munindar P. and Slaats, Tijs and Staples, Mark and Weber, Barbara and Weidlich, Matthias and Weske, Mathias and Xu, Xiwei and Zhu, Liming},
title = {Blockchains for Business Process Management - Challenges and Opportunities},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3183367},
doi = {10.1145/3183367},
abstract = {Blockchain technology offers a sizable promise to rethink the way interorganizational business processes are managed because of its potential to realize execution without a central party serving as a single point of trust (and failure). To stimulate research on this promise and the limits thereof, in this article, we outline the challenges and opportunities of blockchain for business process management (BPM). We first reflect how blockchains could be used in the context of the established BPM lifecycle and second how they might become relevant beyond. We conclude our discourse with a summary of seven research directions for investigating the application of blockchain technology in the context of BPM.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {4},
numpages = {16},
keywords = {business process management, research challenges, Blockchain}
}

@article{10.1145/3185047,
author = {Basole, Rahul C. and Srinivasan, Arjun and Park, Hyunwoo and Patel, Shiv},
title = {Ecoxight: Discovery, Exploration, and Analysis of Business Ecosystems Using Interactive Visualization},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3185047},
doi = {10.1145/3185047},
abstract = {The term ecosystem is used pervasively in industry, government, and academia to describe the complex, dynamic, hyperconnected nature of many social, economic, and technical systems that exist today. Ecosystems are characterized by a large, dynamic, and heterogeneous set of geospatially distributed entities that are interconnected through various types of relationships. This study describes the design and development of ecoxight, a Web-based visualization platform that provides multiple coordinated views of multipartite, multiattribute, dynamic, and geospatial ecosystem data with novel and rich interaction capabilities to augment decision makers ecosystem intelligence. The design of ecoxight was informed by an extensive multiphase field study of executives. The ecoxight platform not only provides capabilities to interactively explore and make sense of ecosystems but also provides rich visual construction capabilities to help decision makers align their mental model. We demonstrate the usability, utility, and value of our system using multiple evaluation studies with practitioners using socially curated data on the emerging application programming interface ecosystem. We report on our findings and conclude with research implications. Collectively, our study contributes to design science research at the intersection of information systems and strategy and the rapidly emerging field of visual enterprise analytics.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {6},
numpages = {26},
keywords = {Network visualization, multipartite graph, geospatial data, temporal network}
}

@article{10.1145/3210548,
author = {De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur},
title = {Machine Learning for the Developing World},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3210548},
doi = {10.1145/3210548},
abstract = {Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {9},
numpages = {14},
keywords = {developing countries, Global development}
}

@article{10.1145/3205849,
author = {Fan, Xiangyu and Niu, Xi},
title = {Implementing and Evaluating Serendipity in Delivering Personalized Health Information},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3205849},
doi = {10.1145/3205849},
abstract = {Serendipity has been recognized to have the potential of enhancing unexpected information discovery. This study shows that decomposing the concept of serendipity into unexpectedness and interest is a useful way for implementing this concept. Experts’ domain knowledge helps in providing serendipitous recommendation, which can be further improved by adaptively incorporating users’ real-time feedback.This research also conducts an empirical user-study to analyze the influence of serendipity in a health news delivery context. A personalized filtering system named MedSDFilter was developed, on top of which serendipitous recommendation was implemented using three approaches: random, static-knowledge-based, and adaptive-knowledge-based models. The three different models were compared. The results indicate that the adaptive-knowledge-based method has the highest ability in helping people discover unexpected and interesting contents. The insights of the research will make researchers and practitioners rethink the way in which search engines and recommender systems operate to address the challenges of discovering unexpected and interesting information. The outcome will have implications for empowering ordinary people with more chances of bumping into beneficial information.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {7},
numpages = {19},
keywords = {medical information delivery, Serendipity, personalized information filtering}
}

@article{10.1145/3185045,
author = {Zimbra, David and Abbasi, Ahmed and Zeng, Daniel and Chen, Hsinchun},
title = {The State-of-the-Art in Twitter Sentiment Analysis: A Review and Benchmark Evaluation},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3185045},
doi = {10.1145/3185045},
abstract = {Twitter has emerged as a major social media platform and generated great interest from sentiment analysis researchers. Despite this attention, state-of-the-art Twitter sentiment analysis approaches perform relatively poorly with reported classification accuracies often below 70%, adversely impacting applications of the derived sentiment information. In this research, we investigate the unique challenges presented by Twitter sentiment analysis and review the literature to determine how the devised approaches have addressed these challenges. To assess the state-of-the-art in Twitter sentiment analysis, we conduct a benchmark evaluation of 28 top academic and commercial systems in tweet sentiment classification across five distinctive data sets. We perform an error analysis to uncover the causes of commonly occurring classification errors. To further the evaluation, we apply select systems in an event detection case study. Finally, we summarize the key trends and takeaways from the review and benchmark evaluation and provide suggestions to guide the design of the next generation of approaches.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {5},
numpages = {29},
keywords = {text mining, opinion mining, Sentiment analysis, natural language processing, twitter, social media, benchmark evaluation}
}

@article{10.1145/3185046,
author = {Purao, Sandeep and Bolloju, Narasimha and Tan, Chuan-Hoo},
title = {A Modeling Language for Conceptual Design of Systems Integration Solutions},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3185046},
doi = {10.1145/3185046},
abstract = {Systems integration—connecting software systems for cross-functional work—is a significant concern in many large organizations, which continue to maintain hundreds, if not thousands, of independently evolving software systems. Current approaches in this space remain ad hoc, and closely tied to technology platforms. Following a design science approach, and via multiple design-evaluate cycles, we develop Systems Integration Requirements Engineering Modeling Language (SIRE-ML) to address this problem. SIRE-ML builds on the foundation of coordination theory, and incorporates important semantic information about the systems integration domain. The article develops constructs in SIRE-ML, and a merge algorithm that allows both functional managers and integration professionals to contribute to building a systems integration solution. Integration models built with SIRE-ML provide benefits such as ensuring coverage and minimizing ambiguity, and can be used to drive implementation with different platforms such as middleware, services, and distributed objects. We evaluate SIRE-ML for ontological expressiveness and report findings about applicability check with an expert panel. The article discusses implications for future research such as tool building and empirical evaluation, as well as implications for practice.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {8},
numpages = {25},
keywords = {SIRE-ML, Design science, conceptual modeling}
}

@article{10.1145/3234465,
author = {Zhu, Chen and Zhu, Hengshu and Xiong, Hui and Ma, Chao and Xie, Fang and Ding, Pengliang and Li, Pan},
title = {Person-Job Fit: Adapting the Right Talent for the Right Job with Joint Representation Learning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3234465},
doi = {10.1145/3234465},
abstract = {Person-Job Fit is the process of matching the right talent for the right job by identifying talent competencies that are required for the job. While many qualitative efforts have been made in related fields, it still lacks quantitative ways of measuring talent competencies as well as the job’s talent requirements. To this end, in this article, we propose a novel end-to-end data-driven model based on a Convolutional Neural Network (CNN), namely, the Person-Job Fit Neural Network (PJFNN), for matching a talent qualification to the requirements of a job. To be specific, PJFNN is a bipartite neural network that can effectively learn the joint representation of Person-Job fitness from historical job applications. In particular, due to the design of a hierarchical representation structure, PJFNN can not only estimate whether a candidate fits a job but also identify which specific requirement items in the job posting are satisfied by the candidate by measuring the distances between corresponding latent representations. Finally, the extensive experiments on a large-scale real-world dataset clearly validate the performance of PJFNN in terms of Person-Job Fit prediction. Also, we provide effective data visualization to show some job and talent benchmark insights obtained by PJFNN.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {12},
numpages = {17},
keywords = {joint representation learning, Recruitment analysis}
}

@article{10.1145/3230713,
author = {Russo, Daniel and Ciancarini, Paolo and Falasconi, Tommaso and Tomasi, Massimo},
title = {A Meta-Model for Information Systems Quality: A Mixed Study of the Financial Sector},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3230713},
doi = {10.1145/3230713},
abstract = {Information Systems Quality (ISQ) is a critical source of competitive advantages for organizations. In a scenario of increasing competition on digital services, ISQ is a competitive differentiation asset. In this regard, managing, maintaining, and evolving IT infrastructures have become a primary concern of organizations. Thus, a technical perspective on ISQ provides useful guidance to meet current challenges. The financial sector is paradigmatic, since it is a traditional business, with highly complex business-critical legacy systems, facing a tremendous change due to market and regulation drivers. We carried out a Mixed-Methods study, performing a Delphi-like study on the financial sector. We developed a specific research framework to pursue this vertical study. Data were collected in four phases starting with a high-level randomly stratified panel of 13 senior managers and then a target panel of 124 carefully selected and well-informed domain experts. We have identified and dealt with several quality factors; they were discussed in a comprehensive model inspired by the ISO 25010, 42010, and 12207 standards, corresponding to software quality, software architecture, and software process, respectively. Our results suggest that the relationship among quality, architecture, and process is a valuable technical perspective to explain the quality of an information system. Thus, we introduce and illustrate a novel meta-model, named SQuAP (Software Quality, Architecture, Process), which is intended to give a comprehensive picture of ISQ by abstracting and connecting detailed individual ISO models.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {11},
numpages = {38},
keywords = {Information systems quality, software quality, mixed methods, delphi study, software architecture, management information systems, software process}
}

@article{10.1145/3230712,
author = {Ketter, Wolfgang and Collins, John and Saar-Tsechansky, Maytal and Marom, Ori},
title = {Information Systems for a Smart Electricity Grid: Emerging Challenges and Opportunities},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3230712},
doi = {10.1145/3230712},
abstract = {The drive for sustainability as evidenced by the Paris Accords is forcing a radical re-examination of the way electricity is produced, managed, and consumed. Research on sustainable smart electricity markets is facilitating the emergence of sustainable energy systems and a revolution in the efficiency and reliability of electricity consumption, production, and distribution. Traditional electricity grids and markets are being disrupted by a range of forces, including the rise of weather-dependent and distributed renewable sources, growing consumer involvement in managing their power consumption and production, and the electrification of transport. These changes will likely bring about complex and dynamic smart electricity markets that rely on analysis of information to inform stakeholders, and on effective integration of stakeholders’ actions. We outline a research agenda on how advances in information-intensive processes are fundamental for facilitating these transformations, describe the roles that such processes will play, and discuss Information Systems research challenges necessary to achieve these goals. These challenges span public policy, privacy, and security; market mechanisms; and data-driven decision support. The diverse challenges we outline also underscore that the diverse IS research perspective is instrumental for addressing the complexity and interdisciplinary nature of this research.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {10},
numpages = {22},
keywords = {trading agent competition, demand response, energy policy, Enabling technologies, sustainability, green IS, price forecasting, dynamic pricing, smart electricity markets, software agents, smart grid, supply chain, energy informatics}
}

@article{10.1145/3273932,
author = {Delano, John D. and Jain, Hemant K. and Sinha, Atish P.},
title = {System Design through the Exploration of Contemporary Web Services},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3273932},
doi = {10.1145/3273932},
abstract = {In this article, we develop a Contemporary Web Service (CWS) repository of system designs, which are encoded as metadata of contemporary web services. We examine if this CWS repository serves as an effective design tool for initial CWS design and as an effective support tool for business users and analysts working together on system design. The CWS repository reduces the cognitive load of both the analyst and the business user as they jointly explore the CWS repository of system designs. It supports an evolutionary approach to system design through rapid selection of appropriate CWS metadata. To accomplish that, we introduce several new design characteristics for the CWS repository. The evaluation results demonstrate that the CWS repository is an effective tool for supporting designers during initial service design, as well as for supporting business users and analysts during system design.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {13},
numpages = {29},
keywords = {Design science research, system design, contemporary web services, repository}
}

@article{10.1145/3309706,
author = {Kratzwald, Bernhard and Feuerriegel, Stefan},
title = {Putting Question-Answering Systems into Practice: Transfer Learning for Efficient Domain Customization},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3309706},
doi = {10.1145/3309706},
abstract = {Traditional information retrieval (such as that offered by web search engines) impedes users with information overload from extensive result pages and the need to manually locate the desired information therein. Conversely, question-answering systems change how humans interact with information systems: users can now ask specific questions and obtain a tailored answer—both conveniently in natural language. Despite obvious benefits, their use is often limited to an academic context, largely because of expensive domain customizations, which means that the performance in domain-specific applications often fails to meet expectations. This article proposes cost-efficient remedies: (i) we leverage metadata through a filtering mechanism, which increases the precision of document retrieval, and (ii) we develop a novel fuse-and-oversample approach for transfer learning to improve the performance of answer extraction. Here, knowledge is inductively transferred from related, yet different, tasks to the domain-specific application, while accounting for potential differences in the sample sizes across both tasks. The resulting performance is demonstrated with actual use cases from a finance company and the film industry, where fewer than 400 question-answer pairs had to be annotated to yield significant performance gains. As a direct implication to management, this presents a promising path to better leveraging of knowledge stored in information systems.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {15},
numpages = {20},
keywords = {deep learning, Question answering, machine comprehension, domain customization, transfer learning}
}

@article{10.1145/3309707,
author = {Yu, Shuo and Zhu, Hongyi and Jiang, Shan and Zhang, Yong and Xing, Chunxiao and Chen, Hsinchun},
title = {Emoticon Analysis for Chinese Social Media and E-Commerce: The AZEmo System},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3309707},
doi = {10.1145/3309707},
abstract = {This article presents a novel system, AZEmo, which extracts and classifies emoticons from the ever-growing critical Chinese social media and E-commerce. An emoticon is a meta-communicative pictorial representation of facial expressions, which helps to describe the sender’s emotional state. To complement non-verbal communication, emoticons are frequently used in social media websites. However, limited research has been done to effectively analyze the affects of emoticons in a Chinese context. In this study, we developed an emoticon analysis system to extract emoticons from Chinese text and classify them into one of seven affect categories. The system is based on a kinesics model that divides emoticons into semantic areas (eyes, mouths, etc.), with improvements for adaptation in the Chinese context. Machine-learning methods were developed based on feature vector extraction of emoticons. Empirical tests were conducted to evaluate the effectiveness of the proposed system in extracting and classifying emoticons, based on corpora from a video sharing website and an E-commerce website. Results showed the effectiveness of the system in detecting and extracting emoticons from text and in interpreting the affects conveyed by emoticons.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {16},
numpages = {22},
keywords = {Affect analysis, Chinese Internet, social media, emoticon}
}

@article{10.1145/3309704,
author = {Lo, Kar Kei and Chau, Michael},
title = {A Penny Is Worth a Thousand? Investigating the Relationship Between Social Media and Penny Stocks},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3309704},
doi = {10.1145/3309704},
abstract = {Increasingly more investors are seeking information from social media to help make investment decisions. Considering that information on penny stocks is often less reported in traditional media, investors may rely more on social media to obtain such information for investment advice. Although previous research has shown that stock opinions in traditional media is a possible predictor of stock returns, no previous research has considered the effect of the stock opinions in social media on these stocks in terms of future stock performance and the moderation effect of penny stocks. In this research, we studied the relationship between social media and the financial performance of penny stocks. We used the net proportion of positive words in stock articles in social media to help predict the future stock performance for penny stocks. The moderation effect of penny stocks on the net fraction of positive words was found to be significant in short terms, revealing a stronger relationship between social media and stock performance at lower price and market capitalization (MC) levels. Based on the findings, we proposed simple strategies utilizing social media and our measure. The results of our applications will be of interest to individual and institutional investors, shareholders, and regulators.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {14},
numpages = {35},
keywords = {Text analytics, stock opinions, social media, penny stocks}
}

@article{10.1145/3318212,
author = {Xiong, Hu and Wang, Yi and Li, Wenchao and Chen, Chien-Ming},
title = {Flexible, Efficient, and Secure Access Delegation in Cloud Computing},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3318212},
doi = {10.1145/3318212},
abstract = {The convenience of the cloud-assisted Internet of Things has led to the need for improved protections for the large volumes of data collected from devices around the world and stored on cloud-based servers. Proxy re-encryption (PRE) has been presented as a suitable mechanism for secure transmission and sharing of files within the cloud. However, existing PRE schemes do not support unidirectional data transformation, fine-grained controls, multiple hops, and identity-based encryption simultaneously. To solve these problems, we propose a unidirectional multi-hop identity based-conditional PRE scheme that meets all of the above requirements. Our proposal has the additional benefits of a constant ciphertext size, non-interactivity, and collusion resistance. We also prove that our scheme is secure against adaptive identity chosen-ciphertext attacks in the standard model.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {2},
numpages = {20},
keywords = {conditional proxy re-encryption, unidirectionality, identity-based proxy re-encryption, Internet of things, multi-usability}
}

@article{10.1145/3309708,
author = {Hartono, Edward and Holsapple, Clyde W.},
title = {Website Visual Design Qualities: A Threefold Framework},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3309708},
doi = {10.1145/3309708},
abstract = {The present study aims to contribute to the information systems (IS) literature by developing a new theoretical perspective that integrates three dimensions of artifact visual design quality—namely aesthetic, functional, and symbolic dimensions—in the investigation of website visual design qualities that influence visitors’ attitudes and behaviors. Results suggest that website aesthetic, functional, and symbolic qualities positively influence intention to use the website and positive word of mouth and that website aesthetic quality positively influences website functional and symbolic qualities. Results also demonstrate that functional and symbolic qualities mediate the relationships between aesthetic quality and intention to use and positive word of mouth.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {1},
numpages = {21},
keywords = {Website design, functional, symbolic, intention, aesthetic, word of mouth}
}

@article{10.1145/3314948,
author = {Mohammadi, Majid and Hofman, Wout and Tan, Yao-Hua},
title = {Simulated Annealing-Based Ontology Matching},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3314948},
doi = {10.1145/3314948},
abstract = {Ontology alignment is a fundamental task to reconcile the heterogeneity among various information systems using distinct information sources. The evolutionary algorithms (EAs) have been already considered as the primary strategy to develop an ontology alignment system. However, such systems have two significant drawbacks: they either need a ground truth that is often unavailable, or they utilize the population-based EAs in a way that they require massive computation and memory. This article presents a new ontology alignment system, called SANOM, which uses the well-known simulated annealing as the principal technique to find the mappings between two given ontologies while no ground truth is available. In contrast to population-based EAs, the simulated annealing need not generate populations, which makes it significantly swift and memory-efficient for the ontology alignment problem. This article models the ontology alignment problem as optimizing the fitness of a state whose optimum is obtained by using the simulated annealing. A complex fitness function is developed that takes advantage of various similarity metrics including string, linguistic, and structural similarities. A randomized warm initialization is specially tailored for the simulated annealing to expedite its convergence. The experiments illustrate that SANOM is competitive with the state-of-the-art and is significantly superior to other EA-based systems.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {3},
numpages = {24},
keywords = {OAEI, Ontology alignment, simulated annealing, SANOM}
}

@article{10.1145/3314949,
author = {Emami, Hojjat},
title = {A Graph-Based Approach to Person Name Disambiguation in Web},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3314949},
doi = {10.1145/3314949},
abstract = {This article presents a name disambiguation approach to resolve ambiguities between person names and group web pages according to the individuals they refer to. The proposed approach exploits two important sources of entity-centric semantic information extracted from web pages, including personal attributes and social relationships. It takes as input the web pages that are results for a person name search. The web pages are analyzed to extract personal attributes and social relationships. The personal attributes and social relationships are mapped into an undirected weighted graph, called attribute-relationship graph. A graph-based clustering algorithm is proposed to group the nodes representing the web pages, each of which refers to a person entity. The outcome is a set of clusters such that the web pages within each cluster refer to the same person. We show the effectiveness of our approach by evaluating it on large-scale datasets WePS-1, WePS-2, and WePS-3. Experimental results are encouraging and show that the proposed method clearly outperforms several baseline methods and also its counterparts.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {4},
numpages = {25},
keywords = {social network analysis, Person name disambiguation, clustering, graph-based name disambiguation}
}

@article{10.1145/3325523,
author = {Sutterer, Paul and Waldherr, Stefan and Bichler, Martin},
title = {Are Truthful Bidders Paying Too Much? Efficiency and Revenue in Display Ad Auctions},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3325523},
doi = {10.1145/3325523},
abstract = {Display ad auctions have become the predominant means to allocate user impressions on a website to advertisers. These auctions are conducted in milliseconds online, whenever a user visits a website. The impressions are typically priced via a simple second-price rule. For single-item auctions, this Vickrey payment rule is known to be incentive-compatible. However, it is unclear whether bidders should still bid truthful in an online auction where impressions (or items) arrive dynamically over time and their valuations are not separable, as is the case with campaign targets or budgets. The allocation process might not maximize welfare and the payments can differ substantially from those paid in an offline auction with a Vickrey-Clarke-Groves (VCG) payment rule or also competitive equilibrium prices. We study the properties of the offline problem and model it as a mathematical program. In numerical experiments, we find that the welfare achieved in the online auction process with truthful bidders is high compared to the theoretical worst-case efficiency, but that the bidders pay significantly more on average compared to what they would need to pay in a corresponding offline auction in thin markets with up to four bidders. However, incentives for bid shading in these second-price auctions decrease quickly with additional competition and bidders risk losing.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {6},
numpages = {18},
keywords = {efficiency, incentives in online auctions, real-time bidding, Display ad auctions}
}

@article{10.1145/3329717,
author = {Kartal, Hasan B. and Liu, Xiaoping and Li, Xiao-Bai},
title = {Differential Privacy for the Vast Majority},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3329717},
doi = {10.1145/3329717},
abstract = {Differential privacy has become one of the widely used mechanisms for protecting sensitive information in databases and information systems. Although differential privacy provides a clear measure of privacy guarantee, it implicitly assumes that each individual corresponds to a single record in the result of a database query. This assumption may not hold in many database query applications. When an individual has multiple records, strict implementation of differential privacy may cause significant information loss. In this study, we extend the differential privacy principle to situations where multiple records in a database are associated with the same individual. We propose a new privacy principle that integrates differential privacy with the Pareto principle in analyzing privacy risk and data utility. When applied to the situations with multiple records per person, the proposed approach can significantly reduce the information loss in the released query results with a relatively small relaxation in the differential privacy guarantee. The effectiveness of the proposed approach is evaluated using three real-world databases.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {8},
numpages = {15},
keywords = {noise perturbation, Pareto principle, database query, Data privacy}
}

@article{10.1145/3333535,
author = {Jiang, Jian-Min and Hong, Zhong and Chen, Yangyang},
title = {Modeling and Analyzing Incremental Natures of Developing Software},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3333535},
doi = {10.1145/3333535},
abstract = {The basic premise of iterative and evolutionary project management is that a project is divided into early, frequent, and short duration delivery steps. Each step attempts to deliver some real value to stakeholders. The increment size and iteration length usually depend on profitability, finance, deadline, and so on, rather than the functionality of a developing system. It is difficult to guarantee the correctness in every iteration step. In this article, we propose a method of ensuring the correctness of iterative design in terms of deadlock-freedom of the behavior of software. The method first obtains the correct (deadlock-free) atomic subsystems of a system using a decomposition approach. In the iterative development process, the method then requires that one atomic subsystem or the composition of multiple atomic subsystems should be regarded as one increment. Every increment is naturally correct and can be completely independently developed, independently deployed, and independently maintained. The currently released system in each iteration step is naturally guaranteed to be correct. It is not necessary for developers to consider the composition of the increment and the previously released system may cause flaws and errors. We also discuss the approach for ensuring correctness when design modifications are made in an iteration step. Finally, we explore the automatic decomposition of a system into multiple atomic subsystems and present the corresponding algorithm. A case demonstrates these results.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {5},
numpages = {32},
keywords = {Iteration, increment, decrement, decomposition}
}

@article{10.1145/3325522,
author = {Han, Xu and Yaraghi, Niam and Gopal, Ram},
title = {Catching Them Red-Handed: Optimizing the Nursing Homes’ Rating System},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3325522},
doi = {10.1145/3325522},
abstract = {The Centers for Medicare 8 Medicaid Services (CMS) launched its nursing home rating system in 2008, which has been widely used among patients, doctors, and insurance companies since then. The system rates nursing homes based on a combination of CMS's inspection results and nursing homes’ self-reported measures. Prior research has shown that the rating system is subject to inflation in the self-reporting procedure, leading to biased overall ratings. Given the limited resources CMS has, it is important to optimize the inspection process and develop an effective audit process to detect and deter inflation.We first examine if the domain that CMS currently inspects is the best choice in terms of minimizing the population of nursing homes that can inflate and minimizing the difficulty of detecting such inflators. To do this, we formulate the problem mathematically and test the model by using publicly available CMS data on nursing home ratings. We show that CMS's current choice of inspection domain is not optimal if it intends to minimize the number of nursing homes that can inflate their reports, and CMS will be better off if it inspects the staffing domain instead. We also show that CMS's current choice of inspection domain is only optimal had there been an audit system in place to complement it. We then design an audit system for CMS which will be coupled with its current inspection strategy to either minimize the initial budget required to conduct the audits or to maximize the efficiency of the audit process. To design the audit system, we consider nursing homes’ reactions to different audit policies, and conduct a detailed simulation study on the optimal audit parameter settings. Our result suggests that CMS should use a moderate audit policy in order to carefully balance the tradeoff between audit net budget and audit efficiency.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {7},
numpages = {26},
keywords = {rating inflation, nursing homes’ rating system, Audit design}
}

@article{10.1145/3342557,
author = {Jain, Hemant and Raghu, T. S. and Yoon, Victoria and Yue, Wei Thoo},
title = {Introduction to Special Section Based on Papers Presented at the Workshop on Information Technology and Systems, 2017},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3342557},
doi = {10.1145/3342557},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {6e},
numpages = {2}
}

@article{10.1145/3343858,
author = {Lai, Jianwei and Zhang, Dongsong and Wang, Sen and Kilic, Isil Doga Yakut and Zhou, Lina},
title = {ThumbStroke: A Virtual Keyboard in Support of Sight-Free and One-Handed Text Entry on Touchscreen Mobile Devices},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3343858},
doi = {10.1145/3343858},
abstract = {The QWERTY keyboard on mobile devices usually requires users’ full visual attention and both hands, which is not always possible. We propose a thumb-stroke-based keyboard, ThumbStroke, to support both sight-free and one-handed text entry. Text entry via ThumbStroke completely relies on the directions of thumb strokes at any place on the screen of a mobile device. It does not require physical press on any specific keys, thus eliminating the need for visual attention and reducing errors due to tiny key size, fat thumbs, limited thumb reachability, and visual occlusion. We empirically evaluated ThumbStroke through a 20-session longitudinal controlled lab experiment. ThumbStroke shows advantages in typing accuracy and user perceptions in comparison to the Escape and QWERTY keyboards and results in faster typing speed than QWERTY in sight-free and one-handed text entry. This study provides novel research contributions to mobile HCI, advancing the design of soft keyboards for one-handed interaction with mobile devices and mobile accessibility.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {11},
numpages = {19},
keywords = {ThumbStroke, one-handed, text entry, keyboard, sight-free, stroke}
}

@article{10.1145/3354288,
author = {Zo, Hangjung and Nazareth, Derek L. and Jain, Hemant K.},
title = {Service-Oriented Application Composition with Evolutionary Heuristics and Multiple Criteria},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3354288},
doi = {10.1145/3354288},
abstract = {The need to create and deploy business application systems rapidly has sparked interest in using web services to compose them. When creating mission-critical business applications through web service compositions, in addition to ensuring that functional requirements are met, designers need to consider the end-to-end reliability, security, performance, and overall cost of the application. As the number of available coarse-grain business services grows, the problem of selecting appropriate services quickly becomes combinatorially explosive for realistic-sized business applications. This article develops a business-process-driven approach for composing service-oriented applications. We use a combination of weights to explore the entire QoS criteria landscape through the use of a multi-criteria genetic algorithm (GA) to identify a Pareto-optimal multidimensional frontier that permits managers to trade off conflicting objectives when selecting a set of services. We illustrate the effectiveness of the approach by applying it to a real-world drop-ship business application and compare its performance to another GA-based approach for service composition.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {10},
numpages = {28},
keywords = {multiple criteria decision making, Service-oriented applications}
}

@article{10.1145/3351158,
author = {Pal, Ranjan and Golubchik, Leana and Psounis, Konstantions and Bandyopadhyay, Tathagata},
title = {On Robust Estimates of Correlated Risk in Cyber-Insured IT Firms: A First Look at Optimal AI-Based Estimates under “Small” Data},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3351158},
doi = {10.1145/3351158},
abstract = {In this article, we comment on the drawbacks of the existing AI-based Bayesian network (BN) cyber-vulnerability analysis (C-VA) model proposed in Mukhopadhyay et al. (2013) to assess cyber-risk in IT firms, where this quantity is usually a joint distribution of multiple risk (random) variables (e.g., quality of antivirus, frequency of monitoring, etc.) coming from heterogeneous distribution families. As a major modeling drawback, Mukhopadhyay et al. (2013) assume that any pair of random variables in the BN are linearly correlated with each other. This simplistic assumption might not always hold true for general IT organizational environments. Thus, the use of the C-VA model in general will result in loose estimates of correlated IT risk and will subsequently affect cyber-insurance companies in framing profitable coverage policies for IT organizations. To this end, we propose methods to (1) find a closed-form expression for the maximal correlation arising between pairs of discrete random variables, whose value finds importance in getting robust estimates of copula-induced computations of organizational cyber-risk, and (2) arrive at a computationally effective mechanism to compute nonlinear correlations among pairs of discrete random variables in the correlation matrix of the CBBN model (Mukhopadhyay et al. 2013). We also prove that an empirical computation of MC using our method converges rapidly, that is, exponentially fast, to the true correlation value in the number of samples. Our proposed method contributes to a tighter estimate of IT cyber-risk under environments of low-risk data availability and will enable insurers to better assess organizational risks and subsequently underwrite profitable cyber-insurance policies.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {9},
numpages = {18},
keywords = {IT cyber-risk, AI, sampling, correlation, Bayesian network, copula}
}

@article{10.1145/3365538,
author = {Kang, Yin and Zhou, Lina},
title = {Helpfulness Assessment of Online Reviews: The Role of Semantic Hierarchy of Product Features},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3365538},
doi = {10.1145/3365538},
abstract = {Effective use of online consumer reviews is hampered by uncertainty about their helpfulness. Despite a growing body of knowledge on indicators of review helpfulness, previous studies have overlooked rich semantic information embedded in review content. Following design science principles, this study introduces a semantic hierarchy of product features by probing the review text. Using the hierarchical framework as a guide, we develop a research model of review helpfulness assessment. In the model, we propose and conceptualize three new factors—breadth, depth, and redundancy, by building on and/or extending product uncertainty, information quality, signaling, and encoding variability theories. The model-testing results lend strong support to the proposed effects of those factors on review helpfulness. They also reveal interesting differences in the effects of redundancy and readability between different types of products. This study embodies knowledge moments of multiple genres of inquiry in design science research, which have multifold research and practical implications.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = nov,
articleno = {12},
numpages = {18},
keywords = {design science, product feature, redundancy, semantic hierarchy, depth, Review helpfulness, breadth}
}

@article{10.1145/3351159,
author = {Park, Jiyong and Cho, Daegon and Lee, Jae Kyu and Lee, Byungtae},
title = {The Economics of Cybercrime: The Role of Broadband and Socioeconomic Status},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3351159},
doi = {10.1145/3351159},
abstract = {Under what conditions is the Internet more likely to be used maliciously for criminal activity? This study examines the conditions under which the Internet is associated with cybercriminal offenses. Using comprehensive state-level data in the United States during 2004–2010, our findings show that there is no clear empirical evidence that the Internet penetration rate is related to the number of Internet crime perpetrators; however, cybercriminal activities are contingent upon socioeconomic factors and connection speed. Specifically, a higher income, more education, a lower poverty rate, and a higher inequality are likely to make the Internet penetration be more positively related with cybercrime perpetrators, which are indeed different from the conditions of terrestrial crime in the real world. In addition, as opposed to narrowband, the broadband connections are significantly and positively associated with the number of Internet crime perpetrators, and it amplifies the aforementioned moderating effects of socioeconomic status on Internet crime offenses. Taken together, cybercrime requires more than just a skilled perpetrator, and it requires an infrastructure to facilitate profiteering from the act.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {13},
numpages = {23},
keywords = {socioeconomic status, Internet penetration, Economics of crime, broadband, cybercrime}
}

@article{10.1145/3370082,
author = {Jannach, Dietmar and Jugovac, Michael},
title = {Measuring the Business Value of Recommender Systems},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3370082},
doi = {10.1145/3370082},
abstract = {Recommender Systems are nowadays successfully used by all major web sites—from e-commerce to social media—to filter content and make suggestions in a personalized way. Academic research largely focuses on the value of recommenders for consumers, e.g., in terms of reduced information overload. To what extent and in which ways recommender systems create business value is, however, much less clear, and the literature on the topic is scattered. In this research commentary, we review existing publications on field tests of recommender systems and report which business-related performance measures were used in such real-world deployments. We summarize common challenges of measuring the business value in practice and critically discuss the value of algorithmic improvements and offline experiments as commonly done in academic environments. Overall, our review indicates that various open questions remain both regarding the realistic quantification of the business effects of recommenders and the performance assessment of recommendation algorithms in&nbsp;academia.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {16},
numpages = {23},
keywords = {field tests, survey, Recommendation, business value}
}

@article{10.1145/3369395,
author = {Chen, Jiawei and Liu, Hongyan and Yang, Yinghui (Catherine) and He, Jun},
title = {Effective Selection of a Compact and High-Quality Review Set with Information Preservation},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3369395},
doi = {10.1145/3369395},
abstract = {Consumers increasingly make informed buying decisions based on reading online reviews for products and services. Due to the large volume of available online reviews, consumers hardly have the time and patience to read them all. This article aims to select a compact set of high-quality reviews that can cover a specific set of product features and related consumer sentiments. Selecting such a subset of reviews can significantly save the time spent on reading reviews while preserving the information needed. A unique review selection problem is defined and modeled as a bi-objective combinatorial optimization problem, which is then transformed into a minimum-cost set cover problem that is NP-complete. Several approximation algorithms are then designed, which can sustain performance guarantees in polynomial time. Our effective selection algorithms can also be upgraded to handle dynamic situations. Comprehensive experiments conducted on twelve real datasets demonstrate that the proposed algorithms significantly outperform benchmark methods by generating a more compact review set with much lower computational cost. The number of reviews selected is much smaller compared to the quantity of all available reviews, and the selection efficiency is deeply increased by accelerating strategies, making it very practical to adopt the methods in real-world online applications.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {15},
numpages = {22},
keywords = {approximation algorithms, Review selection, dynamic updating, information preservation}
}

@article{10.1145/3365537,
author = {Chung, Wingyan and Rao, Bingbing and Wang, Liqiang},
title = {Interaction Models for Detecting Nodal Activities in Temporal Social Media Networks},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3365537},
doi = {10.1145/3365537},
abstract = {Detecting nodal activities in dynamic social networks has strategic importance in many applications, such as online marketing campaigns and homeland security surveillance. How peer-to-peer exchanges in social media can facilitate nodal activity detection is not well explored. Existing models assume network nodes to be static in time and do not adequately consider features from social theories. This research developed and validated two theory-based models, Random Interaction Model (RIM) and Preferential Interaction Model (PIM), to characterize temporal nodal activities in social media networks of human agents. The models capture the network characteristics of randomness and preferential interaction due to community size, human bias, declining connection cost, and rising reachability. The models were compared against three benchmark models (abbreviated as EAM, TAM, and DBMM) using a social media community consisting of 790,462 users who posted over 3,286,473 tweets and formed more than 3,055,797 links during 2013–2015. The experimental results show that both RIM and PIM outperformed EAM and TAM significantly in accuracy across different dates and time windows. Both PIM and RIM scored significantly smaller errors than DBMM did. Structural properties of social networks were found to provide a simple and yet accurate approach to predicting model performances. These results indicate the models’ strong capability of accounting for user interactions in real-world social media networks and temporal activity detection. The research should provide new approaches for temporal network activity detection, develop relevant new measures, and report new findings from large social media datasets.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {14},
numpages = {30},
keywords = {business analytics, social network analysis, dynamic graph modeling, social media analytics, Interaction models}
}

@article{10.1145/3371388,
author = {Li, Hongfei and Shankar, Ramesh and Stallaert, Jan},
title = {Invested or Indebted: <i>Ex-Ante</i> and <i>Ex-Post</i> Reciprocity in Online Knowledge Sharing Communities},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3371388},
doi = {10.1145/3371388},
abstract = {Online communities that curate knowledge critically depend on high-quality contributions from anonymous expert users. Understanding users’ motivation to contribute knowledge helps practitioners design such websites for optimal user contribution and user benefits. Researchers have studied reciprocity as a motivation for users to share knowledge online. In this study, we focus on two different types of reciprocity as drivers of online contribution: ex-post and ex-ante reciprocity. Ex-post reciprocity refers to users who received help from others in the past and pay back by helping others at present. Using a quasi-experiment performed via the instrumental variable method as the identification strategy, we test whether users who received more answers last week answer more questions in the current week on StackOverflow.com. We find a significant positive relationship between ex-post reciprocity and knowledge contribution, and such a reciprocal motivation diminishes with time. Ex-ante reciprocity refers to people helping others in expectation of future help from others. Using data from StackOverflow.com, we take advantage of a natural experiment with a difference-in-differences analysis and find evidence supporting the existence of ex-ante reciprocity. This study offers a new taxonomy for reciprocity and new insights on how reciprocity drives online knowledge sharing.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {1},
numpages = {26},
keywords = {reciprocity, Ex-post, ex-ante, Q&amp;A website, knowledge sharing}
}

@article{10.1145/3384472,
author = {Rezvani, Mohsen and Rezvani, Mojtaba},
title = {A Randomized Reputation System in the Presence of Unfair Ratings},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3384472},
doi = {10.1145/3384472},
abstract = {With the increasing popularity of online shopping markets, a significant number of consumers rely on these venues to meet their demands while choosing different products based on the ratings provided by others. Simultaneously, consumers feel confident in expressing their opinions through ratings. As a result, millions of ratings are generated on the web for different products, services, and dealers. Nonetheless, a noticeable number of users post unfair feedback. Recent studies have shown that reputation escalation is emerging as a new service, by which dealers pay to receive good feedback and escalate their ratings in online shopping markets. Therefore, finding robust and reliable ways to distinguish between fake and trustworthy ratings from users is a crucial task for every online shopping market. Moreover, with the dramatic increase in the number of ratings provided by consumers, scalability has arisen as another significant issue in the existing methods of reputation systems. To tackle these issues, we propose a randomized algorithm that calculates the reputation based on a random sample of the ratings. Since the randomly selected sample has a logarithmic size, it guarantees feasible scalability for large-scale online review systems. In addition, the randomness nature of the algorithm makes it robust against unfair ratings. We provide a thorough theoretical analysis of the proposed algorithm and validate its effectiveness through extensive empirical evaluation using real-world and synthetically generated datasets. Our experimental results show that the proposed method provides a high accuracy while running much faster than the existing iterative filtering approaches.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {2},
numpages = {16},
keywords = {data aggregation, trust computation, Reputation system, online rating, randomized algorithm}
}

@article{10.1145/3382158,
author = {Pierazzi, Fabio and Mezzour, Ghita and Han, Qian and Colajanni, Michele and Subrahmanian, V. S.},
title = {A Data-Driven Characterization of Modern Android Spyware},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3382158},
doi = {10.1145/3382158},
abstract = {According to Nokia’s 2017 Threat Intelligence Report, 68.5% of malware targets the Android platform; Windows is second with 28%, followed by iOS and other platforms with 3.5%. The Android spyware family UAPUSH was responsible for the most infections, and several of the top 20 most common Android malware were spyware. Simply put, modern spyware steals the basic information needed to fuel more deadly attacks such as ransomware and banking fraud. Not surprisingly, some forms of spyware are also classified as banking trojans (e.g., ACECARD). We present a data-driven characterization of the principal factors that distinguish modern Android spyware (July 2016–July 2017) both from goodware and other Android malware, using both traditional and deep ML. First, we propose an Ensemble Late Fusion (ELF) architecture that combines the results of multiple classifiers’ predicted probabilities to generate a final prediction. We show that ELF outperforms several of the best-known traditional and deep learning classifiers. Second, we automatically identify key features that distinguish spyware both from goodware and from other malware. Finally we present a detailed analysis of the factors distinguishing five important families of Android spyware: UAPUSH, PINCER, HEHE, USBCLEAVER, and ACECARD (the last is a hybrid spyware-banking trojan).},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {4},
numpages = {38},
keywords = {malware, Android, spyware, Machine learning, characterization}
}

@article{10.1145/3384473,
author = {Ni, Li and Luo, Wenjian and Lu, Nannan and Zhu, Wenjie},
title = {Mining the Local Dependency Itemset in a Products Network},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3384473},
doi = {10.1145/3384473},
abstract = {Many studies have been conducted on market basket analysis such as association rules and dependent patterns. These studies mainly focus on mining all significant patterns or patterns directly associated with a given item in a dataset. The problem that has not been addressed is how to mine patterns associated with a given item from the local view. This problem becomes very meaningful when the market basket dataset is huge. To address this problem, in this study, first, a new idea called “local dependency itemset” is put forward, which refers to patterns associated with the given item. Second, a framework of mining the local dependency itemset is presented. The framework has two steps, which are executed iteratively. One is expanding the local dependency itemset that initially consists of only the given item; the other is updating the local products network. Third, this framework is implemented by three different dependence indicators and a typical local community detection algorithm. The experimental results confirm that the local dependency itemset is meaningful.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {3},
numpages = {31},
keywords = {local dependency itemset, Data mining, products network, local community detection}
}

@article{10.1145/3386599,
author = {Lu, Haibing and Chen, Xi and Shi, Junmin and Vaidya, Jaideep and Atluri, Vijayalakshmi and Hong, Yuan and Huang, Wei},
title = {Algorithms and Applications to Weighted Rank-One Binary Matrix Factorization},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3386599},
doi = {10.1145/3386599},
abstract = {Many applications use data that are better represented in the binary matrix form, such as click-stream data, market basket data, document-term data, user-permission data in access control, and others. Matrix factorization methods have been widely used tools for the analysis of high-dimensional data, as they automatically extract sparse and meaningful features from data vectors. However, existing matrix factorization methods do not work well for the binary data. One crucial limitation is interpretability, as many matrix factorization methods decompose an input matrix into matrices with fractional or even negative components, which are hard to interpret in many real settings. Some matrix factorization methods, like binary matrix factorization, do limit decomposed matrices to binary values. However, these models are not flexible to accommodate some data analysis tasks, like trading off summary size with quality and discriminating different types of approximation errors. To address those issues, this article presents weighted rank-one binary matrix factorization, which is to approximate a binary matrix by the product of two binary vectors, with parameters controlling different types of approximation errors. By systematically running weighted rank-one binary matrix factorization, one can effectively perform various binary data analysis tasks, like compression, clustering, and pattern discovery. Theoretical properties on weighted rank-one binary matrix factorization are investigated and its connection to problems in other research domains are examined. As weighted rank-one binary matrix factorization in general is NP-hard, efficient and effective algorithms are presented. Extensive studies on applications of weighted rank-one binary matrix factorization are also conducted.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {7},
numpages = {33},
keywords = {clustering, Discrete data, pattern discovery, compression}
}

@article{10.1145/3386160,
author = {Ermakova, Tatiana and Fabian, Benjamin and Kornacka, Marta and Thiebes, Scott and Sunyaev, Ali},
title = {Security and Privacy Requirements for Cloud Computing in Healthcare: Elicitation and Prioritization from a Patient Perspective},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3386160},
doi = {10.1145/3386160},
abstract = {Cloud computing promises essential improvements in healthcare delivery performance. However, its wide adoption in healthcare is yet to be seen, one main reason being patients’ concerns for security and privacy of their sensitive medical records. These concerns can be addressed through corresponding security and privacy requirements within the system engineering process. Despite a plethora of related research, security and privacy requirements for cloud systems and services have seldomly been investigated methodically so far, whereas their individual priorities to increase the system success probability have been neglected. Against this background, this study applies a systematic requirements engineering process: First, based on a systematic literature review, an extensive initial set of security and privacy requirements is elicited. Second, an online survey based on the best-worst scaling method is designed, conducted, and evaluated to determine priorities of security and privacy requirements.Our results show that confidentiality and integrity of medical data are ranked at the top of the hierarchy of prioritized requirements, followed by control of data use and modification, patients’ anonymity, and patients’ control of access rights. Availability, fine-grained access control, revocation of access rights, flexible access, clinicians’ anonymity, as well as usability, scalability, and efficiency of the system complete the ranking. The level of agreement among patients is rather small, but statistically significant at the 0.01 level.The main contribution of the present research comprises the study method and results highlighting the role of strong security and privacy and excluding any trade-offs with system usability. Enabling a richer understanding of patients’ security and privacy requirements for adopting cloud computing in healthcare, these are of particular importance to researchers and practitioners interested in supporting the process of security and privacy engineering for health-cloud solutions. It further represents a supplement that can support time-intensive negotiation meetings between the requirements engineers and patients.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {6},
numpages = {29},
keywords = {healthcare, requirements, privacy, Cloud computing, security}
}

@article{10.1145/3386243,
author = {Unger, Moshe and Tuzhilin, Alexander and Livne, Amit},
title = {Context-Aware Recommendations Based on Deep Learning Frameworks},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3386243},
doi = {10.1145/3386243},
abstract = {In this article, we suggest a novel deep learning recommendation framework that incorporates contextual information into neural collaborative filtering recommendation approaches. Since context is often represented by dynamic and high-dimensional feature space in multiple applications and services, we suggest to model contextual information in various ways for multiple purposes, such as rating prediction, generating top-k recommendations, and classification of users’ feedback. Specifically, based on the suggested framework, we propose three deep context-aware recommendation models based on explicit, unstructured, and structured latent representations of contextual data derived from various contextual dimensions (e.g., time, location, user activity). Offline evaluation on three context-aware datasets confirms that our proposed deep context-aware models surpass state-of-the-art context-aware methods. We also show that utilizing structured latent contexts in the proposed deep recommendation framework achieves significantly better performance than the other context-aware models on all datasets.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {8},
numpages = {15},
keywords = {neural networks, latent, deep learning, context-aware recommendation, Context}
}

@article{10.1145/3386159,
author = {Pal, Ranjan and Psounis, Konstantinos and Crowcroft, Jon and Kelly, Frank and Hui, Pan and Tarkoma, Sasu and Kumar, Abhishek and Kelly, John and Chatterjee, Aritra and Golubchik, Leana and Sastry, Nishanth and Nag, Bodhibrata},
title = {When Are Cyber Blackouts in Modern Service Networks Likely? A Network Oblivious Theory on Cyber (Re)Insurance Feasibility},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3386159},
doi = {10.1145/3386159},
abstract = {Service liability interconnections among globally networked IT- and IoT-driven service organizations create potential channels for cascading service disruptions worth billions of dollars, due to modern cyber-crimes such as DDoS, APT, and ransomware attacks. A natural question that arises in this context is: What is the likelihood of a cyber-blackout?, where the latter term is defined as the probability that all (or a major subset of) organizations in a service chain become dysfunctional in a certain manner due to a cyber-attack at some or all points in the chain. The answer to this question has major implications to risk management businesses such as cyber-insurance when it comes to designing policies by risk-averse insurers for providing coverage to clients in the aftermath of such catastrophic network events. In this article, we investigate this question in general as a function of service chain networks and different cyber-loss distribution types. We show somewhat surprisingly (and discuss the potential practical implications) that, following a cyber-attack, the effect of (a) a network interconnection topology and (b) a wide range of loss distributions on the probability of a cyber-blackout and the increase in total service-related monetary losses across all organizations are mostly very small. The primary rationale behind these results are attributed to degrees of heterogeneity in the revenue base among organizations and the Increasing Failure Rate property of popular (i.i.d/non-i.i.d) loss distributions, i.e., log-concave cyber-loss distributions. The result will enable risk-averse cyber-risk managers to safely infer the impact of cyber-attacks in a worst-case network and distribution oblivious setting.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {5},
numpages = {38},
keywords = {cyber-blackout, systemic risk, Service network}
}

@article{10.1145/3399631,
author = {Belhadi, Asma and Djenouri, Youcef and Lin, Jerry Chun-Wei and Cano, Alberto},
title = {Trajectory Outlier Detection: Algorithms, Taxonomies, Evaluation, and Open Challenges},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3399631},
doi = {10.1145/3399631},
abstract = {Detecting abnormal trajectories is an important task in research and industrial applications, which has attracted considerable attention in recent decades. This work studies the existing trajectory outlier detection algorithms in different industrial domains and applications, including maritime, smart urban transportation, video surveillance, and climate change domains. First, we review several algorithms for trajectory outlier detection. Second, different taxonomies are proposed regarding application-, output-, and algorithm-based levels. Third, evaluation of 10 trajectory outlier detection algorithms is performed on small, large, and big trajectory databases. Finally, future challenges and open issues with regard to trajectory outliers are derived and discussed. This survey offers a general overview of existing trajectory outlier detection algorithms in industrial informatics applications. As a result, mature solutions may be further developed by data mining and machine learning communities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {16},
numpages = {29},
keywords = {Trajectory outlier detection, data mining, industrial informatics applications, machine learning}
}

@article{10.1145/3391251,
author = {Gan, Wensheng and Lin, Jerry Chun-Wei and Chao, Han-Chieh and Fournier-Viger, Philippe and Wang, Xuan and Yu, Philip S.},
title = {Utility-Driven Mining of Trend Information for Intelligent System},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3391251},
doi = {10.1145/3391251},
abstract = {Useful knowledge, embedded in a database, is likely to change over time. Identifying the recent changes in temporal data can provide valuable up-to-date information to decision makers. Nevertheless, techniques for mining high-utility patterns (HUPs) seldom consider recency as a criterion to discover patterns. Thus, the traditional utility mining framework is inadequate for obtaining up-to-date insights about real-world data. In this article, we address this issue by introducing a novel framework, named utility-driven mining of recent/trend high-utility patterns (RUPs), in temporal databases for intelligent systems, based on user-specified minimum recency and minimum utility thresholds. The utility-driven RUP algorithm is based on novel global and conditional downward closure properties, and a recency-utility tree. Moreover, it adopts a vertical compact recency-utility list structure to store the information required by the mining process. The developed RUP algorithm recursively discovers recent high-utility patterns. It is also fast and consumes a small amount of memory due to its pattern discovery approach that does not generate candidates. Two improved versions of the algorithm with additional pruning strategies are also designed to speed up the discovery of patterns by reducing the search space. Results of a substantial experimental evaluation show that the proposed algorithm can efficiently identify all recent HUPs in large-scale databases, and that the improved algorithm performs best.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {14},
numpages = {28},
keywords = {Economic behavior, high-utility pattern, intelligent system, temporal database, utility mining}
}

@article{10.1145/3399630,
author = {Tao, Jie and Zhou, Lina},
title = {A Weakly Supervised WordNet-Guided Deep Learning Approach to Extracting Aspect Terms from Online Reviews},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3399630},
doi = {10.1145/3399630},
abstract = {The unstructured nature of online reviews makes it inefficient and inconvenient for prospective consumers to research and use in support of purchase decision making. The aspects of products provide a fine-grained meaningful perspective for understanding and organizing review texts. Traditional aspect term extraction approaches rely on discrete language models that treat words in isolation. Despite that continuous-space language models have demonstrated promise in addressing a wide range of problems, their application in aspect term extraction faces significant challenges. For instance, existing continuous-space language models typically require large collections of labeled data, which remain difficult to obtain in many domains. More importantly, previous methods are largely data driven but overlook the role of human knowledge in guiding model development. To address these limitations, this study designs and develops weakly supervised WordNet-guided deep learning to aspect term extraction. The approach draws on deep-level semantic information from WordNet to guide not only the selection representative seed terms but also the pruning of aspect candidate terms. The weak supervision is provided by a very small set of labeled data. We conduct a comprehensive evaluation of the proposed method using both direct and indirect methods. The evaluation results with Yelp restaurant reviews demonstrate that our proposed method consistently outperforms all baseline methods including discrete models and the state-of-the-art continuous-space language models for aspect term extraction across both direct and indirect evaluations. The research findings have broad research, technical, and practical implications for various stakeholders of online reviews.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {13},
numpages = {22},
keywords = {Aspect term extraction, deep learning, text analytics, continuous-space language model, semantic knowledge}
}

@article{10.1145/3383780,
author = {Wang, Xiangyu and Zhao, Kang and Zhou, Xun and Street, Nick},
title = {Predicting User Posting Activities in Online Health Communities with Deep Learning},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3383780},
doi = {10.1145/3383780},
abstract = {Online health communities (OHCs) represent a great source of social support for patients and their caregivers. Better predictions of user activities in OHCs can help improve user engagement and retention, which are important to manage and sustain a successful OHC. This article proposes a general framework to predict OHC user posting activities. Deep learning methods are adopted to learn from users’ temporal trajectories in both the volumes and content of posts published over time. Experiments based on data from a popular OHC for cancer survivors demonstrate that the proposed approach can improve the performance of user activity predictions. In addition, several topics of users’ posts are found to have strong impact on predicting users’ activities in the OHC.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {12},
numpages = {15},
keywords = {Predictive model, trajectory mining, text analytics, user churn}
}

@article{10.1145/3382189,
author = {Tan, Liling and Li, Maggie Yundi and Kok, Stanley},
title = {E-Commerce Product Categorization via Machine Translation},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3382189},
doi = {10.1145/3382189},
abstract = {E-commerce platforms categorize their products into a multi-level taxonomy tree with thousands of leaf categories. Conventional methods for product categorization are typically based on machine learning classification algorithms. These algorithms take product information as input (e.g., titles and descriptions) to classify a product into a leaf category. In this article, we propose a new paradigm based on machine translation. In our approach, we translate a product’s natural language description into a sequence of tokens representing a root-to-leaf path in a product taxonomy. In our experiments on two large real-world datasets, we show that our approach achieves better predictive accuracy than a state-of-the-art classification system for product categorization. In addition, we demonstrate that our machine translation models can propose meaningful new paths between previously unconnected nodes in a taxonomy tree, thereby transforming the taxonomy into a directed acyclic graph. We discuss how the resultant taxonomy directed acyclic graph promotes user-friendly navigation, and how it is more adaptable to new products.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {11},
numpages = {14},
keywords = {machine translation, classification, E-commerce}
}

@article{10.1145/3382188,
author = {Gopal, Ram D. and Hidaji, Hooman and Kutlu, Sule Nur and Patterson, Raymond A. and Rolland, Erik and Zhdanov, Dmitry},
title = {Real or Not? Identifying Untrustworthy News Websites Using Third-Party Partnerships},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3382188},
doi = {10.1145/3382188},
abstract = {Untrustworthy content such as fake news and clickbait have become a pervasive problem on the Internet, causing significant socio-political problems around the world. Identifying untrustworthy content is a crucial step in countering them. The current best practices for identification involve content analysis and arduous fact-checking of the content. To complement content analysis, we propose examining websites’ third-parties to identify their trustworthiness. Websites utilize third-parties, also known as their digital supply chains, to create and present content and help the website function. Third-parties are an important indication of a website's business model. Similar websites exhibit similarities in the third-parties they use. Using this perspective, we use machine learning and heuristic methods to discern similarities and dissimilarities in third-party usage, which we use to predict trustworthiness of websites. We demonstrate the effectiveness and robustness of our approach in predicting trustworthiness of websites from a database of News, Fake News, and Clickbait websites. Our approach can be easily and cost-effectively implemented to reinforce current identification methods.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {10},
numpages = {20},
keywords = {machine learning, heuristics, Website third-parties, untrustworthy websites, prediction}
}

@article{10.1145/3391402,
author = {Sreenu, Nenavath},
title = {Cashless Payment Policy and Its Effects on Economic Growth of India: An Exploratory Study},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3391402},
doi = {10.1145/3391402},
abstract = {The present world has moved from cash transactions to cashless transactions. This article examines the impact of implementation of a cashless payment policy on economic development and gradual transition to a cashless economy in India. For this study, the focus is on the time period from 2010 to 2018. The data used for this study are tele transfer, through credit or debit card payment, check payment, and E-money on Indian economic growth. The study has employed the panel vector error correction model, Padroni residual cointegration, and the hypothetical prototypical method. The results show that customers and sellers accept a cashless system policy. In the short period, we have a causality model running from a card system to a check payment and telegraphic transfer system, and a causality model running from a telegraphic payment system to a card payment system. In the long period, there is a positive outcome in using a cashless policy on Indian economic growth. However, the use of a cashless policy on Indian economic development in the short term will be negative, whereas in the long term it will impact positively. Hence, any kind of economic strategy that endorses a cashless payment system cannot have positive impact on the economic development directly.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {15},
numpages = {10},
keywords = {electronic money, Card payment, check payment, and telegraphic transfer, VECM}
}

@article{10.1145/3404392,
author = {Dutta, Kaushik and Fang, Xiao and Jiang, Zhengrui (Jeffrey)},
title = {Introduction to WITS 2018 Special Issue in TMIS},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3404392},
doi = {10.1145/3404392},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {9},
numpages = {2}
}

@article{10.1145/3386250,
author = {Hus\'{a}k, Martin and Bajto\v{s}, Tom\'{a}\v{s} and Ka\v{s}par, Jaroslav and Bou-Harb, Elias and \v{C}eleda, Pavel},
title = {Predictive Cyber Situational Awareness and Personalized Blacklisting: A Sequential Rule Mining Approach},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3386250},
doi = {10.1145/3386250},
abstract = {Cybersecurity adopts data mining for its ability to extract concealed and indistinct patterns in the data, such as for the needs of alert correlation. Inferring common attack patterns and rules from the alerts helps in understanding the threat landscape for the defenders and allows for the realization of cyber situational awareness, including the projection of ongoing attacks. In this article, we explore the use of data mining, namely sequential rule mining, in the analysis of intrusion detection alerts. We employed a dataset of 12 million alerts from 34 intrusion detection systems in 3 organizations gathered in an alert sharing platform, and processed it using our analytical framework. We execute the mining of sequential rules that we use to predict security events, which we utilize to create a predictive blacklist. Thus, the recipients of the data from the sharing platform will receive only a small number of alerts of events that are likely to occur instead of a large number of alerts of past events. The predictive blacklist has the size of only 3% of the raw data, and more than 60% of its entries are shown to be successful in performing accurate predictions in operational, real-world settings.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {19},
numpages = {16},
keywords = {intrusion detection, situational awareness, Data mining, attack prediction}
}

@article{10.1145/3418288,
author = {Kesan, Jay P. and Zhang, Linfeng},
title = {Analysis of Cyber Incident Categories Based on Losses},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3418288},
doi = {10.1145/3418288},
abstract = {The fact that “cyber risk” is indeed a collective term for various distinct risks creates great difficulty in communications. For example, policyholders of “cyber insurance” contracts often have a limited or inaccurate understanding about the coverage that they have. To address this issue, we propose a cyber risk categorization method using clustering techniques. This method classifies cyber incidents based on their consequential losses for insurance and risk management purposes. As a result, it also reveals the relationship between the causes and the outcomes of incidents. Our results show that similar cyber incidents, which are often not properly distinguished, can lead to very different losses. We hope that our work can clarify the differences between cyber risks and provide a set of risk categories that is feasible in practice and for future studies.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {25},
numpages = {28},
keywords = {cyber insurance, cyber losses, Cyber risk}
}

@article{10.1145/3394503,
author = {Sweet, Christopher and Moskal, Stephen and Yang, Shanchieh Jay},
title = {On the Variety and Veracity of Cyber Intrusion Alerts Synthesized by Generative Adversarial Networks},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3394503},
doi = {10.1145/3394503},
abstract = {Many cyber attack actions can be observed, but the observables often exhibit intricate feature dependencies, non-homogeneity, and potentially rare yet critical samples. This work tests the ability to learn, model, and synthesize cyber intrusion alerts through Generative Adversarial Networks (GANs), which explore the feature space by reconciling between randomly generated samples and data that reflect a mixture of diverse attack behaviors without a priori knowledge. Through a comprehensive analysis using Jensen-Shannon Divergence, Conditional and Joint Entropy, and mode drops and additions, we show that the Wasserstein-GAN with Gradient Penalty and Mutual Information is more effective in learning to generate realistic alerts than models without Mutual Information constraints. We further show that the added Mutual Information constraint pushes the model to explore the feature space more thoroughly and increases the generation of low probability, yet critical, alert features. This research demonstrates the novel and promising application of unsupervised GANs to learn from limited yet diverse intrusion alerts to generate synthetic alerts that emulate critical dependencies, opening the door to proactive, data-driven cyber threat analyses.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {22},
numpages = {21},
keywords = {intrusion alert analysis, GAN, cyberattack characterization}
}

@article{10.1145/3397521,
author = {Mehrotra, Sharad and Sharma, Shantanu and Ullman, Jeffrey D. and Ghosh, Dhrubajyoti and Gupta, Peeyush and Mishra, Anurag},
title = {PANDA: Partitioned Data Security on Outsourced Sensitive and Non-Sensitive Data},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3397521},
doi = {10.1145/3397521},
abstract = {Despite extensive research on cryptography, secure and efficient query processing over outsourced data remains an open challenge. This article continues along with the emerging trend in secure data processing that recognizes that the entire dataset may not be sensitive and, hence, non-sensitivity of data can be exploited to overcome limitations of existing encryption-based approaches. We first provide a new security definition, entitled partitioned data security, for guaranteeing that the joint processing of non-sensitive data (in cleartext) and sensitive data (in encrypted form) does not lead to any leakage. Then, this article proposes a new secure approach, entitled query binning (QB), that allows secure execution of queries over non-sensitive and sensitive parts of the data. QB maps a query to a set of queries over the sensitive and non-sensitive data in a way that no leakage will occur due to the joint processing over sensitive and non-sensitive data. In particular, we propose secure algorithms for selection, range, and join queries to be executed over encrypted sensitive and cleartext non-sensitive datasets. Interestingly, in addition to improving performance, we show that QB actually strengthens the security of the underlying cryptographic technique by preventing size, frequency-count, and workload-skew attacks.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {23},
numpages = {41},
keywords = {data partitioning, output-size attack, workload skew attack, Data outsourcing, non-sensitive data, data encryption, sensitive data, scalable cryptography, multi-party computation, Intel SGX, secret-sharing}
}

@article{10.1145/3394504,
author = {Mangino, Antonio and Pour, Morteza Safaei and Bou-Harb, Elias},
title = {Internet-Scale Insecurity of Consumer Internet of Things: An Empirical Measurements Perspective},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3394504},
doi = {10.1145/3394504},
abstract = {The number of Internet-of-Things (IoT) devices actively communicating across the Internet is continually increasing, as these devices are deployed across a variety of sectors, constantly transferring private data across the Internet. Due to the extensive deployment of such devices, the continuous discovery and persistence of IoT-centric vulnerabilities in protocols, applications, hardware, and the improper management of such IoT devices has resulted in the rampant, uncontrolled spread of malware threatening consumer IoT devices.To this end, this work adopts a novel, macroscopic methodology for fingerprinting Internet-scale compromised IoT devices, revealing crucial cyber threat intelligence on the insecurity of consumer IoT devices. By developing data-driven techniques rooted in machine learning methods and analyzing 3.6 TB of network traffic data, we discover 855,916 compromised IP addresses, with 310,164 fingerprinted as IoT. Further analysis reveals China and Brazil to be hosting the most significant population of compromised IoT devices (100,000 and 55,000, respectively). Additionally, we provide a longitudinal analysis on data from one year ago against this work, revealing the evolving trends of IoT exploitation, such as the increased number of vendors targeted by malware, rising from 50 to 131. Moreover, countries such as China (420% increased infected IoT count) and Indonesia (177% increased infected IoT count) have seen notably high increases in infection rates. Last, we compare our geographic results against Global Cybersecurity Index (GCI) ratings, verifying that countries with high GCI ratings, such as the Netherlands and Germany, had relatively low infection rates. However, upon further inspection, we find that the GCI rate does not accurately represent the consumer IoT market, with countries such as China and Russia being rated with “high” CGI scores, yet hosting a large population of infected consumer IoT devices.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {21},
numpages = {24},
keywords = {data science, Internet-of-Things, IoT security, IoT forensics}
}

@article{10.1145/3382159,
author = {Mudgerikar, Anand and Sharma, Puneet and Bertino, Elisa},
title = {Edge-Based Intrusion Detection for IoT Devices},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3382159},
doi = {10.1145/3382159},
abstract = {As the Internet of Things (IoT) is estimated to grow to 25 billion by 2021, there is a need for an effective and efficient Intrusion Detection System (IDS) for IoT devices. Traditional network-based IDSs are unable to efficiently detect IoT malware and new evolving forms of attacks like file-less attacks. In this article, we present a system level Device-Edge split IDS for IoT devices. Our IDS profiles IoT devices according to their “behavior” using system-level information like running process parameters and their system calls in an autonomous, efficient, and scalable manner and then detects anomalous behavior indicative of intrusions. The modular design of our IDS along with a unique device-edge split architecture allows for effective attack detection with minimal overhead on the IoT devices. We have extensively evaluated our system using a dataset of 3,973 traditional IoT malware samples and 8 types of sophisticated file-less attacks recently observed against IoT devices in our testbed. We report the evaluation results in terms of detection efficiency and computational.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {18},
numpages = {21},
keywords = {Intrusion detection, edge, AI, IoT security, malware}
}

@article{10.1145/3397520,
author = {Rangathunga, Dinesha and Roughan, Mathew and Nguyen, Hung},
title = {Mathematical Reconciliation of Medical Privacy Policies},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2158-656X},
url = {https://doi.org/10.1145/3397520},
doi = {10.1145/3397520},
abstract = {Healthcare data are arguably the most private of personal data belonging to a subject. This very private information in the wrong hands can lead to identity theft, prescription fraud, insurance fraud and an array of other crimes. Electronic-health systems such as My Health Record in Australia holds great promise in sharing medical data and improving healthcare quality. But, a key privacy issue in these systems is the misuse of healthcare data by `authorities'. The recent General Data Protection Regulation (GDPR) introduced in the EU aims to reduce personal-data misuse. But, there are no tools currently available to accurately reconcile a domestic E-health policy against the GDPR to identify discrepancies. Reconciling privacy policies is also non-trivial because policies are often written in free text, making them subject to human interpretation.  In this paper, we propose a tool which allows to describe E-health privacy policies granularly, represent them using formal constructs and make the policies precise and explicit. Using this formal framework, our tool can automatically reconcile a domestic E-health policy against the GDPR to identify violations and omissions. We use of our prototype to illustrate several critical flaws in Australia's My Health Record policy.},
journal = {ACM Trans. Manage. Inf. Syst.},
numpages = {1}
}

@article{10.1145/3391231,
author = {Kul, G\"{o}khan and Upadhyaya, Shambhu and Hughes, Andrew},
title = {An Analysis of Complexity of Insider Attacks to Databases},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2158-656X},
url = {https://doi.org/10.1145/3391231},
doi = {10.1145/3391231},
abstract = {Insider attacks are one of the most dangerous threats to an organization.  Unfortunately, they are very difficult to foresee, detect, and defend against due to the trust and responsibilities placed on the employees.  In this paper, we first define the notion of user intent, and construct a model for a common scenario that poses a very high risk for sensitive data stored in the organization's database.  We show that the complexity of identifying pseudo-intents of a user in this scenario is coNP-Complete, and launching a harvester insider attack within the boundaries of the defined threat model takes linear time while a targeted threat model is an NP-Complete problem.  We also discuss the general defense mechanisms against the modeled threats, and show that countering the harvester insider attack takes quadratic time while countering the targeted insider attack can take linear to quadratic time depending on the strategy chosen.  We analyze the adversarial behavior, and show that launching an attack with minimum risk is also an NP-Complete problem.  Finally, we perform timing experiments with the defense mechanisms on SQL query workloads collected from a national bank to test the feasibility of using these systems in real time.},
journal = {ACM Trans. Manage. Inf. Syst.},
numpages = {1}
}

@article{10.1145/3389685,
author = {Nokhbeh Zaeem, Razieh and Barber, K. Suzanne},
title = {The Effect of the GDPR on Privacy Policies: Recent Progress and Future Promise},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2158-656X},
url = {https://doi.org/10.1145/3389685},
doi = {10.1145/3389685},
abstract = {The General Data Protection Regulation (GDPR) is considered by some to be the most important change in data privacy regulation in 20 years. Effective May 2018, the European Union GDPR privacy law applies to any organization that collects and processes the personal information of EU citizens within or outside the EU. In this work, we seek to quantify the progress the GDPR has made in improving privacy policies around the globe. We leverage our data mining tool, PrivacyCheck, to automatically compare three corpora (totaling 550) of privacy policies, pre- and post-GDPR. In addition, to evaluate the current level of compliance with the GDPR around the globe, we manually studied the policies within two corpora (450 policies). We find that the GDPR has made progress in protecting user data, but more progress is necessary---particularly in the area of giving users the right to edit and delete their information---to entirely fulfill the GDPR's promise. We also observe that the GDPR encourages sharing user data with law enforcement, and, as a result, many policies have facilitated such sharing after the GDPR. Finally, we see that, when there is non-compliance with the GDPR, it is often in the form of failing to explicitly indicate compliance, showing an organization's lack of transparency and disclosure regarding their processing and protection of personal information. If Personal Identifiable Information (PII) is the ``currency of the Internet', these findings mark continued alarm regarding an individual's agency to protect and secure their PII assets.},
journal = {ACM Trans. Manage. Inf. Syst.},
numpages = {1}
}

@article{10.1145/3389684,
author = {Alagheband, Mahdi R. and Mashatan, Atefeh and Zihayat, Morteza},
title = {Time-Based Gap Analysis of Cybersecurity Trends in Academic and Digital Media},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3389684},
doi = {10.1145/3389684},
abstract = {This study analyzes cybersecurity trends and proposes a conceptual framework to identify cybersecurity topics of social interest and emerging topics that need to be addressed by researchers in the field. The insights drawn from this framework allow for a more proactive approach to identifying cybersecurity patterns and emerging threats that will ultimately improve the collective cybersecurity posture of the modern society. To achieve this, cybersecurity-oriented content in both media and academic corpora, disseminated between 2008 and 2018, were morphologically analyzed via text mining. A total of 3,556 academic papers obtained from the top-10 highly reputable cybersecurity academic conferences, and 4,163 news articles collected from the New York Times were processed. The LDA topic modeling followed optimal perplexity and coherence scores resulted in 12 trendy topics. Next, the time-based gap between these trendy topics was analyzed to measure the correlation between media and trendy academic topics. Both convergences and divergences between the two cybersecurity corpora were identified, suggesting a strong time-based correlation between these resources. This framework demonstrates the effective use of automated techniques to provide insights about cybersecurity topics of social interest and emerging trends and informs the direction of future academic research in this field.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {20},
numpages = {20},
keywords = {topic modeling, Cybersecurity trends, digital media, trend analysis, academic context}
}

@article{10.1145/3389683,
author = {Sudhakar, Tanuja and Gavrilova, Marina},
title = {Deep Learning for Multi-Instance Biometric Privacy},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {2158-656X},
url = {https://doi.org/10.1145/3389683},
doi = {10.1145/3389683},
abstract = {The fundamental goal of a revocable biometric system is to defend user?s biometrics from being compromised. This research explores an application of deep learning or Convolutional Neural Networks to multi-instance biometrics. Modality features are transformed into revocable templates through the application of random projection. During the user authentication phase, we employ Support Vector Machines, chosen over three other alternative classifiers after carrying out a comparative study. Comparison of the proposed method over  other standard deep learning models, and performance evaluation before and after revocability have also been discussed. Results demonstrate ability to improve identification accuracy and provide sound template security. The system was validated on three multi-instance fingervein and iris databases.},
journal = {ACM Trans. Manage. Inf. Syst.},
numpages = {1}
}

