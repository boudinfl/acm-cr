@article{10.1145/3394957,
author = {BENEVENTANO, DOMENICO and BERGAMASCHI, SONIA and Gagliardelli, Luca and Simonini, Giovanni},
title = {BLAST2: An Efficient Technique for Loose Schema Information Extraction from Heterogeneous Big Data Sources.},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {0},
number = {ja},
issn = {1936-1955},
url = {https://doi.org/10.1145/3394957},
doi = {10.1145/3394957},
abstract = {We present BLAST2, a novel technique to efficiently extract loose schema information, i.e., metadata that can serve as a surrogate of the schema alignment task within the Entity Resolution (ER) process --- to identify records that refer to the same real-world entity --- when integrating multiple, heterogeneous and voluminous data sources. The textit{loose schema information} is exploited for reducing the overall complexity of ER, whose na\"{\i}ve solution would imply O(n^2) comparisons, where n is the number of entity representations involved in the process and can be extracted by both structured and unstructured data sources. BLAST2 is completely unsupervised yet able to achieve almost the same precision and recall of supervised state-of-the-art schema alignment techniques when employed for Entity Resolution tasks, as shown in our experimental evaluation performed on two real-world data sets (composed of 7 and 10 data sources, respectively).},
journal = {J. Data and Information Quality},
numpages = {1}
}

@article{10.1145/3397462,
author = {Caruccio, Loredana and Cirillo, Stefano},
title = {Incremental Discovery of Imprecise Functional Dependencies},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3397462},
doi = {10.1145/3397462},
abstract = {Functional dependencies (fds) are one of the metadata used to assess data quality and to perform data cleaning operations. However, to pursue robustness with respect to data errors, it has been necessary to devise imprecise versions of functional dependencies, yielding relaxed functional dependencies (rfds). Among them, there exists the class of rfds relaxing on the extent, i.e., those admitting the possibility that an fd holds on a subset of data. In the literature, several algorithms to automatically discover rfds from big data collections have been defined. They achieve good performances with respect to the inherent problem complexity. However, most of them are capable of discovering rfds only by batch processing the entire dataset. This is not suitable in the era of big data, where the size of a database instance can grow with high-velocity, and the insertion of new data can invalidate previously holding rfds. Thus, it is necessary to devise incremental discovery algorithms capable of updating the set of holding rfds upon data insertions, without processing the entire dataset. To this end, in this article we propose an incremental discovery algorithm for rfds relaxing on the extent. It manages the validation of candidate rfds and the generation of possibly new rfd candidates upon the insertion of the new tuples, while limiting the size of the overall search space. Experimental results show that the proposed algorithm achieves extremely good performances on real-world datasets.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {19},
numpages = {25},
keywords = {tuple insertions, incremental discovery, Functional dependency, parallelism, discovery algorithm}
}

@article{10.1145/3388786,
author = {Blay, Karen Banahene and Yeomans, Steven and Demian, Peter and Murguia, Danny},
title = {The Information Resilience Framework: Vulnerabilities, Capabilities, and Requirements},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3388786},
doi = {10.1145/3388786},
abstract = {The quality of information is crucial to the success of asset delivery, management, and performance in the Digitised Architecture, Engineering, Construction, and Operations (DAECO) sector. The exposure and sensitivity of information to threats during its lifecycle leaves it vulnerable, affecting the intrinsic, relational, and security dimensions of information quality. A resilient information lifecycle perspective that identifies capabilities and requirements is therefore needed to assure information quality amid threats. This research develops and presents an information resilience (IR) framework by drawing on the theories of resilience, information quality, and vulnerability. In developing the framework, the critical incident technique was employed in interviewing 30 professionals (average of 40 minutes) in addition to reviewing seven project-documents across three digitally-driven infrastructure projects (making up 324 pages of data). The validated capabilities and requirements identified from this study have been collated into the framework and this highlights the need for cognitive-driven capabilities and process-driven requirements in DAECO.},
journal = {J. Data and Information Quality},
month = jul,
articleno = {14},
numpages = {25},
keywords = {information quality, capabilities, vulnerabilities, Information resilience}
}

@article{10.1145/3404193,
author = {Pitoura, Evaggelia},
title = {Social-Minded Measures of Data Quality: Fairness, Diversity, and Lack of Bias},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3404193},
doi = {10.1145/3404193},
abstract = {For decades, research in data-driven algorithmic systems has focused on improving efficiency (making data access faster and lighter) and effectiveness (providing relevant results to users). As data-driven decision making becomes prevalent, there is an increasing need for new measures for evaluating the quality of data systems. In this article, we make the case for social-minded measures, that is, measures that evaluate the effect of a system in society. We focus on three such measures, namely diversity (ensuring that all relevant aspects are represented), lack of bias (processing data without unjustifiable concentration on a particular side), and fairness (non discriminating treatment of data and people).},
journal = {J. Data and Information Quality},
month = jul,
articleno = {12},
numpages = {8},
keywords = {diversity, Fairness, bias}
}

@article{10.1145/3371925,
author = {Visengeriyeva, Larysa and Abedjan, Ziawasch},
title = {Anatomy of Metadata for Data Curation},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3371925},
doi = {10.1145/3371925},
abstract = {Real-world datasets often suffer from various data quality problems. Several data cleaning solutions have been proposed so far. However, data cleaning remains a manual and iterative task that requires domain and technical expertise. Exploiting metadata promises to improve the tedious process of data preparation, because data errors are detectable through metadata. This article investigates the intrinsic connection between metadata and data errors. In this work, we establish a mapping that reflects the connection between data quality issues and extractable metadata using qualitative and quantitative techniques. Additionally, we present a taxonomy based on a closed grammar that covers all existing metadata and allows the composition of novel types of metadata. We provide a case-study to show the practical application of the grammar for generating new metadata for data quality assessment.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {16},
numpages = {30},
keywords = {Data curation, error detection, metadata, data profiling, data quality}
}

@article{10.1145/3377878,
author = {Koumarelas, Ioannis and Jiang, Lan and Naumann, Felix},
title = {Data Preparation for Duplicate Detection},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3377878},
doi = {10.1145/3377878},
abstract = {Data errors represent a major issue in most application workflows. Before any important task can take place, a certain data quality has to be guaranteed by eliminating a number of different errors that may appear in data. Typically, most of these errors are fixed with data preparation methods, such as whitespace removal. However, the particular error of duplicate records, where multiple records refer to the same entity, is usually eliminated independently with specialized techniques. Our work is the first to bring these two areas together by applying data preparation operations under a systematic approach prior to performing duplicate detection.Our process workflow can be summarized as follows: It begins with the user providing as input a sample of the gold standard, the actual dataset, and optionally some constraints to domain-specific data preparations, such as address normalization. The preparation selection operates in two consecutive phases. First, to vastly reduce the search space of ineffective data preparations, decisions are made based on the improvement or worsening of pair similarities. Second, using the remaining data preparations an iterative leave-one-out classification process removes preparations one by one and determines the redundant preparations based on the achieved area under the precision-recall curve (AUC-PR). Using this workflow, we manage to improve the results of duplicate detection up to 19% in AUC-PR.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {15},
numpages = {24},
keywords = {record linkage, Data preparation, duplicate detection, data wrangling, similarity measures}
}

@article{10.1145/3328747,
author = {Colborne, Adrienne and Smit, Michael},
title = {Characterizing Disinformation Risk to Open Data in the Post-Truth Era},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3328747},
doi = {10.1145/3328747},
abstract = {Curated, labeled, high-quality data is a valuable commodity for tasks such as business analytics and machine learning. Open data is a common source of such data—for example, retail analytics draws on open demographic data, and weather forecast systems draw on open atmospheric and ocean data. Open data is released openly by governments to achieve various objectives, such as transparency, informing citizen engagement, or supporting private enterprise. Critical examination of ongoing social changes, including the post-truth phenomenon, suggests the quality, integrity, and authenticity of open data may be at risk. We introduce this risk through various lenses, describe some of the types of risk we expect using a threat model approach, identify approaches to mitigate each risk, and present real-world examples of cases where the risk has already caused harm. As an initial assessment of awareness of this disinformation risk, we compare our analysis to perspectives captured during open data stakeholder consultations in Canada.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {13},
numpages = {13},
keywords = {post-truth, risk mitigation, data quality assurance, risk identification, Open data, fake news}
}

@article{10.1145/3371315,
author = {Ahmadi, Naser and Huynh, Viet-Phi and Meduri, Vamsi and Ortona, Stefano and Papotti, Paolo},
title = {Mining Expressive Rules in Knowledge Graphs},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3371315},
doi = {10.1145/3371315},
abstract = {We describe RuDiK, an algorithm and a system for mining declarative rules over RDF knowledge graphs (KGs). RuDiK can discover rules expressing both positive relationships between KG elements, e.g., “if two persons share at least one parent, they are likely to be siblings,” and negative patterns identifying data contradictions, e.g., “if two persons are married, one cannot be the child of the other” or “the birth year for a person cannot be bigger than her graduation year.” While the first kind of rules identify new facts in the KG, the second kind enables the detection of incorrect triples and the generation of (training) negative examples for learning algorithms. High-quality rules are also critical for any reasoning task involving the KGs.Our approach increases the expressive power of the supported rule language w.r.t. the existing systems. RuDiK discovers rules containing (i) comparisons among literal values and (ii) selection conditions with constants. Richer rules increase the accuracy and the coverage over the facts in the KG for the task at hand. This is achieved with aggressive pruning of the search space and with disk-based algorithms, which enable the execution of the system in commodity machines. Also, RuDiK is robust to errors and missing data in the input graph. It discovers approximate rules with a measure of support that is aware of the quality issues. Our experimental evaluation with real-world KGs shows that RuDiK does better than existing solutions in terms of scalability and that it can identify effective rules for different target applications.},
journal = {J. Data and Information Quality},
month = may,
articleno = {8},
numpages = {27},
keywords = {Rule mining, knowledge graphs, graph dependencies}
}

@article{10.1145/3369875,
author = {Haller, Armin and Fern\'{a}ndez, Javier D. and Kamdar, Maulik R. and Polleres, Axel},
title = {What Are Links in Linked Open Data? A Characterization and Evaluation of Links between Knowledge Graphs on the Web},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3369875},
doi = {10.1145/3369875},
abstract = {Linked Open Data promises to provide guiding principles to publish interlinked knowledge graphs on the Web in the form of findable, accessible, interoperable, and reusable datasets. We argue that while as such, Linked Data may be viewed as a basis for instantiating the FAIR principles, there are still a number of open issues that cause significant data quality issues even when knowledge graphs are published as Linked Data. First, to define boundaries of single coherent knowledge graphs within Linked Data, a principled notion of what a dataset is, or, respectively, what links within and between datasets are, has been missing. Second, we argue that to enable FAIR knowledge graphs, Linked Data misses standardised findability and accessability mechanism via a single entry link. To address the first issue, we (i) propose a rigorous definition of a naming authority for a Linked Data dataset, (ii) define different link types for data in Linked datasets, (iii) provide an empirical analysis of linkage among the datasets of the Linked Open Data cloud, and (iv) analyse the dereferenceability of those links. We base our analyses and link computations on a scalable mechanism implemented on top of the HDT format, which allows us to analyse quantity and quality of different link types at scale.},
journal = {J. Data and Information Quality},
month = may,
articleno = {9},
numpages = {34},
keywords = {Linked Data, RDF}
}

@article{10.1145/3388748,
author = {Rula, Anisa and Zaveri, Amrapali and Simperl, Elena and Demidova, Elena},
title = {Editorial: Special Issue on Quality Assessment of Knowledge Graphs Dedicated to the Memory of Amrapali Zaveri},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3388748},
doi = {10.1145/3388748},
abstract = {This editorial summarizes the content of the Special Issue on Quality Assessment of Knowledge Graphs of the Journal of Data and Information Quality (JDIQ). We dedicate this special issue to the memory of our colleague and friend Amrapali Zaveri.},
journal = {J. Data and Information Quality},
month = apr,
articleno = {7},
numpages = {4},
keywords = {quality assessement, knowledge graphs, Linked Open Data}
}

@article{10.1145/3386687,
author = {Bertossi, Leopoldo and Geerts, Floris},
title = {Data Quality and Explainable AI},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3386687},
doi = {10.1145/3386687},
abstract = {In this work, we provide some insights and develop some ideas, with few technical details, about the role of explanations in Data Quality in the context of data-based machine learning models (ML). In this direction, there are, as expected, roles for causality, and explainable artificial intelligence. The latter area not only sheds light on the models, but also on the data that support model construction. There is also room for defining, identifying, and explaining errors in data, in particular, in ML, and also for suggesting repair actions. More generally, explanations can be used as a basis for defining dirty data in the context of ML, and measuring or quantifying them. We think dirtiness as relative to the ML task at hand, e.g., classification.},
journal = {J. Data and Information Quality},
month = apr,
articleno = {11},
numpages = {9},
keywords = {causes, fairness, Machine learning, bias}
}

@article{10.1145/3372750,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Content-Based Union and Complement Metrics for Dataset Search over RDF Knowledge Graphs},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3372750},
doi = {10.1145/3372750},
abstract = {RDF Knowledge Graphs (or Datasets) contain valuable information that can be exploited for a variety of real-world tasks. However, due to the enormous size of the available RDF datasets, it is difficult to discover the most valuable datasets for a given task. For improving dataset Discoverability, Interlinking, and Reusability, there is a trend for Dataset Search systems. Such systems are mainly based on metadata and ignore the contents; however, in tasks related to data integration and enrichment, the contents of datasets have to be considered. This is important for data integration but also for data enrichment, for instance, quite often datasets’ owners want to enrich the content of their dataset, by selecting datasets that provide complementary information for their dataset. The above tasks require content-based union and complement metrics between any subset of datasets; however, there is a lack of such approaches. For making feasible the computation of such metrics at very large scale, we propose an approach relying on (a) a set of pre-constructed (and periodically refreshed) semantics-aware indexes, and (b) “lattice-based” incremental algorithms that exploit the posting lists of such indexes, as well as set theory properties, for enabling efficient responses at query time. Finally, we discuss the efficiency of the proposed methods by presenting comparative results, and we report measurements for 400 real RDF datasets (containing over 2 billion triples), by exploiting the proposed metrics.},
journal = {J. Data and Information Quality},
month = apr,
articleno = {10},
numpages = {31},
keywords = {data integration, discoverability, dataset quality, enrichment, Dataset search, contextual connectivity, interlinking, lattice of measurements, linked data, relevancy, reusability}
}

@article{10.1145/3349536,
author = {Siagian, Al Hafiz Akbar Maulana and Aritsugi, Masayoshi},
title = {Robustness of Word and Character N-Gram Combinations in Detecting Deceptive and Truthful Opinions},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3349536},
doi = {10.1145/3349536},
abstract = {Opinions in reviews about the quality of products or services can be important information for readers. Unfortunately, such opinions may include deceptive ones posted for some business reasons. To keep the opinions as a valuable and trusted source of information, we propose an approach to detecting deceptive and truthful opinions. Specifically, we explore the use of word and character n-gram combinations, function words, and word syntactic n-grams (word sn-grams) as features for classifiers to deal with this task. We also consider applying word correction to our utilized dataset. Our experiments show that classification results of using the word and character n-gram combination features could perform better than those of employing other features. Although the experiments indicate that applying the word correction might be insignificant, we note that the deceptive opinions tend to have a smaller number of error words than the truthful ones. To examine robustness of our features, we then perform cross-classification tests. Our latter experiments results suggest that using the word and character n-gram combination features could work well in detecting deceptive and truthful opinions. Interestingly, the latter experimental results also indicate that using the word sn-grams as combination features could give good performance.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {5},
numpages = {24},
keywords = {spell correction, Deceptive opinions, robustness, fake reviews, word and character combinations, sn-grams}
}

@article{10.1145/3352591,
author = {Draisbach, Uwe and Christen, Peter and Naumann, Felix},
title = {Transforming Pairwise Duplicates to Entity Clusters for High-Quality Duplicate Detection},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3352591},
doi = {10.1145/3352591},
abstract = {Duplicate detection algorithms produce clusters of database records, each cluster representing a single real-world entity. As most of these algorithms use pairwise comparisons, the resulting (transitive) clusters can be inconsistent: Not all records within a cluster are sufficiently similar to be classified as duplicate. Thus, one of many subsequent clustering algorithms can further improve the result.We explain in detail, compare, and evaluate many of these algorithms and introduce three new clustering algorithms in the specific context of duplicate detection. Two of our three new algorithms use the structure of the input graph to create consistent clusters. Our third algorithm, and many other clustering algorithms, focus on the edge weights, instead. For evaluation, in contrast to related work, we experiment on true real-world datasets, and in addition examine in great detail various pair-selection strategies used in practice. While no overall winner emerges, we are able to identify best approaches for different situations. In scenarios with larger clusters, our proposed algorithm, Extended Maximum Clique Clustering (EMCC), and Markov Clustering show the best results. EMCC especially outperforms Markov Clustering regarding the precision of the results and additionally has the advantage that it can also be used in scenarios where edge weights are not available.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {3},
numpages = {30},
keywords = {entity resolution, data matching, deduplication, clustering, Record linkage}
}

@article{10.1145/3362121,
author = {Firmani, Donatella and Tanca, Letizia and Torlone, Riccardo},
title = {Ethical Dimensions for Data Quality},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3362121},
doi = {10.1145/3362121},
journal = {J. Data and Information Quality},
month = dec,
articleno = {2},
numpages = {5},
keywords = {source selection, knowledge extraction, Data integration}
}

@article{10.1145/3356901,
author = {Shakeel, Yusra and Kr\"{u}ger, Jacob and Nostitz-Wallwitz, Ivonne Von and Saake, Gunter and Leich, Thomas},
title = {Automated Selection and Quality Assessment of Primary Studies: A Systematic Literature Review},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3356901},
doi = {10.1145/3356901},
abstract = {Researchers use&nbsp;systematic literature reviews (SLRs) to synthesize existing evidence regarding a research topic. While being an important means to condense knowledge, conducting an SLR requires a large amount of time and effort. Consequently, researchers have proposed semi-automatic techniques to support different stages of the review process. Two of the most time-consuming tasks are (1) to select primary studies and (2) to assess their quality. In this article, we report an SLR in which we identify, discuss, and synthesize existing techniques of the software-engineering domain that aim to semi-automate these two tasks. Instead of solely providing statistics, we discuss these techniques in detail and compare them, aiming to improve our understanding of supported and unsupported activities. To this end, we identified eight primary studies that report unique techniques that have been published between 2007 and 2016. Most of these techniques rely on text mining and can be beneficial for researchers, but an independent validation using real&nbsp;SLRs is missing for most of them. Moreover, the results indicate the necessity of developing more reliable techniques, providing access to their implementations, and extending their scope to further activities to facilitate the selection and quality assessment of primary studies.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {4},
numpages = {26},
keywords = {tertiary study, software engineering, Systematic literature review, primary study assessment, quality assessment}
}

@article{10.1145/3341107,
author = {Aswani, Reema and Kar, Arpan Kumar and Ilavarasan, P. Vigneswara},
title = {Experience: Managing Misinformation in Social Media—Insights for Policymakers from Twitter Analytics},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3341107},
doi = {10.1145/3341107},
abstract = {Governance of misinformation is a serious concern in social media platforms. Based on experiences gathered from different case studies, we offer insights for the policymakers on managing misinformation in social media. These platforms are widely used for not just communication but also content consumption. Managing misinformation is thus a challenge for policymakers and the platforms. This article explores the factors of rapid propagation of misinformation based on our experiences in the domain. An average of about 1.5 million tweets were analysed in each of the three different cases surrounding misinformation. The findings indicate that the tweet emotion and polarity plays a significant role in determining whether the shared content is authentic or not. A deeper exploration highlights that a higher element of surprise combined with other emotions is present in such tweets. Further, the tweets that show case-neutral content often lack the possibilities of virality when it comes to misinformation. The second case explores whether the misinformation is being propagated intentionally by means of the identified fake profiles or it is done by authentic users, which can also be either intentional, for gaining attention, or unintentional, under the assumption that the information is correct. Last, network attributes, including topological analysis, community, and centrality analysis, also catalyze the propagation of misinformation. Policymakers can utilize these findings in this experience study for the governance of misinformation. Tracking and disruption in any one of the identified drivers could act as a control mechanism to manage misinformation propagation.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {6},
numpages = {18},
keywords = {Misinformation, social media, network science, information propagation, Twitter analytics}
}

@article{10.1145/3326920,
author = {Milo, Tova},
title = {Getting Rid of Data},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3326920},
doi = {10.1145/3326920},
abstract = {We are experiencing an amazing data-centered revolution. Incredible amounts of data are collected, integrated, and analyzed, leading to key breakthroughs in science and society. This well of knowledge, however, is at a great risk if we do not dispense with some of the data flood. First, the amount of generated data grows exponentially and already at 2020 is expected to be more than twice the available storage. Second, even disregarding storage constraints, uncontrolled data retention risks privacy and security, as recognized, e.g., by the recent EU Data Protection reform. Data disposal policies must be developed to benefit and protect organizations and individuals.Retaining the knowledge hidden in the data while respecting storage, processing, and regulatory constraints is a great challenge. The difficulty stems from the distinct, intricate requirements entailed by each type of constraint, the scale and velocity of data, and the constantly evolving needs. While multiple data sketching, summarization, and deletion techniques were developed to address specific aspects of the problem, we are still very far from a comprehensive solution. Every organization has to battle the same tough challenges with ad hoc solutions that are application-specific and rarely sharable.In this article, we will discuss the logical, algorithmic, and methodological foundations required for the systematic disposal of large-scale data, for constraints enforcement and for the development of applications over the retained information. In particular, we will overview relevant related work, highlighting new research challenges and potential reuse of existing techniques.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {1},
numpages = {7},
keywords = {query answering, data management, data retention, Data disposal}
}

@article{10.1145/3309682,
author = {Akhtar, Zahaib and Le, Anh Minh and Nam, Yun Seong and Chen, Jessica and Govindan, Ramesh and Katz-Bassett, Ethan and Rao, Sanjay and Zhan, Jibin},
title = {Improving Adaptive Video Streaming through Session Classification},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3309682},
doi = {10.1145/3309682},
abstract = {With internet video gaining increasing popularity and soaring to dominate network traffic, extensive studies are being carried out on how to achieve higher Quality of Experience (QoE) with the delivery of video content. Associated with the chunk-based streaming protocol, Adaptive Bitrate (ABR) algorithms have recently emerged to cope with the diverse and fluctuating network conditions by dynamically adjusting bitrates for future chunks. This inevitably involves predicting the future throughput of a video session. Some of the session features like Internet Service Provider (ISP), geographical location, and so on, could affect network conditions and contain helpful information for this throughput prediction. In this article, we consider how our knowledge about the session features can be utilized to improve ABR quality via customized parameter settings. We present our ABR-independent, QoE-driven, feature-based partition method to classify the logged video sessions so that different parameter settings could be adopted in different situations to reach better quality. A variation of Decision Tree is developed for the classification and has been applied to a sample ABR for evaluation. The experiment shows that our approach can improve the average bitrate of the sample ABR by 36.1% without causing the increase of the rebuffering ratio where 99% of the sessions can get improvement. It can also improve the rebuffering ratio by 87.7% without causing the decrease of the average bitrate, where, among those sessions involved in rebuffering, 82% receives improvement and 18% remains the same.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {21},
numpages = {29},
keywords = {adaptive bitrate algorithms, Video delivery}
}

@article{10.1145/3313788,
author = {Fard, Amir Ebrahimi and Cunningham, Scott},
title = {Assessing the Readiness of Academia in the Topic of False and Unverified Information},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3313788},
doi = {10.1145/3313788},
abstract = {The spread of false and unverified information has the potential to inflict damage by harming the reputation of individuals or organisations, shaking financial markets, and influencing crowd decisions in important events. This phenomenon needs to be properly curbed, otherwise it can contaminate other aspects of our social life. In this regard, academia as a key institution against false and unverified information is expected to play a pivotal role. Despite a great deal of research in this arena, the amount of progress by academia is not clear yet. This can lead to misjudgements about the performance of the topic of interest that can ultimately result in wrong science policies regarding academic efforts for quelling false and unverified information. In this research, we address this issue by assessing the readiness of academia in the topic of false and unverified information. To this end, we adopt the emergence framework and measure its dimensions (novelty, growth, coherence, and impact) over more than 21,000 articles published by academia about false and unverified information. Our results show the current body of research has had organic growth so far, which is not promising enough for confronting the problem of false and unverified information. To tackle this problem, we suggest an external push strategy that, compared to the early stages of the topic of interest, reinforces the emergence dimensions and leads to a higher level in every dimension.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {17},
numpages = {27},
keywords = {scientific field, misinformation, fake-news, emergence, False information, unverified information, rumour}
}

@article{10.1145/3312750,
author = {Bertino, Elisa and Kundu, Ahish and Sura, Zehra},
title = {Data Transparency with Blockchain and AI Ethics},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3312750},
doi = {10.1145/3312750},
abstract = {Providing a 360° view of a given data item especially for sensitive data is essential toward not only protecting the data and associated privacy but also assuring trust, compliance, and ethics of the systems that use or manage such data. With the advent of General Data Protection Regulation, California Data Privacy Law, and other such regulatory requirements, it is essential to support data transparency in all such dimensions. Moreover, data transparency should not violate privacy and security requirements. In this article, we put forward a vision for how data transparency would be achieved in a de-centralized fashion using blockchain technology.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {16},
numpages = {8},
keywords = {privacy, accountability, Big data, data provenance}
}

@article{10.1145/3339468,
author = {Babcock, Matthew and Beskow, David M. and Carley, Kathleen M.},
title = {Different Faces of False: The Spread and Curtailment of False Information in the <i>Black Panther</i> Twitter Discussion},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3339468},
doi = {10.1145/3339468},
abstract = {The task of combating false information online appears daunting, in part due to a public focus on how quickly it can spread and the clamor for automated platform-based interventions. While such concerns can be warranted, threat analysis and intervention design both benefit from a fuller understanding of different types of false information and of the community responses to them. Here, we present a study of the most tweeted about movie ever (Black Panther) in which the spread of false information of four different types is compared to the ad hoc Twitter community response. We find that (1) false information tweets played a small part in the overall conversation, (2) community-based debunking and shaming responses to false posts about attacks at theaters overwhelmed such posts by orders of magnitude, (3) as another form of community response, one type of false narrative (Satire) was used to attack another (Fake Attacks), and (4) the four types of false-information tweets differed in the use of hashtags and in the role played by originating users and responding users. Overall, this work helps to illustrate the importance of investigating “on-the-ground” community responses to fake news and other types of digital false information and to inform identification and intervention design and implementation.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {18},
numpages = {15},
keywords = {Social media analytics, dynamic network analysis, social networks, disinformation, fake news}
}

@article{10.1145/3317573,
author = {Ding, Junhua and Li, Xinchuan and Kang, Xiaojun and Gudivada, Venkat N.},
title = {A Case Study of the Augmentation and Evaluation of Training Data for Deep Learning},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3317573},
doi = {10.1145/3317573},
abstract = {Deep learning has been widely used for extracting values from big data. As many other machine learning algorithms, deep learning requires significant training data. Experiments have shown both the volume and the quality of training data can significantly impact the effectiveness of the value extraction. In some cases, the volume of training data is not sufficiently large for effectively training a deep learning model. In other cases, the quality of training data is not high enough to achieve the optimal performance. Many approaches have been proposed for augmenting training data to mitigate the deficiency. However, whether the augmented data are “fit for purpose” of deep learning is still a question. A framework for comprehensively evaluating the effectiveness of the augmented data for deep learning is still not available. In this article, we first discuss a data augmentation approach for deep learning. The approach includes two components: the first one is to remove noisy data in a dataset using a machine learning based classification to improve its quality, and the second one is to increase the volume of the dataset for effectively training a deep learning model. To evaluate the quality of the augmented data in fidelity, variety, and veracity, a data quality evaluation framework is proposed. We demonstrated the effectiveness of the data augmentation approach and the data quality evaluation framework through studying an automated classification of biology cell images using deep learning. The experimental results clearly demonstrated the impact of the volume and quality of training data to the performance of deep learning and the importance of the data quality evaluation. The data augmentation approach and the data quality evaluation framework can be straightforwardly adapted for deep learning study in other domains.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {20},
numpages = {22},
keywords = {support vector machine, deep learning, convolutional neural network, machine learning, Data quality, diffraction image}
}

@article{10.1145/3328746,
author = {Bosu, Michael F. and Macdonell, Stephen G.},
title = {Experience: Quality Benchmarking of Datasets Used in Software Effort Estimation},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3328746},
doi = {10.1145/3328746},
abstract = {Data is a cornerstone of empirical software engineering (ESE) research and practice. Data underpin numerous process and project management activities, including the estimation of development effort and the prediction of the likely location and severity of defects in code. Serious questions have been raised, however, over the quality of the data used in ESE. Data quality problems caused by noise, outliers, and incompleteness have been noted as being especially prevalent. Other quality issues, although also potentially important, have received less attention. In this study, we assess the quality of 13 datasets that have been used extensively in research on software effort estimation. The quality issues considered in this article draw on a taxonomy that we published previously based on a systematic mapping of data quality issues in ESE. Our contributions are as follows: (1) an evaluation of the “fitness for purpose” of these commonly used datasets and (2) an assessment of the utility of the taxonomy in terms of dataset benchmarking. We also propose a template that could be used to both improve the ESE data collection/submission process and to evaluate other such datasets, contributing to enhanced awareness of data quality issues in the ESE community and, in time, the availability and use of higher-quality datasets.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {19},
numpages = {38},
keywords = {missing data, empirical software engineering, software effort estimation, benchmarking, noise, Data quality}
}

@article{10.1145/3287763,
author = {Borges, Lu\'{\i}s and Martins, Bruno and Calado, P\'{a}vel},
title = {Combining Similarity Features and Deep Representation Learning for Stance Detection in the Context of Checking Fake News},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3287763},
doi = {10.1145/3287763},
abstract = {Fake news is nowadays an issue of pressing concern, given its recent rise as a potential threat to high-quality journalism and well-informed public discourse. The Fake News Challenge (FNC-1) was organized in early 2017 to encourage the development of machine-learning-based classification systems for stance detection (i.e., for identifying whether a particular news article agrees, disagrees, discusses, or is unrelated to a particular news headline), thus helping in the detection and analysis of possible instances of fake news. This article presents a novel approach to tackle this stance detection problem, based on the combination of string similarity features with a deep neural network architecture that leverages ideas previously advanced in the context of learning-efficient text representations, document classification, and natural language inference. Specifically, we use bi-directional Recurrent Neural Networks (RNNs), together with max-pooling over the temporal/sequential dimension and neural attention, for representing (i) the headline, (ii) the first two sentences of the news article, and (iii) the entire news article. These representations are then combined/compared, complemented with similarity features inspired on other FNC-1 approaches, and passed to a final layer that predicts the stance of the article toward the headline. We also explore the use of external sources of information, specifically large datasets of sentence pairs originally proposed for training and evaluating natural language inference methods to pre-train specific components of the neural network architecture (e.g., the RNNs used for encoding sentences). The obtained results attest to the effectiveness of the proposed ideas and show that our model, particularly when considering pre-training and the combination of neural representations together with similarity features, slightly outperforms the previous state of the art.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {14},
numpages = {26},
keywords = {stance detection, natural language processing, recurrent neural networks, Fake news, fact checking, deep learning}
}

@article{10.1145/3310231,
author = {Abiteboul, Serge and Stoyanovich, Julia},
title = {Transparency, Fairness, Data Protection, Neutrality: Data Management Challenges in the Face of New Regulation},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3310231},
doi = {10.1145/3310231},
abstract = {The data revolution continues to transform every sector of science, industry, and government. Due to the incredible impact of data-driven technology on society, we are becoming increasingly aware of the imperative to use data and algorithms responsibly—in accordance with laws and ethical norms. In this article, we discuss three recent regulatory frameworks: the European Union’s General Data Protection Regulation (GDPR), the New York City Automated Decisions Systems (ADS) Law, and the Net Neutrality principle, which aim to protect the rights of individuals who are impacted by data collection and analysis. These frameworks are prominent examples of a global trend: Governments are starting to recognize the need to regulate data-driven algorithmic technology.Our goal in this article is to bring these regulatory frameworks to the attention of the data management community and to underscore the technical challenges they raise and that we, as a community, are well-equipped to address. The main takeaway of this article is that legal and ethical norms cannot be incorporated into data-driven systems as an afterthought. Rather, we must think in terms of responsibility by design, viewing it as a systems requirement.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {15},
numpages = {9},
keywords = {Transparency, data protection, responsible data science, fairness, neutrality}
}

@article{10.1145/3305258,
author = {Xue, Hao and Wang, Qiaozhi and Luo, Bo and Seo, Hyunjin and Li, Fengjun},
title = {Content-Aware Trust Propagation Toward Online Review Spam Detection},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3305258},
doi = {10.1145/3305258},
abstract = {With the increasing popularity of online review systems, a large volume of user-generated content becomes available to help people make reasonable judgments about the quality of services and products from unknown providers. However, these platforms are frequently abused since fraudulent information can be freely inserted by potentially malicious users without validation. Consequently, online review systems become targets of individual and professional spammers, who insert deceptive reviews by manipulating the rating and/or the content of the reviews.In this work, we propose a review spamming detection scheme based on the deviation between the aspect-specific opinions extracted from individual reviews and the aggregated opinions on the corresponding aspects. In particular, we model the influence on the trustworthiness of the user due to his opinion deviations from the majority in the form of a deviation-based penalty, and integrate this penalty into a three-layer trust propagation framework to iteratively compute the trust scores for users, reviews, and review targets, respectively. The trust scores are effective indicators of spammers, since they reflect the overall deviation of a user from the aggregated aspect-specific opinions across all targets and all aspects. Experiments on the dataset collected from Yelp.com show that the proposed detection scheme based on aspect-specific content-aware trust propagation is able to measure users’ trustworthiness based on opinions expressed in reviews.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {11},
numpages = {31},
keywords = {Spam detection, opinion mining, trust propagation, social networks}
}

@article{10.1145/3297722,
author = {Atanasova, Pepa and Nakov, Preslav and M\`{a}rquez, Llu\'{\i}s and Barr\'{o}n-Cede\~{n}o, Alberto and Karadzhov, Georgi and Mihaylova, Tsvetomila and Mohtarami, Mitra and Glass, James},
title = {Automatic Fact-Checking Using Context and Discourse Information},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3297722},
doi = {10.1145/3297722},
abstract = {We study the problem of automatic fact-checking, paying special attention to the impact of contextual and discourse information. We address two related tasks: (i)&nbsp;detecting check-worthy claims and (ii)&nbsp;fact-checking claims. We develop supervised systems based on neural networks, kernel-based support vector machines, and combinations thereof, which make use of rich input representations in terms of discourse cues and contextual features. For the check-worthiness estimation task, we focus on political debates, and we model the target claim in the context of the full intervention of a participant and the previous and following turns in the debate, taking into account contextual meta information. For the fact-checking task, we focus on answer verification in a community forum, and we model the veracity of the answer with respect to the entire question–answer thread in which it occurs as well as with respect to other related posts from the entire forum. We develop annotated datasets for both tasks and we run extensive experimental evaluation, confirming that both types of information—but especially contextual features—play an important role.},
journal = {J. Data and Information Quality},
month = may,
articleno = {12},
numpages = {27},
keywords = {community question-answering, Fact-checking, discourse}
}

@article{10.1145/3321484,
author = {Hassan, Naeemul and Li, Chengkai and Yang, Jun and Yu, Cong},
title = {Introduction to the Special Issue on Combating Digital Misinformation and Disinformation},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3321484},
doi = {10.1145/3321484},
journal = {J. Data and Information Quality},
month = may,
articleno = {9},
numpages = {3}
}

@article{10.1145/3297720,
author = {M\"{u}ller, Daniel and Jain, Pratiksha and Te, Yieh-Funk},
title = {Augmenting Data Quality through High-Precision Gender Categorization},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3297720},
doi = {10.1145/3297720},
abstract = {Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies’ outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations’ records, if the gender attribute is missing or unreliable.},
journal = {J. Data and Information Quality},
month = may,
articleno = {8},
numpages = {18},
keywords = {gender name mapping, patenting, Data quality improvement, record completion}
}

@article{10.1145/3286488,
author = {Lin, Peng and Song, Qi and Wu, Yinghui and Pi, Jiaxing},
title = {Discovering Patterns for Fact Checking in Knowledge Graphs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3286488},
doi = {10.1145/3286488},
abstract = {This article presents a new framework that incorporates graph patterns to support fact checking in knowledge graphs. Our method discovers discriminant graph patterns to construct classifiers for fact prediction. First, we propose a class of graph fact checking rules (GFCs). A GFC incorporates graph patterns that best distinguish true and false facts of generalized fact statements. We provide statistical measures to characterize useful patterns that are both discriminant and diversified. Second, we show that it is feasible to discover GFCs in large graphs with optimality guarantees. We develop an algorithm that performs localized search to generate a stream of graph patterns, and dynamically assemble the best GFCs from multiple GFC sets, where each set ensures quality scores within certain ranges. The algorithm guarantees a (1/2−ϵ) approximation when it (early) terminates. We also develop a space-efficient alternative that dynamically spawns prioritized patterns with best marginal gains to the verified GFCs. It guarantees a (1−1/e) approximation. Both strategies guarantee a bounded time cost independent of the size of the underlying graph. Third, to support fact checking, we develop two classifiers, which make use of top-ranked GFCs as predictive rules or instance-level features of the pattern matches induced by GFCs, respectively. Using real-world data, we experimentally verify the efficiency and the effectiveness of GFC-based techniques for fact checking in knowledge graphs and verify its application in knowledge exploration and news prediction.},
journal = {J. Data and Information Quality},
month = may,
articleno = {13},
numpages = {27},
keywords = {Fact checking, supervised graph pattern mining, knowledge graph}
}

@article{10.1145/3309699,
author = {Zannettou, Savvas and Sirivianos, Michael and Blackburn, Jeremy and Kourtellis, Nicolas},
title = {The Web of False Information: Rumors, Fake News, Hoaxes, Clickbait, and Various Other Shenanigans},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3309699},
doi = {10.1145/3309699},
abstract = {A new era of Information Warfare has arrived. Various actors, including state-sponsored ones, are weaponizing information on Online Social Networks to run false-information campaigns with targeted manipulation of public opinion on specific topics. These false-information campaigns can have dire consequences to the public: mutating their opinions and actions, especially with respect to critical world events like major elections. Evidently, the problem of false information on the Web is a crucial one and needs increased public awareness as well as immediate attention from law enforcement agencies, public institutions, and in particular, the research community.In this article, we make a step in this direction by providing a typology of the Web’s false-information ecosystem, composed of various types of false-information, actors, and their motives. We report a comprehensive overview of existing research on the false-information ecosystem by identifying several lines of work: (1) how the public perceives false information; (2) understanding the propagation of false information; (3) detecting and containing false information on the Web; and (4) false information on the political stage. In this work, we pay particular attention to political false information as: (1) it can have dire consequences to the community (e.g., when election results are mutated) and (2) previous work shows that this type of false information propagates faster and further when compared to other types of false information. Finally, for each of these lines of work, we report several future research directions that can help us better understand and mitigate the emerging problem of false-information dissemination on the Web.},
journal = {J. Data and Information Quality},
month = may,
articleno = {10},
numpages = {37},
keywords = {fake news, social networks, Survey, hoaxes, clickbait, rumors, false information}
}

@article{10.1145/3297721,
author = {Sillaber, Christian and Mussmann, Andrea and Breu, Ruth},
title = {Experience: Data and Information Quality Challenges in Governance, Risk, and Compliance Management},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3297721},
doi = {10.1145/3297721},
abstract = {Governance, risk, and compliance (GRC) managers often struggle to document the current state of their organizations. This is due to the complexity of their IS landscape, the complex regulatory and organizational environment, and the frequent changes to both. GRC tools seek to support them by integrating existing information sources. However, a comprehensive analysis of how the data is managed in such tools, as well as the impact of data quality, is still missing. To build a basis of empirical data, we conducted a series of interviews with information security managers responsible for GRC management activities in their organizations. The results of a qualitative content analysis of these interviews suggest that decision makers largely depend on high-quality documentation but struggle to maintain their documentation at the required level for long periods of time. This work discusses factors affecting the quality of GRC data and information and provides insights into approaches implemented by organizations to analyze, improve, and maintain the quality of their GRC data and information.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {6},
numpages = {14},
keywords = {information quality, Data quality, management system, Governance risk and compliance}
}

@article{10.1145/3301294,
author = {Lazar, Alina and Jin, Ling and Spurlock, C. Anna and Wu, Kesheng and Sim, Alex and Todd, Annika},
title = {Evaluating the Effects of Missing Values and Mixed Data Types on Social Sequence Clustering Using T-SNE Visualization},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3301294},
doi = {10.1145/3301294},
abstract = {The goal of this work is to investigate the impact of missing values in clustering joint categorical social sequences. Identifying patterns in sociodemographic longitudinal data is important in a number of social science settings. However, performing analytical operations, such as clustering on life course trajectories, is challenging due to the categorical and multidimensional nature of the data, their mixed data types, and corruption by missing and inconsistent values. Data quality issues were investigated previously on single variable sequences. To understand their effects on multivariate sequence analysis, we employ a dataset of mixed data types and missing values, a dissimilarity measure designed for joint categorical sequence data, together with dimensionality reduction methodologies in a systematic design of sequence clustering experiments. Given the categorical nature of our data, we employ an “edit” distance using optimal matching. Because each data record has multiple variables of different types, we investigate the impact of mixing these variables in a single dissimilarity measure. Between variables with binary values and those with multiple nominal values, we find that the ability to overcome missing data problems is more difficult in the nominal domain than in the binary domain. Additionally, alignment of leading missing values can result in systematic biases in dissimilarity matrices and subsequently introduce both artificial clusters and unrealistic interpretations of associated data domains. We demonstrate the usage of t-distributed stochastic neighborhood embedding to visually guide mitigation of such biases by tuning the missing value substitution cost parameter or determining an optimal sequence span.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {7},
numpages = {22},
keywords = {missing values, time series clustering, life trajectories, t-SNE, optimal matching, dimensionality reduction, data quality, Joint sequence analysis}
}

@article{10.1145/3310230,
author = {Fan, Wenfei},
title = {Dependencies for Graphs: Challenges and Opportunities},
year = {2019},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3310230},
doi = {10.1145/3310230},
abstract = {What are graph dependencies? What do we need them for? What new challenges do they introduce? This article tackles these questions. It aims to incite curiosity and interest in this emerging area of research.},
journal = {J. Data and Information Quality},
month = feb,
articleno = {5},
numpages = {12},
keywords = {Dependencies, validation, implication, error detection, satisfiability, certain fixes, dependency discovery, graphs}
}

@article{10.1145/3242095,
author = {Dallachiesa, Michele and Aggarwal, Charu C. and Palpanas, Themis},
title = {Improving Classification Quality in Uncertain Graphs},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3242095},
doi = {10.1145/3242095},
abstract = {In many real applications that use and analyze networked data, the links in the network graph may be erroneous or derived from probabilistic techniques. In such cases, the node classification problem can be challenging, since the unreliability of the links may affect the final results of the classification process. If the information about link reliability is not used explicitly, then the classification accuracy in the underlying network may be affected adversely. In this article, we focus on situations that require the analysis of the uncertainty that is present in the graph structure. We study the novel problem of node classification in uncertain graphs, by treating uncertainty as a first-class citizen. We propose two techniques based on a Bayes model and automatic parameter selection and show that the incorporation of uncertainty in the classification process as a first-class citizen is beneficial. We experimentally evaluate the proposed approach using different real data sets and study the behavior of the algorithms under different conditions. The results demonstrate the effectiveness and efficiency of our approach.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {3},
numpages = {20},
keywords = {label propagation, Network classification, structural classification}
}

@article{10.1145/3274655,
author = {Casey, K. Michael and Jr., Kevin Casey},
title = {Financial Regulatory and Risk Management Challenges Stemming from Firm-Specific Digital Misinformation},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3274655},
doi = {10.1145/3274655},
journal = {J. Data and Information Quality},
month = jan,
articleno = {4},
numpages = {4},
keywords = {social computing, regulatory systems, Digital misinformation, machine learning}
}

@article{10.1145/3284934,
author = {R\'{\i}os, Julio C\'{e}sar Cort\'{e}s and Paton, Norman W. and Fernandes, Alvaro A. A. and Abel, Edward and Keane, John A.},
title = {Crowdsourced Targeted Feedback Collection for Multicriteria Data Source Selection},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3284934},
doi = {10.1145/3284934},
abstract = {A multicriteria data source selection (MCSS) scenario identifies, from a set of candidate data sources, the subset that best meets users’ needs. These needs are expressed using several criteria, which are used to evaluate the candidate data sources. An MCSS problem can be solved using multidimensional optimization techniques that trade off the different objectives. Sometimes one may have uncertain knowledge regarding how well the candidate data sources meet the criteria. In order to overcome this uncertainty, one may rely on end-users or crowds to annotate the data items produced by the sources in relation to the selection criteria. In this article, a proposed Targeted Feedback Collection (TFC) approach is introduced that aims to identify those data items on which feedback should be collected, thereby providing evidence on how the sources satisfy the required criteria. The proposed TFC targets feedback by considering the confidence intervals around the estimated criteria values, with a view to increasing the confidence in the estimates that are most relevant to the multidimensional optimization. Variants of the proposed TFC approach have been developed for use where feedback is expected to be reliable (e.g., where it is provided by trusted experts) and where feedback is expected to be unreliable (e.g., from crowd workers). Both variants have been evaluated, and positive results are reported against other approaches to feedback collection, including active learning, in experiments that involve real-world datasets and crowdsourcing.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {2},
numpages = {27},
keywords = {single-objective optimization, pay as you go, Source selection, feedback collection, uncertainty handling, multiobjective optimization, crowdsourcing}
}

@article{10.1145/3287168,
author = {Srivastava, Divesh and Scannapieco, Monica and Redman, Thomas C.},
title = {Ensuring High-Quality Private Data for Responsible Data Science: Vision and Challenges},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3287168},
doi = {10.1145/3287168},
abstract = {High-quality data is critical for effective data science. As the use of data science has grown, so too have concerns that individuals’ rights to privacy will be violated. This has led to the development of data protection regulations around the globe and the use of sophisticated anonymization techniques to protect privacy. Such measures make it more challenging for the data scientist to understand the data, exacerbating issues of data quality. Responsible data science aims to develop useful insights from the data while fully embracing these considerations.We pose the high-level problem in this article, “How can a data scientist develop the needed trust that private data has high quality?” We then identify a series of challenges for various data-centric communities and outline research questions for data quality and privacy researchers, which would need to be addressed to effectively answer the problem posed in this article.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {1},
numpages = {9},
keywords = {quality of private data, Responsible data science, private data, data trust}
}

@article{10.1145/3268410,
author = {Ferro, Nicola and Fuhr, Norbert and Rauber, Andreas},
title = {Introduction to the Special Issue on Reproducibility in Information Retrieval: Tools and Infrastructures},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3268410},
doi = {10.1145/3268410},
journal = {J. Data and Information Quality},
month = oct,
articleno = {14},
numpages = {4},
keywords = {Reproducibility}
}

@article{10.1145/3239570,
author = {Hopfgartner, Frank and Hanbury, Allan and M\"{u}ller, Henning and Eggel, Ivan and Balog, Krisztian and Brodt, Torben and Cormack, Gordon V. and Lin, Jimmy and Kalpathy-Cramer, Jayashree and Kando, Noriko and Kato, Makoto P. and Krithara, Anastasia and Gollub, Tim and Potthast, Martin and Viegas, Evelyne and Mercer, Simon},
title = {Evaluation-as-a-Service for the Computational Sciences: Overview and Outlook},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3239570},
doi = {10.1145/3239570},
abstract = {Evaluation in empirical computer science is essential to show progress and assess technologies developed. Several research domains such as information retrieval have long relied on systematic evaluation to measure progress: here, the Cranfield paradigm of creating shared test collections, defining search tasks, and collecting ground truth for these tasks has persisted up until now. In recent years, however, several new challenges have emerged that do not fit this paradigm very well: extremely large data sets, confidential data sets as found in the medical domain, and rapidly changing data sets as often encountered in industry. Crowdsourcing has also changed the way in which industry approaches problem-solving with companies now organizing challenges and handing out monetary awards to incentivize people to work on their challenges, particularly in the field of machine learning.This article is based on discussions at a workshop on Evaluation-as-a-Service (EaaS). EaaS is the paradigm of not providing data sets to participants and have them work on the data locally, but keeping the data central and allowing access via Application Programming Interfaces (API), Virtual Machines (VM), or other possibilities to ship executables. The objectives of this article are to summarize and compare the current approaches and consolidate the experiences of these approaches to outline the next steps of EaaS, particularly toward sustainable research infrastructures.The article summarizes several existing approaches to EaaS and analyzes their usage scenarios and also the advantages and disadvantages. The many factors influencing EaaS are summarized, and the environment in terms of motivations for the various stakeholders, from funding agencies to challenge organizers, researchers and participants, to industry interested in supplying real-world problems for which they require solutions.EaaS solves many problems of the current research environment, where data sets are often not accessible to many researchers. Executables of published tools are equally often not available making the reproducibility of results impossible. EaaS, however, creates reusable/citable data sets as well as available executables. Many challenges remain, but such a framework for research can also foster more collaboration between researchers, potentially increasing the speed of obtaining research results.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {15},
numpages = {32},
keywords = {benchmarking, information access systems, Evaluation-as-a-service}
}

@article{10.1145/3242180,
author = {Roy, Dwaipayan and Mitra, Mandar and Ganguly, Debasis},
title = {To Clean or Not to Clean: Document Preprocessing and Reproducibility},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3242180},
doi = {10.1145/3242180},
abstract = {Web document collections such as WT10G, GOV2, and ClueWeb are widely used for text retrieval experiments. Documents in these collections contain a fair amount of non-content-related markup in the form of tags, hyperlinks, and so on. Published articles that use these corpora generally do not provide specific details about how this markup information is handled during indexing. However, this question turns out to be important: Through experiments, we find that including or excluding metadata in the index can produce significantly different results with standard IR models. More importantly, the effect varies across models and collections. For example, metadata filtering is found to be generally beneficial when using BM25, or language modeling with Dirichlet smoothing, but can significantly reduce retrieval effectiveness if language modeling is used with Jelinek-Mercer smoothing. We also observe that, in general, the performance differences become more noticeable as the amount of metadata in the test collections increase. Given this variability, we believe that the details of document preprocessing are significant from the point of view of reproducibility. In a second set of experiments, we also study the effect of preprocessing on query expansion using RM3. In this case, once again, we find that it is generally better to remove markup before using documents for query expansion.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {18},
numpages = {25},
keywords = {Reproducibility, noise, selecting indexable content, web data, metadata preprocessing, relevance feedback}
}

@article{10.1145/3239574,
author = {Kiesel, Johannes and Kneist, Florian and Alshomary, Milad and Stein, Benno and Hagen, Matthias and Potthast, Martin},
title = {Reproducible Web Corpora: Interactive Archiving with Automatic Quality Assessment},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3239574},
doi = {10.1145/3239574},
abstract = {The evolution of web pages from static HTML pages toward dynamic pieces of software has rendered archiving them increasingly difficult. Nevertheless, an accurate, reproducible web archive is a necessity to ensure the reproducibility of web-based research. Archiving web pages reproducibly, however, is currently not part of best practices for web corpus construction. As a result, and despite the ongoing efforts of other stakeholders to archive the web, tools for the construction of reproducible web corpora are insufficient or ill-fitted. This article presents a new tool tailored to this purpose. It relies on emulating user interactions with a web page while recording all network traffic. The customizable user interactions can be replayed on demand, while requests sent by the archived page are served with the recorded responses. The tool facilitates reproducible user studies, user simulations, and evaluations of algorithms that rely on extracting data from web pages. To evaluate our tool, we conduct the first systematic assessment of reproduction quality for rendered web pages. Using our tool, we create a corpus of 10,000&nbsp;web pages carefully sampled from the Common Crawl and manually annotated with regard to reproduction quality via crowdsourcing. Based on this data, we test three approaches to automatic reproduction-quality assessment. An off-the-shelf neural network, trained on visual differences between the web page during archiving and reproduction, matches the manual assessments best. This automatic assessment of reproduction quality allows for immediate bugfixing during archiving and continuous development of our tool as the web continues to evolve.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {17},
numpages = {25},
keywords = {reproduction quality prediction, Web archive, reproduction quality, web collection}
}

@article{10.1145/3239571,
author = {Yang, Peilin and Fang, Hui and Lin, Jimmy},
title = {Anserini: Reproducible Ranking Baselines Using Lucene},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3239571},
doi = {10.1145/3239571},
abstract = {This work tackles the perennial problem of reproducible baselines in information retrieval research, focusing on bag-of-words ranking models. Although academic information retrieval researchers have a long history of building and sharing systems, they are primarily designed to facilitate the publication of research papers. As such, these systems are often incomplete, inflexible, poorly documented, difficult to use, and slow, particularly in the context of modern web-scale collections. Furthermore, the growing complexity of modern software ecosystems and the resource constraints most academic research groups operate under make maintaining open-source systems a constant struggle. However, except for a small number of companies (mostly commercial web search engines) that deploy custom infrastructure, Lucene has become the de facto platform in industry for building search applications. Lucene has an active developer base, a large audience of users, and diverse capabilities to work with heterogeneous collections at scale. However, it lacks systematic support for ad hoc experimentation using standard test collections. We describe Anserini, an information retrieval toolkit built on Lucene that fills this gap. Our goal is to simplify ad hoc experimentation and allow researchers to easily reproduce results with modern bag-of-words ranking models on diverse test collections. With Anserini, we demonstrate that Lucene provides a suitable framework for supporting information retrieval research. Experiments show that our system efficiently indexes large web collections, provides modern ranking models that are on par with research implementations in terms of effectiveness, and supports low-latency query evaluation to facilitate rapid experimentation},
journal = {J. Data and Information Quality},
month = oct,
articleno = {16},
numpages = {20},
keywords = {Ad hoc retrieval, TREC}
}

@article{10.1145/3268408,
author = {Ferro, Nicola and Fuhr, Norbert and Rauber, Andreas},
title = {Introduction to the Special Issue on Reproducibility in Information Retrieval: Evaluation Campaigns, Collections, and Analyses},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3268408},
doi = {10.1145/3268408},
journal = {J. Data and Information Quality},
month = oct,
articleno = {9},
numpages = {4},
keywords = {Reproducibility}
}

@article{10.1145/3241064,
author = {Roitero, Kevin and Passon, Marco and Serra, Giuseppe and Mizzaro, Stefano},
title = {Reproduce. Generalize. Extend. On Information Retrieval Evaluation without Relevance Judgments},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3241064},
doi = {10.1145/3241064},
abstract = {The evaluation of retrieval effectiveness by means of test collections is a commonly used methodology in the information retrieval field. Some researchers have addressed the quite fascinating research question of whether it is possible to evaluate effectiveness completely automatically, without human relevance assessments. Since human relevance assessment is one of the main costs of building a test collection, both in human time and money resources, this rather ambitious goal would have a practical impact. In this article, we reproduce the main results on evaluating information retrieval systems without relevance judgments; furthermore, we generalize such previous work to analyze the effect of test collections, evaluation metrics, and pool depth. We also expand the idea to semi-automatic evaluation and estimation of topic difficulty. Our results show that (i) previous work is overall reproducible, although some specific results are not; (ii) collection, metric, and pool depth impact the automatic evaluation of systems, which is anyway accurate in several cases; (iii) semi-automatic evaluation is an effective methodology; and (iv) automatic evaluation can (to some extent) be used to predict topic difficulty.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {11},
numpages = {32},
keywords = {Test collections, topic difficulty, few topics, reproducibility, automatic retrieval evaluation, relevance judgments}
}

@article{10.1145/3239575,
author = {Jagerman, Rolf and Balog, Krisztian and Rijke, Maarten De},
title = {OpenSearch: Lessons Learned from an Online Evaluation Campaign},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3239575},
doi = {10.1145/3239575},
abstract = {We report on our experience with TREC OpenSearch, an online evaluation campaign that enabled researchers to evaluate their experimental retrieval methods using real users of a live website. Specifically, we focus on the task of ad hoc document retrieval within the academic search domain, and work with two search engines, CiteSeerX and SSOAR, that provide us with traffic. We describe our experimental platform, which is based on the living labs methodology, and report on the experimental results obtained. We also share our experiences, challenges, and the lessons learned from running this track in 2016 and 2017.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {13},
numpages = {15},
keywords = {online evaluation, Living labs}
}

@article{10.1145/3239573,
author = {Roitero, Kevin and Soprano, Michael and Brunello, Andrea and Mizzaro, Stefano},
title = {Reproduce and Improve: An Evolutionary Approach to Select a Few Good Topics for Information Retrieval Evaluation},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3239573},
doi = {10.1145/3239573},
abstract = {Effectiveness evaluation of information retrieval systems by means of a test collection is a widely used methodology. However, it is rather expensive in terms of resources, time, and money; therefore, many researchers have proposed methods for a cheaper evaluation. One particular approach, on which we focus in this article, is to use fewer topics: in TREC-like initiatives, usually system effectiveness is evaluated as the average effectiveness on a set of n topics (usually, n=50, but more than 1,000 have been also adopted); instead of using the full set, it has been proposed to find the best subsets of a few good topics that evaluate the systems in the most similar way to the full set. The computational complexity of the task has so far limited the analysis that has been performed. We develop a novel and efficient approach based on a multi-objective evolutionary algorithm. The higher efficiency of our new implementation allows us to reproduce some notable results on topic set reduction, as well as perform new experiments to generalize and improve such results. We show that our approach is able to both reproduce the main state-of-the-art results and to allow us to analyze the effect of the collection, metric, and pool depth used for the evaluation. Finally, differently from previous studies, which have been mainly theoretical, we are also able to discuss some practical topic selection strategies, integrating results of automatic evaluation approaches.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {12},
numpages = {21},
keywords = {topic selection strategy, evolutionary algorithms, topic sets, reproducibility, few topics, Test collection}
}

@article{10.1145/3239572,
author = {Moffat, Alistair and Scholer, Falk and Yang, Ziying},
title = {Estimating Measurement Uncertainty for Information Retrieval Effectiveness Metrics},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3239572},
doi = {10.1145/3239572},
abstract = {One typical way of building test collections for offline measurement of information retrieval systems is to pool the ranked outputs of different systems down to some chosen depth d and then form relevance judgments for those documents only. Non-pooled documents—ones that did not appear in the top-d sets of any of the contributing systems—are then deemed to be non-relevant for the purposes of evaluating the relative behavior of the systems. In this article, we use RBP-derived residuals to re-examine the reliability of that process. By fitting the RBP parameter ϕ to maximize similarity between AP- and NDCG-induced system rankings, on the one hand, and RBP-induced rankings, on the other, an estimate can be made as to the potential score uncertainty associated with those two recall-based metrics. We then consider the effect that residual size—as an indicator of possible measurement uncertainty in utility-based metrics—has in connection with recall-based metrics by computing the effect of increasing pool sizes and examining the trends that arise in terms of both metric score and system separability using standard statistical tests. The experimental results show that the confidence levels expressed via the p-values generated by statistical tests are only weakly connected to the size of the residual and to the degree of measurement uncertainty caused by the presence of unjudged documents. Statistical confidence estimates are, however, largely consistent as pooling depths are altered. We therefore recommend that all such experimental results should report, in addition to the outcomes of statistical significance tests, the residual measurements generated by a suitably matched weighted-precision metric, to give a clear indication of measurement uncertainty that arises due to the presence of unjudged documents in test collections with finite pooled judgments.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {10},
numpages = {22},
keywords = {effectiveness metric, evaluation, information retrieval, test collection, Evaluation, statistical test}
}

@article{10.1145/3209668,
author = {Bertino, Elisa and Jabal, Amani Abu and Calo, Seraphin and Verma, Dinesh and Williams, Christopher},
title = {The Challenge of Access Control Policies Quality},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3209668},
doi = {10.1145/3209668},
abstract = {Access Control policies allow one to control data sharing among multiple subjects. For high assurance data security, it is critical that such policies be fit for their purpose. In this paper we introduce the notion of “policy quality” and elaborate on its many dimensions, such as consistency, completeness, and minimality. We introduce a framework supporting the analysis of policies with respect to the introduced quality dimensions and elaborate on research challenges, including policy analysis for large-scale distributed systems, assessment of policy correctness, and analysis of policies expressed in richer policy models.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {6},
numpages = {6},
keywords = {policy analysis, provenance, Access control policies}
}

@article{10.1145/3230669,
author = {Karanja, Evanson Mwangi and Masupe, Shedden and Gasennelwe-Jeffrey, Mandu},
title = {Challenge Paper: Towards Open Datasets for Internet of Things Malware},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3230669},
doi = {10.1145/3230669},
journal = {J. Data and Information Quality},
month = sep,
articleno = {7},
numpages = {5},
keywords = {malware datasets, open data, Internet of things}
}

@article{10.1145/3228341,
author = {Puentes, John and Laso, Pedro Merino and Brosset, David},
title = {The Challenge of Quality Evaluation in Fraud Detection},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3228341},
doi = {10.1145/3228341},
journal = {J. Data and Information Quality},
month = sep,
articleno = {5},
numpages = {4},
keywords = {fraud life cycle, context, cumulative indicators, Quality meta-analysis}
}

@article{10.1145/3232852,
author = {Koumarelas, Ioannis and Kroschk, Axel and Mosley, Clifford and Naumann, Felix},
title = {Experience: Enhancing Address Matching with Geocoding and Similarity Measure Selection},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3232852},
doi = {10.1145/3232852},
abstract = {Given a query record, record matching is the problem of finding database records that represent the same real-world object. In the easiest scenario, a database record is completely identical to the query. However, in most cases, problems do arise, for instance, as a result of data errors or data integrated from multiple sources or received from restrictive form fields. These problems are usually difficult, because they require a variety of actions, including field segmentation, decoding of values, and similarity comparisons, each requiring some domain knowledge.In this article, we study the problem of matching records that contain address information, including attributes such as Street-address and City. To facilitate this matching process, we propose a domain-specific procedure to, first, enrich each record with a more complete representation of the address information through geocoding and reverse-geocoding and, second, to select the best similarity measure per each address attribute that will finally help the classifier to achieve the best f-measure. We report on our experience in selecting geocoding services and discovering similarity measures for a concrete but common industry use-case.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {8},
numpages = {16},
keywords = {record linkage, duplicate detection, address parsing, random forest, similarity measures, geocoding, Address matching, geographic information systems, conditional functional dependencies, address normalization}
}

@article{10.1145/3190579,
author = {Bertino, Elisa and Jahanshahi, Mohammad R.},
title = {Adaptive and Cost-Effective Collection of High-Quality Data for Critical Infrastructure and Emergency Management in Smart Cities—Framework and Challenges},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3190579},
doi = {10.1145/3190579},
journal = {J. Data and Information Quality},
month = may,
articleno = {1},
numpages = {6},
keywords = {Civil engineering, edge computing, device swarms}
}

@article{10.1145/3185048,
author = {Zhang, Han and Hill, Shawndra and Rothschild, David},
title = {Addressing Selection Bias in Event Studies with General-Purpose Social Media Panels},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3185048},
doi = {10.1145/3185048},
abstract = {Data from Twitter have been employed in prior research to study the impacts of events. Conventionally, researchers use keyword-based samples of tweets to create a panel of Twitter users who mention event-related keywords during and after an event. However, the keyword-based sampling is limited in its objectivity dimension of data and information quality. First, the technique suffers from selection bias since users who discuss an event are already more likely to discuss event-related topics beforehand. Second, there are no viable control groups for comparison to a keyword-based sample of Twitter users. We propose an alternative sampling approach to construct panels of users defined by their geolocation. Geolocated panels are exogenous to the keywords in users’ tweets, resulting in less selection bias than the keyword panel method. Geolocated panels allow us to follow within-person changes over time and enable the creation of comparison groups. We compare different panels in two real-world settings: response to mass shootings and TV advertising. We first show the strength of the selection biases of keyword panels. Then, we empirically illustrate how geolocated panels reduce selection biases and allow meaningful comparison groups regarding the impact of the studied events. We are the first to provide a clear, empirical example of how a better panel selection design, based on an exogenous variable such as geography, both reduces selection bias compared to the current state of the art and increases the value of Twitter research for studying events. While we advocate for the use of a geolocated panel, we also discuss its weaknesses and application scenario seriously. This article also calls attention to the importance of selection bias in impacting the objectivity of social media data.},
journal = {J. Data and Information Quality},
month = may,
articleno = {4},
numpages = {24},
keywords = {geolocation, social media, survey, non-response bias, Twitter, coverage bias, panels, selection bias}
}

@article{10.1145/3182182,
author = {Flores, Javier and Sun, Jun},
title = {Information Quality Awareness and Information Quality Practice},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3182182},
doi = {10.1145/3182182},
abstract = {Healthcare organizations increasingly rely on electronic information to optimize their operations. Information of high diversity from various sources accentuate the relevance and importance of information quality (IQ). The quality of information needs to be improved to support a more efficient and reliable utilization of healthcare information systems (IS). This can only be achieved through the implementation of initiatives followed by most users across an organization. The purpose of this study is to examine how awareness of IS users about IQ issues would affect their IQ behavior. Based on multiple theoretical frameworks, it is hypothesized that different aspects of user motivation mediate the relationship between the awareness on both beneficial and problematic situations and IQ practice inclination. In addition, social influence and facilitating condition moderate the relationship between IQ practice inclination and overt IQ practice. The theoretical and practical implications of findings are discussed, especially how to enhance IQ compliance in the healthcare settings.},
journal = {J. Data and Information Quality},
month = may,
articleno = {2},
numpages = {18},
keywords = {behavior concerning information quality, motivation concerning information quality, Data quality, practice, information quality, awareness, technology policy}
}

@article{10.1145/3190578,
author = {Bors, Christian and Gschwandtner, Theresia and Kriglstein, Simone and Miksch, Silvia and Pohl, Margit},
title = {Visual Interactive Creation, Customization, and Analysis of Data Quality Metrics},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3190578},
doi = {10.1145/3190578},
abstract = {During data preprocessing, analysts spend a significant part of their time and effort profiling the quality of the data along with cleansing and transforming the data for further analysis. While quality metrics—ranging from general to domain-specific measures—support assessment of the quality of a dataset, there are hardly any approaches to visually support the analyst in customizing and applying such metrics. Yet, visual approaches could facilitate users’ involvement in data quality assessment. We present MetricDoc, an interactive environment for assessing data quality that provides customizable, reusable quality metrics in combination with immediate visual feedback. Moreover, we provide an overview visualization of these quality metrics along with error visualizations that facilitate interactive navigation of the data to determine the causes of quality issues present in the data. In this article, we describe the architecture, design, and evaluation of MetricDoc, which underwent several design cycles, including heuristic evaluation and expert reviews as well as a focus group with data quality, human-computer interaction, and visual analytics experts.},
journal = {J. Data and Information Quality},
month = may,
articleno = {3},
numpages = {26},
keywords = {Data profiling, data quality metrics, visual exploration}
}

@article{10.1145/3177874,
author = {Ashish, Naveen and Patawari, Arihant},
title = {Machine Reading of Biomedical Data Dictionaries},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3177874},
doi = {10.1145/3177874},
abstract = {This article describes an approach for the automated reading of biomedical data dictionaries. Automated reading is the process of extracting element details for each of the data elements from a data dictionary in a document format (such as PDF) to a completely structured representation. A structured representation is essential if the data dictionary metadata are to be used in applications such as data integration and also in evaluating the quality of the associated data. We present an approach and implemented solution for the problem, considering different formats of data dictionaries. We have a particular focus on the most challenging format with a machine-learning classification solution to the problem using conditional random field classifiers. We present an evaluation using several actual data dictionaries, demonstrating the effectiveness of our approach.},
journal = {J. Data and Information Quality},
month = may,
articleno = {21},
numpages = {20},
keywords = {machine-learning classification, Document extraction}
}

@article{10.1145/3190577,
author = {Chiang, Fei and Gairola, Dhruv},
title = {InfoClean: Protecting Sensitive Information in Data Cleaning},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3190577},
doi = {10.1145/3190577},
abstract = {Data quality has become a pervasive challenge for organizations as they wrangle with large, heterogeneous datasets to extract value. Given the proliferation of sensitive and confidential information, it is crucial to consider data privacy concerns during the data cleaning process. For example, in medical database applications, varying levels of privacy are enforced across the attribute values. Attributes such as a patient’s country or city of residence may be less sensitive than the patient’s prescribed medication. Traditional data cleaning techniques assume the data is openly accessible, without considering the differing levels of information sensitivity. In this work, we take the first steps toward a data cleaning model that integrates privacy as part of the data cleaning process. We present a privacy-aware data cleaning framework that differentiates the information content among the attribute values during the data cleaning process to resolve data inconsistencies while minimizing the amount of information disclosed. Our data repair algorithm includes a set of data disclosure operations that considers the information content of the underlying attribute values, while maximizing data utility. Our evaluation using real datasets shows that our algorithm scales well, and achieves improved performance and comparable repair accuracy against existing data cleaning solutions.},
journal = {J. Data and Information Quality},
month = apr,
articleno = {22},
numpages = {26},
keywords = {Data quality, data privacy, data cleaning}
}

@article{10.1145/3185049,
author = {Alshayeb, Mohammad and Shaaban, Yasser and Al-Ghamdi, Jarallah},
title = {SPMDL: Software Product Metrics Definition Language},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3185049},
doi = {10.1145/3185049},
abstract = {Software metrics are becoming more acceptable measures for software quality assessment. However, there is no standard form to represent metric definitions, which would be useful for metrics exchange and customization. In this article, we propose the Software Product Metrics Definition Language (SPMDL). We develop an XML-based description language to define software metrics in a precise and reusable form. Metric definitions in SPMDL are based on meta-models extracted from either source code or design artifacts, such as the Dagstuhl Middle Meta-model, with support for various abstraction levels. The language defines several flexible computation mechanisms, such as extended Object Constraint Language queries and predefined graph operations on the meta-model. SPMDL provides an unambiguous description of the metric definition; it is also easy to use and is extensible.},
journal = {J. Data and Information Quality},
month = apr,
articleno = {20},
numpages = {30},
keywords = {Software metrics, definition language}
}

@article{10.1145/3190576,
author = {Musyaffa, Fathoni A. and Engels, Christiane and Vidal, Maria-Esther and Orlandi, Fabrizio and Auer, S\"{o}ren},
title = {Experience: Open Fiscal Datasets, Common Issues, and Recommendations},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3190576},
doi = {10.1145/3190576},
abstract = {Public administrations are continuously publishing open data, increasing the amount of government open data over time. The published data includes budgets and spending as part of fiscal data; publishing these data is an important part of transparent and accountable governance. However, open fiscal data should also meet open data publication guidelines. When requirements in data guidelines are not met, effective data analysis over published datasets cannot be performed effectively. In this article, we present Open Fiscal Data Publication (OFDP), a framework to assess the quality of open fiscal datasets. We also present an extensive open fiscal data assessment and common data quality issues found; additionally, open fiscal data publishing guidelines are presented. We studied and surveyed main quality factors for open fiscal datasets. Moreover, the collected quality factors have been scored according to the results of a questionnaire to score quality factors within the OFDP assessment framework. We gather and comprehensively analyze a representative set of 77 fiscal datasets from several public administrations across different regions at different levels (e.g., supranational, national, municipality). We characterize quality issues commonly arising in these datasets. Our assessment shows that there are many quality factors in fiscal data publication that still need to be taken care of so that the data can be analyzed effectively. Our proposed guidelines allow for publishing open fiscal data where these quality issues are avoided.},
journal = {J. Data and Information Quality},
month = apr,
articleno = {19},
numpages = {10},
keywords = {Open fiscal data, open data assessment}
}

@article{10.1145/3165712,
author = {Gal, Avigdor and Senderovich, Arik and Weidlich, Matthias},
title = {Challenge Paper: Data Quality Issues in Queue Mining},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3165712},
doi = {10.1145/3165712},
journal = {J. Data and Information Quality},
month = apr,
articleno = {18},
numpages = {5},
keywords = {queue mining, Process mining}
}

@article{10.1145/3174791,
author = {Geerts, Floris and Missier, Paolo and Paton, Norman},
title = {Editorial: Special Issue on Improving the Veracity and Value of Big Data},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3174791},
doi = {10.1145/3174791},
journal = {J. Data and Information Quality},
month = mar,
articleno = {13},
numpages = {2}
}

@article{10.1145/3177873,
author = {Esteves, Diego and Rula, Anisa and Reddy, Aniketh Janardhan and Lehmann, Jens},
title = {Toward Veracity Assessment in RDF Knowledge Bases: An Exploratory Analysis},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3177873},
doi = {10.1145/3177873},
abstract = {Among different characteristics of knowledge bases, data quality is one of the most relevant to maximize the benefits of the provided information. Knowledge base quality assessment poses a number of big data challenges such as high volume, variety, velocity, and veracity. In this article, we focus on answering questions related to the assessment of the veracity of facts through Deep Fact Validation (DeFacto), a triple validation framework designed to assess facts in RDF knowledge bases. Despite current developments in the research area, the underlying framework faces many challenges. This article pinpoints and discusses these issues and conducts a thorough analysis of its pipeline, aiming at reducing the error propagation through its components. Furthermore, we discuss recent developments related to this fact validation as well as describing advantages and drawbacks of state-of-the-art models. As a result of this exploratory analysis, we give insights and directions toward a better architecture to tackle the complex task of fact-checking in knowledge bases.},
journal = {J. Data and Information Quality},
month = feb,
articleno = {16},
numpages = {26},
keywords = {data quality, trustworthiness, linked data, fact checking, exploratory data analysis, DeFacto, benchmark}
}

@article{10.1145/3148239,
author = {Bertossi, Leopoldo and Milani, Mostafa},
title = {Ontological Multidimensional Data Models and Contextual Data Quality},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3148239},
doi = {10.1145/3148239},
abstract = {Data quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. The data under assessment are mapped into the context for additional analysis, processing, and quality data extraction. The resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context, we include a generalized multidimensional data model and a Datalog± ontology with provably good properties in terms of query answering. These main components are used to represent dimension hierarchies, dimensional constraints, and dimensional rules and define predicates for quality data specification. Query answering relies on and triggers navigation through dimension hierarchies and becomes the basic tool for the extraction of quality data. The OMD model is interesting per se beyond applications to data quality. It allows for a logic-based and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {14},
numpages = {36},
keywords = {query answering, weakly-sticky programs, Ontology-based data access, Datalog±}
}

@article{10.1145/3165713,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3165713},
doi = {10.1145/3165713},
abstract = {Although the ultimate objective of Linked Data is linking and integration, it is not currently evident how connected the current Linked Open Data (LOD) cloud is. In this article, we focus on methods, supported by special indexes and algorithms, for performing measurements related to the connectivity of more than two datasets that are useful in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference, i.e., for obtaining complete information about a set of entities, including provenance information; (c) Data Quality Assessment and Improvement, i.e., for assessing the connectivity between any set of datasets and monitoring their evolution over time, as well as for estimating data veracity; (d) Dataset Visualizations; and various other tasks. Since it would be prohibitively expensive to perform all these measurements in a na\"{\i}ve way, in this article, we introduce indexes (and their construction algorithms) that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. For enhancing scalability, we propose parallel index construction algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved speedup using either a single machine or a cluster of machines, and we provide insights regarding the factors that affect efficiency. Finally, we report measurements about the connectivity of the (billion triples-sized) LOD cloud that have never been carried out so far.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {15},
numpages = {49},
keywords = {dataset discovery, dataset selection, Data quality, big data, connectivity, mapreduce, spark, lattice of measurements, linked data}
}

@article{10.1145/3131611,
author = {Chen, Qingyu and Wan, Yu and Zhang, Xiuzhen and Lei, Yang and Zobel, Justin and Verspoor, Karin},
title = {Comparative Analysis of Sequence Clustering Methods for Deduplication of Biological Databases},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3131611},
doi = {10.1145/3131611},
abstract = {The massive volumes of data in biological sequence databases provide a remarkable resource for large-scale biological studies. However, the underlying data quality of these resources is a critical concern. A particular challenge is duplication, in which multiple records have similar sequences, creating a high level of redundancy that impacts database storage, curation, and search. Biological database deduplication has two direct applications: for database curation, where detected duplicates are removed to improve curation efficiency, and for database search, where detected duplicate sequences may be flagged but remain available to support analysis.Clustering methods have been widely applied to biological sequences for database deduplication. Since an exhaustive all-by-all pairwise comparison of sequences cannot scale for a high volume of data, heuristic approaches have been recruited, such as the use of simple similarity thresholds. In this article, we present a comparison between CD-HIT and UCLUST, the two best-known clustering tools for sequence database deduplication. Our contributions include a detailed assessment of the redundancy remaining after deduplication, application of standard clustering evaluation metrics to quantify the cohesion and separation of the clusters generated by each method, and a biological case study that assesses intracluster function annotation consistency to demonstrate the impact of these factors on a practical application of the sequence clustering methods. Our results show that the trade-off between efficiency and accuracy becomes acute when low threshold values are used and when cluster sizes are large. This evaluation leads to practical recommendations for users for more effective uses of the sequence clustering tools for deduplication.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {17},
numpages = {27},
keywords = {Deduplication, validation, clustering, databases}
}

@article{10.1145/3148238,
author = {Heinrich, Bernd and Hristova, Diana and Klier, Mathias and Schiller, Alexander and Szubartowicz, Michael},
title = {Requirements for Data Quality Metrics},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3148238},
doi = {10.1145/3148238},
abstract = {Data quality and especially the assessment of data quality have been intensively discussed in research and practice alike. To support an economically oriented management of data quality and decision making under uncertainty, it is essential to assess the data quality level by means of well-founded metrics. However, if not adequately defined, these metrics can lead to wrong decisions and economic losses. Therefore, based on a decision-oriented framework, we present a set of five requirements for data quality metrics. These requirements are relevant for a metric that aims to support an economically oriented management of data quality and decision making under uncertainty. We further demonstrate the applicability and efficacy of these requirements by evaluating five data quality metrics for different data quality dimensions. Moreover, we discuss practical implications when applying the presented requirements.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {12},
numpages = {32},
keywords = {requirements for metrics, data quality metrics, data quality assessment, Data quality}
}

@article{10.1145/3148240,
author = {Koh, Kyu Han and Fouh, Eric and Farghally, Mohammed F. and Shahin, Hossameldin and Shaffer, Clifford A.},
title = {Experience: Learner Analytics Data Quality for an ETextbook System},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3148240},
doi = {10.1145/3148240},
abstract = {We present lessons learned related to data collection and analysis from 5 years of experience with the eTextbook system OpenDSA. The use of such cyberlearning systems is expanding rapidly in both formal and informal educational settings. Although the precise issues related to any such project are idiosyncratic based on the data collection technology and goals of the project, certain types of data collection problems will be common. We begin by describing the nature of the data transmitted between the student’s client machine and the database server, and our initial database schema for storing interaction log data. We describe many problems that we encountered, with the nature of the problems categorized as syntactic-level data collection issues, issues with relating events to users, or issues with tracking users over time. Relating events to users and tracking the time spent on tasks are both prerequisites to converting syntactic-level interaction streams to semantic-level behavior needed for higher-order analysis of the data. Finally, we describe changes made to our database schema that helped to resolve many of the issues that we had encountered. These changes help advance our ultimate goal of encouraging a change from ineffective learning behavior by students to more productive behavior.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {10},
numpages = {10},
keywords = {eTextbooks, cyberlearning systems, Learner analytics, automated assessment}
}

@article{10.1145/3138806,
author = {Truong, Hong-Linh and Murguzur, Aitor and Yang, Erica},
title = {Challenges in Enabling Quality of Analytics in the Cloud},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3138806},
doi = {10.1145/3138806},
journal = {J. Data and Information Quality},
month = jan,
articleno = {9},
numpages = {4},
keywords = {big data analytics, service management, Cloud computing, data quality}
}

@article{10.1145/3141248,
author = {Cappiello, C. and Cerletti, C. and Fratto, C. and Pernici, B.},
title = {Validating Data Quality Actions in Scoring Processes},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3141248},
doi = {10.1145/3141248},
abstract = {Data quality has gained momentum among organizations upon the realization that poor data quality might cause failures and/or inefficiencies, thus compromising business processes and application results. However, enterprises often adopt data quality assessment and improvement methods based on practical and empirical approaches without conducting a rigorous analysis of the data quality issues and outcome of the enacted data quality improvement practices. In particular, data quality management, especially the identification of the data quality dimensions to be monitored and improved, is performed by knowledge workers on the basis of their skills and experience. Control methods are therefore designed on the basis of expected and evident quality problems; thus, these methods may not be effective in dealing with unknown and/or unexpected problems. This article aims to provide a methodology, based on fault injection, for validating the data quality actions used by organizations. We show how it is possible to check whether the adopted techniques properly monitor the real issues that may damage business processes. At this stage, we focus on scoring processes, i.e., those in which the output represents the evaluation or ranking of a specific object. We show the effectiveness of our proposal by means of a case study in the financial risk management area.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {11},
numpages = {27},
keywords = {assessment, decision processes, Data quality, decision support}
}

@article{10.1145/3143313,
author = {Raschid, Louiqa},
title = {Editor-in-Chief (January 2014-May 2017) Farewell Report},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3143313},
doi = {10.1145/3143313},
journal = {J. Data and Information Quality},
month = oct,
articleno = {7},
numpages = {2}
}

@article{10.1145/3143316,
author = {Catarci, Tiziana},
title = {Foreword from the New JDIQ Editor-in-Chief},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3143316},
doi = {10.1145/3143316},
journal = {J. Data and Information Quality},
month = oct,
articleno = {8},
numpages = {2}
}

@article{10.1145/3090057,
author = {El-Mawass, Nour and Alaboodi, Saad},
title = {Data Quality Challenges in Social Spam Research},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3090057},
doi = {10.1145/3090057},
journal = {J. Data and Information Quality},
month = sep,
articleno = {4},
numpages = {4},
keywords = {online social networks, machine learning, supervised learning, social spam detection, Reproducibility}
}

@article{10.1145/3090056,
author = {Chen, Min and Lukyanenko, Roman and Tremblay, Monica Chiarini},
title = {Information Quality Challenges in Shared Healthcare Decision Making},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3090056},
doi = {10.1145/3090056},
journal = {J. Data and Information Quality},
month = sep,
articleno = {5},
numpages = {3},
keywords = {e--health, Information quality, user generated content, data quality, quality of social media data, shared healthcare decision making}
}

@article{10.1145/3122863,
author = {Basheer, Aseel and Sha, Kewei},
title = {Cluster-Based Quality-Aware Adaptive Data Compression for Streaming Data},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3122863},
doi = {10.1145/3122863},
abstract = {Wireless sensor networks (WSNs) are widely applied in data collection applications. Energy efficiency is one of the most important design goals of WSNs. In this article, we examine the tradeoffs between the energy efficiency and the data quality. First, four attributes used to evaluate data quality are formally defined. Then, we propose a novel data compression algorithm, Quality-Aware Adaptive data Compression (QAAC), to reduce the amount of data communication to save energy. QAAC utilizes an adaptive clustering algorithm to build clusters from dataset; then a code for each cluster is generated and stored in a Huffman encoding tree. The encoding algorithm encodes the original dataset based on the Haffman encoding tree. An improvement algorithm is also designed to reduce the information loss when data are compressed. After the encoded data, the Huffman encoding tree and parameters used in the improvement algorithm have been received at the sink, a decompression algorithm is used to retrieve the approximation of the original dataset. The performance evaluation shows that QAAC is efficient and achieves a much higher compression ratio than lossy and lossless compression algorithms, while it has much smaller information loss than lossy compression algorithms.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {2},
numpages = {33},
keywords = {data compression, adaptive, data quality, clustering algorithm, Wireless sensor networks, energy efficiency}
}

@article{10.1145/3110291,
author = {Corsar, David and Edwards, Peter},
title = {Challenges of Open Data Quality: More Than Just License, Format, and Customer Support},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3110291},
doi = {10.1145/3110291},
journal = {J. Data and Information Quality},
month = sep,
articleno = {3},
numpages = {4},
keywords = {Open data}
}

@article{10.1145/3106236,
author = {Arbuckle, Peter and Kahn, Ezra and Kriesberg, Adam},
title = {Challenge Paper: Challenges to Sharing Data and Models for Life Cycle Assessment},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3106236},
doi = {10.1145/3106236},
journal = {J. Data and Information Quality},
month = sep,
articleno = {6},
numpages = {4},
keywords = {Data documentation, Life Cycle Assessment}
}

@article{10.1145/3106235,
author = {Marcheggiani, Diego and Sebastiani, Fabrizio},
title = {On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3106235},
doi = {10.1145/3106235},
abstract = {In the last five years there has been a flurry of work on information extraction from clinical documents, that is, on algorithms capable of extracting, from the informal and unstructured texts that are generated during everyday clinical practice, mentions of concepts relevant to such practice. Many of these research works are about methods based on supervised learning, that is, methods for training an information extraction system from manually annotated examples. While a lot of work has been devoted to devising learning methods that generate more and more accurate information extractors, no work has been devoted to investigating the effect of the quality of training data on the learning process for the clinical domain. Low quality in training data often derives from the fact that the person who has annotated the data is different from the one against whose judgment the automatically annotated data must be evaluated. In this article, we test the impact of such data quality issues on the accuracy of information extraction systems as applied to the clinical domain. We do this by comparing the accuracy deriving from training data annotated by the authoritative coder (i.e., the one who has also annotated the test data and by whose judgment we must abide) with the accuracy deriving from training data annotated by a different coder, equally expert in the subject matter. The results indicate that, although the disagreement between the two coders (as measured on the training set) is substantial, the difference is (surprisingly enough) not always statistically significant. While the dataset used in the present work originated in a clinical context, the issues we study in this work are of more general interest.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {1},
numpages = {25},
keywords = {Information extraction, radiology reports, medical reports, machine learning, clinical narratives, annotation quality}
}

@article{10.1145/3058750,
author = {St-Maurice, Justin and Burns, Catherine},
title = {An Exploratory Case Study to Understand Primary Care Users and Their Data Quality Tradeoffs},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3058750},
doi = {10.1145/3058750},
abstract = {Primary care data is an important part of the evolving healthcare ecosystem. Generally, users in primary care are expected to provide excellent patient care and record high-quality data. In practice, users must balance sets of priorities regarding care and data. The goal of this study was to understand data quality tradeoffs between timeliness, validity, completeness, and use among primary care users. As a case study, data quality measures and metrics are developed through a focus group session with managers. After calculating and extracting measurements of data quality from six years of historic data, each measure was modeled with logit binomial regression to show correlations, characterize tradeoffs, and investigate data quality interactions. Measures and correlations for completeness, use, and timeliness were calculated for 196,967 patient encounters. Based on the analysis, there was a positive relationship between validity and completeness, and a negative relationship between timeliness and use. Use of data and reductions in entry delay were positively associated with completeness and validity. Our results suggest that if users are not provided with sufficient time to record data as part of their regular workflow, they will prioritize spending available time with patients. As a measurement of a primary care system's effectiveness, the negative correlation between use and timeliness points to a self-reinforcing relationship that provides users with little external value. In the future, additional data can be generated from comparable organizations to test several new hypotheses about primary care users.},
journal = {J. Data and Information Quality},
month = jul,
articleno = {15},
numpages = {24},
keywords = {primary care, statistical analysis, tradeoffs, Case study, data mining}
}

@article{10.1145/3041761,
author = {Wang, Jiannan and Tang, Nan},
title = {Dependable Data Repairing with Fixing Rules},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3041761},
doi = {10.1145/3041761},
abstract = {One of the main challenges that data-cleaning systems face is to automatically identify and repair data errors in a dependable manner. Though data dependencies (also known as integrity constraints) have been widely studied to capture errors in data, automated and dependable data repairing on these errors has remained a notoriously difficult problem. In this work, we introduce an automated approach for dependably repairing data errors, based on a novel class of fixing rules. A fixing rule contains an evidence pattern, a set of negative patterns, and a fact value. The heart of fixing rules is deterministic: given a tuple, the evidence pattern and the negative patterns of a fixing rule are combined to precisely capture which attribute is wrong, and the fact indicates how to correct this error. We study several fundamental problems associated with fixing rules and establish their complexity. We develop efficient algorithms to check whether a set of fixing rules are consistent and discuss approaches to resolve inconsistent fixing rules. We also devise efficient algorithms for repairing data errors using fixing rules. Moreover, we discuss approaches on how to generate a large number of fixing rules from examples or available knowledge bases. We experimentally demonstrate that our techniques outperform other automated algorithms in terms of the accuracy of repairing data errors, using both real-life and synthetic data.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {16},
numpages = {34},
keywords = {Data repairing, fixing rules, dependable}
}

@article{10.1145/3064173,
author = {Abdellaoui, Sabrina and Nader, Fahima and Chalal, Rachid},
title = {QDflows: A System Driven by Knowledge Bases for Designing Quality-Aware Data Flows},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3064173},
doi = {10.1145/3064173},
abstract = {In the big data era, data integration is becoming increasingly important. It is usually handled by data flows processes that extract, transform, and clean data from several sources, and populate the data integration system (DIS). Designing data flows is facing several challenges. In this article, we deal with data quality issues such as (1) specifying a set of quality rules, (2) enforcing them on the data flow pipeline to detect violations, and (3) producing accurate repairs for the detected violations. We propose QDflows, a system for designing quality-aware data flows that considers the following as input: (1) a high-quality knowledge base (KB) as the global schema of integration, (2) a set of data sources and a set of validated users’ requirements, (3) a set of defined mappings between data sources and the KB, and (4) a set of quality rules specified by users. QDflows uses an ontology to design the DIS schema. It offers the ability to define the DIS ontology as a module of the knowledge base, based on validated users’ requirements. The DIS ontology model is then extended with multiple types of quality rules specified by users. QDflows extracts and transforms data from sources to populate the DIS. It detects violations of quality rules enforced on the data flows, constructs repair patterns, searches for horizontal and vertical matches in the knowledge base, and performs an automatic repair when possible or generates possible repairs. It interactively involves users to validate the repair process before loading the clean data into the DIS. Using real-life and synthetic datasets, the DBpedia and Yago knowledge bases, we experimentally evaluate the generality, effectiveness, and efficiency of QDflows. We also showcase an interactive tool implementing our system.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {14},
numpages = {39},
keywords = {knowledge bases, Data flows, data quality, graph-based repairing}
}

@article{10.1145/3092700,
author = {Al-Hussaini, Leena},
title = {Experience: Insights into the Benchmarking Data of Hunspell and Aspell Spell Checkers},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3092700},
doi = {10.1145/3092700},
abstract = {Hunspell is a morphological spell checker and automatic corrector for Macintosh 10.6 and later versions. Aspell is a general spell checker and automatic corrector for the GNU operating system. In this experience article, we present a benchmarking study of the performance of Hunspell and Aspell. Ginger is a general grammatical spell checker that is used as a baseline to compare the performance of Hunspell and Aspell. A benchmark dataset was carefully selected to be a mixture of different error types at different word length levels. Further, the benchmarking data are from very bad spellers and will challenge any spell checker. The extensive study described in this work will characterize the respective softwares and benchmarking data from multiple perspectives and will consider many error statistics. Overall, Hunspell can correct 415/469 words and Aspell can correct 414/469 words. The baseline Ginger can correct 279/469 words. We recommend this dataset as the preferred benchmark dataset for evaluating newly developed “isolated word” spell checkers.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {13},
numpages = {10},
keywords = {Spell checkers, benchmarking data, data statistics, error category, error types, Aspell, Ginger, Hunspell}
}

@article{10.1145/3041762,
author = {Markovic, Milan and Edwards, Peter},
title = {The Challenge of Quality in Social Computation},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3041762},
doi = {10.1145/3041762},
journal = {J. Data and Information Quality},
month = jun,
articleno = {12},
numpages = {3},
keywords = {provenance, crowdsourcing, social computation, social machines, Quality}
}

@article{10.1145/3022698,
author = {Woodall, Philip},
title = {The Data Repurposing Challenge: New Pressures from Data Analytics},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3022698},
doi = {10.1145/3022698},
journal = {J. Data and Information Quality},
month = jun,
articleno = {11},
numpages = {4},
keywords = {re-purposing, information repurposing, information reuse, data reuse, data repurposing, Business intelligence, data analytics}
}

@article{10.1145/3012003,
author = {Goldberg, Sean and Wang, Daisy Zhe and Grant, Christan},
title = {A Probabilistically Integrated System for Crowd-Assisted Text Labeling and Extraction},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3012003},
doi = {10.1145/3012003},
abstract = {The amount of text data has been growing exponentially in recent years, giving rise to automatic information extraction methods that store text annotations in a database. The current state-of-the-art structured prediction methods, however, are likely to contain errors and it is important to be able to manage the overall uncertainty of the database. On the other hand, the advent of crowdsourcing has enabled humans to aid machine algorithms at scale. In this article, we introduce pi-CASTLE, a system that optimizes and integrates human and machine computing as applied to a complex structured prediction problem involving Conditional Random Fields (CRFs). We propose strategies grounded in information theory to select a token subset, formulate questions for the crowd to label, and integrate these labelings back into the database using a method of constrained inference. On both a text segmentation task over academic citations and a named entity recognition task over tweets we show an order of magnitude improvement in accuracy gain over baseline methods.},
journal = {J. Data and Information Quality},
month = feb,
articleno = {10},
numpages = {23},
keywords = {information extraction, probabilistic database systems, probabilistic models, Crowdsourcing}
}

@article{10.1145/2996198,
author = {Shankaranarayanan, G. and Blake, Roger},
title = {From Content to Context: The Evolution and Growth of Data Quality Research},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2996198},
doi = {10.1145/2996198},
abstract = {Research in data and information quality has made significant strides over the last 20 years. It has become a unified body of knowledge incorporating techniques, methods, and applications from a variety of disciplines including information systems, computer science, operations management, organizational behavior, psychology, and statistics. With organizations viewing “Big Data”, social media data, data-driven decision-making, and analytics as critical, data quality has never been more important. We believe that data quality research is reaching the threshold of significant growth and a metamorphosis from focusing on measuring and assessing data quality—content—toward a focus on usage and context. At this stage, it is vital to understand the identity of this research area in order to recognize its current state and to effectively identify an increasing number of research opportunities within. Using Latent Semantic Analysis (LSA) to analyze the abstracts of 972 peer-reviewed journal and conference articles published over the past 20 years, this article contributes by identifying the core topics and themes that define the identity of data quality research. It further explores their trends over time, pointing to the data quality dimensions that have—and have not—been well-studied, and offering insights into topics that may provide significant opportunities in this area.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {9},
numpages = {28},
keywords = {Data quality, text mining, information quality}
}

@article{10.1145/3020206,
author = {Ferro, Nicola},
title = {Reproducibility Challenges in Information Retrieval Evaluation},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3020206},
doi = {10.1145/3020206},
journal = {J. Data and Information Quality},
month = jan,
articleno = {8},
numpages = {4},
keywords = {Reproducibility}
}

@article{10.1145/2998575,
author = {Labouseur, Alan G. and Matheus, Carolyn C.},
title = {An Introduction to Dynamic Data Quality Challenges},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2998575},
doi = {10.1145/2998575},
journal = {J. Data and Information Quality},
month = jan,
articleno = {6},
numpages = {3},
keywords = {relational systems, internet of things, graph systems, big data, Dynamic data quality}
}

@article{10.1145/3012004,
author = {Becker, Christoph and Duretec, Kresimir and Rauber, Andreas},
title = {The Challenge of Test Data Quality in Data Processing},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3012004},
doi = {10.1145/3012004},
journal = {J. Data and Information Quality},
month = jan,
articleno = {7},
numpages = {4},
keywords = {digital curation, data processing, data quality, quality model, model-based testing, ground truth, data formats, digital preservation, Benchmarking, test oracle, test data}
}

@article{10.1145/3005395,
author = {Bizer, Christian and Dong, Luna and Ilyas, Ihab and Vidal, Maria-Esther},
title = {Editorial: Special Issue on Web Data Quality},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3005395},
doi = {10.1145/3005395},
journal = {J. Data and Information Quality},
month = nov,
articleno = {1},
numpages = {3}
}

@article{10.1145/2964909,
author = {Neumaier, Sebastian and Umbrich, J\"{u}rgen and Polleres, Axel},
title = {Automated Quality Assessment of Metadata across Open Data Portals},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2964909},
doi = {10.1145/2964909},
abstract = {The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources.In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {2},
numpages = {29},
keywords = {data portal, Open Data, quality assessment, data quality}
}

@article{10.1145/2992786,
author = {Debattista, Jeremy and Auer, S\"{O}ren and Lange, Christoph},
title = {Luzzu—A Methodology and Framework for Linked Data Quality Assessment},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2992786},
doi = {10.1145/2992786},
abstract = {The increasing variety of Linked Data on the Web makes it challenging to determine the quality of this data and, subsequently, to make this information explicit to data consumers. Despite the availability of a number of tools and frameworks to assess Linked Data Quality, the output of such tools is not suitable for machine consumption, and thus consumers can hardly compare and rank datasets in the order of fitness for use. This article describes a conceptual methodology for assessing Linked Datasets, and Luzzu; a framework for Linked Data Quality Assessment. Luzzu is based on four major components: (1) an extensible interface for defining new quality metrics; (2) an interoperable, ontology-driven back-end for representing quality metadata and quality problems that can be re-used within different semantic frameworks; (3) scalable dataset processors for data dumps, SPARQL endpoints, and big data infrastructures; and (4) a customisable ranking algorithm taking into account user-defined weights. We show that Luzzu scales linearly against the number of triples in a dataset. We also demonstrate the applicability of the Luzzu framework by evaluating and analysing a number of statistical datasets against a variety of metrics. This article contributes towards the definition of a holistic data quality lifecycle, in terms of the co-evolution of linked datasets, with the final aim of improving their quality.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {4},
numpages = {32},
keywords = {Data quality, quality assessment, linked data}
}

@article{10.1145/2992788,
author = {Meusel, Robert and Ritze, Dominique and Paulheim, Heiko},
title = {Towards More Accurate Statistical Profiling of Deployed Schema.Org Microdata},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2992788},
doi = {10.1145/2992788},
abstract = {Being promoted by major search engines such as Google, Yahoo!, Bing, and Yandex, Microdata embedded in web pages, especially using schema.org, has become one of the most important markup languages for the Web. However, deployed Microdata is very often not free from errors, which makes it difficult to estimate the data volume and create an accurate data profile. In addition, as the usage of global identifiers is not common, the real number of entities described by this format in the Web is hard to assess. In this article, we discuss how the subsequent application of data cleaning steps, such as duplicate detection and correction of common schema-based errors, leads to a more realistic view on the data, step by step. The cleaning steps applied include both heuristics for fixing errors as well as means to perform duplicate detection and duplicate elimination. Using the Web Data Commons Microdata corpus, we show that applying such quality improvement methods can essentially change the statistical profile of the dataset and lead to different estimates of both the number of entities as well as the class distribution within the data.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {3},
numpages = {31},
keywords = {data integration, schema.org, Microdata, data cleaning}
}

@article{10.1145/2992787,
author = {De, Sushovan and Hu, Yuheng and Meduri, Venkata Vamsikrishna and Chen, Yi and Kambhampati, Subbarao},
title = {BayesWipe: A Scalable Probabilistic Framework for Improving Data Quality},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2992787},
doi = {10.1145/2992787},
abstract = {Recent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like Conditional Functional Dependencies (which have to be provided by domain experts or learned from a clean sample of the database). In this article, we provide a method for correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable. We evaluate our methods over both synthetic and real data.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {5},
numpages = {30},
keywords = {Data quality, offline and online cleaning, statistical data cleaning}
}

@article{10.1145/2935752,
author = {Morstatter, Fred and Liu, Huan},
title = {Replacing Mechanical Turkers? Challenges in the Evaluation of Models with Semantic Properties},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935752},
doi = {10.1145/2935752},
journal = {J. Data and Information Quality},
month = oct,
articleno = {15},
numpages = {4},
keywords = {evaluation, data mining, automation, crowdsourcing, Artificial intelligence}
}

@article{10.1145/2968452,
author = {Niemeyer, Kyle E. and Smith, Arfon M. and Katz, Daniel S.},
title = {The Challenge and Promise of Software Citation for Credit, Identification, Discovery, and Reuse},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2968452},
doi = {10.1145/2968452},
journal = {J. Data and Information Quality},
month = oct,
articleno = {16},
numpages = {5},
keywords = {attribution, Software citation, software credit}
}

@article{10.1145/2956554,
author = {Liu, Xiaoping and Li, Xiao-Bai and Motiwalla, Luvai and Li, Wenjun and Zheng, Hua and Franklin, Patricia D.},
title = {Preserving Patient Privacy When Sharing Same-Disease Data},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2956554},
doi = {10.1145/2956554},
abstract = {Medical and health data are often collected for studying a specific disease. For such same-disease microdata, a privacy disclosure occurs as long as an individual is known to be in the microdata. Individuals in same-disease microdata are thus subject to higher disclosure risk than those in microdata with different diseases. This important problem has been overlooked in data-privacy research and practice, and no prior study has addressed this problem. In this study, we analyze the disclosure risk for the individuals in same-disease microdata and propose a new metric that is appropriate for measuring disclosure risk in this situation. An efficient algorithm is designed and implemented for anonymizing same-disease data to minimize the disclosure risk while keeping data utility as good as possible. An experimental study was conducted on real patient and population data. Experimental results show that traditional reidentification risk measures underestimate the actual disclosure risk for the individuals in same-disease microdata and demonstrate that the proposed approach is very effective in reducing the actual risk for same-disease data. This study suggests that privacy protection policy and practice for sharing medical and health data should consider not only the individuals’ identifying attributes but also the health and disease information contained in the data. It is recommended that data-sharing entities employ a statistical approach, instead of the HIPAA's Safe Harbor policy, when sharing same-disease microdata.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {17},
numpages = {14},
keywords = {HIPAA, Data sharing, disclosure risk}
}

@article{10.1145/2968332,
author = {Geisler, Sandra and Quix, Christoph and Weber, Sven and Jarke, Matthias},
title = {Ontology-Based Data Quality Management for Data Streams},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2968332},
doi = {10.1145/2968332},
abstract = {Data Stream Management Systems (DSMS) provide real-time data processing in an effective way, but there is always a tradeoff between data quality (DQ) and performance. We propose an ontology-based data quality framework for relational DSMS that includes DQ measurement and monitoring in a transparent, modular, and flexible way. We follow a threefold approach that takes the characteristics of relational data stream management for DQ metrics into account. While (1) Query Metrics respect changes in data quality due to query operations, (2) Content Metrics allow the semantic evaluation of data in the streams. Finally, (3) Application Metrics allow easy user-defined computation of data quality values to account for application specifics. Additionally, a quality monitor allows us to observe data quality values and take counteractions to balance data quality and performance. The framework has been designed along a DQ management methodology suited for data streams. It has been evaluated in the domains of transportation systems and health monitoring.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {18},
numpages = {34},
keywords = {ontologies, data quality assessment, data quality control, Data streams}
}

@article{10.1145/2950109,
author = {D'Urso, Ciro},
title = {EXPERIENCE: Glitches in Databases, How to Ensure Data Quality by Outlier Detection Techniques},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2950109},
doi = {10.1145/2950109},
abstract = {Enterprise's archives are inevitably affected by the presence of data quality problems (also called glitches). This article proposes the application of a new method to analyze the quality of datasets stored in the tables of a database, with no knowledge of the semantics of the data and without the need to define repositories of rules. The proposed method is based on proper revisions of different approaches for outlier detection that are combined to boost overall performance and accuracy. A novel transformation algorithm is conceived that treats the items in database tables as data points in real coordinate space of n dimensions, so that fields containing dates and fields containing text are processed to calculate distances between those data points. The implementation of an iterative approach ensures that global and local outliers are discovered even if they are subject, primarily in datasets with multiple outliers or clusters of outliers, to masking and swamping effects. The application of the method to a set of archives, some of which have been studied extensively in the literature, provides very promising experimental results and outperforms the application of a single other technique. Finally, a list of future research directions is highlighted.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {14},
numpages = {22},
keywords = {outlier identification, databases, data preparation for econometrics of public policy evaluation, Data quality process}
}

@article{10.1145/2935751,
author = {Gelernter, J. and Jha, J.},
title = {Challenges in Ontology Evaluation},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935751},
doi = {10.1145/2935751},
journal = {J. Data and Information Quality},
month = aug,
articleno = {11},
numpages = {4},
keywords = {Ontology repair, ontology editing, ontology validation}
}

@article{10.1145/2935753,
author = {Berti-Equille, Laure and Ba, Mouhamadou Lamine},
title = {Veracity of Big Data: Challenges of Cross-Modal Truth Discovery},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935753},
doi = {10.1145/2935753},
journal = {J. Data and Information Quality},
month = aug,
articleno = {12},
numpages = {3},
keywords = {fact checking, Truth discovery, data quality, data fusion, information extraction}
}

@article{10.1145/2883616,
author = {Chiang, Fei and Sitaramachandran, Siddharth},
title = {Unifying Data and Constraint Repairs},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2883616},
doi = {10.1145/2883616},
abstract = {Integrity constraints play an important role in data design. However, in an operational database, they may not be enforced for many reasons. Hence, over time, data may become inconsistent with respect to the constraints. To manage this, several approaches have proposed techniques to repair the data by finding minimal or lowest cost changes to the data that make it consistent with the constraints. Such techniques are appropriate for applications where only the data changes, but schemas and their constraints remain fixed. In many modern applications, however, constraints may evolve over time as application or business rules change, as data are integrated with new data sources or as the underlying semantics of the data evolves. In such settings, when an inconsistency occurs, it is no longer clear if there is an error in the data (and the data should be repaired) or if the constraints have evolved (and the constraints should be repaired). In this work, we present a novel unified cost model that allows data and constraint repairs to be compared on an equal footing. We consider repairs over a database that is inconsistent with respect to a set of rules, modeled as functional dependencies (FDs). FDs are the most common type of constraint and are known to play an important role in maintaining data quality. We propose modifications to the data and to the FDs such that the data and the constraints are better aligned. We evaluate the quality and scalability of our repair algorithms over synthetic and real datasets. The results show that our repair algorithms not only scale well for large datasets but also are able to accurately capture and correct inconsistencies and accurately decide when a data repair versus a constraint repair is best.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {9},
numpages = {26},
keywords = {data repair, Data quality, constraint repair}
}

@article{10.1145/2939377,
author = {Maltese, Vincenzo and Giunchiglia, Fausto},
title = {Search and Analytics Challenges in Digital Libraries and Archives},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2939377},
doi = {10.1145/2939377},
journal = {J. Data and Information Quality},
month = aug,
articleno = {10},
numpages = {3},
keywords = {data integration, analytics, Cataloging, knowledge graphs}
}

@article{10.1145/2899003,
author = {Haralabopoulos, Giannis and Anagnostopoulos, Ioannis and Zeadally, Sherali},
title = {The Challenge of Improving Credibility of User-Generated Content in Online Social Networks},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2899003},
doi = {10.1145/2899003},
abstract = {In every environment of information exchange, Information Quality (IQ) is considered one of the most important issues. Studies in Online Social Networks (OSNs) analyze a number of related subjects that span both theoretical and practical aspects, from data quality identification and simple attribute classification to quality assessment models for various social environments. Among several factors that affect information quality in online social networks is the credibility of user-generated content. To address this challenge, some proposed solutions include community-based evaluation and labeling of user-generated content in terms of accuracy, clarity, and timeliness, along with well-established real-time data mining techniques.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {13},
numpages = {4},
keywords = {information quality, credibility, User-generated content}
}

@article{10.1145/2893482,
author = {Aiken, Peter},
title = {EXPERIENCE: Succeeding at Data Management—BigCo Attempts to Leverage Data},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2893482},
doi = {10.1145/2893482},
abstract = {In a manner similar to most organizations, BigCompany (BigCo) was determined to benefit strategically from its widely recognized and vast quantities of data. (U.S. government agencies make regular visits to BigCo to learn from its experiences in this area.) When faced with an explosion in data volume, increases in complexity, and a need to respond to changing conditions, BigCo struggled to respond using a traditional, information technology (IT) project-based approach to address these challenges. As BigCo was not data knowledgeable, it did not realize that traditional approaches could not work. Two full years into the initiative, BigCo was far from achieving its initial goals. How much more time, money, and effort would be required before results were achieved? Moreover, could the results be achieved in time to support a larger, critical, technology-driven challenge that also depended on solving the data challenges? While these questions remain unaddressed, these considerations increase our collective understanding of data assets as separate from IT projects. Only by reconceiving data as a strategic asset can organizations begin to address these new challenges. Transformation to a data-driven culture requires far more than technology, which remains just one of three required “stool legs” (people and process being the other two). Seven prerequisites to effectively leveraging data are necessary, but insufficient awareness exists in most organizations—hence, the widespread misfires in these areas, especially when attempting to implement the so-called big data initiatives. Refocusing on foundational data management practices is required for all organizations, regardless of their organizational or data strategies.},
journal = {J. Data and Information Quality},
month = may,
articleno = {8},
numpages = {35},
keywords = {chief data officer, BigCo, data, data warehousing, Data management, policy, information systems, data integration, chief information officer, CDO, analytics, data architecture, strategy, organizational design, enterprise architecture, business intelligence, data governance, conceptual modeling, data stewardship, CIO, IT management, enterprise data executive}
}

@article{10.1145/2903723,
author = {Zhu, Hongwei and Lee, Yang W. and Rosenthal, Arnon S.},
title = {Data Standards Challenges for Interoperable and Quality Data},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2903723},
doi = {10.1145/2903723},
journal = {J. Data and Information Quality},
month = may,
articleno = {4},
numpages = {3},
keywords = {standards development, quality assessment, Data standards, interoperability, data quality}
}

@article{10.1145/2889311,
author = {Christen, Peter and Gayler, Ross W. and Tran, Khoi-Nguyen and Fisher, Jeffrey and Vatsalan, Dinusha},
title = {Automatic Discovery of Abnormal Values in Large Textual Databases},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2889311},
doi = {10.1145/2889311},
abstract = {Textual databases are ubiquitous in many application domains. Examples of textual data range from names and addresses of customers to social media posts and bibliographic records. With online services, individuals are increasingly required to enter their personal details for example when purchasing products online or registering for government services, while many social network and e-commerce sites allow users to post short comments. Many online sites leave open the possibility for people to enter unintended or malicious abnormal values, such as names with errors, bogus values, profane comments, or random character sequences. In other applications, such as online bibliographic databases or comparative online shopping sites, databases are increasingly populated in (semi-) automatic ways through Web crawls. This practice can result in low quality data being added automatically into a database. In this article, we develop three techniques to automatically discover abnormal (unexpected or unusual) values in large textual databases. Following recent work in categorical outlier detection, our assumption is that “normal” values are those that occur frequently in a database, while an individual abnormal value is rare. Our techniques are unsupervised and address the challenge of discovering abnormal values as an outlier detection problem. Our first technique is a basic but efficient q-gram set based technique, the second is based on a probabilistic language model, and the third employs morphological word features to train a one-class support vector machine classifier. Our aim is to investigate and develop techniques that are fast, efficient, and automatic. The output of our techniques can help in the development of rule-based data cleaning and information extraction systems, or be used as training data for further supervised data cleaning procedures. We evaluate our techniques on four large real-world datasets from different domains: two US voter registration databases containing personal details, the 2013 KDD Cup dataset of bibliographic records, and the SNAP Memetracker dataset of phrases from social networking sites. Our results show that our techniques can efficiently and automatically discover abnormal textual values, allowing an organization to conduct efficient data exploration, and improve the quality of their textual databases without the need of requiring explicit training data.},
journal = {J. Data and Information Quality},
month = apr,
articleno = {7},
numpages = {31},
keywords = {one-class classifier, String databases, word features, data quality, support vector machine, outlier detection, probabilistic language model, out-of-vocabulary}
}

@article{10.1145/2896822,
author = {Ulbricht, Robert and Donker, Hilko and Hartmann, Claudio and Hahmann, Martin and Lehner, Wolfgang},
title = {Challenges for Context-Driven Time Series Forecasting},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2896822},
doi = {10.1145/2896822},
abstract = {Predicting time series is a crucial task for organizations, since decisions are often based on uncertain information. Many forecasting models are designed from a generic statistical point of view. However, each real-world application requires domain-specific adaptations to obtain high-quality results. All such specifics are summarized by the term of context. In contrast to current approaches, we want to integrate context as the primary driver in the forecasting process. We introduce context-driven time series forecasting focusing on two exemplary domains: renewable energy and sparse sales data. In view of this, we discuss the challenge of context integration in the individual process steps.},
journal = {J. Data and Information Quality},
month = apr,
articleno = {5},
numpages = {4},
keywords = {sales data, forecast evaluation, model selection, Uncertain data, renewable energy}
}

@article{10.1145/2850420,
author = {Millar, Jeremy R. and Hodson, Douglas D. and Peterson, Gilbert L. and Ahner, Darryl K.},
title = {Data Quality Challenges in Distributed Live-Virtual-Constructive Test Environments},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2850420},
doi = {10.1145/2850420},
journal = {J. Data and Information Quality},
month = apr,
articleno = {2},
numpages = {3},
keywords = {performance estimation, Distributed simulation}
}

@article{10.1145/2834123,
author = {Chapman, Adriane P. and Rosenthal, Arnon and Seligman, Len},
title = {The Challenge of “Quick and Dirty” Information Quality},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2834123},
doi = {10.1145/2834123},
journal = {J. Data and Information Quality},
month = feb,
articleno = {1},
numpages = {4},
keywords = {data sharing, relevance feedback, information search and retrieval, System management, online information sharing, quality assurance}
}

@article{10.1145/2856038,
author = {Lukyanenko, Roman},
title = {Information Quality Research Challenge: Information Quality in the Age of Ubiquitous Digital Intermediation},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2856038},
doi = {10.1145/2856038},
abstract = {As information technology becomes an integral part of daily life, increasingly, people understand the world around them by turning to digital sources as opposed to directly interacting with objects in the physical world. This has ushered in the age of Ubiquitous Digital Intermediation (UDI). With the explosion of UDI, the scope of Information Quality (IQ) research is due to expand dramatically as the challenge becomes to capture the wealth and nuances of human experience. This article presents three key changes to the IQ landscape brought about by UDI, including expansion of the scope of traditional IQ dimensions, digital to physical mapping challenge, and the increased need to manage content authenticity. UDI generates many novel questions and opportunities for the IQ research community.},
journal = {J. Data and Information Quality},
month = feb,
articleno = {3},
numpages = {3},
keywords = {content authenticity, digital to physical mapping, quality of emotions, Ubiquitous digital intermediation}
}

@article{10.1145/2818382,
author = {Ceolin, Davide and Groth, Paul and Maccatrozzo, Valentina and Fokkink, Wan and Hage, Willem Robert Van and Nottamkandath, Archana},
title = {Combining User Reputation and Provenance Analysis for Trust Assessment},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2818382},
doi = {10.1145/2818382},
abstract = {Trust is a broad concept that in many systems is often reduced to user reputation alone. However, user reputation is just one way to determine trust. The estimation of trust can be tackled from other perspectives as well, including by looking at provenance.Here, we present a complete pipeline for estimating the trustworthiness of artifacts given their provenance and a set of sample evaluations. The pipeline is composed of a series of algorithms for (1) extracting relevant provenance features, (2) generating stereotypes of user behavior from provenance features, (3) estimating the reputation of both stereotypes and users, (4) using a combination of user and stereotype reputations to estimate the trustworthiness of artifacts, and (5) selecting sets of artifacts to trust. These algorithms rely on the W3C PROV recommendations for provenance and on evidential reasoning by means of subjective logic.We evaluate the pipeline over two tagging datasets: tags and evaluations from the Netherlands Institute for Sound and Vision’s Waisda? video tagging platform, as well as crowdsourced annotations from the Steve.Museum project. The approach achieves up to 85% precision when predicting tag trustworthiness. Perhaps more importantly, the pipeline provides satisfactory results using relatively little evidence through the use of provenance.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {6},
numpages = {28},
keywords = {machine learning, subjective logic, Trust, uncertainty reasoning, provenance, tags}
}

@article{10.1145/2816821,
author = {Ramadan, Banda and Christen, Peter and Liang, Huizhi and Gayler, Ross W.},
title = {Dynamic Sorted Neighborhood Indexing for Real-Time Entity Resolution},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2816821},
doi = {10.1145/2816821},
abstract = {Real-time Entity Resolution (ER) is the process of matching query records in subsecond time with records in a database that represent the same real-world entity. Indexing techniques are generally used to efficiently extract a set of candidate records from the database that are similar to a query record, and that are to be compared with the query record in more detail. The sorted neighborhood indexing method, which sorts a database and compares records within a sliding window, has been successfully used for ER of large static databases. However, because it is based on static sorted arrays and is designed for batch ER that resolves all records in a database rather than resolving those relating to a single query record, this technique is not suitable for real-time ER on dynamic databases that are constantly updated. We propose a tree-based technique that facilitates dynamic indexing based on the sorted neighborhood method, which can be used for real-time ER, and investigate both static and adaptive window approaches. We propose an approach to reduce query matching times by precalculating the similarities between attribute values stored in neighboring tree nodes. We also propose a multitree solution where different sorting keys are used to reduce the effects of errors and variations in attribute values on matching quality by building several distinct index trees. We experimentally evaluate our proposed techniques on large real datasets, as well as on synthetic data with different data quality characteristics. Our results show that as the index grows, no appreciable increase occurs in both record insertion and query times, and that using multiple trees gives noticeable improvements on matching quality with only a small increase in query time. Compared to earlier indexing techniques for real-time ER, our approach achieves significantly reduced indexing and query matching times while maintaining high matching accuracy.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {15},
numpages = {29},
keywords = {Braided tree, record linkage, data matching, dynamic indexing, real-time query}
}

@article{10.1145/2818379,
author = {Ahlers, Dirk and Krogstie, John},
title = {Document and Corpus Quality Challenges for Knowledge Management in Engineering Enterprises},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2818379},
doi = {10.1145/2818379},
journal = {J. Data and Information Quality},
month = oct,
articleno = {14},
numpages = {3},
keywords = {Document quality, linkability, corpus quality}
}

@article{10.1145/2821019,
author = {Missier, Paolo},
title = {Corrigendum to the Special Issue Editorial in JDIQ Volume 5, Issue 3},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2821019},
doi = {10.1145/2821019},
journal = {J. Data and Information Quality},
month = oct,
articleno = {17},
numpages = {1}
}

@article{10.1145/2822898,
author = {Coletti, Paolo and Murgia, Maurizio},
title = {Design and Construction of a Historical Financial Database of the Italian Stock Market 1973--2011},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2822898},
doi = {10.1145/2822898},
abstract = {This article presents the technical aspects of designing and building a historical database of the Italian Stock Market. The database contains daily market data from 1973 to 2011 and is constructed by merging two main digital sources and several other hand-collected data sources. We analyzed and developed semiautomatic tools to deal with problems related to time-series matchings, quality of data, and numerical errors. We also developed a concatenation structure to allow the handling of company name changes, mergers, and spin-offs without artificially altering numerical series. At the same time, we maintained the transparency of the historical information on each individual company listed. Thanks to the overlapping of digital and hand-collected data, the completed database has a very high level of detail and accuracy. The dataset is particularly suited for any empirical research in financial economics and for more practically oriented numerical applications and forecasting simulations.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {16},
numpages = {23},
keywords = {data integration, stock market, Financial database, data quality}
}

@article{10.1145/2786983,
author = {Bartoli, Alberto and Lorenzo, Andrea De and Medvet, Eric and Tarlao, Fabiano},
title = {Data Quality Challenge: Toward a Tool for String Processing by Examples},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2786983},
doi = {10.1145/2786983},
journal = {J. Data and Information Quality},
month = oct,
articleno = {13},
numpages = {4},
keywords = {String processing, Programming by example}
}

@article{10.1145/2755489,
author = {Basole, Rahul C. and Braunstein, Mark L. and Sun, Jimeng},
title = {Data and Analytics Challenges for a Learning Healthcare System},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2755489},
doi = {10.1145/2755489},
journal = {J. Data and Information Quality},
month = jul,
articleno = {10},
numpages = {4},
keywords = {visualization, data analytics, Data mining, health informatics, data integration}
}

@article{10.1145/2774223,
author = {Zarraga-Rodriguez, Marta and Alvarez, M. Jesus},
title = {Experience: Information Dimensions Affecting Employees’ Perceptions Towards Being Well Informed},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2774223},
doi = {10.1145/2774223},
abstract = {Information is a strategic company resource, but there is no consensus in the literature regarding the set of dimensions to be considered when measuring the quality of the information. Most measures of information quality depend on user perception. Using multiple correlation analysis, we obtain a model that allows us to explain how information quality dimensions influence information consumers’ overall feeling of being well informed. A set of dimensions that any measure of information quality should at least include is proposed. This exploratory study reports the results of a research survey among managers of companies committed to quality management within the framework of a Total Quality Management (TQM) model, which is an information-intensive management model.},
journal = {J. Data and Information Quality},
month = jul,
articleno = {12},
numpages = {14},
keywords = {Information quality dimensions, information quality perceived, Total Quality Management (TQM)}
}

@article{10.1145/2762716,
author = {Naumann, Felix},
title = {Editorial},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2762716},
doi = {10.1145/2762716},
journal = {J. Data and Information Quality},
month = jun,
articleno = {4},
numpages = {1}
}

@article{10.1145/2740965,
author = {Sha, Kewei and Zeadally, Sherali},
title = {Data Quality Challenges in Cyber-Physical Systems},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2740965},
doi = {10.1145/2740965},
journal = {J. Data and Information Quality},
month = jun,
articleno = {8},
numpages = {4},
keywords = {Data quality, faculty data detection, cyber physical systems}
}

@article{10.1145/2736699,
author = {Gennari, Rosella and Tonelli, Sara and Vittorini, Pierpaolo},
title = {Challenges in Quality of Temporal Data — Starting with Gold Standards},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2736699},
doi = {10.1145/2736699},
journal = {J. Data and Information Quality},
month = jun,
articleno = {9},
numpages = {4},
keywords = {qualitative reasoning, Temporal data quality, TimeML}
}

@article{10.1145/2742550,
author = {Varshney, Kush R. and Wei, Dennis and Ramamurthy, Karthikeyan Natesan and Mojsilovi\'{c}, Aleksandra},
title = {Data Challenges in Disease Response: The 2014 Ebola Outbreak and Beyond},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2742550},
doi = {10.1145/2742550},
journal = {J. Data and Information Quality},
month = jun,
articleno = {5},
numpages = {3},
keywords = {data quality, humanitarian crisis, open data, Disaster response}
}

@article{10.1145/2744205,
author = {Todoran, Ion-George and Lecornu, Laurent and Khenchaf, Ali and Caillec, Jean-Marc Le},
title = {A Methodology to Evaluate Important Dimensions of Information Quality in Systems},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2744205},
doi = {10.1145/2744205},
abstract = {Assessing the quality of the information proposed by an information system has become one of the major research topics in the last two decades. A quick literature survey shows that a significant number of information quality frameworks are proposed in different domains of application: management information systems, web information systems, information fusion systems, and so forth. Unfortunately, they do not provide a feasible methodology that is both simple and intuitive to be implemented in practice. In order to address this need, we present in this article a new information quality methodology. Our methodology makes use of existing frameworks and proposes a three-step process capable of tracking the quality changes through the system. In the first step and as a novelty compared to existing studies, we propose decomposing the information system into its elementary modules. Having access to each module allows us to locally define the information quality. Then, in the second step, we model each processing module by a quality transfer function, capturing the module’s influence over the information quality. In the third step, we make use of the previous two steps in order to estimate the quality of the entire information system. Thus, our methodology allows informing the end-user on both output quality and local quality. The proof of concept of our methodology has been carried out considering two applications: an automatic target recognition system and a diagnosis coding support system.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {11},
numpages = {23},
keywords = {complex information system, quality transfer function, quality measures, Information quality}
}

@article{10.1145/2738044,
author = {Grant, Christan Earl and Wang, Daisy Zhe},
title = {A Challenge for Long-Term Knowledge Base Maintenance},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2738044},
doi = {10.1145/2738044},
journal = {J. Data and Information Quality},
month = jun,
articleno = {7},
numpages = {3},
keywords = {probabilistic knowledge base, entity resolution, inference, databases, Knowledge base}
}

@article{10.1145/2747881,
author = {Barnaghi, Payam and Bermudez-Edo, Maria and T\"{o}njes, Ralf},
title = {Challenges for Quality of Data in Smart Cities},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2–3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2747881},
doi = {10.1145/2747881},
journal = {J. Data and Information Quality},
month = jun,
articleno = {6},
numpages = {4},
keywords = {Quality of information, Internet of Things, smart cities}
}

@article{10.1145/2723166,
author = {Lukyanenko, Roman and Parsons, Jeffrey},
title = {Information Quality Research Challenge: Adapting Information Quality Principles to User-Generated Content},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2723166},
doi = {10.1145/2723166},
journal = {J. Data and Information Quality},
month = mar,
articleno = {3},
numpages = {3},
keywords = {User-generated content}
}

@article{10.1145/2724721,
author = {Alonso, Omar},
title = {Challenges with Label Quality for Supervised Learning},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2724721},
doi = {10.1145/2724721},
abstract = {Organizations that develop and use technologies around information retrieval, machine learning, recommender systems, and natural language processing depend on labels for engineering and experimentation. These labels, usually gathered via human computation, are used in machine-learned models for prediction and evaluation purposes. In such scenarios, collecting high-quality labels is a very important part of the overall process. We elaborate on these challenges and discuss research directions.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {2},
numpages = {3},
keywords = {Label quality, crowdsourcing, human computation, machine learning}
}

@article{10.1145/2700832,
author = {Attenberg, Joshua and Ipeirotis, Panos and Provost, Foster},
title = {Beat the Machine: Challenging Humans to Find a Predictive Model's “Unknown Unknowns”},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2700832},
doi = {10.1145/2700832},
abstract = {We present techniques for gathering data that expose errors of automatic predictive models. In certain common settings, traditional methods for evaluating predictive models tend to miss rare but important errors—most importantly, cases for which the model is confident of its prediction (but wrong). In this article, we present a system that, in a game-like setting, asks humans to identify cases that will cause the predictive model-based system to fail. Such techniques are valuable in discovering problematic cases that may not reveal themselves during the normal operation of the system and may include cases that are rare but catastrophic. We describe the design of the system, including design iterations that did not quite work. In particular, the system incentivizes humans to provide examples that are difficult for the model to handle by providing a reward proportional to the magnitude of the predictive model's error. The humans are asked to “Beat the Machine” and find cases where the automatic model (“the Machine”) is wrong. Experiments show that the humans using Beat the Machine identify more errors than do traditional techniques for discovering errors in predictive models, and, indeed, they identify many more errors where the machine is (wrongly) confident it is correct. Furthermore, those cases the humans identify seem to be not simply outliers, but coherent areas missed completely by the model. Beat the Machine identifies the “unknown unknowns.” Beat the Machine has been deployed at an industrial scale by several companies. The main impact has been that firms are changing their perspective on and practice of evaluating predictive models.“There are known knowns. These are things we know that we know. There are known unknowns. That is to say, there are things that we know we don't know. But there are also unknown unknowns. There are things we don't know we don't know.”--Donald Rumsfeld},
journal = {J. Data and Information Quality},
month = mar,
articleno = {1},
numpages = {17},
keywords = {machine learning evaluation, Crowdsourcing, system design, model assessment, incentives, risk identification}
}

@article{10.1145/2692312,
author = {Missier, Paolo and Papotti, Paolo},
title = {Editorial},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2692312},
doi = {10.1145/2692312},
journal = {J. Data and Information Quality},
month = mar,
articleno = {8},
numpages = {4}
}

@article{10.1145/2665070,
author = {Herschel, Melanie},
title = {A Hybrid Approach to Answering Why-Not Questions on Relational Query Results},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2665070},
doi = {10.1145/2665070},
abstract = {In analyzing and debugging data transformations, or more specifically relational queries, a subproblem is to understand why some data are not part of the query result. This problem has recently been addressed from different perspectives for various fragments of relational queries. The different perspectives yield different yet complementary explanations of such missing answers.This article first aims at unifying the different approaches by defining a new type of explanation, called hybrid explanation, that encompasses the variety of previously defined types of explanations. This solution goes beyond simply forming the union of explanations produced by different algorithms and is shown to be able to explain a larger set of missing answers. Second, we present Conseil, an algorithm to generate hybrid explanations. Conseil is also the first algorithm to handle nonmonotonic queries. Experiments on efficiency and explanation quality show that Conseil is comparable and even outperforms previous algorithms.This article extends a previous short conference paper by providing proofs, additional theorems, and a detailed discussion of each step of the Conseil algorithm. It also significantly extends the experimental evaluation on efficiency and explanation quality.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {10},
numpages = {29},
keywords = {Why-not question, query debugging, data provenance, query analysis}
}

@article{10.1145/2700413,
author = {Baillie, Chris and Edwards, Peter and Pignotti, Edoardo},
title = {QUAL: A Provenance-Aware Quality Model},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2700413},
doi = {10.1145/2700413},
abstract = {In this article, we present a model for quality assessment over linked data. This model has been designed to align with emerging standards for provenance on the Web to enable agents to reason about data provenance when performing quality assessment. The model also enables quality assessment provenance to be represented, thus allowing agents to make decisions about reuse of existing assessments. We also discuss the development of an OWL ontology as part of a software framework to support reasoning about data quality and assessment reuse. Finally, we evaluate this framework using two real-world case studies derived from transport and invasive-species monitoring applications.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {12},
numpages = {22},
keywords = {Linked Data, provenance, Semantic Web, ontology, Data quality}
}

@article{10.1145/2687422,
author = {Chong, Stephen and Skalka, Christian and Vaughan, Jeffrey A.},
title = {Self-Identifying Data for Fair Use},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2687422},
doi = {10.1145/2687422},
abstract = {Public-use earth science datasets are a useful resource with the unfortunate feature that their provenance is easily disconnected from their content. “Fair-use policies” typically associated with these datasets require appropriate attribution of providers by users, but sound and complete attribution is difficult if provenance information is lost. To address this, we introduce a technique to directly associate provenance information with sensor datasets. Our technique is similar to traditional watermarking but is intended for application to unstructured time-series datasets. Our approach is potentially imperceptible given sufficient margins of error in datasets and is robust to a number of benign but likely transformations including truncation, rounding, bit-flipping, sampling, and reordering. We provide algorithms for both one-bit and blind mark checking and show how our system can be adapted to various data representation types. Our algorithms are probabilistic in nature and are characterized by both combinatorial and empirical analyses. Mark embedding can be applied at any point in the data life cycle, allowing adaptation of our scheme to social or scientific concerns.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {11},
numpages = {30},
keywords = {self-identifying data, Provenance}
}

@article{10.1145/2700833,
author = {Xu, Hongjiang},
title = {What Are the Most Important Factors for Accounting Information Quality and Their Impact on AIS Data Quality Outcomes?},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2700833},
doi = {10.1145/2700833},
abstract = {The accounting information system (AIS) is one of the most critical systems in any organization. Data quality plays a critical role in a data-intensive, knowledge-based economy. The objective of this study is to identify the most important factors for accounting information quality and their impact on AIS data quality outcomes. The article includes an extensive literature review and summarizes studies in quality management, data quality, accounting information systems, and enterprise planning in helping to identify a set of critical success factors for data quality. The study uses empirical data to answer the research question and test the research hypothesis. Study results show that the top three most important factors that affect accounting information systems’ data quality are top management commitment, the nature of the accounting information systems (such as the suitability of the systems), and input controls. The article further uses regression analysis to test the effect of those factors on AIS data quality, finding that there is a significant positive relationship between the perceived performance of the three most important factors and perceived AIS data quality outcomes.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {14},
numpages = {22},
keywords = {accounting information systems, critical success factors, Data quality}
}

@article{10.1145/2641575,
author = {Mezzanzanica, Mario and Boselli, Roberto and Cesarini, Mirko and Mercorio, Fabio},
title = {A Model-Based Approach for Developing Data Cleansing Solutions},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2641575},
doi = {10.1145/2641575},
abstract = {The data extracted from electronic archives is a valuable asset; however, the issue of the (poor) data quality should be addressed before performing data analysis and decision-making activities. Poor data quality is frequently cleansed using business rules derived from domain knowledge. Unfortunately, the process of designing and implementing cleansing activities based on business rules requires a relevant effort. In this article, we illustrate a model-based approach useful to perform inconsistency identification and corrective interventions, thus simplifying the process of developing cleansing activities. The article shows how the cleansing activities required to perform a sensitivity analysis can be easily developed using the proposed model-based approach. The sensitivity analysis provides insights on how the cleansing activities can affect the results of indicators computation. The approach has been successfully used on a database describing the working histories of an Italian area population. A model formalizing how data should evolve over time (i.e., a data consistency model) in such domain was created (by means of formal methods) and used to perform the cleansing and sensitivity analysis activities.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {13},
numpages = {28},
keywords = {data verification, data consistency, data believability, ETL, Data quality}
}

@article{10.1145/2665069,
author = {Cheah, You-Wei and Plale, Beth},
title = {Provenance Quality Assessment Methodology and Framework},
year = {2014},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2665069},
doi = {10.1145/2665069},
abstract = {Data provenance, a form of metadata describing the life cycle of a data product, is crucial in the sharing of research data. Research data, when shared over decades, requires recipients to make a determination of both use and trust. That is, can they use the data? More importantly, can they trust it? Knowing the data are of high quality is one factor to establishing fitness for use and trust. Provenance can be used to assert the quality of the data, but the quality of the provenance must be known as well. We propose a framework for assessing the quality of data provenance. We identify quality issues in data provenance, establish key quality dimensions, and define a framework of analysis. We apply the analysis framework to synthetic and real-world provenance.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {9},
numpages = {20},
keywords = {scientific workflow, Data provenance, graph analysis, provenance quality}
}

@article{10.1145/2629568,
author = {Glowalla, Paul and Sunyaev, Ali},
title = {Process-Driven Data Quality Management: A Critical Review on the Application of Process Modeling Languages},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2629568},
doi = {10.1145/2629568},
abstract = {Data quality is critical to organizational success. In order to improve and sustain data quality in the long term, process-driven data quality management (PDDQM) seeks to redesign processes that create or modify data. Consequently, process modeling is mandatory for PDDQM. Current research examines process modeling languages with respect to representational capabilities. However, there is a gap, since process modeling languages for PDDQM are not considered. We address this research gap by providing a synthesis of the varying applications of process modeling languages for PDDQM. We conducted a keyword-based literature review in conferences as well as 74 highranked information systems and computer science journals, reviewing 1,555 articles from 1995 onwards. For practitioners, it is possible to integrate the quality perspective within broadly applied process models. For further research, we derive representational requirements for PDDQM that should be integrated within existing process modeling languages. However, there is a need for further representational analysis to examine the adequacy of upcoming process modeling languages. New or enhanced process modeling languages may substitute for PDDQM-specific process modeling languages and facilitate development of a broadly applicable and accepted process modeling language for PDDQM.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {7},
numpages = {30},
keywords = {conceptual modeling, data and knowledge visualization., Information quality, process modeling, data quality}
}

@article{10.1145/2648781,
author = {Naumann, Felix},
title = {Editorial},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2648781},
doi = {10.1145/2648781},
journal = {J. Data and Information Quality},
month = sep,
articleno = {1},
numpages = {1}
}

@article{10.1145/2629687,
author = {Vogel, Tobias and Heise, Arvid and Draisbach, Uwe and Lange, Dustin and Naumann, Felix},
title = {Reach for Gold: An Annealing Standard to Evaluate Duplicate Detection Results},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2629687},
doi = {10.1145/2629687},
abstract = {Duplicates in a database are one of the prime causes of poor data quality and are at the same time among the most difficult data quality problems to alleviate. To detect and remove such duplicates, many commercial and academic products and methods have been developed. The evaluation of such systems is usually in need of pre-classified results. Such gold standards are often expensive to come by (much manual classification is necessary), not representative (too small or too synthetic), and proprietary and thus preclude repetition (company-internal data). This lament has been uttered in many papers and even more paper reviews.The proposed annealing standard is a structured set of duplicate detection results, some of which are manually verified and some of which are merely validated by many classifiers. As more and more classifiers are evaluated against the annealing standard, more and more results are verified and validation becomes more and more confident. We formally define gold, silver, and the annealing standard and their maintenance. Experiments show how quickly an annealing standard converges to a gold standard. Finally, we provide an annealing standard for 750,000 CDs to the duplicate detection community.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {5},
numpages = {25},
keywords = {gold standard, Annealing standard, duplicate detection, silver standard, classification}
}

@article{10.1145/2629603,
author = {Talburt, John and Williams, Therese L. and Redman, Thomas C. and Becker, David},
title = {Information Quality Research Challenge: Predicting and Quantifying the Impact of Social Issues on Information Quality Programs},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2629603},
doi = {10.1145/2629603},
journal = {J. Data and Information Quality},
month = sep,
articleno = {2},
numpages = {3},
keywords = {Information quality, cultural success factors, data quality, research problems, social issues}
}

@article{10.1145/2629604,
author = {Christen, Peter and Vatsalan, Dinusha and Verykios, Vassilios S.},
title = {Challenges for Privacy Preservation in Data Integration},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2629604},
doi = {10.1145/2629604},
abstract = {Techniques for integrating data from diverse sources have attracted significant interest in recent years. Much of today’s data collected by businesses and governments are about people, and integrating such data across organizations can raise privacy concerns. Various techniques that preserve privacy during data integration have been developed, but several challenges persist that need to be solved before such techniques become useful in practical applications. We elaborate on these challenges and discuss research directions.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {4},
numpages = {3},
keywords = {data matching, privacy-preserving record linkage, Privacy techniques}
}

@article{10.1145/2629605,
author = {Rahm, Erhard},
title = {Discovering Product Counterfeits in Online Shops: A Big Data Integration Challenge},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2629605},
doi = {10.1145/2629605},
journal = {J. Data and Information Quality},
month = sep,
articleno = {3},
numpages = {3}
}

@article{10.1145/2631923,
author = {Fan, Wenfei and Geerts, Floris and Tang, Nan and Yu, Wenyuan},
title = {Conflict Resolution with Data Currency and Consistency},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2631923},
doi = {10.1145/2631923},
abstract = {This article introduces a new approach for conflict resolution: given a set of tuples pertaining to the same entity, it identifies a single tuple in which each attribute has the latest and consistent value in the set. This problem is important in data integration, data cleaning, and query answering. It is, however, challenging since in practice, reliable time stamps are often absent, among other things. We propose a model for conflict resolution by specifying data currency in terms of partial currency orders and currency constraints and by enforcing data consistency with constant conditional functional dependencies. We show that identifying data currency orders helps us repair inconsistent data, and vice versa. We investigate a number of fundamental problems associated with conflict resolution and establish their complexity. In addition, we introduce a framework and develop algorithms for conflict resolution by integrating data currency and consistency inferences into a single process and by interacting with users. We experimentally verify the accuracy and efficiency of our methods using real-life and synthetic data.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {6},
numpages = {37},
keywords = {data cleaning, currency constraints, Conditional functional dependency}
}

@article{10.1145/2579167,
author = {Raschid, Louiqa},
title = {Editorial},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2579167},
doi = {10.1145/2579167},
journal = {J. Data and Information Quality},
month = may,
articleno = {14},
numpages = {2}
}

@article{10.1145/2567663,
author = {Martin, Nigel and Poulovassilis, Alexandra and Wang, Jianing},
title = {A Methodology and Architecture Embedding Quality Assessment in Data Integration},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2567663},
doi = {10.1145/2567663},
abstract = {Data integration aims to combine heterogeneous information sources and to provide interfaces for accessing the integrated resource. Data integration is a collaborative task that may involve many people with different degrees of experience, knowledge of the application domain, and expectations relating to the integrated resource. It may be difficult to determine and control the quality of an integrated resource due to these factors. In this article, we propose a data integration methodology that has embedded within it iterative quality assessment and improvement of the integrated resource. We also propose an architecture for the realisation of this methodology. The quality assessment is based on an ontology representation of different users’ quality requirements and of the main elements of the integrated resource. We use description logic as the formal basis for reasoning about users’ quality requirements and for validating that an integrated resource satisfies these requirements. We define quality factors and associated metrics which enable the quality of alternative global schemas for an integrated resource to be assessed quantitively, and hence the improvement which results from the refinement of a global schema following our methodology to be measured. We evaluate our approach through a large-scale real-life case study in biological data integration in which an integrated resource is constructed from three autononous proteomics data sources.},
journal = {J. Data and Information Quality},
month = may,
articleno = {17},
numpages = {40},
keywords = {data quality assessment, Data integration, data quality}
}

@article{10.1145/2567656,
author = {Wijnhoven, Fons and Amrit, Chintan and Dietz, Pim},
title = {Value-Based File Retention: File Attributes as File Value and Information Waste Indicators},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2567656},
doi = {10.1145/2567656},
abstract = {Several file retention policy methods propose that a file retention policy should be based on file value. Though such a retention policy might increase the value of accessible files, the method to arrive at such a policy is underresearched. This article discusses how one can arrive at a method for developing file retention policies based on the use values of files. The method’s applicability is initially assessed through a case study at Capgemini, Netherlands. In the case study, we hypothesize that one can develop a file retention policy by testing causal relations between file attributes (as used by file retention methods) and the use value of files. Unfortunately, most file attributes used by file retention methods have a weak correlation with file value, resulting in the conclusion that these methods do not well select out high- and low-value files. This would imply the ineffectiveness of the used attributes in our study or errors in our conceptualization of file value. We continue with the last possibility and develop indicators for file utility (with low utility being waste). With this approach we were able to detect waste files, in a sample of files, with an accuracy of 80%. We therefore not only suggest further research in information waste detection as part of a file retention policy, but also to further explore other file attributes that could better predict file value and file utility.},
journal = {J. Data and Information Quality},
month = may,
articleno = {15},
numpages = {17},
keywords = {data mining, case study, Methodology, quantitative}
}

@article{10.1145/2567657,
author = {Fan, Wenfei and Ma, Shuai and Tang, Nan and Yu, Wenyuan},
title = {Interaction between Record Matching and Data Repairing},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2567657},
doi = {10.1145/2567657},
abstract = { Central to a data cleaning system are record matching and data repairing. Matching aims to identify tuples that refer to the same real-world object, and repairing is to make a database consistent by fixing errors in the data by using integrity constraints. These are typically treated as separate processes in current data cleaning systems, based on heuristic solutions. This article studies a new problem in connection with data cleaning, namely the interaction between record matching and data repairing. We show that repairing can effectively help us identify matches, and vice versa. To capture the interaction, we provide a uniform framework that seamlessly unifies repairing and matching operations to clean a database based on integrity constraints, matching rules, and master data. We give a full treatment of fundamental problems associated with data cleaning via matching and repairing, including the static analyses of constraints and rules taken together, and the complexity, termination, and determinism analyses of data cleaning. We show that these problems are hard, ranging from NP-complete or coNP-complete, to PSPACE-complete. Nevertheless, we propose efficient algorithms to clean data via both matching and repairing. The algorithms find deterministic fixes and reliable fixes based on confidence and entropy analyses, respectively, which are more accurate than fixes generated by heuristics. Heuristic fixes are produced only when deterministic or reliable fixes are unavailable. We experimentally verify that our techniques can significantly improve the accuracy of record matching and data repairing that are taken as separate processes, using real-life and synthetic data.},
journal = {J. Data and Information Quality},
month = may,
articleno = {16},
numpages = {38},
keywords = {conditional functional dependency, matching dependency, record matching, Data repairing}
}

@article{10.1145/2458517.2458520,
author = {Biran, Dov and Zack, Michael H. and Briotta, Richard J.},
title = {Competitive Intelligence and Information Quality: A Game-Theoretic Perspective},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2458517.2458520},
doi = {10.1145/2458517.2458520},
abstract = {To better understand a competitor's tactical and strategic plans, companies need to take a closer look at competitive intelligence or they risk missing lucrative opportunities. Because of this there is a growing interest in competitive intelligence and intelligence information gathering systems (IIS). This article uses game-theoretic concepts to develop an analytic framework to assess the value of deploying a competitive intelligence gathering information system. Modeling the competitive environment as a game provides a useful approach to study and evaluate competitive strategies given diverse assumptions about the quality of the information known by the players. When determining the value of deploying an IIS, decision makers need to examine three components of the competitive environment: the competitive rules of the game, the state of player knowledge, and the reliability of the information gathered. This framework focuses on competitive environments where the players' state of knowledge (i.e., common versus covert knowledge) and the reliability of the information generated are essential to the decision making process. The article concludes with implications for research and practice.},
journal = {J. Data and Information Quality},
month = may,
articleno = {12},
numpages = {20},
keywords = {strategic knowledge management, Game theory, knowledge management, competitive strategy, intelligence information gathering systems, competitive intelligence}
}

@article{10.1145/2458517.2458521,
author = {Joglekar, Nitin R. and Anderson, Edward G. and Shankaranarayanan, G.},
title = {Accuracy of Aggregate Data in Distributed Project Settings: Model, Analysis and Implications},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2458517.2458521},
doi = {10.1145/2458517.2458521},
abstract = {We examine the management of data accuracy in inter-organizational data exchanges using the context of distributed software projects. Organizations typically manage projects by outsourcing portions of the project to partners. Managing a portfolio of such projects requires sharing data regarding the status of work-in-progress residing with the partners and estimates of these projects' completion times. Portfolio managers use these data to assign projects to be outsourced to partners. These data are rarely accurate. Unless these data are filtered, inaccuracies can lead to myopic and expensive sourcing decisions. We develop a model that uses project-status data to identify an optimal assignment of projects to be outsourced. This model permits corruption of project-status data. We use this model to compute the costs of using perfect versus inaccurate project-status data and show that the costs of deviation from optimal are sizable when the inaccuracy in the data is significant. We further propose a filter to correct inaccurate project-status data and generate an estimate of true progress. With this filter, depending on the relative magnitudes of errors, we show that accuracy of project-status data can be improved and the associated economic benefit is significant. We illustrate the improvement in accuracy and associated economic benefit by instantiating the model and the filter. We further elaborate on how the model parameters may be estimated and used in practice.},
journal = {J. Data and Information Quality},
month = may,
articleno = {13},
numpages = {22},
keywords = {data quality, value of data, Aggregate project planning, Kalman filtering}
}

@article{10.1145/2458517.2458519,
author = {Cao, Lan and Zhu, Hongwei},
title = {Normal Accidents: Data Quality Problems in ERP-Enabled Manufacturing},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2458517.2458519},
doi = {10.1145/2458517.2458519},
abstract = {The efficient operation of Enterprise Resource Planning (ERP) systems largely depends on data quality. ERP can improve data quality and information sharing within an organization. It can also pose challenges to data quality. While it is well known that data quality is important in ERP systems, most existing research has focused on identifying the factors affecting the implementation and the business values of ERP. With normal accident theory as a theoretical lens, we examine data quality problems in ERP using a case study of a large, fast-growing multinational manufacturer headquartered in China. Our findings show that organizations that have successfully implemented ERP can still experience certain data quality problems. We identify major data quality problems in data production, storage and maintenance, and utilization processes. We also analyze the causes of these data quality problems by linking them to certain characteristics of ERP systems within an organizational context. Our analysis shows that problems resulting from the tight coupling effects and the complexity of ERP-enabled manufacturing systems can be inevitable. This study will help researchers and practitioners formulate data management strategies that are effective in the presence of certain “normal” data quality problems.},
journal = {J. Data and Information Quality},
month = may,
articleno = {11},
numpages = {26},
keywords = {normal accident, enterprise resource planning, ERP, complexity, tight coupling, Data quality}
}

@article{10.1145/2435221.2435225,
author = {Panse, Fabian and van Keulen, Maurice and Ritter, Norbert},
title = {Indeterministic Handling of Uncertain Decisions in Deduplication},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2435221.2435225},
doi = {10.1145/2435221.2435225},
abstract = {In current research and practice, deduplication is usually considered as a deterministic approach in which database tuples are either declared to be duplicates or not. In ambiguous situations, however, it is often not completely clear-cut, which tuples represent the same real-world entity. In deterministic approaches, many realistic possibilities may be ignored, which in turn can lead to false decisions. In this article, we present an indeterministic approach for deduplication by using a probabilistic target model including techniques for proper probabilistic interpretation of similarity matching results. Thus, instead of deciding for one of the most likely situations, all realistic situations are modeled in the resultant data. This approach minimizes the negative impact of false decisions. Moreover, the deduplication process becomes almost fully automatic and human effort can be largely reduced. To increase applicability, we introduce several semi-indeterministic methods that heuristically reduce the set of indeterministically handled decisions in several meaningful ways. We also describe a full-indeterministic method for theoretical and presentational reasons.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {9},
numpages = {25},
keywords = {Uncertainty, Probabilistic Data, Deduplication}
}

@article{10.1145/2435221.2435223,
author = {Song, Dezhao and Heflin, Jeff},
title = {Domain-Independent Entity Coreference for Linking Ontology Instances},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2435221.2435223},
doi = {10.1145/2435221.2435223},
abstract = {The objective of entity coreference is to determine if different mentions (e.g., person names, place names, database records, ontology instances, etc.) refer to the same real word object. Entity coreference algorithms can be used to detect duplicate database records and to determine if two Semantic Web instances represent the same underlying real word entity. The key issues in developing an entity coreference algorithm include how to locate context information and how to utilize the context appropriately. In this article, we present a novel entity coreference algorithm for ontology instances. For scalability reasons, we select a neighborhood of each instance from an RDF graph. To determine the similarity between two instances, our algorithm computes the similarity between comparable property values in the neighborhood graphs. The similarity of distinct URIs and blank nodes is computed by comparing their outgoing links. In an attempt to reduce the impact of distant nodes on the final similarity measure, we explore a distance-based discounting approach. To provide the best possible domain-independent matches, we propose an approach to compute the discriminability of triples in order to assign weights to the context information. We evaluated our algorithm using different instance categories from five datasets. Our experiments show that the best results are achieved by including both our discounting and triple discrimination approaches.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {7},
numpages = {29},
keywords = {discriminability, domain-independence, ontology, semantic web, Entity coreference}
}

@article{10.1145/2435221.2435226,
author = {Zhou, Yinle and Nelson, Eric and Kobayashi, Fumiko and Talburt, John R.},
title = {A Graduate-Level Course on Entity Resolution and Information Quality: A Step toward ER Education},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2435221.2435226},
doi = {10.1145/2435221.2435226},
abstract = {This article discusses the topics, approaches, and lessons learned in teaching a graduate-level course covering entity resolution (ER) and its relationship to information quality (IQ). The course surveys a broad spectrum of ER topics and activities including entity reference extraction, entity reference preparation, entity reference resolution techniques, entity identity management, and entity relationship analysis. The course content also attempts to balance aspects of ER theory with practical application through a series of laboratory exercises coordinated with the lecture topics. As an additional teaching aid, a configurable, open-source entity resolution engine (OYSTER) was developed that allows students to experience with different types of ER architectures including merge-purge, record linking, identity resolution, and identity capture.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {10},
numpages = {10},
keywords = {corporate house-holding, graduate-level ER course, information quality, record linkage, measurement, data quality, Entity resolution}
}

@article{10.1145/2435221.2435222,
author = {Talburt, John R.},
title = {SPECIAL ISSUE ON ENTITY RESOLUTION Overview: The Criticality of Entity Resolution in Data and Information Quality},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2435221.2435222},
doi = {10.1145/2435221.2435222},
journal = {J. Data and Information Quality},
month = mar,
articleno = {6},
numpages = {2}
}

@article{10.1145/2435221.2435224,
author = {Nuray-Turan, Rabia and Kalashnikov, Dmitri V. and Mehrotra, Sharad},
title = {Adaptive Connection Strength Models for Relationship-Based Entity Resolution},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2435221.2435224},
doi = {10.1145/2435221.2435224},
abstract = {Entity Resolution (ER) is a data quality challenge that deals with ambiguous references in data and whose task is to identify all references that co-refer. Due to practical significance of the ER problem, many creative ER techniques have been proposed in the past, including those that analyze relationships that exist among entities in data. Such approaches view the database as an entity-relationship graph, where direct and indirect relationships correspond to paths in the graph. These techniques rely on measuring the connection strength among various nodes in the graph by using a connection strength (CS) model. While such approaches have demonstrated significant advantage over traditional ER techniques, currently they also have a significant limitation: the CS models that they use are intuition-based fixed models that tend to behave well in general, but are very generic and not tuned to a specific domain, leading to suboptimal result quality. Hence, in this article we propose an approach that employs supervised learning to adapt the connection strength measure to the given domain using the available past/training data. The adaptive approach has several advantages: it increases both the quality and efficiency of ER and it also minimizes the domain analyst participation needed to tune the CS model to the given domain. The extensive empirical evaluation demonstrates that the proposed approach reaches up to 8% higher accuracy than the graph-based ER methods that use fixed and intuition-based CS models.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {8},
numpages = {22},
keywords = {reference disambiguation, graph-based disambiguation, relationship analysis, lookup, Entity resolution, connection strength}
}

@article{10.1145/2378016.2378019,
author = {Cur\'{e}, Olivier},
title = {Improving the Data Quality of Drug Databases Using Conditional Dependencies and Ontologies},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2378016.2378019},
doi = {10.1145/2378016.2378019},
abstract = {Many health care systems and services exploit drug related information stored in databases. The poor data quality of these databases, e.g. inaccuracy of drug contraindications, can lead to catastrophic consequences for the health condition of patients. Hence it is important to ensure their quality in terms of data completeness and soundness.In the database domain, standard Functional Dependencies (FDs) and INclusion Dependencies (INDs), have been proposed to prevent the insertion of incorrect data. But they are generally not expressive enough to represent a domain-specific set of constraints. To this end, conditional dependencies, i.e. standard dependencies extended with tableau patterns containing constant values, have been introduced and several methods have been proposed for their discovery and representation. The quality of drug databases can be considerably improved by their usage.Moreover, pharmacology information is inherently hierarchical and many standards propose graph structures to represent them, e.g. the Anatomical Therapeutic Chemical classification (ATC) or OpenGalen’s terminology. In this article, we emphasize that the technologies of the Semantic Web are adapted to represent these hierarchical structures, i.e. in RDFS and OWL. We also present a solution for representing conditional dependencies using a query language defined for these graph oriented structures, namely SPARQL. The benefits of this approach are interoperability with applications and ontologies of the Semantic Web as well as a reasoning-based query execution solution to clean underlying databases.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {3},
numpages = {21},
keywords = {Data quality, conditional dependencies, description logics}
}

@article{10.1145/2378016.2378020,
author = {McNaull, James and Augusto, Juan Carlos and Mulvenna, Maurice and McCullagh, Paul},
title = {Data and Information Quality Issues in Ambient Assisted Living Systems},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2378016.2378020},
doi = {10.1145/2378016.2378020},
abstract = {Demographic aging, as a result of people living for longer, has put an increased burden on health and social care provision across most of the economies of the developed and developing world. In order to cope with the greater numbers of older people, together with increasing prevalence of chronic diseases, governments are looking to new ways to provide care and support to older people and their care providers. A growing trend is where health and social care providers are moving towards the use of assisted living technologies to provide care and assistance in the home. In this article, the research area of Ambient Assisted Living (AAL) systems is examined and the data, information and the higher-level contextual knowledge quality issues in relation to these systems, is discussed. Lack of quality control may result in an AAL system providing assistance and support based upon incorrect data, information and knowledge inputs, and this may have a detrimental effect on the person making use of the system. We propose a model whereby contextual knowledge gained during the AAL system’s reasoning cycle can be fed back to aid in further quality checking at the various architectural layers, and a realistic AAL scenario is provided to support this. Future research should be conducted in these areas, with the requirement of building quality criteria into the design and implementation of AAL systems.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {4},
numpages = {15},
keywords = {context-aware computing, Ambient assisted living, assisted living, quality of context}
}

@article{10.1145/2378016.2378017,
author = {O’Donoghue, John and Grimson, Jane and Seelman, Katherine},
title = {Introduction to the Special Issue on Information Quality: The Challenges and Opportunities in Healthcare Systems and Services},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2378016.2378017},
doi = {10.1145/2378016.2378017},
journal = {J. Data and Information Quality},
month = oct,
articleno = {1},
numpages = {4}
}

@article{10.1145/2378016.2378018,
author = {Collins, Claire and Janssens, Kelly},
title = {Creating a General (Family) Practice Epidemiological Database in Ireland - Data Quality Issue Management},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2378016.2378018},
doi = {10.1145/2378016.2378018},
abstract = {In Ireland, while detailed information is available regarding hospital attendance, little is known regarding general (family) practice attendance. However, it is conservatively estimated that there are almost nine times as many general practice encounters than there are hospital encounters each year in Ireland. This represents a very significant gap in health information. Indeed, general practice has been shown in other countries to be an important and rich source of information about the health of the population, their behaviors and their utilization of health services. Funded by the Health Information and Quality Authority (HIQA), the Irish College of General Practitioners (ICGP) undertook a feasibility study of diagnostic coding of routinely entered patient data and the creation of a national general practice morbidity and epidemiological database (GPMED project). This article outlines the process of data quality issue management undertaken.The study’s findings suggest that the quality of data collection and reporting structures available in general practice throughout Ireland at the outset of this project were not adequate to permit the creation of a database of sufficient quality for service planning and policy or epidemiological research. Challenges include the dearth of a minimum standard of data recorded in consultations by GPs and the absence of the digital data recording and exporting infrastructure within Irish patient management software systems. In addition, there is at present a lack of recognition regarding the value of such data for patient management and service planning---including importantly, data collectors who do not fully accept the merit of maintaining data, which has a direct consequence for data quality. The work of this project has substantial implications for the data available to the health sector in Ireland and contributes to the knowledge base internationally regarding general practice morbidity data.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {2},
numpages = {9},
keywords = {Data quality, epidemiology, family practice}
}

@article{10.1145/2378016.2378021,
author = {O’Donoghue, John and Herbert, John},
title = {Data Management within MHealth Environments: Patient Sensors, Mobile Devices, and Databases},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2378016.2378021},
doi = {10.1145/2378016.2378021},
abstract = {Pervasive environments generate large quantities of data, originating from backend servers, portable devices, and wireless mobile sensors. Pervasive sensing devices that monitor properties of the environment (including human beings) can be a large data source. Unprocessed datasets may include data that is faulty and irrelevant, and data that is important and useful. If not managed correctly the large amount of data from a data-rich pervasive environment may result in information overload or delivery of incorrect information.Context-sensitive quality data management aims to gather, verify, process, and manage the multiple data sources in a pervasive environment in order to deliver high quality, relevant information to the end-user. Managing the quality of data from different sources, correlating related data, and making use of context, are all essential in providing end users with accurate and meaningful data in real time. This requirement is especially true for critical applications such as in a medical environment.This article presents the Data Management System (DMS) architecture. It is designed to deliver quality data service to its users. The DMS architecture employs an agent-based middleware to intelligently and effectively manage all pervasive data sources, and to make use of context to deliver relevant information to the end-user. Two of the DMS components are presented: (1) data validation and (2) data consistency. The DMS components have been rigorously evaluated using various medical-based test cases.This article demonstrates a careful, precise approach to data based on the quality of the data and the context of its use. It emphasises the DMS architecture and the role of software agents in providing quality data management.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {5},
numpages = {20},
keywords = {body area network, mHealth, Data quality, data management}
}

@article{10.1145/2348828.2348830,
author = {Varol, Cihan and Bayrak, Coskun},
title = {Hybrid Matching Algorithm for Personal Names},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2348828.2348830},
doi = {10.1145/2348828.2348830},
abstract = {Companies acquire personal information from phone, World Wide Web, or email in order to sell or send an advertisement about their product. However, when this information is acquired, moved, copied, or edited, the data may lose its quality. Often, the use of data administrators or a tool that has limited capabilities to correct the mistyped information can cause many problems. Moreover, most of the correction techniques are particularly implemented for the words used in daily conversations. Since personal names have different characteristics compared to general text, a hybrid matching algorithm (PNRS) which employs phonetic encoding, string matching and statistical facts to provide a possible candidate for misspelled names is developed. At the end, the efficiency of the proposed algorithm is compared with other well known spelling correction techniques.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {8},
numpages = {18},
keywords = {Data quality, phonetic strategy, spelling correction, edit-distance, information quality}
}

@article{10.1145/2348828.2348829,
author = {Robb, David A. and Bowen, Paul L. and Borthick, A. Faye and Rohde, Fiona H.},
title = {Improving New Users’ Query Performance: Deterring Premature Stopping of Query Revision with Information for Forming Ex Ante Expectations},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2348828.2348829},
doi = {10.1145/2348828.2348829},
abstract = {As the volume of data in organizational databases grows, organizations are seeking to use this data to improve organizational success. To this end, users are being asked to query these databases to provide information to help answer questions posed by key management personnel. Users who have had extensive experience with an organization’s data can often detect the presence of errors in their queries when query results do not correspond to their ex ante expectations. New users, however, are less familiar with the data they will be querying. Having no, or limited, ex ante expectations for query results, new users may be unaware that the result produced by their query is incorrect. Unwarranted confidence in the correctness of their queries predisposes these users to stop looking for query errors even when their queries still contain errors. This behavior, premature stopping of query revision, prompts investigating whether new users’ query performance would improve if they were not only provided with, but used, readily available information to form ex ante expectations. Our results demonstrated a threshold effect in new users heeding information for forming ex ante expectations. That is, the mere availability of information for forming ex ante expectations made no difference in query performance. When admonishing users to heed ex ante information, however, there was an associated increase in the accuracy of their queries. These results suggest that users unfamiliar with a particular database might make fewer query errors if they not only received readily available information but were then prompted to use the information to form ex ante expectations for query results.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {7},
numpages = {22},
keywords = {database querying, information request, stopping rule, query accuracy, threshold effect, Confidence}
}

@article{10.1145/2287714.2287716,
author = {Yang, Yanjuan and Mannino, Michael},
title = {An Experimental Comparison of a Document Deception Detection Policy Using Real and Artificial Deception},
year = {2012},
issue_date = {August 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2287714.2287716},
doi = {10.1145/2287714.2287716},
abstract = {Developing policies to screen documents for deception is often hampered by the cost of data collection and the inability to evaluate policy alternatives due to lack of data. To lower data collection costs and increase the amount of data, artificially generated deception data can be used, but the impact of using artificially generated deception data is not well understood. This article studies the impact of artificially generated deception on document screening policies. The deception and truth data were collected from financial aid applications, a document-centric area with limited resources for screening. Real deception was augmented with artificial data generated by noise and deception generation models. Using the real data and artificially generated data, we designed an innovative experiment with deception type and deception rate as factors, and harmonic mean and cost as outcome variables. We used two budget models (fixed and variable) typically employed by financial aid offices to measure the cost of noncompliance in financial aid applications. The analysis included an evaluation of a common policy for deception screening using both fixed and varying screening rates. The results of the experiment provided evidence of similar performance of screening policy with real and artificial deception, suggesting the possibility of using artificially generated deception to reduce the costs associated with obtaining training data.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {6},
numpages = {25},
keywords = {boosted deception, noise, Screening policy, natural deception, data generation model, deception, artificial deception}
}

@article{10.1145/2287714.2287715,
author = {Yakout, Mohamed and Atallah, Mikhail J. and Elmagarmid, Ahmed},
title = {Efficient and Practical Approach for Private Record Linkage},
year = {2012},
issue_date = {August 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2287714.2287715},
doi = {10.1145/2287714.2287715},
abstract = {Record linkage is used to associate entities from multiple data sources. For example, two organizations contemplating a merger may want to know how common their customer bases are so that they may better assess the benefits of the merger. Another example is a database of people who are forbidden from a certain activity by regulators, may need to be compared to a list of people engaged in that activity. The autonomous entities who wish to carry out the record matching computation are often reluctant to fully share their data; they fear losing control over its subsequent dissemination and usage, or they want to insure privacy because the data is proprietary or confidential, and/or they are cautious simply because privacy laws forbid its disclosure or regulate the form of that disclosure. In such cases, the problem of carrying out the linkage computation without full data exchange has been called private record linkage. Previous private record linkage techniques have made use of a third party. We provide efficient techniques for private record linkage that improve on previous work in that (1) our techniques make no use of a third party, and (2) they achieve much better performance than previous schemes in terms of their execution time while maintaining acceptable quality of output compared to nonprivacy settings. Our protocol consists of two phases. The first phase primarily produces candidate record pairs for matching, by carrying out a very fast (but not accurate) matching between such pairs of records. The second phase is a novel protocol for efficiently computing distances between each candidate pair (without any expensive cryptographic operations such as modular exponentiations). Our experimental evaluation of our approach validates these claims.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {5},
numpages = {28},
keywords = {private information retrieval, privacy, linkage, secure scalar product, private linkage, integration, Record linkage}
}

@article{10.1145/2184442.2184444,
author = {Elizabeth, D. Shiloah and Nehemiah, H. Khanna and Raj, C. Sunil Retmin and Kannan, A.},
title = {A Novel Segmentation Approach for Improving Diagnostic Accuracy of CAD Systems for Detecting Lung Cancer from Chest Computed Tomography Images},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2184442.2184444},
doi = {10.1145/2184442.2184444},
abstract = {Segmentation of lung tissue is an important and challenging task in any computer aided diagnosis system. The accuracy of the segmentation subsystem determines the performance of the other subsystems in any computer aided diagnosis system based on image analysis. We propose a novel technique for segmentation of lung tissue from computed tomography of the chest. Manual segmentation of lung parenchyma becomes difficult with an enormous volume of images. The goal of this work is to present an automated approach to segmentation of lung parenchyma from the rest of the chest CT image. The approach involves the conventional optimal thresholding technique and operations based on convex edge and centroid properties of the lung region. The segmentation technique proposed in this article can be used to preprocess lung images given to a computer aided diagnosis system for diagnosis of lung disorders. This improves the diagnostic performance of the system. This has been tested by using it in a computer aided diagnosis system that was used for detection of lung cancer from chest computed tomography images. The results obtained show that the lungs can be correctly segmented even in the presence of peripheral pathology bearing regions; pathology bearing regions that could not be detected using a CAD system that applies optimal thresholding could be detected using a CAD system using out proposed approach for segmentation of lungs.},
journal = {J. Data and Information Quality},
month = may,
articleno = {4},
numpages = {16},
keywords = {thresholding, Computer aided diagnosis system, morphological operations, segmentation, convex image, juxtapleural nodules}
}

@article{10.1145/2184442.2184443,
author = {Sunyaev, Ali and Chornyi, Dmitry},
title = {Supporting Chronic Disease Care Quality: Design and Implementation of a Health Service and Its Integration with Electronic Health Records},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2184442.2184443},
doi = {10.1145/2184442.2184443},
abstract = {Chronic medical conditions take a huge toll on lives of a growing number of people and are a major contributor to the rising costs in healthcare. As patients are increasingly willing to take an active part in managing their conditions, chronic disease self-management programs and information systems that support them are recognized for their potential to improve the quality of healthcare delivery. These programs often rely on recording longitudinal patient data and analyzing it. Therefore, maintaining appropriate data quality is important for self-management programs to be efficient and safe. We designed and implemented a prototype of a health self-management service for chronically ill people. It is a distributed application that supports patients with diabetes at tracking their blood glucose levels. The main design goals were usability, extensibility, security, and interoperability. The system integrates with the Microsoft HealthVault and Google Health personal health record platforms. It utilizes industry-strength storage and security mechanisms, is scalable, and as a result, can be used to gather, securely store, and analyze patient data over long periods of time. In this article we examine how software information technology can support chronic disease self-management and its impact on the quality of patient data. Furthermore, we describe the requirements that drove the system's development, its architecture, and design decisions.},
journal = {J. Data and Information Quality},
month = may,
articleno = {3},
numpages = {21},
keywords = {health management system, personal health records, chronic disease management, Data quality}
}

@article{10.1145/2166788.2166789,
author = {Sachdeva, Shelly and Bhalla, Subhash},
title = {Semantic Interoperability in Standardized Electronic Health Record Databases},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2166788.2166789},
doi = {10.1145/2166788.2166789},
abstract = {Different clinics and hospitals have their own information systems to maintain patient data. This hinders the exchange of data among systems (and organizations). Hence there is a need to provide standards for data exchange. In digitized form, the individual patient's medical record can be stored, retrieved, and shared over a network through enhancement in information technology. Thus, electronic health records (EHRs) should be standardized, incorporating semantic interoperability. A subsequent step requires that healthcare professionals and patients get involved in using the EHRs, with the help of technological developments. This study aims to provide different approaches in understanding some current and challenging concepts in health informatics. Successful handling of these challenges will lead to improved quality in healthcare by reducing medical errors, decreasing costs, and enhancing patient care. The study is focused on the following goals: (1) understanding the role of EHRs; (2) understanding the need for standardization to improve quality; (3) establishing interoperability in maintaining EHRs; (4) examining a framework for standardization and interoperability (the openEHR architecture; (5) identifying the role of archetypes for knowledge-based systems; and (6) understanding the difficulties in querying HER data.},
journal = {J. Data and Information Quality},
month = may,
articleno = {1},
numpages = {37},
keywords = {standardization in EHR, semantic interoperability, archetype-based EHR, data quality in healthcare, Electronic health records, openEHR, quality-based EHR}
}

@article{10.1145/2166788.2166790,
author = {Brown, Steven and Rosenbloom, Trent S. and Hardenbrook, Shawn P. and Clark, Terry and Fielstein, Elliot and Elkin, Peter and Speroff, Ted},
title = {Documentation Quality and Time Costs: A Randomized Controlled Trial of Structured Entry versus Dictation},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2166788.2166790},
doi = {10.1145/2166788.2166790},
abstract = {The Department of Veterans Affairs (VA) performs over 800,000 disability exams and distributes over $37 billion in disability benefits per year. VA developed and deployed a computer-based disability exam documentation system in order to improve exam report quality and timeliness. We conducted a randomized controlled trial comparing joint disability examinations supported by computerized templates to the examinations documented via dictation, to determine if the system met the intended goals or had unintended consequences. Consenting veterans were randomized to undergo exams documented using computerized templates or via dictation. We compared exam report quality, documentation time costs, encounter length, total time to fulfill an exam request with a finalized exam report, and veteran satisfaction. Computer-based templates resulted in disability exam reports that had higher quality scores (p. 0.042) and were returned to the requesting office faster than exam reports created via dictation (p. 0.02).Documentation time and veteran satisfaction were similar for both the documentation techniques. Encounter length was significantly longer for the template group. Computer-based templates impacted the VA disability evaluation system by improving report quality scores and production time and lengthening encounter times. Oversight bodies have called for mandated use of computer-based templates nationwide. We believe mandates regarding use of health information technology should be guided by data regarding its positive and negative impacts.},
journal = {J. Data and Information Quality},
month = may,
articleno = {2},
numpages = {17}
}

@article{10.1145/2107536.2107538,
author = {M\"{u}ller, Heiko and Freytag, Johann-Christoph and Leser, Ulf},
title = {Improving Data Quality by Source Analysis},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2107536.2107538},
doi = {10.1145/2107536.2107538},
abstract = {In many domains, data cleaning is hampered by our limited ability to specify a comprehensive set of integrity constraints to assist in identification of erroneous data. An alternative approach to improve data quality is to exploit different data sources that contain information about the same set of objects. Such overlapping sources highlight hot-spots of poor data quality through conflicting data values and immediately provide alternative values for conflict resolution. In order to derive a dataset of high quality, we can merge the overlapping sources based on a quality assessment of the conflicting values. The quality of the resulting dataset, however, is highly dependent on our ability to asses the quality of conflicting values effectively.The main objective of this article is to introduce methods that aid the developer of an integrated system over overlapping, but contradicting sources in the task of improving the quality of data. Value conflicts between contradicting sources are often systematic, caused by some characteristic of the different sources. Our goal is to identify such systematic differences and outline data patterns that occur in conjunction with them. Evaluated by an expert user, the regularities discovered provide insights into possible conflict reasons and help to assess the quality of inconsistent values. The contributions of this article are two concepts of systematic conflicts: contradiction patterns and minimal update sequences. Contradiction patterns resemble a special form of association rules that summarize characteristic data properties for conflict occurrence. We adapt existing association rule mining algorithms for mining contradiction patterns. Contradiction patterns, however, view each class of conflicts in isolation, sometimes leading to largely overlapping patterns. Sequences of set-oriented update operations that transform one data source into the other are compact descriptions for all regular differences among the sources. We consider minimal update sequences as the most likely explanation for observed differences between overlapping data sources. Furthermore, the order of operations within the sequences point out potential dependencies between systematic differences. Finding minimal update sequences, however, is beyond reach in practice. We show that the problem already is NP-complete for a restricted set of operations. In the light of this intractability result, we present heuristics that lead to convincing results for all examples we considered.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {15},
numpages = {38},
keywords = {quality assessment, data cleaning, semantic distance measure, Conflict resolution}
}

@article{10.1145/2107536.2107539,
author = {Gelman, Irit Askira},
title = {Biases in Multi-Criteria, Satisficing Decisions Due to Data Errors},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2107536.2107539},
doi = {10.1145/2107536.2107539},
abstract = {This inquiry centers on an asymmetry, or bias, in the accuracy of multi-criteria, conjunctive, and disjunctive decisions, which originates from fundamental properties of the logical conjunction and disjunction operations. A mathematical-statistical analysis indicates that, as we keep adding criteria to a multi-criteria conjunctive or disjunctive decision rule, errors in the data produce decision errors asymmetrically. As a result, in conjunctive decisions, the probability of a false negative increases while the probability of a false positive decreases. In contrast, in disjunctive decisions, as we keep adding criteria, the probability of a false positive increases while that of a false negative decreases. For instance, in a conjunctive business decision rule, the probability of overlooking a bargain can be far greater than the probability of misjudging an unattractive offer to be a good one. A series of Monte Carlo simulations validates the analytical findings and explores the contribution of several additional factors.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {16},
numpages = {24}
}

@article{10.1145/2063504.2063505,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Editorial Notes Classification and Assessment of Large Amounts of Data: Examples in the Healthcare Industry and Collaborative Digital Libraries},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2063504.2063505},
doi = {10.1145/2063504.2063505},
journal = {J. Data and Information Quality},
month = dec,
articleno = {12},
numpages = {2}
}

@article{10.1145/2063504.2063506,
author = {Laur\'{\i}a, Eitel J. M. and March, Alan D.},
title = {Combining Bayesian Text Classification and Shrinkage to Automate Healthcare Coding: A Data Quality Analysis},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2063504.2063506},
doi = {10.1145/2063504.2063506},
abstract = {This article analyzes the data quality issues that emerge when training a shrinkage-based classifier with noisy data. A statistical text analysis technique based on a shrinkage-based variation of multinomial naive Bayes is applied to a set of free-text discharge diagnoses occurring in a number of hospitalizations. All of these diagnoses were previously coded according to the Spanish Edition of ICD9-CM. We deal with the issue of analyzing the predictive power and robustness of the statistical machine learning algorithm proposed for ICD-9-CM classification. We explore the effect of training the models using both clean and noisy data. In particular our work investigates the extent to which errors in free-text diagnoses propagate to the classification model. A measure of predictive accuracy is calculated for the text classification algorithm under analysis. Subsequently, the quality of the sample data is incrementally deteriorated by simulating errors in the text and/or codes. The predictive accuracy is recomputed for each of the noisy samples for comparison purposes. Our research shows that the shrinkage-based classifier is a valid alternative to automate ICD9-CM coding even under circumstances in which the quality of the training data is in question.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {13},
numpages = {22},
keywords = {automated coding, ICD9-CM, shrinkage, data quality, text classification}
}

@article{10.1145/2063504.2063507,
author = {Dalip, Daniel Hasan and Gon\c{c}alves, Marcos Andr\'{e} and Cristo, Marco and Calado, P\'{a}vel},
title = {Automatic Assessment of Document Quality in Web Collaborative Digital Libraries},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2063504.2063507},
doi = {10.1145/2063504.2063507},
abstract = {The old dream of a universal repository containing all of human knowledge and culture is becoming possible through the Internet and the Web. Moreover, this is happening with the direct collaborative participation of people. Wikipedia is a great example. It is an enormous repository of information with free access and open edition, created by the community in a collaborative manner. However, this large amount of information, made available democratically and virtually without any control, raises questions about its quality. In this work, we explore a significant number of quality indicators and study their capability to assess the quality of articles from three Web collaborative digital libraries. Furthermore, we explore machine learning techniques to combine these quality indicators into one single assessment. Through experiments, we show that the most important quality indicators are those which are also the easiest to extract, namely, the textual features related to the structure of the article. Moreover, to the best of our knowledge, this work is the first that shows an empirical comparison between Web collaborative digital libraries regarding the task of assessing article quality.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {14},
numpages = {30},
keywords = {Quality assessment, wiki, machine learning, quality features, SVM}
}

@article{10.1145/1891879.1891884,
author = {Ngugi, Benjamin and Kahn, Beverly K. and Tremaine, Marilyn},
title = {Typing Biometrics: Impact of Human Learning on Performance Quality},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1891879.1891884},
doi = {10.1145/1891879.1891884},
abstract = {The use of stolen personal-identifying information, like Social Security numbers, to commit identity fraud continues to be a major problem. The fact that an impostor can pass as the genuine user by possession of stolen identification information is a weakness in current authentication systems. Adding a biometric layer to the traditional knowledge and token-based authentication systems is one way to counter this problem. Physical biometrics, such as fingerprint systems, are highly accurate; hence, they would be the first choice for such applications but are often inappropriate. Behavioral biometrics, like biometric typing patterns, have the potential to fill this gap as another level of security but this research identified some deficiencies in performance quality. Two research streams for improvements have emerged. The first approach attempts to improve performance by building better classifiers, while the second attempts to attain the same goal by using richer identifying inputs. Both streams assume that the typing biometric patterns are stable over time. This study investigates the validity of this assumption by analyzing how students’ typing patterns behave over time. The results demonstrate that typing patterns change over time due to learning resulting in several performance quality challenges. First, the changing patterns lead to deteriorating authentication accuracy. Second, the relevancy of the reference biometric template created during training becomes questionable. Third, the deterioration in accuracy compromises the security of the whole system and fourth, the net effect brings to question whether the biometric keypad is no longer “fit for use” as an authentication system. These are critical data quality issues that need to be addressed if behavioral biometrics are to play a significant role in minimizing authentication fraud. Possible solutions to the problem, including biometric template updating and choice of uncorrelated PIN combinations, are suggested as potential topics for future research.},
journal = {J. Data and Information Quality},
month = feb,
articleno = {11},
numpages = {21},
keywords = {learning, typing patterns, keypads, Biometric}
}

@article{10.1145/1891879.1891880,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Editorial: In Search of Novel Ideas and Solutions with a Broader Context of Data Quality in Mind},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1891879.1891880},
doi = {10.1145/1891879.1891880},
journal = {J. Data and Information Quality},
month = feb,
articleno = {7},
numpages = {3}
}

@article{10.1145/1891879.1891881,
author = {Blake, Roger and Mangiameli, Paul},
title = {The Effects and Interactions of Data Quality and Problem Complexity on Classification},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1891879.1891881},
doi = {10.1145/1891879.1891881},
abstract = {Data quality remains a persistent problem in practice and a challenge for research. In this study we focus on the four dimensions of data quality noted as the most important to information consumers, namely accuracy, completeness, consistency, and timeliness. These dimensions are of particular concern for operational systems, and most importantly for data warehouses, which are often used as the primary data source for analyses such as classification, a general type of data mining. However, the definitions and conceptual models of these dimensions have not been collectively considered with respect to data mining in general or classification in particular. Nor have they been considered for problem complexity. Conversely, these four dimensions of data quality have only been indirectly addressed by data mining research. Using definitions and constructs of data quality dimensions, our research evaluates the effects of both data quality and problem complexity on generated data and tests the results in a real-world case. Six different classification outcomes selected from the spectrum of classification algorithms show that data quality and problem complexity have significant main and interaction effects. From the findings of significant effects, the economics of higher data quality are evaluated for a frequent application of classification and illustrated by the real-world case.},
journal = {J. Data and Information Quality},
month = feb,
articleno = {8},
numpages = {28},
keywords = {Data quality, data quality metrics and measurements, data mining, information quality}
}

@article{10.1145/1891879.1891882,
author = {Askira Gelman, Irit},
title = {GIGO or Not GIGO: The Accuracy of Multi-Criteria Satisficing Decisions},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1891879.1891882},
doi = {10.1145/1891879.1891882},
abstract = {The relationship between data accuracy and the resulting information accuracy is of great interest in numerous problem domains. An understanding of this relationship can improve the efficiency of data management and increase the accuracy and utility of information in problem-solving settings. Nonetheless, our understanding of that relationship is still partial. In fact, even the sign of the relationship is not well understood. Nearly all researchers have embraced the popular belief in GIGO (Garbage In, Garbage Out), which indicates a strong positive link between input accuracy and output accuracy. However, there is evidence that hints to a more complex association. This article addresses the relationship between input accuracy and output accuracy, particularly the sign of that relationship, in satisficing decisions that apply a conjunctive or disjunctive rule for combining selected criteria. Analysis of a simple scenario shows a surprising result: the sign of that relationship varies; higher input accuracy can lead to lower output accuracy. This article derives criteria that determine the sign of the relationship and explains and illustrates conditions in which the sign is negative. The findings of this research imply certain rules for guiding data quality management resource allocation and design decisions in similar scenarios.},
journal = {J. Data and Information Quality},
month = feb,
articleno = {9},
numpages = {27},
keywords = {analytical modeling, accuracy, Multi-criteria satisficing decisions, data quality management, garbage in garbage out, GIGO, data quality}
}

@article{10.1145/1891879.1891883,
author = {Fan, Xiaoming and Wang, Jianyong and Pu, Xu and Zhou, Lizhu and Lv, Bing},
title = {On Graph-Based Name Disambiguation},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1891879.1891883},
doi = {10.1145/1891879.1891883},
abstract = {Name ambiguity stems from the fact that many people or objects share identical names in the real world. Such name ambiguity decreases the performance of document retrieval, Web search, information integration, and may cause confusion in other applications. Due to the same name spellings and lack of information, it is a nontrivial task to distinguish them accurately. In this article, we focus on investigating the problem in digital libraries to distinguish publications written by authors with identical names. We present an effective framework named GHOST (abbreviation for GrapHical framewOrk for name diSambiguaTion), to solve the problem systematically. We devise a novel similarity metric, and utilize only one type of attribute (i.e., coauthorship) in GHOST. Given the similarity matrix, intermediate results are grouped into clusters with a recently introduced powerful clustering algorithm called Affinity Propagation. In addition, as a complementary technique, user feedback can be used to enhance the performance. We evaluated the framework on the real DBLP and PubMed datasets, and the experimental results show that GHOST can achieve both high precision and recall.},
journal = {J. Data and Information Quality},
month = feb,
articleno = {10},
numpages = {23},
keywords = {clustering, similarity, graph, Name disambiguation}
}

@article{10.1145/1805286.1805287,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Editors’ Comments: ACM Journal of Data and Information Quality (JDIQ) is Alive and Well!},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1805286.1805287},
doi = {10.1145/1805286.1805287},
journal = {J. Data and Information Quality},
month = jul,
articleno = {1},
numpages = {5}
}

@article{10.1145/1805286.1805292,
author = {Talburt, John R. and Madnick, Stuart E. and Lee, Yang W.},
title = {Call for Papers Special Issue on Entity Resolution},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1805286.1805292},
doi = {10.1145/1805286.1805292},
journal = {J. Data and Information Quality},
month = jul,
articleno = {6},
numpages = {2}
}

@article{10.1145/1805286.1805289,
author = {Jensen, Matthew L. and Burgoon, Judee K. and Nunamaker, Jay F.},
title = {Judging the Credibility of Information Gathered from Face-to-Face Interactions},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1805286.1805289},
doi = {10.1145/1805286.1805289},
abstract = {One of the most pernicious threats to information quality comes through perpetration of deception by information suppliers. Deception undermines many critical dimensions of information quality, such as accuracy, completeness, and believability. Despite this threat, information gatherers are ill equipped to assess the credibility of information suppliers. This work presents a prototype system that examines messages gathered during direct, face-to-face information gathering. The system unobtrusively identifies kinesic and linguistic features that may indicate deception in information suppliers’ messages. System use was found to significantly improve assessment ability in between-subjects and within-subjects tests. The improved ability to accurately assess credibility during face-to-face interactions should yield higher information quality.},
journal = {J. Data and Information Quality},
month = jul,
articleno = {3},
numpages = {20},
keywords = {information veracity, linguistics, human-computer interaction, decision-aids, kinesics, deception detection, Credibility assessment}
}

@article{10.1145/1805286.1805291,
author = {Magnani, Matteo and Montesi, Danilo},
title = {A Survey on Uncertainty Management in Data Integration},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1805286.1805291},
doi = {10.1145/1805286.1805291},
abstract = {In the last few years, uncertainty management has come to be recognized as a fundamental aspect of data integration. It is now accepted that it may not be possible to remove uncertainty generated during data integration processes and that uncertainty in itself may represent a source of relevant information. Several issues, such as the aggregation of uncertain mappings and the querying of uncertain mediated schemata, have been addressed by applying well-known uncertainty management theories. However, several problems lie unresolved. This article sketches an initial picture of this highly active research area; it details existing works in the light of a homogeneous framework, and identifies and discusses the leading issues awaiting solutions.},
journal = {J. Data and Information Quality},
month = jul,
articleno = {5},
numpages = {33},
keywords = {uncertainty, Data integration}
}

@article{10.1145/1805286.1805288,
author = {Tremblay, Monica Chiarini and Dutta, Kaushik and Vandermeer, Debra},
title = {Using Data Mining Techniques to Discover Bias Patterns in Missing Data},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1805286.1805288},
doi = {10.1145/1805286.1805288},
abstract = {In today’s data-rich environment, decision makers draw conclusions from data repositories that may contain data quality problems. In this context, missing data is an important and known problem, since it can seriously affect the accuracy of conclusions drawn. Researchers have described several approaches for dealing with missing data, primarily attempting to infer values or estimate the impact of missing data on conclusions. However, few have considered approaches to characterize patterns of bias in missing data, that is, to determine the specific attributes that predict the missingness of data values. Knowledge of the specific systematic bias patterns in the incidence of missing data can help analysts more accurately assess the quality of conclusions drawn from data sets with missing data. This research proposes a methodology to combine a number of Knowledge Discovery and Data Mining techniques, including association rule mining, to discover patterns in related attribute values that help characterize these bias patterns. We demonstrate the efficacy of our proposed approach by applying it on a demo census dataset seeded with biased missing data. The experimental results show that our approach was able to find seeded biases and filter out most seeded noise.},
journal = {J. Data and Information Quality},
month = jul,
articleno = {2},
numpages = {19},
keywords = {missing data, Data quality, pattern discovery}
}

@article{10.1145/1805286.1805290,
author = {Meda, Hema S. and Sen, Anup Kumar and Bagchi, Amitava},
title = {On Detecting Data Flow Errors in Workflows},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1805286.1805290},
doi = {10.1145/1805286.1805290},
abstract = {When designing a business workflow, it is customary practice to create the control flow structure first and to ensure its correctness. Information about the flow of data is introduced subsequently into the workflow and its correctness is independently verified. Improper specification of data requirements of tasks and XOR splits can cause problems such as wrong branching at XOR splits and the failure of tasks to execute. Here we present a graph traversal algorithm called GTforDF for detecting data flow errors in both nested and unstructured workflows, and illustrate its operation on realistic examples. Two of these have interconnected loops and are free of control flow errors, and the third one is an unstructured loop-free workflow. Our approach extends and generalizes data flow verification methods that have been recently proposed. It also makes use of the concept of corresponding pairs lately introduced in control flow verification. It thus has the potential for development into a unified algorithmic procedure for the concurrent detection of control flow and data flow errors. The correctness of the algorithm has been proved theoretically. It has also been tested experimentally on many examples.},
journal = {J. Data and Information Quality},
month = jul,
articleno = {4},
numpages = {31},
keywords = {Data flow errors, Corresponding pair, Workflow management}
}

@article{10.1145/1659225.1659226,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Editors’ Comments Where the JDIQ Articles Come From: Incubating Research in an Emerging Field},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/1659225.1659226},
doi = {10.1145/1659225.1659226},
journal = {J. Data and Information Quality},
month = dec,
articleno = {13},
numpages = {5}
}

@article{10.1145/1659225.1659227,
author = {Sessions, V. and Valtorta, M.},
title = {Towards a Method for Data Accuracy Assessment Utilizing a Bayesian Network Learning Algorithm},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/1659225.1659227},
doi = {10.1145/1659225.1659227},
abstract = {This research develops a data quality algorithm entitled the Accuracy Assessment Algorithm (AAA). This is an extension of research in developing an enhancement to a Bayesian Network (BN) learning algorithm called the Data Quality (DQ) algorithm. This new algorithm is concerned with estimating the accuracy levels of a dataset by assessing the quality of the data with no prior knowledge of the dataset. The AAA and associated metrics were tested using two canonical BNs and one large-scale medical network. The article presents the results regarding the efficacy of the algorithm and the implications for future research and practice.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {14},
numpages = {34},
keywords = {Bayesian networks, PC algorithm, accuracy levels, data quality assessment}
}

@article{10.1145/1659225.1659228,
author = {Even, Adir and Shankaranarayanan, G.},
title = {Dual Assessment of Data Quality in Customer Databases},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/1659225.1659228},
doi = {10.1145/1659225.1659228},
abstract = {Quantitative assessment of data quality is critical for identifying the presence of data defects and the extent of the damage due to these defects. Quantitative assessment can help define realistic quality improvement targets, track progress, evaluate the impacts of different solutions, and prioritize improvement efforts accordingly. This study describes a methodology for quantitatively assessing both impartial and contextual data quality in large datasets. Impartial assessment measures the extent to which a dataset is defective, independent of the context in which that dataset is used. Contextual assessment, as defined in this study, measures the extent to which the presence of defects reduces a dataset’s utility, the benefits gained by using that dataset in a specific context. The dual assessment methodology is demonstrated in the context of Customer Relationship Management (CRM), using large data samples from real-world datasets. The results from comparing the two assessments offer important insights for directing quality maintenance efforts and prioritizing quality improvement solutions for this dataset. The study describes the steps and the computation involved in the dual-assessment methodology and discusses the implications for applying the methodology in other business contexts and data environments.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {15},
numpages = {29},
keywords = {total data quality management, customer relationship management, CRM, databases, Data quality, information value}
}

@article{10.1145/1659225.1659229,
author = {Fisher, Craig W. and Lauria, Eitel J. M. and Matheus, Carolyn C.},
title = {An Accuracy Metric: Percentages, Randomness, and Probabilities},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/1659225.1659229},
doi = {10.1145/1659225.1659229},
abstract = {Practitioners and researchers regularly refer to error rates or accuracy percentages of databases. The former is the number of cells in error divided by the total number of cells; the latter is the number of correct cells divided by the total number of cells. However, databases may have similar error rates (or accuracy percentages) but differ drastically in the complexity of their accuracy problems. A simple percent does not provide information as to whether the errors are systematic or randomly distributed throughout the database. We expand the accuracy metric to include a randomness measure and include a probability distribution value. The proposed randomness check is based on the Lempel-Ziv (LZ) complexity measure. Through two simulation studies we show that the LZ complexity measure can clearly differentiate as to whether the errors are random or systematic. This determination is a significant first step and is a major departure from the percentage-alone technique. Once it is determined that the errors are random, a probability distribution, Poisson, is used to help address various managerial questions.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {16},
numpages = {21},
keywords = {Data and information quality, randomness, complexity}
}

@article{10.1145/1659225.1659230,
author = {Ababneh, Sufyan and Ansari, Rashid and Khokhar, Ashfaq},
title = {Compensated Signature Embedding for Multimedia Content Authentication},
year = {2009},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/1659225.1659230},
doi = {10.1145/1659225.1659230},
abstract = {One of the main goals of digital content authentication and preservation techniques is to guarantee the originality and quality of the information. In this article, robust watermarking is used to embed content-based fragile signatures in multimedia signals to achieve efficient authentication without requiring any third-party reference or side information. To overcome the signature alteration caused by the embedding perturbation and other possible encoding operations, a closed-form compensation technique is proposed for ensuring signature consistency by employing a Lagrangian-based approach. A minimum distortion criterion is used to ensure signal quality. The effectiveness of the proposed approach is investigated with simulations of examples of image authentication in which signatures are designed to reveal tamper localization. Results using quantitative performance criteria show successful authentication over a range of robustness in embedding watermarks using both QIM-DM and spread-spectrum techniques. A comparison with two iterative compensation schemes is also presented.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {17},
numpages = {20},
keywords = {compensated signature embedding, Content authentication, watermarking}
}

@article{10.1145/1577840.1577841,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Editorial Letter for the Special Issue on Data Quality in Databases and Information Systems},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577841},
doi = {10.1145/1577840.1577841},
journal = {J. Data and Information Quality},
month = sep,
articleno = {6},
numpages = {2}
}

@article{10.1145/1577840.1577843,
author = {Dash, Manoranjan and Singhania, Ayush},
title = {Mining in Large Noisy Domains},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577843},
doi = {10.1145/1577840.1577843},
abstract = {In this article we address the issue of how to mine efficiently in large and noisy data. We propose an efficient sampling algorithm (Concise) as a solution for large and noisy data. Concise is far more superior than the Simple Random Sampling (SRS) in selecting a representative sample. Particularly when the data is very large and noisy, Concise achieves the maximum gain over SRS. The comparison is in terms of their impact on subsequent data mining tasks, specifically, classification, clustering, and association rule mining. We compared Concise with a few existing noise removal algorithms followed by SRS. Although the accuracy of mining results are similar, Concise spends very little time compared to the existing algorithms because Concise has linear time complexity.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {8},
numpages = {30},
keywords = {classification, association rule mining, Information filtering, selection process, sampling, clustering, data mining}
}

@article{10.1145/1577840.1577842,
author = {Naumann, Felix and Raschid, Louiqa},
title = {Guest Editorial for the Special Issue on Data Quality in Databases},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577842},
doi = {10.1145/1577840.1577842},
journal = {J. Data and Information Quality},
month = sep,
articleno = {7},
numpages = {3}
}

@article{10.1145/1577840.1577846,
author = {Embury, Suzanne M. and Missier, Paolo and Sampaio, Sandra and Greenwood, R. Mark and Preece, Alun D.},
title = {Incorporating Domain-Specific Information Quality Constraints into Database Queries},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577846},
doi = {10.1145/1577840.1577846},
abstract = {The range of information now available in queryable repositories opens up a host of possibilities for new and valuable forms of data analysis. Database query languages such as SQL and XQuery offer a concise and high-level means by which such analyses can be implemented, facilitating the extraction of relevant data subsets into either generic or bespoke data analysis environments. Unfortunately, the quality of data in these repositories is often highly variable. The data is still useful, but only if the consumer is aware of the data quality problems and can work around them. Standard query languages offer little support for this aspect of data management. In principle, however, it should be possible to embed constraints describing the consumer’s data quality requirements into the query directly, so that the query evaluator can take over responsibility for enforcing them during query processing.Most previous attempts to incorporate information quality constraints into database queries have been based around a small number of highly generic quality measures, which are defined and computed by the information provider. This is a useful approach in some application areas but, in practice, quality criteria are more commonly determined by the user of the information not by the provider. In this article, we explore an approach to incorporating quality constraints into database queries where the definition of quality is set by the user and not the provider of the information. Our approach is based around the concept of a quality view, a configurable quality assessment component into which domain-specific notions of quality can be embedded. We examine how quality views can be incorporated into XQuery, and draw from this the language features that are required in general to embed quality views into any query language. We also propose some syntactic sugar on top of XQuery to simplify the process of querying with quality constraints.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {11},
numpages = {31},
keywords = {Information quality, XQuery, database query languages, views}
}

@article{10.1145/1577840.1577847,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Call for Papers Special Issue on Healthcare Information Quality: The Challenges and Opportunities in Healthcare Systems and Services},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577847},
doi = {10.1145/1577840.1577847},
journal = {J. Data and Information Quality},
month = sep,
articleno = {12},
numpages = {3}
}

@article{10.1145/1577840.1577845,
author = {Klein, A. and Lehner, W.},
title = {Representing Data Quality in Sensor Data Streaming Environments},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577845},
doi = {10.1145/1577840.1577845},
abstract = {Sensors in smart-item environments capture data about product conditions and usage to support business decisions as well as production automation processes. A challenging issue in this application area is the restricted quality of sensor data due to limited sensor precision and sensor failures. Moreover, data stream processing to meet resource constraints in streaming environments introduces additional noise and decreases the data quality. In order to avoid wrong business decisions due to dirty data, quality characteristics have to be captured, processed, and provided to the respective business task. However, the issue of how to efficiently provide applications with information about data quality is still an open research problem.In this article, we address this problem by presenting a flexible model for the propagation and processing of data quality. The comprehensive analysis of common data stream processing operators and their impact on data quality allows a fruitful data evaluation and diminishes incorrect business decisions. Further, we propose the data quality model control to adapt the data quality granularity to the data stream interestingness.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {10},
numpages = {28},
keywords = {data quality, Data stream processing, smart items}
}

@article{10.1145/1577840.1577844,
author = {Moustakides, George V. and Verykios, Vassilios S.},
title = {Optimal Stopping: A Record-Linkage Approach},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/1577840.1577844},
doi = {10.1145/1577840.1577844},
abstract = {Record-linkage is the process of identifying whether two separate records refer to the same real-world entity when some elements of the record’s identifying information (attributes) agree and others disagree. Existing record-linkage decision methodologies use the outcomes from the comparisons of the whole set of attributes. Here, we propose an alternative scheme that assesses the attributes sequentially, allowing for a decision to made at any attribute’s comparison stage, and thus before exhausting all available attributes. The scheme we develop is optimum in that it minimizes a well-defined average cost criterion while the corresponding optimum solution can be easily mapped into a decision tree to facilitate the record-linkage decision process. Experimental results performed in real datasets indicate the superiority of our methodology compared to existing approaches.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {9},
numpages = {34},
keywords = {Record-linkage, duplicate detection, optimal stopping}
}

@article{10.1145/1515693.1515694,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Editorial for the Inaugural Issue of the ACM Journal of Data and Information Quality (JDIQ)},
year = {2009},
issue_date = {June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1515693.1515694},
doi = {10.1145/1515693.1515694},
journal = {J. Data and Information Quality},
month = jun,
articleno = {1},
numpages = {6}
}

@article{10.1145/1515693.1516680,
author = {Madnick, Stuart E. and Wang, Richard Y. and Lee, Yang W. and Zhu, Hongwei},
title = {Overview and Framework for Data and Information Quality Research},
year = {2009},
issue_date = {June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1515693.1516680},
doi = {10.1145/1515693.1516680},
abstract = {Awareness of data and information quality issues has grown rapidly in light of the critical role played by the quality of information in our data-intensive, knowledge-based economy. Research in the past two decades has produced a large body of data quality knowledge and has expanded our ability to solve many data and information quality problems. In this article, we present an overview of the evolution and current landscape of data and information quality research. We introduce a framework to characterize the research along two dimensions: topics and methods. Representative papers are cited for purposes of illustrating the issues addressed and the methods used. We also identify and discuss challenges to be addressed in future research.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {2},
numpages = {22},
keywords = {information quality, Data quality, research topics, research methods}
}

@article{10.1145/1515693.1515695,
author = {Li, Xiao-Bai},
title = {A Bayesian Approach for Estimating and Replacing Missing Categorical Data},
year = {2009},
issue_date = {June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1515693.1515695},
doi = {10.1145/1515693.1515695},
abstract = {We propose a new approach for estimating and replacing missing categorical data. With this approach, the posterior probabilities of a missing attribute value belonging to a certain category are estimated using the simple Bayes method. Two alternative methods for replacing the missing value are proposed: The first replaces the missing value with the value having the estimated maximum probability; the second uses a value that is selected with probability proportional to the estimated posterior distribution. The effectiveness of the proposed approach is evaluated based on some important data quality measures for data warehousing and data mining. The results of the experimental study demonstrate the effectiveness of the proposed approach.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {3},
numpages = {11},
keywords = {Missing data, simple Bayes, data quality}
}

@article{10.1145/1515693.1515696,
author = {Weber, Kristin and Otto, Boris and \"{O}sterle, Hubert},
title = {One Size Does Not Fit All---A Contingency Approach to Data Governance},
year = {2009},
issue_date = {June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1515693.1515696},
doi = {10.1145/1515693.1515696},
abstract = {Enterprizes need Data Quality Management (DQM) to respond to strategic and operational challenges demanding high-quality corporate data. Hitherto, companies have mostly assigned accountabilities for DQM to Information Technology (IT) departments. They have thereby neglected the organizational issues critical to successful DQM. With data governance, however, companies may implement corporate-wide accountabilities for DQM that encompass professionals from business and IT departments. This research aims at starting a scientific discussion on data governance by transferring concepts from IT governance and organizational theory to the previously largely ignored field of data governance. The article presents the first results of a community action research project on data governance comprising six international companies from various industries. It outlines a data governance model that consists of three components (data quality roles, decision areas, and responsibilities), which together form a responsibility assignment matrix. The data governance model documents data quality roles and their type of interaction with DQM activities. In addition, the article describes a data governance contingency model and demonstrates the influence of performance strategy, diversification breadth, organization structure, competitive strategy, degree of process harmonization, degree of market regulation, and decision-making style on data governance. Based on these findings, companies can structure their specific data governance model.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {4},
numpages = {27},
keywords = {data quality management, contingency theory, IT governance, data governance model, Data governance}
}

@article{10.1145/1515693.1515697,
author = {Heinrich, B. and Klier, M. and Kaiser, M.},
title = {A Procedure to Develop Metrics for Currency and Its Application in CRM},
year = {2009},
issue_date = {June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/1515693.1515697},
doi = {10.1145/1515693.1515697},
abstract = {Due to the importance of using up-to-date data in information systems, this article analyzes how the data-quality dimension currency can be quantified. Based on several requirements (e.g., normalization and interpretability) and a literature review, we design a procedure to develop probability-based metrics for currency which can be adjusted to the specific characteristics of data attribute values. We evaluate the presented procedure with regard to the requirements and illustrate the applicability as well as its practical benefit. In cooperation with a major German mobile services provider, the procedure was applied in the field of campaign management in order to improve both success rates and profits.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {5},
numpages = {28},
keywords = {Data quality, information quality, metrics}
}

